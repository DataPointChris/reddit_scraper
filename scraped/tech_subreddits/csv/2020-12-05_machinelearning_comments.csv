comment_id,post_id,comment,upvotes
gept50k,k7bokn,"Fuck these idiots who are either intentionally sabotaging communities across all of society or are so short sighted they are apparently throwing drinks at each other over a code base.

What's next?

Formula 1 pit crews are gonna spray each other with gasoline over which nuts are better for shortening tire changes during pit stops?",2.0
gep77j6,k77sxz,"Jeff's email writes:

&gt;Timnit responded with an email requiring that a number of conditions be met in order for her to continue working at Google, including revealing the identities of every person who Megan and I had spoken to and consulted as part of the review of the paper and the exact feedback.  Timnit wrote that if we didn’t meet these demands, she would leave Google and work on an end date.

This makes it sound like the resignation was more of a decision on Timnit's part (""do this unreasonable thing or I'm leaving"").  However, Timnit [writes](https://twitter.com/timnitGebru/status/1334352694664957952) on Twitter:

&gt;I was fired by [@JeffDean](https://twitter.com/JeffDean) for my email to Brain women and Allies. My corp account has been cutoff. So I've been immediately fired :-)

Which makes it sound like the precipitating event was the angry email linked on platformer (which to be fair does sound like ""quitting talk""--""stop writing your documents because it doesn’t make a difference"", ""I suggest focusing on leadership accountability and thinking through what types of pressures can also be applied from the outside"", etc.)

So there's a key factual issue unresolved here--did Timnit say she would quit if her demands weren't met?  Or is this something Jeff Dean [made up](https://twitter.com/EricaJoy/status/1335015980230045698)?

Has Timnit explicitly denied this business about the conditions anywhere?  Or has she just chosen to frame the story as ""I was fired by Jeff Dean"" without offering an explicit denial?  Looking to hear from the Timnit fans here",69.0
gep9v94,k77sxz,"She mentioned herself the conditional resignation in the first tweet or second tweet on the subject, like two days ago. So it’s unlikely he’s making that up.",55.0
gepeoj8,k77sxz,Does anyone have a link to this tweet?,5.0
gepjkeo,k77sxz,"[https://twitter.com/timnitGebru/status/1334341991795142667](https://twitter.com/timnitGebru/status/1334341991795142667)

&gt;Apparently my manager’s manager sent an email my direct reports saying she accepted my resignation. I hadn’t resigned—I had asked for simple conditions first and said I would respond when I’m back from vacation. But I guess she decided for me :) that’s the lawyer speak.

and [https://twitter.com/timnitGebru/status/1334343577044979712](https://twitter.com/timnitGebru/status/1334343577044979712)

&gt;I said here are the conditions. If you can meet them great I’ll take my name off this paper, if not then I can work on a last date. Then she sent an email to my direct reports saying she has accepted my resignation. So that is google for you folks. You saw it happen right here.

&amp;#x200B;

So, /u/1xKzERRdLm \- in answer to your questions of ""did Timnit say she would quit if her demands weren't  met?  Or is this something Jeff Dean [made up](https://twitter.com/EricaJoy/status/1335015980230045698)? Has Timnit explicitly denied this business about the conditions anywhere?"" ...it looks like Timnit has actually *confirmed* these things, rather than denying them.   Based on reading her tweets (in conjunction with Jeff's email), it really looks like she wrote ""if you don't do these things, I quit"" and Google came back with ""ok, so you've quit.""",24.0
gepjvr8,k77sxz,ty,2.0
gepf1zk,k77sxz,It's in the OP.,5.0
gepi6z8,k77sxz,oh im dumb lol,1.0
gep8lhb,k77sxz,This is my understanding as a random internet bystander.,10.0
gep9lej,k77sxz,"&gt; So there's a key factual issue unresolved here--did Timnit say she would quit if her demands weren't met? Or is this something Jeff Dean made up?

I mean, yeah she did say she'd be happy to talk about finding a good last date so that a replacement could be put in place, and she could do a proper handover, once she was back from vacation.

Google said ""a good last date is yesterday"". That's not ""accepting a resignation"", that's firing someone.",41.0
gepgerm,k77sxz,"The moment you resign you should be prepared to walk the front door immediately. That is nothing new in corporate world. First time I resigned they told me that. The two weeks notice is just a nicety. 

Actually the advise I got about resigning was to be sure to have all your stuff backed up before even hinting at it.",39.0
gepi18n,k77sxz,"Exactly. I'm surprised people are surprised. Perhaps things are very different in silicon valley, but everywhere else it's pretty standard, even in Europe where I am where labor laws are more progressive than the US.",20.0
gepn43d,k77sxz,"no these people are just very sheltered, embarrassing level of ignorance to how the rest of the workforce is.  You pretty much never get to set the terms of your resignation lol",13.0
gepj2ez,k77sxz,"That's fucked up. Get a god damn union, and at least try to find an employer that actually appreciates what you do.

I've never had a job where I didn't do a proper handover.",-4.0
geplbc6,k77sxz,"You are resigning out of your own volition. In my case it was to a better job. No one forced you to do that. Even with a Union, the company has no obligation to keep you after you resign",11.0
gepqvta,k77sxz,"&gt; Even with a Union, the company has no obligation to keep you after you resign

Getting off topic, but some union agreements will definitely have terms about how resignations work.",2.0
gepa4nq,k77sxz,"Thanks for representing the pro-Timnit perspective, upvoted.

It seems like maybe what happened was she had delivered her ultimatum, Google wasn't having it, so there was a plan for her to leave, and then she started stirring things up on the mailing list (""stop writing your documents and start applying pressure from the outside""), and Google was like ""we aren't going to pay you a salary to stir things up like this"".",26.0
gepazjd,k77sxz,"&gt; It seems like maybe what happened was she had delivered her ultimatum, Google wasn't having it, so there was a plan for her to leave, and then she started stirring things up on the mailing list 

That's a specific sequence of events I haven't seen anywhere else. In particular, I don't think Gebru or anyone else has indicated she got any response regarding her conditions for the paper retraction prior to her firing.",7.0
gepcfoi,k77sxz,"Aren't they still paying her a salary for the notice period, just not having her work during it which is pretty standard?",5.0
geprqh2,k77sxz,"It's a moot distinction anyways, her childish antics justified firing her.",3.0
gepd6jr,k77sxz,What's the difference between setting a good last date and resignation?,8.0
gepebc2,k77sxz,Have you never been actually working anywhere?,-17.0
gepffum,k77sxz,(Edit: M)Any big company revokes your access at the instant you hand in your 2 week notice,12.0
gepfyr7,k77sxz,This isn't true. Why do people keep saying this? My husband and I have 3 faangs between us and this hasn't happened at any.,7.0
gepgaw4,k77sxz,"&gt; Why do people keep saying this? 

Because it is a very common experience. 

Perhaps not every company does this.",13.0
gepi8wh,k77sxz,What's relevant here is Google and their historical practices. I know several people who have left Google (but not Google Brain) and this didn't happen to them.,5.0
gepqf3y,k77sxz,It would of course happen if you are blasting angry emails to a sizable internal mailing list,3.0
gepp2s2,k77sxz,Pretty common in financial sector with NDA's. Give your 2 week notice and you're escorted out. Your belongings are usually mailed back to you.,3.0
gephfst,k77sxz,"It's certainly true for most other companies, even more so if you're going to a competitor. Just put yourself in the shoes of the employer for a minute. Of course you'll want to have the person out ASAP as your interests are no longer aligned. Maybe FAANG is different?",2.0
geps5vl,k77sxz,"I don’t know why I’m not seeing this in more places, but having never been in this position, I could very well be wrong.

Isn’t it likely the case that Timnit isn’t entitled to severance if she resigns? People are freaking out about Jeff Dean “gaslighting” her by saying resignation, but if he publicly says she was fired, then that would have legal implications, right?

Secondly, I get that it wasn’t very nice to let her go immediately, but doing handovers are primarily for the benefit of the company. So if Google decides that they don’t need her to help with transition / if they deemed that her staying at the company any longer would be a risk, then I think that it makes sense.

Anyone whos worked in a corporate setting knows that you can 100% get fired for sending emails in poor taste, and her submitting the terms for her resignation was an opportunity for Google to get rid of her with no strings attached. I’m not saying I wouldn’t be pissed if it happened to me, but from an outside perspective, it seems like she played herself a bit",2.0
gept3mc,k77sxz,"Yes, this is one very small part. She can still be entitled to unemployment - which is paltry - if she was forced to ""resign"", like in this case.

Severance is supposed to be standard as paying different classes of employees different severance amounts can open the company up to discriminations charges. Generally it is 2-4 weeks for every year you worked at Google. I have heard of cases where people who were fired but had inside dirt on the company were paid larger sums.

These are all paltry sums for a company like Google. Skipping out on severance did not factor into their decision to treat her like this.",1.0
gepmrfd,k77sxz,"I think that people shouldn't be surprised to have their resignation accepted if they offer an ultimatum like that, but it could have been handled much better by just giving her a couple of weeks notice. I suspect that the real reason her resignation was made effective immediately was the email sent to the Brain women and Allies since it explicitly asked other employees to stop working on DEI things and even effectively asked them to lobby Congress to put external pressure on Google. However, if she hadn't written that email I suspect the long term outcome would probably have been the same.",7.0
gepnvro,k77sxz,"You don’t have to suspect it. The HR person told Timnit this explicitly. https://twitter.com/timnitgebru/status/1334364734418726912?s=21

Basically - 
1) do x/y/a or I will resign from Google
2) we won’t do x/y/z. We accept your resignation.
3) By The Way, you sent a pretty inappropriate email. Thus we accept your resignation as of now.",10.0
gepor94,k77sxz,Thanks for the clarification. I think people are confused because there are effectively two reasons for this: the paper and the email - and I've seen a lot more focus on discussing the paper.,1.0
geps504,k77sxz,Imaging what further tantrums and emails she would send if they'd given her another two weeks to do so.,1.0
geprlmp,k77sxz,"I have been in a very similar situation where the company said I quit and I maintained I did not quit. I suspect this is what happened with Timnit. I will bet she did not resign voluntarily, but Google HR and Legal have determined on their own side that it was a ""legal"" equivalent of resigning. A legal ""Gotcha!"".

I worked for a company with a very high attrition rate and poor management. Manager was being verbally and mentally abusive. I was on a PIP. I wanted to stay in the company and transfer teams but manager blocked it. With the abusive mental situation increasing day by day, I could not bear it anymore. One coworker even killed himself over a similar situation he had.

I spoke about severance with a person on the HR team who was dealing with my case. HR person said it was a possibility and talked about what to expect without committing to it. I was recovering from a miscarriage so I was really concerned about health insurance continuation. So I went to my manager to say that I want to consider resigning if the company provides me severance. Manager asked for a final date. I gave him a date and stressed it was dependent on whether the company would give me severance.

Well turns out the manager emailed a different HR person immediately saying I resigned effective date X. The HR lady I worked with? She was either fired or let go the day before I had my conversation with my manager, with no notes.

HR maintained that I resigned willfully. I had meeting with HR person two where he used unprofessional, abusive language toward me. It was like dealing with a mobster. They said, after consulting Legal, that I had legally resigned by giving them a date even if it was not my intention.

So much BS. HR has many tricks like this up their sleeves. Hope she gets good legal representation. Her physical characteristics put her under two legally protected classes that should help her case in the courts. Too bad she was not over 40, another class with greater protections.",2.0
geps1xw,k77sxz,"She was fired for telling hundreds of employees to stop writing. Stop doing their job. Prior to that she made a list of demands for a “full explanation”. She got very bad career advice from someone. 

The irony is her paper is calling for higher standards by google tooling. So the review process that rubber stamped her paper grew teeth finally and realized hers didn’t have current research (and really offered nothing new).",2.0
gep91rx,k77sxz,"It's a bit tangential, but I saw a twitter thread which seems to me to be a fairly coherent summary of her dispute with LeCun and others. I found this helpful because I was previously unable to coherently summarize her criticisms of LeCun - she complained that he was talking about bias in training data, said that was wrong, and then linked to a talk by her buddy about bias in training data.

https://twitter.com/jonst0kes/status/1335024531140964352

&gt;So what should the ML researchers do to address this, &amp; to make sure that these algos they produce aren't trained to misrecognize black faces &amp; deny black home loans etc? Well, what LeCun wants is a fix -- procedural or otherwise. Like maybe a warning label, or protocol.

&gt;...the point is to eliminate the entire field as it's presently constructed, &amp; to reconstitute it as something else -- not nerdy white dudes doing nerdy white dude things, but folx doing folx things where also some algos pop out who knows what else but it'll be inclusive!

&gt;Anyway, the TL;DR here is this: LeCun made the mistake of thinking he was in a discussion with a colleague about ML. But really he was in a discussion about power -- which group w/ which hereditary characteristics &amp; folkways gets to wield the terrifying sword of AI, &amp; to what end

For those more familiar, is this a reasonable summary of Gebru's position (albeit with very different mood affiliation)?",39.0
gepaf6f,k77sxz,"I remember that this take is inline with how I saw the situation. But this is still a pretty biased summary, it shouldn’t be a problem to read the actual tweets if you want to draw your own conclusion.",25.0
gepbacc,k77sxz,"I read them, but I was unable to make heads or tails of what Gebru was arguing at the time. In contrast this summary is quite explicit and clear. So my question - insofar as the summary is biased, what is it wrong about?

Or by ""biased"" do you simply mean that jonst0kes clearly doesn't have a high opinion of Gebru and this comes out in his summary?

I suppose this interview with her [elsewhere](https://www.technologyreview.com/2018/02/14/145462/were-in-a-diversity-crisis-black-in-ais-founder-on-whats-poisoning-the-algorithms-in-our/) does support jonst0kes interpretation also (I only found it a few min ago).",12.0
gepfkja,k77sxz,"&gt; Or by ""biased"" do you simply mean that jonst0kes clearly doesn't have a high opinion of Gebru and this comes out in his summary?

That seems to be a fair definition of ""biased"", doesn't it?",11.0
gepglas,k77sxz,"I normally interpret ""biased"" to mean ""incorrect in a particular direction"".",-3.0
gepny7s,k77sxz,"this is a slightly strange definition of biased to find in /r/machinelearning but, I guess, not in the context of this tread.",3.0
geppsje,k77sxz,"If there's anything this whole debacle has given me it's to not believe for a second that anyone with actual ML expertise actually posts here.

Or at least, that their posts get any amount of upvotes.",2.0
geposzh,k77sxz,That's literally a colloquial version of the statistical definition of bias: https://en.wikipedia.org/wiki/Bias_(statistics),1.0
geppois,k77sxz,I intended no offense. I meant to suggest I do not usually consider bias to be pejorative.,2.0
gepsi55,k77sxz,Well biased can also be leaving out certain details or arguments to support a particular narrative. Although if you consider leaving out details as incorrect then your definition is still good.,1.0
gepgirc,k77sxz,"This summary and his entire thread is totally judgmental from his perspective. Saying ""LeCun was professional and earnest, and Gebru and her allies behaved like entitled bullies."" ... iike wtf ...   


His perspective is you should accept my apology the way I want it because I apologized in public &lt;- this in and of itself is problematic",-3.0
gepspyt,k77sxz,She should accept his apology because she didn't even deserve an apology.,0.0
gepbx9o,k77sxz,"&gt; eliminate the entire field as it's presently constructed

Err, that needs to be much expanded upon because it seems absurd that anyone with any clout would think ""tear it all down and start again"".",20.0
gepgr7l,k77sxz,,
gepglyi,k77sxz,This article has a good summary of the criticisms of LeCun in that incident: [https://venturebeat.com/2020/06/26/ai-weekly-a-deep-learning-pioneers-teachable-moment-on-ai-bias/](https://venturebeat.com/2020/06/26/ai-weekly-a-deep-learning-pioneers-teachable-moment-on-ai-bias/?fbclid=IwAR3ROqXg1rT724cNa2ZxXAP47qd23h7tbxBuLUi_SsJCMZ6GaXyBTVbtXDc),13.0
geph62z,k77sxz,This is a very good link to direct to the people that think it's a literal expert in the field misunderstanding year one concepts. Thanks.,8.0
gephuza,k77sxz,"That article also doesn't seem to disagree much with jonst0kes. It doesn't say LeCun was factually incorrect about anything, but merely criticizes him for his attempts to focus on factual claims about ML models.

Instead, it mostly focuses on LeCun's violations of [lese majeste](https://en.wikipedia.org/wiki/L%C3%A8se-majest%C3%A9):

&gt;LeCun finished the thread by suggesting Gebru avoid getting emotional in her response — a comment many female AI researchers interpreted as sexist.
&gt;...gaslighting...",1.0
gepih5z,k77sxz,"No, it doesn't disagree with him at all. But I felt like it did a better job at summarizing what the actual issues were in a coherent way beyond the histrionic characterizations that have been posted.",7.0
geplhtj,k77sxz,I'm glad we're in agreement on the facts and differ merely on which article provides a clearer explanation.,1.0
gepgs7u,k77sxz,"Outside of the attacks and bad faith misinterpreting, I would say Gebru point would be that yea data causes bias but how did those biases make in into the data? Why did no one realize/care/fix the biases? Was it because there weren’t people of color/women to make it a priority or to have the perspectives that white men might not have about what would be considered a bias in the data? I think this could be a civil point to be made to LeCun but rather it was an attack - one which he didn’t respond particularly well to (17 long tweet thread).",6.0
gepry8y,k77sxz,"&gt; Why did no one realize/care/fix the biases?

This is a very important point that I think is often missed. Every algorithm that gets put into production cross dozens of people’s desk for review. Every paper that gets published is peer reviewed. The *decision that something is good enough to put out there* is something that can and should be criticized when it’s done poorly.

A particularly compelling example of this is the thing from 2015 where people started realizing Google Photos was identifying photos of black men as photos of gorillas. After this became publicly known, Google announced that they had “fixed the problem.” However an [what they actually did](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/) was ban the program from labeling things as “gorilla.”

I’m extremely sympathetic to the idea that sometimes the best technology we have isn’t perfect, and while we should strive to make it better that doesn’t always mean that we shouldn’t use it in its nascent form. At the same time, I think that anyone who claims that the underlying problem (whatever it was) with Google Photos was fixed by removing the label “gorilla” is either an idiot or a Google employee.

It’s possible that, in practice, this patch was good enough. It’s possible that it wasn’t. But which ever is the case, the determination that the program was good enough post patch is both a technical and a sociopolitical question that the people who approved the continuation of the use of this AI program are morally accountable for.",2.0
gepjp1w,k77sxz,"&gt; bad faith misinterpreting

Can you state which claim made by the above tweet thread you believe is an incorrect interpretation, and perhaps state what a correct interpretation would be?

&gt;I would say Gebru point would be that yea data causes bias but how did those biases make in into the data?

In the example under discussion, we know the answer. It's because more white people than black people took photographs and uploaded them to Flickr under a creative commons license.

If you want a deeper answer, I'd suggest looking into the reasons certain groups of people are less willing to perform the uncompensated labor of contributing to the intellectual commons. There have certainly been a few papers and articles about this, though they (for obvious reasons if you know the culture of academia) don't phrase it the same way I did. 

&gt;Why did no one realize/care/fix the biases?

You'll have to ask the black people who chose not to perform the unpaid labor of uploading photos to Flickr and giving them away. 

&gt;Was it because there weren’t people of color/women...

No. 3/5 of the authors of the paper are people of color and only 1/5 is a white man: http://pulse.cs.duke.edu/",1.0
geplvfo,k77sxz,"Maybe you misinterpreted what I was saying, I meant that Gebru was misinterpreting LeCun. My other comments were meant more generally, I didn’t remember the specifics of the exact facial recognition application they talked about. I don’t think it’s stretch to say that there can be underlying causes about why data might end up biased with any given application.",3.0
gepomtn,k77sxz,I think I did misinterpret. Sorry!,2.0
gep7ays,k77sxz,"why is this whole topic so important to this community? i have never heard of those people, so im kinda out of the loop",38.0
gep8j83,k77sxz,"It serves as a proxy for something that's been building for a while: How should the ML community deal with ethical concerns? Having ethics experts as part of the company seemed to be one solution, but that raises more questions: How much power should they be given? How can companies strike a balance between making sure that the ethics people get their views properly considered, and balancing their recommendations against everything else they must consider? Should recommendations made by the ethics people be considered final and unquestionable, or should they be subject to another layer of scrutiny (and if the latter, how is that done without effectively either establishing a new ""ethics person"" or rendering the original ethics people completely toothless)?

These are very important questions for us to think and talk about, and this drama gives us the chance to do so. Of course, it's going to be difficult to try to focus less on the he-said/she-said part of this and more on the larger issues it's connected to. But that's preferable to not discussing it at all.",86.0
gep96g4,k77sxz,"In addition to what you said, this idea of ""whistle-blower protections"" for technologists has been increasingly discussed in the AI ethics community, and now we have a situation that could potentially be the poster-child for why we need these types of protections for AI ethicists.",40.0
gepa54j,k77sxz,"Excellent point, I didn't even think about that.",9.0
gephf8u,k77sxz,"Let’s not just throw out words like “whistle-blower”. She was already collaborating with people outside Google and had already sent out the paper. 

She submitted paper late for review, Googlers reviewed and decided they didn’t want Google’s name on it in its current form. Instead of trying to fix the issues and resubmitting she decided to give an ultimatum and create drama.",14.0
gepq04c,k77sxz,"&gt; She submitted paper late for review,

No she didn't, this is a lie that's been spread widely, and has been equally widely debunked by people at Google and Google Brain specifically.",0.0
gepgjnh,k77sxz,I think the most important ethics issues of ML implementation are centered around whether or not it can be used reliably to do the things the sales guys told you it could do.  Right now there are companies out there that are using models trained with crappy incomplete data sets that are selling their services to police departments to identify people from grainy security camera footage.  I don’t have a link to the article about this but I saw it here a few months ago about someone being misidentified and arrested solely based on the answer shat out by some algo no one can even look at.  I think this is a much bigger issue than the whole “does xyz model work better for white people?” thing.,9.0
gepjehy,k77sxz,"&gt; It serves as a proxy for something that's been building for a while: How should the ML community deal with ethical concerns? Having ethics experts as part of the company seemed to be one solution, but that raises more questions: How much power should they be given?

I'm not an ML person, but I'm here because I think there's some confusion about what exactly her role was. She wasn't in a compliance-type role but rather it was academic-type where she studied the concept ML ethics not specific to Google. As someone who has done banking compliance there's a huge difference between doing compliance versus talking about things in a broad context. 

&gt;Should recommendations made by the ethics people be considered final and unquestionable, or should they be subject to another layer of scrutiny (and if the latter, how is that done without effectively either establishing a new ""ethics person"" or rendering the original ethics people completely toothless)?

What she was doing was effectively going outside the company to the media, which irrespective of what someone can do internally it's completely different when you speak publicly about your employer especially in a way that could be considered negative. I for instance working in compliance wielded a lot of power internally where I was the final word where no manager or senior manager of mine would interfere and the executives and managers I was reporting on had to do what I said, but if I wanted to get something published in the media about the bank's compliance I'd expect to have layer-upon-layer of review and approval. It's not that she was crafting internal compliance methods but rather trying to put her employer in a negative light publicly, which if she was working on internal processes we'd be having a different conversation and she might still be employed.",6.0
gept5a9,k77sxz,I haven't seen anyone in any of these threads discussing these deeper issues though...,1.0
geparq3,k77sxz,"You'll see my take on the situation. I've had an opinion on it since the time I saw what happened with her and Yann LeCun.

She's the same person who caused a huge fuss on Twitter some months ago by blowing up a comment from Yann LeCun regarding an unbalanced training set (which using that project's methods - or most methods that anyone has ever used - was simply true). She accused him of racism and ignoring her work and basically called him a prominent white member of the establishment. Tonnes of people who enable assholes and call it bravery rallied behind her on Twitter, and it became a case where you have to defend someone who gets beaten up on without cause. Yann LeCun quit Twitter for a while as a result, and now people like Ian Goodfellow are retweeting support for demands to have her get her job back. It's become apparent that if we don't want certain people to have license to vilify anyone on a moment's notice (who must respond to a mob who already isn't going to interpret the response in good faith), we have to say something. People are already silencing themselves for protection.",47.0
gep8jg6,k77sxz,"When prominent voices in ML community start taking sides, it becomes a matter of public interest.",9.0
gepk7p3,k77sxz,"At least Jeff Dean is considered a legend in programming world. His being part of this drama is part of the reason this got so much attention. Besides, the angles of racism, anti-feminism, Google culture are also spicing  up the drama.",5.0
gep9tmm,k77sxz,,
gep8lxu,k77sxz,"What strikes me most is ""it was approved for submission and submitted"". Ok, but by whom? Timnit? Jeff? Someone else?

To me it sounds as if it could not have been Timnit herself because it just doesn't make sense she would have to offically ""approve"" her own submission. Given her Twitter behaviour it's understandable they don't want to tell her her internal reviewers maybe they have an internal anonymous review process - but wouldn't Jeff mention that? 

So it might be quite a normal process that internal reviewers would get disclosed and just in Timnits case they didn't want to tell her.

Ofc these are all speculations.",12.0
gep9upt,k77sxz,"You can easily go read what her colleagues at Google Brain are saying about the process in a few of the articles linked in the OP, and on Twitter, including people at PR who actually do the internal reviews. Basically, papers get submitted with no review _all the time_, and there's no two week pre-submission deadline _anywhere_.",21.0
gepe8eq,k77sxz,"Yeah, that was my take from reading the two e-mails and the wired and mit articles just confirmed it.

TBH I don't think this was a smart move from google's side. Now everyone is talking about the paper they wanted to avoid getting published. They could have accepted her condition and just fired her after the retraction.",11.0
gepfemz,k77sxz,"I imagine they don't care about these rules if you publish a new hyperparameter for some transformer architecture, but they'll care a whole lot if your paper is trying to eviscerate BERT which they've just massively invested into",21.0
gepk9jy,k77sxz,I have interned at Google before and they are pretty serious with their review processes. They don't want to risk getting sued for plagiarism and/or other legal matters.,7.0
gepkrlh,k77sxz,For interns I absolutely believe they are.,3.0
gepryom,k77sxz,"This is absolutely not true. Nobody, and I repeat, *nobody* submitted without a review and getting approval.

What is true, it's that the review can be pretty lightweight. If you introduce a new optimizer with only experiments on public things, and no policy, PR, or legal implications whatsoever, then the review will be simple and is done in an hour or so.",1.0
gepr1i8,k77sxz,"Timnit has a history of doxxing and attacking people without cause. Just recently she has named a HR person on Twitter because ""she has a suspicion"" that person is behind her termination. Last week she called (possibly) the reviewers  ""privileged white men"" even though she does not know who they are. She has greatly expandded her twitter count in the last few days. I would not want to be one of the people who reviewed her paper right now, if she gets her hands on that list, they are going to be vilified by the likes of Anima Anandkumar.  

Just look at the people in their sights: Pinker, Scott Aaronson, Yann LeCunn, Jeff Dean and many others. For no other reason they they are purportedly not ""allies"" because they do not bend over backwards to their ridiculous whims.  It's disgusting.",1.0
gep62y0,k77sxz,Is there a TLDR version yet? Or is it just he said she said still.,15.0
gep813q,k77sxz,[deleted],14.0
gep8yyl,k77sxz,"&gt; This was evident by her inability to even comprehend that imbalanced test sets are not always due to bias.

I'm pretty sure if you actually read what she said, that wasn't it.",12.0
gep9935,k77sxz,"Sounds like you're depicting the exact tone deafness this field (and in general most people) exhibit towards systemic racism: ""oh, we didn't explicitly mean to be racist, it's just society/data/economics made it that way! ""

I am not condoning Gebrus tactics, if her hostility made people quit twitter and stuff. But if someone did point your models are racist, its probably okay if you didn't notice first, but if you don't go out of your way to correct your methods going forward, as a whole field (to the extent where peer reviewers also enforce it at that stage, though the review process with ML literature by itself is a joke) then you are definitely at least a teeny bit racist if not merely tone-deaf.",26.0
gepcb01,k77sxz,"How do you distinguish between a dataset accurately reflecting reality, and a dataset being ""biased"" and leading to ""racist"" results?

Since I am tone deaf, I'm going to ask the question about training a dataset to predict machine failure in an industrial process. Most of the machines are made by GE and a much smaller number are made by Mitsubishi. Failure is also rare and the data suggests GE and Mitsubishi machines have different failure rates. 

Is this a ""bias"" I need to correct? Or do you think it's possible that an underlying reality about machines exists, and perhaps GE's machines have different failure rates than Mitsubishi?",12.0
gepcxng,k77sxz,"This sort of issue is ultimately about algorithmic fairness, which is probably one the hairiest non-mathematical topics in CS right now. There've been several attempts to render it mathematical, but they keep turning out to be self-contradictory, or to contradict common-sense examples of fair behavior. 

Kearns (of Kearns and Vazirani) and Roth (well-known in the differential privacy space) have a pop-sci book on the topic. While I personally think their proposed solutions are kind of garbage, the first half of the book does a good job surveying efforts prior to its publication.",14.0
gepfahh,k77sxz,"I'm well aware of the algo fairness field. My favored approach to this is simply to specify your ethical preferences in your objective function, and separately build accurate classifiers that feed into this.

This approach is unpopular in activist circles. If your objective function is F(# murders, # incarcerations, racial disparity), then I can take gradients and discover exactly how many people you'll be willing to kill to reduce a racial disparity. Or alternately, how many predatory loans you'll issue to black people for similar reasons.

(I'm using ""predatory"" in the 2008 sense - issuing loans that you know the borrower is unlikely to repay.)

The activists therefore try to square the circle with /u/mamaBiskothu's approach which is a bunch of word salad that hints (without explicitly saying) that the data must somehow be wrong and that anyone who disagrees is raaacist.

My question is designed to more explicitly probe that, by designing a problem that is mathematically identical but has no emotional content. (Unless perhaps you own GE stock or something.)",11.0
geplekl,k77sxz,"I found your example of machine failure quite useful, so let's use it. We probably want the model to predict GE machines fail more, if GE machines fail more. But importantly, you are training from failure report of GE machines, not failure of GE machines. If you learn, in some other ways, GE machine failures are twice more likely to be reported compared to Mitsubishi machines failures, you will want to adjust the model, because the goal is to predict machine failure, not to predict machine failure reporting.

Similarly, the model of recidivism should take into account the model of law enforcement, just as the model of machine failure should take into account the model of failure reporting, if the goal is to predict crime, not to predict caught crime where catching process itself is biased.",3.0
gepmyfh,k77sxz,"&gt;Similarly, the model of recidivism should take into account the model of law enforcement, just as the model of machine failure should take into account the model of failure reporting, if the goal is to predict crime, not to predict caught crime where catching process itself is biased.

This is very true.

Now in lending (one popular topic in AI ethics) you don't need to worry about this very much - basically every lender furnishes approximately 100% of delinquency reports to credit bureaus. 

In criminology there's more of an issue but we aren't exactly groping around in the dark. We have a variety of data sources that encode bias differently:

- Arrest and conviction data
- Crime reports (go to police station and say ""I was robbed!"")
- Dead bodies with a cause likely to be murder, tracked by both police and CDC
- NCVS (phone poll, ""Have you been robbed in the past 12 months?"" - excludes murder for obvious reasons.)
- Demographics of crime victimization (most crime is intraracial, so ).

So for example, bias in arrests will be absent in crime reports and NCVS - therefore if NCVS and arrests don't match up, a bias in arrests can be detected. If your murder victims are 25% black but arrests are 50%, that's similarly an indication of bias in arrests.

This direction is pretty unpopular because while there's a roughly 400% racial disparity to explain, these directions rarely indicate a bias larger than 50%.",3.0
gepnd9a,k77sxz,"Yup. 100% agreed on lending. For lending, or click, or in app purchase, you pretty much have the ground truth. I am just pointing out arrest is NOT ground truth, and alternative data source like NCVS is important. I think we are pretty much in agreement.",3.0
gepjvk4,k77sxz,"When did I say the data is wrong? If the data says that more people of a particular race are likely to default on loans then that's not being disputed. However, if you as a ML researcher say I don't give a crap about societal or human effects, I only do the math, it's up to the company to be ethical, my responses will unfortunately start obeying Godwins law which never helps.

What am I proposing? You don't need to involve an ethics board on every model you generate, probably not for a model trying to differentiate between GE and Mitsubishi (though whether your choice there was deliberate to point racist connotations of inferior asian manufacturing is not clear), but  you as a data researcher needs to acknowledge that you have been accidentally put in a place to make outsized decisions that can affect people's lives, and you should constantly ask yourself every day whether what you do helps society walk towards a better place or in the opposite direction. Of course you can choose to ignore those and outsource that to Mark Zuckerberg and Jeff Dean or Just say Jesus take the wheel, but stop saying you're not racist. Either you are condoning racism or you acknowledge you have zero authority over any of your actions, no better than a glorified code monkey doing its masters bidding.

Further, ""coding ethical preferences into your algorithm"" sounds exactly like the fever dream of a disconnected tone deaf equation writer who thinks it's acceptable to write equations that change people's lives and give the knobs to an alien looking more tone deaf billionaire. You're not Alan Turing and this is not World War II. If you're tasked with writing a death panel model you have the choice to refuse and hold evetyone involved to have a discussion about the ethics of doing so, I'm sure the 6-7 figure salary you make and the Severance package you get are cushier than what the person in the hood getting the payday loan your model approved is going to get as a choice otherwise.

Also I'm not some amateur, my day job involves crunching terabytes of data that concern people's lives, and I see every day many of my colleagues blatantly ignore minor things about the data that could change some lives more than others, while a few colleagues actively look for and notice the smallest of data transformations that could mean a lot to the insights we get from it, in terms of de-marginalizing groups of people among other things. I'm trying to learn from the latter group; all we can do is try! 

Also What exactly is a word salad? Isn't all text word salad? Or do you mean anything that cannot be put into equations is not your concern? So alphabet soup then?

Further Edit: just noticed your profile says you're director of data science for Simpl, ""India's largest pay later company"" aka white payday loan lord over third world country with brown people, deciding who gets money and who doesn't. I'm sure you have models that choose who gets loans and who doesn't, I am curious if you have a caste parameter in your equation?",0.0
gepsp5s,k77sxz,"&gt;and you should constantly ask yourself every day whether what you do helps society walk towards a better place or in the opposite direction.

Ok. What's ""better""?

&gt;Of course you can choose to ignore those and outsource that to Mark Zuckerberg and Jeff Dean or Just say Jesus take the wheel, but stop saying you're not racist. 

I'm not saying that. I'm saying I don't even know what you mean by ""racist"", and therefore I cannot answer the question.

&gt;Further, ""coding ethical preferences into your algorithm""

That isn't what I said. I said put ethical preferences into your *objective function* specifically.

Now that's exactly what you want to do as well if your ethics are consistent. (It's a simple topology problem to show that consistent decisions =&gt; existence of objective function.) It's just that I want to do it transparently and explicitly.

&gt;If you're tasked with writing a death panel model you have the choice to refuse and hold evetyone involved to have a discussion about the ethics of doing so

Ok. We have a discussion about ethics. What's the result?

&gt;I'm sure you have models that choose who gets loans and who doesn't, I am curious if you have a caste parameter in your equation?

As of the last time I worked at Simple they did not. From what I've heard they've done very little on the ML side since I left, so probably they still don't. 

Also, consider Simpl's western counterparts, e.g. Affirm/Afterpay/Klarna/etc. I'm willing to bet at least one of them has an Indian or Chinese person leading underwriting/ML/risk/similar functions. Would you similarly characterize that person as ""a Chinese loan lord over poor white people, deciding who gets loans and who doesn't?"" If not, why not?

What if the ML, underwriting or risk lead was Jewish?",1.0
gepkyfu,k77sxz,wow who knew that difficult real world problems wouldn't translate into simple math,1.0
gepj7qh,k77sxz,"I think the point is whether the model should reflect the reality. I agree with you below that ultimately such concerns simply should be the part of the loss function.

We want the model to think GE machines fail more if GE machines fail more. Do we want the model to think doctor is he and nurse is she if it is indeed the case in some novels in BookCorpus that pronoun he is likely to resolve to doctor and pronoun she is likely to resolve to nurse? To me it makes total sense to want to have gender blind model here and include in the loss function P(doctor is he)-P(doctor is she) as penalty. It is an empirical quesiton how much this changed loss function cost in terms of perplexity, and it is a question of value judgement how much perplexity loss one is willing to tolerate to get rid of gender bias. All in all, this kind of research seems valuable to me, if not very important.",5.0
gepkwu6,k77sxz,"&gt; How do you distinguish between a dataset accurately reflecting reality, and a dataset being ""biased"" and leading to ""racist"" results?

As a researcher in this space, I don't think you can de-tangle the dataset from what you plan to do with the outputs of the model. If I built a credit scoring model on ""racist data"", I can give the model's outputs a non-profit that focuses on providing free financial literacy training to low-income families (e.g. Give financial literacy training to households that have a low probability of paying back the loan). Thus, your ""racist"" dataset can be used to combat the racial wealth gap.

Part of the problem, in my opinion, is that we don't talk enough about how we can perform machine learning research to support NGOs. Everyone knows about internships with Facebook and Google, but did any of your advisors in grad school tell you about ML internships/research opportunities at the [UN Global Pulse Lab](https://www.unglobalpulse.org/)? It's not Google's or JPMorgan's mandate to fix societal inequalities, but this is the mandate of the UN and other NGOs and non-profits. We can't really get ""better data"" unless we support these organizations that aim to correct the underlying societal issues that lead to ""biased datasets"".",4.0
gepm1jt,k77sxz,"At least one thing you can do is to see if there is hidden stratification on performance. Is the model systematically worse for Mitsubishi than GE machines? Difference in rates is ok if model performance is similar for two machines.

I work on med-AI models and we definitely see race, sex, and age bias for certain diseases, but we must be vigilant that this is real vs something else. The first step would be to check the pr-auc (or whatever relevant metric) for predictions for protected groups (race, sex, age) are sufficiently similar.",1.0
gepojwk,k77sxz,"In my example, it is almost certainly worse (in the sense of lower roc_auc) on Mitsubishi machines due to the smaller sample size. 

However I guess one difference between the industrial example and human examples is that we rarely expect different groups of humans to differ a lot. 

E.g. in the famous COMPAS example, the model performs equally well because the model is basically just a survival model on # of past crimes, # of past violent crimes, age and gender. (Other features are basically irrelevant.) I.e. the coefficient on # of past violent crimes is the same for whites and blacks. 

But in any case, these are questions that can in principle be answered mainly by statistical analysis. So my core question: can I apply the same kind of statistical analysis to industrial machines that I can to criminology? If not, why not?",1.0
geppcu1,k77sxz,But we have different expectation on fairness in different contexts. For crime and banking we expect conditional fairness (outcome should not be conditioned on protected group). For breakdowns (disease) we expect that outcome can be conditioned on protected group (at most you can look at stratified performance). Your machine example is breakdown/disease and not like crime/loan application.,1.0
gepqqvt,k77sxz,"Sure, but that's a different question - decisions vs predictions.

I.e., suppose I want to build a credit issuing rule. I might choose to ignore facts I'm not allowed to look at. But that doesn't mean my belief that they hold predictive power is wrong. 

Concretely, I can believe that logistic regression on (FICO, race) beats logistic regression on FICO alone, but still use the latter one to avoid legal trouble.",1.0
gepomss,k77sxz,"&gt; How do you distinguish between a dataset accurately reflecting reality, and a dataset being ""biased"" and leading to ""racist"" results?

Jesus fcking christ.",1.0
gep9x6g,k77sxz,[deleted],13.0
gepbkio,k77sxz,"Did you read this in the comment you replied to:

&gt; if you don't go out of your way to correct your methods going forward ...  then you are definitely at least a teeny bit racist if not merely tone-deaf

Your comment sounds like you're actively resisting going out of your way to fix these issues of bias and to keep things as they are. Congratulations! That kind of approach is, indeed, racist. There are ways to fix these biases either by constructing and evaluating models in ways that doesn't just reward fitting well on white faces (for example), or by helping to construct more equitable datasets. Both of these take more effort than sampling from imbalanced data because one is lazy.",-3.0
gepbt3i,k77sxz,[deleted],15.0
gepcb8z,k77sxz,"Okay, this really appears like a semantic issue. It seemed to me that the way you started in these comments made it seem very much like this _wouldn't_ be your position - which I'm sure isn't how you meant it to appear.",1.0
gepcljo,k77sxz,[deleted],8.0
gepll0a,k77sxz,"Can you clarify ""imbalanced""? I work in fraud prevention where my datasets are incredibly imbalanced (think 1000:1 good:bad).  We tend towards Gradient Boosting as traditional fraud strategies are single decision trees so XGBoost feels like a natural extension.  Other than tweaking scale pos weight parameters we don't typically undersample goods.  Can you explain how this is a poor approach? Not trying to be argumentative, genuinely hoping to improve our process.  We do get really good results with this approach.",1.0
gepgfmr,k77sxz,"It is not but it would be racist if we don't correct models that are trained on this data , eg if we know that model doesn't recognize black or Asian faces but still deploy it.",0.0
gephqmu,k77sxz,[deleted],6.0
_,k77sxz,,
gepphl6,k77sxz,"I guess my question is \*how\* do you correct it?

Not that you shouldn't try, you should definitely try. 

But I guess, the issue of who gets to try, and how, is one that deserves a lot of thought. And I feel like the sort of attempt that would get a lot of accolades online might end up itself cementing certain racist ideas (e.g. how to deal with mixed-race samples (I'm saying this as a mixed-race person who feels uncomfortable filling in anything for ""race""...))",1.0
gepdyo9,k77sxz,"&gt; If a model samples US demographics, you would expect an imbalanced training set, correct? Is that imbalance racist?

I think that the imbalance/dataset is not racist, but it does say something about the society that generated the demographical data though. Also, I do find quite hard to negate the existence of historical structure/institutions that were created by racist people that molded such a society and resulted in the demographical data, so I think it is fair to say that the society itself does have problems with racism (and that these problems penetrated into the structures of the society itself). 

So, technically:

&gt;  We can fix it - but the original training/testing imbalance is not due to racism - it's just due to (in this instance) simple demographics.

This is due to racism. Racism created these unbalances. However, the dataset is not racist, because it is just some sample with no intentions itself. The imbalance is not racist, it is just something that exists. But its (the imbalance) existence is **due to** racism. But now this is just semantics (which seems to be the main disagreement here).

We do need to be careful so that the models don't reproduce the same (negative) biases that this specific society had.",-5.0
gepey02,k77sxz,Are you claiming the differences of population sizes of various demographics is an artifact of racism?,12.0
gepffny,k77sxz,"I am quite sure that by ""demographics"" here we are including socio-economic status (and other indicators), aren't we? What I am claiming is that having underrepresentation of certain groups in some specific strata of the population (while others are overrepresented) is a result of racism.

For example, the page on wikipedia on ""demographics"" does include things like ""Income"", ""Birth Rate"" and ""Economic Class"", so it is quite a widespread use of the word ""demographics"" to include things besides just counting the (absolute) population sizes. 

https://en.wikipedia.org/wiki/Demographics_of_the_United_States#Income",-5.0
gepfyeu,k77sxz,I’m talking about whatever population size differences that were being discussed and were being reflected by random sampling that you take issue with. Is your claim that these population size differences are an artifact of racism?,5.0
gepgo8d,k77sxz,"I am not following what you mean. What are the ""population size differences"" you are talking and what are you hypothetically sampling in that case?

My claim is: 

&gt; What I am claiming is that having underrepresentation of certain groups in some specific strata of the population (while others are overrepresented) is a result of racism.

Anything else and we would be disagreeing on something that doesn't make sense. So I need you to clarify exactly what you mean by ""population size differences"" and what data you are sampling and from which population.",-2.0
gepjp6n,k77sxz,"“This is due to racism. Racism created these unbalances.”  This is your statement. I think it’s clear what imbalance we’re talking about. The reason for differences in representation while random sampling is underlying population size differences. 

Are you saying the population size differences are an artifact of racism?",3.0
_,k77sxz,,
gepay1r,k77sxz,"I'm going to trap myself into the ever-changing usage of the term ""racist"" here, but absolutely yes in this case, without a doubt.",-6.0
gepbc4o,k77sxz,[deleted],13.0
gepdt7s,k77sxz,"I guess you can make a tool that benefits most people or you can make a tool that benefits most groups. If you sample people and train your model, you'll be biased towards the majority. If your tool/model is only/mostly usable by the majority then they claim that it is racists. However,  If you first sample groups and then people within those groups, you are arguably not racist but you might be worse according to a measure based on randomly sampling people. 

The question is: which utility are we trying to maximise? Performance across people directly or performance across groups? And if we pick the latter, which groups?",1.0
gepbn8l,k77sxz,"""If a country subjugates and jails black people for centuries and then makes tools that don't work for black people in the future, there is nothing racist about that. It's just sampling.""

Strawmanning you here but that's exactly how this take comes off and the best way I can explain it.",-10.0
gepfvl5,k77sxz,"It's pretty easy to see this is false. Here's another case where people call the algos racist: blink detection in cameras.

Here's the classic example: https://funnyjunk.com/funny_pictures/937069/Blank/#0bf665_936625

The problem with your theory is that the camera was designed and made by people in a country that is super racist in favor of (some) narrow-eyed people and literally murdered non-narrow eyed people for hundreds of years in pursuit of it's imperialist ambitions (but has been quite peaceful for the past 70 or so years). Somehow it still comes out biased against narrow-eyed people.

Why do you think that is? Are the Japanese engineers at Nikon racist against themselves?",4.0
gepgc8c,k77sxz,I guess this is the quality of response I deserve for strawmanning first lmao,-4.0
gepce90,k77sxz,[removed],3.0
gepd0ur,k77sxz,[removed],-1.0
gepdgdp,k77sxz,[removed],1.0
_,k77sxz,,
gepcf2w,k77sxz,"I'm a bit confused about this conclusion... is there a single country on the planet that would yield a perfectly balanced data set with this type of random sampling? 

If not, does that mean every country is racist? Or does that mean random sampling is racist? Or both?

To continue this line of logic, does this mean that over-/under-sampling is the solution for racism?",4.0
gepihq1,k77sxz,[deleted],0.0
gepjmbh,k77sxz,"I am literally not claiming data is racist. You're being intentionally obtuse over and over and making ad hominem attacks.

You aren't the arbiter of who works in a field because you don't understand concepts tangential to it.",3.0
gepjy1z,k77sxz,[deleted],0.0
gepldes,k77sxz,You do not seem to understand what I am saying or you're intentionally being intellectually dishonest.,1.0
_,k77sxz,,
gepl3p8,k77sxz,[deleted],0.0
geplr2w,k77sxz,Just downvote me and move on if your responses are just going to keep attacking me for concepts you don't understand.,1.0
_,k77sxz,,
gepo6dd,k77sxz,"Here's basically the situation as I (newbie to this field) understand it, please correct where I'm wrong:

\- There are things in the world that are certain ways. They should not be the way they are, but if you neutrally collect data from where you can observe, your data will observe the way things are, not the way they should be.

\- Models and data making predictions from past occurrences can end up making things stay the way they are. By, essentially, not distinguishing between ""things permanently locked into the human condition"" and ""things that are this way now, but can change"" (a subset of which is ""things that we want to change"", for any given 'we'), data tends to lump the latter into the former.

This is how the result becomes biased -- by observing a result of a statistical occurrence (e.g. non-white people have fewer PhDs \*at the moment, because of racism and society\*) and confusing it with a permanent fact (e.g. non-white people are less capable; make predictions about what the world will be like in 2120 which assume that white people will have the same percentage of PhDs).

Obviously, the really tricky part is in places that are more subtle

&amp;#x200B;

\- But when a person, or a ""movement"", attempts to fix it... I mean, this is getting into politics in that uncomfortable way, but like. So... trying to put this as objectively and impersonally as possible. - There are multiple possible ways to fix problems in the world, both discrimination-related problems, and other problems. One thing that I see pretty constantly is that these conversations only happen between the most confrontational people. Because they're a factor of confrontation, I feel like the nuance doesn't get discussed, and the best, most effective answer doesn't even get formulated. The dig-your-hells-in, don't give in to that person because they're the bad guy, don't-budge-an-inch answer is what gets on the table, and on a ballot, literal or otherwise.

Like, for example, what are some ways to deal with the issue of bias in language analysis programs? According to the MIT article, it seems the paper pointed out, among other things, the issues that a) the algorithm can't separate racist, harmful speech from other kinds of speech, and that b) marginalized groups have less of their writing on the internet, because they have less internet access, and therefore they have less representation in the algorithm.

And off the top of my head I can think of several other issues with having an algorithm that *absorbs and just accepts* language, and predicts future usage from it.

Honestly, I think the thing about the computer-generated self-help books actually fooling people is pretty funny. I don't think it's shocking to think about what would happen if someone used something like that to generate content about an important issue, because the truth is? Most human-created content is that thoughtless. And that's the real problem at the center of this and pretty much all other issues -- if the general public could be trusted to be smarter, and conscious about what they read, what they believe, what they do, etc....

Digression aside,

So, a solution to problem A might be to have a human go in and separate out the ""racist speech"" from ""everything else"". And then the ""sexist speech"", ""homophobic speech"", etc....

Likewise, a solution to problem B might be to have a human go in and amplify the voices that are deemed to be marginalized....

&amp;#x200B;

But how?

Who gets to be the one making those decisions? Who decides which voices are legitimately marginalized and deserving of equality, and which are just weird, and should stay where they are? (Population size? But where do you draw the lines there? Even if you want to say, ""oh, ethnicity,"" how? Tracing family trees? Not every member of an ethnicity publishes writing, how do you decide which voice of the group gets to be enshrined as *the* voice of the group? ""Oh, geography"", how? People who live within area XYZ, people who work within that area...? How long do they have to have lived there in order to count as the ""voice"" of that place? And what about less demographically traceable forms of marginalization?)

And with problem A, we obviously all know certain groups that would go in ""racist speech"" quarantine, but where do you draw the line? Do you create separate levels? And what do you do with the ""racist content"" once you've quarantined it? How do you have your moderators know the context of every post, and not confuse anti-racist factual information for the racism it's documenting, and not confuse an innocuous word in one language for a harmful word in a different one, etc.

Because the moderators are always going to have their own biases, and even if their biases are generally in the right direction, they're not perfect.

For example, if you look back 100 years, 200 years, 500 years, in any society, there were always different ideas about what should be preserved, what should change, and how. If you look at different people advocating for gender equality and women's rights 100 years ago, their image of the ideal situation would still look very different from what people advocate today, even though they were at the forefront in their time.

The difference is, right now we have technology like this that can end up cementing current views in ways that might not get corrected for centuries, if ever. Personally, there are a lot of things that are considered okay even by the mainstream left/progressive people, or that are even considered positive by those groups, that I personally think are sexist, and that I personally think are holding back gender equality. An algorithm created today based on unfiltered data might crystallize society with all of today's problems, or it might take the world back 10 years in terms of progress and equality. But an algorithm filtered by people with this or that philosophy might take us forward a certain amount and then stop, and crystallize there. I'm not saying they should adopt all of my views exactly - I'm just one person. There are probably thousands of other people like me with very clear views about something that needs to change that people aren't talking about, but afraid to say so. And with real far-right people on ballots all over the world, that's not really the most pressing issue...

And then there's the problem that even if Google and Amazon and Facebook decided to really, honestly, purely be responsible - say, to stop using those techniques with the massive carbon footprint - there are other organizations in the world that wouldn't be so scrupulous, and someone needs to counter them...

&amp;#x200B;

It's thorny, and I feel like none of these people is really dealing with these situations properly, giving them the consideration they deserve. I feel like you can't get to be a public figure and decision maker in any real society if you're going to give issues the consideration they deserve instead of being confrontational and grandstanding. That applies to business, politics, pretty much anything. I mean, Thucydides pointed that out during the Peloponnesian War and it still holds true today...

&amp;#x200B;

Sorry this is long and rambling, I just... I think *at this breaking point*, both sides acted unproductively and I don't have much faith in humanity that these issues will get productively discussed.",2.0
gep9obo,k77sxz,"&gt; I am not condoning Gebrus tactics, if her hostility made people quit twitter and stuff.

It literally didn't.",-8.0
gephtnn,k77sxz,"Exactly. One of the canonical examples of bias is ml is translating a non gendered language to English and you end up with outputs like ""she is a nurse."" If all a human had to go on is [gender neutral pronoun] in context of nurse, you'd pick ""she"". A lot of times it's just modelling underlying conditional probability. The pronoun example gets used with undertones of bigotry (of basically all of the English corpus) when it's really not.",4.0
gepnex6,k77sxz,"The ad hom against her is completely unwarranted and false. Really shows why most productive ML researchers are not part of, and do not interact with, the reddit ML community.

Timnit's PhD was advised by Fei-Fei Li (you know, the one from Imagenet), and she has done state-of-the-art work on ""fine-grained object detection"". Just look at [her publications](https://ai.stanford.edu/~tgebru/) from her degree. Instead of attacking her background, how about we try to understand why so many ML folks think they have completely understood or have solved the ethical issues associated with AI while at the same time dismissing social science. Working on ML does not make one an expert on the social, ethical, environmental, or other real world impacts of the technology.",2.0
gepcqjt,k77sxz,"This is an important point and when you take out the context of race, gender, or other polarizing classes; and instead insert different medical conditions that occur in different proportions in the population - we need to ask how we’re gonna deal with it? 

We know that ML/DL performs worse on the minority classes (not race, just whatever is in the lesser n number).  So the solution is to balance the data set. But if you’re trying to re-create real world performance, incidence and prevalence, is that a bug or a feature?  Or another words do you want the imbalanced data set over the balanced one?

I wish someone could tell me because I have not been able to figure this out. References?",1.0
gep65eq,k77sxz,"He said, she and hundreds of her colleagues said, more like.",-20.0
gepcfpk,k77sxz,Silence speaks volumes,1.0
gep78hs,k77sxz,[removed],39.0
gep858n,k77sxz,[removed],16.0
gepaokh,k77sxz,,
gep98ac,k77sxz,[removed],-12.0
gepjfk9,k77sxz,"It's easy to lose the bigger picture. While it seems that both Gebru and Google have handled this matter incorrectly, I must appreciate how seriously the ML community takes equality. 

I hope to see the future where ML-algorithms control every consumer unit as equally insignificant. Imagine a world without race or gender. Only consumer.",9.0
gepcliq,k77sxz,"Here is the paper in question, for those who want to read it.
https://gofile.io/d/WfcxoF",16.0
gepgwr0,k77sxz,"If it's you that posted, the pink squares that hide things are just pink squares on top and you can copy the text below and view fyi.",9.0
gepho7m,k77sxz,lmao love how the censor boxes momentarily vanish when you zoom in or out,5.0
geppvyf,k77sxz,You can actually copy and paste the hidden text and reveal all six researchers' names and emails…,3.0
gepr0zg,k77sxz,Wait is there a version with those boxes removed?,1.0
gepqozl,k77sxz,"Thanks for sharing.

I haven't had a chance to read through this yet, can anybody summarize?

Part of me feels that surely this language model must have encoded some amount of the systemic bias, sexism, and racism endemic to much of the English speaking world. Another part of me feels that if you look for that bias with the a priori assumption that's it's there, then you'll find it no matter what.

Guess I'll have to carve out some time and read the paper itself!",1.0
gepstxj,k77sxz,"This reads more like an opinion piece than a paper, and does not say anything be that was not known and, as Jeff says, most issues are actively being worked on.

Also, notice the frequent by use of news articles as references, including one with `index.htmlandneedbettercitations` lol",1.0
gep8iwm,k77sxz,"Seems to me like google was looking for a way to get rid of her and she gave them exactly that. Cant blame google though, just glancing through her twitter and the way that email was written makes me think that she is toxic and entitled person that is really hard to work with.",55.0
gepsmtm,k77sxz,"Google wasn’t looking for a reason when she submitted. She gave them reasons long after when she had a meltdown and told hundreds of google employees to not work anymore. 

I don’t stand with a millionaire prima donna. Pull this in academia and you’re ruined.",4.0
gepia1a,k77sxz,"Yes, I think she also really overstates her importance to the company. Ethical AI researchers mostly bring PR benefits rather than financial benefits to companies like Google. While I get that getting fired/resigned is a big deal *for her*, Google probably just thought that the small PR plus they get from having her is not worth the trouble she's causing.",14.0
gep99vg,k77sxz,"&gt; Seems to me like google was looking for a way to get rid of her and she gave them exactly that.

By... writing a paper?",-24.0
gepaxe1,k77sxz,By telling them she is ready to resign if certain conditions can’t be met maybe??,25.0
gep9ojx,k77sxz,"If I understand it correctly (got the info here: https://www.reddit.com/r/MachineLearning/comments/k5ryva/d_ethical_ai_researcher_timnit_gebru_claims_to/?utm_source=reddit&amp;utm_medium=usertext&amp;utm_name=MachineLearning&amp;utm_content=t3_k77sxz) she basically gave them an ultimatum ""Do X or I ressign"". So google just refused the ultimatum and now she is out.",17.0
gepaiay,k77sxz,,
gepg1iv,k77sxz,"No not by writing a fluffy paper but probably by giving an ultimatum

AND sending a mail to internal group telling them to STOP doing all DEI work 

AND in the same internal message asking people to try to put pressure on her employer through Congress.

Based on that message it seems she was also bragging about some other instance an year ago where she threatened to sue her employer.",10.0
gepj3y1,k77sxz,,
gep8ldy,k77sxz,I'm not informed enough to comment on the social issues she has brought up but I do believe this is shedding some light on problems with the internal peer review process at Google. And it seems to me that it mainly stems from conflicts between researchers and the goals of their managers.,9.0
geppsra,k77sxz,"&gt;This happened to me last year. I was in the middle of a potential lawsuit for which Kat Herller and I hired feminist lawyers who threatened to sue Google

When did Timnit Gebru even start working at google? 2017 or 2018? And she almost immediately tried to sue them?

Two years later she's issuing ultimatums because she doesn't like how some internal process works? 

Given her penchant for creating drama, I have a feeling these are not the only two incidents. Good riddance.",6.0
gepog2n,k77sxz,"This raises an important issue. 

If the future of funding for AI ethics research is tied up with industry and companies have unlimited rights to veto any papers they don't like, then the field isn't really going to exist at all. All we'll really get is papers that make companies look good and reflect the ethical values of industry which might be at odds with the ethical values of society at large. AI ethicists need to be able to write papers critical of industry otherwise they can never affect change.

If anyone thinks it's not an issue for companies to make every important ethical decision about the future of AI, then I don't know what else to say other than you're being optimistic. Companies are amoral, driven by the profit motive, and they can't be trusted to create an AI field that works for the good of society at large without some oversight.",3.0
gep68xt,k77sxz,So bored of all this...,39.0
gep90en,k77sxz,"It's pretty funny that some people keep saying how much they hate the drama and yet they scramble to post on a *checks notes* megathread devoted to consolidating the discussion and cordoning the drama just to say how much they hate the drama, wish timnit would go away, think Google did nothing wrong because timnit is a drama queen, they just want to focus on science and not politics, etc. 

Lol.",45.0
gepcuew,k77sxz,"Being annoyed by others feeds my superiority complex.


I need that some times, you know?",20.0
gepi33w,k77sxz,You're really not though!,1.0
gepfb1s,k77sxz,"Why is my post being silently removed??!:

Actually the first entry in the timeline should be this tweet:

https://twitter.com/timnitGebru/status/1331757629996109824?s=19

From Nov 26:

""Nothing like a bunch of privileged White men trying to squash research by marginalized communities for marginalized communities by ordering them to STOP with ZERO conversation. The amount of disrespect is incredible. Every time I think about it my blood starts boiling again.""

wherein Gebru publicly criticizes her Google colleagues using racist and sexist inflammatory language.

I think it's clear based on this and her attacks on Yann LeCun that she's a bully and was let go for her bullying behaviour.",18.0
gepq3u8,k77sxz,"Timnit, if you are reading this: former colleague here. You were wondering

&gt; Am I radioactive? Why did nobody talk to me about this?

Yes, you hit the nail on the head. That is exactly it. Anything that is not singing you or your work praises gets turned into an attack on you and all possible minorities immediately and, possibly, into big drama. Hence, nobody dares give you honest negative feedback. Ain't got time to deal with this in addition to doing everything else a researcher does.

I hope this whole episode will make you more receptive to negative constructive feedback, not less. I wish you all the best in future endeavors.",6.0
gepriy3,k77sxz,"Hadn't heard of Timnit until this incident, but this seems like an accurate representation..

On twitter she is retweeting one glorifying tweet after the other and almost never replies to tweets even remotely critical of her.",2.0
geprzdh,k77sxz,Sounds like Trump.,2.0
geps5hk,k77sxz,Indeed it does.. shows that oftentimes people on both extreme ends of the political spectrum are actually not too different,2.0
gepsufx,k77sxz,"The fact that coworkers that speak against her are behind throwaways while coworkers that are in support speaks volumes of the power of Gebru's hate mob.

The same hate mob that can chase a Turing award winner off Twitter can and will obliterate any normal professional.",1.0
gepm0vu,k77sxz,Why is this TMZ stuff even relevant for us?,3.0
gep6pmr,k77sxz,Oh come on..,6.0
gepmsm6,k77sxz,I encourage everyone to read this [article](https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/),2.0
gepdayw,k77sxz,Sorry if i sound like an ass but is there a small TLDR to this. Even a para would work. TIA,1.0
gept6vq,k77sxz,"Toxic person goes too far, faces consequences. A SJW's nighmare.",1.0
gepdk8b,k77sxz,"TLDR;

* Google hired Timnit Gebru as an AI ethics expert.
* Timnit's research claimed that Google's AI language models are unethical because of their racial bias and high electricity consumption.  She and several others planned to release an academic paper showing this research.
* Timnit was told to remove her name from the paper by her manager at Google, and was not told why.
* Timnit said she would resign unless Google told her exactly why she was being ordered to remove her name from the paper.
* Google locked her out of her accounts, effectively terminating her employment.",-7.0
gepf3vl,k77sxz,"A little context: 

* Google makes money from the use of its language AI models
* A research paper showing that Google's AI language models are unethical might be damaging to Google's business as it relates to those language models
* Google publicly stated a few reasons for censoring the paper.  Those reasons have been called called into question by former and current Google employees.",10.0
gepflno,k77sxz,"That's her side, yes.",8.0
gepnmxg,k77sxz,"&gt; * She and several others planned to release an academic paper showing this research.

&gt; * Timnit was told to remove her name from the paper by her manager at Google, and was not told why.

I think there's missing context between these two. Namely:

* The paper was submitted for review 1 day before its deadline - the review process requires *""two weeks""* according to Jeff or *""at least 1 week""* according to Timnit

* Reviewers found issues (e.g: it didn't take into account latest research), and the authors were given feedback (according to Jeff)

* As the paper had already been submitted externally without waiting for feedback, and it didn't meet the standards to have Google's affiliation stamped on it, Google demanded the paper be retracted

&gt; Timnit said she would resign unless Google told her exactly why she was being ordered to remove her name from the paper.

She had already been given feedback on why the paper didn't meet Google's standards (according to Jeff).

I think the main problematic demand was revealing the identities of the reviewers and everyone they consulted with (which are typically kept anonymous for integrity).",1.0
gep8eq4,k77sxz,[removed],-3.0
gepa3mc,k77sxz,,
gep8s1n,k77sxz,Who? Oh.. Don't care,-13.0
gepmyts,k77sxz,Why did you take the time to comment then?,0.0
geps1x7,k77sxz,Ohhhh drammmmaaaa.,1.0
gepselv,k77sxz,"I guess the thing that I’m surprised about and that bother me is why, when she sent her email referring to an end date, her manager didn’t respond with, “hey, can we talk about this first?”

As a manager in big tech, that’s what I surely would have done and have done in similar situations.

That, to me, is the sketchy part of this story. If Google really cared about her and her work, the first allusion to quitting wouldn’t have resulted in termination, but rather been cause for concern and outreach efforts. 

That’s why this feels like a firing to me rather than a resignation. The judges will call is how they see it, and I’m not saying my point has any legal merit, but it does just seem to be a shitty thing to do.",0.0
gepf3a2,k77sxz,"This shit again? many people get abused every day, phd students, researchers etc. Why this should be important while there is a much bigger problem? There are many researchers who do not get funding for their work! come on",-10.0
gepf19d,k77sxz,,
gep3kbc,k774cf,"1) Take a picture of a cat

2) Draw a red rectangle around the cat, label this rectangle ""Dog""

3) Put this on a mug

There you go. No captions, no memes, just a mislabeled cat.",108.0
gep5f8p,k774cf,You should go into business selling those!,18.0
gepa6kh,k774cf,I would take one. Definitely my humor,5.0
gephi2s,k774cf,Bonus points if at the bottom of the frame it says “confidence: 0.98”,13.0
gepb475,k774cf,1000x. This is hilarious and I would laugh every time I picked up that mug.,6.0
gep32no,k774cf,"There is a toy called the ""**Turing Tumble**"" that is actually pretty cool (despite the lame name).  It uses marbles and levers to let you create logic gates.  It's a little expensive, but for a hardcore AI nerd it's probably great gift:  [https://imgur.com/tUn8AAe](https://imgur.com/tUn8AAe)",11.0
gepfz2u,k774cf,sounds great,1.0
gephkgm,k774cf,A properly cleaned dataset.,12.0
gep5yc5,k774cf,These would make a great gift for AI programmers: https://www.nvidia.com/en-us/data-center/dgx-a100/,27.0
gep7pmj,k774cf,"Hey, it's me, your fellow AI programmer.",13.0
gepol61,k774cf,Cant afford that. will have to make do with    rtx 2060,2.0
gepev1g,k774cf,"Digits socks

[https://www.paulsmith.com/uk/men-s-off-white-numbers-socks](https://www.paulsmith.com/uk/men-s-off-white-numbers-socks)",6.0
gept38g,k774cf,Do these come in thigh high?,2.0
gep6csc,k774cf,More programming than AI related:  you could get them some cool looking programmer keyboards.,5.0
gepl7ja,k774cf,"""Neural Networks for Babies"", if they recently became parents 
https://www.amazon.com/Neural-Networks-Babies-Baby-University-ebook/dp/B086T76TBY",4.0
gepgx70,k774cf,"There is a little book called ""**Arrival Mind**"" that's fun.  It's a picture book (for grown-ups) about the dangers of AI with great artwork.  The hardcover version is a little expensive but is a nice coffee table piece. [https://www.outlandpublishing.com/arrival-mind](https://www.outlandpublishing.com/arrival-mind)",2.0
gepil9t,k774cf,ram,1.0
gepre5s,k774cf,"There was a “deep learning hype” parody UNO game deck that was at CVPR (if not there, then SIGGRAPH) a few years ago. I can’t for the life of me find a reference to that anywhere online, but it was pretty great.",1.0
geptciq,k774cf,"Any rtx 3000 series graphics card depending on your budget
They range from USD $399 to $1500+",1.0
gep8vhk,k774cf,"Jetson nano if they are a ""maker"" sort.",1.0
gepacgd,k774cf,"This kit puts tank treads on your phone and lets you program it to roll around.  It's a fun gift for programmers, but a little expensive: [https://www.robotshop.com/en/mini-dfrobotshop-rover-mobile-smartphone-development-kit.html](https://www.robotshop.com/en/mini-dfrobotshop-rover-mobile-smartphone-development-kit.html?gclid=CjwKCAiA_Kz-BRAJEiwAhJNY7xHPBkK5kqj29_dkjdFDPvqJw6z6k3Rd7i6CUt45qUw0qlWnmCIMoBoC0UUQAvD_BwE)",1.0
gepqt9g,k774cf,Get them ps5 or xbox :),0.0
gepeuvk,k76nfv,Thank you,2.0
geozkna,k76nfv,"# Highlighting document text with Extractive QA

Notebook link: [https://colab.research.google.com/github/neuml/txtmarker/blob/master/examples/02\_Highlighting\_with\_Transformers.ipynb](https://colab.research.google.com/github/neuml/txtmarker/blob/master/examples/02_Highlighting_with_Transformers.ipynb)

GitHub: [https://github.com/neuml/txtmarker](https://github.com/neuml/txtmarker)

This notebook uses Hugging Face Transformers to run extractive question-answering and highlights answers within a set of articles.

In the example above, the following questions are run against a PDF article and the answers are highlighted.

* What is this article about?
* How are hashes used?
* What was added to hashing in Python3?",1.0
gep4p1s,k76nfv,"Hm.... Can you tell me which language is this??
Python programming or javascript??
Or maybe anything else",-1.0
gepert1,k76nfv,The library and provided notebook are in Python.,1.0
geoufhn,k7574p,"I'm sure it could be great but that GUI is the worst I've seen in years. Tech matters but so does how you show it to other people, I've seen nuclear reactors' panels that looked less complicated.

Now even without that, on this subreddit I think it's also important to explain how you did something and not just only copy/paste the same paragraph as you did everywhere else (facebook, youtube description, other ML subs etc.). I'm not here to finance patreons but to talk about ML.

To stay credible I think it's also important to not compare what you're doing with the real world. Sentences like ""Acquisition starts from scratch like a real baby acquires his native language"" don't mean much even if it's regurlaly used in few-shot learning papers, same goes for ""Virtual agent will be activated by his own emotions"", that's not a scientific way to present an ML algorithm.",7.0
geoxtah,k7574p,"Thank you for your comment. This is not a GUI for user. I've used this GUI for research just to see how things happen when I'm talk with a computer. I did not to present ML algorithm, it was just for illustration . :-) I mean that virtual agent ""mood"" will be affected by the recognized phrase.",-4.0
geost5f,k753gp,Thats a very interesting question. I dont think there are any regulations regarding GAN generation of new Data,7.0
geoxeez,k753gp,"This would be interesting to try with older composers such as Mozart, in particular, because I know there have been centuries of composers like Tchaikovsky and Strauss writing things to intentionally imitate Mozart. Wonder if it's possible to generate a ""Mozart"" vs. a ""Strauss Mozartiana""... interesting.",4.0
geoxje9,k753gp,Yes older music id generally in public domain. I am hoping to do more recent works like would love to try the beatles! Does not seem very legal though :(,2.0
gep1arz,k753gp,"It’s perfectly legal as long as it is personal or non commercial use. You won’t actually get a “song” out of GANs unless you are also using Large language models. Generation is better suited for instrumental music.
It becomes murky legally only if you were to distribute it and claim yourself as the artist.",1.0
gep1fqh,k753gp,"&gt;neuralautomaton

Great thanks! Yes it is just for a project, I was just concerned i might not be able to post it on a blog or github. :) x",1.0
geoxg8z,k753gp,On the wikipedia article it says that gan stuff is legal as long as it doesn’t try to put anime titties on a real life person without their consent.,2.0
gep6bjv,k753gp,"I did some research on that question, but that was a long time ago and I don't remember all the details. IIRC it's still a grey area and we'll only know for sure once a situation like that reaches the courts. But it seems to lean more on the ""fair use"" side, as long as it doesn't overfit too much.

A while ago there was a lot of drama involved with a GAN trained on furry stuff, because they used lots of artworks without author permission, and people seriously misunderstood how the whole thing worked. There were many threats of lawsuits but nothing actually happened. It was quite interesting because it was actually overfitting, you could recognize individual characters.",1.0
gep11cb,k753gp,"Who cares, honestly? Do you violate any type of law because you remember a particular music? Is remembering a crime?",0.0
gepbbok,k753gp,"I don't think so. there two issues legally though that come to mind.  
1) downloading and storing the data for purposes other than listening to it on your own. especially if you release the dataset. 
2) Copy rights infringement laws are crazy  - like even a sequence of notes may potentially be ""claimed"" and there's a good chance that the model will create something that is ""claimed"".",1.0
gepiz3o,k753gp,thanks! So if you state that it is designed to mimic this particular artist's style it still might be a problem?,1.0
gepjqjf,k753gp,"for example. if someone does a cover of a song they have to pay royalties. another - autotune was patented. 

the style itself i think is actually less ""patentable"" unless it's well defined - like uses a special set of instruments or some special beat, specific synthesizer sounds, unique technic ...etc 
usually law suits are around sequences of notes - not the style.

just to clarify though. this is not legal advice of any kind. i am not a lawyer and have no particular specilization in the legality of music - just general knowledge and pieces of information I've accumulated over time.",1.0
gepoo37,k753gp,thank you ! :),1.0
gepk285,k753gp,"https://en.m.wikipedia.org/wiki/Music_plagiarism
check this out",1.0
gepk3t3,k753gp,"**[Music plagiarism](https://en.wikipedia.org/wiki/Music plagiarism)**

Music plagiarism is the use or close imitation of another author's music while representing it as one's own original work. Plagiarism in music now occurs in two contexts—with a musical idea (that is, a melody or motif) or sampling (taking a portion of one sound recording and reusing it in a different song). For a legal history of the latter see sampling.  

[About Me](https://www.reddit.com/user/wikipedia_text_bot/comments/jrn2mj/about_me/) - [Opt out](https://www.reddit.com/user/wikipedia_text_bot/comments/jrti43/opt_out_here/) - OP can reply !delete to delete - [Article of the day](https://redd.it/k6wsfi)",2.0
gepk36y,k753gp,"Desktop link: https://en.wikipedia.org/wiki/Music_plagiarism
***
 ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^303328. [^^Found ^^a ^^bug?](https://reddit.com/message/compose/?to=swim1929&amp;subject=Bug&amp;message=https://reddit.com/r/MachineLearning/comments/k753gp/p_data_for_generative_models/gepk285/)",1.0
geoqive,k73s20,There are some red flags.,5.0
gep1ig8,k73s20,There is wide range of papers in this field producing excellent results.I have worked on some of it myself. This seems fishy though.,2.0
geohykc,k71pgj,"Whether they count as ""interesting techniques"" depends on your perspective, but I'd include work in any subfield where the limiting factor is small datasets rather than compute time. E.g., a lot of work in machine learning for healthcare falls into this category. Probably ML for most data types except text, images, video, and speech does as well. I worked in a mostly applied research group for a long time and we averaged just over one GPU per student without much issue.

There's also the entire field of data mining, which only sometimes uses deep learning and often uses algorithms that run fine on CPUs. See, e.g., the [KDD proceedings](https://www.kdd.org/kdd2020/proceedings/) or basically all of [Eamonn Keogh's papers](http://www.cs.ucr.edu/~eamonn/selected_publications.htm).",5.0
geo6x0f,k70l26,It isn’t. Those issues are closed and from 2018.,2.0
geowp4g,k70l26,Got it! thanks,1.0
geoei4l,k6zt2n,"Could you post this video to Crossminds.ai ? under ""Research Paper"" of ""Community"". Thanks.",1.0
geofi8f,k6zt2n,Sure!,2.0
geo9v5k,k6yk3s,"This is perfect, I was just looking for some off-the-shelf image quality metrics to approximate perceptual quality. Thank you!",3.0
geo25at,k6yk3s,"Great! Thanks. I like two things: 1) docs is to the point and clear, ND 2) you included the references for each quality metric. I assume validation was done by comparing with other codes?",2.0
geohw5r,k6yk3s,"Thanks! Indeed, I compared the values with other implementations to be sure that my code is consistent with what most people use.",1.0
geony1l,k6yk3s,"Great initiative! However, have you guys checked `kornia.losses` ? We already have some of this metrics and losses - all well tested and maintained. We share the same motivation for the existence of that module, so would be nice to complete with the missing ones :)

*https://github.com/kornia/kornia
*https://github.com/kornia/kornia/tree/master/kornia/losses",2.0
geov8iw,k6yk3s,"Thanks!

Indeed, I knew about Kornia. In fact I even considered contributing, but I was afraid by the, rather heavy, code style that you use. For instance, the type/shape assertions in every function, is, in my opinion, unnecessary (ML engineers should be able to read the documentation) and slows down (a very tiny bit) the implementation(s), especially when used repeatedly (which it is).

However, don't take me wrong: I really like Kornia and have used it several times in the past!",2.0
geocvat,k6yk3s,"This is great, thank you so much. I also as a beginner was very frustrated that I could not find a simple please where I can get all of these losses.",1.0
geo8l9p,k6yk3s,"Seems really cool! I’m not terribly familiar with the field, so I’m wondering how you’d pronounce the package’s name? If it’s likely to be spelled out (I.e., somebody saying it like “S P I Q”) then no worries. However, I could see people pronouncing it as if it rhymes with “stick”, in which case it will sound like an ethnic slur against people in latinx communities. Just wanted to bring it to your attention!",1.0
geoiq23,k6yk3s,"Thanks! I always pronounced it as ""stick"" with a ""p"". It never occurred to me that it could have been hard to say for some people. (I'm a French native speaker and ""spiq"" seemed natural)

BUT I hear you out, and I'm not against a rename (anyway there is not a lot of stars to the repo). What do you think about PIQA (for pytorch image quality assessment) ? 
There is no python package with that name yet AND it is a cool ref to pokemon!

Edit: pronounced ""pika""",6.0
gepdwbw,k6yk3s,"I love that new name! Thank you so much for being receptive to the feedback. It’s a small thing to do, but you just made the community a much more inclusive place.",2.0
geocte6,k6yk3s,"It is based on piq so I assume it is pronounced 
S-piq?

Either way I agree, this is not a good name no matter how you write it out. Quite unfortunate",0.0
gep8ecz,k6yk3s,"bro, let's be real, everyone who can create or use this package has more than 100IQ, so is very likely not racist. If it is plainly obvious the intended use was not racist, I don't see why a rename is necessary.",0.0
gepa3dh,k6yk3s,"Damn, I misunderstood the problem... I thought it was ""hard to pronounce"" not that it was an insult... Anyway, I didn't like the name so I modified it (and I like the new one better!)",3.0
gepgky8,k6yk3s,The new one is awesome!,1.0
gepfwb0,k6yk3s,"Hey! Well, first things first. I’m not saying that the package’s author is racist. To be honest, it seems unreasonable to assume that the author would necessarily be aware that the package’s name is a homophone for a slur. 

I’m just imagining somebody from the Latinx community getting involved in ML much the same way that many of us have - watching YouTube videos and reading blogs. They come across a brilliant CV package that does everything they need it to do, but realize that everybody pronounces it as a too-familiar slur, in fact a slur that represents many of the terrible stereotypes they have to fight against daily. That would be such a shitty thing to happen. And I’m sure none of us would want that to happen and would take every opportunity to make sure it doesn’t, especially when confronted with new info that we’ve inadvertently done something like that. I’m very grateful to the package author for being so receptive to that and understanding that neither of us are coming from a place of malice or disrespect.",1.0
genxd83,k6y3tt,"There a lot of factors at play here but generally most neural net operations are translated into GEMM operations. So if your network results in large GEMMs it would have better GPU utilization, having a large batch size also helps since there is more compute to be done .

Secondly, the architecture of the network is important , say there are dependencies in the network which inhibit a chunk of consecutive operations then the GPU has to wait for data to arrive from the memory which can further reduce the overall utilization. 

But the advertised numbers for peak performance are usually only achievable if all you are doing is fused multiply adds (FMA).

Finally, the framework which mapped your network to the GPU ops may also have inefficiencies but for popular neural networks this is less of a concern.",14.0
genzawp,k6y3tt,"I hear what you’re saying, but under 40%... rough! I thought it was closer to 70%, but 40%. Ouch.
I guess from this perspective, without the bandwidth to support the FLOPS, the advertised FLOPS count is effectively useless, it’s only good for marketing, but not actually useful.",1.0
genzke4,k6y3tt,You can get high efficiency for some kernels ( say large GEMMs) Winograd amenable convolutions but that would be for that particular operation/layer certainly not the whole network.,2.0
geo51bz,k6y3tt,"&gt; without the bandwidth to support the FLOPS

The issue that OP highlighted has to do with latency rather than bandwidth. Whenever you wait for something from RAM, it's often because of the latency; very rarely is it due to bandwidth constraints.

This is why many researchers are excited about the Infinity Cache present on AMD's new GPUs, they get 128MB of a last-level cache.",1.0
geo5ywv,k6y3tt,"&gt;many researchers

Can you point me to ML research projects that use AMD GPUs? Everyone I know uses Nvidia GPUs. Can't wait till AMD ~~catches up~~ creates a competitive stack / introduces robust framework support to provide some much-needed competition.",2.0
geob8pi,k6y3tt,Even if AMD gains very little market traction it may still motivate Nvidia to introduce similar features.,1.0
geop5ps,k6y3tt,"Not really. In any case, GPU are not designed with low latency in mind. They use latency hiding techniques to mitigate that. So, while some thread is waiting for data from the RAM, it is preempted by another one, so that the GPU does not stay idle.
Many (if not most) GPU workloads are memory bandwidth limited.

Regarding caches, they sure do provide lower latency, but also have a much bigger bandwidth.

Ref : https://crd.lbl.gov/assets/Uploads/cug19-roofline-final.pdf",2.0
geomwf0,k6y3tt,"TLDR; I spend a lot of time optimising NNs for edge deployment and have experienced FLOPs to be a poor predictor of runtime performance. 

I believe what you are observing here is that there's a substantial gap between what the 'speed of light', theoretical max possible OPs per second of a GPU is and what can be achieved in practice.

At the end of the day, getting code to run efficiently on hardware is a balance between exploiting parallelization, memory locality and the introduction of redundant work. Look at the specifics of how GEMM is implemented to understand the redundant work.

The problem with your analysis here is probably not the calculations, it's that you're assuming that FLOPs is a reliable metric for quantifying the computational efficiency of a neural network. As I mentioned above, there's lots of tradeoffs to be made here and these tradeoffs are grossly impacted by the network architecture. 

One of the most expensive operations your GPU/CPU has to do is a load or a store to DRAM. You might think of DRAM as fast memory but in comparison to on chip memory (SRAM/cache) we're talking an order of magnitude or two slower. The general design strategy for such hardware is to do as much work as feasibly possible with loaded data before loading more. This is what will increase utilisation of the hardware, ensuring the big tensor cores are not sat idle waiting for memory transfers. 

In practice, this means that your network architecture is going to govern performance as things like Residual connections increase memory demand as you need to save these feature maps for a later date, if they fall out of cache they need to be loaded again etc. 

Unfortunately, these kind of optimisations are not always instinctive or intuitive and this is why there are papers like Google's NetAdapt or hardware aware NAS strategies focusing on optimal network architectures for a given platform. An example I can think of to hand is how 5x5 Conv2D operations are often not exactly 2.77 times slower than their equivalent 3x3 as they can make use of data in cache. 

Hopefully this helps!",14.0
geojbof,k6y3tt,"Just out of interest, have you ever checked the GPU util% as reported by the OS while the training is in progress?",3.0
geoje10,k6y3tt,That utilization is #of cuda cores being used. Not flop utilization,2.0
geojhfo,k6y3tt,"I guess what I mean is that if the GPU is pegged at 100% while training but the FLOP utilization is much less, you'd have to conclude the difference is due to overhead somewhere?",2.0
geojzrd,k6y3tt,"Ahh

Yeah the OS reported GPU utilization for ReaNet50 i1k training is 90%+. But this only results in only about 17% FLOP utilization.",1.0
geojj6k,k6y3tt,"low GPU utilization = bad engineering.   
my guess is, take a look at the dataloader. it might be the bottleneck.   
if could be the case that the GPU is starving, and that's why you get low utilization numbers.",2.0
geojqg4,k6y3tt,"These numbers are from Nvidia optimized numbers. I’m pretty sure they optimized the models since it’s effectively a marketing page.
Unless you’re saying Nvidia engineers are bad at using Nvidia hardware. Are there faster implementations than the ones submitted to mlperf?

Edit: look at the sources from the original post. It’s all from Nvidia.",1.0
geoqqci,k6y3tt,"You are using for the theoretical peak performance the number for the tensor cores (ie matrix multiplication). 
While large networks have lots of matmul operations, this is not the only thing that happens in those kernels: reading parameters, flow control, read/write of data. Those operations do take real compute cycles (and related power budget). 

In practice, NN training are very close to peak efficiency, and the typical efficiency issues are more around small layers not filling the machine, and data not coming fast enough from the CPU.",3.0
gepragr,k6y3tt,"&gt; In practice, NN training are very close to peak efficiency

Yes I agree with this, but as calculated above, the FLOP efficiency for RN50 training is 17%. That is the peak efficiency of a GPU training RN50. Nvidia engineers could not get their GPUs to do better for that network.
Yes in practice NNs have some operations that are memory bound. NN workloads have flow control logic and read/write of data which all should be taken into account. Taking everything into account Nvidia engineers could only get 17% FLOP efficiency for RN50 training;
where FLOP efficiency or FLOP utilization = used FLOPS / available FLOPS.",1.0
geo2wdk,k6y3tt,"\&gt; 366.5GFLOP

where did you get this number for BERT?",1.0
geo5pv7,k6y3tt,You can calculate it yourself. There are also online sources like [this](https://www.semanticscholar.org/paper/Deep-Learning-Cookbook%3A-Recipes-for-your-AI-and-Serebryakov-Milojicic/92354879d988f1f28e0551a3ea383ea58cc09a5d/figure/5),1.0
geo6ym8,k6y3tt,"This can be another weak point, I am not confident how numbers from your link has been calculated, there also can be differences in codebases(there are many different BERT implementations available at the moment), compiler chain, hardware.",1.0
geo7k96,k6y3tt,"In the original post I wrote:

&gt; If I made a mistake, this is probably where it happened. If you have you have more accurate FLOP counts for these networks let me know what type of utilization numbers you get.

Do you have better estimates?
Different implementations / FLOP counts might produce a 10% difference. Let's say that produces a utilization of 40% instead of 36.8%. Are you saying a utilization of 40% is much better than 36.8%?",1.0
geo9oud,k6y3tt,"&gt;In the original post I wrote:

I was thinking it is related to ""3"" multiplier. It is very possible that forward/back/update have very different FLOP counts.

\&gt; Do you have better estimates? 

I honestly have no idea, that part of the stack is black box/magic to me, that's why I was wondering where did you get your numbers.",1.0
geoa7vg,k6y3tt,"Yeah the 3 multiplier is the ""jankiest"" part of the calculation. A multiplier of 2.5 to 3 is relatively standard.

If we instead use the 2.5x multiplier it gets even worse:

&amp;#x200B;

\- ResNet50 (on 1x A100) = 2.5 \* 8.2GFLOP \* 2,084images/sec / (1 \* 312teraFLOPS) = 0.14% utilization

\- ResNet50 (on 8x A100) = 2.5 \* 8.2GFLOP \* 16,114images/sec / (8 \* 312teraFLOPS) = 0.13% utilization

\- ResNet50 (on 8x V100) = 2.5 \* 8.2GFLOP \* 11,180images/sec / (8 \* 125teraFLOPS) = 0.23% utilization

\- BERT Large (on 8x A100) = 2.5 \* 366.5GFLOP \* 836sequences/sec / (8 \* 312teraFLOPS) = 0.31% utilization

\- BERT Large (on 8x V100) = 2.5 \* 366.5GFLOP \* 354sequences/sec / (8 \* 125teraFLOPS) = 0.32% utilization",1.0
gep8n1h,k6y3tt,"Have you ever run a program that you can calculate the theoretical maximum flops for? I bet this isn't a NN-specific issue. I bet the advertised flops number is achievable only if everything is set up perfectly (caching, parallelization, etc)",1.0
geppar2,k6y3tt,"&gt;Have you ever run a program that you can calculate the theoretical maximum flops for?

Nvidia does have benchmark workloads which supposedly fully utilize the FLOPS.

&gt;everything is set up perfectly (caching, parallelization, etc)

Nvidia has teams of engineers trying to make sure neural networks are set-up perfectly for optimal performance. I guess that's not enough.",1.0
gepp3yz,k6y3tt,"Let me point some ways to make this analysis more precise.

Theoretical FLOPs required for a GEMM backward is 2x required for that GEMMs forward so you're right here.

However, this does not account for the weight update. Also, the weight update doesn't scale with the number of GPUs in data-parallel training. So with 8 GPUs, the weight update time for these models might actually be significant.

Another factor is the allreduce time. In data-parallel multiGPU training you have to communicate the gradient data between the GPUs. Often, this can be overlapped with the GEMMs in backward-pass compute but some of the communication has to be exposed, leading to longer iteration time.

Lastly, many kernels in BERT and RN50 are memory-bound, not compute-bound. Actually, pretty much everything except convolutions and matmuls would be memory-bound. So this includes stuff like activations, BCE loss, batch-norm etc. For kernels like this you cannot use the FLOPs efficiently because there's no way to get the data from memory to the streaming multiprocessors fast enough. The correct way to estimate the time to perform such a kernel is to divide the number of bytes to be moved by the memory-bandwidth number from the GPUs data-sheet.

Doing a proper performance analysis for these networks is actually quite a lot of work. It's also impossible without assuming some concrete batch size. Having said that, based on the reasons above, I believe the utilizations are actually higher than you've listed.",1.0
gepqa5a,k6y3tt,"I use a multiplier of 3 for fwd, bwd, and updt. see [here](https://www.reddit.com/r/MachineLearning/comments/k6y3tt/d_why_is_gpu_utilization_so_bad_when_training/geoa7vg?utm_source=share&amp;utm_medium=web2x&amp;context=3)

You listed a bunch of reasons why GPU FLOP utilization will be low (ie cant parallelize optimizer step, allreduce issues, memory bound operations).

So you list a bunch of reasons why GPU FLOP utilization will be low, then your conclusion is:
&gt; based on the reasons above, I believe the utilizations are actually higher than you've listed.
Can you please help me understand your reasoning?
The calculated FLOP utilization in the original post takes numbers from Nvidia optimized models and uses simple math that is hard to screw up.

When looking at specific operations in the NN, FLOP utilization might be a LOT higher, but when calculating the FLOP utilization for the whole model, a model like RN50, this number is about 17%.",1.0
gepeef9,k6y3tt,"I'm not worried that a GPU is underutilised by a single process, I simply run multiple processes in parallel to experiment with tuning hyperparameters faster. And simple modifications to PyTorch can dramatically speed up performance of most models (see my other post).",1.0
geprvtc,k6y3tt,"Network training has low FLOP utilization because some other aspect of the system is already being fully utilized eg GPU memory bandwidth is already being maxed out. Adding move processes in parallel will not help.

Have you ever trained ResNet50 on i1k and had the capacity on your GPU to run another process? Nvidia Engineers use 8 GPU for this training. Do you have a GPU where you can do this training on a single GPU and have extra capacity for more processes? What type of Next-Gen GPUs do you have access to?",1.0
gepcx1t,k6y3tt,"I started with an open-source PyTorch GitHub ML project and with a few modifications to the Dataloader, increased speed from 1,800 images/second to 8,000. One epoch went from 35 seconds to 8. GPU usage on my RTX 2080 Ti went from 3% to 20%, and running eight processes in parallel over two GPUs made for faster experimentation with hyperparameters. Can now run 8 experiments in 8 hours rather than 2 every 30 hours. I was actually surprised at how much difference an hour of dev time made. Still lots to try, would love to try other techniques such as mixed precision and a C++ based runner rather than pure Python.

If anyone has experience with this sort of optimisation, and wants practice trying to squeeze more performance out of a PyTorch project, PM me - happy to swap performance tips.

Update: another 25% speed boost with mixed precision. It was embarrassingly easy to add (took a couple of minutes) and worked first time. +1 to the PyTorch Devs.",0.0
gepou6a,k6y3tt,"The original post uses numbers from Nvidia optimized models.

The GPUs utilization is 90%+ but this results in a FLOP utilization of 17%. See [this](https://www.reddit.com/r/MachineLearning/comments/k6y3tt/d_why_is_gpu_utilization_so_bad_when_training/geojbof?utm_source=share&amp;utm_medium=web2x&amp;context=3) discussion",1.0
geox8oc,k6wd1b,"It was easy to guess which songs were composed by humans and which by ML. If you're looking for the best composing AI I've came accross so far, have a look at https://openai.com/blog/musenet/.",1.0
gep5ghe,k6t96m,"Since this post has now been locked, please redirect all discussion to the megathread.

https://www.reddit.com/r/MachineLearning/comments/k77sxz/d_timnit_gebru_and_google_megathread/",1.0
gemwgxe,k6t96m,I can’t wait to not read any of this and believe whatever the top comment on this post tells me to believe about this situation. /s,690.0
gemzm8r,k6t96m,Scenes when this becomes the top comment,116.0
gen6109,k6t96m,Thats what I was going to do and I'm stuck with your comment!,73.0
gen7s4z,k6t96m,Ah darn. Sort by controversial?,30.0
genifj7,k6t96m,"... But Pagliacci, you are the top comment.",22.0
gen33w1,k6t96m,It's not a mistake that yours is the top comment on this thread is it?,18.0
genqjap,k6t96m,Still waiting for GPT-3 to tell me how to think.,14.0
gencxdi,k6t96m,"Amateur move, I already made up my mind and I am here only to upvote the comments that agree with my viewpoint and downvote those who dont.",29.0
gen2t10,k6t96m,how the turntables,19.0
gendj2c,k6t96m,"That was my plane as well, but the top comment is your. 

Not even /s , I had enough reading yesterday's post. Feels like people spend more time discussing human drama then the actual ML",5.0
geogyvq,k6t96m,Well you kind of ruined that top comment strategy thanks,3.0
genqxgu,k6t96m,What happens when you are the top comment ehh?,1.0
genji9o,k6t96m,"As someone who has read all of it, it still misses a fundamental point.  Timnit was asking for more clarification of the review process and then they fired her for making a statement.  That is the microagression that should be discussed. Jeff is just trying to appeal to authority by focusing on a different problem.  Also to be objective, it would make sense to read Timnit's response as well:  [https://twitter.com/timnitGebru/status/1335017529635467272](https://twitter.com/timnitGebru/status/1335017529635467272)",-9.0
geopg3u,k6t96m,"So her argument is that because she let them know she was writing a paper, it didn't matter that she only gave them a day to read and approve it? Also funny she points out that the policy is 1 week in order to 'debunk' the 2 week claim.

Also, she makes it sound like she got fired for submitting it with 1 day's notice. She **resigned** because they wouldn't give her the names of the people who reviewed her paper. Bet she would have gotten those people fired if she had those names in her hand.",3.0
gen6g4s,k6t96m,,
gent2l9,k6t96m,"Although I do not have enough information to say anything with certainty (aka I am most probably wrong)z it seems the real problem is Timmit's reaction/approach to finding out that her paper did not pass the internal review process. Given that she has published many papers at Google in the past in the area of AI ethics, I find it hard to believe that Google decided to single this paper out and tried to ""suppress"" it. Most likely, her reaction (which in my limitedly-informed opinion) was over the top like she has done multiple times on social media (against le cun, jeff dean on a separate issue). And thus, the employer decided they no longer wanted to work with someone who was a troublemaker despite being immensely talented in her field. At the end of the day, cool heads on both sides would have prevented this public drama unless the public drama was the end goal.",82.0
geojhfs,k6t96m,"Looking at her Twitter feed and the emails she has sent to internal groups, I don’t think she would have ever left without creating huge drama! 

Some people work on solving protein folding, some work on creating drama!",29.0
geoys8v,k6t96m,"If you look at her PhD thesis I am just saddened to be honest, it's a diatribe of attacking machine vision without even engaging in the field. It just reinforces the stereotype that they offer grants and positions to the most vocal people from minority groups instead of the most talented ones.

Could've been someone working on actual statistical techniques related to sampling bias instead of someone pointing the finger and even arguing that things like ethnic bias are not caused by data bias but by algorithmic bias. Absolutely no causal reasoning why a convolutional neural network would be better suited for learning caucasoid faces compared to africanoid faces.

Yann LeCun, a Turing Award recipient, rightfully argued to her that instead of using an American aggregate like FlickFaceHQ, she could use a data set from Senegal and see if the same holds true. What followed was that [she had people harass him off Twitter and smear his name](https://syncedreview.com/2020/06/30/yann-lecun-quits-twitter-amid-acrimonious-exchanges-on-ai-bias/) because she couldn't engage him in the argument. She was never actually asked internally to prove her claims with data or statistics, probably out of fear of the person doing so would be harassed or gotten fired. It was only a matter of time before someone in the internal review board said enough is enough and ask her to give scientific proof to her claims.",31.0
gep0g1m,k6t96m,"Yeah, AI ethics is a field that's been reduced to finger wagging by these knuckleheads. It could tackle a lot of interesting questions (for instance, what kind of inputs these models are invariant to for example) that have wider implications for the whole field. Instead we have inane debates where 3 different factions are using 3 different meaning of bias etc.",13.0
geoqpbw,k6t96m,"We had an head of diversity and inclusion who did the same on their way out of the company, built up so much drama when she was sacked/demoted that overall brought more division than unity during their time at the company.",11.0
geoy9dk,k6t96m,"Wow, who could have predicted that bringing political ideology to the workplace would create division?",13.0
gep03oe,k6t96m,Every company needs a slaying kween though,-2.0
geozrjh,k6t96m,"It’s pretty incredible how many people in this thread are just taking Jeff Dean’s word at face value, then also saying how toxic Timnit is for injecting politics into the workplace while blindly accepting Dean’s version as truth, as if that acceptance isn’t purely guided by their own political biases. So many people convinced of their own objectivity because they’re taking the word of a corporate executive over the word of hundreds of employees now speaking out. Incredible r/iamsmart stuff here.

The internal review process is a PR smoke screen and anyone who has worked at Google or any large corporation knows it’s a bullshit excuse. Here’s a whole thread of former Google employees explaining how the internal review process is meaningless and is basically always ignored except for this instance where it was weaponized against Inmit:


https://twitter.com/_karenhao/status/1335058748218482689?s=21",11.0
gep1fc6,k6t96m,"You've managed to say nothing at all with these words.

Why do you think Timnit was fired?",0.0
gep2ckl,k6t96m,"Looks like your account is a burner or fake. -13 karma and hardly any activity. Against my better instincts, I’ll reply to you even though you’re clearly acting in bad faith with that baffling reply ignoring my points.

I’m not speculating on why Timnit is fired because I don’t have nearly enough information. That was not the intent of my comments but you’re trying to pretend that’s the only subject worth discussing.

The intent of my comments is to point the absurd hypocrisy of commenters in this thread taking Dean’s comments at face value and pretending that doing so is the objective course of action, devoid of political bias.

For context, I’ve worked in multiple FAANG companies including Google. Once you move to the director level at these companies everything you say is carefully crafted by PR and legal to protect the interests of the company in the event of eventual lawsuits. The idea that Dean would somehow write an honest letter detailing any potential shortcomings on the side of Google is laughable. All he did was lay a legal groundwork for her liability while doing “we can do better” platitudes, and y’all bought it while patting yourselves on the back for being anti-SJW.

If you buy what corporate executives say at face value, it’s because you have a vested interest in doing so and aren’t interested in personal growth.",6.0
geozsjo,k6t96m,,
geok25e,k6t96m,"What Jeff omitted is that the paper passed the normal review five weeks prior, and his PR-whitewashing was new (""actively working on improving our paper review processes,"" sure) and his sole initiative.",8.0
geonna9,k6t96m,Evidence for it passing review 5 weeks prior?,31.0
gep21au,k6t96m,"Note that even [Jeff Dean](https://www.reddit.com/r/MachineLearning/comments/k6467v/n_the_email_that_got_ethical_ai_researcher_timnit/), also not unbiased source, confirms it did pass reviews.

&gt; Unfortunately, this particular paper was only shared with a day's notice before its deadline — we require two weeks for this sort of review — and then instead of awaiting reviewer feedback, it was approved for submission and submitted.

As far as I can tell, the paper was approved in the standard way, and no one is contesting it.",5.0
gep1pne,k6t96m,"While not an unbiased source, [Standing with Dr. Timnit Gebru](https://googlewalkout.medium.com/standing-with-dr-timnit-gebru-isupporttimnit-believeblackwomen-6dadc300d382) gives 5 weeks timeline, and I haven't seen anyone directly contesting it.

&gt; Dr. Gebru and her colleagues worked for months on a paper that was under review at an academic conference. In late November, five weeks after the piece had been internally reviewed and approved for publication through standard processes, Google leadership made the decision to censor it, without warning or cause.",1.0
genpg2c,k6t96m,"I'm not in the industry, but this just seems like the internal politics of a company. This stuff happens every day in corporations I've worked for. 

Would someone be willing to explain to me what elevates this as something of interest?",31.0
geodxak,k6t96m,She has a Twitter army and knows how to use it,55.0
geou9tv,k6t96m,"She was co-lead of the Ethics in Artificial Intelligence team at Google. Google is a huge player in the AI field and has immense resources, so her departure - even if totally friendly and by her choice - would always be something that many would follow with interest.

The fact that she was not only fired, but that she was fired in a situation that involved a paper that raised some ethical concerns in areas that are crucial to Google (https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/ ) raised a *lot* of red flags, and got many people worried.",11.0
gep32qf,k6t96m,"Crucial to Google, as in literally powering Google search. In MIT Technology Review's words, ""BERT, as noted above, now also powers Google search, the company's cash cow"".

It is naive to expect Google to be neutral in a matter involving Google's cash cow.",6.0
gep2oum,k6t96m,,
gemwbwi,k6t96m,"This is indeed confusing. I read the document, but there is something not clear, she threatened them to meet her demands to show the identities of the reviewers at Google? why she asked for that. Is that indeed right that she focused only on critique  and not mentioning the effort to mitigate these problems. She did not said clearly she want to resign, but her way was actually reflecting that, but still it is not legal to fire her and clearly Jeff post is weird according to this issue. I am sorry I am from robotics community, but I want to understand who is the right in this situation",63.0
gemz2pv,k6t96m,"&gt; She did not she clearly she want to resign, but her way was actually reflecting that.

There is another email we're not yet seeing which has a list of demands. Google isn't about to leak it and it's not in Timnit's interests to have it seen either.",82.0
gen1ocy,k6t96m,"This is **pure speculation** but it seems possible that they would have wanted her out anyway and the ""ultimatum email"" gave them the perfect legal recourse to make that happen.

Edit: Just realized this was already suggested by u/Mr-Yellow [below](https://www.reddit.com/r/MachineLearning/comments/k6t96m/d_jeff_deans_official_post_regarding_timnit/gemzvtd).",42.0
gen7b52,k6t96m,"Well she did hire lawyers last year and threatened to sue the company. I wouldn't be surprised if she was on their radar as a ticking time bomb. 

It's also unclear how the process of submitting these papers works. Dean's letter says that it was approved before management could review it. So who approved it? Someone may've overstepped their bounds.",44.0
gen1ys4,k6t96m,"That's my read. Her mind was stuck in Twitterverse and didn't see the political reality. Put foot down with the confidence of having an army of backing, they rubbed their hands together and gleefully accepted her resignation.",53.0
gemzfvd,k6t96m,"Her email was leaked too. I saw it on Twitter yesterday, let me see if I can find it",7.0
gemzmam,k6t96m,I haven't seen one with the 3 ultimatums. Only stuff from the periphery.,18.0
gemzwyp,k6t96m,"So there's this: https://www.platformer.news/p/the-withering-email-that-got-an-ethical

Are you saying that there is an additional email? (Not terribly familiar with the specifics of all this, just happened to see it yesterday)",4.0
gen06d8,k6t96m,"Yeah I don't believe that's the email in question. That's the title of the article, but not the content.

It's where she kicked the hornets nest, but not where she delivered the ultimatums.",29.0
gen0nx9,k6t96m,"Ah, yeah I think you're right. I found this on Twitter which alludes to one of the three points: https://twitter.com/timnitGebru/status/1334881120920498180?s=19

All around rough situation. Both of them are so respected, it's weird to see them at odds

Edit: actually she summarized the 3 points here: https://twitter.com/timnitGebru/status/1334900391302098944?s=20

1 Tell us exactly the process that led to retraction order and who exactly was involved. 

2 Have a series of meetings with the ethical ai team about process. 

3 have an understanding of research parameters, what can be done/not, who can make these censorship decisions etc.",17.0
gen20vy,k6t96m,"For the lazy, she said that the following ""basically"" characterizes one of the conditions:

&gt;We demand that Jeff Dean (Google Senior Fellow and Senior Vice-President of Research), Megan Kacholia (Vice-President of Engineering for the Google Brain organization), and those who were involved with the decision to censor Dr. Gebru’s paper meet with the Ethical AI team to explain the process by which the paper was unilaterally rejected by leadership.

She acknowledged in her email that she received feedback through ""a privileged and confidential document"" but it sounds like she thought it was too general, vague, or legalistic.",12.0
gen5lhk,k6t96m,Sounds she represent all her underlings absolutely yet she felt oppressor from someone higher than her rank. Typical egomaniac behavior.,5.0
gemzkfl,k6t96m,,
geoo55n,k6t96m,"Yeah, I don't know all the background story and I don't like monopolies in general. But she said, meet these demands/requests and if not I'll resign. And they replied saying we accept your resignation. It's a perfectly sane response for the initial demand.

She's just creating more drama at this point tbh.",8.0
genwuf1,k6t96m,"""Who is right"" - I see this online way too often. Keep in mind that someone being proven wrong, doesn't mean the other person was right. It's entirely possible they were both in the wrong.

Also, California is an at will employment state. The company can legally fire her because they don't like the color of her shirt she wore that day, or literally due to the flip of a coin. Another problem online...everyone thinking they know the laws.",15.0
gemzf5t,k6t96m,[deleted],4.0
gen0h6m,k6t96m,"Then explain why a respected researcher would suddenly not what to ""do a proper lit review"". Because she's a bully?",10.0
gen2cot,k6t96m,[removed],-3.0
gen2oa9,k6t96m,,
gen0d8w,k6t96m,"Maybe you are right thats why it weird that she asked for the identity of the reviewers? but from what I see on Twitter she is so beloved by many people and rarely find any critique against her... I am sorry I am not from the field, but I am curious.",3.0
gen0m1y,k6t96m,"&gt; but from what I see on Twitter she is so beloved by many people and rarely find any critique against her

An illusion created by the divisive nature of twitter. If you were to speak out in one of those threads you'd quickly find yourself a victim of the mob. People know this and self-censor.",32.0
gemw0zc,k6t96m,"It’s odd to prevent a submission based on missing references to the latest research. This is easy to rectify during peer review. Google AI employees are [posting on Hacker news](https://news.ycombinator.com/item?id=25307167) saying that they’ve never heard of pubapproval being used for peer review or to critique the scientific rigor of the work, but rather to ensure IP doesn’t leak.

Other circumstances aside, it sounds like management didn’t like the content/finding of the paper. What’s the point of having in-house ethicists if they cannot publish when management doesn’t like what they have to say? 

Is it possible to do Ethics &amp; AI research at Google if a papers‘ findings are critical of Google’s product offering?",206.0
genam41,k6t96m,"I’ve worked on research within a few orgs, commercial, non-profit, and governmental. It is absolutely standard for a place to require you submit your work for internal review several weeks before you first submit for external review. It is absolutely standard for a place to refuse to allow you to submit externally if you haven’t passed the internal reviews. It is, unfortunately, absolutely standard for internal review comments to be nonsense about, say, not citing other work done within the org.",60.0
genps0w,k6t96m,And it is absolutely standard for highly cited researchers to loudly denounce when their papers are blocked.,-6.0
genvddy,k6t96m,Since when did Timnit Gebru become a highly cited researcher  [😂](https://emojipedia.org/face-with-tears-of-joy/),18.0
geodz2i,k6t96m,"IDK, when did she get [over two thousand citations](https://scholar.google.com/citations?user=lemnAcwAAAAJ&amp;hl=en)?",9.0
geoobyp,k6t96m,,
genx6pt,k6t96m,,
gemz4eq,k6t96m,"I don't work at google, but my org (CMS) reviews all aspects of papers (style to methodology) to ensure only high quality papers are associated with it. Maybe its misplaced, but I'm surprised that is uncommon apparently.",64.0
geo1383,k6t96m,I don't think it's uncommon. All papers going out from my org (gov. branch) have to pass internal review before submission to a journal. Its a bit weird to me that it got blown up into a huge issue.,20.0
geonzvq,k6t96m,"Well, she blew it up. She could have made the edits and resubmitted. Instead she called for hundreds of employees to stop working and demand to know the names of her negative reviewers. 

I have nothing but great admiration and respect for her, but her ego got in the way and has learned she’s not a god after all.",9.0
genk6k8,k6t96m,"I think it's more that the missing references undermined the conclusion of the paper. If the conclusion is ""Nothing is being done to mitigate the environmental and discriminatory ethical issues created by using big models"", and there's lots of research addressing these problems, the conclusion isn't a strong one.",43.0
geojhov,k6t96m,"I used to think this, but now we have some hints about the content of the paper from [MIT Technology Review](https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/) and I doubt this is the case:

&gt; The version of the paper we saw does also nod to several research efforts on reducing the size and computational costs of large language models, and on measuring the embedded bias of models. It argues, however, that these efforts have not been enough. ""I'm very open to seeing what other references we ought to be including,"" Bender said.

That's definitely not ""nothing is being done"" conclusion.",11.0
geonyqf,k6t96m,,
gen54rj,k6t96m,"The authors had many weeks to make changes to the paper. I shared this yesterday:

[https://www.reddit.com/r/MachineLearning/comments/k69eq0/n\_the\_abstract\_of\_the\_paper\_that\_led\_to\_timnit/gejt4c0?utm\_source=share&amp;utm\_medium=web2x&amp;context=3](https://www.reddit.com/r/MachineLearning/comments/k69eq0/n_the_abstract_of_the_paper_that_led_to_timnit/gejt4c0?utm_source=share&amp;utm_medium=web2x&amp;context=3) 

An organizer of the conference publicly confirmed today that the conference reviews are not even completed yet:

[https://twitter.com/ShannonVallor/status/1334981835739328512](https://twitter.com/ShannonVallor/status/1334981835739328512)

This doesn't strictly conflict with anything stated in Jeff 's Dean's post. However, the wording of the post strongly implies that retraction was the only solution and that this was a time critical matter. Cleary neither of those are true.",31.0
gendcag,k6t96m,"&gt; However, the wording of the post strongly implies that retraction was the only solution

Where do you get this from?

I don't read this at all.

My reading is that the feedback process from Google was that she needed to make certain improvements, and she disagreed, and that was where the impasse came from.",7.0
geneb0s,k6t96m,"She was never given the option to make improvements or changes.

She was first told to withdraw with no explanation whatsoever, and then after pressuring for an explanation, was given one that she couldn't share with the other collaborators, and no option to amend the paper, it was still simply that she had to withdraw without attempting to address the feedback.",25.0
genfcmz,k6t96m,"Yes and within Dean's post this makes it sound final: ""*We acknowledge that the authors were extremely disappointed with the decision that Megan and I ultimately made, especially as they’d already submitted the paper.*""",16.0
genfgpz,k6t96m,"&gt; She was never given the option to make improvements or changes.

Please cite?  I don't see anything that explicitly states that.",10.0
gengab1,k6t96m,"Look at the link I already shared:

&gt;https://twitter.com/ShannonVallor/status/1334981835739328512

The paper hasn't even received feedback from the conference reviewers. The authors were presumably ready to make further changes. 

Look at what that link was a reply to:

[https://twitter.com/emilymbender/status/1334965581699665923](https://twitter.com/emilymbender/status/1334965581699665923)

A coauthor of the paper has stated that they not receive feedback: ""...*the only signal is: don't publish.*"" 

They also stated elsewhere that they shared a draft with 30+ researchers in the field to request feedback. This doesn't sound like the actions of people unwilling to make changes to their paper. 

So, there is nothing to support your reading that ""the feedback process from Google was that she needed to make certain improvements, and she disagreed.""",4.0
genhtec,k6t96m,"This is all very confused.  The links you share don't support your claims at all. 

You made a very specific claim: that she wasn't given an opportunity to improve the paper.

&gt; So, there is nothing to support your reading that ""the feedback process from Google was that she needed to make certain improvements, and she disagreed.""

All sources (Google and Timnit) agree on the following:

* She was given some feedback on the paper from Google

* She disagreed with that feedback, and was very unhappy with it (her associated ultimatums #1 and #2)

Neither primary source (neither Google nor Timnit) make a claim that she wasn't able to update her paper, *if she agreed to incorporate Google's feedback*.

If we're going to make assumptions, as you seem to be (""this doesn't sound like""), then we should also be including the very rational point that if she was not permitted to change her paper in time, she almost certainly would have said that, as it is only to her benefit (i.e., she looks reasonable) and Google's detriment (they look unreasonable).

""I tried to incorporate their edits and feedback, but they wouldn't let me"" would be a powerful statement and claim.  But it is not one she is making.

&gt; They also stated elsewhere that they shared a draft with 30+ researchers in the field to request feedback. This doesn't sound like the actions of people unwilling to make changes to their paper.

This isn't really relevant.  You're describing, in essence, the peer review process, in comparison to specific feedback from your employer.

E.g., if I'm a climate change scientist working at Exxon, and have some cool new paper, I will probably share it for peer review.  And I'll be very open to including their suggestions, probably.

That doesn't mean that I'm equally open to including Exxon's feedback.

&gt; https://twitter.com/ShannonVallor/status/1334981835739328512

This tweet is equally consistent with a world where she simply didn't want to make those edits.

Yup, there was plenty of time to make edits.

No, she didn't want to.

&gt; https://twitter.com/emilymbender/status/1334965581699665923

Where does it state that she is a co-author of this paper?",13.0
geot693,k6t96m,"https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/

This article confirms bender as a coauthor.",2.0
genjvfd,k6t96m,"At this point you seem like you are sealioning. 

However, your history of posts makes it seem like you are not a troll. So if you don't want to come across as a troll, I suggest you do your own research to confirm whether Emily Bender was an author of this paper and don't ask people here to spend time doing this for you.",-9.0
geooejf,k6t96m,"This is your response? This is very disappointing. You were mistaken and the commenter corrected your errors, so you resort to ad hominem instead of evaluating the merit of the comment.  This is precisely why educated readers don’t take our work in CRT seriously.",2.0
genl421,k6t96m,"&gt; At this point you seem like you are sealioning.

This is exceedingly disappointing.  I've carefully laid out my concerns logically and incrementally, and there is nothing inflammatory contained within.

Re:Emily--I see now; I did look, but this seems to be buried down multiple levels on Google's search.

Taking a step back--

In situations like this, both sides have enormous incentives to be vague about timelines and omit contrary facts.  *Particularly* when you know that legal action is inevitable, it is generally exceedingly helpful to start from the POV of, could there be an alternate interpretation, where all claims by both parties would still survive as truthful in court?

Here, 100%.

Your claim about what is going on here could certainly be true.

But alternate interpretations still hold, given every piece of primary source information we have from Google and Timnit.  

With Emily's statement, we still have a lack of clarity as to what feedback (there may have been a variety of feedback) and why wasn't shared (i.e., was this Google's choice or Timnit's--which may sound redunctionist).

What you perhaps see as sealioning is real-world (earned in blood) experience with dealing with issues just like this, including in and around the govt/legal system.  I'm well aware of how both sides can give colored views of the scenario, particularly when both sides have strong financial (legal...) incentive to do so.",8.0
gennqn6,k6t96m,"There's obviously no evidence *against* your suggestion that Google gave the option to edit the paper but only to Timnit Gebru who then refused to makes edits based on that feedback.

But if true, why didn't Jeff Dean say that the authors made edits based on feedback from 30 external people but then refused to from their own company? That would make his argument stronger. Much stronger than ""*We acknowledge that the authors were extremely disappointed with the decision that Megan and I ultimately made, especially as they’d already submitted the paper.""*",4.0
_,k6t96m,,
geor2mv,k6t96m,,
geopqc7,k6t96m,"&gt; sealioning

That's not what sealioning is. What you're doing is far closer to the definition.",2.0
genft4t,k6t96m,"Which part, that she was initially given no feedback whatsoever (implying no opportunity to address it)? That's from her twitter thread.

That she wasn't given the option to share the feedback? The feedback was given in a privileged and confidential document.

That even after she was given the feedback she was unable to amend the paper? Well it's implied given that she couldn't share the feedback with the other authors of the paper. But also nonpublic sources.",3.0
gengjuy,k6t96m,"&gt; That even after she was given the feedback she was unable to amend the paper?

This part.

&gt; Well it's implied given that she couldn't share the feedback with the other authors of the paper. 

Please cite. 

Never anywhere was there a claim that she couldn't share the actual paper feedback.

&gt; But also nonpublic sources.

Out of bounds here.",3.0
gengpse,k6t96m,"Do you know what ""privileged and confidential"" means in a business context? It does in fact mean not allowed to share with anyone else.

Here also is an excerpt from her email that was used as a justification to fire her:

&gt;And you are told after a while, that your manager can read you a privileged and confidential document and you’re not supposed to even know who contributed to this document, who wrote this feedback, what process was followed or anything. You write a detailed document discussing whatever pieces of feedback you can find, asking for questions and clarifications, and it is completely ignored. And you’re met with, once again, an order to retract the paper with no engagement whatsoever.

Do you read that as her having the opportunity to incorporate the feedback?",4.0
genil6f,k6t96m,"Understood, and this could, in fact, align with what you are referring to.  

That said, I try to withhold judgment until there is sufficient clarity--Timnit is (possibly purposefully) somewhat vague here on timelines, when she did or didn't receive feedback, what specifically was ""privileged and confidential"" (was it actually the list of paper feedback? or was there more), was this the *first and only* time she'd received feedback from Google on this paper (""haven’t heard from PR &amp; Policy besides them asking you for updates""--OK, had she heard from other folks?), and so forth.",3.0
geniyht,k6t96m,"&gt; what specifically was ""privileged and confidential"" (was it actually the list of paper feedback? or was there more)

The paper feedback. Perhaps there was more, but the paper feedback itself was considered privileged and confidential.

&gt; or was there more), was this the *first and only* time she'd received feedback from Google on this paper

This depends on what you mean. She notes that she had gotten review from 30+ other researchers prior to submitting for pub-approval, and pub-approval was approved by her manager and the required approvers.

But PR and policy reviewing aren't doing a literature review. And those two things shouldn't be conflated. And yet the claimed justification for pulling the paper is that it didn't meet the requisite *technical* bar.",4.0
_,k6t96m,,
geoo2vf,k6t96m,This is her version of events?,0.0
geoo4am,k6t96m,"Yes, and to my knowledge verified by others involved in the paper.",1.0
geoohbn,k6t96m,That’s not verification. Thank you for confirming the bias.,1.0
geoonby,k6t96m,"&gt; That’s not verification

How is it not? Other people directly involved verified her story. What better verification is there? Google stating ""yeah we did something incredibly stupid""?

Google has not disputed any of those claims, despite them having been made prior to this statement. If they're untrue, why not dispute them?",1.0
geop7zr,k6t96m,"Really? You don’t see any conflict of interest by a coauthor (and close friend)? 

The other commenter already explained the review process. Even if Google was wrong, they explained exactly what was wrong in the paper. Timnit now says that they’ll eventually publish the paper with the edits in place as if that vindicates her. This is highly unethical (irony!) 

She should have done this in the first place. This is the worst kind of prima donna meltdown I’ve seen in a long time as an academic and it is extremely difficult to watch her unravel.",1.0
geopiq3,k6t96m,"I'm confused by what you're saying, and suffice to say you have a misunderstanding of the situation.

&gt; Timnit now says that they’ll eventually publish the paper with the edits in place as if that vindicates her.

Given that the entire time her goal was to be able to understand and incorporate the feedback, I don't see how you can call this unethical. Her problem wasn't ever getting feedback (by all accounts she got tons, from literally dozens of peers), it was getting no feedback and the paper spiked without the ability to respond or incorporate it.

&gt; She should have done this in the first place

This is what she tried to do in the first place. What do you think happened?

&gt; Really? You don’t see any conflict of interest by a coauthor (and close friend)? 

No more than from the organization who fired her. And I'm inclined to believe multiple individuals over a single organization.",2.0
_,k6t96m,,
gen1cfl,k6t96m,"&gt; Other circumstances aside, it sounds like management didn’t like the content/finding of the paper. What’s the point of having in-house ethicists if they cannot publish when management doesn’t like what they have to say?

PR.

This response seems to indicate it was more about the content. The issue seems to be like if a paper in an fossil fuels corporation's research division tried to publish something that acknowledged climate changed exist and was caused by fossil fuels but had no solution for this but just showed the problem. The oil company then fired that researcher because the paper didn't have a solution for climate change. Obviously even fossil fuel companies would be okay with research with had a clear solution to climate change that was equally as profitable. 

I get why Google wouldn't like the critique but I take issue with 

&gt;And to be clear, we are deeply committed to continuing our research on topics that are of particular importance to individual and intellectual diversity  -- from unfair social and technical bias in ML models, to the paucity of representative training data, to involving social context in AI systems.  That work is critical and I want our research programs to deliver more work on these topics -- not less.

This is clearly not the case. Google's critical work is serving ads.

TLDR; Does anyone here think Shell or Enron is a green fuels company just because their ads say they are doing all this research on alternative fuels? Then why do we do the equivalent with Google?",51.0
gen8c88,k6t96m,"

&gt;Unfortunately, this particular paper was only shared with a day’s notice before its deadline — we require two weeks for this sort of review — and then instead of awaiting reviewer feedback, it was approved for submission and submitted.

How can we draw this conclusion from that?

Maybe google would have censored the shit out of the paper, but maybe the review process would have been healthy and improved the paper. We literally do not know, since the proper review process was never given a chance. 

We're just assuming google would do the evil thing, which isn't necessarily even unlikely, I still want to see them do it before holding them accountable for it.",15.0
geo6fcc,k6t96m,"I'm not at Google, but I'm involved in publication requests approvals at another large company.

I can see myself raising a big deal if this is an Nth case when someone submits something for review with only a day or two to spare - and especially if they have proceeded and submitted a paper.  I can even see someone from Legal going further and starting to do an Ethics investigation of that person.

Because someone her level knows the approval process very well.  She would have also known the ""oops, I forgot, but I really need this quickly"" procedures.  Again, I don't know Google, but in my company I get these regularly enough -- usually it means the paper authors are emailing me and others in the approval chain (ours is a multi-stage approval process, and I'm 5th out of 6 stages) -- and saying ""can you please review it?  I'm sorry -- this deadline came up quickly for reasons X, Y and Z"", etc., etc.  

So if it looks like someone is trying to circumvent the process, it's a huge red flag.

And if there are 3rd party authors, that's another potential issue. Not sure how it works at Google, but again, I want to know when and how they already talked to those other co-authors.  Most of the time the answer is ""oh, we have a relationship because we are in the same trade working group, and we were just chatting, and she asked whether X would be possible, and I thought it wouldn't, but then I went home and thought that maybe we could do it in Y way, and...."".  So normal reasonable answers.  But worth asking.  And people would expect me to be asking them, and they know that I'm going to ask them and, again, that means giving us weeks to review the publication request.

And yet another possibility: there was already an internal decision to keep at least some of it as a Trade Secret, and she went and published or disclosed to others.  Why?  that's a different question.  But in corporate processes that too is a cause for a crackdown.",6.0
gen9k9n,k6t96m,"I would read the first response of this thread which gives insight of this process before when it doesn’t involved bad stuff for google 

Or look at sister companies/foundations

https://www.theguardian.com/technology/2017/aug/30/new-america-foundation-google-funding-firings",1.0
genauhe,k6t96m,"That's why I said ""which isn't necessarily even unlikely"", I don't have a high opinion of large companies ethical chops.  But poor actions in one case doesn't prove poor actions in all future cases. 

Imagine if she had gone through the review process and then could show us exactly what google had made her change and how it was covering up real ethical problems. That would be some enlightening critique of google.",12.0
gendwoz,k6t96m,"+1 

But you don't shit on the hands feeding you. Gebru seemed to put all her social science skills ahead of the consequences. Bad PR means lower investor confidence, outlook &amp; millions lost. Its no longer a matter of social choices, but avoidable damage. I m not pro-FAANG, but I guess they definitely are doing more than oil companies for the environment. Publications like hers cast doubts on what's going on, what's fact vs. fiction because it publishes under Google affiliation and criticizes their own practices, contrary to all the power saving &amp; efficienct data center news now &amp; then. That's what Jeff Dean was probably trying to avoid",9.0
genfsw0,k6t96m,The problem is what I pointed out elsewhere is that these groups roles in big corporations is to make kool aid and not drink it. If you drink that kool aid you will lead yourself to get fired,9.0
genfw92,k6t96m,FAANG are racing each other to help oil companies extract more oil and become their primary cloud providers. [Very interesting article for reference.](https://logicmag.io/nature/oil-is-the-new-data/),5.0
geng1wx,k6t96m,"Agreed. But their direct footprint is way lesser. And MSFT and Google are actually trying hard to reduce the carbon footprint. Not saints, but not sinners entirely.",2.0
genxzde,k6t96m,"As of September 14, [Google has a lifetime carbon footprint of zero](https://blog.google/outreach-initiatives/sustainability/our-third-decade-climate-action-realizing-carbon-free-future/). [Microsoft is aiming to be there by 2050.](https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/)",7.0
genghkn,k6t96m,Reducing their own carbon footprint doesn’t exactly mitigate leveraging all their technologies to accelerate and advance global oil extraction...,-3.0
genrxet,k6t96m,"Even with the most ambitious climate plans, fossil fuels are going to be a necessary component in the energy equation for decades.",2.0
gend9pp,k6t96m,I think that her toxicity finally outweighed the PR advantages Google enjoyed by having a token black researcher and they just looked for a way to fire her without making them look too bad.,42.0
geo48n9,k6t96m,pretty much. g00gle hires a professional whiner/sh\*tstirrer. Gets surprised and angry when she whines/starts stirring sh&amp;t.,9.0
gemxm6s,k6t96m,"I doubt it's simply about not referencing ""the latest research."" It sounds more like there was no effort to even consider the research that had been done for the topics proposed.

And if that's the case, it definitely makes sense it would be rejected. No reputable institution would approve of a paper that disregards a large portion of the research to suit its author's hypothesis (or in this case agenda).",65.0
gen1dj9,k6t96m,"FWIW, the reviewer of the paper has given their thoughts on the paper: [https://www.reddit.com/r/MachineLearning/comments/k69eq0/n\_the\_abstract\_of\_the\_paper\_that\_led\_to\_timnit/gejt4c0?utm\_source=share&amp;utm\_medium=web2x&amp;context=3](https://www.reddit.com/r/MachineLearning/comments/k69eq0/n_the_abstract_of_the_paper_that_led_to_timnit/gejt4c0?utm_source=share&amp;utm_medium=web2x&amp;context=3)

\&gt; However, the authors had (and still have) many weeks to update the paper before publication. The email from Google implies (but carefully does not state) that the only solution was the paper's retraction. That was not the case. Like almost all conferences and journals, this venue allowed edits to the paper after the reviews.",56.0
geo6q5u,k6t96m,"It sounds like she was the one who started dictating conditions to them though.  The ""if you don't tell me exactly who reviewed what, I would quit.""

Not at Google, but at my company this is where Legal steps in and says ""nope, this is not the way things work here.""  And Legal can come down hard enough even on most senior management about how it's best not to have even any appearance of favoritism. 

I can guarantee you that if someone were to pull that move at my company, (1) they would certainly not get their demands met, and (2) there would be an HR investigation about them -- and if there had been other issues, it's possible the company would breathe easily if the person decides to depart the company on their own terms.",6.0
geop2eg,k6t96m,"At Google, it's always known who has reviewed a paper, so a dialog between reviewer and author is possible.

It was only this one paper where that process wasn't followed, and all she demanded was the same process that every other paper went through.",8.0
gen422h,k6t96m,"This would have been handled in peer review, though. Although there is some very high quality research that comes out of google, they also pretty regularly put out papers that overstate contributions, ignore existing subfields, rediscover RL concepts from the 80s &amp; 90s etc. It's interesting to frame this as Timnit having an 'agenda' (when the point of a position paper is to make an argument) while google is somehow being objective about this. I think it's pretty obvious that this type of criticism would have been a $-sink/potential liability issue for Google and that's why they blocked it, not because they were academically upset there were a few missing references.",29.0
genk4hm,k6t96m,Not really when that paper authors are also essentially the conference organizers.,6.0
gemyqkw,k6t96m,"Ding, ding. Exactly how I understood it.",-10.0
gemxinh,k6t96m,"Other teams publish works that are either not possible for others to implement or have low societal or political impact. They are also not easy to be understood unless someone has background in ML.
Ethics however is easy to understand and has high political as well as societal impact. So using extra care regarding it, is totally normal.",11.0
genkmzd,k6t96m,"No, the paper rambled on problems without even mentioning any work that is already addressing these problems! It actually read more like an opinion piece or a blog post than a paper, including citations to newspapers.",17.0
genkvz6,k6t96m,"I'm not a Google employee, but I have been involved in the Google approval process due to collaboration. I was led to believe that your point is correct: the purpose of the review is to make sure that nothing is published that Google would like to patent or keep to itself so as to gain a technological business advantage or whatever.

I'm really hoping that at some point there is a bit of backlash to the degree that the academic ML community is allowing itself to be controlled by corporate interests. Google is a terrible offender in this regard but there are plenty of other cases of this. For example Andrew Ng who dedicated several years to assisting a company that was created solely to assist the subjugation of freedom of speech in China, and is then fully embraced by Stanford upon his return.",13.0
gemy9jv,k6t96m,"It's not simply just missing references. I would recommend you to read [this comment](https://www.reddit.com/r/MachineLearning/comments/k6467v/n_the_email_that_got_ethical_ai_researcher_timnit/gejra4a?utm_source=share&amp;utm_medium=web2x&amp;context=3), and also [this one](https://www.reddit.com/r/MachineLearning/comments/k6t96m/d_jeff_deans_official_post_regarding_timnit/gemxbha?utm_source=share&amp;utm_medium=web2x&amp;context=3).",17.0
geohhfn,k6t96m,"Those comments seem to suggest that the paper was rejected because the paper was overly critical of Google's practices. Regardless of what ordinary corporate action would be, shouldn't that be a big honking red flag to machine learning scientists?",5.0
genfknt,k6t96m,"Yes. Dean didn't say this outright due to obvious reasons. But overall I think the decision to push back the paper is totally reasonable. Usually, scientific research should be about unbiased cold facts but unfortunately we are getting into a grey area with it comes to AI Ethics (for example, environmental impact vs economic/business impact). Companies still have moral obligations but it's not that you can do whatever you want to push your ideologiy. Gebru will have the freedom to express her opinion and share her research from another organization.",2.0
gemwscw,k6t96m,"This is a very simplistic comment. There are tradeoffs between fairness and revenue generating products, as there are with security, privacy, and legal risk. What is the point of having a privacy expert (or security or legal) if they don't like your product decisions. Well, the point is to have an in-house discussion with the company execs make the call whether the tradeoff is worth it. I don't expect the security or privacy team to start writing public papers undermining the company's position with respect to Android/Youtube/Ads/Assistant/etc., and looks like Google does is not going to tolerate this from its ML ethics team.",12.0
gemzjc8,k6t96m,"It's a bit silly to frame this as the paper being critical of Google product decisions.

What is clear is that the concerns raised from leadership were not, at least obviously, about harms to the core business.

From Timnit's perspective, her main issue was that these concerns were raised to *HR* and then relayed to her verbally by a VP because she wasn't even allowed to look at the concerns for herself.

Does that seem like a normal or even professional research environment to you? Does that sound like the kind of situation that might lead to growing frustration?

One can be as obsequious as one wishes to be without normalizing this.",30.0
genprkj,k6t96m,"&gt; From Timnit's perspective, her main issue was that these concerns were raised to HR and then relayed to her verbally by a VP because she wasn't even allowed to look at the concerns for herself.

She also submitted the paper without giving the internal reviewers the 2 weeks' notice which is apparently standard? They could have told her to retract it based on that alone, and that would've been both normal and fairly professional.",5.0
geoewek,k6t96m,"It is apparently not standard? e.g.

&gt; I was once an intern at Brain, wanted to submit a paper with a 1day deadline and the internal review was very fast so we did not have problems. Given the stringent ML deadlines I could not imagine how much of a pain would be if every paper actually underwent such two-week process. (https://twitter.com/mena_gonzalo/status/1335066989191106561)",6.0
gemzqoj,k6t96m,"Security and legal risk are expected to be discussed behind closed doors. Researchers in ethics are expected to publish papers for public discourse—transparent discussion is the entire point of the position.

IMHO, the abstract of the paper is quite reasonable: [https://www.reddit.com/r/MachineLearning/comments/k69eq0/n\_the\_abstract\_of\_the\_paper\_that\_led\_to\_timnit/](https://www.reddit.com/r/MachineLearning/comments/k69eq0/n_the_abstract_of_the_paper_that_led_to_timnit/). If even this very light criticism is unacceptable to Google, it’s hard to imagine that an Ethics Researcher at Google will be able to publish other papers that critique the company’s products, even if true. It’s not “Ethics” if things can only be discussed when convenient.",16.0
gen1r3u,k6t96m,"&gt;Researchers in ethics are expected to publish papers for public discourse—transparent discussion is the entire point of the position.

This has happened before. The issue is these groups aren't actually good faith efforts for their purported missions and people who drink the koolaid and start to think so are just going to get sacked.

https://www.theguardian.com/technology/2017/aug/30/new-america-foundation-google-funding-firings",14.0
gen23e5,k6t96m,"why do you think ML fairness is different from security/privacy/legal risks? Should the ML ethics researcher be allowed to publish a paper that puts the company in a negative light, but the privacy or security or legal expert be confined to close doors? For example, perhaps there are some privacy issues associated with Assistant - should the privacy team publish a paper expressing it? I think you are right that many people think that way, but it is not clear to me why this is so.",1.0
gen2y2w,k6t96m,"Security: the practice of first informing company privately of zero-day and then publicly / transparently revealing say 60 days later seems like a reasonable practice.

Legal: attorney-client privilege is the norm here, default=secrecy

Privacy: absolutely should and must be transparent. Legally required (ie privacy policy), and we grant whistleblower protection for a reason. If there’s a privacy issue with Assistant that goes beyond the Privacy Policy, and no internal will to fix, this is illegal and absolutely should be made public.

ML fairness: if your role is a Research position, you are a member of the academic community and unlike the previous categories, publishing a paper is the expected forum for discourse.",7.0
genqcm2,k6t96m,"&gt; Privacy: absolutely should and must be transparent. Legally required (ie privacy policy), and we grant whistleblower protection for a reason. If there’s a privacy issue with Assistant that goes beyond the Privacy Policy, and no internal will to fix, this is illegal and absolutely should be made public.

That's a massive oversimplification of privacy... Yes, sometimes big companies violate privacy laws, but probably 90% of users' privacy concerns are, in fact, completely legal and covered in their privacy policy. Hiding your actions in a lengthy legal document which is intentionally worded as abstractly as possible to cover almost any imaginable use case - that is not anywhere close to ""transparent.""

If an employee has real privacy concerns internally, but it is strictly concerned with legally permissible actions, they have no legal recourse to share that information with the public.",2.0
gen4eve,k6t96m,,
geo57kt,k6t96m,"Right in the link op posted:

&gt; Unfortunately, this particular paper was only shared with a day’s notice before its deadline — we require two weeks for this sort of review — and then instead of awaiting reviewer feedback, it was approved for submission and submitted.",2.0
geol331,k6t96m,"The paper had been submitted and approved five weeks prior, per the Walkout employees' open letter.",3.0
geo3q0w,k6t96m,Well ignoring contrary or recent work is deliberate confirmation bias in your publication. To me that is not acceptable regardless of the content.,2.0
gen2abx,k6t96m,"I agree with you, but it also seems odd to me to put up a fight about adding a couple of references to a paper. This is literally a ""quick fix"".",3.0
genhzw5,k6t96m,"I think it boils down to someone thumbing their nose at the process and the company wanting to enforce that.

If this person/team can get away with it, how many others might stop following the process?

And then there is her reaction to being challenged on the process.

Her paper could be complete in the right, the corrections could have been slight, and maybe the person who tried to put a stop to it would have been overruled.  But none of that matters when you go rogue in a big company.

You just caused yourself to lose a fight you otherwise would have won.  You have to pick your battles, not everything has to turn into a code red.

And after reading the abstract, it seems like such a small hill to die on for a seemingly milquetoast criticism.",11.0
gendey1,k6t96m,"&gt; but it also seems odd to me to put up a fight about adding a couple of references to a paper

This was clearly about conclusions stemming from references.",5.0
gemxi5f,k6t96m,They probably want papers associated with Google to be impressive. That isn't a strange desire.,0.0
gen1idi,k6t96m,You're being quite willfully in denial of the most parsimonious explanation.,-5.0
gen1oou,k6t96m,"and what would that be?

That they didn't want a Google paper complaining about ML energy consumption?",14.0
gen6j7w,k6t96m,Apparently not!,1.0
gep160v,k6t96m,"Cristian Szegedy (lead author of Inception papers) said that in recent years, it is a standard process to send the papers for internal review. The person who said in Twitter that he was part of reviewing for Google and that they never checked for paper quality has not been in Google for years. So, it is very likely that with Brain getting bigger, they have enforced higher standards and now do quality internal reviewing. From anecdotal evidence, I tend to agree with Szegedy. A colleague of mine who is interning at Google had to send his paper for internal review/approval before the CVPR deadline, and the review was about the quality in addition to IP leakage.

Finally, this was a positional paper that shits in BERT et al. Google Brain has spent billions in salaries alone during the last few years, and BERT has been their flagship product, a product that has brought money to Google (most of the search queries now use BERT). Criticizing it for being bad for the environment, producing racist and sexist text is not something that Google would like. Especially, if there have been works from Google that try to mitigate those issues, with Gebru deliberately choosing to not cite. And even if she would have cited them, this is not a paper that Google would like to see the light of the day. Indeed, it is totally in their right to do so. After all, they are a private company whose goal is to bring value to shareholders. I think that Timnit and everyone else who works there knows this very well. If she truly wants to publish whatever she wants, then she should join some academic institution (I believe she has the credentials to start as a tenure-track assistant professor), and then she would be able to write these types of papers freely. Of course, she would also need to take an 80% paycut. But if you enjoy getting paid half a million-dollar or whatever a staff scientist gets paid, you need to know that you can publish only what the company wants and what the company thinks provides value for them. Bear in mind, that there are companies that do not allow publishing at all. It is the price to pay for getting rich fast.",1.0
geo4lpz,k6t96m,"When you give out ultimatums be prepared to accept the consequences. Play stupid games, win stupid prizes.",25.0
gemyimy,k6t96m,I would like this opportunity to urge everyone to stop making new threads over a standard company dispute that is highly politicized. There have been multiple conference results out recently and those papers need more attention on them than this.,58.0
gemyx5d,k6t96m,"We are actually taking a break before NeurIPS! Don't worry, all of this will be over very soon!",22.0
gen853i,k6t96m,[deleted],-4.0
geo7oot,k6t96m,Amen!,1.0
gent9xk,k6t96m,I don't understand why people are so confused about this. An employee tried to publish a paper to the direct harm of the employer and levied various threats with a history of controversy. So she was fired. Do you expect Jeff Dean to come out and directly say this? Of course not. Then stop analyzing his particular response.,44.0
geo0cr8,k6t96m,"She is now using the race card by implying that by Google firing her, Google is against black people in tech and against diversity.Thankfully I left the ML community in twitter because of toxic behaviour like these.",25.0
geokj5y,k6t96m,She also seems to be throwing in casual retweets about sexism... cause why not!,10.0
geoqqjj,k6t96m,"Why is the field seemingly filled with that type of people in the US? (SJW, ..) probably just on Twitter?",2.0
georj9a,k6t96m,"It's just the US/twitter.
I'm happy if someone is providing solutions in regards to bais in ML that is great. But some people build their whole carriers on finding problems instead of solving them to the point that they ignore existing solutions. I can't imagine having a conversation with those people without them mentioning their gender, race, or political topics, they invested so much they can't just see the world as it is.

Twitter doesn't have a downvote button only retweet and favourite so controversial idea float up in the timeline. That is one of the reason they are active daily on social media",7.0
genyhdl,k6t96m,[deleted],-1.0
geo2oz2,k6t96m,Sounds like she said “I will quit unless you do these specific things I demand”. They said “Ok well we aren’t doing those so we accept your resignation”,19.0
genvldn,k6t96m,"is anywhere safe from drama and politics?  god, please, save me from the inanity of it all.",14.0
geo7cfl,k6t96m,"Of course not, but that's normal. Every aspect of life is dictated in some way by a policy, although you only really notice when that policy changes or is trying to be changed. And drama is just something that happens often when people disagree and when not all the information about that disagreement is available.",3.0
geo50g3,k6t96m,"nope. The sjws don't like it that people aren't interested in their cult. So they've basically pursued you into your pastimes and anywhere you can run to make you care.

So whether you're in a machine learning sub, or playing a video game, or eating breakfest cereal. They're going to be squawking at you incessantly about intersectionality, and injustice, and patriarchy etc etc. Its like Jehovah's Witnesses on steroids if they had a monopoly on every communications and corporate medium on the planet. You will share in their OCD 24/7 whether you want to or not.",0.0
geo749v,k6t96m,"This is a really bizarre take. I get that it's mostly a rant, but like, you do get that people are just trying to improve the world by making sure issues are apparent so they can be fixed rather than ignored? You can disagree with which issues are present, but you at least understand very basically how people think right?",10.0
geodzuz,k6t96m,"Agreed, and that's why we need more discussion about Jesus and the role of Christianity in machine learning.",6.0
geoab4h,k6t96m,"My definition of 'improving' the world does not mean equality of outcome at the expense of equality of opportunity, the automatic assumption that the existence of two sexes is a bad thing and that we have to go on a crusade to force men and women to be identically represented in all fields and indistinguishable in every way like bacteria. Or that we should transition from taking offense at something meant to cause offense to taking offense at anything we can twist to be offensive. Or having  someone who removes the word 'blacklist' and gleefully runs around twitter looking for people's lives to destroy be held as the moral standard rather than an actual good person.",1.0
geoqrx5,k6t96m,"I’m guessing you’re not at all familiar with the research done in the Google AI ethics lab. How did we go from discussing a paper on biases in nlp language models(this is what the paper that was rejected was about) to equality of outcome and gender ?

My guess is that you’re taking whatever is going on in the zeitgeist and projecting here",1.0
geozbbb,k6t96m,A cursory look at Timnit Gebru's toxic Twitter feed will help you with this conundrum.,2.0
gep4uo9,k6t96m,"Good practice if an employee has a grievance is that you discuss it with them. In this case Jeff Dean did not speak with her at all nor her line manager.

Google have virtually no black employees in AI so whatever policies they have for inclusion are not working. And if your approach to disatisfied employees is just fire them it looks like you are not a good employer. Worse if it is a senior employee responsible for ethics and inclusion.",5.0
gemytms,k6t96m,"&gt; the paper itself had some important gaps that prevented us from being comfortable putting Google affiliation on it. 

Should have just stuck with *""It was submitted too late for review and inclusion.""*",31.0
gen3wlt,k6t96m,"Everyone hugs deadlines.  Google employees often submit things late for publication review.  When they do so, they are taking a chance.  If there are no substantive changes needed, they can sometimes get away with it.  But if there are, they risk having to retract their submission, which is embarrassing and frustrating for everyone.

So yes, needing changes is part of the full story here.  Had no changes been needed, the fact that it was submitted late wouldn't have mattered.",28.0
gen43o8,k6t96m,"Both sides are revealing their poor behaviour here, but this smells like something the PR division wrote.",18.0
genlgg8,k6t96m,"Corporations cannot speak with nuance. 

Everything only needs to be viewed through the lens of incentive. 

If someone up the lines compensation depends on papers not smearing the company then they are gonna try an squash it. 

When large corps start things like ethics review it is so they can have control over the narrative, not to improve ethics.

Everything is optics and if they can improve the optics without affecting stock prices expect problems.",10.0
gemvpmi,k6t96m,/u/netw0rkf10w Does Jeff's post change your theory (https://old.reddit.com/r/MachineLearning/comments/k6467v/n_the_email_that_got_ethical_ai_researcher_timnit/gejra4a/?context=3) at all?,7.0
gemxbha,k6t96m,"By contrast, **it confirms my theory**:

&amp;#x200B;

&gt;It’s more than just a single approver or immediate research peers; it’s a process where we engage a wide range of researchers, **social scientists**, **ethicists**, **policy &amp; privacy advisors**, and **human rights** specialists from across Research and Google overall. **These reviewers ensure that, for example, the research we publish** **paints a full enough picture and takes into account the latest relevant research we’re aware of**, and of course that it adheres to our [AI Principles](https://ai.google/principles).

&amp;#x200B;

&gt;This paper surveyed valid concerns with large language models, and in fact many teams at Google are actively working on these issues. **We’re engaging the authors to ensure their input informs the work we’re doing**, and I’m confident it will have a positive impact on many of our research and product efforts.

&amp;#x200B;

&gt;But the paper itself had some important gaps that prevented us from being comfortable putting Google affiliation on it.  For example, **it didn’t include important findings on how models can be made more efficient and actually reduce overall environmental impact, and it didn’t take into account some recent work at Google and elsewhere on mitigating bias in language models**.   Highlighting risks without pointing out methods for researchers and developers to understand and mitigate those risks misses the mark on helping with these problems.  As always, feedback on paper drafts generally makes them stronger when they ultimately appear.",17.0
gemzuxf,k6t96m,Have you read the paper? What makes you so confident that the paper frames her employer as negatively as you make it seem?,14.0
gen48l1,k6t96m,"&gt; What makes you so confident that the paper frames her employer as negatively as you make it seem?

Not the person you responded to, but the fact that they told her to retract it instead of changing it, is probably a good indicator that they weren't happy with the contents i.e. it was critical of some part of Google's vision for their research.",13.0
genw56c,k6t96m,"There's another possibility, which is that they expected her to be super resistant and hard to work with over the revisions, so it was easier to ask her to just take it down while they worked through the issues. I haven't seen any statement or implication from Google that the paper could never be resubmitted at any point.

I mean, they delivered the feedback to her through HR in a private and confidential document and went to great lengths to protect the identities of the reviewers. To me, this makes it look like people were scared to death of working with an employee known to be explosive.

And sure enough, her response to the feedback was to [publicly denigrate her leadership on Twitter](https://twitter.com/timnitGebru/status/1331757629996109824), make a bunch of demands, and threaten to quit.",5.0
gen5rqk,k6t96m,"Hi. I am as confident as you are when you ask your question, i.e. as a random member on an online forum discussing about a saga between some person and their company, both of which they don't know much about apart through the information seen on the Internet.

Just like many others, I am giving my observations and hypotheses about the topic. If you see my comments confident, then sorry because that is not my intention at all. I was just trying to present hypotheses with logic arguments. I'm going to edit the above comment to remove the part about paper framing because it may sound, as you said, a bit confident. Let's keep a nice discussion atmosphere.

It seems nobody here has read the paper (except the Google Brainer reviewer in the Abstract thread), so if one has a theory for their own sake, they deduce it from known facts and information. Here the fact is that Google doesn't like Gebru's paper. Do you think that's because there are some missing references? That would be too naive to think. And that's how I have my deduction. It turns out in the end that Jeff Dean's message is aligned with my theory (you can disagree with this but it doesn't change anything, my theory remains a theory, I didn't state it as facts.)

Cheers!",10.0
gencz2e,k6t96m,"Without disclosing too much, I am more knowledgeable than a random member of an online forum.

I'm just a bit baffled because I see a lot of people making inferences and reading between the lines about stuff that they apparently don't have a solid grasp of.

One of the things to keep in mind about certain statements you might read is that these are crafted by teams of highly paid experts. What's more important than what they do say is what they strongly insinuate without explicitly saying so. The end result is that many people come away thinking that they ""know""  something which was never actually said. I've seen this happen time and time again.",5.0
genhl2u,k6t96m,"Thanks for the kind reply! I think I am fully aware of the issues you are raising, and I totally agree with them. I personally always read from both sides of the story before drawing any conclusions/theories (if any).

&gt; I'm just a bit baffled because I see a lot of people making inferences and reading between the lines about stuff that they apparently don't have a solid grasp of.

This also explains the (good) intention of my comments. If you cannot stop people from making ""bad"" inferences, show them ""good"" ones. Of course I am not confident that mines are good, but they are somehow founded. Maybe this is not a good thing to do after all, maybe staying silent would be better? I don't know...

&gt; One of the things to keep in mind about certain statements you might read is that these are crafted by **teams of highly paid experts**. What's more important than what they do say is what they strongly insinuate without explicitly saying so. The end result is that many people come away thinking that they ""know"" something which was never actually said. I've seen this happen time and time again.

This is indeed very tricky! I would like to add something to that though. You seem to be an experienced and cautious person, so maybe this is not necessary, but just in case (and for the sake of other people reading this): **Similar things can be said about Timnit Gebru.** Google is a giant and has teams of highly paid experts, but do not ever underestimate Gebru. **She is a very powerful woman. Who else is able to wobble Facebook AI and Google Research the one after the other?** Look at how Google Research is struggling in handling the current situation (despite their teams of experts, yes), and remember how it was for Facebook AI. One should be cautious about what Google says, but they should be **equally** cautious about what Gebru says as well.

Regards.",9.0
geo6r1e,k6t96m,"Lol, why is everyone pretending like they don't know what happened? She wrote a paper which makes look Google bad, Jeff et al. recommended her to correct some parts of the paper by pointing out all the other research that has shown how Google is trying to solve them, but she and other female co-authors weren't ready to change the tone of the paper because of WOKE culture and they have already made mind on their point of view. Google being publicly traded company can't allow such writing to be affiliated with their name that undermines them, as she refused to modify her paper citing how she can't stand for this &amp; gave a clear impression of why she can't stand there where her work is not approved and stated that she will have to think about leaving the company, Google managers knowing how toxic her behavior is decided to cut her out despite being torn apart by media and WOKE twitter people.

Simply put; don't shit where you eat. It doesn't matter if you're straight white dude, asian, black, or a popular lgbtq+ person",24.0
geolo0u,k6t96m,Well maybe next time don't hire a professional activist masquerading as an AI researcher,7.0
geozj43,k6t96m,"&gt; But the paper itself had some important gaps that prevented us from being comfortable putting Google affiliation on it.  For example, it didn’t include important findings on how models can be made more efficient and actually reduce overall environmental impact, and it didn’t take into account some recent work at Google and elsewhere on mitigating bias in language models.   Highlighting risks without pointing out methods for researchers and developers to understand and mitigate those risks misses the mark on helping with these problems.  As always, feedback on paper drafts generally makes them stronger when they ultimately appear.

This doesn't sound like a fatal flaw. Couldn't they have just had the authors who are employees address their concerns during revise and resubmit instead of ordering retraction?",2.0
genvj93,k6t96m,"I think this is an example that demonstrates the limits of corporate industrial research groups in academic discourse.

Public universities have been described as the 'critics &amp; conscience of society', and assuming they take that role seriously, university researchers are in the best position to credibly publish on topics like AI Ethics without being subjected to pressures that might introduce bias.

I strongly support industrial research groups publishing on technical matters (as long as they do truthfully and it is carefully peer reviewed and ideally replicated by third parties) - the chances of bias creeping in from internal pressure is relatively low.

I also strongly support corporations appointing people to act as their critic &amp; conscience internally - i.e. not to publish, but to advise them of potential issues early.

But when it comes to hiring someone to work in a field that is predominantly about being a critic &amp; conscience (such as any form of ethics, including AI ethics), and to publish externally in academic journals, allowing that to happen in the normal hierarchical corporate context is always going to lead to an apparent conflict of interest, and lead to papers which are more spin than genuine. And it is quite likely that this is exactly what companies who hire in these circumstances want. Medical journals often deal with the same kind of conflict of interest, given research is often funded by drug and device companies - and they handle it by requiring a conflict of interest statement, and sometimes requiring everyone who contributed to by a co-author or be acknowledged. To gain credibility, companies often pay university affiliated researchers with no input into design of the study or the write up, only the subject to be studied.

So Gebru is absolutely right to object to a process that, at the least, creates a perception of a conflict of interest, on a paper she is staking her reputation on. I think this ultimately demonstrates a misalignment between what Google may have wanted out of the relationship with her (to leverage her reputation to improve its reputation) and what she wanted (to genuinely act as a critic and conscience). If Google is genuine about wanting to advance AI ethics, it could fix this by setting things up so it pays but doesn't influence papers coming out (e.g. by funding a university or setting up an arms length organisation it funds with appropriate controls). Journals and conferences in the field should probably enact more controls to counter this type of bias.",8.0
geowzhw,k6t96m,"What an absolute fucking sleeze. She didn't counter her own claims to make google look better and so she cant publish her own work. As an AI ethicist whatever. Then try and corporate speak us all out because they know your average joe will see her gender, skin color, and any moment of humanity to dismiss her outright.",3.0
geor1bd,k6t96m,Glad I’m not working in a country where woke culture is that toxic and prevalent,3.0
gep34vx,k6t96m,Which country do you work in? I want to move there,1.0
gen17r4,k6t96m,[Link to his Twitter post](https://twitter.com/JeffDean/status/1334953632719011840),1.0
geoyvyl,k6t96m,what's DEI?,1.0
gep5ebo,k6t96m,"Diversity, Equality or Equity, and Inclusion.",2.0
geoywfq,k6t96m,,
gep3qxa,k6t96m,[deleted],1.0
gep4ind,k6t96m,I don't even see a point to debate on whether that is the case or not when whichever way you look at it a firing was deserved anyway. Unless you think tantrums at this scale are perfectly excusable.,1.0
geope9k,k6t96m,"the basic and universal requirement in per review process is it's total anonymity. 

Protection against harassment in the case of negative remarks, protection against circle jerk favoritism in case of reciprocal positive ""favors"" (second problem is much much more general than people think).

The basic and universal danger for any researcher is to saddle moral licensing horse. It kills his/her research side.

 It is much much more general problem than people think.",1.0
genedaf,k6t96m,RemindMe! 2 days,1.0
genefz3,k6t96m,"I will be messaging you in 2 days on [**2020-12-06 23:52:33 UTC**](http://www.wolframalpha.com/input/?i=2020-12-06%2023:52:33%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/k6t96m/d_jeff_deans_official_post_regarding_timnit/genedaf/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Fk6t96m%2Fd_jeff_deans_official_post_regarding_timnit%2Fgenedaf%2F%5D%0A%0ARemindMe%21%202020-12-06%2023%3A52%3A33%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20k6t96m)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
geozun1,k6t96m,"Reposting this comment I made because it broadly applies to OP’s post in general:

————

It’s pretty incredible how many people in this thread are just taking Jeff Dean’s word at face value, then also saying how toxic Timnit is for injecting politics into the workplace while blindly accepting Dean’s version as truth, as if that acceptance isn’t purely guided by their own political biases. So many people convinced of their own objectivity because they’re taking the word of a corporate executive over the word of hundreds of employees now speaking out. Incredible r/iamsmart stuff here.

The internal review process is a PR smoke screen and anyone who has worked at Google or any large corporation knows it’s a bullshit excuse. Here’s a whole thread of former Google employees explaining how the internal review process is meaningless and is basically always ignored except for this instance where it was weaponized against Inmit:


https://twitter.com/_karenhao/status/1335058748218482689?s=21",0.0
gep2knz,k6t96m,Is that so? I thought the consensus was that she may have been on Google's radar already because of her past behavior and here she unintentionally gave them a way out.,1.0
gep3afc,k6t96m,"I’m not speculating on why she was fired. There’s a fire hose of information coming out right now about her and her tenure at Google and previous companies. It would be impossible to try and argue or discuss every single point, most people seem to be cherry picking here and there to prove their existing bias.

My point here is that taking Dean’s letter at face value is foolish and the anti-SJW crowd are doing so because they have preconceived biases about people like Timnit.",3.0
gemwxhc,k6t96m,,
gemmz18,k6s5k9,"Title:Investigating Learning in Deep Neural Networks using Layer-Wise Weight Change  

Authors:[Ayush Manish Agrawal](https://arxiv.org/search/cs?searchtype=author&amp;query=Agrawal%2C+A+M), [Atharva Tendle](https://arxiv.org/search/cs?searchtype=author&amp;query=Tendle%2C+A), [Harshvardhan Sikka](https://arxiv.org/search/cs?searchtype=author&amp;query=Sikka%2C+H), [Sahib Singh](https://arxiv.org/search/cs?searchtype=author&amp;query=Singh%2C+S), [Amr Kayid](https://arxiv.org/search/cs?searchtype=author&amp;query=Kayid%2C+A)  

&gt; Abstract: Understanding the per-layer learning dynamics of deep neural networks is of significant interest as it may provide insights into how neural networks learn and the potential for better training regimens. We investigate learning in Deep Convolutional Neural Networks (CNNs) by measuring the relative weight change of layers while training. Several interesting trends emerge in a variety of CNN architectures across various computer vision classification tasks, including the overall increase in relative weight change of later layers as compared to earlier ones.  

[PDF Link](https://arxiv.org/pdf/2011.06735) | [Landing Page](https://arxiv.org/abs/2011.06735) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2011.06735/)",2.0
gemv13o,k6s5k9,Accepted in the NewInML workshop at NeurIPS 2020,1.0
geo7cr4,k6qw9t,"I can't promise that you'll immediately figure out how to turn this into concrete model improvements, but I might check out https://github.com/interpretml/interpret, if you haven't already.

(Also see https://github.com/interpretml/interpret-community.)

Tooling package by Microsoft to help with model interpretability.  

Obviously not the only one out there, but IMO one of the nicer and more straightforward packages available.

As a side note--and this is verges into waxing philosophical--

&gt; But I find that it is nevertheless rare for these techniques to lead to insights that improve model performance.

This is I think unfortunately rare, as a general statement--deriving value from interpretability is hard work.  (If it gave up gains more easily, we'd see a lot more research into this space.)

From my own personal experience, FWIW, a lot of the gains will come less from direct model hacking, per se, and more from making sure that you really, really understand your data--most often, either because it is dirty in some ways (that would behoove you to clean up) and/or because it will help you understand features to add or ways to reframe the data.

(Even in deep learning, a lot of de facto feature engineering goes on, even implicitly--BERT, for example, is not just Transformer + a lot of data, it is a bunch of choices on how the data is organized and fed into the model, which is, in essence, feature engineering.)

If a package like what I linked helps with that data exploration, great.  Otherwise, you can often get a ton of leverage (at least in domains like structured data and text; images are harder) from just looking at differences between your gold and predicted labels, and iteratively using that to hone in on likely sources of divergences.",4.0
gepsf0p,k6qw9t,"That's a very good answer, thank you!

Since I work mainly with structured data, my reply will be more applicable to that.

I have also found that most of the increased performance comes from data understanding and proper data cleaning (which goes far beyond dealing missing values). I once started with an XGBoost model for credit risk prediction, and after a year on the dataset, Logistic Regression outperformed the original model, simply because of improved data quality.

I think this is especially valuable advice - many of the techniques could be employed in this process:

&gt;you can often get a ton of leverage (at least in domains like structured  data and text; images are harder) from just looking at differences  between your gold and predicted labels, and iteratively using that to  hone in on likely sources of divergences.",1.0
gemu26h,k6qw9t,"Here is a good blog on a variety of techniques we've implemented at [Fiddler.AI](https://Fiddler.AI) 

[https://blog.fiddler.ai/2020/09/ai-explained-video-series/](https://blog.fiddler.ai/2020/09/ai-explained-video-series/)",-1.0
geo11uh,k6pis9,"View in your timezone:  
[Wednesday, December 9, at 8:20am PST][0]  

[0]: https://timee.io/20201209T1620?tl=%5BR%5D%20NeurIPS%202020%20%7C%20Probabilistic%20Approaches%20for%20Algorithmic%20Recourse%20With%20Limited%20Causal%20Knowledge",1.0
gem32q9,k6kl20,"Title:Generalized Object Detection on Fisheye Cameras for Autonomous Driving: Dataset, Representations and Baseline  

Authors:[Hazem Rashed](https://arxiv.org/search/cs?searchtype=author&amp;query=Rashed%2C+H), [Eslam Mohamed](https://arxiv.org/search/cs?searchtype=author&amp;query=Mohamed%2C+E), [Ganesh Sistu](https://arxiv.org/search/cs?searchtype=author&amp;query=Sistu%2C+G), [Varun Ravi Kumar](https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar%2C+V+R), [Ciaran Eising](https://arxiv.org/search/cs?searchtype=author&amp;query=Eising%2C+C), [Ahmad El-Sallab](https://arxiv.org/search/cs?searchtype=author&amp;query=El- Sallab%2C+A), [Senthil Yogamani](https://arxiv.org/search/cs?searchtype=author&amp;query=Yogamani%2C+S)  

&gt; Abstract: Object detection is a comprehensively studied problem in autonomous driving. However, it has been relatively less explored in the case of fisheye cameras. The standard bounding box fails in fisheye cameras due to the strong radial distortion, particularly in the image's periphery. We explore better representations like oriented bounding box, ellipse, and generic polygon for object detection in fisheye images in this work. We use the IoU metric to compare these representations using accurate instance segmentation ground truth. We design a novel curved bounding box model that has optimal properties for fisheye distortion models. We also design a curvature adaptive perimeter sampling method for obtaining polygon vertices, improving relative mAP score by 4.9% compared to uniform sampling. Overall, the proposed polygon model improves mIoU relative accuracy by 40.3%. It is the first detailed study on object detection on fisheye cameras for autonomous driving scenarios to the best of our knowledge. The dataset comprising of 10,000 images along with all the object representations ground truth will be made public to encourage further research. We summarize our work in a short video with qualitative results at [this https URL](https://youtu.be/iLkOzvJpL-A).  

[PDF Link](https://arxiv.org/pdf/2012.02124v1) | [Landing Page](https://arxiv.org/abs/2012.02124v1) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2012.02124v1/)",1.0
