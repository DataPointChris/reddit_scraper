comment_id,post_id,comment,upvotes
g8nvhog,j9q6bp,"If you are imagining an expert system (if/else decision tree) then try the `experta` package. If you need it to comply with the sklearn regressor or classifier API, then you can create a class with `__init__()`, `fit()` and `predict()` methods.

* `__init__` : instantiate your custom experta `KnowledgeEngine` instance.
* `fit` : calculate any stats on the dataset that you might need and adjust the parameters within the engine ( can just pass for most problems)
* `predict` : use `self.engine.declare()` to intake the state/feature data and return the output of engine.run() which must be a numpy array to work in an sklearn Pipeline",1
g8f2vm9,j8vepi,"The hyperparameters tol and max_inter are generally used to tell the model when to stop it's optimization for fitting the parameters of the logistic regression. Generally, tuning these parameters won't make a big difference to the predictive power of your model.

The penalty and C parameter deal with regularisation. This is a concept in ML used to prevent overfitting of your model. Overfitting happens when your model performs poorly on unseen/test data compared to your training data. This usually happens when your model is too complex (perfectly tuned models are flexible enough to learn something from your data but not too flexible to overfit/memorize your training data).

The penalty parameter lets you choose what type of regularisation you want to apply. There are two types of regularisation L1 (lasso) and L2 (ridge). L1 regularisation forces some of your parameters for the unimportant features to zero (these features are dropped from the model- LASSO does automated feature selection). L2 regularisation forces some of your parameters for the unimportant features to zero but not exactly 0 (these features are not dropped from the model). Elastic net is a combination of L1 and L2 regularisation.

The C parameter lets you choose how much regularisation you want to apply. In the case of L1 regularisation, more of it means more features dropped from the model. If you drop enough unimportant features from your model, your model will not overfit. If you drop too many features you may lose some important information to learn from your data.",1
g8mmeq2,j8vepi,I love this response. Thank you so much.,1
g7u6e66,j5pcm6,Try installing with Anaconda,1
g7waffb,j5pcm6,"So what I had to do:

\&gt;conda uninstall scikit-learn numpy scipy

\&gt;conda remove --force scikit-learn numpy scipy

\&gt;pip uninstall scikit-learn numpy scipy

\&gt;pip install -U scikit-learn numpy scipy --user",1
g785qxo,j2v11w,"If you looked at the source code you would have noticed the ""algorithm"" kwarg which specifies that if you don't choose the ""brute"" option it uses a tree data structure. This may introduce slight discrepancies between the fit model and the ""true"" distribution of the data.",4
g79d2mb,j2v11w,"Thanks for your prompt response, I got it overnight.

I agree. I like the speed of the default model, I‚Äôll time it but I reckon it‚Äôs there to be quicker than brute.

I suppose I am then doing my own little brute method with the literal Euclidean nearest neighbour.  The combination works for my purpose.

Nice one, thanks.

Edit: two thoughts.  First, my data is very entropic so maybe trees ain‚Äôt so good.  Second, my code still might be wrong so brute is a good check that I haven‚Äôt screwed up my trig.",1
g7a2j0z,j2v11w,"If there are ties (several neighbors with the same score) then scikit chooses the first one.

A trivial example to show how it works

    x = np.random.randint(0, 5, (20, 1))
    y = np.arange(len(x))

    knn = KNeighborsClassifier(1)
    knn.fit(x, y)
    knn.predict(x)",2
g7b8lel,j2v11w,"Good point.  It‚Äôs not that, I removed duplicates for my purposes.  Thanks though.  üôè",1
g7dd7oa,j2v11w,"Oops!  My method of removing duplicates just inherits the same behaviour.  

I‚Äôm going to add fuzz/noise to them that that will work for my purpose.  

Cheers for the stimulus üíì",1
g70358r,j1idtx,"Without knowing the kind of system you're running this on (RAM, amount of cores etc, etc) or the hyperparameters you're trying to optimise on, you're generating 300 trees. That could take a bit. 

Try decreasing the number of trees (say, 2?). If it's still hanging, message back and we can try to troubleshoot.",1
g2w82j9,igtkry,"Nope. Sklearn doesn't return F / t / p values. You can code your own, but otherwise just use statsmodels.",2
g0b6kn2,i3546z,"Not sklearn specifically, but graph theory could be useful.",1
fyn0442,hu6y83,The clusters and centroids returned are the final results.,2
fynnk7i,hu6y83,Gotcha. Me and a friend had kind of come to a realization that might be the case for something we are working on but we weren't too sure. Thanks for answering!,2
g77yegr,ht4ol1,Hi.  I‚Äôve made AUC from small numbers of points. Treat them like polygons and literally calculate the rectangles and triangles?,1
fx4lpgh,hm6td7,"I think many ppl have got 100% on it, because when I implemented a simple CNN I was able to get 94%.

As for the type of model, CNNs are usually quite reliable for image classification. However, you can still get an accuracy of 85+ using just Dense and Flatten layers without Convolutions and Poolings",2
fx71pfp,hm6td7,"Thanks. Do you have any links where it shows people getting 100%?

I was given this link where it shows the best performance is around 96-97%:
https://paperswithcode.com/sota/image-classification-on-fashion-mnist",1
fv3gbqa,hakk07,suggest to use seaborn with lmplot(),1
fv3wp7z,hakk07,Convert language frame using one hot encoder,1
fvoi9f5,h0w3xx,"I don't have a background in math/science either... That said, check out stat quest youtube videos. Yes, they do include math, but honestly, its really not possible to grasp these algorithms without at least some math and logic involved. So if you want to really understand these, then you'll be required to understand things like derivates and alpha, etc.. Good luck!",1
ftgob55,gziaus,"Remember that k-means is not a classification algorithm. It only does clustering. Also note that 53 + 47 = 67 + 33 = 100.

Now can you explain what's going on?",2
ftgxkkr,gziaus,"Yes. I thought about that. It looks like it was just comparing original and predicted labels side by side and based on their order of occurrence only two possible result was produced, N and 100-N. Then how to judge the accuracy of the algorithm? How to get the cluster quality details?",1
fqwpb6e,glb4cy,"[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) is a data preprocessing technique, not a model fitting technique. The questions you're asking make me think that you want to fit a polynomial model to your data, that's *not* what this Class is for.",2
frim32i,glb4cy,"Got is, thanks!",1
fpj3n47,gcvtsm,"You'll probably have to implement your own [estimator](https://scikit-learn.org/stable/developers/develop.html). Then you can just pass that to your search function.

Note that stepwise feature selection is generally poor and should be avoided (see e.g. [here](https://redd.it/ehmi22)).",1
fmtws0m,fx6kdy,"Just from experience that‚Äôs a little high for a distance metric based classifier. Generally there will tend to be some on borders between classifications that will flip flop based on the corpus of observations you have. If you share code we can check to make sure, but the best part of these types of fun datasets is finding surprising ways to get things to work.

I would suggest triple checking over fitting with a holdout set, but congrats on your good training!",1
fmu1rkf,fx6kdy,"Thank you, well actzally the code is quite short, so i share it here:

# Get data
from sklearn.datasets import load_digits()

dataset = load_digits()

X = dataset[""data""]

y = dataset[""target""]


# tsne
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, init=""pca"")

X_embedded = tsne.fit_transform(X)

# For viz
import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))

plt.scatter(x=X_embedded[:,0], y=X_embedded[:,1], c=y)

plt.show()

# Generating sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fitting
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV

knc_params = {""n_neighbors"":[3,5,7,15]}

knc = KNeighborsClassifier()

gs = GridSearchCV(knc, knc_params, cv=5)

gs.fit(X_train, y_train)

# Run model
pred = gs.predict(X_test)


from sklearn.metrics import classification_report

print(classification_report(y_test, pred))


from sklearn.metrics import confusion_matrix

cf = confusion_matrix(y_test, pred)



import seaborn as sns

sns.heatmap(cf, annot=True)



________



Thats it, i hope i dont have a made a typo, i am on the phone right now üòÖ
Any insights to whats ""wrong""?",1
fmvfmvr,fx6kdy,"I don't know, but I think you have to fit tsne only with train, and transform train and test without using the test dataset to train tsne. When you use tsne on X it has information that came from test. In that way you are trickering the algorithm using information you supposily don't know yet(test data)!",1
fmvsjcf,fx6kdy,"Thank you, you are right, thats a silly mistake from me. Sadly tsne does not have a transform method... So i guess thats it for now. But thank you very much, bow i know i should pay more attention l.",1
fm4sp5c,ft2pcp,Is this where the group  is set to -1 for outliers? Or do you mean to filter out part of the data before pushing the rest through DBScan?,1
fm7kg61,ft2pcp,"Yes -1 signifies noise. But I want it to not classify trunks, so that I can remove low vegetation, but not trunks. But I've not figured out how to get it work that way.",1
fm5r3pk,ft1kmb,"From OneHotEncoder docs:

&gt;	Parameters
categories‚Äòauto‚Äô or a list of array-like, default=‚Äôauto‚Äô
Categories (unique values) per feature:
‚Äòauto‚Äô : Determine categories automatically from the training data.
list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.

You passed a string, which would error out. Try passing a list of categories, or switch to auto.

As to your error message, that did not come from the above code, but similarly you need to read the docs and use ‚Äòcategories‚Äô",1
fm700ta,ft1kmb,"oh = OneHotEncoder(categories = X\[:, 3\])

X= oh.fit\_transform(X).toarray() 

gives out 

""too many indices"" error",1
fmd17bl,ft1kmb,"You need to read the docs. Please look at them and check for what it asks for, not the column but the categories in the ith column.",1
fm704im,ft1kmb,"oh = OneHotEncoder(categories = X\[3\])

X= oh.fit\_transform(X).toarray()

gives 1D array instead of 2D array",1
fl3s1gy,fm1oov,"I think what your code does is pass  cols_ordinal to an imputer, return it as colums, and in parallel pass cols_ordinal to ordinal encoder and return those as even more columns. So ordinal encoder does not get the imputed columns! For that you need to pipeline imputer end encoder, and pass them to columntransformer as one pipeline.",1
fkyov8e,flg6a1,"Well, you just have to follow the code. There isn't one place. Eg for this metric, you'll see that the variable `self.effective_metric_` is created. Use Ctrl-F to see that eventually it's passed in to a `BallTree` as the `metric` parameter. So, open `_ball_tree.py`, and repeat.",2
fl6ns3e,flg6a1,"If you‚Äôre lucky a paper is cited in a comment. If you‚Äôre luckier the code follows the paper.
Or translate the code yourself, test important chunks of it to be sure.",1
fk1xbio,ffx9zw,"Hi, I don‚Äôt know what kind of output you got from the code you attached there, but here is the almost exact same example from the Tfidf transformer page:

&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)
&gt;&gt;&gt; print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
&gt;&gt;&gt; print(X.shape)
(4, 9)

Found at this link:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

And so it seems the Tfidf transformer SHOULD take in your corpus (a list of lists, where each inner list is a list of strings) and compute the corresponding IDF",2
fjxqzkb,ff9602,"I cannot verify this without jumping in and testing, which I can do in a bit. However, my suspicion is that since the Random Forest can take a multi-class scenario, there must be some sort of aggregation on the various class-level accuracy scores (or any other of the simple performance measures) and in this case it seems to be the arithmetic mean. So, in your case of only True False labeling, the mean of a single class accuracy score is that class accuracy score.",1
fk15zy7,ff9602,"Thank you very much, have you done some testing by chance?",1
fk2m6vq,ff9602,I believe the graph is a ‚ÄúROC‚Äù graph which compares true positives to false positives,1
fj4aa4k,fbfky4,"SKLearn dbscan accepts many pairwise distance functions, Euclidean, Manhattan, etc. So it can be a Euclidean function, if you want, but it is not by necessity. 

[DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Distance Metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances)",2
fj4mosa,fbfky4,"That's not what I asked. I asked about the parameter epsilon, if the neighborhood distance it is restricting for core/border membership is a euclidean distance. i.e. how would you choose epsilon on a fresh dataset?",0
fj4qu8h,fbfky4,"Simply put, asking how you select and tune hyper parameters is a very different question than if said hyper parameter is ‚ÄúEuclidean‚Äù. In this case, the metric is Euclidean because it is measuring Euclidean distance (by default). If you would like to select a hyper parameter of some function on Euclidean distance, you must tune it using the SKLearn libraries for this purpose, e.g. GridSearch. This will allow you to iteratively examine a space of hyper parameter combinations, and then select the best tuning. Lastly you can stack cross validation on each of these steps to increase the confidence you have in selecting the right tunings. This is more or less the core of the value add of a data scientist, and should be where most of the time is spent.",2
fj5yf2x,fbfky4,"Generally, most distance measurements, that I'm aware of in data science, are Euclidean, including the epsilon. Of course, you're free to choose Manhattan, other Minkowski distance, or any distance measure of your choosing.",1
fh5hpqs,f1dizs,"If I am understanding correctly, you want to define a function that accepts a time series and returns the lowest non-outlier value. This task really isn‚Äôt well suited to machine learning in the common sci kit learn sense. Specifically, this is because you do operations on the inputs and don‚Äôt necessarily return something from the set of inputs, rather either a class or continuous number.

If you want to make this current process more robust, you could winsorize your data at a couple standard deviations, then take the bottom. Alternatively take the second percentile.",1
fh5mtfh,f1dizs,"Thank you for that input, kind of what I was suspecting, but wasn‚Äôt entirely sure.",1
fgi8ngf,eylu4p,"Well, this blog post was originally posted on Neuraxio‚Äôs blog. It is very aggressive (or even toxic) marketing, I guess.",0
fgl9y7e,eylu4p,It has never been submitted to r/scikit_learn before.,1
fglaho0,eylu4p,"I know for sure that it was posted in /r/deeplearning earlier and this post was cross-posted there as well.

Also, I am convinced that some judgements about scikit-learn are merely too strong and others are controversal. For example, the blogpost says that joblib is bad because it is not able to serialize some objects. The funny fact is that joblib is based on pickle which is the best serialization facility in Python. Numpy, SciPy, and Pandas support pickling. If a library does not support pickleable objects, it is an issue of the library not joblib.",1
fftgv14,ev1as7,"If you have a well defined tree, it will be deterministic, so depending on the structure, this could be conquered using a simple for loop or recursion. SKLearn allows you to save and load models via pickle, but provides no easy mechanism for loading a tree from outside. Really though, all you should need in the simplest form is a large stack of ifs and returns.",1
ffumz33,ev1as7,I don‚Äôt know if I understood your answer correctly. Do you mean by ‚Äûa large stack of ifs and returns‚Äú that I should hard code my tree?,1
ffuthk7,ev1as7,"PMML is like an XML or YAML format, so if you can read this into some data structure you could iterate over that with a loop or recursion. But if your tree is small or you don‚Äôt feel comfortable writing a loop like that, an if tree will do precisely the same thing.",1
ffww1x9,ev1as7,Thank you very much :),1
fdo92j7,em9fxf,"&gt; I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc. For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

This could be called a type of ensemble learning, but I wouldn't recommend this, especially to a beginner. Instead, for each model, you should look at model.score(Xtest, ytest). The higher the better. That allows you to choose just one model.

&gt; The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

That can happen. It's not really an error. But presumably the score() will be low, and you can reject that model.

&gt; MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

That can happen. It might indicate bad hyperparameters. For now, I would suggest to just reject that model.

&gt; I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Most people who talk about NFL don't have a clue, and you can ignore them. You don't need to test all parameters and methods, but it's good to test a few. When you know more, you'll start to understand which models and hyperparameters are relevant for you to test.

My recommendation is to follow any tutorial that walks through sklearn with a specific dataset, eg the Titanic dataset. Don't try to program anything on your own before doing this. I recommend Andrew Ng's ML course on Coursera.

Finally, I recommend r/MLQuestions and r/learnmachinelearning rather than r/scikit_learn.",2
fdq8sjz,em9fxf,"Thank you for your gold advices. Really appreciated. I'll check accuracy score, pick the best one and follow all materials out there to improve my ML journey. Thanks",1
fc0d721,effb98,"In a famous 1996 paper, David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. ... There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
by Aur√©lien G√©ron",3
fc0f7vz,effb98,"Thank you. Yeah I have checked the book by Aurelien Geron, but can you still guide me where will I find exactly what Im looking for?",0
fc07b6i,effb98,"For algorithms try LinearRegression/Ridge (the easiest to interperate/know how they work), RandomForestRegressor or if you don't mind another package, Xgboost (which will most likely perform the best or have similar performance to the random forest, but is difficult to interperate)

Your data seems to have 1 or more rows for 1 patient, all these algorithms i proposed can only take in one row to predict some value. This means you will need to pre process/prepare the data to work well with these algorithms. I would recommend the use of aggregate functions (count of past visits, average wait times, etc.) to convert these many rows into 1. You could also lag your data so you have columns like 'time last visit', 'time of second to last visit' etc. The day of week and time of day could also make for some interesting features for you to play with :)

To train the model you will use the fit(X, y) argument once you declare your model. X is the features (time of visit, day of week, time since last visit, disease, ...) and y is what you predict (time of action).

You will also want to split your dataset into a training and test dataset. The train is what you fit the algorithm with and the test is used to see if the algorithm will actually work on new unseen data.

This is quite a brief high level overview of everything you will need to do, i also recommend tackling the titanic and housing prices datasets on kaggle.com as there are some fantastic solutions for you to learn from which is what started me off with machine learning :)

Hope this is helpful to you",1
fc0f3we,effb98,"Wow, that is a very detailed explanation. Thank you so much!! I wish i could give you a medal. 

Btw, how many features do you think I should include?",2
fc0vp3d,effb98,"Haha, no need for a medal, I'm happy enough to try and get someone into ML :)

Unfortunately there isn't really a fixed number of features that's guaranteed to work, and different algorithms might work better with certain combinations (e.g. The regression models don't work well with highly correlated features, so you will most likely need to remove them before modeling). I recommend having too many to start with (it's common to see datasets with 10-1000 features) and if needed, apply some sort of feature selection/regularization techniques to reduce the number used in the model :)

You'll never know if a feature is any good unless you try it!",1
fc1qa5g,effb98,"You should probably look into modeling this using a Poisson distribution, this is a pretty well studied problem in statistics. Here's an introductory blog post: [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459](https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459)",1
f8dxf0w,e0h8ao,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askprogramming] [Column transformer throwing away some features?](https://www.reddit.com/r/AskProgramming/comments/e0hbkw/column_transformer_throwing_away_some_features/)

&amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1
f8516p7,dyzwn9,"Hi, a few things. You should be using a classifier ‚ÄúMLP Classifier‚Äù to do a prediction that will give you a confidence as output. Unless you‚Äôre trying to predict the amount of rain the next day, in which case ~80% R2 would be excellent.

You are able to pass deep network shapes into your model, using the ‚Äúhidden_layer_sizes‚Äù argument. Similarly you can change ReLU to be the TanH or Sigmoid functions and the solver for how weight and bias changes are calculated. That is the extent of basic MLP tuning available, but is pretty robust for academic projects like this.

I *highly suggest* trying other modes like the Naive Bayes, SVM, Boosted Trees, Random Forest, and Logistic Regression. These also have large spaces of hyper parameters to tune and cross validate over. 

If you really want custom written network functions, you‚Äôre probably going to need to go the TensorFlow route which is much lower level.",1
f6xomto,dtnh3f,"I'm pretty new to machine learning and scikit-learn, but not a complete beginner. For a school assignment, I'm trying to optimise the hyperparameter alpha of ridge regression by cross validation using 30% of data as the holdout set.

Standard stuff, I've done this before. First I consistently got alpha=0 as my best alpha. Ok, the model is not overfitting, least squares solution is best. However, as seen in the image, my group then noticed that alpha=-85 gives the best performance on the test set.I am utterly confused. Ridge regression minimizes the cost function

||y - Xw||\^2 + alpha \* ||w||\^2,

so a negative alpha should lead to weights that are artifically inflated weight above the optimal least squares weights? So how come these weight give a better prediction on the test set, as given by mean square error as the metric?

I am completely confused. First I thought there was something wrong with my CV scheme, so I reduced the code to the minimum moving parts, using scikit-learn methods. Still, the problem persists, with several different random seeds. Code below:

    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=0.3)
    model=Ridge(alpha=0, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=0, mse:"")
    print(mse)
    model=Ridge(alpha=-85, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=-85, mse:"")
    print(mse)

This outputs:

     Alpha=0, mse:
    
     12.49668153434439
    
     Alpha=-85, mse:
    
     12.487210630035198

Anyone know what is going on here? At this point, I am seriously considering just implementing ridge regression from scratch, and seeing if the problem persists...

EDIT:

I implemented Ridge regression from scratch, without external libraries.

    def RidgeOwn(X, y, alpha):
        holder1=(np.dot(X.T, X))
        holder2=holder1+np.dot(alpha,np.identity(holder1.shape[0]))
        holder2=np.linalg.inv(holder2)
        holder3=np.dot(holder2, X.T)
        weights=np.dot(holder3, y)
        return weights

The same problem persists, the optimal alpha is now -24. No idea what is happening. There must be something very wrong with my input data, right? But I cant imagine what would cause this...",1
f6yc9kc,dtnh3f,"Antiregularization is a thing and can happen quite often, especially if you feature engineered using PCA or otherwise did other sorts of regularization. 

This gives a much more in depth look: https://stats.stackexchange.com/questions/328630/is-ridge-regression-useless-in-high-dimensions-n-ll-p-how-can-ols-fail-to",2
f6yzky6,dtnh3f,"Hmm... I read the link, fascinating. But I do not think everything is working as intended. No PCA, other regularisation or preprocessing has been done. And I very much doubt negative alpha is intented to be correct on an introductory course like this...
The question remains, what could be causing this... Must be something strange with the input?",1
f6z6tle,dtnh3f,"I can only speculate without running the analysis on my own machine, but your code looks correct at a glance. My only suggestions would to be to confirm you have no type errors, run some k-fold experiments iterating up the alpha, and try a lasso or other regularized OLS and see if the results are similar. It could simply be correct as it is rare you would even consider a negative L2 alpha, and your professor may even have bounded their hyper parameter search space at 0 as I would the majority of the time.",1
f6xn6gq,dtiy98,"Shuffle Split: This function create infinite iterations of your data where the test and train are randomly assigned at each iteration. Therefore, you can have a point that is repeated in testing or repeated in training. This could cause issues in ensuring you have a proper validation score if certain classes are over or under represented.

K-Fold: This function will shuffle your data, then draw boundaries every len(data)/(k+1) observations. You then use one of the k+1 resulting folds as a validation set and train on the rest.

That is, a shuffle split with a 20% test proportion will generate infinitely many randomly split 80/20 train/test buckets. A K=4 fold split will leave you with 5 buckets, of which you treat one as your 20% validation and iterate through 5 times to get a generalized score.

If you are doing classification with imbalanced classes, a stratified version of both exists which maintains the distribution of your response variable amongst the folds and splits.

Generally, K folds is seen as the proper method as you prevent any bias from random sampling.

Specifically to your question, you must never use the test set from a different train set in shuffle split because you are very likely to have the same data in your train and test which is a huge information leakage and invalidates any model performance metric.",1
f6yev8r,dtiy98,Thanks alot for your thoroughly explained comment. That was helpful.,1
f1x7n0w,dar9z6,"Depending on how many different commands you want to look at (and it‚Äôs a little unclear why SKLearn would be your package of choice rather than just rote statistics, correlation matrices, etc. ) you could encode the categories using a (I forget precisely the name but) OneHotEncoder or LabelEncoder. This would give you the ability to run multiclass classification algorithms.",2
f1yw2xd,dar9z6,"aha, I have multiple protocols and whole lot of commands... we may be talking about 50-60 major commands with most of usage and same amount of commands with less usage for like 5-6 thousand times a month.

I want this to ultimately be used as a way so we can identify bots and crawlers and stuff. im completely new to this and have been a back-end developer. now I want to improve on my reports of usage and gather more useful information. like some bot is hitting these types of commands in that interval, so this new thing working on same interval and commands with marginal differences, so this might be of the same family? and things like that. I don't even know what should be used for this!",2
f22pmhw,dar9z6,"If I am understanding correctly, this is an unsupervised problem (that you do not have a training set with what would be the correct predictions). In this case, you want to look into ""unsupervised learning"" and ""clustering"". This may give you the ability to segment your larger dataset into one that could potential give you information about different heterogenous groups in your data, but this hinges on you having more data than just a text command.

In this case though, unfortunately I would expect that it will be difficult to do what you want. A first step would be identifying existing traits of bots (i.e. tons of commands faster than a human could send from the same ip, etc.) and attempt to conquer the problem that way. To my knowledge SKLearn does not have many good ways to deal with this type of problem other than LabelPropogation but these are advanced techniques. You may have luck with programatic solutions more than machine learning. Good luck! Please feel free to message if you have questions.",2
ezsq8kj,d244x4,Post very short code.,2
excka8y,cs2urp,"You can just add the two arrays after one-hot encoding each separately.

I never heard of two-hot, that's interesting! Could you describe the situation more.

I guess the levels are the same for the two variables?",1
exctsp1,cs2urp,"Thank you.

To give you a better idea of what I'm dealing with, the data frame describes housing data; it's just one of the tutorial data sets on Kaggle. There are two categorical features to describe sections of the basement. They have the same categories, ""unfinished"", ""good living quarters"", etc, and the order does not matter. I could just one-hot encode both of them, but that would result in twice as many columns as I need for these features. For this example, it doesn't really matter, but I could see how knowing the best way to implement two-hot encoding could be useful in the future.

With one-hot encoding, a row of two features with five categories would look like this: 

[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]
while two-hot encoding would look like this: 

[0, 1, 1, 0, 0]",1
exea82m,cs2urp,"Thanks!

I suggested using +which could give the value 2 in some cases (so now ternary variables). If you do want binary you would combine with or. This discards information, potentially.",1
ewdbblx,cnnacy,RFE removes features based on the coefficient values.,1
ewermxj,cnl2a5,"I can‚Äôt remember what the output is for the kmeans cluster centers, but your slicing of ‚Äúcenters‚Äù may be referencing data with a numerical index or something.",2
ew3ud66,cmmbi5,"I‚Äôve seen bizarre slowdowns when intel MKL fights with other multiprocessing methods, basically making way more threads than can be efficiently processed. The severity of the problem seems to be OS and python installation dependent. Try shutting off MKL mutithreading by setting the environment variable ‚ÄúMKL_NUM_THREADS‚Äù to 1.",1
ew3vm3l,cmmbi5,I use openblas,1
evycli4,clpubv,by googling,1
evyddn0,clpubv,"search for Anomaly Detection in Network Traffic 


https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf",1
evydlnu,clpubv,"https://paperswithcode.com/paper/network-traffic-anomaly-detection-using 

here code with paper
using dl",1
evulpwt,cl88rf,Make sure your python build is the anaconda version and the sublime python path points to to Anaconda3 dir (JSON user settings of Sublime Text 3),1
ew0qyin,cl88rf,If nothing else works. Uninstall everything and install a clean version of Anaconda distribution and point Sublime to the proper path.,1
ew0xmus,cl88rf,I just restarted the whole computer but thanks for help,1
etq1qkx,cc2mxr,"if by numpy you mean pandas, then

&gt; X = dataframe.values[1:]  # features
&gt;
&gt; y = dataframe.values[0]  # target
 
should do the trick",1
etumb5d,cc2mxr,Thanks. So basic I'm almost embarrassed for asking.,2
etj4faq,cbm4g6,"What defines those data points (dots)? Is it just 2d location (x, y)? How did you label the original data points as blue or red?

&amp;#x200B;

If they are linearly separable (you could draw a straight line between them) then a linear SVM is probably your best bet. Still, it would be good to know more about the actual problem.",1
etrdap9,cbm4g6,"A point difines a line with its rho and theta (so 2D). There are left lines (red points) in an area of the graph, and right lines (blue points) in another part of the graph. So the blue and red points are clearly linearly separable, but this could be interesting if I'd like to only separate the blue form the red.  I want my application to be able to say ""this new point determines a left line"" or a right line, or none.",1
erxuz4x,c4q5i6,"Try
` from sklearn.cluster import kmeans `

I think the module is cluster and the class is kmeans.",1
erzcvj2,c4q5i6,"I finally successed. But in a way I've not expected:  


import  sklearn.cluster.k\_means\_  as kmean

&amp;#x200B;

kmeans = kmean.KMeans()  


Why should I do it in such indirect way...",1
eqko7d2,bylpjd,"Just use 

model_name.fit(w_cancelled_data_X, w_cancelled_data_Y)

Then for your active contracts:

model_name.predict(active_data_X)",1
eqkq7gb,bylpjd,"Wow! That did it. I was making this way more complicated than I needed to.   I really appreciate the help. Out of curiosity, do you have any good suggestions on how to export the full predictions to a csv? Thanks for the help again. I truly appreciate it.",2
eqkrf7t,bylpjd,"No problem. If you are familiar with the pandas library, I‚Äôd convert active_data to a DataFrame: 

import pandas as pd

active_data_X = pd.DataFrame(active_data) 

‚Äîside note: there‚Äôs a method that reads csv files and converts them to DataFrames, which goes as follows: active_data_X = pd.read_csv(‚Äúpath/filename.csv‚Äù)  ‚Äî

And then run:

active_data_X[‚Äòprediction‚Äô] = model_name.fit(active_data_X)

And then:

active_data_X.to_csv(‚Äúresults.csv‚Äù)

Edit: added the close ‚Äú",1
eqo01ik,bylpjd,I didn't even realise that `sklearn` worked with `DataFrame`s. TIL!,2
eo9wn6b,bqtxes,"&gt; `cluster_centers_` : array, [n_clusters, n_features]
Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these will not be consistent with labels_.

&gt; init : {‚Äòk-means++‚Äô, ‚Äòrandom‚Äô or an ndarray}
Method for initialization, defaults to ‚Äòk-means++‚Äô:
‚Äòk-means++‚Äô : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.
‚Äòrandom‚Äô: choose k observations (rows) at random from data for the initial centroids.
**If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.**",1
eobky9y,bqtxes,missed that.. thanks!,1
eoace2y,bqtxes,"You can save the cluster centers as a .npy file with np.save('filename', kmeans\_obj.	
cluster\_centers\_)

Then you can load with np.load('filename')",1
ela96qt,bevq9d,hmm maybe divide the image into pieces and run each thread for blob detection?,1
ekuebnz,bcwbv4,Bootstapping the models will get you the variance of the accuracy.,1
ekwf5jw,bcwbv4,How can I do that?,1
elc8rti,bcwbv4,"Thanks all got it by looping on bootstrap samples.
Thnaks",1
ekw9ipq,bcwbv4,"It's a binomial distribution, therefore if the accuracy is p, the variance is n*p*(1-p), where n is the size of the set used to calculate the accuracy.",1
ekrkuj8,bcbduy,"RFs in sklearn produce probabilities of labels, which can be used to calculate ROCs which in turn can let you ""tune"" the probability to minimise false positives.",1
ekmvywi,bbxi9t,"Just record the cluster means. Then when new data comes in, compare it to each mean and put it in the one with the closest mean.",2
ekp91di,bbxi9t,"Ah, yes, that will work, thanks!",1
eiy0oo4,b36h5a,U mean they are just different objects or oredict differently too?. What about feature importances in both?,1
eiy0uzg,b36h5a,"Sorry I didn't get your question ?. I mean a random forest with the conditions mentioned in my question should create exactly same trees ( estimators ), while training. What do you think about that ?.",1
eiy1c8r,b36h5a,It should. What i meant was how are u differentiating both the eatimators. On what basis?,1
eiy1x70,b36h5a,"I create a random forest (scikitlearn) with number of estimators 10(This with create 10 different decison trees). I can access each of theses estimators(dection trees).  

Code Example

model = RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019)¬†

Different estimtors/ decision trees can be accessed by 
model.estimators_[2] , model.estimators_[5] etc.


Should model.estimators_[1] ,model.estimators_[2] and model.estimators_[5] be same ?",1
eiy3cvm,b36h5a,They both will be different instances internally both might have the same structure. I have not really analysed the eatimators before. U can try to predict a few examples with each estimator to check,1
eiy4t6e,b36h5a,You don't need to predict. You can just visualize the the individual trees. I did that and they are different,1
eiy4u16,b36h5a,That is strange. Can u share the notebook over git or something?,1
eizifmi,b36h5a,"Why would it be creating the same decision tree over and over again?  That's not how a random forest works, even when you don't bootstrap and fix the seed.

If you created two forests with a fixed seed and the same parameters, they would be the same.",1
ehtkjb6,axgj2c,"great idea!

think you should add number and scale of factor vars, can greatly impact runtime 

&amp;#x200B;

also the amount of duplicate columns

&amp;#x200B;

i like it though... make it for r

&amp;#x200B;",1
ehuex1m,axgj2c,"Thank you for the feedback u/weightsandbayes we really appreciate it!  

Adding the variance was definitely something we were thinking about. I think this would a good avenue to explore, we should give it a try and I agree for many algos variance definitely plays a role. 

I haven‚Äôt used R in quite a while, what library should we tackle first in your opinion if we were to build a similar thing?",1
ehtqae2,axgj2c,"Fantastic idea. Could it be extended to other libraries? MLib, H2O, Keras, etc.",1
ehuev0d,axgj2c,"u/dj_ski_mask thanks for asking and you raise a great point.  
We built our library in a very scalable way, for example adding support for a new scikit learn algo is as simple as updating the config Json and running the model estimator.  
Adding a new algorithm here: [https://github.com/nathan-toubiana/scitime/blob/master/scitime/\_config.json](https://github.com/nathan-toubiana/scitime/blob/master/scitime/_config.json)   
And running the \_data function here: [https://github.com/nathan-toubiana/scitime#how-to-use-\_datapy-to-generate-data--fit-models](https://github.com/nathan-toubiana/scitime#how-to-use-_datapy-to-generate-data--fit-models)  


In principle nothing really prevents us from extending this to other libraries  
One challenge if we want to extend this outside Scikit-learn is that we are using scikit-learn specific methods throughout the code base.  
We would probably want to wrap our functions with a Library layer to specify what library we‚Äôre targeting. But It definitely can be done !",2
ehufxkc,axgj2c,By vars I meant variables haha ,1
ehuh90d,axgj2c,"Got it!

But by number of vars do you mean number of columns ? If so it's already factored in.

The distribution of each variable is also something we should look into.  
",1
ecs1ses,aahf76,"For gains, wouldn't it be simpler to just multiply by the gain factor?",2
ecs20oe,aahf76,"But then it will not be sign-conscious. If the gain is not a constant, but a function.",1
ect12sr,aahf76,"This sounds like it has nothing to do with sklearn.

Multiplication is typical especially for audio which I guess your signal might be - you could clarify.

If you know what you're doing then yes, you could use np.sign.",1
ect7y0q,aahf76,"Well not directly obviously, but this could be a common use-case in sklearn. Particularly I'm trying to apply a ""windowing function"" (not really a window, but similar principle) to the signal in such way that the window is slightly different for the y- than the y+ part. Thus for y &lt; 0 I want to apply window1, but for y &gt; 0 window2.",1
ec1cetm,a75oid,"You're not doing anything wrong. These metrics include the number of predictions of a class in the denominator so they divide by zero in this case. The NN, for some of your workloads, just never predicts that class. You could ensure that there are plenty of examples of that class in the training set. Apart from that all you can do is choose to report F and precision only when there are plenty of samples in the test set.",1
ec1d4g8,a75oid,"I wonder, why does it change though? As if MLPClassifier() fits a different fit every time I run the program. Even when it uses the same params? Yes, since MLPCLassifier() is implemented using stochastic gradient? But then, if I get ""errored results"" and ""non-errored results"", then are both valid? Or should I discard results that give this problem? The difference that occurs in prediction accuracy, when the error occurs, is quite drastic. 0.85 vs \~0.65 or even \~0.45, when this error pops up. So it ""seems"" that the MLPClassifier somehow fails occasionally, on this data set.",1
ec1gpdh,a75oid,"This is why we often report a cross-validated value, not just a single value. Yes, it could be that the classifier just fails sometimes. You can try different architectures and hyper parameters, especially initialisation and optimizer to see if it becomes more reliable, or try collecting more data.",1
ec1gv1e,a75oid,"What are you referring to with cross-validation? You mean that one ought to cross\_validate on the model, rather than fit the model a single time?",1
ec1ho19,a75oid,Yes,1
ec1hpud,a75oid,"But what does this help? If a cross\_validate ""fold"" produces the error, then it will be reflected to the averages of that cross\_validate? So even then one'd need to perhaps look for ""clean runs of cross\_validate""?",1
ec1u38k,a75oid,"It helps only in that if we report an accuracy value, it's an honest one (with error bounds if we like). It doesn't help to avoid the runs that go bad - for that see my earlier answer.",1
ec0d5su,a753f2,"Solution: I was accidentally using the MLPRegressor() class, when I need to use MLPClassifier() to get the output as multiclass.",1
ec1gxk7,a746h0,"The best way to estimate is to try running with a very small proportion of your data, say 0.00001, and then increasing by powers of 10 and seeing how it scales.

If you're seeing 10% CPU in a long-running computation, it could be because the job is disk-bound. Alternatively if you have 8 or 12 cores, and sklearn is using 1 and Task Manager reports it as a percentage of the total.",2
egyudwg,a73oda,"The C value, penalty, and random_state are the only ones I adjust. Random state is just stay consistent so really only the C value and penalty.",1
eagk9z2,a0c8dw,"It's more of an ML question than an SKLearn one. Training MLPs is still kind of a black art.

You can certainly try a different optimizer (I see they have LBGFS, Adam, SGD), momentum on/off, different learning rate, shuffle on. Also if you changed the default values for tol or anything else, try going back to the default. Apart from that, it might be worthwhile normalising your data columnwise (or it might not).",1
eagl4du,a0at1q,"Yes. You can check this out by creating and fitting the MLP and then looking at the `coefs_` attribute. First of all it doesn't crash! And second the `coefs_` are the same shape you'd expect with (7,).

It doesn't seem to be documented, so if there isn't a ticket in Github, you could raise one, either to remove this behaviour or to document it.",1
eahj20o,a09ukl,"train_test_split takes X and y, not Xy",1
eagw9zj,a06iin,"The amount of parameters you're attempting to optimise over is 10*2*11*3*3*2 = 3960. 

This isn't necessarily an issue straight up, but if your dataset is large and you are running a lot of decision trees with several tweaks to their parameters it's going to take some time.

You can try to scale back the parameters you're optimising over (bootstrap = false for example is one that can go...) as a start.",2
eahhbwl,a06iin,Thanks; Yeah I can try reducing the parameter matrix. so far I've reduced the cv and iterations,1
eahlc2t,a06iin,"I'd suggest lowering the n_estimators.

You mentioned this is a dataset that had an associated paper. What parameters did they use?",2
eai03lt,a06iin,"I tried lowering the n\_iter as much as possible and reducing cv but that didn't help. The paper I'm following didn't list parameters, and also they used WEKA not scikit learn so it doesn't really carry over",2
eajcy7s,a06iin,"&gt;I tried lowering the n\_iter as much as possible and reducing cv but that didn't help.

I ran your code using the MNIST and it ran fine, so that's not the problem. What's the size of the dataset your working with? Is your system actually using resources while running or has it hung?

&gt;The paper I'm following didn't list parameters

That's just bad research.

&gt;and also they used WEKA not scikit learn so it doesn't really carry over

The parameters are named differently and use different defaults, but you'd be able to replicate in sklearn... if they gave their parameter values... ;)

&amp;#x200B;",2
eajybfy,a06iin,"thank you for checking; I got a friend to run my code on his computer and it finished in like 1 min. So I think my laptop is just crappy, or I need to update something or reinstall it :P the size of the data set was small, like 131x 22 so I don't know what the problem is. I guess its just bad hardware",2
eak503r,a06iin,Seems like. :( ,1
ea2as8t,9yhcqq,"Do you mean having sklearn calculate coefficient p values, determine goodness of fit or out of sample validation?",2
ea3f85a,9yhcqq,"Whatever is meant when one says ""testing the model"".",0
ea5jij5,9yhcqq,"Hence my question. There are several different ""tests of a model"", ranging from ""which variables are related to the prediction"" to ""can the model's prediction be applied to new data"". 

Understanding what you want is half the battle.",1
ea85xvi,9yhcqq,"I'd imagine that library developers would develop a set of tests tied to any particular method. So if I use LinearRegression(), then somewhere near there are at least ""common tests"" for LinearRegression(). Also, since not all tests are suitable for all models, that should dictate how a library is designed.",1
ea87yte,9yhcqq,Have fun then!,1
ea1t6zy,9yhcqq,"Yes, have you looked at the manual? It's great.",0
ea1yi0j,9yhcqq,&gt;[https://scikit-learn.org/stable/auto\_examples/linear\_model/plot\_ols.html](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html) ?,1
ea287y4,9ygvsi,"The model is trained ""in-place"". Even when you call .fit(), you don't have to assign the model back to the variable you instantiated it in. Internally it just adjusts all the self.coeffs_  and self.intercept_ and all other model.params without creating a new model to hold them.",1
e9ve9uw,9sbqvm,"You can use an IDE like PyCharm, set up a break point in the \`fit\` method and then use the tools to step through the code one line at a time. You can set up different variables to watch and see them change.",1
dyirkg8,8emc3b,"I think there is not a definitive answer, but you can try with the following approaches:

1. You can just append the PageRank as another feature;
2. you can use the bag of words matrix to classify and then, use the classification prediction as an input for another model (which has PageRank as a feature);
3. You can classify both separately and use both predictions as inputs for a third classification model.

(2 and 3 are also called ensemble methods, look on the web for stacking).

Regards.


",2
dxnqyib,81xksg,"Use TFidfVectorizer instead, it has a mindf and maxdf args for that tuning. Just initialize mindf =3 to ignore terms that appear in less than 3 docs",1
dxo9maz,81xksg,Oh thank you very much! You can't imagine how many times I've read [the documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and I've always missed those parameters.,1
dxoduia,81xksg,You are very welcome ,1
dun5ic5,7z4qi7,"The first time you fit the model you had 4 possible classes. [1, 2, 3, 4] But when you called partial fit, you had the label for the new point be 5. Standard classifiers are not able to accommodate adding new possible classes after the initial setup.",1
dunbold,7z4qi7,[deleted],1
dunbtit,7z4qi7,"See this changed code, it works fine without fit() 
     
     from sklearn import neighbors, linear_model
     import numpy as np
  
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
        X.append([6, 9])
        y.append(5)
    
        X.append([10, 50])
        y.append(6)
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""log"")
    
        #sgd_clf.fit(X, y)
    
        #print(sgd_clf.predict([[6, 9]]))
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here",1
dunbvpo,7z4qi7,I understand i need to give class labels before initial fit().  But then fit uses all the new data and generates a model from scratch . what is the purpose of partial_fit then ? ,1
dviwuxk,7z4qi7,I found a work around by taking some random classes like np.arange(100) in first partial_fit(),1
dtvqk2p,7vw4ap,"Consider building a neural net, e.g. with [Keras](https://keras.io/). Since neural networks are basically trained via batch processing, they can be [tuned with new observation data](https://stats.stackexchange.com/questions/220169/how-do-i-add-more-training-data-to-pre-trained-deep-learning-net). Check out [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) for a good introduction.",1
dtxahq1,7vw4ap,"Thanks brylie, thats a wonderful book :) Could you please let me know if i can use scikit knn model along with keras ? Like i train with scikit knn model and just use batch training feature of keras. Is this possible ? I will read the book in detail today. Thanks again :) ",1
dtxbssd,7vw4ap,"Keras is much more comprehensive for deep learning than scikit-learn neural network. You might be able to create a hybrid model with KNN and a neural network. However, consider starting with one or the other, for simplicity.",1
dtxcf6f,7vw4ap,"which one can be trained fast ? I see that neural networks work slower. In that case, will scikit incremental learning would be a better fit ?
 http://scikit-learn.org/stable/modules/scaling_strategies.html

When exactly i need to use keras vs scikit neural networks if fast response back is my criteria than accuracy ? 

",1
dtxq4s6,7vw4ap,"I think there are at least two types of 'response time' to consider:
- training - where the model learns from provided data
- inference - where the model classifies or predicts based on new data

Classification time will likely be similar between models. Training time may vary greatly, but there are ways to speed up training in some cases. To which do you refer to when you say 'fast response back'?",1
dtzdf7h,7vw4ap,"Hi brylie, For training. If i get a newly registered user, i want to add his data to the already existing trained dataset and perform the inference step.

Currently, Just because of one/two users, i am training the whole dataset which is taking lot of time to go to inference step. 
",1
dtzepe9,7vw4ap,"Back to your original question, I am not sure it is possible to re-train a kNN classifier by adding new observations to an existing model. It is likely you will need to train a new model instance using all of the data (or a train/test split approach).

However, you can [take an existing Keras model and run its `fit()` method on new data](https://github.com/keras-team/keras/issues/1868#issuecomment-272078441), which will update the existing model. 

Keras may save time in the long run since you can update the model. There also ways to  speed up the Keras training, e.g. by using a GPU.",2
du04ux2,7vw4ap,thank u brylie :),1
du3dv06,7vw4ap,"Brylie, if i create a keras model, can i still be able to get the closest matches for my test data from trained dataset ? 

For example, with knn i have flexibility to get the closest matches to the test data with kneighbours function. Can this be possible with keras ?

Can i train a scikit model and then pass this trained model to keras ? 

 Sorry if my questions are naive. i am learning machine learning",1
dn4vbad,70m5l1,Check out the SciPy Cookbook revipe foe [Matplotlib: django](http://scipy-cookbook.readthedocs.io/items/Matplotlib_Django.html).,1
dn4vqr7,70m5l1,You might also like to [use Bokeh charts in a django project](https://www.hackerearth.com/practice/notes/bokeh-interactive-visualization-library-use-graph-with-django-template/).,1
dn4vx8g,70m5l1,See also: [django-chartflo](https://github.com/synw/django-chartflo),1
g8nvhog,j9q6bp,"If you are imagining an expert system (if/else decision tree) then try the `experta` package. If you need it to comply with the sklearn regressor or classifier API, then you can create a class with `__init__()`, `fit()` and `predict()` methods.

* `__init__` : instantiate your custom experta `KnowledgeEngine` instance.
* `fit` : calculate any stats on the dataset that you might need and adjust the parameters within the engine ( can just pass for most problems)
* `predict` : use `self.engine.declare()` to intake the state/feature data and return the output of engine.run() which must be a numpy array to work in an sklearn Pipeline",1
g8f2vm9,j8vepi,"The hyperparameters tol and max_inter are generally used to tell the model when to stop it's optimization for fitting the parameters of the logistic regression. Generally, tuning these parameters won't make a big difference to the predictive power of your model.

The penalty and C parameter deal with regularisation. This is a concept in ML used to prevent overfitting of your model. Overfitting happens when your model performs poorly on unseen/test data compared to your training data. This usually happens when your model is too complex (perfectly tuned models are flexible enough to learn something from your data but not too flexible to overfit/memorize your training data).

The penalty parameter lets you choose what type of regularisation you want to apply. There are two types of regularisation L1 (lasso) and L2 (ridge). L1 regularisation forces some of your parameters for the unimportant features to zero (these features are dropped from the model- LASSO does automated feature selection). L2 regularisation forces some of your parameters for the unimportant features to zero but not exactly 0 (these features are not dropped from the model). Elastic net is a combination of L1 and L2 regularisation.

The C parameter lets you choose how much regularisation you want to apply. In the case of L1 regularisation, more of it means more features dropped from the model. If you drop enough unimportant features from your model, your model will not overfit. If you drop too many features you may lose some important information to learn from your data.",1
g8mmeq2,j8vepi,I love this response. Thank you so much.,1
g7u6e66,j5pcm6,Try installing with Anaconda,1
g7waffb,j5pcm6,"So what I had to do:

\&gt;conda uninstall scikit-learn numpy scipy

\&gt;conda remove --force scikit-learn numpy scipy

\&gt;pip uninstall scikit-learn numpy scipy

\&gt;pip install -U scikit-learn numpy scipy --user",1
g785qxo,j2v11w,"If you looked at the source code you would have noticed the ""algorithm"" kwarg which specifies that if you don't choose the ""brute"" option it uses a tree data structure. This may introduce slight discrepancies between the fit model and the ""true"" distribution of the data.",4
g79d2mb,j2v11w,"Thanks for your prompt response, I got it overnight.

I agree. I like the speed of the default model, I‚Äôll time it but I reckon it‚Äôs there to be quicker than brute.

I suppose I am then doing my own little brute method with the literal Euclidean nearest neighbour.  The combination works for my purpose.

Nice one, thanks.

Edit: two thoughts.  First, my data is very entropic so maybe trees ain‚Äôt so good.  Second, my code still might be wrong so brute is a good check that I haven‚Äôt screwed up my trig.",1
g7a2j0z,j2v11w,"If there are ties (several neighbors with the same score) then scikit chooses the first one.

A trivial example to show how it works

    x = np.random.randint(0, 5, (20, 1))
    y = np.arange(len(x))

    knn = KNeighborsClassifier(1)
    knn.fit(x, y)
    knn.predict(x)",2
g7b8lel,j2v11w,"Good point.  It‚Äôs not that, I removed duplicates for my purposes.  Thanks though.  üôè",1
g7dd7oa,j2v11w,"Oops!  My method of removing duplicates just inherits the same behaviour.  

I‚Äôm going to add fuzz/noise to them that that will work for my purpose.  

Cheers for the stimulus üíì",1
g70358r,j1idtx,"Without knowing the kind of system you're running this on (RAM, amount of cores etc, etc) or the hyperparameters you're trying to optimise on, you're generating 300 trees. That could take a bit. 

Try decreasing the number of trees (say, 2?). If it's still hanging, message back and we can try to troubleshoot.",1
g2w82j9,igtkry,"Nope. Sklearn doesn't return F / t / p values. You can code your own, but otherwise just use statsmodels.",2
g0b6kn2,i3546z,"Not sklearn specifically, but graph theory could be useful.",1
fyn0442,hu6y83,The clusters and centroids returned are the final results.,2
fynnk7i,hu6y83,Gotcha. Me and a friend had kind of come to a realization that might be the case for something we are working on but we weren't too sure. Thanks for answering!,2
g77yegr,ht4ol1,Hi.  I‚Äôve made AUC from small numbers of points. Treat them like polygons and literally calculate the rectangles and triangles?,1
fx4lpgh,hm6td7,"I think many ppl have got 100% on it, because when I implemented a simple CNN I was able to get 94%.

As for the type of model, CNNs are usually quite reliable for image classification. However, you can still get an accuracy of 85+ using just Dense and Flatten layers without Convolutions and Poolings",2
fx71pfp,hm6td7,"Thanks. Do you have any links where it shows people getting 100%?

I was given this link where it shows the best performance is around 96-97%:
https://paperswithcode.com/sota/image-classification-on-fashion-mnist",1
fv3gbqa,hakk07,suggest to use seaborn with lmplot(),1
fv3wp7z,hakk07,Convert language frame using one hot encoder,1
fvoi9f5,h0w3xx,"I don't have a background in math/science either... That said, check out stat quest youtube videos. Yes, they do include math, but honestly, its really not possible to grasp these algorithms without at least some math and logic involved. So if you want to really understand these, then you'll be required to understand things like derivates and alpha, etc.. Good luck!",1
ftgob55,gziaus,"Remember that k-means is not a classification algorithm. It only does clustering. Also note that 53 + 47 = 67 + 33 = 100.

Now can you explain what's going on?",2
ftgxkkr,gziaus,"Yes. I thought about that. It looks like it was just comparing original and predicted labels side by side and based on their order of occurrence only two possible result was produced, N and 100-N. Then how to judge the accuracy of the algorithm? How to get the cluster quality details?",1
fqwpb6e,glb4cy,"[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) is a data preprocessing technique, not a model fitting technique. The questions you're asking make me think that you want to fit a polynomial model to your data, that's *not* what this Class is for.",2
frim32i,glb4cy,"Got is, thanks!",1
fpj3n47,gcvtsm,"You'll probably have to implement your own [estimator](https://scikit-learn.org/stable/developers/develop.html). Then you can just pass that to your search function.

Note that stepwise feature selection is generally poor and should be avoided (see e.g. [here](https://redd.it/ehmi22)).",1
fmtws0m,fx6kdy,"Just from experience that‚Äôs a little high for a distance metric based classifier. Generally there will tend to be some on borders between classifications that will flip flop based on the corpus of observations you have. If you share code we can check to make sure, but the best part of these types of fun datasets is finding surprising ways to get things to work.

I would suggest triple checking over fitting with a holdout set, but congrats on your good training!",1
fmu1rkf,fx6kdy,"Thank you, well actzally the code is quite short, so i share it here:

# Get data
from sklearn.datasets import load_digits()

dataset = load_digits()

X = dataset[""data""]

y = dataset[""target""]


# tsne
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, init=""pca"")

X_embedded = tsne.fit_transform(X)

# For viz
import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))

plt.scatter(x=X_embedded[:,0], y=X_embedded[:,1], c=y)

plt.show()

# Generating sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fitting
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV

knc_params = {""n_neighbors"":[3,5,7,15]}

knc = KNeighborsClassifier()

gs = GridSearchCV(knc, knc_params, cv=5)

gs.fit(X_train, y_train)

# Run model
pred = gs.predict(X_test)


from sklearn.metrics import classification_report

print(classification_report(y_test, pred))


from sklearn.metrics import confusion_matrix

cf = confusion_matrix(y_test, pred)



import seaborn as sns

sns.heatmap(cf, annot=True)



________



Thats it, i hope i dont have a made a typo, i am on the phone right now üòÖ
Any insights to whats ""wrong""?",1
fmvfmvr,fx6kdy,"I don't know, but I think you have to fit tsne only with train, and transform train and test without using the test dataset to train tsne. When you use tsne on X it has information that came from test. In that way you are trickering the algorithm using information you supposily don't know yet(test data)!",1
fmvsjcf,fx6kdy,"Thank you, you are right, thats a silly mistake from me. Sadly tsne does not have a transform method... So i guess thats it for now. But thank you very much, bow i know i should pay more attention l.",1
fm4sp5c,ft2pcp,Is this where the group  is set to -1 for outliers? Or do you mean to filter out part of the data before pushing the rest through DBScan?,1
fm7kg61,ft2pcp,"Yes -1 signifies noise. But I want it to not classify trunks, so that I can remove low vegetation, but not trunks. But I've not figured out how to get it work that way.",1
fm5r3pk,ft1kmb,"From OneHotEncoder docs:

&gt;	Parameters
categories‚Äòauto‚Äô or a list of array-like, default=‚Äôauto‚Äô
Categories (unique values) per feature:
‚Äòauto‚Äô : Determine categories automatically from the training data.
list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.

You passed a string, which would error out. Try passing a list of categories, or switch to auto.

As to your error message, that did not come from the above code, but similarly you need to read the docs and use ‚Äòcategories‚Äô",1
fm700ta,ft1kmb,"oh = OneHotEncoder(categories = X\[:, 3\])

X= oh.fit\_transform(X).toarray() 

gives out 

""too many indices"" error",1
fmd17bl,ft1kmb,"You need to read the docs. Please look at them and check for what it asks for, not the column but the categories in the ith column.",1
fm704im,ft1kmb,"oh = OneHotEncoder(categories = X\[3\])

X= oh.fit\_transform(X).toarray()

gives 1D array instead of 2D array",1
fl3s1gy,fm1oov,"I think what your code does is pass  cols_ordinal to an imputer, return it as colums, and in parallel pass cols_ordinal to ordinal encoder and return those as even more columns. So ordinal encoder does not get the imputed columns! For that you need to pipeline imputer end encoder, and pass them to columntransformer as one pipeline.",1
fkyov8e,flg6a1,"Well, you just have to follow the code. There isn't one place. Eg for this metric, you'll see that the variable `self.effective_metric_` is created. Use Ctrl-F to see that eventually it's passed in to a `BallTree` as the `metric` parameter. So, open `_ball_tree.py`, and repeat.",2
fl6ns3e,flg6a1,"If you‚Äôre lucky a paper is cited in a comment. If you‚Äôre luckier the code follows the paper.
Or translate the code yourself, test important chunks of it to be sure.",1
fk1xbio,ffx9zw,"Hi, I don‚Äôt know what kind of output you got from the code you attached there, but here is the almost exact same example from the Tfidf transformer page:

&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)
&gt;&gt;&gt; print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
&gt;&gt;&gt; print(X.shape)
(4, 9)

Found at this link:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

And so it seems the Tfidf transformer SHOULD take in your corpus (a list of lists, where each inner list is a list of strings) and compute the corresponding IDF",2
fjxqzkb,ff9602,"I cannot verify this without jumping in and testing, which I can do in a bit. However, my suspicion is that since the Random Forest can take a multi-class scenario, there must be some sort of aggregation on the various class-level accuracy scores (or any other of the simple performance measures) and in this case it seems to be the arithmetic mean. So, in your case of only True False labeling, the mean of a single class accuracy score is that class accuracy score.",1
fk15zy7,ff9602,"Thank you very much, have you done some testing by chance?",1
fk2m6vq,ff9602,I believe the graph is a ‚ÄúROC‚Äù graph which compares true positives to false positives,1
fj4aa4k,fbfky4,"SKLearn dbscan accepts many pairwise distance functions, Euclidean, Manhattan, etc. So it can be a Euclidean function, if you want, but it is not by necessity. 

[DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Distance Metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances)",2
fj4mosa,fbfky4,"That's not what I asked. I asked about the parameter epsilon, if the neighborhood distance it is restricting for core/border membership is a euclidean distance. i.e. how would you choose epsilon on a fresh dataset?",0
fj4qu8h,fbfky4,"Simply put, asking how you select and tune hyper parameters is a very different question than if said hyper parameter is ‚ÄúEuclidean‚Äù. In this case, the metric is Euclidean because it is measuring Euclidean distance (by default). If you would like to select a hyper parameter of some function on Euclidean distance, you must tune it using the SKLearn libraries for this purpose, e.g. GridSearch. This will allow you to iteratively examine a space of hyper parameter combinations, and then select the best tuning. Lastly you can stack cross validation on each of these steps to increase the confidence you have in selecting the right tunings. This is more or less the core of the value add of a data scientist, and should be where most of the time is spent.",2
fj5yf2x,fbfky4,"Generally, most distance measurements, that I'm aware of in data science, are Euclidean, including the epsilon. Of course, you're free to choose Manhattan, other Minkowski distance, or any distance measure of your choosing.",1
fh5hpqs,f1dizs,"If I am understanding correctly, you want to define a function that accepts a time series and returns the lowest non-outlier value. This task really isn‚Äôt well suited to machine learning in the common sci kit learn sense. Specifically, this is because you do operations on the inputs and don‚Äôt necessarily return something from the set of inputs, rather either a class or continuous number.

If you want to make this current process more robust, you could winsorize your data at a couple standard deviations, then take the bottom. Alternatively take the second percentile.",1
fh5mtfh,f1dizs,"Thank you for that input, kind of what I was suspecting, but wasn‚Äôt entirely sure.",1
fgi8ngf,eylu4p,"Well, this blog post was originally posted on Neuraxio‚Äôs blog. It is very aggressive (or even toxic) marketing, I guess.",0
fgl9y7e,eylu4p,It has never been submitted to r/scikit_learn before.,1
fglaho0,eylu4p,"I know for sure that it was posted in /r/deeplearning earlier and this post was cross-posted there as well.

Also, I am convinced that some judgements about scikit-learn are merely too strong and others are controversal. For example, the blogpost says that joblib is bad because it is not able to serialize some objects. The funny fact is that joblib is based on pickle which is the best serialization facility in Python. Numpy, SciPy, and Pandas support pickling. If a library does not support pickleable objects, it is an issue of the library not joblib.",1
fftgv14,ev1as7,"If you have a well defined tree, it will be deterministic, so depending on the structure, this could be conquered using a simple for loop or recursion. SKLearn allows you to save and load models via pickle, but provides no easy mechanism for loading a tree from outside. Really though, all you should need in the simplest form is a large stack of ifs and returns.",1
ffumz33,ev1as7,I don‚Äôt know if I understood your answer correctly. Do you mean by ‚Äûa large stack of ifs and returns‚Äú that I should hard code my tree?,1
ffuthk7,ev1as7,"PMML is like an XML or YAML format, so if you can read this into some data structure you could iterate over that with a loop or recursion. But if your tree is small or you don‚Äôt feel comfortable writing a loop like that, an if tree will do precisely the same thing.",1
ffww1x9,ev1as7,Thank you very much :),1
fdo92j7,em9fxf,"&gt; I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc. For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

This could be called a type of ensemble learning, but I wouldn't recommend this, especially to a beginner. Instead, for each model, you should look at model.score(Xtest, ytest). The higher the better. That allows you to choose just one model.

&gt; The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

That can happen. It's not really an error. But presumably the score() will be low, and you can reject that model.

&gt; MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

That can happen. It might indicate bad hyperparameters. For now, I would suggest to just reject that model.

&gt; I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Most people who talk about NFL don't have a clue, and you can ignore them. You don't need to test all parameters and methods, but it's good to test a few. When you know more, you'll start to understand which models and hyperparameters are relevant for you to test.

My recommendation is to follow any tutorial that walks through sklearn with a specific dataset, eg the Titanic dataset. Don't try to program anything on your own before doing this. I recommend Andrew Ng's ML course on Coursera.

Finally, I recommend r/MLQuestions and r/learnmachinelearning rather than r/scikit_learn.",2
fdq8sjz,em9fxf,"Thank you for your gold advices. Really appreciated. I'll check accuracy score, pick the best one and follow all materials out there to improve my ML journey. Thanks",1
fc0d721,effb98,"In a famous 1996 paper, David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. ... There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
by Aur√©lien G√©ron",3
fc0f7vz,effb98,"Thank you. Yeah I have checked the book by Aurelien Geron, but can you still guide me where will I find exactly what Im looking for?",0
fc07b6i,effb98,"For algorithms try LinearRegression/Ridge (the easiest to interperate/know how they work), RandomForestRegressor or if you don't mind another package, Xgboost (which will most likely perform the best or have similar performance to the random forest, but is difficult to interperate)

Your data seems to have 1 or more rows for 1 patient, all these algorithms i proposed can only take in one row to predict some value. This means you will need to pre process/prepare the data to work well with these algorithms. I would recommend the use of aggregate functions (count of past visits, average wait times, etc.) to convert these many rows into 1. You could also lag your data so you have columns like 'time last visit', 'time of second to last visit' etc. The day of week and time of day could also make for some interesting features for you to play with :)

To train the model you will use the fit(X, y) argument once you declare your model. X is the features (time of visit, day of week, time since last visit, disease, ...) and y is what you predict (time of action).

You will also want to split your dataset into a training and test dataset. The train is what you fit the algorithm with and the test is used to see if the algorithm will actually work on new unseen data.

This is quite a brief high level overview of everything you will need to do, i also recommend tackling the titanic and housing prices datasets on kaggle.com as there are some fantastic solutions for you to learn from which is what started me off with machine learning :)

Hope this is helpful to you",1
fc0f3we,effb98,"Wow, that is a very detailed explanation. Thank you so much!! I wish i could give you a medal. 

Btw, how many features do you think I should include?",2
fc0vp3d,effb98,"Haha, no need for a medal, I'm happy enough to try and get someone into ML :)

Unfortunately there isn't really a fixed number of features that's guaranteed to work, and different algorithms might work better with certain combinations (e.g. The regression models don't work well with highly correlated features, so you will most likely need to remove them before modeling). I recommend having too many to start with (it's common to see datasets with 10-1000 features) and if needed, apply some sort of feature selection/regularization techniques to reduce the number used in the model :)

You'll never know if a feature is any good unless you try it!",1
fc1qa5g,effb98,"You should probably look into modeling this using a Poisson distribution, this is a pretty well studied problem in statistics. Here's an introductory blog post: [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459](https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459)",1
f8dxf0w,e0h8ao,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askprogramming] [Column transformer throwing away some features?](https://www.reddit.com/r/AskProgramming/comments/e0hbkw/column_transformer_throwing_away_some_features/)

&amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1
f8516p7,dyzwn9,"Hi, a few things. You should be using a classifier ‚ÄúMLP Classifier‚Äù to do a prediction that will give you a confidence as output. Unless you‚Äôre trying to predict the amount of rain the next day, in which case ~80% R2 would be excellent.

You are able to pass deep network shapes into your model, using the ‚Äúhidden_layer_sizes‚Äù argument. Similarly you can change ReLU to be the TanH or Sigmoid functions and the solver for how weight and bias changes are calculated. That is the extent of basic MLP tuning available, but is pretty robust for academic projects like this.

I *highly suggest* trying other modes like the Naive Bayes, SVM, Boosted Trees, Random Forest, and Logistic Regression. These also have large spaces of hyper parameters to tune and cross validate over. 

If you really want custom written network functions, you‚Äôre probably going to need to go the TensorFlow route which is much lower level.",1
f6xomto,dtnh3f,"I'm pretty new to machine learning and scikit-learn, but not a complete beginner. For a school assignment, I'm trying to optimise the hyperparameter alpha of ridge regression by cross validation using 30% of data as the holdout set.

Standard stuff, I've done this before. First I consistently got alpha=0 as my best alpha. Ok, the model is not overfitting, least squares solution is best. However, as seen in the image, my group then noticed that alpha=-85 gives the best performance on the test set.I am utterly confused. Ridge regression minimizes the cost function

||y - Xw||\^2 + alpha \* ||w||\^2,

so a negative alpha should lead to weights that are artifically inflated weight above the optimal least squares weights? So how come these weight give a better prediction on the test set, as given by mean square error as the metric?

I am completely confused. First I thought there was something wrong with my CV scheme, so I reduced the code to the minimum moving parts, using scikit-learn methods. Still, the problem persists, with several different random seeds. Code below:

    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=0.3)
    model=Ridge(alpha=0, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=0, mse:"")
    print(mse)
    model=Ridge(alpha=-85, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=-85, mse:"")
    print(mse)

This outputs:

     Alpha=0, mse:
    
     12.49668153434439
    
     Alpha=-85, mse:
    
     12.487210630035198

Anyone know what is going on here? At this point, I am seriously considering just implementing ridge regression from scratch, and seeing if the problem persists...

EDIT:

I implemented Ridge regression from scratch, without external libraries.

    def RidgeOwn(X, y, alpha):
        holder1=(np.dot(X.T, X))
        holder2=holder1+np.dot(alpha,np.identity(holder1.shape[0]))
        holder2=np.linalg.inv(holder2)
        holder3=np.dot(holder2, X.T)
        weights=np.dot(holder3, y)
        return weights

The same problem persists, the optimal alpha is now -24. No idea what is happening. There must be something very wrong with my input data, right? But I cant imagine what would cause this...",1
f6yc9kc,dtnh3f,"Antiregularization is a thing and can happen quite often, especially if you feature engineered using PCA or otherwise did other sorts of regularization. 

This gives a much more in depth look: https://stats.stackexchange.com/questions/328630/is-ridge-regression-useless-in-high-dimensions-n-ll-p-how-can-ols-fail-to",2
f6yzky6,dtnh3f,"Hmm... I read the link, fascinating. But I do not think everything is working as intended. No PCA, other regularisation or preprocessing has been done. And I very much doubt negative alpha is intented to be correct on an introductory course like this...
The question remains, what could be causing this... Must be something strange with the input?",1
f6z6tle,dtnh3f,"I can only speculate without running the analysis on my own machine, but your code looks correct at a glance. My only suggestions would to be to confirm you have no type errors, run some k-fold experiments iterating up the alpha, and try a lasso or other regularized OLS and see if the results are similar. It could simply be correct as it is rare you would even consider a negative L2 alpha, and your professor may even have bounded their hyper parameter search space at 0 as I would the majority of the time.",1
f6xn6gq,dtiy98,"Shuffle Split: This function create infinite iterations of your data where the test and train are randomly assigned at each iteration. Therefore, you can have a point that is repeated in testing or repeated in training. This could cause issues in ensuring you have a proper validation score if certain classes are over or under represented.

K-Fold: This function will shuffle your data, then draw boundaries every len(data)/(k+1) observations. You then use one of the k+1 resulting folds as a validation set and train on the rest.

That is, a shuffle split with a 20% test proportion will generate infinitely many randomly split 80/20 train/test buckets. A K=4 fold split will leave you with 5 buckets, of which you treat one as your 20% validation and iterate through 5 times to get a generalized score.

If you are doing classification with imbalanced classes, a stratified version of both exists which maintains the distribution of your response variable amongst the folds and splits.

Generally, K folds is seen as the proper method as you prevent any bias from random sampling.

Specifically to your question, you must never use the test set from a different train set in shuffle split because you are very likely to have the same data in your train and test which is a huge information leakage and invalidates any model performance metric.",1
f6yev8r,dtiy98,Thanks alot for your thoroughly explained comment. That was helpful.,1
f1x7n0w,dar9z6,"Depending on how many different commands you want to look at (and it‚Äôs a little unclear why SKLearn would be your package of choice rather than just rote statistics, correlation matrices, etc. ) you could encode the categories using a (I forget precisely the name but) OneHotEncoder or LabelEncoder. This would give you the ability to run multiclass classification algorithms.",2
f1yw2xd,dar9z6,"aha, I have multiple protocols and whole lot of commands... we may be talking about 50-60 major commands with most of usage and same amount of commands with less usage for like 5-6 thousand times a month.

I want this to ultimately be used as a way so we can identify bots and crawlers and stuff. im completely new to this and have been a back-end developer. now I want to improve on my reports of usage and gather more useful information. like some bot is hitting these types of commands in that interval, so this new thing working on same interval and commands with marginal differences, so this might be of the same family? and things like that. I don't even know what should be used for this!",2
f22pmhw,dar9z6,"If I am understanding correctly, this is an unsupervised problem (that you do not have a training set with what would be the correct predictions). In this case, you want to look into ""unsupervised learning"" and ""clustering"". This may give you the ability to segment your larger dataset into one that could potential give you information about different heterogenous groups in your data, but this hinges on you having more data than just a text command.

In this case though, unfortunately I would expect that it will be difficult to do what you want. A first step would be identifying existing traits of bots (i.e. tons of commands faster than a human could send from the same ip, etc.) and attempt to conquer the problem that way. To my knowledge SKLearn does not have many good ways to deal with this type of problem other than LabelPropogation but these are advanced techniques. You may have luck with programatic solutions more than machine learning. Good luck! Please feel free to message if you have questions.",2
ezsq8kj,d244x4,Post very short code.,2
excka8y,cs2urp,"You can just add the two arrays after one-hot encoding each separately.

I never heard of two-hot, that's interesting! Could you describe the situation more.

I guess the levels are the same for the two variables?",1
exctsp1,cs2urp,"Thank you.

To give you a better idea of what I'm dealing with, the data frame describes housing data; it's just one of the tutorial data sets on Kaggle. There are two categorical features to describe sections of the basement. They have the same categories, ""unfinished"", ""good living quarters"", etc, and the order does not matter. I could just one-hot encode both of them, but that would result in twice as many columns as I need for these features. For this example, it doesn't really matter, but I could see how knowing the best way to implement two-hot encoding could be useful in the future.

With one-hot encoding, a row of two features with five categories would look like this: 

[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]
while two-hot encoding would look like this: 

[0, 1, 1, 0, 0]",1
exea82m,cs2urp,"Thanks!

I suggested using +which could give the value 2 in some cases (so now ternary variables). If you do want binary you would combine with or. This discards information, potentially.",1
ewdbblx,cnnacy,RFE removes features based on the coefficient values.,1
ewermxj,cnl2a5,"I can‚Äôt remember what the output is for the kmeans cluster centers, but your slicing of ‚Äúcenters‚Äù may be referencing data with a numerical index or something.",2
ew3ud66,cmmbi5,"I‚Äôve seen bizarre slowdowns when intel MKL fights with other multiprocessing methods, basically making way more threads than can be efficiently processed. The severity of the problem seems to be OS and python installation dependent. Try shutting off MKL mutithreading by setting the environment variable ‚ÄúMKL_NUM_THREADS‚Äù to 1.",1
ew3vm3l,cmmbi5,I use openblas,1
evycli4,clpubv,by googling,1
evyddn0,clpubv,"search for Anomaly Detection in Network Traffic 


https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf",1
evydlnu,clpubv,"https://paperswithcode.com/paper/network-traffic-anomaly-detection-using 

here code with paper
using dl",1
evulpwt,cl88rf,Make sure your python build is the anaconda version and the sublime python path points to to Anaconda3 dir (JSON user settings of Sublime Text 3),1
ew0qyin,cl88rf,If nothing else works. Uninstall everything and install a clean version of Anaconda distribution and point Sublime to the proper path.,1
ew0xmus,cl88rf,I just restarted the whole computer but thanks for help,1
etq1qkx,cc2mxr,"if by numpy you mean pandas, then

&gt; X = dataframe.values[1:]  # features
&gt;
&gt; y = dataframe.values[0]  # target
 
should do the trick",1
etumb5d,cc2mxr,Thanks. So basic I'm almost embarrassed for asking.,2
etj4faq,cbm4g6,"What defines those data points (dots)? Is it just 2d location (x, y)? How did you label the original data points as blue or red?

&amp;#x200B;

If they are linearly separable (you could draw a straight line between them) then a linear SVM is probably your best bet. Still, it would be good to know more about the actual problem.",1
etrdap9,cbm4g6,"A point difines a line with its rho and theta (so 2D). There are left lines (red points) in an area of the graph, and right lines (blue points) in another part of the graph. So the blue and red points are clearly linearly separable, but this could be interesting if I'd like to only separate the blue form the red.  I want my application to be able to say ""this new point determines a left line"" or a right line, or none.",1
erxuz4x,c4q5i6,"Try
` from sklearn.cluster import kmeans `

I think the module is cluster and the class is kmeans.",1
erzcvj2,c4q5i6,"I finally successed. But in a way I've not expected:  


import  sklearn.cluster.k\_means\_  as kmean

&amp;#x200B;

kmeans = kmean.KMeans()  


Why should I do it in such indirect way...",1
eqko7d2,bylpjd,"Just use 

model_name.fit(w_cancelled_data_X, w_cancelled_data_Y)

Then for your active contracts:

model_name.predict(active_data_X)",1
eqkq7gb,bylpjd,"Wow! That did it. I was making this way more complicated than I needed to.   I really appreciate the help. Out of curiosity, do you have any good suggestions on how to export the full predictions to a csv? Thanks for the help again. I truly appreciate it.",2
eqkrf7t,bylpjd,"No problem. If you are familiar with the pandas library, I‚Äôd convert active_data to a DataFrame: 

import pandas as pd

active_data_X = pd.DataFrame(active_data) 

‚Äîside note: there‚Äôs a method that reads csv files and converts them to DataFrames, which goes as follows: active_data_X = pd.read_csv(‚Äúpath/filename.csv‚Äù)  ‚Äî

And then run:

active_data_X[‚Äòprediction‚Äô] = model_name.fit(active_data_X)

And then:

active_data_X.to_csv(‚Äúresults.csv‚Äù)

Edit: added the close ‚Äú",1
eqo01ik,bylpjd,I didn't even realise that `sklearn` worked with `DataFrame`s. TIL!,2
eo9wn6b,bqtxes,"&gt; `cluster_centers_` : array, [n_clusters, n_features]
Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these will not be consistent with labels_.

&gt; init : {‚Äòk-means++‚Äô, ‚Äòrandom‚Äô or an ndarray}
Method for initialization, defaults to ‚Äòk-means++‚Äô:
‚Äòk-means++‚Äô : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.
‚Äòrandom‚Äô: choose k observations (rows) at random from data for the initial centroids.
**If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.**",1
eobky9y,bqtxes,missed that.. thanks!,1
eoace2y,bqtxes,"You can save the cluster centers as a .npy file with np.save('filename', kmeans\_obj.	
cluster\_centers\_)

Then you can load with np.load('filename')",1
ela96qt,bevq9d,hmm maybe divide the image into pieces and run each thread for blob detection?,1
ekuebnz,bcwbv4,Bootstapping the models will get you the variance of the accuracy.,1
ekwf5jw,bcwbv4,How can I do that?,1
elc8rti,bcwbv4,"Thanks all got it by looping on bootstrap samples.
Thnaks",1
ekw9ipq,bcwbv4,"It's a binomial distribution, therefore if the accuracy is p, the variance is n*p*(1-p), where n is the size of the set used to calculate the accuracy.",1
ekrkuj8,bcbduy,"RFs in sklearn produce probabilities of labels, which can be used to calculate ROCs which in turn can let you ""tune"" the probability to minimise false positives.",1
ekmvywi,bbxi9t,"Just record the cluster means. Then when new data comes in, compare it to each mean and put it in the one with the closest mean.",2
ekp91di,bbxi9t,"Ah, yes, that will work, thanks!",1
eiy0oo4,b36h5a,U mean they are just different objects or oredict differently too?. What about feature importances in both?,1
eiy0uzg,b36h5a,"Sorry I didn't get your question ?. I mean a random forest with the conditions mentioned in my question should create exactly same trees ( estimators ), while training. What do you think about that ?.",1
eiy1c8r,b36h5a,It should. What i meant was how are u differentiating both the eatimators. On what basis?,1
eiy1x70,b36h5a,"I create a random forest (scikitlearn) with number of estimators 10(This with create 10 different decison trees). I can access each of theses estimators(dection trees).  

Code Example

model = RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019)¬†

Different estimtors/ decision trees can be accessed by 
model.estimators_[2] , model.estimators_[5] etc.


Should model.estimators_[1] ,model.estimators_[2] and model.estimators_[5] be same ?",1
eiy3cvm,b36h5a,They both will be different instances internally both might have the same structure. I have not really analysed the eatimators before. U can try to predict a few examples with each estimator to check,1
eiy4t6e,b36h5a,You don't need to predict. You can just visualize the the individual trees. I did that and they are different,1
eiy4u16,b36h5a,That is strange. Can u share the notebook over git or something?,1
eizifmi,b36h5a,"Why would it be creating the same decision tree over and over again?  That's not how a random forest works, even when you don't bootstrap and fix the seed.

If you created two forests with a fixed seed and the same parameters, they would be the same.",1
ehtkjb6,axgj2c,"great idea!

think you should add number and scale of factor vars, can greatly impact runtime 

&amp;#x200B;

also the amount of duplicate columns

&amp;#x200B;

i like it though... make it for r

&amp;#x200B;",1
ehuex1m,axgj2c,"Thank you for the feedback u/weightsandbayes we really appreciate it!  

Adding the variance was definitely something we were thinking about. I think this would a good avenue to explore, we should give it a try and I agree for many algos variance definitely plays a role. 

I haven‚Äôt used R in quite a while, what library should we tackle first in your opinion if we were to build a similar thing?",1
ehtqae2,axgj2c,"Fantastic idea. Could it be extended to other libraries? MLib, H2O, Keras, etc.",1
ehuev0d,axgj2c,"u/dj_ski_mask thanks for asking and you raise a great point.  
We built our library in a very scalable way, for example adding support for a new scikit learn algo is as simple as updating the config Json and running the model estimator.  
Adding a new algorithm here: [https://github.com/nathan-toubiana/scitime/blob/master/scitime/\_config.json](https://github.com/nathan-toubiana/scitime/blob/master/scitime/_config.json)   
And running the \_data function here: [https://github.com/nathan-toubiana/scitime#how-to-use-\_datapy-to-generate-data--fit-models](https://github.com/nathan-toubiana/scitime#how-to-use-_datapy-to-generate-data--fit-models)  


In principle nothing really prevents us from extending this to other libraries  
One challenge if we want to extend this outside Scikit-learn is that we are using scikit-learn specific methods throughout the code base.  
We would probably want to wrap our functions with a Library layer to specify what library we‚Äôre targeting. But It definitely can be done !",2
ehufxkc,axgj2c,By vars I meant variables haha ,1
ehuh90d,axgj2c,"Got it!

But by number of vars do you mean number of columns ? If so it's already factored in.

The distribution of each variable is also something we should look into.  
",1
ecs1ses,aahf76,"For gains, wouldn't it be simpler to just multiply by the gain factor?",2
ecs20oe,aahf76,"But then it will not be sign-conscious. If the gain is not a constant, but a function.",1
ect12sr,aahf76,"This sounds like it has nothing to do with sklearn.

Multiplication is typical especially for audio which I guess your signal might be - you could clarify.

If you know what you're doing then yes, you could use np.sign.",1
ect7y0q,aahf76,"Well not directly obviously, but this could be a common use-case in sklearn. Particularly I'm trying to apply a ""windowing function"" (not really a window, but similar principle) to the signal in such way that the window is slightly different for the y- than the y+ part. Thus for y &lt; 0 I want to apply window1, but for y &gt; 0 window2.",1
ec1cetm,a75oid,"You're not doing anything wrong. These metrics include the number of predictions of a class in the denominator so they divide by zero in this case. The NN, for some of your workloads, just never predicts that class. You could ensure that there are plenty of examples of that class in the training set. Apart from that all you can do is choose to report F and precision only when there are plenty of samples in the test set.",1
ec1d4g8,a75oid,"I wonder, why does it change though? As if MLPClassifier() fits a different fit every time I run the program. Even when it uses the same params? Yes, since MLPCLassifier() is implemented using stochastic gradient? But then, if I get ""errored results"" and ""non-errored results"", then are both valid? Or should I discard results that give this problem? The difference that occurs in prediction accuracy, when the error occurs, is quite drastic. 0.85 vs \~0.65 or even \~0.45, when this error pops up. So it ""seems"" that the MLPClassifier somehow fails occasionally, on this data set.",1
ec1gpdh,a75oid,"This is why we often report a cross-validated value, not just a single value. Yes, it could be that the classifier just fails sometimes. You can try different architectures and hyper parameters, especially initialisation and optimizer to see if it becomes more reliable, or try collecting more data.",1
ec1gv1e,a75oid,"What are you referring to with cross-validation? You mean that one ought to cross\_validate on the model, rather than fit the model a single time?",1
ec1ho19,a75oid,Yes,1
ec1hpud,a75oid,"But what does this help? If a cross\_validate ""fold"" produces the error, then it will be reflected to the averages of that cross\_validate? So even then one'd need to perhaps look for ""clean runs of cross\_validate""?",1
ec1u38k,a75oid,"It helps only in that if we report an accuracy value, it's an honest one (with error bounds if we like). It doesn't help to avoid the runs that go bad - for that see my earlier answer.",1
ec0d5su,a753f2,"Solution: I was accidentally using the MLPRegressor() class, when I need to use MLPClassifier() to get the output as multiclass.",1
ec1gxk7,a746h0,"The best way to estimate is to try running with a very small proportion of your data, say 0.00001, and then increasing by powers of 10 and seeing how it scales.

If you're seeing 10% CPU in a long-running computation, it could be because the job is disk-bound. Alternatively if you have 8 or 12 cores, and sklearn is using 1 and Task Manager reports it as a percentage of the total.",2
egyudwg,a73oda,"The C value, penalty, and random_state are the only ones I adjust. Random state is just stay consistent so really only the C value and penalty.",1
eagk9z2,a0c8dw,"It's more of an ML question than an SKLearn one. Training MLPs is still kind of a black art.

You can certainly try a different optimizer (I see they have LBGFS, Adam, SGD), momentum on/off, different learning rate, shuffle on. Also if you changed the default values for tol or anything else, try going back to the default. Apart from that, it might be worthwhile normalising your data columnwise (or it might not).",1
eagl4du,a0at1q,"Yes. You can check this out by creating and fitting the MLP and then looking at the `coefs_` attribute. First of all it doesn't crash! And second the `coefs_` are the same shape you'd expect with (7,).

It doesn't seem to be documented, so if there isn't a ticket in Github, you could raise one, either to remove this behaviour or to document it.",1
eahj20o,a09ukl,"train_test_split takes X and y, not Xy",1
eagw9zj,a06iin,"The amount of parameters you're attempting to optimise over is 10*2*11*3*3*2 = 3960. 

This isn't necessarily an issue straight up, but if your dataset is large and you are running a lot of decision trees with several tweaks to their parameters it's going to take some time.

You can try to scale back the parameters you're optimising over (bootstrap = false for example is one that can go...) as a start.",2
eahhbwl,a06iin,Thanks; Yeah I can try reducing the parameter matrix. so far I've reduced the cv and iterations,1
eahlc2t,a06iin,"I'd suggest lowering the n_estimators.

You mentioned this is a dataset that had an associated paper. What parameters did they use?",2
eai03lt,a06iin,"I tried lowering the n\_iter as much as possible and reducing cv but that didn't help. The paper I'm following didn't list parameters, and also they used WEKA not scikit learn so it doesn't really carry over",2
eajcy7s,a06iin,"&gt;I tried lowering the n\_iter as much as possible and reducing cv but that didn't help.

I ran your code using the MNIST and it ran fine, so that's not the problem. What's the size of the dataset your working with? Is your system actually using resources while running or has it hung?

&gt;The paper I'm following didn't list parameters

That's just bad research.

&gt;and also they used WEKA not scikit learn so it doesn't really carry over

The parameters are named differently and use different defaults, but you'd be able to replicate in sklearn... if they gave their parameter values... ;)

&amp;#x200B;",2
eajybfy,a06iin,"thank you for checking; I got a friend to run my code on his computer and it finished in like 1 min. So I think my laptop is just crappy, or I need to update something or reinstall it :P the size of the data set was small, like 131x 22 so I don't know what the problem is. I guess its just bad hardware",2
eak503r,a06iin,Seems like. :( ,1
ea2as8t,9yhcqq,"Do you mean having sklearn calculate coefficient p values, determine goodness of fit or out of sample validation?",2
ea3f85a,9yhcqq,"Whatever is meant when one says ""testing the model"".",0
ea5jij5,9yhcqq,"Hence my question. There are several different ""tests of a model"", ranging from ""which variables are related to the prediction"" to ""can the model's prediction be applied to new data"". 

Understanding what you want is half the battle.",1
ea85xvi,9yhcqq,"I'd imagine that library developers would develop a set of tests tied to any particular method. So if I use LinearRegression(), then somewhere near there are at least ""common tests"" for LinearRegression(). Also, since not all tests are suitable for all models, that should dictate how a library is designed.",1
ea87yte,9yhcqq,Have fun then!,1
ea1t6zy,9yhcqq,"Yes, have you looked at the manual? It's great.",0
ea1yi0j,9yhcqq,&gt;[https://scikit-learn.org/stable/auto\_examples/linear\_model/plot\_ols.html](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html) ?,1
ea287y4,9ygvsi,"The model is trained ""in-place"". Even when you call .fit(), you don't have to assign the model back to the variable you instantiated it in. Internally it just adjusts all the self.coeffs_  and self.intercept_ and all other model.params without creating a new model to hold them.",1
e9ve9uw,9sbqvm,"You can use an IDE like PyCharm, set up a break point in the \`fit\` method and then use the tools to step through the code one line at a time. You can set up different variables to watch and see them change.",1
dyirkg8,8emc3b,"I think there is not a definitive answer, but you can try with the following approaches:

1. You can just append the PageRank as another feature;
2. you can use the bag of words matrix to classify and then, use the classification prediction as an input for another model (which has PageRank as a feature);
3. You can classify both separately and use both predictions as inputs for a third classification model.

(2 and 3 are also called ensemble methods, look on the web for stacking).

Regards.


",2
dxnqyib,81xksg,"Use TFidfVectorizer instead, it has a mindf and maxdf args for that tuning. Just initialize mindf =3 to ignore terms that appear in less than 3 docs",1
dxo9maz,81xksg,Oh thank you very much! You can't imagine how many times I've read [the documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and I've always missed those parameters.,1
dxoduia,81xksg,You are very welcome ,1
dun5ic5,7z4qi7,"The first time you fit the model you had 4 possible classes. [1, 2, 3, 4] But when you called partial fit, you had the label for the new point be 5. Standard classifiers are not able to accommodate adding new possible classes after the initial setup.",1
dunbold,7z4qi7,[deleted],1
dunbtit,7z4qi7,"See this changed code, it works fine without fit() 
     
     from sklearn import neighbors, linear_model
     import numpy as np
  
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
        X.append([6, 9])
        y.append(5)
    
        X.append([10, 50])
        y.append(6)
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""log"")
    
        #sgd_clf.fit(X, y)
    
        #print(sgd_clf.predict([[6, 9]]))
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here",1
dunbvpo,7z4qi7,I understand i need to give class labels before initial fit().  But then fit uses all the new data and generates a model from scratch . what is the purpose of partial_fit then ? ,1
dviwuxk,7z4qi7,I found a work around by taking some random classes like np.arange(100) in first partial_fit(),1
dtvqk2p,7vw4ap,"Consider building a neural net, e.g. with [Keras](https://keras.io/). Since neural networks are basically trained via batch processing, they can be [tuned with new observation data](https://stats.stackexchange.com/questions/220169/how-do-i-add-more-training-data-to-pre-trained-deep-learning-net). Check out [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) for a good introduction.",1
dtxahq1,7vw4ap,"Thanks brylie, thats a wonderful book :) Could you please let me know if i can use scikit knn model along with keras ? Like i train with scikit knn model and just use batch training feature of keras. Is this possible ? I will read the book in detail today. Thanks again :) ",1
dtxbssd,7vw4ap,"Keras is much more comprehensive for deep learning than scikit-learn neural network. You might be able to create a hybrid model with KNN and a neural network. However, consider starting with one or the other, for simplicity.",1
dtxcf6f,7vw4ap,"which one can be trained fast ? I see that neural networks work slower. In that case, will scikit incremental learning would be a better fit ?
 http://scikit-learn.org/stable/modules/scaling_strategies.html

When exactly i need to use keras vs scikit neural networks if fast response back is my criteria than accuracy ? 

",1
dtxq4s6,7vw4ap,"I think there are at least two types of 'response time' to consider:
- training - where the model learns from provided data
- inference - where the model classifies or predicts based on new data

Classification time will likely be similar between models. Training time may vary greatly, but there are ways to speed up training in some cases. To which do you refer to when you say 'fast response back'?",1
dtzdf7h,7vw4ap,"Hi brylie, For training. If i get a newly registered user, i want to add his data to the already existing trained dataset and perform the inference step.

Currently, Just because of one/two users, i am training the whole dataset which is taking lot of time to go to inference step. 
",1
dtzepe9,7vw4ap,"Back to your original question, I am not sure it is possible to re-train a kNN classifier by adding new observations to an existing model. It is likely you will need to train a new model instance using all of the data (or a train/test split approach).

However, you can [take an existing Keras model and run its `fit()` method on new data](https://github.com/keras-team/keras/issues/1868#issuecomment-272078441), which will update the existing model. 

Keras may save time in the long run since you can update the model. There also ways to  speed up the Keras training, e.g. by using a GPU.",2
du04ux2,7vw4ap,thank u brylie :),1
du3dv06,7vw4ap,"Brylie, if i create a keras model, can i still be able to get the closest matches for my test data from trained dataset ? 

For example, with knn i have flexibility to get the closest matches to the test data with kneighbours function. Can this be possible with keras ?

Can i train a scikit model and then pass this trained model to keras ? 

 Sorry if my questions are naive. i am learning machine learning",1
dn4vbad,70m5l1,Check out the SciPy Cookbook revipe foe [Matplotlib: django](http://scipy-cookbook.readthedocs.io/items/Matplotlib_Django.html).,1
dn4vqr7,70m5l1,You might also like to [use Bokeh charts in a django project](https://www.hackerearth.com/practice/notes/bokeh-interactive-visualization-library-use-graph-with-django-template/).,1
dn4vx8g,70m5l1,See also: [django-chartflo](https://github.com/synw/django-chartflo),1
g8nvhog,j9q6bp,"If you are imagining an expert system (if/else decision tree) then try the `experta` package. If you need it to comply with the sklearn regressor or classifier API, then you can create a class with `__init__()`, `fit()` and `predict()` methods.

* `__init__` : instantiate your custom experta `KnowledgeEngine` instance.
* `fit` : calculate any stats on the dataset that you might need and adjust the parameters within the engine ( can just pass for most problems)
* `predict` : use `self.engine.declare()` to intake the state/feature data and return the output of engine.run() which must be a numpy array to work in an sklearn Pipeline",1
g8f2vm9,j8vepi,"The hyperparameters tol and max_inter are generally used to tell the model when to stop it's optimization for fitting the parameters of the logistic regression. Generally, tuning these parameters won't make a big difference to the predictive power of your model.

The penalty and C parameter deal with regularisation. This is a concept in ML used to prevent overfitting of your model. Overfitting happens when your model performs poorly on unseen/test data compared to your training data. This usually happens when your model is too complex (perfectly tuned models are flexible enough to learn something from your data but not too flexible to overfit/memorize your training data).

The penalty parameter lets you choose what type of regularisation you want to apply. There are two types of regularisation L1 (lasso) and L2 (ridge). L1 regularisation forces some of your parameters for the unimportant features to zero (these features are dropped from the model- LASSO does automated feature selection). L2 regularisation forces some of your parameters for the unimportant features to zero but not exactly 0 (these features are not dropped from the model). Elastic net is a combination of L1 and L2 regularisation.

The C parameter lets you choose how much regularisation you want to apply. In the case of L1 regularisation, more of it means more features dropped from the model. If you drop enough unimportant features from your model, your model will not overfit. If you drop too many features you may lose some important information to learn from your data.",1
g8mmeq2,j8vepi,I love this response. Thank you so much.,1
g7u6e66,j5pcm6,Try installing with Anaconda,1
g7waffb,j5pcm6,"So what I had to do:

\&gt;conda uninstall scikit-learn numpy scipy

\&gt;conda remove --force scikit-learn numpy scipy

\&gt;pip uninstall scikit-learn numpy scipy

\&gt;pip install -U scikit-learn numpy scipy --user",1
g785qxo,j2v11w,"If you looked at the source code you would have noticed the ""algorithm"" kwarg which specifies that if you don't choose the ""brute"" option it uses a tree data structure. This may introduce slight discrepancies between the fit model and the ""true"" distribution of the data.",4
g79d2mb,j2v11w,"Thanks for your prompt response, I got it overnight.

I agree. I like the speed of the default model, I‚Äôll time it but I reckon it‚Äôs there to be quicker than brute.

I suppose I am then doing my own little brute method with the literal Euclidean nearest neighbour.  The combination works for my purpose.

Nice one, thanks.

Edit: two thoughts.  First, my data is very entropic so maybe trees ain‚Äôt so good.  Second, my code still might be wrong so brute is a good check that I haven‚Äôt screwed up my trig.",1
g7a2j0z,j2v11w,"If there are ties (several neighbors with the same score) then scikit chooses the first one.

A trivial example to show how it works

    x = np.random.randint(0, 5, (20, 1))
    y = np.arange(len(x))

    knn = KNeighborsClassifier(1)
    knn.fit(x, y)
    knn.predict(x)",2
g7b8lel,j2v11w,"Good point.  It‚Äôs not that, I removed duplicates for my purposes.  Thanks though.  üôè",1
g7dd7oa,j2v11w,"Oops!  My method of removing duplicates just inherits the same behaviour.  

I‚Äôm going to add fuzz/noise to them that that will work for my purpose.  

Cheers for the stimulus üíì",1
g70358r,j1idtx,"Without knowing the kind of system you're running this on (RAM, amount of cores etc, etc) or the hyperparameters you're trying to optimise on, you're generating 300 trees. That could take a bit. 

Try decreasing the number of trees (say, 2?). If it's still hanging, message back and we can try to troubleshoot.",1
g2w82j9,igtkry,"Nope. Sklearn doesn't return F / t / p values. You can code your own, but otherwise just use statsmodels.",2
g0b6kn2,i3546z,"Not sklearn specifically, but graph theory could be useful.",1
fyn0442,hu6y83,The clusters and centroids returned are the final results.,2
fynnk7i,hu6y83,Gotcha. Me and a friend had kind of come to a realization that might be the case for something we are working on but we weren't too sure. Thanks for answering!,2
g77yegr,ht4ol1,Hi.  I‚Äôve made AUC from small numbers of points. Treat them like polygons and literally calculate the rectangles and triangles?,1
fx4lpgh,hm6td7,"I think many ppl have got 100% on it, because when I implemented a simple CNN I was able to get 94%.

As for the type of model, CNNs are usually quite reliable for image classification. However, you can still get an accuracy of 85+ using just Dense and Flatten layers without Convolutions and Poolings",2
fx71pfp,hm6td7,"Thanks. Do you have any links where it shows people getting 100%?

I was given this link where it shows the best performance is around 96-97%:
https://paperswithcode.com/sota/image-classification-on-fashion-mnist",1
fv3gbqa,hakk07,suggest to use seaborn with lmplot(),1
fv3wp7z,hakk07,Convert language frame using one hot encoder,1
fvoi9f5,h0w3xx,"I don't have a background in math/science either... That said, check out stat quest youtube videos. Yes, they do include math, but honestly, its really not possible to grasp these algorithms without at least some math and logic involved. So if you want to really understand these, then you'll be required to understand things like derivates and alpha, etc.. Good luck!",1
ftgob55,gziaus,"Remember that k-means is not a classification algorithm. It only does clustering. Also note that 53 + 47 = 67 + 33 = 100.

Now can you explain what's going on?",2
ftgxkkr,gziaus,"Yes. I thought about that. It looks like it was just comparing original and predicted labels side by side and based on their order of occurrence only two possible result was produced, N and 100-N. Then how to judge the accuracy of the algorithm? How to get the cluster quality details?",1
fqwpb6e,glb4cy,"[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) is a data preprocessing technique, not a model fitting technique. The questions you're asking make me think that you want to fit a polynomial model to your data, that's *not* what this Class is for.",2
frim32i,glb4cy,"Got is, thanks!",1
fpj3n47,gcvtsm,"You'll probably have to implement your own [estimator](https://scikit-learn.org/stable/developers/develop.html). Then you can just pass that to your search function.

Note that stepwise feature selection is generally poor and should be avoided (see e.g. [here](https://redd.it/ehmi22)).",1
fmtws0m,fx6kdy,"Just from experience that‚Äôs a little high for a distance metric based classifier. Generally there will tend to be some on borders between classifications that will flip flop based on the corpus of observations you have. If you share code we can check to make sure, but the best part of these types of fun datasets is finding surprising ways to get things to work.

I would suggest triple checking over fitting with a holdout set, but congrats on your good training!",1
fmu1rkf,fx6kdy,"Thank you, well actzally the code is quite short, so i share it here:

# Get data
from sklearn.datasets import load_digits()

dataset = load_digits()

X = dataset[""data""]

y = dataset[""target""]


# tsne
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, init=""pca"")

X_embedded = tsne.fit_transform(X)

# For viz
import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))

plt.scatter(x=X_embedded[:,0], y=X_embedded[:,1], c=y)

plt.show()

# Generating sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fitting
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV

knc_params = {""n_neighbors"":[3,5,7,15]}

knc = KNeighborsClassifier()

gs = GridSearchCV(knc, knc_params, cv=5)

gs.fit(X_train, y_train)

# Run model
pred = gs.predict(X_test)


from sklearn.metrics import classification_report

print(classification_report(y_test, pred))


from sklearn.metrics import confusion_matrix

cf = confusion_matrix(y_test, pred)



import seaborn as sns

sns.heatmap(cf, annot=True)



________



Thats it, i hope i dont have a made a typo, i am on the phone right now üòÖ
Any insights to whats ""wrong""?",1
fmvfmvr,fx6kdy,"I don't know, but I think you have to fit tsne only with train, and transform train and test without using the test dataset to train tsne. When you use tsne on X it has information that came from test. In that way you are trickering the algorithm using information you supposily don't know yet(test data)!",1
fmvsjcf,fx6kdy,"Thank you, you are right, thats a silly mistake from me. Sadly tsne does not have a transform method... So i guess thats it for now. But thank you very much, bow i know i should pay more attention l.",1
fm4sp5c,ft2pcp,Is this where the group  is set to -1 for outliers? Or do you mean to filter out part of the data before pushing the rest through DBScan?,1
fm7kg61,ft2pcp,"Yes -1 signifies noise. But I want it to not classify trunks, so that I can remove low vegetation, but not trunks. But I've not figured out how to get it work that way.",1
fm5r3pk,ft1kmb,"From OneHotEncoder docs:

&gt;	Parameters
categories‚Äòauto‚Äô or a list of array-like, default=‚Äôauto‚Äô
Categories (unique values) per feature:
‚Äòauto‚Äô : Determine categories automatically from the training data.
list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.

You passed a string, which would error out. Try passing a list of categories, or switch to auto.

As to your error message, that did not come from the above code, but similarly you need to read the docs and use ‚Äòcategories‚Äô",1
fm700ta,ft1kmb,"oh = OneHotEncoder(categories = X\[:, 3\])

X= oh.fit\_transform(X).toarray() 

gives out 

""too many indices"" error",1
fmd17bl,ft1kmb,"You need to read the docs. Please look at them and check for what it asks for, not the column but the categories in the ith column.",1
fm704im,ft1kmb,"oh = OneHotEncoder(categories = X\[3\])

X= oh.fit\_transform(X).toarray()

gives 1D array instead of 2D array",1
fl3s1gy,fm1oov,"I think what your code does is pass  cols_ordinal to an imputer, return it as colums, and in parallel pass cols_ordinal to ordinal encoder and return those as even more columns. So ordinal encoder does not get the imputed columns! For that you need to pipeline imputer end encoder, and pass them to columntransformer as one pipeline.",1
fkyov8e,flg6a1,"Well, you just have to follow the code. There isn't one place. Eg for this metric, you'll see that the variable `self.effective_metric_` is created. Use Ctrl-F to see that eventually it's passed in to a `BallTree` as the `metric` parameter. So, open `_ball_tree.py`, and repeat.",2
fl6ns3e,flg6a1,"If you‚Äôre lucky a paper is cited in a comment. If you‚Äôre luckier the code follows the paper.
Or translate the code yourself, test important chunks of it to be sure.",1
fk1xbio,ffx9zw,"Hi, I don‚Äôt know what kind of output you got from the code you attached there, but here is the almost exact same example from the Tfidf transformer page:

&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)
&gt;&gt;&gt; print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
&gt;&gt;&gt; print(X.shape)
(4, 9)

Found at this link:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

And so it seems the Tfidf transformer SHOULD take in your corpus (a list of lists, where each inner list is a list of strings) and compute the corresponding IDF",2
fjxqzkb,ff9602,"I cannot verify this without jumping in and testing, which I can do in a bit. However, my suspicion is that since the Random Forest can take a multi-class scenario, there must be some sort of aggregation on the various class-level accuracy scores (or any other of the simple performance measures) and in this case it seems to be the arithmetic mean. So, in your case of only True False labeling, the mean of a single class accuracy score is that class accuracy score.",1
fk15zy7,ff9602,"Thank you very much, have you done some testing by chance?",1
fk2m6vq,ff9602,I believe the graph is a ‚ÄúROC‚Äù graph which compares true positives to false positives,1
fj4aa4k,fbfky4,"SKLearn dbscan accepts many pairwise distance functions, Euclidean, Manhattan, etc. So it can be a Euclidean function, if you want, but it is not by necessity. 

[DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Distance Metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances)",2
fj4mosa,fbfky4,"That's not what I asked. I asked about the parameter epsilon, if the neighborhood distance it is restricting for core/border membership is a euclidean distance. i.e. how would you choose epsilon on a fresh dataset?",0
fj4qu8h,fbfky4,"Simply put, asking how you select and tune hyper parameters is a very different question than if said hyper parameter is ‚ÄúEuclidean‚Äù. In this case, the metric is Euclidean because it is measuring Euclidean distance (by default). If you would like to select a hyper parameter of some function on Euclidean distance, you must tune it using the SKLearn libraries for this purpose, e.g. GridSearch. This will allow you to iteratively examine a space of hyper parameter combinations, and then select the best tuning. Lastly you can stack cross validation on each of these steps to increase the confidence you have in selecting the right tunings. This is more or less the core of the value add of a data scientist, and should be where most of the time is spent.",2
fj5yf2x,fbfky4,"Generally, most distance measurements, that I'm aware of in data science, are Euclidean, including the epsilon. Of course, you're free to choose Manhattan, other Minkowski distance, or any distance measure of your choosing.",1
fh5hpqs,f1dizs,"If I am understanding correctly, you want to define a function that accepts a time series and returns the lowest non-outlier value. This task really isn‚Äôt well suited to machine learning in the common sci kit learn sense. Specifically, this is because you do operations on the inputs and don‚Äôt necessarily return something from the set of inputs, rather either a class or continuous number.

If you want to make this current process more robust, you could winsorize your data at a couple standard deviations, then take the bottom. Alternatively take the second percentile.",1
fh5mtfh,f1dizs,"Thank you for that input, kind of what I was suspecting, but wasn‚Äôt entirely sure.",1
fgi8ngf,eylu4p,"Well, this blog post was originally posted on Neuraxio‚Äôs blog. It is very aggressive (or even toxic) marketing, I guess.",0
fgl9y7e,eylu4p,It has never been submitted to r/scikit_learn before.,1
fglaho0,eylu4p,"I know for sure that it was posted in /r/deeplearning earlier and this post was cross-posted there as well.

Also, I am convinced that some judgements about scikit-learn are merely too strong and others are controversal. For example, the blogpost says that joblib is bad because it is not able to serialize some objects. The funny fact is that joblib is based on pickle which is the best serialization facility in Python. Numpy, SciPy, and Pandas support pickling. If a library does not support pickleable objects, it is an issue of the library not joblib.",1
fftgv14,ev1as7,"If you have a well defined tree, it will be deterministic, so depending on the structure, this could be conquered using a simple for loop or recursion. SKLearn allows you to save and load models via pickle, but provides no easy mechanism for loading a tree from outside. Really though, all you should need in the simplest form is a large stack of ifs and returns.",1
ffumz33,ev1as7,I don‚Äôt know if I understood your answer correctly. Do you mean by ‚Äûa large stack of ifs and returns‚Äú that I should hard code my tree?,1
ffuthk7,ev1as7,"PMML is like an XML or YAML format, so if you can read this into some data structure you could iterate over that with a loop or recursion. But if your tree is small or you don‚Äôt feel comfortable writing a loop like that, an if tree will do precisely the same thing.",1
ffww1x9,ev1as7,Thank you very much :),1
fdo92j7,em9fxf,"&gt; I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc. For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

This could be called a type of ensemble learning, but I wouldn't recommend this, especially to a beginner. Instead, for each model, you should look at model.score(Xtest, ytest). The higher the better. That allows you to choose just one model.

&gt; The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

That can happen. It's not really an error. But presumably the score() will be low, and you can reject that model.

&gt; MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

That can happen. It might indicate bad hyperparameters. For now, I would suggest to just reject that model.

&gt; I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Most people who talk about NFL don't have a clue, and you can ignore them. You don't need to test all parameters and methods, but it's good to test a few. When you know more, you'll start to understand which models and hyperparameters are relevant for you to test.

My recommendation is to follow any tutorial that walks through sklearn with a specific dataset, eg the Titanic dataset. Don't try to program anything on your own before doing this. I recommend Andrew Ng's ML course on Coursera.

Finally, I recommend r/MLQuestions and r/learnmachinelearning rather than r/scikit_learn.",2
fdq8sjz,em9fxf,"Thank you for your gold advices. Really appreciated. I'll check accuracy score, pick the best one and follow all materials out there to improve my ML journey. Thanks",1
fc0d721,effb98,"In a famous 1996 paper, David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. ... There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
by Aur√©lien G√©ron",3
fc0f7vz,effb98,"Thank you. Yeah I have checked the book by Aurelien Geron, but can you still guide me where will I find exactly what Im looking for?",0
fc07b6i,effb98,"For algorithms try LinearRegression/Ridge (the easiest to interperate/know how they work), RandomForestRegressor or if you don't mind another package, Xgboost (which will most likely perform the best or have similar performance to the random forest, but is difficult to interperate)

Your data seems to have 1 or more rows for 1 patient, all these algorithms i proposed can only take in one row to predict some value. This means you will need to pre process/prepare the data to work well with these algorithms. I would recommend the use of aggregate functions (count of past visits, average wait times, etc.) to convert these many rows into 1. You could also lag your data so you have columns like 'time last visit', 'time of second to last visit' etc. The day of week and time of day could also make for some interesting features for you to play with :)

To train the model you will use the fit(X, y) argument once you declare your model. X is the features (time of visit, day of week, time since last visit, disease, ...) and y is what you predict (time of action).

You will also want to split your dataset into a training and test dataset. The train is what you fit the algorithm with and the test is used to see if the algorithm will actually work on new unseen data.

This is quite a brief high level overview of everything you will need to do, i also recommend tackling the titanic and housing prices datasets on kaggle.com as there are some fantastic solutions for you to learn from which is what started me off with machine learning :)

Hope this is helpful to you",1
fc0f3we,effb98,"Wow, that is a very detailed explanation. Thank you so much!! I wish i could give you a medal. 

Btw, how many features do you think I should include?",2
fc0vp3d,effb98,"Haha, no need for a medal, I'm happy enough to try and get someone into ML :)

Unfortunately there isn't really a fixed number of features that's guaranteed to work, and different algorithms might work better with certain combinations (e.g. The regression models don't work well with highly correlated features, so you will most likely need to remove them before modeling). I recommend having too many to start with (it's common to see datasets with 10-1000 features) and if needed, apply some sort of feature selection/regularization techniques to reduce the number used in the model :)

You'll never know if a feature is any good unless you try it!",1
fc1qa5g,effb98,"You should probably look into modeling this using a Poisson distribution, this is a pretty well studied problem in statistics. Here's an introductory blog post: [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459](https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459)",1
f8dxf0w,e0h8ao,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askprogramming] [Column transformer throwing away some features?](https://www.reddit.com/r/AskProgramming/comments/e0hbkw/column_transformer_throwing_away_some_features/)

&amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1
f8516p7,dyzwn9,"Hi, a few things. You should be using a classifier ‚ÄúMLP Classifier‚Äù to do a prediction that will give you a confidence as output. Unless you‚Äôre trying to predict the amount of rain the next day, in which case ~80% R2 would be excellent.

You are able to pass deep network shapes into your model, using the ‚Äúhidden_layer_sizes‚Äù argument. Similarly you can change ReLU to be the TanH or Sigmoid functions and the solver for how weight and bias changes are calculated. That is the extent of basic MLP tuning available, but is pretty robust for academic projects like this.

I *highly suggest* trying other modes like the Naive Bayes, SVM, Boosted Trees, Random Forest, and Logistic Regression. These also have large spaces of hyper parameters to tune and cross validate over. 

If you really want custom written network functions, you‚Äôre probably going to need to go the TensorFlow route which is much lower level.",1
f6xomto,dtnh3f,"I'm pretty new to machine learning and scikit-learn, but not a complete beginner. For a school assignment, I'm trying to optimise the hyperparameter alpha of ridge regression by cross validation using 30% of data as the holdout set.

Standard stuff, I've done this before. First I consistently got alpha=0 as my best alpha. Ok, the model is not overfitting, least squares solution is best. However, as seen in the image, my group then noticed that alpha=-85 gives the best performance on the test set.I am utterly confused. Ridge regression minimizes the cost function

||y - Xw||\^2 + alpha \* ||w||\^2,

so a negative alpha should lead to weights that are artifically inflated weight above the optimal least squares weights? So how come these weight give a better prediction on the test set, as given by mean square error as the metric?

I am completely confused. First I thought there was something wrong with my CV scheme, so I reduced the code to the minimum moving parts, using scikit-learn methods. Still, the problem persists, with several different random seeds. Code below:

    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=0.3)
    model=Ridge(alpha=0, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=0, mse:"")
    print(mse)
    model=Ridge(alpha=-85, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=-85, mse:"")
    print(mse)

This outputs:

     Alpha=0, mse:
    
     12.49668153434439
    
     Alpha=-85, mse:
    
     12.487210630035198

Anyone know what is going on here? At this point, I am seriously considering just implementing ridge regression from scratch, and seeing if the problem persists...

EDIT:

I implemented Ridge regression from scratch, without external libraries.

    def RidgeOwn(X, y, alpha):
        holder1=(np.dot(X.T, X))
        holder2=holder1+np.dot(alpha,np.identity(holder1.shape[0]))
        holder2=np.linalg.inv(holder2)
        holder3=np.dot(holder2, X.T)
        weights=np.dot(holder3, y)
        return weights

The same problem persists, the optimal alpha is now -24. No idea what is happening. There must be something very wrong with my input data, right? But I cant imagine what would cause this...",1
f6yc9kc,dtnh3f,"Antiregularization is a thing and can happen quite often, especially if you feature engineered using PCA or otherwise did other sorts of regularization. 

This gives a much more in depth look: https://stats.stackexchange.com/questions/328630/is-ridge-regression-useless-in-high-dimensions-n-ll-p-how-can-ols-fail-to",2
f6yzky6,dtnh3f,"Hmm... I read the link, fascinating. But I do not think everything is working as intended. No PCA, other regularisation or preprocessing has been done. And I very much doubt negative alpha is intented to be correct on an introductory course like this...
The question remains, what could be causing this... Must be something strange with the input?",1
f6z6tle,dtnh3f,"I can only speculate without running the analysis on my own machine, but your code looks correct at a glance. My only suggestions would to be to confirm you have no type errors, run some k-fold experiments iterating up the alpha, and try a lasso or other regularized OLS and see if the results are similar. It could simply be correct as it is rare you would even consider a negative L2 alpha, and your professor may even have bounded their hyper parameter search space at 0 as I would the majority of the time.",1
f6xn6gq,dtiy98,"Shuffle Split: This function create infinite iterations of your data where the test and train are randomly assigned at each iteration. Therefore, you can have a point that is repeated in testing or repeated in training. This could cause issues in ensuring you have a proper validation score if certain classes are over or under represented.

K-Fold: This function will shuffle your data, then draw boundaries every len(data)/(k+1) observations. You then use one of the k+1 resulting folds as a validation set and train on the rest.

That is, a shuffle split with a 20% test proportion will generate infinitely many randomly split 80/20 train/test buckets. A K=4 fold split will leave you with 5 buckets, of which you treat one as your 20% validation and iterate through 5 times to get a generalized score.

If you are doing classification with imbalanced classes, a stratified version of both exists which maintains the distribution of your response variable amongst the folds and splits.

Generally, K folds is seen as the proper method as you prevent any bias from random sampling.

Specifically to your question, you must never use the test set from a different train set in shuffle split because you are very likely to have the same data in your train and test which is a huge information leakage and invalidates any model performance metric.",1
f6yev8r,dtiy98,Thanks alot for your thoroughly explained comment. That was helpful.,1
f1x7n0w,dar9z6,"Depending on how many different commands you want to look at (and it‚Äôs a little unclear why SKLearn would be your package of choice rather than just rote statistics, correlation matrices, etc. ) you could encode the categories using a (I forget precisely the name but) OneHotEncoder or LabelEncoder. This would give you the ability to run multiclass classification algorithms.",2
f1yw2xd,dar9z6,"aha, I have multiple protocols and whole lot of commands... we may be talking about 50-60 major commands with most of usage and same amount of commands with less usage for like 5-6 thousand times a month.

I want this to ultimately be used as a way so we can identify bots and crawlers and stuff. im completely new to this and have been a back-end developer. now I want to improve on my reports of usage and gather more useful information. like some bot is hitting these types of commands in that interval, so this new thing working on same interval and commands with marginal differences, so this might be of the same family? and things like that. I don't even know what should be used for this!",2
f22pmhw,dar9z6,"If I am understanding correctly, this is an unsupervised problem (that you do not have a training set with what would be the correct predictions). In this case, you want to look into ""unsupervised learning"" and ""clustering"". This may give you the ability to segment your larger dataset into one that could potential give you information about different heterogenous groups in your data, but this hinges on you having more data than just a text command.

In this case though, unfortunately I would expect that it will be difficult to do what you want. A first step would be identifying existing traits of bots (i.e. tons of commands faster than a human could send from the same ip, etc.) and attempt to conquer the problem that way. To my knowledge SKLearn does not have many good ways to deal with this type of problem other than LabelPropogation but these are advanced techniques. You may have luck with programatic solutions more than machine learning. Good luck! Please feel free to message if you have questions.",2
ezsq8kj,d244x4,Post very short code.,2
excka8y,cs2urp,"You can just add the two arrays after one-hot encoding each separately.

I never heard of two-hot, that's interesting! Could you describe the situation more.

I guess the levels are the same for the two variables?",1
exctsp1,cs2urp,"Thank you.

To give you a better idea of what I'm dealing with, the data frame describes housing data; it's just one of the tutorial data sets on Kaggle. There are two categorical features to describe sections of the basement. They have the same categories, ""unfinished"", ""good living quarters"", etc, and the order does not matter. I could just one-hot encode both of them, but that would result in twice as many columns as I need for these features. For this example, it doesn't really matter, but I could see how knowing the best way to implement two-hot encoding could be useful in the future.

With one-hot encoding, a row of two features with five categories would look like this: 

[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]
while two-hot encoding would look like this: 

[0, 1, 1, 0, 0]",1
exea82m,cs2urp,"Thanks!

I suggested using +which could give the value 2 in some cases (so now ternary variables). If you do want binary you would combine with or. This discards information, potentially.",1
ewdbblx,cnnacy,RFE removes features based on the coefficient values.,1
ewermxj,cnl2a5,"I can‚Äôt remember what the output is for the kmeans cluster centers, but your slicing of ‚Äúcenters‚Äù may be referencing data with a numerical index or something.",2
ew3ud66,cmmbi5,"I‚Äôve seen bizarre slowdowns when intel MKL fights with other multiprocessing methods, basically making way more threads than can be efficiently processed. The severity of the problem seems to be OS and python installation dependent. Try shutting off MKL mutithreading by setting the environment variable ‚ÄúMKL_NUM_THREADS‚Äù to 1.",1
ew3vm3l,cmmbi5,I use openblas,1
evycli4,clpubv,by googling,1
evyddn0,clpubv,"search for Anomaly Detection in Network Traffic 


https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf",1
evydlnu,clpubv,"https://paperswithcode.com/paper/network-traffic-anomaly-detection-using 

here code with paper
using dl",1
evulpwt,cl88rf,Make sure your python build is the anaconda version and the sublime python path points to to Anaconda3 dir (JSON user settings of Sublime Text 3),1
ew0qyin,cl88rf,If nothing else works. Uninstall everything and install a clean version of Anaconda distribution and point Sublime to the proper path.,1
ew0xmus,cl88rf,I just restarted the whole computer but thanks for help,1
etq1qkx,cc2mxr,"if by numpy you mean pandas, then

&gt; X = dataframe.values[1:]  # features
&gt;
&gt; y = dataframe.values[0]  # target
 
should do the trick",1
etumb5d,cc2mxr,Thanks. So basic I'm almost embarrassed for asking.,2
etj4faq,cbm4g6,"What defines those data points (dots)? Is it just 2d location (x, y)? How did you label the original data points as blue or red?

&amp;#x200B;

If they are linearly separable (you could draw a straight line between them) then a linear SVM is probably your best bet. Still, it would be good to know more about the actual problem.",1
etrdap9,cbm4g6,"A point difines a line with its rho and theta (so 2D). There are left lines (red points) in an area of the graph, and right lines (blue points) in another part of the graph. So the blue and red points are clearly linearly separable, but this could be interesting if I'd like to only separate the blue form the red.  I want my application to be able to say ""this new point determines a left line"" or a right line, or none.",1
erxuz4x,c4q5i6,"Try
` from sklearn.cluster import kmeans `

I think the module is cluster and the class is kmeans.",1
erzcvj2,c4q5i6,"I finally successed. But in a way I've not expected:  


import  sklearn.cluster.k\_means\_  as kmean

&amp;#x200B;

kmeans = kmean.KMeans()  


Why should I do it in such indirect way...",1
eqko7d2,bylpjd,"Just use 

model_name.fit(w_cancelled_data_X, w_cancelled_data_Y)

Then for your active contracts:

model_name.predict(active_data_X)",1
eqkq7gb,bylpjd,"Wow! That did it. I was making this way more complicated than I needed to.   I really appreciate the help. Out of curiosity, do you have any good suggestions on how to export the full predictions to a csv? Thanks for the help again. I truly appreciate it.",2
eqkrf7t,bylpjd,"No problem. If you are familiar with the pandas library, I‚Äôd convert active_data to a DataFrame: 

import pandas as pd

active_data_X = pd.DataFrame(active_data) 

‚Äîside note: there‚Äôs a method that reads csv files and converts them to DataFrames, which goes as follows: active_data_X = pd.read_csv(‚Äúpath/filename.csv‚Äù)  ‚Äî

And then run:

active_data_X[‚Äòprediction‚Äô] = model_name.fit(active_data_X)

And then:

active_data_X.to_csv(‚Äúresults.csv‚Äù)

Edit: added the close ‚Äú",1
eqo01ik,bylpjd,I didn't even realise that `sklearn` worked with `DataFrame`s. TIL!,2
eo9wn6b,bqtxes,"&gt; `cluster_centers_` : array, [n_clusters, n_features]
Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these will not be consistent with labels_.

&gt; init : {‚Äòk-means++‚Äô, ‚Äòrandom‚Äô or an ndarray}
Method for initialization, defaults to ‚Äòk-means++‚Äô:
‚Äòk-means++‚Äô : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.
‚Äòrandom‚Äô: choose k observations (rows) at random from data for the initial centroids.
**If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.**",1
eobky9y,bqtxes,missed that.. thanks!,1
eoace2y,bqtxes,"You can save the cluster centers as a .npy file with np.save('filename', kmeans\_obj.	
cluster\_centers\_)

Then you can load with np.load('filename')",1
ela96qt,bevq9d,hmm maybe divide the image into pieces and run each thread for blob detection?,1
ekuebnz,bcwbv4,Bootstapping the models will get you the variance of the accuracy.,1
ekwf5jw,bcwbv4,How can I do that?,1
elc8rti,bcwbv4,"Thanks all got it by looping on bootstrap samples.
Thnaks",1
ekw9ipq,bcwbv4,"It's a binomial distribution, therefore if the accuracy is p, the variance is n*p*(1-p), where n is the size of the set used to calculate the accuracy.",1
ekrkuj8,bcbduy,"RFs in sklearn produce probabilities of labels, which can be used to calculate ROCs which in turn can let you ""tune"" the probability to minimise false positives.",1
ekmvywi,bbxi9t,"Just record the cluster means. Then when new data comes in, compare it to each mean and put it in the one with the closest mean.",2
ekp91di,bbxi9t,"Ah, yes, that will work, thanks!",1
eiy0oo4,b36h5a,U mean they are just different objects or oredict differently too?. What about feature importances in both?,1
eiy0uzg,b36h5a,"Sorry I didn't get your question ?. I mean a random forest with the conditions mentioned in my question should create exactly same trees ( estimators ), while training. What do you think about that ?.",1
eiy1c8r,b36h5a,It should. What i meant was how are u differentiating both the eatimators. On what basis?,1
eiy1x70,b36h5a,"I create a random forest (scikitlearn) with number of estimators 10(This with create 10 different decison trees). I can access each of theses estimators(dection trees).  

Code Example

model = RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019)¬†

Different estimtors/ decision trees can be accessed by 
model.estimators_[2] , model.estimators_[5] etc.


Should model.estimators_[1] ,model.estimators_[2] and model.estimators_[5] be same ?",1
eiy3cvm,b36h5a,They both will be different instances internally both might have the same structure. I have not really analysed the eatimators before. U can try to predict a few examples with each estimator to check,1
eiy4t6e,b36h5a,You don't need to predict. You can just visualize the the individual trees. I did that and they are different,1
eiy4u16,b36h5a,That is strange. Can u share the notebook over git or something?,1
eizifmi,b36h5a,"Why would it be creating the same decision tree over and over again?  That's not how a random forest works, even when you don't bootstrap and fix the seed.

If you created two forests with a fixed seed and the same parameters, they would be the same.",1
ehtkjb6,axgj2c,"great idea!

think you should add number and scale of factor vars, can greatly impact runtime 

&amp;#x200B;

also the amount of duplicate columns

&amp;#x200B;

i like it though... make it for r

&amp;#x200B;",1
ehuex1m,axgj2c,"Thank you for the feedback u/weightsandbayes we really appreciate it!  

Adding the variance was definitely something we were thinking about. I think this would a good avenue to explore, we should give it a try and I agree for many algos variance definitely plays a role. 

I haven‚Äôt used R in quite a while, what library should we tackle first in your opinion if we were to build a similar thing?",1
ehtqae2,axgj2c,"Fantastic idea. Could it be extended to other libraries? MLib, H2O, Keras, etc.",1
ehuev0d,axgj2c,"u/dj_ski_mask thanks for asking and you raise a great point.  
We built our library in a very scalable way, for example adding support for a new scikit learn algo is as simple as updating the config Json and running the model estimator.  
Adding a new algorithm here: [https://github.com/nathan-toubiana/scitime/blob/master/scitime/\_config.json](https://github.com/nathan-toubiana/scitime/blob/master/scitime/_config.json)   
And running the \_data function here: [https://github.com/nathan-toubiana/scitime#how-to-use-\_datapy-to-generate-data--fit-models](https://github.com/nathan-toubiana/scitime#how-to-use-_datapy-to-generate-data--fit-models)  


In principle nothing really prevents us from extending this to other libraries  
One challenge if we want to extend this outside Scikit-learn is that we are using scikit-learn specific methods throughout the code base.  
We would probably want to wrap our functions with a Library layer to specify what library we‚Äôre targeting. But It definitely can be done !",2
ehufxkc,axgj2c,By vars I meant variables haha ,1
ehuh90d,axgj2c,"Got it!

But by number of vars do you mean number of columns ? If so it's already factored in.

The distribution of each variable is also something we should look into.  
",1
ecs1ses,aahf76,"For gains, wouldn't it be simpler to just multiply by the gain factor?",2
ecs20oe,aahf76,"But then it will not be sign-conscious. If the gain is not a constant, but a function.",1
ect12sr,aahf76,"This sounds like it has nothing to do with sklearn.

Multiplication is typical especially for audio which I guess your signal might be - you could clarify.

If you know what you're doing then yes, you could use np.sign.",1
ect7y0q,aahf76,"Well not directly obviously, but this could be a common use-case in sklearn. Particularly I'm trying to apply a ""windowing function"" (not really a window, but similar principle) to the signal in such way that the window is slightly different for the y- than the y+ part. Thus for y &lt; 0 I want to apply window1, but for y &gt; 0 window2.",1
ec1cetm,a75oid,"You're not doing anything wrong. These metrics include the number of predictions of a class in the denominator so they divide by zero in this case. The NN, for some of your workloads, just never predicts that class. You could ensure that there are plenty of examples of that class in the training set. Apart from that all you can do is choose to report F and precision only when there are plenty of samples in the test set.",1
ec1d4g8,a75oid,"I wonder, why does it change though? As if MLPClassifier() fits a different fit every time I run the program. Even when it uses the same params? Yes, since MLPCLassifier() is implemented using stochastic gradient? But then, if I get ""errored results"" and ""non-errored results"", then are both valid? Or should I discard results that give this problem? The difference that occurs in prediction accuracy, when the error occurs, is quite drastic. 0.85 vs \~0.65 or even \~0.45, when this error pops up. So it ""seems"" that the MLPClassifier somehow fails occasionally, on this data set.",1
ec1gpdh,a75oid,"This is why we often report a cross-validated value, not just a single value. Yes, it could be that the classifier just fails sometimes. You can try different architectures and hyper parameters, especially initialisation and optimizer to see if it becomes more reliable, or try collecting more data.",1
ec1gv1e,a75oid,"What are you referring to with cross-validation? You mean that one ought to cross\_validate on the model, rather than fit the model a single time?",1
ec1ho19,a75oid,Yes,1
ec1hpud,a75oid,"But what does this help? If a cross\_validate ""fold"" produces the error, then it will be reflected to the averages of that cross\_validate? So even then one'd need to perhaps look for ""clean runs of cross\_validate""?",1
ec1u38k,a75oid,"It helps only in that if we report an accuracy value, it's an honest one (with error bounds if we like). It doesn't help to avoid the runs that go bad - for that see my earlier answer.",1
ec0d5su,a753f2,"Solution: I was accidentally using the MLPRegressor() class, when I need to use MLPClassifier() to get the output as multiclass.",1
ec1gxk7,a746h0,"The best way to estimate is to try running with a very small proportion of your data, say 0.00001, and then increasing by powers of 10 and seeing how it scales.

If you're seeing 10% CPU in a long-running computation, it could be because the job is disk-bound. Alternatively if you have 8 or 12 cores, and sklearn is using 1 and Task Manager reports it as a percentage of the total.",2
egyudwg,a73oda,"The C value, penalty, and random_state are the only ones I adjust. Random state is just stay consistent so really only the C value and penalty.",1
eagk9z2,a0c8dw,"It's more of an ML question than an SKLearn one. Training MLPs is still kind of a black art.

You can certainly try a different optimizer (I see they have LBGFS, Adam, SGD), momentum on/off, different learning rate, shuffle on. Also if you changed the default values for tol or anything else, try going back to the default. Apart from that, it might be worthwhile normalising your data columnwise (or it might not).",1
eagl4du,a0at1q,"Yes. You can check this out by creating and fitting the MLP and then looking at the `coefs_` attribute. First of all it doesn't crash! And second the `coefs_` are the same shape you'd expect with (7,).

It doesn't seem to be documented, so if there isn't a ticket in Github, you could raise one, either to remove this behaviour or to document it.",1
eahj20o,a09ukl,"train_test_split takes X and y, not Xy",1
eagw9zj,a06iin,"The amount of parameters you're attempting to optimise over is 10*2*11*3*3*2 = 3960. 

This isn't necessarily an issue straight up, but if your dataset is large and you are running a lot of decision trees with several tweaks to their parameters it's going to take some time.

You can try to scale back the parameters you're optimising over (bootstrap = false for example is one that can go...) as a start.",2
eahhbwl,a06iin,Thanks; Yeah I can try reducing the parameter matrix. so far I've reduced the cv and iterations,1
eahlc2t,a06iin,"I'd suggest lowering the n_estimators.

You mentioned this is a dataset that had an associated paper. What parameters did they use?",2
eai03lt,a06iin,"I tried lowering the n\_iter as much as possible and reducing cv but that didn't help. The paper I'm following didn't list parameters, and also they used WEKA not scikit learn so it doesn't really carry over",2
eajcy7s,a06iin,"&gt;I tried lowering the n\_iter as much as possible and reducing cv but that didn't help.

I ran your code using the MNIST and it ran fine, so that's not the problem. What's the size of the dataset your working with? Is your system actually using resources while running or has it hung?

&gt;The paper I'm following didn't list parameters

That's just bad research.

&gt;and also they used WEKA not scikit learn so it doesn't really carry over

The parameters are named differently and use different defaults, but you'd be able to replicate in sklearn... if they gave their parameter values... ;)

&amp;#x200B;",2
eajybfy,a06iin,"thank you for checking; I got a friend to run my code on his computer and it finished in like 1 min. So I think my laptop is just crappy, or I need to update something or reinstall it :P the size of the data set was small, like 131x 22 so I don't know what the problem is. I guess its just bad hardware",2
eak503r,a06iin,Seems like. :( ,1
ea2as8t,9yhcqq,"Do you mean having sklearn calculate coefficient p values, determine goodness of fit or out of sample validation?",2
ea3f85a,9yhcqq,"Whatever is meant when one says ""testing the model"".",0
ea5jij5,9yhcqq,"Hence my question. There are several different ""tests of a model"", ranging from ""which variables are related to the prediction"" to ""can the model's prediction be applied to new data"". 

Understanding what you want is half the battle.",1
ea85xvi,9yhcqq,"I'd imagine that library developers would develop a set of tests tied to any particular method. So if I use LinearRegression(), then somewhere near there are at least ""common tests"" for LinearRegression(). Also, since not all tests are suitable for all models, that should dictate how a library is designed.",1
ea87yte,9yhcqq,Have fun then!,1
ea1t6zy,9yhcqq,"Yes, have you looked at the manual? It's great.",0
ea1yi0j,9yhcqq,&gt;[https://scikit-learn.org/stable/auto\_examples/linear\_model/plot\_ols.html](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html) ?,1
ea287y4,9ygvsi,"The model is trained ""in-place"". Even when you call .fit(), you don't have to assign the model back to the variable you instantiated it in. Internally it just adjusts all the self.coeffs_  and self.intercept_ and all other model.params without creating a new model to hold them.",1
e9ve9uw,9sbqvm,"You can use an IDE like PyCharm, set up a break point in the \`fit\` method and then use the tools to step through the code one line at a time. You can set up different variables to watch and see them change.",1
dyirkg8,8emc3b,"I think there is not a definitive answer, but you can try with the following approaches:

1. You can just append the PageRank as another feature;
2. you can use the bag of words matrix to classify and then, use the classification prediction as an input for another model (which has PageRank as a feature);
3. You can classify both separately and use both predictions as inputs for a third classification model.

(2 and 3 are also called ensemble methods, look on the web for stacking).

Regards.


",2
dxnqyib,81xksg,"Use TFidfVectorizer instead, it has a mindf and maxdf args for that tuning. Just initialize mindf =3 to ignore terms that appear in less than 3 docs",1
dxo9maz,81xksg,Oh thank you very much! You can't imagine how many times I've read [the documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and I've always missed those parameters.,1
dxoduia,81xksg,You are very welcome ,1
dun5ic5,7z4qi7,"The first time you fit the model you had 4 possible classes. [1, 2, 3, 4] But when you called partial fit, you had the label for the new point be 5. Standard classifiers are not able to accommodate adding new possible classes after the initial setup.",1
dunbold,7z4qi7,[deleted],1
dunbtit,7z4qi7,"See this changed code, it works fine without fit() 
     
     from sklearn import neighbors, linear_model
     import numpy as np
  
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
        X.append([6, 9])
        y.append(5)
    
        X.append([10, 50])
        y.append(6)
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""log"")
    
        #sgd_clf.fit(X, y)
    
        #print(sgd_clf.predict([[6, 9]]))
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here",1
dunbvpo,7z4qi7,I understand i need to give class labels before initial fit().  But then fit uses all the new data and generates a model from scratch . what is the purpose of partial_fit then ? ,1
dviwuxk,7z4qi7,I found a work around by taking some random classes like np.arange(100) in first partial_fit(),1
dtvqk2p,7vw4ap,"Consider building a neural net, e.g. with [Keras](https://keras.io/). Since neural networks are basically trained via batch processing, they can be [tuned with new observation data](https://stats.stackexchange.com/questions/220169/how-do-i-add-more-training-data-to-pre-trained-deep-learning-net). Check out [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) for a good introduction.",1
dtxahq1,7vw4ap,"Thanks brylie, thats a wonderful book :) Could you please let me know if i can use scikit knn model along with keras ? Like i train with scikit knn model and just use batch training feature of keras. Is this possible ? I will read the book in detail today. Thanks again :) ",1
dtxbssd,7vw4ap,"Keras is much more comprehensive for deep learning than scikit-learn neural network. You might be able to create a hybrid model with KNN and a neural network. However, consider starting with one or the other, for simplicity.",1
dtxcf6f,7vw4ap,"which one can be trained fast ? I see that neural networks work slower. In that case, will scikit incremental learning would be a better fit ?
 http://scikit-learn.org/stable/modules/scaling_strategies.html

When exactly i need to use keras vs scikit neural networks if fast response back is my criteria than accuracy ? 

",1
dtxq4s6,7vw4ap,"I think there are at least two types of 'response time' to consider:
- training - where the model learns from provided data
- inference - where the model classifies or predicts based on new data

Classification time will likely be similar between models. Training time may vary greatly, but there are ways to speed up training in some cases. To which do you refer to when you say 'fast response back'?",1
dtzdf7h,7vw4ap,"Hi brylie, For training. If i get a newly registered user, i want to add his data to the already existing trained dataset and perform the inference step.

Currently, Just because of one/two users, i am training the whole dataset which is taking lot of time to go to inference step. 
",1
dtzepe9,7vw4ap,"Back to your original question, I am not sure it is possible to re-train a kNN classifier by adding new observations to an existing model. It is likely you will need to train a new model instance using all of the data (or a train/test split approach).

However, you can [take an existing Keras model and run its `fit()` method on new data](https://github.com/keras-team/keras/issues/1868#issuecomment-272078441), which will update the existing model. 

Keras may save time in the long run since you can update the model. There also ways to  speed up the Keras training, e.g. by using a GPU.",2
du04ux2,7vw4ap,thank u brylie :),1
du3dv06,7vw4ap,"Brylie, if i create a keras model, can i still be able to get the closest matches for my test data from trained dataset ? 

For example, with knn i have flexibility to get the closest matches to the test data with kneighbours function. Can this be possible with keras ?

Can i train a scikit model and then pass this trained model to keras ? 

 Sorry if my questions are naive. i am learning machine learning",1
dn4vbad,70m5l1,Check out the SciPy Cookbook revipe foe [Matplotlib: django](http://scipy-cookbook.readthedocs.io/items/Matplotlib_Django.html).,1
dn4vqr7,70m5l1,You might also like to [use Bokeh charts in a django project](https://www.hackerearth.com/practice/notes/bokeh-interactive-visualization-library-use-graph-with-django-template/).,1
dn4vx8g,70m5l1,See also: [django-chartflo](https://github.com/synw/django-chartflo),1
g8nvhog,j9q6bp,"If you are imagining an expert system (if/else decision tree) then try the `experta` package. If you need it to comply with the sklearn regressor or classifier API, then you can create a class with `__init__()`, `fit()` and `predict()` methods.

* `__init__` : instantiate your custom experta `KnowledgeEngine` instance.
* `fit` : calculate any stats on the dataset that you might need and adjust the parameters within the engine ( can just pass for most problems)
* `predict` : use `self.engine.declare()` to intake the state/feature data and return the output of engine.run() which must be a numpy array to work in an sklearn Pipeline",1
g8f2vm9,j8vepi,"The hyperparameters tol and max_inter are generally used to tell the model when to stop it's optimization for fitting the parameters of the logistic regression. Generally, tuning these parameters won't make a big difference to the predictive power of your model.

The penalty and C parameter deal with regularisation. This is a concept in ML used to prevent overfitting of your model. Overfitting happens when your model performs poorly on unseen/test data compared to your training data. This usually happens when your model is too complex (perfectly tuned models are flexible enough to learn something from your data but not too flexible to overfit/memorize your training data).

The penalty parameter lets you choose what type of regularisation you want to apply. There are two types of regularisation L1 (lasso) and L2 (ridge). L1 regularisation forces some of your parameters for the unimportant features to zero (these features are dropped from the model- LASSO does automated feature selection). L2 regularisation forces some of your parameters for the unimportant features to zero but not exactly 0 (these features are not dropped from the model). Elastic net is a combination of L1 and L2 regularisation.

The C parameter lets you choose how much regularisation you want to apply. In the case of L1 regularisation, more of it means more features dropped from the model. If you drop enough unimportant features from your model, your model will not overfit. If you drop too many features you may lose some important information to learn from your data.",1
g8mmeq2,j8vepi,I love this response. Thank you so much.,1
g7u6e66,j5pcm6,Try installing with Anaconda,1
g7waffb,j5pcm6,"So what I had to do:

\&gt;conda uninstall scikit-learn numpy scipy

\&gt;conda remove --force scikit-learn numpy scipy

\&gt;pip uninstall scikit-learn numpy scipy

\&gt;pip install -U scikit-learn numpy scipy --user",1
g785qxo,j2v11w,"If you looked at the source code you would have noticed the ""algorithm"" kwarg which specifies that if you don't choose the ""brute"" option it uses a tree data structure. This may introduce slight discrepancies between the fit model and the ""true"" distribution of the data.",3
g79d2mb,j2v11w,"Thanks for your prompt response, I got it overnight.

I agree. I like the speed of the default model, I‚Äôll time it but I reckon it‚Äôs there to be quicker than brute.

I suppose I am then doing my own little brute method with the literal Euclidean nearest neighbour.  The combination works for my purpose.

Nice one, thanks.

Edit: two thoughts.  First, my data is very entropic so maybe trees ain‚Äôt so good.  Second, my code still might be wrong so brute is a good check that I haven‚Äôt screwed up my trig.",1
g7a2j0z,j2v11w,"If there are ties (several neighbors with the same score) then scikit chooses the first one.

A trivial example to show how it works

    x = np.random.randint(0, 5, (20, 1))
    y = np.arange(len(x))

    knn = KNeighborsClassifier(1)
    knn.fit(x, y)
    knn.predict(x)",2
g7b8lel,j2v11w,"Good point.  It‚Äôs not that, I removed duplicates for my purposes.  Thanks though.  üôè",1
g7dd7oa,j2v11w,"Oops!  My method of removing duplicates just inherits the same behaviour.  

I‚Äôm going to add fuzz/noise to them that that will work for my purpose.  

Cheers for the stimulus üíì",1
g70358r,j1idtx,"Without knowing the kind of system you're running this on (RAM, amount of cores etc, etc) or the hyperparameters you're trying to optimise on, you're generating 300 trees. That could take a bit. 

Try decreasing the number of trees (say, 2?). If it's still hanging, message back and we can try to troubleshoot.",1
g2w82j9,igtkry,"Nope. Sklearn doesn't return F / t / p values. You can code your own, but otherwise just use statsmodels.",2
g0b6kn2,i3546z,"Not sklearn specifically, but graph theory could be useful.",1
fyn0442,hu6y83,The clusters and centroids returned are the final results.,2
fynnk7i,hu6y83,Gotcha. Me and a friend had kind of come to a realization that might be the case for something we are working on but we weren't too sure. Thanks for answering!,2
g77yegr,ht4ol1,Hi.  I‚Äôve made AUC from small numbers of points. Treat them like polygons and literally calculate the rectangles and triangles?,1
fx4lpgh,hm6td7,"I think many ppl have got 100% on it, because when I implemented a simple CNN I was able to get 94%.

As for the type of model, CNNs are usually quite reliable for image classification. However, you can still get an accuracy of 85+ using just Dense and Flatten layers without Convolutions and Poolings",2
fx71pfp,hm6td7,"Thanks. Do you have any links where it shows people getting 100%?

I was given this link where it shows the best performance is around 96-97%:
https://paperswithcode.com/sota/image-classification-on-fashion-mnist",1
fv3gbqa,hakk07,suggest to use seaborn with lmplot(),1
fv3wp7z,hakk07,Convert language frame using one hot encoder,1
fvoi9f5,h0w3xx,"I don't have a background in math/science either... That said, check out stat quest youtube videos. Yes, they do include math, but honestly, its really not possible to grasp these algorithms without at least some math and logic involved. So if you want to really understand these, then you'll be required to understand things like derivates and alpha, etc.. Good luck!",1
ftgob55,gziaus,"Remember that k-means is not a classification algorithm. It only does clustering. Also note that 53 + 47 = 67 + 33 = 100.

Now can you explain what's going on?",2
ftgxkkr,gziaus,"Yes. I thought about that. It looks like it was just comparing original and predicted labels side by side and based on their order of occurrence only two possible result was produced, N and 100-N. Then how to judge the accuracy of the algorithm? How to get the cluster quality details?",1
fqwpb6e,glb4cy,"[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) is a data preprocessing technique, not a model fitting technique. The questions you're asking make me think that you want to fit a polynomial model to your data, that's *not* what this Class is for.",2
frim32i,glb4cy,"Got is, thanks!",1
fpj3n47,gcvtsm,"You'll probably have to implement your own [estimator](https://scikit-learn.org/stable/developers/develop.html). Then you can just pass that to your search function.

Note that stepwise feature selection is generally poor and should be avoided (see e.g. [here](https://redd.it/ehmi22)).",1
fmtws0m,fx6kdy,"Just from experience that‚Äôs a little high for a distance metric based classifier. Generally there will tend to be some on borders between classifications that will flip flop based on the corpus of observations you have. If you share code we can check to make sure, but the best part of these types of fun datasets is finding surprising ways to get things to work.

I would suggest triple checking over fitting with a holdout set, but congrats on your good training!",1
fmu1rkf,fx6kdy,"Thank you, well actzally the code is quite short, so i share it here:

# Get data
from sklearn.datasets import load_digits()

dataset = load_digits()

X = dataset[""data""]

y = dataset[""target""]


# tsne
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, init=""pca"")

X_embedded = tsne.fit_transform(X)

# For viz
import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))

plt.scatter(x=X_embedded[:,0], y=X_embedded[:,1], c=y)

plt.show()

# Generating sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fitting
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV

knc_params = {""n_neighbors"":[3,5,7,15]}

knc = KNeighborsClassifier()

gs = GridSearchCV(knc, knc_params, cv=5)

gs.fit(X_train, y_train)

# Run model
pred = gs.predict(X_test)


from sklearn.metrics import classification_report

print(classification_report(y_test, pred))


from sklearn.metrics import confusion_matrix

cf = confusion_matrix(y_test, pred)



import seaborn as sns

sns.heatmap(cf, annot=True)



________



Thats it, i hope i dont have a made a typo, i am on the phone right now üòÖ
Any insights to whats ""wrong""?",1
fmvfmvr,fx6kdy,"I don't know, but I think you have to fit tsne only with train, and transform train and test without using the test dataset to train tsne. When you use tsne on X it has information that came from test. In that way you are trickering the algorithm using information you supposily don't know yet(test data)!",1
fmvsjcf,fx6kdy,"Thank you, you are right, thats a silly mistake from me. Sadly tsne does not have a transform method... So i guess thats it for now. But thank you very much, bow i know i should pay more attention l.",1
fm4sp5c,ft2pcp,Is this where the group  is set to -1 for outliers? Or do you mean to filter out part of the data before pushing the rest through DBScan?,1
fm7kg61,ft2pcp,"Yes -1 signifies noise. But I want it to not classify trunks, so that I can remove low vegetation, but not trunks. But I've not figured out how to get it work that way.",1
fm5r3pk,ft1kmb,"From OneHotEncoder docs:

&gt;	Parameters
categories‚Äòauto‚Äô or a list of array-like, default=‚Äôauto‚Äô
Categories (unique values) per feature:
‚Äòauto‚Äô : Determine categories automatically from the training data.
list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.

You passed a string, which would error out. Try passing a list of categories, or switch to auto.

As to your error message, that did not come from the above code, but similarly you need to read the docs and use ‚Äòcategories‚Äô",1
fm700ta,ft1kmb,"oh = OneHotEncoder(categories = X\[:, 3\])

X= oh.fit\_transform(X).toarray() 

gives out 

""too many indices"" error",1
fmd17bl,ft1kmb,"You need to read the docs. Please look at them and check for what it asks for, not the column but the categories in the ith column.",1
fm704im,ft1kmb,"oh = OneHotEncoder(categories = X\[3\])

X= oh.fit\_transform(X).toarray()

gives 1D array instead of 2D array",1
fl3s1gy,fm1oov,"I think what your code does is pass  cols_ordinal to an imputer, return it as colums, and in parallel pass cols_ordinal to ordinal encoder and return those as even more columns. So ordinal encoder does not get the imputed columns! For that you need to pipeline imputer end encoder, and pass them to columntransformer as one pipeline.",1
fkyov8e,flg6a1,"Well, you just have to follow the code. There isn't one place. Eg for this metric, you'll see that the variable `self.effective_metric_` is created. Use Ctrl-F to see that eventually it's passed in to a `BallTree` as the `metric` parameter. So, open `_ball_tree.py`, and repeat.",2
fl6ns3e,flg6a1,"If you‚Äôre lucky a paper is cited in a comment. If you‚Äôre luckier the code follows the paper.
Or translate the code yourself, test important chunks of it to be sure.",1
fk1xbio,ffx9zw,"Hi, I don‚Äôt know what kind of output you got from the code you attached there, but here is the almost exact same example from the Tfidf transformer page:

&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)
&gt;&gt;&gt; print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
&gt;&gt;&gt; print(X.shape)
(4, 9)

Found at this link:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

And so it seems the Tfidf transformer SHOULD take in your corpus (a list of lists, where each inner list is a list of strings) and compute the corresponding IDF",2
fjxqzkb,ff9602,"I cannot verify this without jumping in and testing, which I can do in a bit. However, my suspicion is that since the Random Forest can take a multi-class scenario, there must be some sort of aggregation on the various class-level accuracy scores (or any other of the simple performance measures) and in this case it seems to be the arithmetic mean. So, in your case of only True False labeling, the mean of a single class accuracy score is that class accuracy score.",1
fk15zy7,ff9602,"Thank you very much, have you done some testing by chance?",1
fk2m6vq,ff9602,I believe the graph is a ‚ÄúROC‚Äù graph which compares true positives to false positives,1
fj4aa4k,fbfky4,"SKLearn dbscan accepts many pairwise distance functions, Euclidean, Manhattan, etc. So it can be a Euclidean function, if you want, but it is not by necessity. 

[DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Distance Metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances)",2
fj4mosa,fbfky4,"That's not what I asked. I asked about the parameter epsilon, if the neighborhood distance it is restricting for core/border membership is a euclidean distance. i.e. how would you choose epsilon on a fresh dataset?",0
fj4qu8h,fbfky4,"Simply put, asking how you select and tune hyper parameters is a very different question than if said hyper parameter is ‚ÄúEuclidean‚Äù. In this case, the metric is Euclidean because it is measuring Euclidean distance (by default). If you would like to select a hyper parameter of some function on Euclidean distance, you must tune it using the SKLearn libraries for this purpose, e.g. GridSearch. This will allow you to iteratively examine a space of hyper parameter combinations, and then select the best tuning. Lastly you can stack cross validation on each of these steps to increase the confidence you have in selecting the right tunings. This is more or less the core of the value add of a data scientist, and should be where most of the time is spent.",2
fj5yf2x,fbfky4,"Generally, most distance measurements, that I'm aware of in data science, are Euclidean, including the epsilon. Of course, you're free to choose Manhattan, other Minkowski distance, or any distance measure of your choosing.",1
fh5hpqs,f1dizs,"If I am understanding correctly, you want to define a function that accepts a time series and returns the lowest non-outlier value. This task really isn‚Äôt well suited to machine learning in the common sci kit learn sense. Specifically, this is because you do operations on the inputs and don‚Äôt necessarily return something from the set of inputs, rather either a class or continuous number.

If you want to make this current process more robust, you could winsorize your data at a couple standard deviations, then take the bottom. Alternatively take the second percentile.",1
fh5mtfh,f1dizs,"Thank you for that input, kind of what I was suspecting, but wasn‚Äôt entirely sure.",1
fgi8ngf,eylu4p,"Well, this blog post was originally posted on Neuraxio‚Äôs blog. It is very aggressive (or even toxic) marketing, I guess.",0
fgl9y7e,eylu4p,It has never been submitted to r/scikit_learn before.,1
fglaho0,eylu4p,"I know for sure that it was posted in /r/deeplearning earlier and this post was cross-posted there as well.

Also, I am convinced that some judgements about scikit-learn are merely too strong and others are controversal. For example, the blogpost says that joblib is bad because it is not able to serialize some objects. The funny fact is that joblib is based on pickle which is the best serialization facility in Python. Numpy, SciPy, and Pandas support pickling. If a library does not support pickleable objects, it is an issue of the library not joblib.",1
fftgv14,ev1as7,"If you have a well defined tree, it will be deterministic, so depending on the structure, this could be conquered using a simple for loop or recursion. SKLearn allows you to save and load models via pickle, but provides no easy mechanism for loading a tree from outside. Really though, all you should need in the simplest form is a large stack of ifs and returns.",1
ffumz33,ev1as7,I don‚Äôt know if I understood your answer correctly. Do you mean by ‚Äûa large stack of ifs and returns‚Äú that I should hard code my tree?,1
ffuthk7,ev1as7,"PMML is like an XML or YAML format, so if you can read this into some data structure you could iterate over that with a loop or recursion. But if your tree is small or you don‚Äôt feel comfortable writing a loop like that, an if tree will do precisely the same thing.",1
ffww1x9,ev1as7,Thank you very much :),1
fdo92j7,em9fxf,"&gt; I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc. For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

This could be called a type of ensemble learning, but I wouldn't recommend this, especially to a beginner. Instead, for each model, you should look at model.score(Xtest, ytest). The higher the better. That allows you to choose just one model.

&gt; The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

That can happen. It's not really an error. But presumably the score() will be low, and you can reject that model.

&gt; MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

That can happen. It might indicate bad hyperparameters. For now, I would suggest to just reject that model.

&gt; I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Most people who talk about NFL don't have a clue, and you can ignore them. You don't need to test all parameters and methods, but it's good to test a few. When you know more, you'll start to understand which models and hyperparameters are relevant for you to test.

My recommendation is to follow any tutorial that walks through sklearn with a specific dataset, eg the Titanic dataset. Don't try to program anything on your own before doing this. I recommend Andrew Ng's ML course on Coursera.

Finally, I recommend r/MLQuestions and r/learnmachinelearning rather than r/scikit_learn.",2
fdq8sjz,em9fxf,"Thank you for your gold advices. Really appreciated. I'll check accuracy score, pick the best one and follow all materials out there to improve my ML journey. Thanks",1
fc0d721,effb98,"In a famous 1996 paper, David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. ... There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
by Aur√©lien G√©ron",3
fc0f7vz,effb98,"Thank you. Yeah I have checked the book by Aurelien Geron, but can you still guide me where will I find exactly what Im looking for?",0
fc07b6i,effb98,"For algorithms try LinearRegression/Ridge (the easiest to interperate/know how they work), RandomForestRegressor or if you don't mind another package, Xgboost (which will most likely perform the best or have similar performance to the random forest, but is difficult to interperate)

Your data seems to have 1 or more rows for 1 patient, all these algorithms i proposed can only take in one row to predict some value. This means you will need to pre process/prepare the data to work well with these algorithms. I would recommend the use of aggregate functions (count of past visits, average wait times, etc.) to convert these many rows into 1. You could also lag your data so you have columns like 'time last visit', 'time of second to last visit' etc. The day of week and time of day could also make for some interesting features for you to play with :)

To train the model you will use the fit(X, y) argument once you declare your model. X is the features (time of visit, day of week, time since last visit, disease, ...) and y is what you predict (time of action).

You will also want to split your dataset into a training and test dataset. The train is what you fit the algorithm with and the test is used to see if the algorithm will actually work on new unseen data.

This is quite a brief high level overview of everything you will need to do, i also recommend tackling the titanic and housing prices datasets on kaggle.com as there are some fantastic solutions for you to learn from which is what started me off with machine learning :)

Hope this is helpful to you",1
fc0f3we,effb98,"Wow, that is a very detailed explanation. Thank you so much!! I wish i could give you a medal. 

Btw, how many features do you think I should include?",2
fc0vp3d,effb98,"Haha, no need for a medal, I'm happy enough to try and get someone into ML :)

Unfortunately there isn't really a fixed number of features that's guaranteed to work, and different algorithms might work better with certain combinations (e.g. The regression models don't work well with highly correlated features, so you will most likely need to remove them before modeling). I recommend having too many to start with (it's common to see datasets with 10-1000 features) and if needed, apply some sort of feature selection/regularization techniques to reduce the number used in the model :)

You'll never know if a feature is any good unless you try it!",1
fc1qa5g,effb98,"You should probably look into modeling this using a Poisson distribution, this is a pretty well studied problem in statistics. Here's an introductory blog post: [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459](https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459)",1
f8dxf0w,e0h8ao,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askprogramming] [Column transformer throwing away some features?](https://www.reddit.com/r/AskProgramming/comments/e0hbkw/column_transformer_throwing_away_some_features/)

&amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1
f8516p7,dyzwn9,"Hi, a few things. You should be using a classifier ‚ÄúMLP Classifier‚Äù to do a prediction that will give you a confidence as output. Unless you‚Äôre trying to predict the amount of rain the next day, in which case ~80% R2 would be excellent.

You are able to pass deep network shapes into your model, using the ‚Äúhidden_layer_sizes‚Äù argument. Similarly you can change ReLU to be the TanH or Sigmoid functions and the solver for how weight and bias changes are calculated. That is the extent of basic MLP tuning available, but is pretty robust for academic projects like this.

I *highly suggest* trying other modes like the Naive Bayes, SVM, Boosted Trees, Random Forest, and Logistic Regression. These also have large spaces of hyper parameters to tune and cross validate over. 

If you really want custom written network functions, you‚Äôre probably going to need to go the TensorFlow route which is much lower level.",1
f6xomto,dtnh3f,"I'm pretty new to machine learning and scikit-learn, but not a complete beginner. For a school assignment, I'm trying to optimise the hyperparameter alpha of ridge regression by cross validation using 30% of data as the holdout set.

Standard stuff, I've done this before. First I consistently got alpha=0 as my best alpha. Ok, the model is not overfitting, least squares solution is best. However, as seen in the image, my group then noticed that alpha=-85 gives the best performance on the test set.I am utterly confused. Ridge regression minimizes the cost function

||y - Xw||\^2 + alpha \* ||w||\^2,

so a negative alpha should lead to weights that are artifically inflated weight above the optimal least squares weights? So how come these weight give a better prediction on the test set, as given by mean square error as the metric?

I am completely confused. First I thought there was something wrong with my CV scheme, so I reduced the code to the minimum moving parts, using scikit-learn methods. Still, the problem persists, with several different random seeds. Code below:

    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=0.3)
    model=Ridge(alpha=0, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=0, mse:"")
    print(mse)
    model=Ridge(alpha=-85, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=-85, mse:"")
    print(mse)

This outputs:

     Alpha=0, mse:
    
     12.49668153434439
    
     Alpha=-85, mse:
    
     12.487210630035198

Anyone know what is going on here? At this point, I am seriously considering just implementing ridge regression from scratch, and seeing if the problem persists...

EDIT:

I implemented Ridge regression from scratch, without external libraries.

    def RidgeOwn(X, y, alpha):
        holder1=(np.dot(X.T, X))
        holder2=holder1+np.dot(alpha,np.identity(holder1.shape[0]))
        holder2=np.linalg.inv(holder2)
        holder3=np.dot(holder2, X.T)
        weights=np.dot(holder3, y)
        return weights

The same problem persists, the optimal alpha is now -24. No idea what is happening. There must be something very wrong with my input data, right? But I cant imagine what would cause this...",1
f6yc9kc,dtnh3f,"Antiregularization is a thing and can happen quite often, especially if you feature engineered using PCA or otherwise did other sorts of regularization. 

This gives a much more in depth look: https://stats.stackexchange.com/questions/328630/is-ridge-regression-useless-in-high-dimensions-n-ll-p-how-can-ols-fail-to",2
f6yzky6,dtnh3f,"Hmm... I read the link, fascinating. But I do not think everything is working as intended. No PCA, other regularisation or preprocessing has been done. And I very much doubt negative alpha is intented to be correct on an introductory course like this...
The question remains, what could be causing this... Must be something strange with the input?",1
f6z6tle,dtnh3f,"I can only speculate without running the analysis on my own machine, but your code looks correct at a glance. My only suggestions would to be to confirm you have no type errors, run some k-fold experiments iterating up the alpha, and try a lasso or other regularized OLS and see if the results are similar. It could simply be correct as it is rare you would even consider a negative L2 alpha, and your professor may even have bounded their hyper parameter search space at 0 as I would the majority of the time.",1
f6xn6gq,dtiy98,"Shuffle Split: This function create infinite iterations of your data where the test and train are randomly assigned at each iteration. Therefore, you can have a point that is repeated in testing or repeated in training. This could cause issues in ensuring you have a proper validation score if certain classes are over or under represented.

K-Fold: This function will shuffle your data, then draw boundaries every len(data)/(k+1) observations. You then use one of the k+1 resulting folds as a validation set and train on the rest.

That is, a shuffle split with a 20% test proportion will generate infinitely many randomly split 80/20 train/test buckets. A K=4 fold split will leave you with 5 buckets, of which you treat one as your 20% validation and iterate through 5 times to get a generalized score.

If you are doing classification with imbalanced classes, a stratified version of both exists which maintains the distribution of your response variable amongst the folds and splits.

Generally, K folds is seen as the proper method as you prevent any bias from random sampling.

Specifically to your question, you must never use the test set from a different train set in shuffle split because you are very likely to have the same data in your train and test which is a huge information leakage and invalidates any model performance metric.",1
f6yev8r,dtiy98,Thanks alot for your thoroughly explained comment. That was helpful.,1
f1x7n0w,dar9z6,"Depending on how many different commands you want to look at (and it‚Äôs a little unclear why SKLearn would be your package of choice rather than just rote statistics, correlation matrices, etc. ) you could encode the categories using a (I forget precisely the name but) OneHotEncoder or LabelEncoder. This would give you the ability to run multiclass classification algorithms.",2
f1yw2xd,dar9z6,"aha, I have multiple protocols and whole lot of commands... we may be talking about 50-60 major commands with most of usage and same amount of commands with less usage for like 5-6 thousand times a month.

I want this to ultimately be used as a way so we can identify bots and crawlers and stuff. im completely new to this and have been a back-end developer. now I want to improve on my reports of usage and gather more useful information. like some bot is hitting these types of commands in that interval, so this new thing working on same interval and commands with marginal differences, so this might be of the same family? and things like that. I don't even know what should be used for this!",2
f22pmhw,dar9z6,"If I am understanding correctly, this is an unsupervised problem (that you do not have a training set with what would be the correct predictions). In this case, you want to look into ""unsupervised learning"" and ""clustering"". This may give you the ability to segment your larger dataset into one that could potential give you information about different heterogenous groups in your data, but this hinges on you having more data than just a text command.

In this case though, unfortunately I would expect that it will be difficult to do what you want. A first step would be identifying existing traits of bots (i.e. tons of commands faster than a human could send from the same ip, etc.) and attempt to conquer the problem that way. To my knowledge SKLearn does not have many good ways to deal with this type of problem other than LabelPropogation but these are advanced techniques. You may have luck with programatic solutions more than machine learning. Good luck! Please feel free to message if you have questions.",2
ezsq8kj,d244x4,Post very short code.,2
excka8y,cs2urp,"You can just add the two arrays after one-hot encoding each separately.

I never heard of two-hot, that's interesting! Could you describe the situation more.

I guess the levels are the same for the two variables?",1
exctsp1,cs2urp,"Thank you.

To give you a better idea of what I'm dealing with, the data frame describes housing data; it's just one of the tutorial data sets on Kaggle. There are two categorical features to describe sections of the basement. They have the same categories, ""unfinished"", ""good living quarters"", etc, and the order does not matter. I could just one-hot encode both of them, but that would result in twice as many columns as I need for these features. For this example, it doesn't really matter, but I could see how knowing the best way to implement two-hot encoding could be useful in the future.

With one-hot encoding, a row of two features with five categories would look like this: 

[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]
while two-hot encoding would look like this: 

[0, 1, 1, 0, 0]",1
exea82m,cs2urp,"Thanks!

I suggested using +which could give the value 2 in some cases (so now ternary variables). If you do want binary you would combine with or. This discards information, potentially.",1
ewdbblx,cnnacy,RFE removes features based on the coefficient values.,1
ewermxj,cnl2a5,"I can‚Äôt remember what the output is for the kmeans cluster centers, but your slicing of ‚Äúcenters‚Äù may be referencing data with a numerical index or something.",2
ew3ud66,cmmbi5,"I‚Äôve seen bizarre slowdowns when intel MKL fights with other multiprocessing methods, basically making way more threads than can be efficiently processed. The severity of the problem seems to be OS and python installation dependent. Try shutting off MKL mutithreading by setting the environment variable ‚ÄúMKL_NUM_THREADS‚Äù to 1.",1
ew3vm3l,cmmbi5,I use openblas,1
evycli4,clpubv,by googling,1
evyddn0,clpubv,"search for Anomaly Detection in Network Traffic 


https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf",1
evydlnu,clpubv,"https://paperswithcode.com/paper/network-traffic-anomaly-detection-using 

here code with paper
using dl",1
evulpwt,cl88rf,Make sure your python build is the anaconda version and the sublime python path points to to Anaconda3 dir (JSON user settings of Sublime Text 3),1
ew0qyin,cl88rf,If nothing else works. Uninstall everything and install a clean version of Anaconda distribution and point Sublime to the proper path.,1
ew0xmus,cl88rf,I just restarted the whole computer but thanks for help,1
etq1qkx,cc2mxr,"if by numpy you mean pandas, then

&gt; X = dataframe.values[1:]  # features
&gt;
&gt; y = dataframe.values[0]  # target
 
should do the trick",1
etumb5d,cc2mxr,Thanks. So basic I'm almost embarrassed for asking.,2
etj4faq,cbm4g6,"What defines those data points (dots)? Is it just 2d location (x, y)? How did you label the original data points as blue or red?

&amp;#x200B;

If they are linearly separable (you could draw a straight line between them) then a linear SVM is probably your best bet. Still, it would be good to know more about the actual problem.",1
etrdap9,cbm4g6,"A point difines a line with its rho and theta (so 2D). There are left lines (red points) in an area of the graph, and right lines (blue points) in another part of the graph. So the blue and red points are clearly linearly separable, but this could be interesting if I'd like to only separate the blue form the red.  I want my application to be able to say ""this new point determines a left line"" or a right line, or none.",1
erxuz4x,c4q5i6,"Try
` from sklearn.cluster import kmeans `

I think the module is cluster and the class is kmeans.",1
erzcvj2,c4q5i6,"I finally successed. But in a way I've not expected:  


import  sklearn.cluster.k\_means\_  as kmean

&amp;#x200B;

kmeans = kmean.KMeans()  


Why should I do it in such indirect way...",1
eqko7d2,bylpjd,"Just use 

model_name.fit(w_cancelled_data_X, w_cancelled_data_Y)

Then for your active contracts:

model_name.predict(active_data_X)",1
eqkq7gb,bylpjd,"Wow! That did it. I was making this way more complicated than I needed to.   I really appreciate the help. Out of curiosity, do you have any good suggestions on how to export the full predictions to a csv? Thanks for the help again. I truly appreciate it.",2
eqkrf7t,bylpjd,"No problem. If you are familiar with the pandas library, I‚Äôd convert active_data to a DataFrame: 

import pandas as pd

active_data_X = pd.DataFrame(active_data) 

‚Äîside note: there‚Äôs a method that reads csv files and converts them to DataFrames, which goes as follows: active_data_X = pd.read_csv(‚Äúpath/filename.csv‚Äù)  ‚Äî

And then run:

active_data_X[‚Äòprediction‚Äô] = model_name.fit(active_data_X)

And then:

active_data_X.to_csv(‚Äúresults.csv‚Äù)

Edit: added the close ‚Äú",1
eqo01ik,bylpjd,I didn't even realise that `sklearn` worked with `DataFrame`s. TIL!,2
eo9wn6b,bqtxes,"&gt; `cluster_centers_` : array, [n_clusters, n_features]
Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these will not be consistent with labels_.

&gt; init : {‚Äòk-means++‚Äô, ‚Äòrandom‚Äô or an ndarray}
Method for initialization, defaults to ‚Äòk-means++‚Äô:
‚Äòk-means++‚Äô : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.
‚Äòrandom‚Äô: choose k observations (rows) at random from data for the initial centroids.
**If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.**",1
eobky9y,bqtxes,missed that.. thanks!,1
eoace2y,bqtxes,"You can save the cluster centers as a .npy file with np.save('filename', kmeans\_obj.	
cluster\_centers\_)

Then you can load with np.load('filename')",1
ela96qt,bevq9d,hmm maybe divide the image into pieces and run each thread for blob detection?,1
ekuebnz,bcwbv4,Bootstapping the models will get you the variance of the accuracy.,1
ekwf5jw,bcwbv4,How can I do that?,1
elc8rti,bcwbv4,"Thanks all got it by looping on bootstrap samples.
Thnaks",1
ekw9ipq,bcwbv4,"It's a binomial distribution, therefore if the accuracy is p, the variance is n*p*(1-p), where n is the size of the set used to calculate the accuracy.",1
ekrkuj8,bcbduy,"RFs in sklearn produce probabilities of labels, which can be used to calculate ROCs which in turn can let you ""tune"" the probability to minimise false positives.",1
ekmvywi,bbxi9t,"Just record the cluster means. Then when new data comes in, compare it to each mean and put it in the one with the closest mean.",2
ekp91di,bbxi9t,"Ah, yes, that will work, thanks!",1
eiy0oo4,b36h5a,U mean they are just different objects or oredict differently too?. What about feature importances in both?,1
eiy0uzg,b36h5a,"Sorry I didn't get your question ?. I mean a random forest with the conditions mentioned in my question should create exactly same trees ( estimators ), while training. What do you think about that ?.",1
eiy1c8r,b36h5a,It should. What i meant was how are u differentiating both the eatimators. On what basis?,1
eiy1x70,b36h5a,"I create a random forest (scikitlearn) with number of estimators 10(This with create 10 different decison trees). I can access each of theses estimators(dection trees).  

Code Example

model = RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019)¬†

Different estimtors/ decision trees can be accessed by 
model.estimators_[2] , model.estimators_[5] etc.


Should model.estimators_[1] ,model.estimators_[2] and model.estimators_[5] be same ?",1
eiy3cvm,b36h5a,They both will be different instances internally both might have the same structure. I have not really analysed the eatimators before. U can try to predict a few examples with each estimator to check,1
eiy4t6e,b36h5a,You don't need to predict. You can just visualize the the individual trees. I did that and they are different,1
eiy4u16,b36h5a,That is strange. Can u share the notebook over git or something?,1
eizifmi,b36h5a,"Why would it be creating the same decision tree over and over again?  That's not how a random forest works, even when you don't bootstrap and fix the seed.

If you created two forests with a fixed seed and the same parameters, they would be the same.",1
ehtkjb6,axgj2c,"great idea!

think you should add number and scale of factor vars, can greatly impact runtime 

&amp;#x200B;

also the amount of duplicate columns

&amp;#x200B;

i like it though... make it for r

&amp;#x200B;",1
ehuex1m,axgj2c,"Thank you for the feedback u/weightsandbayes we really appreciate it!  

Adding the variance was definitely something we were thinking about. I think this would a good avenue to explore, we should give it a try and I agree for many algos variance definitely plays a role. 

I haven‚Äôt used R in quite a while, what library should we tackle first in your opinion if we were to build a similar thing?",1
ehtqae2,axgj2c,"Fantastic idea. Could it be extended to other libraries? MLib, H2O, Keras, etc.",1
ehuev0d,axgj2c,"u/dj_ski_mask thanks for asking and you raise a great point.  
We built our library in a very scalable way, for example adding support for a new scikit learn algo is as simple as updating the config Json and running the model estimator.  
Adding a new algorithm here: [https://github.com/nathan-toubiana/scitime/blob/master/scitime/\_config.json](https://github.com/nathan-toubiana/scitime/blob/master/scitime/_config.json)   
And running the \_data function here: [https://github.com/nathan-toubiana/scitime#how-to-use-\_datapy-to-generate-data--fit-models](https://github.com/nathan-toubiana/scitime#how-to-use-_datapy-to-generate-data--fit-models)  


In principle nothing really prevents us from extending this to other libraries  
One challenge if we want to extend this outside Scikit-learn is that we are using scikit-learn specific methods throughout the code base.  
We would probably want to wrap our functions with a Library layer to specify what library we‚Äôre targeting. But It definitely can be done !",2
ehufxkc,axgj2c,By vars I meant variables haha ,1
ehuh90d,axgj2c,"Got it!

But by number of vars do you mean number of columns ? If so it's already factored in.

The distribution of each variable is also something we should look into.  
",1
ecs1ses,aahf76,"For gains, wouldn't it be simpler to just multiply by the gain factor?",2
ecs20oe,aahf76,"But then it will not be sign-conscious. If the gain is not a constant, but a function.",1
ect12sr,aahf76,"This sounds like it has nothing to do with sklearn.

Multiplication is typical especially for audio which I guess your signal might be - you could clarify.

If you know what you're doing then yes, you could use np.sign.",1
ect7y0q,aahf76,"Well not directly obviously, but this could be a common use-case in sklearn. Particularly I'm trying to apply a ""windowing function"" (not really a window, but similar principle) to the signal in such way that the window is slightly different for the y- than the y+ part. Thus for y &lt; 0 I want to apply window1, but for y &gt; 0 window2.",1
ec1cetm,a75oid,"You're not doing anything wrong. These metrics include the number of predictions of a class in the denominator so they divide by zero in this case. The NN, for some of your workloads, just never predicts that class. You could ensure that there are plenty of examples of that class in the training set. Apart from that all you can do is choose to report F and precision only when there are plenty of samples in the test set.",1
ec1d4g8,a75oid,"I wonder, why does it change though? As if MLPClassifier() fits a different fit every time I run the program. Even when it uses the same params? Yes, since MLPCLassifier() is implemented using stochastic gradient? But then, if I get ""errored results"" and ""non-errored results"", then are both valid? Or should I discard results that give this problem? The difference that occurs in prediction accuracy, when the error occurs, is quite drastic. 0.85 vs \~0.65 or even \~0.45, when this error pops up. So it ""seems"" that the MLPClassifier somehow fails occasionally, on this data set.",1
ec1gpdh,a75oid,"This is why we often report a cross-validated value, not just a single value. Yes, it could be that the classifier just fails sometimes. You can try different architectures and hyper parameters, especially initialisation and optimizer to see if it becomes more reliable, or try collecting more data.",1
ec1gv1e,a75oid,"What are you referring to with cross-validation? You mean that one ought to cross\_validate on the model, rather than fit the model a single time?",1
ec1ho19,a75oid,Yes,1
ec1hpud,a75oid,"But what does this help? If a cross\_validate ""fold"" produces the error, then it will be reflected to the averages of that cross\_validate? So even then one'd need to perhaps look for ""clean runs of cross\_validate""?",1
ec1u38k,a75oid,"It helps only in that if we report an accuracy value, it's an honest one (with error bounds if we like). It doesn't help to avoid the runs that go bad - for that see my earlier answer.",1
ec0d5su,a753f2,"Solution: I was accidentally using the MLPRegressor() class, when I need to use MLPClassifier() to get the output as multiclass.",1
ec1gxk7,a746h0,"The best way to estimate is to try running with a very small proportion of your data, say 0.00001, and then increasing by powers of 10 and seeing how it scales.

If you're seeing 10% CPU in a long-running computation, it could be because the job is disk-bound. Alternatively if you have 8 or 12 cores, and sklearn is using 1 and Task Manager reports it as a percentage of the total.",2
egyudwg,a73oda,"The C value, penalty, and random_state are the only ones I adjust. Random state is just stay consistent so really only the C value and penalty.",1
eagk9z2,a0c8dw,"It's more of an ML question than an SKLearn one. Training MLPs is still kind of a black art.

You can certainly try a different optimizer (I see they have LBGFS, Adam, SGD), momentum on/off, different learning rate, shuffle on. Also if you changed the default values for tol or anything else, try going back to the default. Apart from that, it might be worthwhile normalising your data columnwise (or it might not).",1
eagl4du,a0at1q,"Yes. You can check this out by creating and fitting the MLP and then looking at the `coefs_` attribute. First of all it doesn't crash! And second the `coefs_` are the same shape you'd expect with (7,).

It doesn't seem to be documented, so if there isn't a ticket in Github, you could raise one, either to remove this behaviour or to document it.",1
eahj20o,a09ukl,"train_test_split takes X and y, not Xy",1
eagw9zj,a06iin,"The amount of parameters you're attempting to optimise over is 10*2*11*3*3*2 = 3960. 

This isn't necessarily an issue straight up, but if your dataset is large and you are running a lot of decision trees with several tweaks to their parameters it's going to take some time.

You can try to scale back the parameters you're optimising over (bootstrap = false for example is one that can go...) as a start.",2
eahhbwl,a06iin,Thanks; Yeah I can try reducing the parameter matrix. so far I've reduced the cv and iterations,1
eahlc2t,a06iin,"I'd suggest lowering the n_estimators.

You mentioned this is a dataset that had an associated paper. What parameters did they use?",2
eai03lt,a06iin,"I tried lowering the n\_iter as much as possible and reducing cv but that didn't help. The paper I'm following didn't list parameters, and also they used WEKA not scikit learn so it doesn't really carry over",2
eajcy7s,a06iin,"&gt;I tried lowering the n\_iter as much as possible and reducing cv but that didn't help.

I ran your code using the MNIST and it ran fine, so that's not the problem. What's the size of the dataset your working with? Is your system actually using resources while running or has it hung?

&gt;The paper I'm following didn't list parameters

That's just bad research.

&gt;and also they used WEKA not scikit learn so it doesn't really carry over

The parameters are named differently and use different defaults, but you'd be able to replicate in sklearn... if they gave their parameter values... ;)

&amp;#x200B;",2
eajybfy,a06iin,"thank you for checking; I got a friend to run my code on his computer and it finished in like 1 min. So I think my laptop is just crappy, or I need to update something or reinstall it :P the size of the data set was small, like 131x 22 so I don't know what the problem is. I guess its just bad hardware",2
eak503r,a06iin,Seems like. :( ,1
ea2as8t,9yhcqq,"Do you mean having sklearn calculate coefficient p values, determine goodness of fit or out of sample validation?",2
ea3f85a,9yhcqq,"Whatever is meant when one says ""testing the model"".",0
ea5jij5,9yhcqq,"Hence my question. There are several different ""tests of a model"", ranging from ""which variables are related to the prediction"" to ""can the model's prediction be applied to new data"". 

Understanding what you want is half the battle.",1
ea85xvi,9yhcqq,"I'd imagine that library developers would develop a set of tests tied to any particular method. So if I use LinearRegression(), then somewhere near there are at least ""common tests"" for LinearRegression(). Also, since not all tests are suitable for all models, that should dictate how a library is designed.",1
ea87yte,9yhcqq,Have fun then!,1
ea1t6zy,9yhcqq,"Yes, have you looked at the manual? It's great.",0
ea1yi0j,9yhcqq,&gt;[https://scikit-learn.org/stable/auto\_examples/linear\_model/plot\_ols.html](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html) ?,1
ea287y4,9ygvsi,"The model is trained ""in-place"". Even when you call .fit(), you don't have to assign the model back to the variable you instantiated it in. Internally it just adjusts all the self.coeffs_  and self.intercept_ and all other model.params without creating a new model to hold them.",1
e9ve9uw,9sbqvm,"You can use an IDE like PyCharm, set up a break point in the \`fit\` method and then use the tools to step through the code one line at a time. You can set up different variables to watch and see them change.",1
dyirkg8,8emc3b,"I think there is not a definitive answer, but you can try with the following approaches:

1. You can just append the PageRank as another feature;
2. you can use the bag of words matrix to classify and then, use the classification prediction as an input for another model (which has PageRank as a feature);
3. You can classify both separately and use both predictions as inputs for a third classification model.

(2 and 3 are also called ensemble methods, look on the web for stacking).

Regards.


",2
dxnqyib,81xksg,"Use TFidfVectorizer instead, it has a mindf and maxdf args for that tuning. Just initialize mindf =3 to ignore terms that appear in less than 3 docs",1
dxo9maz,81xksg,Oh thank you very much! You can't imagine how many times I've read [the documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and I've always missed those parameters.,1
dxoduia,81xksg,You are very welcome ,1
dun5ic5,7z4qi7,"The first time you fit the model you had 4 possible classes. [1, 2, 3, 4] But when you called partial fit, you had the label for the new point be 5. Standard classifiers are not able to accommodate adding new possible classes after the initial setup.",1
dunbold,7z4qi7,[deleted],1
dunbtit,7z4qi7,"See this changed code, it works fine without fit() 
     
     from sklearn import neighbors, linear_model
     import numpy as np
  
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
        X.append([6, 9])
        y.append(5)
    
        X.append([10, 50])
        y.append(6)
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""log"")
    
        #sgd_clf.fit(X, y)
    
        #print(sgd_clf.predict([[6, 9]]))
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here",1
dunbvpo,7z4qi7,I understand i need to give class labels before initial fit().  But then fit uses all the new data and generates a model from scratch . what is the purpose of partial_fit then ? ,1
dviwuxk,7z4qi7,I found a work around by taking some random classes like np.arange(100) in first partial_fit(),1
dtvqk2p,7vw4ap,"Consider building a neural net, e.g. with [Keras](https://keras.io/). Since neural networks are basically trained via batch processing, they can be [tuned with new observation data](https://stats.stackexchange.com/questions/220169/how-do-i-add-more-training-data-to-pre-trained-deep-learning-net). Check out [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) for a good introduction.",1
dtxahq1,7vw4ap,"Thanks brylie, thats a wonderful book :) Could you please let me know if i can use scikit knn model along with keras ? Like i train with scikit knn model and just use batch training feature of keras. Is this possible ? I will read the book in detail today. Thanks again :) ",1
dtxbssd,7vw4ap,"Keras is much more comprehensive for deep learning than scikit-learn neural network. You might be able to create a hybrid model with KNN and a neural network. However, consider starting with one or the other, for simplicity.",1
dtxcf6f,7vw4ap,"which one can be trained fast ? I see that neural networks work slower. In that case, will scikit incremental learning would be a better fit ?
 http://scikit-learn.org/stable/modules/scaling_strategies.html

When exactly i need to use keras vs scikit neural networks if fast response back is my criteria than accuracy ? 

",1
dtxq4s6,7vw4ap,"I think there are at least two types of 'response time' to consider:
- training - where the model learns from provided data
- inference - where the model classifies or predicts based on new data

Classification time will likely be similar between models. Training time may vary greatly, but there are ways to speed up training in some cases. To which do you refer to when you say 'fast response back'?",1
dtzdf7h,7vw4ap,"Hi brylie, For training. If i get a newly registered user, i want to add his data to the already existing trained dataset and perform the inference step.

Currently, Just because of one/two users, i am training the whole dataset which is taking lot of time to go to inference step. 
",1
dtzepe9,7vw4ap,"Back to your original question, I am not sure it is possible to re-train a kNN classifier by adding new observations to an existing model. It is likely you will need to train a new model instance using all of the data (or a train/test split approach).

However, you can [take an existing Keras model and run its `fit()` method on new data](https://github.com/keras-team/keras/issues/1868#issuecomment-272078441), which will update the existing model. 

Keras may save time in the long run since you can update the model. There also ways to  speed up the Keras training, e.g. by using a GPU.",2
du04ux2,7vw4ap,thank u brylie :),1
du3dv06,7vw4ap,"Brylie, if i create a keras model, can i still be able to get the closest matches for my test data from trained dataset ? 

For example, with knn i have flexibility to get the closest matches to the test data with kneighbours function. Can this be possible with keras ?

Can i train a scikit model and then pass this trained model to keras ? 

 Sorry if my questions are naive. i am learning machine learning",1
dn4vbad,70m5l1,Check out the SciPy Cookbook revipe foe [Matplotlib: django](http://scipy-cookbook.readthedocs.io/items/Matplotlib_Django.html).,1
dn4vqr7,70m5l1,You might also like to [use Bokeh charts in a django project](https://www.hackerearth.com/practice/notes/bokeh-interactive-visualization-library-use-graph-with-django-template/).,1
dn4vx8g,70m5l1,See also: [django-chartflo](https://github.com/synw/django-chartflo),1
g8nvhog,j9q6bp,"If you are imagining an expert system (if/else decision tree) then try the `experta` package. If you need it to comply with the sklearn regressor or classifier API, then you can create a class with `__init__()`, `fit()` and `predict()` methods.

* `__init__` : instantiate your custom experta `KnowledgeEngine` instance.
* `fit` : calculate any stats on the dataset that you might need and adjust the parameters within the engine ( can just pass for most problems)
* `predict` : use `self.engine.declare()` to intake the state/feature data and return the output of engine.run() which must be a numpy array to work in an sklearn Pipeline",1
g8f2vm9,j8vepi,"The hyperparameters tol and max_inter are generally used to tell the model when to stop it's optimization for fitting the parameters of the logistic regression. Generally, tuning these parameters won't make a big difference to the predictive power of your model.

The penalty and C parameter deal with regularisation. This is a concept in ML used to prevent overfitting of your model. Overfitting happens when your model performs poorly on unseen/test data compared to your training data. This usually happens when your model is too complex (perfectly tuned models are flexible enough to learn something from your data but not too flexible to overfit/memorize your training data).

The penalty parameter lets you choose what type of regularisation you want to apply. There are two types of regularisation L1 (lasso) and L2 (ridge). L1 regularisation forces some of your parameters for the unimportant features to zero (these features are dropped from the model- LASSO does automated feature selection). L2 regularisation forces some of your parameters for the unimportant features to zero but not exactly 0 (these features are not dropped from the model). Elastic net is a combination of L1 and L2 regularisation.

The C parameter lets you choose how much regularisation you want to apply. In the case of L1 regularisation, more of it means more features dropped from the model. If you drop enough unimportant features from your model, your model will not overfit. If you drop too many features you may lose some important information to learn from your data.",1
g8mmeq2,j8vepi,I love this response. Thank you so much.,1
g7u6e66,j5pcm6,Try installing with Anaconda,1
g7waffb,j5pcm6,"So what I had to do:

\&gt;conda uninstall scikit-learn numpy scipy

\&gt;conda remove --force scikit-learn numpy scipy

\&gt;pip uninstall scikit-learn numpy scipy

\&gt;pip install -U scikit-learn numpy scipy --user",1
g785qxo,j2v11w,"If you looked at the source code you would have noticed the ""algorithm"" kwarg which specifies that if you don't choose the ""brute"" option it uses a tree data structure. This may introduce slight discrepancies between the fit model and the ""true"" distribution of the data.",3
g79d2mb,j2v11w,"Thanks for your prompt response, I got it overnight.

I agree. I like the speed of the default model, I‚Äôll time it but I reckon it‚Äôs there to be quicker than brute.

I suppose I am then doing my own little brute method with the literal Euclidean nearest neighbour.  The combination works for my purpose.

Nice one, thanks.

Edit: two thoughts.  First, my data is very entropic so maybe trees ain‚Äôt so good.  Second, my code still might be wrong so brute is a good check that I haven‚Äôt screwed up my trig.",1
g7a2j0z,j2v11w,"If there are ties (several neighbors with the same score) then scikit chooses the first one.

A trivial example to show how it works

    x = np.random.randint(0, 5, (20, 1))
    y = np.arange(len(x))

    knn = KNeighborsClassifier(1)
    knn.fit(x, y)
    knn.predict(x)",2
g7b8lel,j2v11w,"Good point.  It‚Äôs not that, I removed duplicates for my purposes.  Thanks though.  üôè",1
g7dd7oa,j2v11w,"Oops!  My method of removing duplicates just inherits the same behaviour.  

I‚Äôm going to add fuzz/noise to them that that will work for my purpose.  

Cheers for the stimulus üíì",1
g70358r,j1idtx,"Without knowing the kind of system you're running this on (RAM, amount of cores etc, etc) or the hyperparameters you're trying to optimise on, you're generating 300 trees. That could take a bit. 

Try decreasing the number of trees (say, 2?). If it's still hanging, message back and we can try to troubleshoot.",1
g2w82j9,igtkry,"Nope. Sklearn doesn't return F / t / p values. You can code your own, but otherwise just use statsmodels.",2
g0b6kn2,i3546z,"Not sklearn specifically, but graph theory could be useful.",1
fyn0442,hu6y83,The clusters and centroids returned are the final results.,2
fynnk7i,hu6y83,Gotcha. Me and a friend had kind of come to a realization that might be the case for something we are working on but we weren't too sure. Thanks for answering!,2
g77yegr,ht4ol1,Hi.  I‚Äôve made AUC from small numbers of points. Treat them like polygons and literally calculate the rectangles and triangles?,1
fx4lpgh,hm6td7,"I think many ppl have got 100% on it, because when I implemented a simple CNN I was able to get 94%.

As for the type of model, CNNs are usually quite reliable for image classification. However, you can still get an accuracy of 85+ using just Dense and Flatten layers without Convolutions and Poolings",2
fx71pfp,hm6td7,"Thanks. Do you have any links where it shows people getting 100%?

I was given this link where it shows the best performance is around 96-97%:
https://paperswithcode.com/sota/image-classification-on-fashion-mnist",1
fv3gbqa,hakk07,suggest to use seaborn with lmplot(),1
fv3wp7z,hakk07,Convert language frame using one hot encoder,1
fvoi9f5,h0w3xx,"I don't have a background in math/science either... That said, check out stat quest youtube videos. Yes, they do include math, but honestly, its really not possible to grasp these algorithms without at least some math and logic involved. So if you want to really understand these, then you'll be required to understand things like derivates and alpha, etc.. Good luck!",1
ftgob55,gziaus,"Remember that k-means is not a classification algorithm. It only does clustering. Also note that 53 + 47 = 67 + 33 = 100.

Now can you explain what's going on?",2
ftgxkkr,gziaus,"Yes. I thought about that. It looks like it was just comparing original and predicted labels side by side and based on their order of occurrence only two possible result was produced, N and 100-N. Then how to judge the accuracy of the algorithm? How to get the cluster quality details?",1
fqwpb6e,glb4cy,"[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) is a data preprocessing technique, not a model fitting technique. The questions you're asking make me think that you want to fit a polynomial model to your data, that's *not* what this Class is for.",2
frim32i,glb4cy,"Got is, thanks!",1
fpj3n47,gcvtsm,"You'll probably have to implement your own [estimator](https://scikit-learn.org/stable/developers/develop.html). Then you can just pass that to your search function.

Note that stepwise feature selection is generally poor and should be avoided (see e.g. [here](https://redd.it/ehmi22)).",1
fmtws0m,fx6kdy,"Just from experience that‚Äôs a little high for a distance metric based classifier. Generally there will tend to be some on borders between classifications that will flip flop based on the corpus of observations you have. If you share code we can check to make sure, but the best part of these types of fun datasets is finding surprising ways to get things to work.

I would suggest triple checking over fitting with a holdout set, but congrats on your good training!",1
fmu1rkf,fx6kdy,"Thank you, well actzally the code is quite short, so i share it here:

# Get data
from sklearn.datasets import load_digits()

dataset = load_digits()

X = dataset[""data""]

y = dataset[""target""]


# tsne
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, init=""pca"")

X_embedded = tsne.fit_transform(X)

# For viz
import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))

plt.scatter(x=X_embedded[:,0], y=X_embedded[:,1], c=y)

plt.show()

# Generating sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fitting
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV

knc_params = {""n_neighbors"":[3,5,7,15]}

knc = KNeighborsClassifier()

gs = GridSearchCV(knc, knc_params, cv=5)

gs.fit(X_train, y_train)

# Run model
pred = gs.predict(X_test)


from sklearn.metrics import classification_report

print(classification_report(y_test, pred))


from sklearn.metrics import confusion_matrix

cf = confusion_matrix(y_test, pred)



import seaborn as sns

sns.heatmap(cf, annot=True)



________



Thats it, i hope i dont have a made a typo, i am on the phone right now üòÖ
Any insights to whats ""wrong""?",1
fmvfmvr,fx6kdy,"I don't know, but I think you have to fit tsne only with train, and transform train and test without using the test dataset to train tsne. When you use tsne on X it has information that came from test. In that way you are trickering the algorithm using information you supposily don't know yet(test data)!",1
fmvsjcf,fx6kdy,"Thank you, you are right, thats a silly mistake from me. Sadly tsne does not have a transform method... So i guess thats it for now. But thank you very much, bow i know i should pay more attention l.",1
fm4sp5c,ft2pcp,Is this where the group  is set to -1 for outliers? Or do you mean to filter out part of the data before pushing the rest through DBScan?,1
fm7kg61,ft2pcp,"Yes -1 signifies noise. But I want it to not classify trunks, so that I can remove low vegetation, but not trunks. But I've not figured out how to get it work that way.",1
fm5r3pk,ft1kmb,"From OneHotEncoder docs:

&gt;	Parameters
categories‚Äòauto‚Äô or a list of array-like, default=‚Äôauto‚Äô
Categories (unique values) per feature:
‚Äòauto‚Äô : Determine categories automatically from the training data.
list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.

You passed a string, which would error out. Try passing a list of categories, or switch to auto.

As to your error message, that did not come from the above code, but similarly you need to read the docs and use ‚Äòcategories‚Äô",1
fm700ta,ft1kmb,"oh = OneHotEncoder(categories = X\[:, 3\])

X= oh.fit\_transform(X).toarray() 

gives out 

""too many indices"" error",1
fmd17bl,ft1kmb,"You need to read the docs. Please look at them and check for what it asks for, not the column but the categories in the ith column.",1
fm704im,ft1kmb,"oh = OneHotEncoder(categories = X\[3\])

X= oh.fit\_transform(X).toarray()

gives 1D array instead of 2D array",1
fl3s1gy,fm1oov,"I think what your code does is pass  cols_ordinal to an imputer, return it as colums, and in parallel pass cols_ordinal to ordinal encoder and return those as even more columns. So ordinal encoder does not get the imputed columns! For that you need to pipeline imputer end encoder, and pass them to columntransformer as one pipeline.",1
fkyov8e,flg6a1,"Well, you just have to follow the code. There isn't one place. Eg for this metric, you'll see that the variable `self.effective_metric_` is created. Use Ctrl-F to see that eventually it's passed in to a `BallTree` as the `metric` parameter. So, open `_ball_tree.py`, and repeat.",2
fl6ns3e,flg6a1,"If you‚Äôre lucky a paper is cited in a comment. If you‚Äôre luckier the code follows the paper.
Or translate the code yourself, test important chunks of it to be sure.",1
fk1xbio,ffx9zw,"Hi, I don‚Äôt know what kind of output you got from the code you attached there, but here is the almost exact same example from the Tfidf transformer page:

&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)
&gt;&gt;&gt; print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
&gt;&gt;&gt; print(X.shape)
(4, 9)

Found at this link:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

And so it seems the Tfidf transformer SHOULD take in your corpus (a list of lists, where each inner list is a list of strings) and compute the corresponding IDF",2
fjxqzkb,ff9602,"I cannot verify this without jumping in and testing, which I can do in a bit. However, my suspicion is that since the Random Forest can take a multi-class scenario, there must be some sort of aggregation on the various class-level accuracy scores (or any other of the simple performance measures) and in this case it seems to be the arithmetic mean. So, in your case of only True False labeling, the mean of a single class accuracy score is that class accuracy score.",1
fk15zy7,ff9602,"Thank you very much, have you done some testing by chance?",1
fk2m6vq,ff9602,I believe the graph is a ‚ÄúROC‚Äù graph which compares true positives to false positives,1
fj4aa4k,fbfky4,"SKLearn dbscan accepts many pairwise distance functions, Euclidean, Manhattan, etc. So it can be a Euclidean function, if you want, but it is not by necessity. 

[DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Distance Metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances)",2
fj4mosa,fbfky4,"That's not what I asked. I asked about the parameter epsilon, if the neighborhood distance it is restricting for core/border membership is a euclidean distance. i.e. how would you choose epsilon on a fresh dataset?",0
fj4qu8h,fbfky4,"Simply put, asking how you select and tune hyper parameters is a very different question than if said hyper parameter is ‚ÄúEuclidean‚Äù. In this case, the metric is Euclidean because it is measuring Euclidean distance (by default). If you would like to select a hyper parameter of some function on Euclidean distance, you must tune it using the SKLearn libraries for this purpose, e.g. GridSearch. This will allow you to iteratively examine a space of hyper parameter combinations, and then select the best tuning. Lastly you can stack cross validation on each of these steps to increase the confidence you have in selecting the right tunings. This is more or less the core of the value add of a data scientist, and should be where most of the time is spent.",2
fj5yf2x,fbfky4,"Generally, most distance measurements, that I'm aware of in data science, are Euclidean, including the epsilon. Of course, you're free to choose Manhattan, other Minkowski distance, or any distance measure of your choosing.",1
fh5hpqs,f1dizs,"If I am understanding correctly, you want to define a function that accepts a time series and returns the lowest non-outlier value. This task really isn‚Äôt well suited to machine learning in the common sci kit learn sense. Specifically, this is because you do operations on the inputs and don‚Äôt necessarily return something from the set of inputs, rather either a class or continuous number.

If you want to make this current process more robust, you could winsorize your data at a couple standard deviations, then take the bottom. Alternatively take the second percentile.",1
fh5mtfh,f1dizs,"Thank you for that input, kind of what I was suspecting, but wasn‚Äôt entirely sure.",1
fgi8ngf,eylu4p,"Well, this blog post was originally posted on Neuraxio‚Äôs blog. It is very aggressive (or even toxic) marketing, I guess.",0
fgl9y7e,eylu4p,It has never been submitted to r/scikit_learn before.,1
fglaho0,eylu4p,"I know for sure that it was posted in /r/deeplearning earlier and this post was cross-posted there as well.

Also, I am convinced that some judgements about scikit-learn are merely too strong and others are controversal. For example, the blogpost says that joblib is bad because it is not able to serialize some objects. The funny fact is that joblib is based on pickle which is the best serialization facility in Python. Numpy, SciPy, and Pandas support pickling. If a library does not support pickleable objects, it is an issue of the library not joblib.",1
fftgv14,ev1as7,"If you have a well defined tree, it will be deterministic, so depending on the structure, this could be conquered using a simple for loop or recursion. SKLearn allows you to save and load models via pickle, but provides no easy mechanism for loading a tree from outside. Really though, all you should need in the simplest form is a large stack of ifs and returns.",1
ffumz33,ev1as7,I don‚Äôt know if I understood your answer correctly. Do you mean by ‚Äûa large stack of ifs and returns‚Äú that I should hard code my tree?,1
ffuthk7,ev1as7,"PMML is like an XML or YAML format, so if you can read this into some data structure you could iterate over that with a loop or recursion. But if your tree is small or you don‚Äôt feel comfortable writing a loop like that, an if tree will do precisely the same thing.",1
ffww1x9,ev1as7,Thank you very much :),1
fdo92j7,em9fxf,"&gt; I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc. For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

This could be called a type of ensemble learning, but I wouldn't recommend this, especially to a beginner. Instead, for each model, you should look at model.score(Xtest, ytest). The higher the better. That allows you to choose just one model.

&gt; The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

That can happen. It's not really an error. But presumably the score() will be low, and you can reject that model.

&gt; MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

That can happen. It might indicate bad hyperparameters. For now, I would suggest to just reject that model.

&gt; I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Most people who talk about NFL don't have a clue, and you can ignore them. You don't need to test all parameters and methods, but it's good to test a few. When you know more, you'll start to understand which models and hyperparameters are relevant for you to test.

My recommendation is to follow any tutorial that walks through sklearn with a specific dataset, eg the Titanic dataset. Don't try to program anything on your own before doing this. I recommend Andrew Ng's ML course on Coursera.

Finally, I recommend r/MLQuestions and r/learnmachinelearning rather than r/scikit_learn.",2
fdq8sjz,em9fxf,"Thank you for your gold advices. Really appreciated. I'll check accuracy score, pick the best one and follow all materials out there to improve my ML journey. Thanks",1
fc0d721,effb98,"In a famous 1996 paper, David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. ... There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
by Aur√©lien G√©ron",3
fc0f7vz,effb98,"Thank you. Yeah I have checked the book by Aurelien Geron, but can you still guide me where will I find exactly what Im looking for?",0
fc07b6i,effb98,"For algorithms try LinearRegression/Ridge (the easiest to interperate/know how they work), RandomForestRegressor or if you don't mind another package, Xgboost (which will most likely perform the best or have similar performance to the random forest, but is difficult to interperate)

Your data seems to have 1 or more rows for 1 patient, all these algorithms i proposed can only take in one row to predict some value. This means you will need to pre process/prepare the data to work well with these algorithms. I would recommend the use of aggregate functions (count of past visits, average wait times, etc.) to convert these many rows into 1. You could also lag your data so you have columns like 'time last visit', 'time of second to last visit' etc. The day of week and time of day could also make for some interesting features for you to play with :)

To train the model you will use the fit(X, y) argument once you declare your model. X is the features (time of visit, day of week, time since last visit, disease, ...) and y is what you predict (time of action).

You will also want to split your dataset into a training and test dataset. The train is what you fit the algorithm with and the test is used to see if the algorithm will actually work on new unseen data.

This is quite a brief high level overview of everything you will need to do, i also recommend tackling the titanic and housing prices datasets on kaggle.com as there are some fantastic solutions for you to learn from which is what started me off with machine learning :)

Hope this is helpful to you",1
fc0f3we,effb98,"Wow, that is a very detailed explanation. Thank you so much!! I wish i could give you a medal. 

Btw, how many features do you think I should include?",2
fc0vp3d,effb98,"Haha, no need for a medal, I'm happy enough to try and get someone into ML :)

Unfortunately there isn't really a fixed number of features that's guaranteed to work, and different algorithms might work better with certain combinations (e.g. The regression models don't work well with highly correlated features, so you will most likely need to remove them before modeling). I recommend having too many to start with (it's common to see datasets with 10-1000 features) and if needed, apply some sort of feature selection/regularization techniques to reduce the number used in the model :)

You'll never know if a feature is any good unless you try it!",1
fc1qa5g,effb98,"You should probably look into modeling this using a Poisson distribution, this is a pretty well studied problem in statistics. Here's an introductory blog post: [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459](https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459)",1
f8dxf0w,e0h8ao,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askprogramming] [Column transformer throwing away some features?](https://www.reddit.com/r/AskProgramming/comments/e0hbkw/column_transformer_throwing_away_some_features/)

&amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1
f8516p7,dyzwn9,"Hi, a few things. You should be using a classifier ‚ÄúMLP Classifier‚Äù to do a prediction that will give you a confidence as output. Unless you‚Äôre trying to predict the amount of rain the next day, in which case ~80% R2 would be excellent.

You are able to pass deep network shapes into your model, using the ‚Äúhidden_layer_sizes‚Äù argument. Similarly you can change ReLU to be the TanH or Sigmoid functions and the solver for how weight and bias changes are calculated. That is the extent of basic MLP tuning available, but is pretty robust for academic projects like this.

I *highly suggest* trying other modes like the Naive Bayes, SVM, Boosted Trees, Random Forest, and Logistic Regression. These also have large spaces of hyper parameters to tune and cross validate over. 

If you really want custom written network functions, you‚Äôre probably going to need to go the TensorFlow route which is much lower level.",1
f6xomto,dtnh3f,"I'm pretty new to machine learning and scikit-learn, but not a complete beginner. For a school assignment, I'm trying to optimise the hyperparameter alpha of ridge regression by cross validation using 30% of data as the holdout set.

Standard stuff, I've done this before. First I consistently got alpha=0 as my best alpha. Ok, the model is not overfitting, least squares solution is best. However, as seen in the image, my group then noticed that alpha=-85 gives the best performance on the test set.I am utterly confused. Ridge regression minimizes the cost function

||y - Xw||\^2 + alpha \* ||w||\^2,

so a negative alpha should lead to weights that are artifically inflated weight above the optimal least squares weights? So how come these weight give a better prediction on the test set, as given by mean square error as the metric?

I am completely confused. First I thought there was something wrong with my CV scheme, so I reduced the code to the minimum moving parts, using scikit-learn methods. Still, the problem persists, with several different random seeds. Code below:

    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=0.3)
    model=Ridge(alpha=0, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=0, mse:"")
    print(mse)
    model=Ridge(alpha=-85, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=-85, mse:"")
    print(mse)

This outputs:

     Alpha=0, mse:
    
     12.49668153434439
    
     Alpha=-85, mse:
    
     12.487210630035198

Anyone know what is going on here? At this point, I am seriously considering just implementing ridge regression from scratch, and seeing if the problem persists...

EDIT:

I implemented Ridge regression from scratch, without external libraries.

    def RidgeOwn(X, y, alpha):
        holder1=(np.dot(X.T, X))
        holder2=holder1+np.dot(alpha,np.identity(holder1.shape[0]))
        holder2=np.linalg.inv(holder2)
        holder3=np.dot(holder2, X.T)
        weights=np.dot(holder3, y)
        return weights

The same problem persists, the optimal alpha is now -24. No idea what is happening. There must be something very wrong with my input data, right? But I cant imagine what would cause this...",1
f6yc9kc,dtnh3f,"Antiregularization is a thing and can happen quite often, especially if you feature engineered using PCA or otherwise did other sorts of regularization. 

This gives a much more in depth look: https://stats.stackexchange.com/questions/328630/is-ridge-regression-useless-in-high-dimensions-n-ll-p-how-can-ols-fail-to",2
f6yzky6,dtnh3f,"Hmm... I read the link, fascinating. But I do not think everything is working as intended. No PCA, other regularisation or preprocessing has been done. And I very much doubt negative alpha is intented to be correct on an introductory course like this...
The question remains, what could be causing this... Must be something strange with the input?",1
f6z6tle,dtnh3f,"I can only speculate without running the analysis on my own machine, but your code looks correct at a glance. My only suggestions would to be to confirm you have no type errors, run some k-fold experiments iterating up the alpha, and try a lasso or other regularized OLS and see if the results are similar. It could simply be correct as it is rare you would even consider a negative L2 alpha, and your professor may even have bounded their hyper parameter search space at 0 as I would the majority of the time.",1
f6xn6gq,dtiy98,"Shuffle Split: This function create infinite iterations of your data where the test and train are randomly assigned at each iteration. Therefore, you can have a point that is repeated in testing or repeated in training. This could cause issues in ensuring you have a proper validation score if certain classes are over or under represented.

K-Fold: This function will shuffle your data, then draw boundaries every len(data)/(k+1) observations. You then use one of the k+1 resulting folds as a validation set and train on the rest.

That is, a shuffle split with a 20% test proportion will generate infinitely many randomly split 80/20 train/test buckets. A K=4 fold split will leave you with 5 buckets, of which you treat one as your 20% validation and iterate through 5 times to get a generalized score.

If you are doing classification with imbalanced classes, a stratified version of both exists which maintains the distribution of your response variable amongst the folds and splits.

Generally, K folds is seen as the proper method as you prevent any bias from random sampling.

Specifically to your question, you must never use the test set from a different train set in shuffle split because you are very likely to have the same data in your train and test which is a huge information leakage and invalidates any model performance metric.",1
f6yev8r,dtiy98,Thanks alot for your thoroughly explained comment. That was helpful.,1
f1x7n0w,dar9z6,"Depending on how many different commands you want to look at (and it‚Äôs a little unclear why SKLearn would be your package of choice rather than just rote statistics, correlation matrices, etc. ) you could encode the categories using a (I forget precisely the name but) OneHotEncoder or LabelEncoder. This would give you the ability to run multiclass classification algorithms.",2
f1yw2xd,dar9z6,"aha, I have multiple protocols and whole lot of commands... we may be talking about 50-60 major commands with most of usage and same amount of commands with less usage for like 5-6 thousand times a month.

I want this to ultimately be used as a way so we can identify bots and crawlers and stuff. im completely new to this and have been a back-end developer. now I want to improve on my reports of usage and gather more useful information. like some bot is hitting these types of commands in that interval, so this new thing working on same interval and commands with marginal differences, so this might be of the same family? and things like that. I don't even know what should be used for this!",2
f22pmhw,dar9z6,"If I am understanding correctly, this is an unsupervised problem (that you do not have a training set with what would be the correct predictions). In this case, you want to look into ""unsupervised learning"" and ""clustering"". This may give you the ability to segment your larger dataset into one that could potential give you information about different heterogenous groups in your data, but this hinges on you having more data than just a text command.

In this case though, unfortunately I would expect that it will be difficult to do what you want. A first step would be identifying existing traits of bots (i.e. tons of commands faster than a human could send from the same ip, etc.) and attempt to conquer the problem that way. To my knowledge SKLearn does not have many good ways to deal with this type of problem other than LabelPropogation but these are advanced techniques. You may have luck with programatic solutions more than machine learning. Good luck! Please feel free to message if you have questions.",2
ezsq8kj,d244x4,Post very short code.,2
excka8y,cs2urp,"You can just add the two arrays after one-hot encoding each separately.

I never heard of two-hot, that's interesting! Could you describe the situation more.

I guess the levels are the same for the two variables?",1
exctsp1,cs2urp,"Thank you.

To give you a better idea of what I'm dealing with, the data frame describes housing data; it's just one of the tutorial data sets on Kaggle. There are two categorical features to describe sections of the basement. They have the same categories, ""unfinished"", ""good living quarters"", etc, and the order does not matter. I could just one-hot encode both of them, but that would result in twice as many columns as I need for these features. For this example, it doesn't really matter, but I could see how knowing the best way to implement two-hot encoding could be useful in the future.

With one-hot encoding, a row of two features with five categories would look like this: 

[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]
while two-hot encoding would look like this: 

[0, 1, 1, 0, 0]",1
exea82m,cs2urp,"Thanks!

I suggested using +which could give the value 2 in some cases (so now ternary variables). If you do want binary you would combine with or. This discards information, potentially.",1
ewdbblx,cnnacy,RFE removes features based on the coefficient values.,1
ewermxj,cnl2a5,"I can‚Äôt remember what the output is for the kmeans cluster centers, but your slicing of ‚Äúcenters‚Äù may be referencing data with a numerical index or something.",2
ew3ud66,cmmbi5,"I‚Äôve seen bizarre slowdowns when intel MKL fights with other multiprocessing methods, basically making way more threads than can be efficiently processed. The severity of the problem seems to be OS and python installation dependent. Try shutting off MKL mutithreading by setting the environment variable ‚ÄúMKL_NUM_THREADS‚Äù to 1.",1
ew3vm3l,cmmbi5,I use openblas,1
evycli4,clpubv,by googling,1
evyddn0,clpubv,"search for Anomaly Detection in Network Traffic 


https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf",1
evydlnu,clpubv,"https://paperswithcode.com/paper/network-traffic-anomaly-detection-using 

here code with paper
using dl",1
evulpwt,cl88rf,Make sure your python build is the anaconda version and the sublime python path points to to Anaconda3 dir (JSON user settings of Sublime Text 3),1
ew0qyin,cl88rf,If nothing else works. Uninstall everything and install a clean version of Anaconda distribution and point Sublime to the proper path.,1
ew0xmus,cl88rf,I just restarted the whole computer but thanks for help,1
etq1qkx,cc2mxr,"if by numpy you mean pandas, then

&gt; X = dataframe.values[1:]  # features
&gt;
&gt; y = dataframe.values[0]  # target
 
should do the trick",1
etumb5d,cc2mxr,Thanks. So basic I'm almost embarrassed for asking.,2
etj4faq,cbm4g6,"What defines those data points (dots)? Is it just 2d location (x, y)? How did you label the original data points as blue or red?

&amp;#x200B;

If they are linearly separable (you could draw a straight line between them) then a linear SVM is probably your best bet. Still, it would be good to know more about the actual problem.",1
etrdap9,cbm4g6,"A point difines a line with its rho and theta (so 2D). There are left lines (red points) in an area of the graph, and right lines (blue points) in another part of the graph. So the blue and red points are clearly linearly separable, but this could be interesting if I'd like to only separate the blue form the red.  I want my application to be able to say ""this new point determines a left line"" or a right line, or none.",1
erxuz4x,c4q5i6,"Try
` from sklearn.cluster import kmeans `

I think the module is cluster and the class is kmeans.",1
erzcvj2,c4q5i6,"I finally successed. But in a way I've not expected:  


import  sklearn.cluster.k\_means\_  as kmean

&amp;#x200B;

kmeans = kmean.KMeans()  


Why should I do it in such indirect way...",1
eqko7d2,bylpjd,"Just use 

model_name.fit(w_cancelled_data_X, w_cancelled_data_Y)

Then for your active contracts:

model_name.predict(active_data_X)",1
eqkq7gb,bylpjd,"Wow! That did it. I was making this way more complicated than I needed to.   I really appreciate the help. Out of curiosity, do you have any good suggestions on how to export the full predictions to a csv? Thanks for the help again. I truly appreciate it.",2
eqkrf7t,bylpjd,"No problem. If you are familiar with the pandas library, I‚Äôd convert active_data to a DataFrame: 

import pandas as pd

active_data_X = pd.DataFrame(active_data) 

‚Äîside note: there‚Äôs a method that reads csv files and converts them to DataFrames, which goes as follows: active_data_X = pd.read_csv(‚Äúpath/filename.csv‚Äù)  ‚Äî

And then run:

active_data_X[‚Äòprediction‚Äô] = model_name.fit(active_data_X)

And then:

active_data_X.to_csv(‚Äúresults.csv‚Äù)

Edit: added the close ‚Äú",1
eqo01ik,bylpjd,I didn't even realise that `sklearn` worked with `DataFrame`s. TIL!,2
eo9wn6b,bqtxes,"&gt; `cluster_centers_` : array, [n_clusters, n_features]
Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these will not be consistent with labels_.

&gt; init : {‚Äòk-means++‚Äô, ‚Äòrandom‚Äô or an ndarray}
Method for initialization, defaults to ‚Äòk-means++‚Äô:
‚Äòk-means++‚Äô : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.
‚Äòrandom‚Äô: choose k observations (rows) at random from data for the initial centroids.
**If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.**",1
eobky9y,bqtxes,missed that.. thanks!,1
eoace2y,bqtxes,"You can save the cluster centers as a .npy file with np.save('filename', kmeans\_obj.	
cluster\_centers\_)

Then you can load with np.load('filename')",1
ela96qt,bevq9d,hmm maybe divide the image into pieces and run each thread for blob detection?,1
ekuebnz,bcwbv4,Bootstapping the models will get you the variance of the accuracy.,1
ekwf5jw,bcwbv4,How can I do that?,1
elc8rti,bcwbv4,"Thanks all got it by looping on bootstrap samples.
Thnaks",1
ekw9ipq,bcwbv4,"It's a binomial distribution, therefore if the accuracy is p, the variance is n*p*(1-p), where n is the size of the set used to calculate the accuracy.",1
ekrkuj8,bcbduy,"RFs in sklearn produce probabilities of labels, which can be used to calculate ROCs which in turn can let you ""tune"" the probability to minimise false positives.",1
ekmvywi,bbxi9t,"Just record the cluster means. Then when new data comes in, compare it to each mean and put it in the one with the closest mean.",2
ekp91di,bbxi9t,"Ah, yes, that will work, thanks!",1
eiy0oo4,b36h5a,U mean they are just different objects or oredict differently too?. What about feature importances in both?,1
eiy0uzg,b36h5a,"Sorry I didn't get your question ?. I mean a random forest with the conditions mentioned in my question should create exactly same trees ( estimators ), while training. What do you think about that ?.",1
eiy1c8r,b36h5a,It should. What i meant was how are u differentiating both the eatimators. On what basis?,1
eiy1x70,b36h5a,"I create a random forest (scikitlearn) with number of estimators 10(This with create 10 different decison trees). I can access each of theses estimators(dection trees).  

Code Example

model = RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019)¬†

Different estimtors/ decision trees can be accessed by 
model.estimators_[2] , model.estimators_[5] etc.


Should model.estimators_[1] ,model.estimators_[2] and model.estimators_[5] be same ?",1
eiy3cvm,b36h5a,They both will be different instances internally both might have the same structure. I have not really analysed the eatimators before. U can try to predict a few examples with each estimator to check,1
eiy4t6e,b36h5a,You don't need to predict. You can just visualize the the individual trees. I did that and they are different,1
eiy4u16,b36h5a,That is strange. Can u share the notebook over git or something?,1
eizifmi,b36h5a,"Why would it be creating the same decision tree over and over again?  That's not how a random forest works, even when you don't bootstrap and fix the seed.

If you created two forests with a fixed seed and the same parameters, they would be the same.",1
ehtkjb6,axgj2c,"great idea!

think you should add number and scale of factor vars, can greatly impact runtime 

&amp;#x200B;

also the amount of duplicate columns

&amp;#x200B;

i like it though... make it for r

&amp;#x200B;",1
ehuex1m,axgj2c,"Thank you for the feedback u/weightsandbayes we really appreciate it!  

Adding the variance was definitely something we were thinking about. I think this would a good avenue to explore, we should give it a try and I agree for many algos variance definitely plays a role. 

I haven‚Äôt used R in quite a while, what library should we tackle first in your opinion if we were to build a similar thing?",1
ehtqae2,axgj2c,"Fantastic idea. Could it be extended to other libraries? MLib, H2O, Keras, etc.",1
ehuev0d,axgj2c,"u/dj_ski_mask thanks for asking and you raise a great point.  
We built our library in a very scalable way, for example adding support for a new scikit learn algo is as simple as updating the config Json and running the model estimator.  
Adding a new algorithm here: [https://github.com/nathan-toubiana/scitime/blob/master/scitime/\_config.json](https://github.com/nathan-toubiana/scitime/blob/master/scitime/_config.json)   
And running the \_data function here: [https://github.com/nathan-toubiana/scitime#how-to-use-\_datapy-to-generate-data--fit-models](https://github.com/nathan-toubiana/scitime#how-to-use-_datapy-to-generate-data--fit-models)  


In principle nothing really prevents us from extending this to other libraries  
One challenge if we want to extend this outside Scikit-learn is that we are using scikit-learn specific methods throughout the code base.  
We would probably want to wrap our functions with a Library layer to specify what library we‚Äôre targeting. But It definitely can be done !",2
ehufxkc,axgj2c,By vars I meant variables haha ,1
ehuh90d,axgj2c,"Got it!

But by number of vars do you mean number of columns ? If so it's already factored in.

The distribution of each variable is also something we should look into.  
",1
ecs1ses,aahf76,"For gains, wouldn't it be simpler to just multiply by the gain factor?",2
ecs20oe,aahf76,"But then it will not be sign-conscious. If the gain is not a constant, but a function.",1
ect12sr,aahf76,"This sounds like it has nothing to do with sklearn.

Multiplication is typical especially for audio which I guess your signal might be - you could clarify.

If you know what you're doing then yes, you could use np.sign.",1
ect7y0q,aahf76,"Well not directly obviously, but this could be a common use-case in sklearn. Particularly I'm trying to apply a ""windowing function"" (not really a window, but similar principle) to the signal in such way that the window is slightly different for the y- than the y+ part. Thus for y &lt; 0 I want to apply window1, but for y &gt; 0 window2.",1
ec1cetm,a75oid,"You're not doing anything wrong. These metrics include the number of predictions of a class in the denominator so they divide by zero in this case. The NN, for some of your workloads, just never predicts that class. You could ensure that there are plenty of examples of that class in the training set. Apart from that all you can do is choose to report F and precision only when there are plenty of samples in the test set.",1
ec1d4g8,a75oid,"I wonder, why does it change though? As if MLPClassifier() fits a different fit every time I run the program. Even when it uses the same params? Yes, since MLPCLassifier() is implemented using stochastic gradient? But then, if I get ""errored results"" and ""non-errored results"", then are both valid? Or should I discard results that give this problem? The difference that occurs in prediction accuracy, when the error occurs, is quite drastic. 0.85 vs \~0.65 or even \~0.45, when this error pops up. So it ""seems"" that the MLPClassifier somehow fails occasionally, on this data set.",1
ec1gpdh,a75oid,"This is why we often report a cross-validated value, not just a single value. Yes, it could be that the classifier just fails sometimes. You can try different architectures and hyper parameters, especially initialisation and optimizer to see if it becomes more reliable, or try collecting more data.",1
ec1gv1e,a75oid,"What are you referring to with cross-validation? You mean that one ought to cross\_validate on the model, rather than fit the model a single time?",1
ec1ho19,a75oid,Yes,1
ec1hpud,a75oid,"But what does this help? If a cross\_validate ""fold"" produces the error, then it will be reflected to the averages of that cross\_validate? So even then one'd need to perhaps look for ""clean runs of cross\_validate""?",1
ec1u38k,a75oid,"It helps only in that if we report an accuracy value, it's an honest one (with error bounds if we like). It doesn't help to avoid the runs that go bad - for that see my earlier answer.",1
ec0d5su,a753f2,"Solution: I was accidentally using the MLPRegressor() class, when I need to use MLPClassifier() to get the output as multiclass.",1
ec1gxk7,a746h0,"The best way to estimate is to try running with a very small proportion of your data, say 0.00001, and then increasing by powers of 10 and seeing how it scales.

If you're seeing 10% CPU in a long-running computation, it could be because the job is disk-bound. Alternatively if you have 8 or 12 cores, and sklearn is using 1 and Task Manager reports it as a percentage of the total.",2
egyudwg,a73oda,"The C value, penalty, and random_state are the only ones I adjust. Random state is just stay consistent so really only the C value and penalty.",1
eagk9z2,a0c8dw,"It's more of an ML question than an SKLearn one. Training MLPs is still kind of a black art.

You can certainly try a different optimizer (I see they have LBGFS, Adam, SGD), momentum on/off, different learning rate, shuffle on. Also if you changed the default values for tol or anything else, try going back to the default. Apart from that, it might be worthwhile normalising your data columnwise (or it might not).",1
eagl4du,a0at1q,"Yes. You can check this out by creating and fitting the MLP and then looking at the `coefs_` attribute. First of all it doesn't crash! And second the `coefs_` are the same shape you'd expect with (7,).

It doesn't seem to be documented, so if there isn't a ticket in Github, you could raise one, either to remove this behaviour or to document it.",1
eahj20o,a09ukl,"train_test_split takes X and y, not Xy",1
eagw9zj,a06iin,"The amount of parameters you're attempting to optimise over is 10*2*11*3*3*2 = 3960. 

This isn't necessarily an issue straight up, but if your dataset is large and you are running a lot of decision trees with several tweaks to their parameters it's going to take some time.

You can try to scale back the parameters you're optimising over (bootstrap = false for example is one that can go...) as a start.",2
eahhbwl,a06iin,Thanks; Yeah I can try reducing the parameter matrix. so far I've reduced the cv and iterations,1
eahlc2t,a06iin,"I'd suggest lowering the n_estimators.

You mentioned this is a dataset that had an associated paper. What parameters did they use?",2
eai03lt,a06iin,"I tried lowering the n\_iter as much as possible and reducing cv but that didn't help. The paper I'm following didn't list parameters, and also they used WEKA not scikit learn so it doesn't really carry over",2
eajcy7s,a06iin,"&gt;I tried lowering the n\_iter as much as possible and reducing cv but that didn't help.

I ran your code using the MNIST and it ran fine, so that's not the problem. What's the size of the dataset your working with? Is your system actually using resources while running or has it hung?

&gt;The paper I'm following didn't list parameters

That's just bad research.

&gt;and also they used WEKA not scikit learn so it doesn't really carry over

The parameters are named differently and use different defaults, but you'd be able to replicate in sklearn... if they gave their parameter values... ;)

&amp;#x200B;",2
eajybfy,a06iin,"thank you for checking; I got a friend to run my code on his computer and it finished in like 1 min. So I think my laptop is just crappy, or I need to update something or reinstall it :P the size of the data set was small, like 131x 22 so I don't know what the problem is. I guess its just bad hardware",2
eak503r,a06iin,Seems like. :( ,1
ea2as8t,9yhcqq,"Do you mean having sklearn calculate coefficient p values, determine goodness of fit or out of sample validation?",2
ea3f85a,9yhcqq,"Whatever is meant when one says ""testing the model"".",0
ea5jij5,9yhcqq,"Hence my question. There are several different ""tests of a model"", ranging from ""which variables are related to the prediction"" to ""can the model's prediction be applied to new data"". 

Understanding what you want is half the battle.",1
ea85xvi,9yhcqq,"I'd imagine that library developers would develop a set of tests tied to any particular method. So if I use LinearRegression(), then somewhere near there are at least ""common tests"" for LinearRegression(). Also, since not all tests are suitable for all models, that should dictate how a library is designed.",1
ea87yte,9yhcqq,Have fun then!,1
ea1t6zy,9yhcqq,"Yes, have you looked at the manual? It's great.",0
ea1yi0j,9yhcqq,&gt;[https://scikit-learn.org/stable/auto\_examples/linear\_model/plot\_ols.html](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html) ?,1
ea287y4,9ygvsi,"The model is trained ""in-place"". Even when you call .fit(), you don't have to assign the model back to the variable you instantiated it in. Internally it just adjusts all the self.coeffs_  and self.intercept_ and all other model.params without creating a new model to hold them.",1
e9ve9uw,9sbqvm,"You can use an IDE like PyCharm, set up a break point in the \`fit\` method and then use the tools to step through the code one line at a time. You can set up different variables to watch and see them change.",1
dyirkg8,8emc3b,"I think there is not a definitive answer, but you can try with the following approaches:

1. You can just append the PageRank as another feature;
2. you can use the bag of words matrix to classify and then, use the classification prediction as an input for another model (which has PageRank as a feature);
3. You can classify both separately and use both predictions as inputs for a third classification model.

(2 and 3 are also called ensemble methods, look on the web for stacking).

Regards.


",2
dxnqyib,81xksg,"Use TFidfVectorizer instead, it has a mindf and maxdf args for that tuning. Just initialize mindf =3 to ignore terms that appear in less than 3 docs",1
dxo9maz,81xksg,Oh thank you very much! You can't imagine how many times I've read [the documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and I've always missed those parameters.,1
dxoduia,81xksg,You are very welcome ,1
dun5ic5,7z4qi7,"The first time you fit the model you had 4 possible classes. [1, 2, 3, 4] But when you called partial fit, you had the label for the new point be 5. Standard classifiers are not able to accommodate adding new possible classes after the initial setup.",1
dunbold,7z4qi7,[deleted],1
dunbtit,7z4qi7,"See this changed code, it works fine without fit() 
     
     from sklearn import neighbors, linear_model
     import numpy as np
  
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
        X.append([6, 9])
        y.append(5)
    
        X.append([10, 50])
        y.append(6)
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""log"")
    
        #sgd_clf.fit(X, y)
    
        #print(sgd_clf.predict([[6, 9]]))
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here",1
dunbvpo,7z4qi7,I understand i need to give class labels before initial fit().  But then fit uses all the new data and generates a model from scratch . what is the purpose of partial_fit then ? ,1
dviwuxk,7z4qi7,I found a work around by taking some random classes like np.arange(100) in first partial_fit(),1
dtvqk2p,7vw4ap,"Consider building a neural net, e.g. with [Keras](https://keras.io/). Since neural networks are basically trained via batch processing, they can be [tuned with new observation data](https://stats.stackexchange.com/questions/220169/how-do-i-add-more-training-data-to-pre-trained-deep-learning-net). Check out [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) for a good introduction.",1
dtxahq1,7vw4ap,"Thanks brylie, thats a wonderful book :) Could you please let me know if i can use scikit knn model along with keras ? Like i train with scikit knn model and just use batch training feature of keras. Is this possible ? I will read the book in detail today. Thanks again :) ",1
dtxbssd,7vw4ap,"Keras is much more comprehensive for deep learning than scikit-learn neural network. You might be able to create a hybrid model with KNN and a neural network. However, consider starting with one or the other, for simplicity.",1
dtxcf6f,7vw4ap,"which one can be trained fast ? I see that neural networks work slower. In that case, will scikit incremental learning would be a better fit ?
 http://scikit-learn.org/stable/modules/scaling_strategies.html

When exactly i need to use keras vs scikit neural networks if fast response back is my criteria than accuracy ? 

",1
dtxq4s6,7vw4ap,"I think there are at least two types of 'response time' to consider:
- training - where the model learns from provided data
- inference - where the model classifies or predicts based on new data

Classification time will likely be similar between models. Training time may vary greatly, but there are ways to speed up training in some cases. To which do you refer to when you say 'fast response back'?",1
dtzdf7h,7vw4ap,"Hi brylie, For training. If i get a newly registered user, i want to add his data to the already existing trained dataset and perform the inference step.

Currently, Just because of one/two users, i am training the whole dataset which is taking lot of time to go to inference step. 
",1
dtzepe9,7vw4ap,"Back to your original question, I am not sure it is possible to re-train a kNN classifier by adding new observations to an existing model. It is likely you will need to train a new model instance using all of the data (or a train/test split approach).

However, you can [take an existing Keras model and run its `fit()` method on new data](https://github.com/keras-team/keras/issues/1868#issuecomment-272078441), which will update the existing model. 

Keras may save time in the long run since you can update the model. There also ways to  speed up the Keras training, e.g. by using a GPU.",2
du04ux2,7vw4ap,thank u brylie :),1
du3dv06,7vw4ap,"Brylie, if i create a keras model, can i still be able to get the closest matches for my test data from trained dataset ? 

For example, with knn i have flexibility to get the closest matches to the test data with kneighbours function. Can this be possible with keras ?

Can i train a scikit model and then pass this trained model to keras ? 

 Sorry if my questions are naive. i am learning machine learning",1
dn4vbad,70m5l1,Check out the SciPy Cookbook revipe foe [Matplotlib: django](http://scipy-cookbook.readthedocs.io/items/Matplotlib_Django.html).,1
dn4vqr7,70m5l1,You might also like to [use Bokeh charts in a django project](https://www.hackerearth.com/practice/notes/bokeh-interactive-visualization-library-use-graph-with-django-template/).,1
dn4vx8g,70m5l1,See also: [django-chartflo](https://github.com/synw/django-chartflo),1
g8nvhog,j9q6bp,"If you are imagining an expert system (if/else decision tree) then try the `experta` package. If you need it to comply with the sklearn regressor or classifier API, then you can create a class with `__init__()`, `fit()` and `predict()` methods.

* `__init__` : instantiate your custom experta `KnowledgeEngine` instance.
* `fit` : calculate any stats on the dataset that you might need and adjust the parameters within the engine ( can just pass for most problems)
* `predict` : use `self.engine.declare()` to intake the state/feature data and return the output of engine.run() which must be a numpy array to work in an sklearn Pipeline",1
g8f2vm9,j8vepi,"The hyperparameters tol and max_inter are generally used to tell the model when to stop it's optimization for fitting the parameters of the logistic regression. Generally, tuning these parameters won't make a big difference to the predictive power of your model.

The penalty and C parameter deal with regularisation. This is a concept in ML used to prevent overfitting of your model. Overfitting happens when your model performs poorly on unseen/test data compared to your training data. This usually happens when your model is too complex (perfectly tuned models are flexible enough to learn something from your data but not too flexible to overfit/memorize your training data).

The penalty parameter lets you choose what type of regularisation you want to apply. There are two types of regularisation L1 (lasso) and L2 (ridge). L1 regularisation forces some of your parameters for the unimportant features to zero (these features are dropped from the model- LASSO does automated feature selection). L2 regularisation forces some of your parameters for the unimportant features to zero but not exactly 0 (these features are not dropped from the model). Elastic net is a combination of L1 and L2 regularisation.

The C parameter lets you choose how much regularisation you want to apply. In the case of L1 regularisation, more of it means more features dropped from the model. If you drop enough unimportant features from your model, your model will not overfit. If you drop too many features you may lose some important information to learn from your data.",1
g8mmeq2,j8vepi,I love this response. Thank you so much.,1
g7u6e66,j5pcm6,Try installing with Anaconda,1
g7waffb,j5pcm6,"So what I had to do:

\&gt;conda uninstall scikit-learn numpy scipy

\&gt;conda remove --force scikit-learn numpy scipy

\&gt;pip uninstall scikit-learn numpy scipy

\&gt;pip install -U scikit-learn numpy scipy --user",1
g785qxo,j2v11w,"If you looked at the source code you would have noticed the ""algorithm"" kwarg which specifies that if you don't choose the ""brute"" option it uses a tree data structure. This may introduce slight discrepancies between the fit model and the ""true"" distribution of the data.",3
g79d2mb,j2v11w,"Thanks for your prompt response, I got it overnight.

I agree. I like the speed of the default model, I‚Äôll time it but I reckon it‚Äôs there to be quicker than brute.

I suppose I am then doing my own little brute method with the literal Euclidean nearest neighbour.  The combination works for my purpose.

Nice one, thanks.

Edit: two thoughts.  First, my data is very entropic so maybe trees ain‚Äôt so good.  Second, my code still might be wrong so brute is a good check that I haven‚Äôt screwed up my trig.",1
g7a2j0z,j2v11w,"If there are ties (several neighbors with the same score) then scikit chooses the first one.

A trivial example to show how it works

    x = np.random.randint(0, 5, (20, 1))
    y = np.arange(len(x))

    knn = KNeighborsClassifier(1)
    knn.fit(x, y)
    knn.predict(x)",2
g7b8lel,j2v11w,"Good point.  It‚Äôs not that, I removed duplicates for my purposes.  Thanks though.  üôè",1
g7dd7oa,j2v11w,"Oops!  My method of removing duplicates just inherits the same behaviour.  

I‚Äôm going to add fuzz/noise to them that that will work for my purpose.  

Cheers for the stimulus üíì",1
g70358r,j1idtx,"Without knowing the kind of system you're running this on (RAM, amount of cores etc, etc) or the hyperparameters you're trying to optimise on, you're generating 300 trees. That could take a bit. 

Try decreasing the number of trees (say, 2?). If it's still hanging, message back and we can try to troubleshoot.",1
g2w82j9,igtkry,"Nope. Sklearn doesn't return F / t / p values. You can code your own, but otherwise just use statsmodels.",2
g0b6kn2,i3546z,"Not sklearn specifically, but graph theory could be useful.",1
fyn0442,hu6y83,The clusters and centroids returned are the final results.,2
fynnk7i,hu6y83,Gotcha. Me and a friend had kind of come to a realization that might be the case for something we are working on but we weren't too sure. Thanks for answering!,2
g77yegr,ht4ol1,Hi.  I‚Äôve made AUC from small numbers of points. Treat them like polygons and literally calculate the rectangles and triangles?,1
fx4lpgh,hm6td7,"I think many ppl have got 100% on it, because when I implemented a simple CNN I was able to get 94%.

As for the type of model, CNNs are usually quite reliable for image classification. However, you can still get an accuracy of 85+ using just Dense and Flatten layers without Convolutions and Poolings",2
fx71pfp,hm6td7,"Thanks. Do you have any links where it shows people getting 100%?

I was given this link where it shows the best performance is around 96-97%:
https://paperswithcode.com/sota/image-classification-on-fashion-mnist",1
fv3gbqa,hakk07,suggest to use seaborn with lmplot(),1
fv3wp7z,hakk07,Convert language frame using one hot encoder,1
fvoi9f5,h0w3xx,"I don't have a background in math/science either... That said, check out stat quest youtube videos. Yes, they do include math, but honestly, its really not possible to grasp these algorithms without at least some math and logic involved. So if you want to really understand these, then you'll be required to understand things like derivates and alpha, etc.. Good luck!",1
ftgob55,gziaus,"Remember that k-means is not a classification algorithm. It only does clustering. Also note that 53 + 47 = 67 + 33 = 100.

Now can you explain what's going on?",2
ftgxkkr,gziaus,"Yes. I thought about that. It looks like it was just comparing original and predicted labels side by side and based on their order of occurrence only two possible result was produced, N and 100-N. Then how to judge the accuracy of the algorithm? How to get the cluster quality details?",1
fqwpb6e,glb4cy,"[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) is a data preprocessing technique, not a model fitting technique. The questions you're asking make me think that you want to fit a polynomial model to your data, that's *not* what this Class is for.",2
frim32i,glb4cy,"Got is, thanks!",1
fpj3n47,gcvtsm,"You'll probably have to implement your own [estimator](https://scikit-learn.org/stable/developers/develop.html). Then you can just pass that to your search function.

Note that stepwise feature selection is generally poor and should be avoided (see e.g. [here](https://redd.it/ehmi22)).",1
fmtws0m,fx6kdy,"Just from experience that‚Äôs a little high for a distance metric based classifier. Generally there will tend to be some on borders between classifications that will flip flop based on the corpus of observations you have. If you share code we can check to make sure, but the best part of these types of fun datasets is finding surprising ways to get things to work.

I would suggest triple checking over fitting with a holdout set, but congrats on your good training!",1
fmu1rkf,fx6kdy,"Thank you, well actzally the code is quite short, so i share it here:

# Get data
from sklearn.datasets import load_digits()

dataset = load_digits()

X = dataset[""data""]

y = dataset[""target""]


# tsne
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, init=""pca"")

X_embedded = tsne.fit_transform(X)

# For viz
import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))

plt.scatter(x=X_embedded[:,0], y=X_embedded[:,1], c=y)

plt.show()

# Generating sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fitting
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV

knc_params = {""n_neighbors"":[3,5,7,15]}

knc = KNeighborsClassifier()

gs = GridSearchCV(knc, knc_params, cv=5)

gs.fit(X_train, y_train)

# Run model
pred = gs.predict(X_test)


from sklearn.metrics import classification_report

print(classification_report(y_test, pred))


from sklearn.metrics import confusion_matrix

cf = confusion_matrix(y_test, pred)



import seaborn as sns

sns.heatmap(cf, annot=True)



________



Thats it, i hope i dont have a made a typo, i am on the phone right now üòÖ
Any insights to whats ""wrong""?",1
fmvfmvr,fx6kdy,"I don't know, but I think you have to fit tsne only with train, and transform train and test without using the test dataset to train tsne. When you use tsne on X it has information that came from test. In that way you are trickering the algorithm using information you supposily don't know yet(test data)!",1
fmvsjcf,fx6kdy,"Thank you, you are right, thats a silly mistake from me. Sadly tsne does not have a transform method... So i guess thats it for now. But thank you very much, bow i know i should pay more attention l.",1
fm4sp5c,ft2pcp,Is this where the group  is set to -1 for outliers? Or do you mean to filter out part of the data before pushing the rest through DBScan?,1
fm7kg61,ft2pcp,"Yes -1 signifies noise. But I want it to not classify trunks, so that I can remove low vegetation, but not trunks. But I've not figured out how to get it work that way.",1
fm5r3pk,ft1kmb,"From OneHotEncoder docs:

&gt;	Parameters
categories‚Äòauto‚Äô or a list of array-like, default=‚Äôauto‚Äô
Categories (unique values) per feature:
‚Äòauto‚Äô : Determine categories automatically from the training data.
list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.

You passed a string, which would error out. Try passing a list of categories, or switch to auto.

As to your error message, that did not come from the above code, but similarly you need to read the docs and use ‚Äòcategories‚Äô",1
fm700ta,ft1kmb,"oh = OneHotEncoder(categories = X\[:, 3\])

X= oh.fit\_transform(X).toarray() 

gives out 

""too many indices"" error",1
fmd17bl,ft1kmb,"You need to read the docs. Please look at them and check for what it asks for, not the column but the categories in the ith column.",1
fm704im,ft1kmb,"oh = OneHotEncoder(categories = X\[3\])

X= oh.fit\_transform(X).toarray()

gives 1D array instead of 2D array",1
fl3s1gy,fm1oov,"I think what your code does is pass  cols_ordinal to an imputer, return it as colums, and in parallel pass cols_ordinal to ordinal encoder and return those as even more columns. So ordinal encoder does not get the imputed columns! For that you need to pipeline imputer end encoder, and pass them to columntransformer as one pipeline.",1
fkyov8e,flg6a1,"Well, you just have to follow the code. There isn't one place. Eg for this metric, you'll see that the variable `self.effective_metric_` is created. Use Ctrl-F to see that eventually it's passed in to a `BallTree` as the `metric` parameter. So, open `_ball_tree.py`, and repeat.",2
fl6ns3e,flg6a1,"If you‚Äôre lucky a paper is cited in a comment. If you‚Äôre luckier the code follows the paper.
Or translate the code yourself, test important chunks of it to be sure.",1
fk1xbio,ffx9zw,"Hi, I don‚Äôt know what kind of output you got from the code you attached there, but here is the almost exact same example from the Tfidf transformer page:

&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)
&gt;&gt;&gt; print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
&gt;&gt;&gt; print(X.shape)
(4, 9)

Found at this link:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

And so it seems the Tfidf transformer SHOULD take in your corpus (a list of lists, where each inner list is a list of strings) and compute the corresponding IDF",2
fjxqzkb,ff9602,"I cannot verify this without jumping in and testing, which I can do in a bit. However, my suspicion is that since the Random Forest can take a multi-class scenario, there must be some sort of aggregation on the various class-level accuracy scores (or any other of the simple performance measures) and in this case it seems to be the arithmetic mean. So, in your case of only True False labeling, the mean of a single class accuracy score is that class accuracy score.",1
fk15zy7,ff9602,"Thank you very much, have you done some testing by chance?",1
fk2m6vq,ff9602,I believe the graph is a ‚ÄúROC‚Äù graph which compares true positives to false positives,1
fj4aa4k,fbfky4,"SKLearn dbscan accepts many pairwise distance functions, Euclidean, Manhattan, etc. So it can be a Euclidean function, if you want, but it is not by necessity. 

[DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Distance Metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances)",2
fj4mosa,fbfky4,"That's not what I asked. I asked about the parameter epsilon, if the neighborhood distance it is restricting for core/border membership is a euclidean distance. i.e. how would you choose epsilon on a fresh dataset?",0
fj4qu8h,fbfky4,"Simply put, asking how you select and tune hyper parameters is a very different question than if said hyper parameter is ‚ÄúEuclidean‚Äù. In this case, the metric is Euclidean because it is measuring Euclidean distance (by default). If you would like to select a hyper parameter of some function on Euclidean distance, you must tune it using the SKLearn libraries for this purpose, e.g. GridSearch. This will allow you to iteratively examine a space of hyper parameter combinations, and then select the best tuning. Lastly you can stack cross validation on each of these steps to increase the confidence you have in selecting the right tunings. This is more or less the core of the value add of a data scientist, and should be where most of the time is spent.",2
fj5yf2x,fbfky4,"Generally, most distance measurements, that I'm aware of in data science, are Euclidean, including the epsilon. Of course, you're free to choose Manhattan, other Minkowski distance, or any distance measure of your choosing.",1
fh5hpqs,f1dizs,"If I am understanding correctly, you want to define a function that accepts a time series and returns the lowest non-outlier value. This task really isn‚Äôt well suited to machine learning in the common sci kit learn sense. Specifically, this is because you do operations on the inputs and don‚Äôt necessarily return something from the set of inputs, rather either a class or continuous number.

If you want to make this current process more robust, you could winsorize your data at a couple standard deviations, then take the bottom. Alternatively take the second percentile.",1
fh5mtfh,f1dizs,"Thank you for that input, kind of what I was suspecting, but wasn‚Äôt entirely sure.",1
fgi8ngf,eylu4p,"Well, this blog post was originally posted on Neuraxio‚Äôs blog. It is very aggressive (or even toxic) marketing, I guess.",0
fgl9y7e,eylu4p,It has never been submitted to r/scikit_learn before.,1
fglaho0,eylu4p,"I know for sure that it was posted in /r/deeplearning earlier and this post was cross-posted there as well.

Also, I am convinced that some judgements about scikit-learn are merely too strong and others are controversal. For example, the blogpost says that joblib is bad because it is not able to serialize some objects. The funny fact is that joblib is based on pickle which is the best serialization facility in Python. Numpy, SciPy, and Pandas support pickling. If a library does not support pickleable objects, it is an issue of the library not joblib.",1
fftgv14,ev1as7,"If you have a well defined tree, it will be deterministic, so depending on the structure, this could be conquered using a simple for loop or recursion. SKLearn allows you to save and load models via pickle, but provides no easy mechanism for loading a tree from outside. Really though, all you should need in the simplest form is a large stack of ifs and returns.",1
ffumz33,ev1as7,I don‚Äôt know if I understood your answer correctly. Do you mean by ‚Äûa large stack of ifs and returns‚Äú that I should hard code my tree?,1
ffuthk7,ev1as7,"PMML is like an XML or YAML format, so if you can read this into some data structure you could iterate over that with a loop or recursion. But if your tree is small or you don‚Äôt feel comfortable writing a loop like that, an if tree will do precisely the same thing.",1
ffww1x9,ev1as7,Thank you very much :),1
fdo92j7,em9fxf,"&gt; I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc. For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

This could be called a type of ensemble learning, but I wouldn't recommend this, especially to a beginner. Instead, for each model, you should look at model.score(Xtest, ytest). The higher the better. That allows you to choose just one model.

&gt; The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

That can happen. It's not really an error. But presumably the score() will be low, and you can reject that model.

&gt; MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

That can happen. It might indicate bad hyperparameters. For now, I would suggest to just reject that model.

&gt; I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Most people who talk about NFL don't have a clue, and you can ignore them. You don't need to test all parameters and methods, but it's good to test a few. When you know more, you'll start to understand which models and hyperparameters are relevant for you to test.

My recommendation is to follow any tutorial that walks through sklearn with a specific dataset, eg the Titanic dataset. Don't try to program anything on your own before doing this. I recommend Andrew Ng's ML course on Coursera.

Finally, I recommend r/MLQuestions and r/learnmachinelearning rather than r/scikit_learn.",2
fdq8sjz,em9fxf,"Thank you for your gold advices. Really appreciated. I'll check accuracy score, pick the best one and follow all materials out there to improve my ML journey. Thanks",1
fc0d721,effb98,"In a famous 1996 paper, David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. ... There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
by Aur√©lien G√©ron",3
fc0f7vz,effb98,"Thank you. Yeah I have checked the book by Aurelien Geron, but can you still guide me where will I find exactly what Im looking for?",0
fc07b6i,effb98,"For algorithms try LinearRegression/Ridge (the easiest to interperate/know how they work), RandomForestRegressor or if you don't mind another package, Xgboost (which will most likely perform the best or have similar performance to the random forest, but is difficult to interperate)

Your data seems to have 1 or more rows for 1 patient, all these algorithms i proposed can only take in one row to predict some value. This means you will need to pre process/prepare the data to work well with these algorithms. I would recommend the use of aggregate functions (count of past visits, average wait times, etc.) to convert these many rows into 1. You could also lag your data so you have columns like 'time last visit', 'time of second to last visit' etc. The day of week and time of day could also make for some interesting features for you to play with :)

To train the model you will use the fit(X, y) argument once you declare your model. X is the features (time of visit, day of week, time since last visit, disease, ...) and y is what you predict (time of action).

You will also want to split your dataset into a training and test dataset. The train is what you fit the algorithm with and the test is used to see if the algorithm will actually work on new unseen data.

This is quite a brief high level overview of everything you will need to do, i also recommend tackling the titanic and housing prices datasets on kaggle.com as there are some fantastic solutions for you to learn from which is what started me off with machine learning :)

Hope this is helpful to you",1
fc0f3we,effb98,"Wow, that is a very detailed explanation. Thank you so much!! I wish i could give you a medal. 

Btw, how many features do you think I should include?",2
fc0vp3d,effb98,"Haha, no need for a medal, I'm happy enough to try and get someone into ML :)

Unfortunately there isn't really a fixed number of features that's guaranteed to work, and different algorithms might work better with certain combinations (e.g. The regression models don't work well with highly correlated features, so you will most likely need to remove them before modeling). I recommend having too many to start with (it's common to see datasets with 10-1000 features) and if needed, apply some sort of feature selection/regularization techniques to reduce the number used in the model :)

You'll never know if a feature is any good unless you try it!",1
fc1qa5g,effb98,"You should probably look into modeling this using a Poisson distribution, this is a pretty well studied problem in statistics. Here's an introductory blog post: [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459](https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459)",1
f8dxf0w,e0h8ao,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askprogramming] [Column transformer throwing away some features?](https://www.reddit.com/r/AskProgramming/comments/e0hbkw/column_transformer_throwing_away_some_features/)

&amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1
f8516p7,dyzwn9,"Hi, a few things. You should be using a classifier ‚ÄúMLP Classifier‚Äù to do a prediction that will give you a confidence as output. Unless you‚Äôre trying to predict the amount of rain the next day, in which case ~80% R2 would be excellent.

You are able to pass deep network shapes into your model, using the ‚Äúhidden_layer_sizes‚Äù argument. Similarly you can change ReLU to be the TanH or Sigmoid functions and the solver for how weight and bias changes are calculated. That is the extent of basic MLP tuning available, but is pretty robust for academic projects like this.

I *highly suggest* trying other modes like the Naive Bayes, SVM, Boosted Trees, Random Forest, and Logistic Regression. These also have large spaces of hyper parameters to tune and cross validate over. 

If you really want custom written network functions, you‚Äôre probably going to need to go the TensorFlow route which is much lower level.",1
f6xomto,dtnh3f,"I'm pretty new to machine learning and scikit-learn, but not a complete beginner. For a school assignment, I'm trying to optimise the hyperparameter alpha of ridge regression by cross validation using 30% of data as the holdout set.

Standard stuff, I've done this before. First I consistently got alpha=0 as my best alpha. Ok, the model is not overfitting, least squares solution is best. However, as seen in the image, my group then noticed that alpha=-85 gives the best performance on the test set.I am utterly confused. Ridge regression minimizes the cost function

||y - Xw||\^2 + alpha \* ||w||\^2,

so a negative alpha should lead to weights that are artifically inflated weight above the optimal least squares weights? So how come these weight give a better prediction on the test set, as given by mean square error as the metric?

I am completely confused. First I thought there was something wrong with my CV scheme, so I reduced the code to the minimum moving parts, using scikit-learn methods. Still, the problem persists, with several different random seeds. Code below:

    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=0.3)
    model=Ridge(alpha=0, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=0, mse:"")
    print(mse)
    model=Ridge(alpha=-85, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=-85, mse:"")
    print(mse)

This outputs:

     Alpha=0, mse:
    
     12.49668153434439
    
     Alpha=-85, mse:
    
     12.487210630035198

Anyone know what is going on here? At this point, I am seriously considering just implementing ridge regression from scratch, and seeing if the problem persists...

EDIT:

I implemented Ridge regression from scratch, without external libraries.

    def RidgeOwn(X, y, alpha):
        holder1=(np.dot(X.T, X))
        holder2=holder1+np.dot(alpha,np.identity(holder1.shape[0]))
        holder2=np.linalg.inv(holder2)
        holder3=np.dot(holder2, X.T)
        weights=np.dot(holder3, y)
        return weights

The same problem persists, the optimal alpha is now -24. No idea what is happening. There must be something very wrong with my input data, right? But I cant imagine what would cause this...",1
f6yc9kc,dtnh3f,"Antiregularization is a thing and can happen quite often, especially if you feature engineered using PCA or otherwise did other sorts of regularization. 

This gives a much more in depth look: https://stats.stackexchange.com/questions/328630/is-ridge-regression-useless-in-high-dimensions-n-ll-p-how-can-ols-fail-to",2
f6yzky6,dtnh3f,"Hmm... I read the link, fascinating. But I do not think everything is working as intended. No PCA, other regularisation or preprocessing has been done. And I very much doubt negative alpha is intented to be correct on an introductory course like this...
The question remains, what could be causing this... Must be something strange with the input?",1
f6z6tle,dtnh3f,"I can only speculate without running the analysis on my own machine, but your code looks correct at a glance. My only suggestions would to be to confirm you have no type errors, run some k-fold experiments iterating up the alpha, and try a lasso or other regularized OLS and see if the results are similar. It could simply be correct as it is rare you would even consider a negative L2 alpha, and your professor may even have bounded their hyper parameter search space at 0 as I would the majority of the time.",1
f6xn6gq,dtiy98,"Shuffle Split: This function create infinite iterations of your data where the test and train are randomly assigned at each iteration. Therefore, you can have a point that is repeated in testing or repeated in training. This could cause issues in ensuring you have a proper validation score if certain classes are over or under represented.

K-Fold: This function will shuffle your data, then draw boundaries every len(data)/(k+1) observations. You then use one of the k+1 resulting folds as a validation set and train on the rest.

That is, a shuffle split with a 20% test proportion will generate infinitely many randomly split 80/20 train/test buckets. A K=4 fold split will leave you with 5 buckets, of which you treat one as your 20% validation and iterate through 5 times to get a generalized score.

If you are doing classification with imbalanced classes, a stratified version of both exists which maintains the distribution of your response variable amongst the folds and splits.

Generally, K folds is seen as the proper method as you prevent any bias from random sampling.

Specifically to your question, you must never use the test set from a different train set in shuffle split because you are very likely to have the same data in your train and test which is a huge information leakage and invalidates any model performance metric.",1
f6yev8r,dtiy98,Thanks alot for your thoroughly explained comment. That was helpful.,1
f1x7n0w,dar9z6,"Depending on how many different commands you want to look at (and it‚Äôs a little unclear why SKLearn would be your package of choice rather than just rote statistics, correlation matrices, etc. ) you could encode the categories using a (I forget precisely the name but) OneHotEncoder or LabelEncoder. This would give you the ability to run multiclass classification algorithms.",2
f1yw2xd,dar9z6,"aha, I have multiple protocols and whole lot of commands... we may be talking about 50-60 major commands with most of usage and same amount of commands with less usage for like 5-6 thousand times a month.

I want this to ultimately be used as a way so we can identify bots and crawlers and stuff. im completely new to this and have been a back-end developer. now I want to improve on my reports of usage and gather more useful information. like some bot is hitting these types of commands in that interval, so this new thing working on same interval and commands with marginal differences, so this might be of the same family? and things like that. I don't even know what should be used for this!",2
f22pmhw,dar9z6,"If I am understanding correctly, this is an unsupervised problem (that you do not have a training set with what would be the correct predictions). In this case, you want to look into ""unsupervised learning"" and ""clustering"". This may give you the ability to segment your larger dataset into one that could potential give you information about different heterogenous groups in your data, but this hinges on you having more data than just a text command.

In this case though, unfortunately I would expect that it will be difficult to do what you want. A first step would be identifying existing traits of bots (i.e. tons of commands faster than a human could send from the same ip, etc.) and attempt to conquer the problem that way. To my knowledge SKLearn does not have many good ways to deal with this type of problem other than LabelPropogation but these are advanced techniques. You may have luck with programatic solutions more than machine learning. Good luck! Please feel free to message if you have questions.",2
ezsq8kj,d244x4,Post very short code.,2
excka8y,cs2urp,"You can just add the two arrays after one-hot encoding each separately.

I never heard of two-hot, that's interesting! Could you describe the situation more.

I guess the levels are the same for the two variables?",1
exctsp1,cs2urp,"Thank you.

To give you a better idea of what I'm dealing with, the data frame describes housing data; it's just one of the tutorial data sets on Kaggle. There are two categorical features to describe sections of the basement. They have the same categories, ""unfinished"", ""good living quarters"", etc, and the order does not matter. I could just one-hot encode both of them, but that would result in twice as many columns as I need for these features. For this example, it doesn't really matter, but I could see how knowing the best way to implement two-hot encoding could be useful in the future.

With one-hot encoding, a row of two features with five categories would look like this: 

[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]
while two-hot encoding would look like this: 

[0, 1, 1, 0, 0]",1
exea82m,cs2urp,"Thanks!

I suggested using +which could give the value 2 in some cases (so now ternary variables). If you do want binary you would combine with or. This discards information, potentially.",1
ewdbblx,cnnacy,RFE removes features based on the coefficient values.,1
ewermxj,cnl2a5,"I can‚Äôt remember what the output is for the kmeans cluster centers, but your slicing of ‚Äúcenters‚Äù may be referencing data with a numerical index or something.",2
ew3ud66,cmmbi5,"I‚Äôve seen bizarre slowdowns when intel MKL fights with other multiprocessing methods, basically making way more threads than can be efficiently processed. The severity of the problem seems to be OS and python installation dependent. Try shutting off MKL mutithreading by setting the environment variable ‚ÄúMKL_NUM_THREADS‚Äù to 1.",1
ew3vm3l,cmmbi5,I use openblas,1
evycli4,clpubv,by googling,1
evyddn0,clpubv,"search for Anomaly Detection in Network Traffic 


https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf",1
evydlnu,clpubv,"https://paperswithcode.com/paper/network-traffic-anomaly-detection-using 

here code with paper
using dl",1
evulpwt,cl88rf,Make sure your python build is the anaconda version and the sublime python path points to to Anaconda3 dir (JSON user settings of Sublime Text 3),1
ew0qyin,cl88rf,If nothing else works. Uninstall everything and install a clean version of Anaconda distribution and point Sublime to the proper path.,1
ew0xmus,cl88rf,I just restarted the whole computer but thanks for help,1
etq1qkx,cc2mxr,"if by numpy you mean pandas, then

&gt; X = dataframe.values[1:]  # features
&gt;
&gt; y = dataframe.values[0]  # target
 
should do the trick",1
etumb5d,cc2mxr,Thanks. So basic I'm almost embarrassed for asking.,2
etj4faq,cbm4g6,"What defines those data points (dots)? Is it just 2d location (x, y)? How did you label the original data points as blue or red?

&amp;#x200B;

If they are linearly separable (you could draw a straight line between them) then a linear SVM is probably your best bet. Still, it would be good to know more about the actual problem.",1
etrdap9,cbm4g6,"A point difines a line with its rho and theta (so 2D). There are left lines (red points) in an area of the graph, and right lines (blue points) in another part of the graph. So the blue and red points are clearly linearly separable, but this could be interesting if I'd like to only separate the blue form the red.  I want my application to be able to say ""this new point determines a left line"" or a right line, or none.",1
erxuz4x,c4q5i6,"Try
` from sklearn.cluster import kmeans `

I think the module is cluster and the class is kmeans.",1
erzcvj2,c4q5i6,"I finally successed. But in a way I've not expected:  


import  sklearn.cluster.k\_means\_  as kmean

&amp;#x200B;

kmeans = kmean.KMeans()  


Why should I do it in such indirect way...",1
eqko7d2,bylpjd,"Just use 

model_name.fit(w_cancelled_data_X, w_cancelled_data_Y)

Then for your active contracts:

model_name.predict(active_data_X)",1
eqkq7gb,bylpjd,"Wow! That did it. I was making this way more complicated than I needed to.   I really appreciate the help. Out of curiosity, do you have any good suggestions on how to export the full predictions to a csv? Thanks for the help again. I truly appreciate it.",2
eqkrf7t,bylpjd,"No problem. If you are familiar with the pandas library, I‚Äôd convert active_data to a DataFrame: 

import pandas as pd

active_data_X = pd.DataFrame(active_data) 

‚Äîside note: there‚Äôs a method that reads csv files and converts them to DataFrames, which goes as follows: active_data_X = pd.read_csv(‚Äúpath/filename.csv‚Äù)  ‚Äî

And then run:

active_data_X[‚Äòprediction‚Äô] = model_name.fit(active_data_X)

And then:

active_data_X.to_csv(‚Äúresults.csv‚Äù)

Edit: added the close ‚Äú",1
eqo01ik,bylpjd,I didn't even realise that `sklearn` worked with `DataFrame`s. TIL!,2
eo9wn6b,bqtxes,"&gt; `cluster_centers_` : array, [n_clusters, n_features]
Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these will not be consistent with labels_.

&gt; init : {‚Äòk-means++‚Äô, ‚Äòrandom‚Äô or an ndarray}
Method for initialization, defaults to ‚Äòk-means++‚Äô:
‚Äòk-means++‚Äô : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.
‚Äòrandom‚Äô: choose k observations (rows) at random from data for the initial centroids.
**If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.**",1
eobky9y,bqtxes,missed that.. thanks!,1
eoace2y,bqtxes,"You can save the cluster centers as a .npy file with np.save('filename', kmeans\_obj.	
cluster\_centers\_)

Then you can load with np.load('filename')",1
ela96qt,bevq9d,hmm maybe divide the image into pieces and run each thread for blob detection?,1
ekuebnz,bcwbv4,Bootstapping the models will get you the variance of the accuracy.,1
ekwf5jw,bcwbv4,How can I do that?,1
elc8rti,bcwbv4,"Thanks all got it by looping on bootstrap samples.
Thnaks",1
ekw9ipq,bcwbv4,"It's a binomial distribution, therefore if the accuracy is p, the variance is n*p*(1-p), where n is the size of the set used to calculate the accuracy.",1
ekrkuj8,bcbduy,"RFs in sklearn produce probabilities of labels, which can be used to calculate ROCs which in turn can let you ""tune"" the probability to minimise false positives.",1
ekmvywi,bbxi9t,"Just record the cluster means. Then when new data comes in, compare it to each mean and put it in the one with the closest mean.",2
ekp91di,bbxi9t,"Ah, yes, that will work, thanks!",1
eiy0oo4,b36h5a,U mean they are just different objects or oredict differently too?. What about feature importances in both?,1
eiy0uzg,b36h5a,"Sorry I didn't get your question ?. I mean a random forest with the conditions mentioned in my question should create exactly same trees ( estimators ), while training. What do you think about that ?.",1
eiy1c8r,b36h5a,It should. What i meant was how are u differentiating both the eatimators. On what basis?,1
eiy1x70,b36h5a,"I create a random forest (scikitlearn) with number of estimators 10(This with create 10 different decison trees). I can access each of theses estimators(dection trees).  

Code Example

model = RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019)¬†

Different estimtors/ decision trees can be accessed by 
model.estimators_[2] , model.estimators_[5] etc.


Should model.estimators_[1] ,model.estimators_[2] and model.estimators_[5] be same ?",1
eiy3cvm,b36h5a,They both will be different instances internally both might have the same structure. I have not really analysed the eatimators before. U can try to predict a few examples with each estimator to check,1
eiy4t6e,b36h5a,You don't need to predict. You can just visualize the the individual trees. I did that and they are different,1
eiy4u16,b36h5a,That is strange. Can u share the notebook over git or something?,1
eizifmi,b36h5a,"Why would it be creating the same decision tree over and over again?  That's not how a random forest works, even when you don't bootstrap and fix the seed.

If you created two forests with a fixed seed and the same parameters, they would be the same.",1
ehtkjb6,axgj2c,"great idea!

think you should add number and scale of factor vars, can greatly impact runtime 

&amp;#x200B;

also the amount of duplicate columns

&amp;#x200B;

i like it though... make it for r

&amp;#x200B;",1
ehuex1m,axgj2c,"Thank you for the feedback u/weightsandbayes we really appreciate it!  

Adding the variance was definitely something we were thinking about. I think this would a good avenue to explore, we should give it a try and I agree for many algos variance definitely plays a role. 

I haven‚Äôt used R in quite a while, what library should we tackle first in your opinion if we were to build a similar thing?",1
ehtqae2,axgj2c,"Fantastic idea. Could it be extended to other libraries? MLib, H2O, Keras, etc.",1
ehuev0d,axgj2c,"u/dj_ski_mask thanks for asking and you raise a great point.  
We built our library in a very scalable way, for example adding support for a new scikit learn algo is as simple as updating the config Json and running the model estimator.  
Adding a new algorithm here: [https://github.com/nathan-toubiana/scitime/blob/master/scitime/\_config.json](https://github.com/nathan-toubiana/scitime/blob/master/scitime/_config.json)   
And running the \_data function here: [https://github.com/nathan-toubiana/scitime#how-to-use-\_datapy-to-generate-data--fit-models](https://github.com/nathan-toubiana/scitime#how-to-use-_datapy-to-generate-data--fit-models)  


In principle nothing really prevents us from extending this to other libraries  
One challenge if we want to extend this outside Scikit-learn is that we are using scikit-learn specific methods throughout the code base.  
We would probably want to wrap our functions with a Library layer to specify what library we‚Äôre targeting. But It definitely can be done !",2
ehufxkc,axgj2c,By vars I meant variables haha ,1
ehuh90d,axgj2c,"Got it!

But by number of vars do you mean number of columns ? If so it's already factored in.

The distribution of each variable is also something we should look into.  
",1
ecs1ses,aahf76,"For gains, wouldn't it be simpler to just multiply by the gain factor?",2
ecs20oe,aahf76,"But then it will not be sign-conscious. If the gain is not a constant, but a function.",1
ect12sr,aahf76,"This sounds like it has nothing to do with sklearn.

Multiplication is typical especially for audio which I guess your signal might be - you could clarify.

If you know what you're doing then yes, you could use np.sign.",1
ect7y0q,aahf76,"Well not directly obviously, but this could be a common use-case in sklearn. Particularly I'm trying to apply a ""windowing function"" (not really a window, but similar principle) to the signal in such way that the window is slightly different for the y- than the y+ part. Thus for y &lt; 0 I want to apply window1, but for y &gt; 0 window2.",1
ec1cetm,a75oid,"You're not doing anything wrong. These metrics include the number of predictions of a class in the denominator so they divide by zero in this case. The NN, for some of your workloads, just never predicts that class. You could ensure that there are plenty of examples of that class in the training set. Apart from that all you can do is choose to report F and precision only when there are plenty of samples in the test set.",1
ec1d4g8,a75oid,"I wonder, why does it change though? As if MLPClassifier() fits a different fit every time I run the program. Even when it uses the same params? Yes, since MLPCLassifier() is implemented using stochastic gradient? But then, if I get ""errored results"" and ""non-errored results"", then are both valid? Or should I discard results that give this problem? The difference that occurs in prediction accuracy, when the error occurs, is quite drastic. 0.85 vs \~0.65 or even \~0.45, when this error pops up. So it ""seems"" that the MLPClassifier somehow fails occasionally, on this data set.",1
ec1gpdh,a75oid,"This is why we often report a cross-validated value, not just a single value. Yes, it could be that the classifier just fails sometimes. You can try different architectures and hyper parameters, especially initialisation and optimizer to see if it becomes more reliable, or try collecting more data.",1
ec1gv1e,a75oid,"What are you referring to with cross-validation? You mean that one ought to cross\_validate on the model, rather than fit the model a single time?",1
ec1ho19,a75oid,Yes,1
ec1hpud,a75oid,"But what does this help? If a cross\_validate ""fold"" produces the error, then it will be reflected to the averages of that cross\_validate? So even then one'd need to perhaps look for ""clean runs of cross\_validate""?",1
ec1u38k,a75oid,"It helps only in that if we report an accuracy value, it's an honest one (with error bounds if we like). It doesn't help to avoid the runs that go bad - for that see my earlier answer.",1
ec0d5su,a753f2,"Solution: I was accidentally using the MLPRegressor() class, when I need to use MLPClassifier() to get the output as multiclass.",1
ec1gxk7,a746h0,"The best way to estimate is to try running with a very small proportion of your data, say 0.00001, and then increasing by powers of 10 and seeing how it scales.

If you're seeing 10% CPU in a long-running computation, it could be because the job is disk-bound. Alternatively if you have 8 or 12 cores, and sklearn is using 1 and Task Manager reports it as a percentage of the total.",2
egyudwg,a73oda,"The C value, penalty, and random_state are the only ones I adjust. Random state is just stay consistent so really only the C value and penalty.",1
eagk9z2,a0c8dw,"It's more of an ML question than an SKLearn one. Training MLPs is still kind of a black art.

You can certainly try a different optimizer (I see they have LBGFS, Adam, SGD), momentum on/off, different learning rate, shuffle on. Also if you changed the default values for tol or anything else, try going back to the default. Apart from that, it might be worthwhile normalising your data columnwise (or it might not).",1
eagl4du,a0at1q,"Yes. You can check this out by creating and fitting the MLP and then looking at the `coefs_` attribute. First of all it doesn't crash! And second the `coefs_` are the same shape you'd expect with (7,).

It doesn't seem to be documented, so if there isn't a ticket in Github, you could raise one, either to remove this behaviour or to document it.",1
eahj20o,a09ukl,"train_test_split takes X and y, not Xy",1
eagw9zj,a06iin,"The amount of parameters you're attempting to optimise over is 10*2*11*3*3*2 = 3960. 

This isn't necessarily an issue straight up, but if your dataset is large and you are running a lot of decision trees with several tweaks to their parameters it's going to take some time.

You can try to scale back the parameters you're optimising over (bootstrap = false for example is one that can go...) as a start.",2
eahhbwl,a06iin,Thanks; Yeah I can try reducing the parameter matrix. so far I've reduced the cv and iterations,1
eahlc2t,a06iin,"I'd suggest lowering the n_estimators.

You mentioned this is a dataset that had an associated paper. What parameters did they use?",2
eai03lt,a06iin,"I tried lowering the n\_iter as much as possible and reducing cv but that didn't help. The paper I'm following didn't list parameters, and also they used WEKA not scikit learn so it doesn't really carry over",2
eajcy7s,a06iin,"&gt;I tried lowering the n\_iter as much as possible and reducing cv but that didn't help.

I ran your code using the MNIST and it ran fine, so that's not the problem. What's the size of the dataset your working with? Is your system actually using resources while running or has it hung?

&gt;The paper I'm following didn't list parameters

That's just bad research.

&gt;and also they used WEKA not scikit learn so it doesn't really carry over

The parameters are named differently and use different defaults, but you'd be able to replicate in sklearn... if they gave their parameter values... ;)

&amp;#x200B;",2
eajybfy,a06iin,"thank you for checking; I got a friend to run my code on his computer and it finished in like 1 min. So I think my laptop is just crappy, or I need to update something or reinstall it :P the size of the data set was small, like 131x 22 so I don't know what the problem is. I guess its just bad hardware",2
eak503r,a06iin,Seems like. :( ,1
ea2as8t,9yhcqq,"Do you mean having sklearn calculate coefficient p values, determine goodness of fit or out of sample validation?",2
ea3f85a,9yhcqq,"Whatever is meant when one says ""testing the model"".",0
ea5jij5,9yhcqq,"Hence my question. There are several different ""tests of a model"", ranging from ""which variables are related to the prediction"" to ""can the model's prediction be applied to new data"". 

Understanding what you want is half the battle.",1
ea85xvi,9yhcqq,"I'd imagine that library developers would develop a set of tests tied to any particular method. So if I use LinearRegression(), then somewhere near there are at least ""common tests"" for LinearRegression(). Also, since not all tests are suitable for all models, that should dictate how a library is designed.",1
ea87yte,9yhcqq,Have fun then!,1
ea1t6zy,9yhcqq,"Yes, have you looked at the manual? It's great.",0
ea1yi0j,9yhcqq,&gt;[https://scikit-learn.org/stable/auto\_examples/linear\_model/plot\_ols.html](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html) ?,1
ea287y4,9ygvsi,"The model is trained ""in-place"". Even when you call .fit(), you don't have to assign the model back to the variable you instantiated it in. Internally it just adjusts all the self.coeffs_  and self.intercept_ and all other model.params without creating a new model to hold them.",1
e9ve9uw,9sbqvm,"You can use an IDE like PyCharm, set up a break point in the \`fit\` method and then use the tools to step through the code one line at a time. You can set up different variables to watch and see them change.",1
dyirkg8,8emc3b,"I think there is not a definitive answer, but you can try with the following approaches:

1. You can just append the PageRank as another feature;
2. you can use the bag of words matrix to classify and then, use the classification prediction as an input for another model (which has PageRank as a feature);
3. You can classify both separately and use both predictions as inputs for a third classification model.

(2 and 3 are also called ensemble methods, look on the web for stacking).

Regards.


",2
dxnqyib,81xksg,"Use TFidfVectorizer instead, it has a mindf and maxdf args for that tuning. Just initialize mindf =3 to ignore terms that appear in less than 3 docs",1
dxo9maz,81xksg,Oh thank you very much! You can't imagine how many times I've read [the documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and I've always missed those parameters.,1
dxoduia,81xksg,You are very welcome ,1
dun5ic5,7z4qi7,"The first time you fit the model you had 4 possible classes. [1, 2, 3, 4] But when you called partial fit, you had the label for the new point be 5. Standard classifiers are not able to accommodate adding new possible classes after the initial setup.",1
dunbold,7z4qi7,[deleted],1
dunbtit,7z4qi7,"See this changed code, it works fine without fit() 
     
     from sklearn import neighbors, linear_model
     import numpy as np
  
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
        X.append([6, 9])
        y.append(5)
    
        X.append([10, 50])
        y.append(6)
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""log"")
    
        #sgd_clf.fit(X, y)
    
        #print(sgd_clf.predict([[6, 9]]))
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here",1
dunbvpo,7z4qi7,I understand i need to give class labels before initial fit().  But then fit uses all the new data and generates a model from scratch . what is the purpose of partial_fit then ? ,1
dviwuxk,7z4qi7,I found a work around by taking some random classes like np.arange(100) in first partial_fit(),1
dtvqk2p,7vw4ap,"Consider building a neural net, e.g. with [Keras](https://keras.io/). Since neural networks are basically trained via batch processing, they can be [tuned with new observation data](https://stats.stackexchange.com/questions/220169/how-do-i-add-more-training-data-to-pre-trained-deep-learning-net). Check out [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) for a good introduction.",1
dtxahq1,7vw4ap,"Thanks brylie, thats a wonderful book :) Could you please let me know if i can use scikit knn model along with keras ? Like i train with scikit knn model and just use batch training feature of keras. Is this possible ? I will read the book in detail today. Thanks again :) ",1
dtxbssd,7vw4ap,"Keras is much more comprehensive for deep learning than scikit-learn neural network. You might be able to create a hybrid model with KNN and a neural network. However, consider starting with one or the other, for simplicity.",1
dtxcf6f,7vw4ap,"which one can be trained fast ? I see that neural networks work slower. In that case, will scikit incremental learning would be a better fit ?
 http://scikit-learn.org/stable/modules/scaling_strategies.html

When exactly i need to use keras vs scikit neural networks if fast response back is my criteria than accuracy ? 

",1
dtxq4s6,7vw4ap,"I think there are at least two types of 'response time' to consider:
- training - where the model learns from provided data
- inference - where the model classifies or predicts based on new data

Classification time will likely be similar between models. Training time may vary greatly, but there are ways to speed up training in some cases. To which do you refer to when you say 'fast response back'?",1
dtzdf7h,7vw4ap,"Hi brylie, For training. If i get a newly registered user, i want to add his data to the already existing trained dataset and perform the inference step.

Currently, Just because of one/two users, i am training the whole dataset which is taking lot of time to go to inference step. 
",1
dtzepe9,7vw4ap,"Back to your original question, I am not sure it is possible to re-train a kNN classifier by adding new observations to an existing model. It is likely you will need to train a new model instance using all of the data (or a train/test split approach).

However, you can [take an existing Keras model and run its `fit()` method on new data](https://github.com/keras-team/keras/issues/1868#issuecomment-272078441), which will update the existing model. 

Keras may save time in the long run since you can update the model. There also ways to  speed up the Keras training, e.g. by using a GPU.",2
du04ux2,7vw4ap,thank u brylie :),1
du3dv06,7vw4ap,"Brylie, if i create a keras model, can i still be able to get the closest matches for my test data from trained dataset ? 

For example, with knn i have flexibility to get the closest matches to the test data with kneighbours function. Can this be possible with keras ?

Can i train a scikit model and then pass this trained model to keras ? 

 Sorry if my questions are naive. i am learning machine learning",1
dn4vbad,70m5l1,Check out the SciPy Cookbook revipe foe [Matplotlib: django](http://scipy-cookbook.readthedocs.io/items/Matplotlib_Django.html).,1
dn4vqr7,70m5l1,You might also like to [use Bokeh charts in a django project](https://www.hackerearth.com/practice/notes/bokeh-interactive-visualization-library-use-graph-with-django-template/).,1
dn4vx8g,70m5l1,See also: [django-chartflo](https://github.com/synw/django-chartflo),1
g8nvhog,j9q6bp,"If you are imagining an expert system (if/else decision tree) then try the `experta` package. If you need it to comply with the sklearn regressor or classifier API, then you can create a class with `__init__()`, `fit()` and `predict()` methods.

* `__init__` : instantiate your custom experta `KnowledgeEngine` instance.
* `fit` : calculate any stats on the dataset that you might need and adjust the parameters within the engine ( can just pass for most problems)
* `predict` : use `self.engine.declare()` to intake the state/feature data and return the output of engine.run() which must be a numpy array to work in an sklearn Pipeline",1
g8f2vm9,j8vepi,"The hyperparameters tol and max_inter are generally used to tell the model when to stop it's optimization for fitting the parameters of the logistic regression. Generally, tuning these parameters won't make a big difference to the predictive power of your model.

The penalty and C parameter deal with regularisation. This is a concept in ML used to prevent overfitting of your model. Overfitting happens when your model performs poorly on unseen/test data compared to your training data. This usually happens when your model is too complex (perfectly tuned models are flexible enough to learn something from your data but not too flexible to overfit/memorize your training data).

The penalty parameter lets you choose what type of regularisation you want to apply. There are two types of regularisation L1 (lasso) and L2 (ridge). L1 regularisation forces some of your parameters for the unimportant features to zero (these features are dropped from the model- LASSO does automated feature selection). L2 regularisation forces some of your parameters for the unimportant features to zero but not exactly 0 (these features are not dropped from the model). Elastic net is a combination of L1 and L2 regularisation.

The C parameter lets you choose how much regularisation you want to apply. In the case of L1 regularisation, more of it means more features dropped from the model. If you drop enough unimportant features from your model, your model will not overfit. If you drop too many features you may lose some important information to learn from your data.",1
g8mmeq2,j8vepi,I love this response. Thank you so much.,1
g7u6e66,j5pcm6,Try installing with Anaconda,1
g7waffb,j5pcm6,"So what I had to do:

\&gt;conda uninstall scikit-learn numpy scipy

\&gt;conda remove --force scikit-learn numpy scipy

\&gt;pip uninstall scikit-learn numpy scipy

\&gt;pip install -U scikit-learn numpy scipy --user",1
g785qxo,j2v11w,"If you looked at the source code you would have noticed the ""algorithm"" kwarg which specifies that if you don't choose the ""brute"" option it uses a tree data structure. This may introduce slight discrepancies between the fit model and the ""true"" distribution of the data.",5
g79d2mb,j2v11w,"Thanks for your prompt response, I got it overnight.

I agree. I like the speed of the default model, I‚Äôll time it but I reckon it‚Äôs there to be quicker than brute.

I suppose I am then doing my own little brute method with the literal Euclidean nearest neighbour.  The combination works for my purpose.

Nice one, thanks.

Edit: two thoughts.  First, my data is very entropic so maybe trees ain‚Äôt so good.  Second, my code still might be wrong so brute is a good check that I haven‚Äôt screwed up my trig.",1
g7a2j0z,j2v11w,"If there are ties (several neighbors with the same score) then scikit chooses the first one.

A trivial example to show how it works

    x = np.random.randint(0, 5, (20, 1))
    y = np.arange(len(x))

    knn = KNeighborsClassifier(1)
    knn.fit(x, y)
    knn.predict(x)",2
g7b8lel,j2v11w,"Good point.  It‚Äôs not that, I removed duplicates for my purposes.  Thanks though.  üôè",1
g7dd7oa,j2v11w,"Oops!  My method of removing duplicates just inherits the same behaviour.  

I‚Äôm going to add fuzz/noise to them that that will work for my purpose.  

Cheers for the stimulus üíì",1
g70358r,j1idtx,"Without knowing the kind of system you're running this on (RAM, amount of cores etc, etc) or the hyperparameters you're trying to optimise on, you're generating 300 trees. That could take a bit. 

Try decreasing the number of trees (say, 2?). If it's still hanging, message back and we can try to troubleshoot.",1
g2w82j9,igtkry,"Nope. Sklearn doesn't return F / t / p values. You can code your own, but otherwise just use statsmodels.",2
g0b6kn2,i3546z,"Not sklearn specifically, but graph theory could be useful.",1
fyn0442,hu6y83,The clusters and centroids returned are the final results.,2
fynnk7i,hu6y83,Gotcha. Me and a friend had kind of come to a realization that might be the case for something we are working on but we weren't too sure. Thanks for answering!,2
g77yegr,ht4ol1,Hi.  I‚Äôve made AUC from small numbers of points. Treat them like polygons and literally calculate the rectangles and triangles?,1
fx4lpgh,hm6td7,"I think many ppl have got 100% on it, because when I implemented a simple CNN I was able to get 94%.

As for the type of model, CNNs are usually quite reliable for image classification. However, you can still get an accuracy of 85+ using just Dense and Flatten layers without Convolutions and Poolings",2
fx71pfp,hm6td7,"Thanks. Do you have any links where it shows people getting 100%?

I was given this link where it shows the best performance is around 96-97%:
https://paperswithcode.com/sota/image-classification-on-fashion-mnist",1
fv3gbqa,hakk07,suggest to use seaborn with lmplot(),1
fv3wp7z,hakk07,Convert language frame using one hot encoder,1
fvoi9f5,h0w3xx,"I don't have a background in math/science either... That said, check out stat quest youtube videos. Yes, they do include math, but honestly, its really not possible to grasp these algorithms without at least some math and logic involved. So if you want to really understand these, then you'll be required to understand things like derivates and alpha, etc.. Good luck!",1
ftgob55,gziaus,"Remember that k-means is not a classification algorithm. It only does clustering. Also note that 53 + 47 = 67 + 33 = 100.

Now can you explain what's going on?",2
ftgxkkr,gziaus,"Yes. I thought about that. It looks like it was just comparing original and predicted labels side by side and based on their order of occurrence only two possible result was produced, N and 100-N. Then how to judge the accuracy of the algorithm? How to get the cluster quality details?",1
fqwpb6e,glb4cy,"[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) is a data preprocessing technique, not a model fitting technique. The questions you're asking make me think that you want to fit a polynomial model to your data, that's *not* what this Class is for.",2
frim32i,glb4cy,"Got is, thanks!",1
fpj3n47,gcvtsm,"You'll probably have to implement your own [estimator](https://scikit-learn.org/stable/developers/develop.html). Then you can just pass that to your search function.

Note that stepwise feature selection is generally poor and should be avoided (see e.g. [here](https://redd.it/ehmi22)).",1
fmtws0m,fx6kdy,"Just from experience that‚Äôs a little high for a distance metric based classifier. Generally there will tend to be some on borders between classifications that will flip flop based on the corpus of observations you have. If you share code we can check to make sure, but the best part of these types of fun datasets is finding surprising ways to get things to work.

I would suggest triple checking over fitting with a holdout set, but congrats on your good training!",1
fmu1rkf,fx6kdy,"Thank you, well actzally the code is quite short, so i share it here:

# Get data
from sklearn.datasets import load_digits()

dataset = load_digits()

X = dataset[""data""]

y = dataset[""target""]


# tsne
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, init=""pca"")

X_embedded = tsne.fit_transform(X)

# For viz
import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))

plt.scatter(x=X_embedded[:,0], y=X_embedded[:,1], c=y)

plt.show()

# Generating sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fitting
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV

knc_params = {""n_neighbors"":[3,5,7,15]}

knc = KNeighborsClassifier()

gs = GridSearchCV(knc, knc_params, cv=5)

gs.fit(X_train, y_train)

# Run model
pred = gs.predict(X_test)


from sklearn.metrics import classification_report

print(classification_report(y_test, pred))


from sklearn.metrics import confusion_matrix

cf = confusion_matrix(y_test, pred)



import seaborn as sns

sns.heatmap(cf, annot=True)



________



Thats it, i hope i dont have a made a typo, i am on the phone right now üòÖ
Any insights to whats ""wrong""?",1
fmvfmvr,fx6kdy,"I don't know, but I think you have to fit tsne only with train, and transform train and test without using the test dataset to train tsne. When you use tsne on X it has information that came from test. In that way you are trickering the algorithm using information you supposily don't know yet(test data)!",1
fmvsjcf,fx6kdy,"Thank you, you are right, thats a silly mistake from me. Sadly tsne does not have a transform method... So i guess thats it for now. But thank you very much, bow i know i should pay more attention l.",1
fm4sp5c,ft2pcp,Is this where the group  is set to -1 for outliers? Or do you mean to filter out part of the data before pushing the rest through DBScan?,1
fm7kg61,ft2pcp,"Yes -1 signifies noise. But I want it to not classify trunks, so that I can remove low vegetation, but not trunks. But I've not figured out how to get it work that way.",1
fm5r3pk,ft1kmb,"From OneHotEncoder docs:

&gt;	Parameters
categories‚Äòauto‚Äô or a list of array-like, default=‚Äôauto‚Äô
Categories (unique values) per feature:
‚Äòauto‚Äô : Determine categories automatically from the training data.
list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.

You passed a string, which would error out. Try passing a list of categories, or switch to auto.

As to your error message, that did not come from the above code, but similarly you need to read the docs and use ‚Äòcategories‚Äô",1
fm700ta,ft1kmb,"oh = OneHotEncoder(categories = X\[:, 3\])

X= oh.fit\_transform(X).toarray() 

gives out 

""too many indices"" error",1
fmd17bl,ft1kmb,"You need to read the docs. Please look at them and check for what it asks for, not the column but the categories in the ith column.",1
fm704im,ft1kmb,"oh = OneHotEncoder(categories = X\[3\])

X= oh.fit\_transform(X).toarray()

gives 1D array instead of 2D array",1
fl3s1gy,fm1oov,"I think what your code does is pass  cols_ordinal to an imputer, return it as colums, and in parallel pass cols_ordinal to ordinal encoder and return those as even more columns. So ordinal encoder does not get the imputed columns! For that you need to pipeline imputer end encoder, and pass them to columntransformer as one pipeline.",1
fkyov8e,flg6a1,"Well, you just have to follow the code. There isn't one place. Eg for this metric, you'll see that the variable `self.effective_metric_` is created. Use Ctrl-F to see that eventually it's passed in to a `BallTree` as the `metric` parameter. So, open `_ball_tree.py`, and repeat.",2
fl6ns3e,flg6a1,"If you‚Äôre lucky a paper is cited in a comment. If you‚Äôre luckier the code follows the paper.
Or translate the code yourself, test important chunks of it to be sure.",1
fk1xbio,ffx9zw,"Hi, I don‚Äôt know what kind of output you got from the code you attached there, but here is the almost exact same example from the Tfidf transformer page:

&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)
&gt;&gt;&gt; print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
&gt;&gt;&gt; print(X.shape)
(4, 9)

Found at this link:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

And so it seems the Tfidf transformer SHOULD take in your corpus (a list of lists, where each inner list is a list of strings) and compute the corresponding IDF",2
fjxqzkb,ff9602,"I cannot verify this without jumping in and testing, which I can do in a bit. However, my suspicion is that since the Random Forest can take a multi-class scenario, there must be some sort of aggregation on the various class-level accuracy scores (or any other of the simple performance measures) and in this case it seems to be the arithmetic mean. So, in your case of only True False labeling, the mean of a single class accuracy score is that class accuracy score.",1
fk15zy7,ff9602,"Thank you very much, have you done some testing by chance?",1
fk2m6vq,ff9602,I believe the graph is a ‚ÄúROC‚Äù graph which compares true positives to false positives,1
fj4aa4k,fbfky4,"SKLearn dbscan accepts many pairwise distance functions, Euclidean, Manhattan, etc. So it can be a Euclidean function, if you want, but it is not by necessity. 

[DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[Distance Metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances)",2
fj4mosa,fbfky4,"That's not what I asked. I asked about the parameter epsilon, if the neighborhood distance it is restricting for core/border membership is a euclidean distance. i.e. how would you choose epsilon on a fresh dataset?",0
fj4qu8h,fbfky4,"Simply put, asking how you select and tune hyper parameters is a very different question than if said hyper parameter is ‚ÄúEuclidean‚Äù. In this case, the metric is Euclidean because it is measuring Euclidean distance (by default). If you would like to select a hyper parameter of some function on Euclidean distance, you must tune it using the SKLearn libraries for this purpose, e.g. GridSearch. This will allow you to iteratively examine a space of hyper parameter combinations, and then select the best tuning. Lastly you can stack cross validation on each of these steps to increase the confidence you have in selecting the right tunings. This is more or less the core of the value add of a data scientist, and should be where most of the time is spent.",2
fj5yf2x,fbfky4,"Generally, most distance measurements, that I'm aware of in data science, are Euclidean, including the epsilon. Of course, you're free to choose Manhattan, other Minkowski distance, or any distance measure of your choosing.",1
fh5hpqs,f1dizs,"If I am understanding correctly, you want to define a function that accepts a time series and returns the lowest non-outlier value. This task really isn‚Äôt well suited to machine learning in the common sci kit learn sense. Specifically, this is because you do operations on the inputs and don‚Äôt necessarily return something from the set of inputs, rather either a class or continuous number.

If you want to make this current process more robust, you could winsorize your data at a couple standard deviations, then take the bottom. Alternatively take the second percentile.",1
fh5mtfh,f1dizs,"Thank you for that input, kind of what I was suspecting, but wasn‚Äôt entirely sure.",1
fgi8ngf,eylu4p,"Well, this blog post was originally posted on Neuraxio‚Äôs blog. It is very aggressive (or even toxic) marketing, I guess.",0
fgl9y7e,eylu4p,It has never been submitted to r/scikit_learn before.,1
fglaho0,eylu4p,"I know for sure that it was posted in /r/deeplearning earlier and this post was cross-posted there as well.

Also, I am convinced that some judgements about scikit-learn are merely too strong and others are controversal. For example, the blogpost says that joblib is bad because it is not able to serialize some objects. The funny fact is that joblib is based on pickle which is the best serialization facility in Python. Numpy, SciPy, and Pandas support pickling. If a library does not support pickleable objects, it is an issue of the library not joblib.",1
fftgv14,ev1as7,"If you have a well defined tree, it will be deterministic, so depending on the structure, this could be conquered using a simple for loop or recursion. SKLearn allows you to save and load models via pickle, but provides no easy mechanism for loading a tree from outside. Really though, all you should need in the simplest form is a large stack of ifs and returns.",1
ffumz33,ev1as7,I don‚Äôt know if I understood your answer correctly. Do you mean by ‚Äûa large stack of ifs and returns‚Äú that I should hard code my tree?,1
ffuthk7,ev1as7,"PMML is like an XML or YAML format, so if you can read this into some data structure you could iterate over that with a loop or recursion. But if your tree is small or you don‚Äôt feel comfortable writing a loop like that, an if tree will do precisely the same thing.",1
ffww1x9,ev1as7,Thank you very much :),1
fdo92j7,em9fxf,"&gt; I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc. For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

This could be called a type of ensemble learning, but I wouldn't recommend this, especially to a beginner. Instead, for each model, you should look at model.score(Xtest, ytest). The higher the better. That allows you to choose just one model.

&gt; The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

That can happen. It's not really an error. But presumably the score() will be low, and you can reject that model.

&gt; MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

That can happen. It might indicate bad hyperparameters. For now, I would suggest to just reject that model.

&gt; I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Most people who talk about NFL don't have a clue, and you can ignore them. You don't need to test all parameters and methods, but it's good to test a few. When you know more, you'll start to understand which models and hyperparameters are relevant for you to test.

My recommendation is to follow any tutorial that walks through sklearn with a specific dataset, eg the Titanic dataset. Don't try to program anything on your own before doing this. I recommend Andrew Ng's ML course on Coursera.

Finally, I recommend r/MLQuestions and r/learnmachinelearning rather than r/scikit_learn.",2
fdq8sjz,em9fxf,"Thank you for your gold advices. Really appreciated. I'll check accuracy score, pick the best one and follow all materials out there to improve my ML journey. Thanks",1
fc0d721,effb98,"In a famous 1996 paper, David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. ... There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
by Aur√©lien G√©ron",3
fc0f7vz,effb98,"Thank you. Yeah I have checked the book by Aurelien Geron, but can you still guide me where will I find exactly what Im looking for?",0
fc07b6i,effb98,"For algorithms try LinearRegression/Ridge (the easiest to interperate/know how they work), RandomForestRegressor or if you don't mind another package, Xgboost (which will most likely perform the best or have similar performance to the random forest, but is difficult to interperate)

Your data seems to have 1 or more rows for 1 patient, all these algorithms i proposed can only take in one row to predict some value. This means you will need to pre process/prepare the data to work well with these algorithms. I would recommend the use of aggregate functions (count of past visits, average wait times, etc.) to convert these many rows into 1. You could also lag your data so you have columns like 'time last visit', 'time of second to last visit' etc. The day of week and time of day could also make for some interesting features for you to play with :)

To train the model you will use the fit(X, y) argument once you declare your model. X is the features (time of visit, day of week, time since last visit, disease, ...) and y is what you predict (time of action).

You will also want to split your dataset into a training and test dataset. The train is what you fit the algorithm with and the test is used to see if the algorithm will actually work on new unseen data.

This is quite a brief high level overview of everything you will need to do, i also recommend tackling the titanic and housing prices datasets on kaggle.com as there are some fantastic solutions for you to learn from which is what started me off with machine learning :)

Hope this is helpful to you",1
fc0f3we,effb98,"Wow, that is a very detailed explanation. Thank you so much!! I wish i could give you a medal. 

Btw, how many features do you think I should include?",2
fc0vp3d,effb98,"Haha, no need for a medal, I'm happy enough to try and get someone into ML :)

Unfortunately there isn't really a fixed number of features that's guaranteed to work, and different algorithms might work better with certain combinations (e.g. The regression models don't work well with highly correlated features, so you will most likely need to remove them before modeling). I recommend having too many to start with (it's common to see datasets with 10-1000 features) and if needed, apply some sort of feature selection/regularization techniques to reduce the number used in the model :)

You'll never know if a feature is any good unless you try it!",1
fc1qa5g,effb98,"You should probably look into modeling this using a Poisson distribution, this is a pretty well studied problem in statistics. Here's an introductory blog post: [https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459](https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459)",1
f8dxf0w,e0h8ao,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/askprogramming] [Column transformer throwing away some features?](https://www.reddit.com/r/AskProgramming/comments/e0hbkw/column_transformer_throwing_away_some_features/)

&amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",1
f8516p7,dyzwn9,"Hi, a few things. You should be using a classifier ‚ÄúMLP Classifier‚Äù to do a prediction that will give you a confidence as output. Unless you‚Äôre trying to predict the amount of rain the next day, in which case ~80% R2 would be excellent.

You are able to pass deep network shapes into your model, using the ‚Äúhidden_layer_sizes‚Äù argument. Similarly you can change ReLU to be the TanH or Sigmoid functions and the solver for how weight and bias changes are calculated. That is the extent of basic MLP tuning available, but is pretty robust for academic projects like this.

I *highly suggest* trying other modes like the Naive Bayes, SVM, Boosted Trees, Random Forest, and Logistic Regression. These also have large spaces of hyper parameters to tune and cross validate over. 

If you really want custom written network functions, you‚Äôre probably going to need to go the TensorFlow route which is much lower level.",1
f6xomto,dtnh3f,"I'm pretty new to machine learning and scikit-learn, but not a complete beginner. For a school assignment, I'm trying to optimise the hyperparameter alpha of ridge regression by cross validation using 30% of data as the holdout set.

Standard stuff, I've done this before. First I consistently got alpha=0 as my best alpha. Ok, the model is not overfitting, least squares solution is best. However, as seen in the image, my group then noticed that alpha=-85 gives the best performance on the test set.I am utterly confused. Ridge regression minimizes the cost function

||y - Xw||\^2 + alpha \* ||w||\^2,

so a negative alpha should lead to weights that are artifically inflated weight above the optimal least squares weights? So how come these weight give a better prediction on the test set, as given by mean square error as the metric?

I am completely confused. First I thought there was something wrong with my CV scheme, so I reduced the code to the minimum moving parts, using scikit-learn methods. Still, the problem persists, with several different random seeds. Code below:

    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=0.3)
    model=Ridge(alpha=0, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=0, mse:"")
    print(mse)
    model=Ridge(alpha=-85, fit_intercept=True, normalize=False, copy_X=True, tol=0.001,     solver=""auto"")
    model=model.fit(train_X, train_y) 
    predicted=model.predict(val_X)
    mse=mean_squared_error(val_y, predicted)
    print(""Alpha=-85, mse:"")
    print(mse)

This outputs:

     Alpha=0, mse:
    
     12.49668153434439
    
     Alpha=-85, mse:
    
     12.487210630035198

Anyone know what is going on here? At this point, I am seriously considering just implementing ridge regression from scratch, and seeing if the problem persists...

EDIT:

I implemented Ridge regression from scratch, without external libraries.

    def RidgeOwn(X, y, alpha):
        holder1=(np.dot(X.T, X))
        holder2=holder1+np.dot(alpha,np.identity(holder1.shape[0]))
        holder2=np.linalg.inv(holder2)
        holder3=np.dot(holder2, X.T)
        weights=np.dot(holder3, y)
        return weights

The same problem persists, the optimal alpha is now -24. No idea what is happening. There must be something very wrong with my input data, right? But I cant imagine what would cause this...",1
f6yc9kc,dtnh3f,"Antiregularization is a thing and can happen quite often, especially if you feature engineered using PCA or otherwise did other sorts of regularization. 

This gives a much more in depth look: https://stats.stackexchange.com/questions/328630/is-ridge-regression-useless-in-high-dimensions-n-ll-p-how-can-ols-fail-to",2
f6yzky6,dtnh3f,"Hmm... I read the link, fascinating. But I do not think everything is working as intended. No PCA, other regularisation or preprocessing has been done. And I very much doubt negative alpha is intented to be correct on an introductory course like this...
The question remains, what could be causing this... Must be something strange with the input?",1
f6z6tle,dtnh3f,"I can only speculate without running the analysis on my own machine, but your code looks correct at a glance. My only suggestions would to be to confirm you have no type errors, run some k-fold experiments iterating up the alpha, and try a lasso or other regularized OLS and see if the results are similar. It could simply be correct as it is rare you would even consider a negative L2 alpha, and your professor may even have bounded their hyper parameter search space at 0 as I would the majority of the time.",1
f6xn6gq,dtiy98,"Shuffle Split: This function create infinite iterations of your data where the test and train are randomly assigned at each iteration. Therefore, you can have a point that is repeated in testing or repeated in training. This could cause issues in ensuring you have a proper validation score if certain classes are over or under represented.

K-Fold: This function will shuffle your data, then draw boundaries every len(data)/(k+1) observations. You then use one of the k+1 resulting folds as a validation set and train on the rest.

That is, a shuffle split with a 20% test proportion will generate infinitely many randomly split 80/20 train/test buckets. A K=4 fold split will leave you with 5 buckets, of which you treat one as your 20% validation and iterate through 5 times to get a generalized score.

If you are doing classification with imbalanced classes, a stratified version of both exists which maintains the distribution of your response variable amongst the folds and splits.

Generally, K folds is seen as the proper method as you prevent any bias from random sampling.

Specifically to your question, you must never use the test set from a different train set in shuffle split because you are very likely to have the same data in your train and test which is a huge information leakage and invalidates any model performance metric.",1
f6yev8r,dtiy98,Thanks alot for your thoroughly explained comment. That was helpful.,1
f1x7n0w,dar9z6,"Depending on how many different commands you want to look at (and it‚Äôs a little unclear why SKLearn would be your package of choice rather than just rote statistics, correlation matrices, etc. ) you could encode the categories using a (I forget precisely the name but) OneHotEncoder or LabelEncoder. This would give you the ability to run multiclass classification algorithms.",2
f1yw2xd,dar9z6,"aha, I have multiple protocols and whole lot of commands... we may be talking about 50-60 major commands with most of usage and same amount of commands with less usage for like 5-6 thousand times a month.

I want this to ultimately be used as a way so we can identify bots and crawlers and stuff. im completely new to this and have been a back-end developer. now I want to improve on my reports of usage and gather more useful information. like some bot is hitting these types of commands in that interval, so this new thing working on same interval and commands with marginal differences, so this might be of the same family? and things like that. I don't even know what should be used for this!",2
f22pmhw,dar9z6,"If I am understanding correctly, this is an unsupervised problem (that you do not have a training set with what would be the correct predictions). In this case, you want to look into ""unsupervised learning"" and ""clustering"". This may give you the ability to segment your larger dataset into one that could potential give you information about different heterogenous groups in your data, but this hinges on you having more data than just a text command.

In this case though, unfortunately I would expect that it will be difficult to do what you want. A first step would be identifying existing traits of bots (i.e. tons of commands faster than a human could send from the same ip, etc.) and attempt to conquer the problem that way. To my knowledge SKLearn does not have many good ways to deal with this type of problem other than LabelPropogation but these are advanced techniques. You may have luck with programatic solutions more than machine learning. Good luck! Please feel free to message if you have questions.",2
ezsq8kj,d244x4,Post very short code.,2
excka8y,cs2urp,"You can just add the two arrays after one-hot encoding each separately.

I never heard of two-hot, that's interesting! Could you describe the situation more.

I guess the levels are the same for the two variables?",1
exctsp1,cs2urp,"Thank you.

To give you a better idea of what I'm dealing with, the data frame describes housing data; it's just one of the tutorial data sets on Kaggle. There are two categorical features to describe sections of the basement. They have the same categories, ""unfinished"", ""good living quarters"", etc, and the order does not matter. I could just one-hot encode both of them, but that would result in twice as many columns as I need for these features. For this example, it doesn't really matter, but I could see how knowing the best way to implement two-hot encoding could be useful in the future.

With one-hot encoding, a row of two features with five categories would look like this: 

[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]
while two-hot encoding would look like this: 

[0, 1, 1, 0, 0]",1
exea82m,cs2urp,"Thanks!

I suggested using +which could give the value 2 in some cases (so now ternary variables). If you do want binary you would combine with or. This discards information, potentially.",1
ewdbblx,cnnacy,RFE removes features based on the coefficient values.,1
ewermxj,cnl2a5,"I can‚Äôt remember what the output is for the kmeans cluster centers, but your slicing of ‚Äúcenters‚Äù may be referencing data with a numerical index or something.",2
ew3ud66,cmmbi5,"I‚Äôve seen bizarre slowdowns when intel MKL fights with other multiprocessing methods, basically making way more threads than can be efficiently processed. The severity of the problem seems to be OS and python installation dependent. Try shutting off MKL mutithreading by setting the environment variable ‚ÄúMKL_NUM_THREADS‚Äù to 1.",1
ew3vm3l,cmmbi5,I use openblas,1
evycli4,clpubv,by googling,1
evyddn0,clpubv,"search for Anomaly Detection in Network Traffic 


https://www.gta.ufrj.br/~alvarenga/files/CPE826/Ahmed2016-Survey.pdf",1
evydlnu,clpubv,"https://paperswithcode.com/paper/network-traffic-anomaly-detection-using 

here code with paper
using dl",1
evulpwt,cl88rf,Make sure your python build is the anaconda version and the sublime python path points to to Anaconda3 dir (JSON user settings of Sublime Text 3),1
ew0qyin,cl88rf,If nothing else works. Uninstall everything and install a clean version of Anaconda distribution and point Sublime to the proper path.,1
ew0xmus,cl88rf,I just restarted the whole computer but thanks for help,1
etq1qkx,cc2mxr,"if by numpy you mean pandas, then

&gt; X = dataframe.values[1:]  # features
&gt;
&gt; y = dataframe.values[0]  # target
 
should do the trick",1
etumb5d,cc2mxr,Thanks. So basic I'm almost embarrassed for asking.,2
etj4faq,cbm4g6,"What defines those data points (dots)? Is it just 2d location (x, y)? How did you label the original data points as blue or red?

&amp;#x200B;

If they are linearly separable (you could draw a straight line between them) then a linear SVM is probably your best bet. Still, it would be good to know more about the actual problem.",1
etrdap9,cbm4g6,"A point difines a line with its rho and theta (so 2D). There are left lines (red points) in an area of the graph, and right lines (blue points) in another part of the graph. So the blue and red points are clearly linearly separable, but this could be interesting if I'd like to only separate the blue form the red.  I want my application to be able to say ""this new point determines a left line"" or a right line, or none.",1
erxuz4x,c4q5i6,"Try
` from sklearn.cluster import kmeans `

I think the module is cluster and the class is kmeans.",1
erzcvj2,c4q5i6,"I finally successed. But in a way I've not expected:  


import  sklearn.cluster.k\_means\_  as kmean

&amp;#x200B;

kmeans = kmean.KMeans()  


Why should I do it in such indirect way...",1
eqko7d2,bylpjd,"Just use 

model_name.fit(w_cancelled_data_X, w_cancelled_data_Y)

Then for your active contracts:

model_name.predict(active_data_X)",1
eqkq7gb,bylpjd,"Wow! That did it. I was making this way more complicated than I needed to.   I really appreciate the help. Out of curiosity, do you have any good suggestions on how to export the full predictions to a csv? Thanks for the help again. I truly appreciate it.",2
eqkrf7t,bylpjd,"No problem. If you are familiar with the pandas library, I‚Äôd convert active_data to a DataFrame: 

import pandas as pd

active_data_X = pd.DataFrame(active_data) 

‚Äîside note: there‚Äôs a method that reads csv files and converts them to DataFrames, which goes as follows: active_data_X = pd.read_csv(‚Äúpath/filename.csv‚Äù)  ‚Äî

And then run:

active_data_X[‚Äòprediction‚Äô] = model_name.fit(active_data_X)

And then:

active_data_X.to_csv(‚Äúresults.csv‚Äù)

Edit: added the close ‚Äú",1
eqo01ik,bylpjd,I didn't even realise that `sklearn` worked with `DataFrame`s. TIL!,2
eo9wn6b,bqtxes,"&gt; `cluster_centers_` : array, [n_clusters, n_features]
Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these will not be consistent with labels_.

&gt; init : {‚Äòk-means++‚Äô, ‚Äòrandom‚Äô or an ndarray}
Method for initialization, defaults to ‚Äòk-means++‚Äô:
‚Äòk-means++‚Äô : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.
‚Äòrandom‚Äô: choose k observations (rows) at random from data for the initial centroids.
**If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.**",1
eobky9y,bqtxes,missed that.. thanks!,1
eoace2y,bqtxes,"You can save the cluster centers as a .npy file with np.save('filename', kmeans\_obj.	
cluster\_centers\_)

Then you can load with np.load('filename')",1
ela96qt,bevq9d,hmm maybe divide the image into pieces and run each thread for blob detection?,1
ekuebnz,bcwbv4,Bootstapping the models will get you the variance of the accuracy.,1
ekwf5jw,bcwbv4,How can I do that?,1
elc8rti,bcwbv4,"Thanks all got it by looping on bootstrap samples.
Thnaks",1
ekw9ipq,bcwbv4,"It's a binomial distribution, therefore if the accuracy is p, the variance is n*p*(1-p), where n is the size of the set used to calculate the accuracy.",1
ekrkuj8,bcbduy,"RFs in sklearn produce probabilities of labels, which can be used to calculate ROCs which in turn can let you ""tune"" the probability to minimise false positives.",1
ekmvywi,bbxi9t,"Just record the cluster means. Then when new data comes in, compare it to each mean and put it in the one with the closest mean.",2
ekp91di,bbxi9t,"Ah, yes, that will work, thanks!",1
eiy0oo4,b36h5a,U mean they are just different objects or oredict differently too?. What about feature importances in both?,1
eiy0uzg,b36h5a,"Sorry I didn't get your question ?. I mean a random forest with the conditions mentioned in my question should create exactly same trees ( estimators ), while training. What do you think about that ?.",1
eiy1c8r,b36h5a,It should. What i meant was how are u differentiating both the eatimators. On what basis?,1
eiy1x70,b36h5a,"I create a random forest (scikitlearn) with number of estimators 10(This with create 10 different decison trees). I can access each of theses estimators(dection trees).  

Code Example

model = RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019)¬†

Different estimtors/ decision trees can be accessed by 
model.estimators_[2] , model.estimators_[5] etc.


Should model.estimators_[1] ,model.estimators_[2] and model.estimators_[5] be same ?",1
eiy3cvm,b36h5a,They both will be different instances internally both might have the same structure. I have not really analysed the eatimators before. U can try to predict a few examples with each estimator to check,1
eiy4t6e,b36h5a,You don't need to predict. You can just visualize the the individual trees. I did that and they are different,1
eiy4u16,b36h5a,That is strange. Can u share the notebook over git or something?,1
eizifmi,b36h5a,"Why would it be creating the same decision tree over and over again?  That's not how a random forest works, even when you don't bootstrap and fix the seed.

If you created two forests with a fixed seed and the same parameters, they would be the same.",1
ehtkjb6,axgj2c,"great idea!

think you should add number and scale of factor vars, can greatly impact runtime 

&amp;#x200B;

also the amount of duplicate columns

&amp;#x200B;

i like it though... make it for r

&amp;#x200B;",1
ehuex1m,axgj2c,"Thank you for the feedback u/weightsandbayes we really appreciate it!  

Adding the variance was definitely something we were thinking about. I think this would a good avenue to explore, we should give it a try and I agree for many algos variance definitely plays a role. 

I haven‚Äôt used R in quite a while, what library should we tackle first in your opinion if we were to build a similar thing?",1
ehtqae2,axgj2c,"Fantastic idea. Could it be extended to other libraries? MLib, H2O, Keras, etc.",1
ehuev0d,axgj2c,"u/dj_ski_mask thanks for asking and you raise a great point.  
We built our library in a very scalable way, for example adding support for a new scikit learn algo is as simple as updating the config Json and running the model estimator.  
Adding a new algorithm here: [https://github.com/nathan-toubiana/scitime/blob/master/scitime/\_config.json](https://github.com/nathan-toubiana/scitime/blob/master/scitime/_config.json)   
And running the \_data function here: [https://github.com/nathan-toubiana/scitime#how-to-use-\_datapy-to-generate-data--fit-models](https://github.com/nathan-toubiana/scitime#how-to-use-_datapy-to-generate-data--fit-models)  


In principle nothing really prevents us from extending this to other libraries  
One challenge if we want to extend this outside Scikit-learn is that we are using scikit-learn specific methods throughout the code base.  
We would probably want to wrap our functions with a Library layer to specify what library we‚Äôre targeting. But It definitely can be done !",2
ehufxkc,axgj2c,By vars I meant variables haha ,1
ehuh90d,axgj2c,"Got it!

But by number of vars do you mean number of columns ? If so it's already factored in.

The distribution of each variable is also something we should look into.  
",1
ecs1ses,aahf76,"For gains, wouldn't it be simpler to just multiply by the gain factor?",2
ecs20oe,aahf76,"But then it will not be sign-conscious. If the gain is not a constant, but a function.",1
ect12sr,aahf76,"This sounds like it has nothing to do with sklearn.

Multiplication is typical especially for audio which I guess your signal might be - you could clarify.

If you know what you're doing then yes, you could use np.sign.",1
ect7y0q,aahf76,"Well not directly obviously, but this could be a common use-case in sklearn. Particularly I'm trying to apply a ""windowing function"" (not really a window, but similar principle) to the signal in such way that the window is slightly different for the y- than the y+ part. Thus for y &lt; 0 I want to apply window1, but for y &gt; 0 window2.",1
ec1cetm,a75oid,"You're not doing anything wrong. These metrics include the number of predictions of a class in the denominator so they divide by zero in this case. The NN, for some of your workloads, just never predicts that class. You could ensure that there are plenty of examples of that class in the training set. Apart from that all you can do is choose to report F and precision only when there are plenty of samples in the test set.",1
ec1d4g8,a75oid,"I wonder, why does it change though? As if MLPClassifier() fits a different fit every time I run the program. Even when it uses the same params? Yes, since MLPCLassifier() is implemented using stochastic gradient? But then, if I get ""errored results"" and ""non-errored results"", then are both valid? Or should I discard results that give this problem? The difference that occurs in prediction accuracy, when the error occurs, is quite drastic. 0.85 vs \~0.65 or even \~0.45, when this error pops up. So it ""seems"" that the MLPClassifier somehow fails occasionally, on this data set.",1
ec1gpdh,a75oid,"This is why we often report a cross-validated value, not just a single value. Yes, it could be that the classifier just fails sometimes. You can try different architectures and hyper parameters, especially initialisation and optimizer to see if it becomes more reliable, or try collecting more data.",1
ec1gv1e,a75oid,"What are you referring to with cross-validation? You mean that one ought to cross\_validate on the model, rather than fit the model a single time?",1
ec1ho19,a75oid,Yes,1
ec1hpud,a75oid,"But what does this help? If a cross\_validate ""fold"" produces the error, then it will be reflected to the averages of that cross\_validate? So even then one'd need to perhaps look for ""clean runs of cross\_validate""?",1
ec1u38k,a75oid,"It helps only in that if we report an accuracy value, it's an honest one (with error bounds if we like). It doesn't help to avoid the runs that go bad - for that see my earlier answer.",1
ec0d5su,a753f2,"Solution: I was accidentally using the MLPRegressor() class, when I need to use MLPClassifier() to get the output as multiclass.",1
ec1gxk7,a746h0,"The best way to estimate is to try running with a very small proportion of your data, say 0.00001, and then increasing by powers of 10 and seeing how it scales.

If you're seeing 10% CPU in a long-running computation, it could be because the job is disk-bound. Alternatively if you have 8 or 12 cores, and sklearn is using 1 and Task Manager reports it as a percentage of the total.",2
egyudwg,a73oda,"The C value, penalty, and random_state are the only ones I adjust. Random state is just stay consistent so really only the C value and penalty.",1
