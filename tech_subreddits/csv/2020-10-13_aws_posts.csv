post_id,post_title,post_body,upvotes,subreddit,date
jajm2p,“No instance in Tag” on run command for patch baseline,Getting this when trying to get a patch baseline to run on an instance. Both the target and the instance have “Patch Group” as the tag key and the same value. Has anyone had any similar issues/aware of a potential issue?,1,aws,2020-10-13
jahzb5,How to consume a port exposed by a Fargate ECS docker,"Hi everyone,

I have a docker application that I want to consume through REST from Lambda.
I've created a Task definition for my application that expose a port in Fargate ECS.
But I can't find resource how to consume this exposed port from Lambda:

I want to consume this application through REST because it takes several seconds to load all of required data before application to be ready.
Of course, I don't want for my customers to have this wait.
I've put a Lambda before this application to manage basic stuff (quota, authorization).

Thanks in advance",0,aws,2020-10-13
jahpkg,RDS + ECS + SSH Tunnel in Docker,"First off, I'm sorry if this has been answered before and if my terminology is a little off, AWS is fairly new to me.  I can't seem to find much information about my specific issue and I'll try to give a run down.

First I've got an RDS instance that is only accessible through a ssh tunnel to the server.  Then the server is accessible through a route setup in route53, for example ""db.access"".  I can use the tunnel and psql to successfully connect to the database on my local machine.

I've got an ECS service that I used `ecs-cli local create --task-def-remote` to  get the task for my server.  When I run the docker container using `ecs-cli local up` it runs the service which tries to connect to my RDS database but fails with this log message:

Unable to obtain connection from database (jdbc:postgresql://db.access:5432/kit\_service) for user 'dbuseadmin\_admin': Connection to db.access:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.

I've tried numerous suggestions, adding my hosts file as volume, along with these guides:

[https://emmer.dev/blog/tunneling-a-database-connection-with-docker-compose/](https://emmer.dev/blog/tunneling-a-database-connection-with-docker-compose/)

[https://github.com/cagataygurturk/docker-ssh-tunnel](https://github.com/cagataygurturk/docker-ssh-tunnel)

Any chance, someone can tell me what I am doing wrong and why I can't connect to my database through the docker container?

Thanks in advance.",2,aws,2020-10-13
jadw8n,Serverless Swagger UI for API Gateway,"Quite long but detailed (I hope!) article on how to create an always up-to-date Swagger UI website for API Gateway, protected with Cognito authentication - missing part for sharing API documentation. For TL;DR see the repo with full example - link at the bottom.

[https://betterdev.blog/serverless-swagger-ui-for-api-gateway/](https://betterdev.blog/serverless-swagger-ui-for-api-gateway/)",6,aws,2020-10-13
jag7yo,Firewalls in AWS Environments,"We have been testing firewalls in AWS for use with a cloud-only solution in government regulated environments. The compliance standards are very broad general requirements (e.g. system shall use a firewall, system shall use IDS, etc.) There are no specific requirements as to what functionality these security tools must provide, so there is flexibility in selecting the various security tools.

All of our traffic is encrypted, and we cannot do HTTPS inspection using the firewalls. Based on this, it is my belief that the NGFW features will operate in a limited capability even though the vendors have claimed they provide significant protections on encrypted traffic they cannot see.

Since AWS has unique infrastructure as compared to what the network perimeter firewalls were originally designed for, my sense is that having a firewall from one of the mainstream vendors (e.g. Checkpoint, Palo Alto, etc.) sounds good but is not necessarily the best route.  

I am beginning to think that using the native AWS features such as security groups (for basic firewall functionality), AWS native security tools (e.g. Guard Duty, Inspector, Cloudtrail, etc.) is the best route. The one non-AWS tool that seems like it might be valuable would be one of the cloud specific tools to help monitor/manage the native AWS features (e.g. Dome 9, Prisma, Cloud One, etc.) 

Based on the above, I wanted to pose the following questions:

1. What am I missing?
2. Are you using one of the traditional network perimeter firewalls in your cloud solution/environment?
3. What have been your experiences with firewall type solutions in AWS?",1,aws,2020-10-13
jagnon,Cloning AD Joined Instances,"We currently have 3 Windows AD joined instances. A DC, A Web Server, and SQL. I would like to make a clone of these 3 instances and add them to the current environment. What is the best way of doing this? I read something about removing machines from the domain and running sysprep. 

However, I don't believe this is how you would clone a DC. I would be adding an additional DC to the domain. I know I can just spin up an additional DC and add it, which might be easier. Any help would be appreciated!",1,aws,2020-10-13
jaghu7,A History of AWS Services,,2,aws,2020-10-13
jadohz,Need advice for using RDS with Hetzner cloud,"Hey guys,

I want to use Hetzner cloud as their prices are rock solid (10x less than AWS for the same kind of performance). In order to deal with the lag my api will be on Hetzner germany and I will have an RDS in Frankfurt.

&amp;#x200B;

Because of that I will not be able to use a vpc since the api will not live inside AWS.

So do you think that is okay security wise to have an rds publically exposed so it can connect to the api and whitelist only the api ip address?

&amp;#x200B;

Many thanks",2,aws,2020-10-13
jadisj,Set alarm on sending rate SES,"Unfortunately one of our systems went into a loop and started sending replies to a out of office reply (internally luckily), but resulting in over 100k emails sent in the last 24 hours.

https://imgur.com/a/6sF0gTL

As a result that we've hit our limit for 24 hours, I requested a temporary limit increase to circumvent this, but I don't want to run into this issue again.

I tried setting up a cloudwatch alarm for my SES metrics, but those metrics don't correlate to what is in my SES Sending Statistics screen:
https://imgur.com/a/3vmJ4iH

What parameters do I need to put in here to get the same graphs?",1,aws,2020-10-13
jadi6c,EC2 VPN Data Transfer price question,"So I've setup a VPN on my EC2 server. How will Amazon calculate my data transfer usage? Say I downloaded 1GB of data via VPN, will they count it as 1GB IN (From internet to EC2) + 1GB OUT (From EC2 to my device)?",1,aws,2020-10-13
jacvi8,Ever Wonder About Snowflake and Databricks' Relationship with AWS as Allies &amp; Competitors?,"Last month the Future Data Conference had Ben Horowitz (partner at Andreessan-Horowitz) give his thoughts,[I thought it was interesting](https://medium.com/whispering-data/the-3-most-interesting-ideas-from-the-future-data-conference-caf849fa4958?source=friends_link&amp;sk=b6fbdd7e4bc35f9d1298c981a7f12820)!",2,aws,2020-10-13
ja5oyh,Can anyone recommend a good aws certification boot camp?,"Hi everyone, I'm currently working a part time job that leaves me with an extra 4 hours a day and I want to move into a good paying career in tech/it.

 I was considering going into an aws certification boot camp program. Can anyone recommend a good boot camp program? Which one of the certifications should I be aiming for? Just a little background: I have a bachelor's degree in cinema and I am very familiar with the servers/hardware that powers the aws cloud. I worked at the factory that built the aws servers.",2,aws,2020-10-13
ja9u5n,How can I make a real time dashboard (with a time lag of less than a minute)?,"  

Question- How can I make a real time dashboard (with a time lag of less than a minute)? 

My database is available in AWS Postgres RDS instance. I also made dashboards in Quicksight but it can refresh data hourly only and now we need real time dashboarding.",2,aws,2020-10-13
ja9zsb,Connect EC2 instances without IP (using DNS?),"I have now searched for a couple of hours for this without a result... this shows me that my approach is maybe flawed or I am using the wrong search terms...

I want multiple EC2 instances to work together. They are in a private subnet. They don't need to be reachable on the internet. 

Of course they can reach one another using their IPs. However, if I rebuild an instance it will get a new IP. Therefore I want them to have internal DNS names within their VPC so they can reach each other using ""server1.local"" or ""server2.local"".

Is that possible using AWS tools?",1,aws,2020-10-13
jaa0v2,sagemaker,"When i open sagemaker studio and jupyter lab, beside coding in notebooks and terminal is there option to open .py file and write scripts or just notebooks?",1,aws,2020-10-13
jab71r,Windows CIFS replacement options,"Our team utilizes several applications and systems that are being migrated to AWS. We also use an extensive amount of excel spreadsheets for Third-Party compliance report tracking. We have been told by our internal security group that once the transition of the systems servers and application servers are completed (December 2020) they will not open ports to our Windows file share

I am researching, frantically I might add, for a solution in AWS that fits the requirements.

* Ability to write (Alteryx) excel file outputs programmatically 
* Teams to be able to access files in a file directory type of interface (GUI) team is best described as technically autistic (They are highly skilled in working our specific systems, but new technology introduction is a painstaking, loooooong training process.)
* Read, write files as needed. Techs pull generated docs (Alteryx) from file system, research critical issues, add notes/ticket creations/etc and save back to file store.

&amp;#x200B;

It seems a simple list. I thought a EC2 with EFS mounted was our answer. Our data needs is small, under 200GB with the vast majority being archival (120GB historical archive) access for a small team of techs (30), on Windows machines.  


But the rub there was Windows, since EFS has no native windows support. 

I just found  Amazon FSx but haven't researched it enough.  


Any suggestions?",1,aws,2020-10-13
jaaqr8,API Gateway &lt;-&gt; EC2 connection,"Hi guys,

I'm trying to set up an EC2 with an application that I want to be consumed only by API Gateway and hides the endpoints from everything else.

I tried with a private VPC but then API Gateway doesn't find the endpoints. 

&amp;#x200B;

I'm really lost now, if API Gateway had a static Ip I could make a security group that blocked everything else and use a pubic VPC but doens't seem to be the case.

&amp;#x200B;

Any ideas I could try? 

&amp;#x200B;

Thanks!",1,aws,2020-10-13
ja8oca,Routing question,"Hello,

I'm wondering how to redirect :  
[domain.com/link1](https://domain.com/link1) \-&gt; ELB1  
[domain.com/link2](https://domain.com/link1) \-&gt; ELB2  
[domain.com/link](https://domain.com/link1)3 -&gt; ELB3  
...  


Route53 doesn't seem to allow this, or should an ELB (ELBa) should take care of this ?  
like:

[domain.com/link1](https://domain.com/link1) \-&gt; ELBa/link1 -&gt; ELB1  
[domain.com/link2](https://domain.com/link1) \-&gt; ELBa/link2 -&gt; ELB2  
[domain.com/link](https://domain.com/link1)3 -&gt; ELBa/link3 -&gt; ELB3  


How would you achieve this ?

Thank you",1,aws,2020-10-13
ja822t,We now have AWS TLS termination at the network load balancer. What’s the difference with ALB and NLB? Both can perform the same actions now?,,2,aws,2020-10-13
ja8hg3,Timestream vs. InfluxDB,"We have been using influxdb in the past as a time series database, however, I would be curious as to how it compares to Timestream? Apart from the obvious advantages of Timestream being a fully managed database, is there anything Timestream does that influxdb can’t do?
Thanks!",10,aws,2020-10-13
ja6uqm,How might you unit test a AWS step function or would you?,"I've been building out this high throughput orchestrator for our customer lately. The state machine itself follows a KISS approach and I take advantage of various service integration patterns with callbacks (e.g. sns:publish.waitForTaskToken, sqs:sendmessage.waitForTaskToken) to keep my states language simple.  

Our customer and the program leads have stressed unit tests and I've struggled conceptualizing how one would even begin to write unit tests for steps. A step function state machine (like the one i've explained above) is dependent on many external service integrations (e.g. microservices, sns, sqs, lambdas). From my keyboard, this calls for functional tests or integration testing versus unit testing.  

I've gone out and tried mock'd services like [__moto__](https://github.com/spulec/moto) but their stepfunction support isn't well developed out yet. It becomes even less useful when you're performing callbacks and nested executions of state machines. After awhile you'll find the support just isn't quite there yet for steps.  

As of now, I have defined a json schema for the input of my step function. This ensures that the input adheres strictly to a defined schema. However, the schema validation is done in my proxy (being a lambda) right before I start an execution of my state machine. This is all at runtime though so it doesn't really help me in satisfying this whole unit test requirement.  

I have contemplated on writing a boto3 app that instantiates and executes my state machine definition. I've even created mock'd interfaces for these various service integrations. This is alot of work though, it requires provisioning resources, pain in the ass to maintain and in theory is more of a functional test.  

Any ideas out there?",2,aws,2020-10-13
ja5dqq,AWS Serverless Well-Architected Framework Whitepaper,"A very good whitepaper to learn, and implement the Well-Architected framework for the AWS Serverless technologies - [https://d1.awsstatic.com/whitepapers/architecture/AWS-Serverless-Applications-Lens.pdf](https://d1.awsstatic.com/whitepapers/architecture/AWS-Serverless-Applications-Lens.pdf)",0,aws,2020-10-13
ja4e2t,AWS Direct Connect to Provide a Client with Private Access to Cloud SaaS,"Hey Guys,

We've got a client who wants to ""turn off access to the public internet"" and yet still connect to our application (a SaaS, accessed over the public internet). Its taken along battle with this client to get our application out of their infrastructure (totally non-standard, snowflake deployment) and into ours so going back on-prem isn't an option. I won't go into the whys because the post'll be too long.  


The best that I've come up with is that we migrate their application stack to AWS in its own VPC and assist them to create a Direct Connect virtual private interface on our VPC (because they'll need to use their DC presence, their IPs, their routers to advertise the new routes internally and probably create internal DNS for the host) and deliver the service that way.  


That supports their ""turn off access to the public internet"" requirement but reading through direct connect, I'm unsure if this is a feasable approach. Has anyone here done similar? am I barking up the wrong tree?  


Theres a very long history with this client so pls don't ask ""why didn't this come up in scoping the migration out of their premisis"" because... it did... Our entire company collectively rolled their eyes and cried ""oh ffs"" when this came through. Any thoughts would be appreciated.",1,aws,2020-10-13
ja48p9,What should I input for the GitHub App connection name? How would I create one for AWS? Thanks!,,1,aws,2020-10-13
ja4b5e,How can I know if someone read/hacked my EC2 instance's storage that's only accessible via SSH from the IP I had at the moment?,"I stupidly stored my email and password in plain text which a program I have on it needs to run (days ago), and about an hour or two ago there was a login attempt from somewhere in the US (I'm outside the country). I thought that it was encrypted by default since you need a private key to SSH in, but maybe it's not. Or it was an AWS employee, or something completely unrelated.",2,aws,2020-10-13
ja3qx9,What do you think of the new EC2 UI?,"AWS recently launched it's new EC2 UI which has been slowly rolling out to AWS accounts. 

For those of you that have experienced it:

What do you like about it?   
What do you dislike about it?

My pros:  
\- A much cleaner look and feel (love the compact view on EC2 instances)  
\- I think the new interface is way more informative with clearer tabs when selecting instances. For example, the storage tab is way more descriptive without needing a bunch of tabs open to pull the same information.  
\- Still really fast to interact with which is awesome when compared to other cloud providers' UI.  
My cons: 

\- Removal of popup overlay for settings changes. For example, It now brings you to a new page when you want to make a change any of the instance settings.   
This removes your ability to reference the full instance list and your selection in the background when making changes. (This seems to be a theme with the new UIs throughout AWS - wort of all the new Route53 interface when adding records to a zone).

Would love to hear your thought as well.",1,aws,2020-10-13
ja1p5f,CloudTrail Metrics for Athena,"I'd like to enable CloudTrail metrics for an Athena workgroup. I'm curious about if there are costs for doing so. I'm not really seeing any info about costs online anywhere, so if anyone has information it would be appreciated.",1,aws,2020-10-13
ja33sv,How to solve InvalidAccessKeyId Error?,"Preface:

\- created new AWS account  
\- created React Native project with Expo  
\- npm install -g aws-amplify  
\- done amplify configure &amp; amplify init &amp; amplify add auth  
\- I have a bucket that's public and content is also public  
\- Root is trying to download a .txt file (for testing) form the S3 bucket

When I do:

`let data = await Storage.get( 'TestingAWS.txt',{bucket:'amplify-awsvideoapp-dev-03350-deployment', region:'eu-west-2',})`

data comes back as a big URL that, when pasted into the browser, reads:

`InvalidAccessKeyIdThe AWS Access Key Id you provided does not exist in our records.undefined054CA2067F7770CC5SoqgZz7t/gqU8Evoy5bBmGDelnRhy0tX5kjy60lIaULeYugIZWb8CVJeKZioM3iLli9qaOgrz8=`

I've seen a few posts about this but I can't seem to fix the issue. All my credentials are correct.",1,aws,2020-10-13
ja2i76,UDP -&gt; Lambda possible options?,"I'm trying to figure out a way to push UDP packets into Lambda.  Seems Lambda can't listen on UDP and I'm not finding much in terms of other kits that will get the job done.

&amp;#x200B;

The goal here is to be serverless, so an EC2 instance breaks it.  


Any ideas?",1,aws,2020-10-13
ja0cdc,AWS Cloudformation repo structure,"Hey good folks of r/aws ! What does your infrastrucuture repo look like? Are there any great references  that you wish you had found before? 

I'm especially interested in how you structure templates for different regions/ clients.

Do your database scripts go into the same repo?",1,aws,2020-10-13
ja1k9g,I built a GitHub Action that deploys static sites to Cloudfront,,240,aws,2020-10-13
ja0z5e,"CloudWatch Application Insights offers new, improved user interface",,2,aws,2020-10-13
j9x1hk,Deployment template pattern suggestions,"I'm looking for suggestions of open sourced projects, libraries, patterns, etc. that could be of help.

I'm designing a multi-region processing pipeline using a combination of Lambda's and Fargate, that are glued together using EventBridge-to-EventBridge and then EventBridge-to-Fargate/Lambda as the mechanism for cross-region data transfer and processing invocation.  The EventBridge's will already be provisioned, and the pipeline deployment will need to plug into the EventBridge's by creating the necessary ""Rules"" for the pipeline to operate.

I'm also implementing a canary methodology into the deployment of the pipeline (i.e. turn on region A and make sure it's working correctly, turn on region B and make sure it's working correctly, etc.).

The thing is, I also want to develop a way to enable the other developers on my team to create additional processing pipelines that easily ""plug in"" to the architecture (i.e. add rules to the EventBridge's, etc.) and easily adopt the multi-region and canary deployment characteristics of this first pipeline I'm creating.  

Additionally, we're a Serverless shop.

One idea I had was to follow something along the lines of the following:

1. Provision the VPC's, EventBridge and other infrastructure components that will be common to all future pipelines across many regions.  There will be lite-weight infrastructure for doing data ingestion in many regions and then more heavy-weight infrastructure for managing the ingestion tasks and Elasticsearch in 2-3 regions.
2. Provision any applications (e.g. Lambda's) that will be common for all processing pipelines (for example, there will be one Lambda which pushes the post-processed data to Elasticsearch).  This might be in its own stack or combined with the stack that deploys the infrastructure in step #1
3. (This is where my thoughts get a bit fuzzier) Create something that can be relatively easily dropped into a future pipeline stack that will allow the developer to write a single stack but that will deploy the pipeline into multiple regions (based on the regions made available in the stack created by step #1) and deployed into those regions with a canary methodology.  


I was thinking step #3 above could be somehow facilitated with CDK?

Does anyone have any thoughts on how I can create this component that can be either ""dropped in"" or ""depended on"" to (relatively) easily add multi-region and canary-deployment characteristics to future pipeline stacks?

Thanks...",2,aws,2020-10-13
j9yy2b,Domain not working after setting it up in AWS,"I bought my domain name from GoDaddy and followed the following tutorials to set up my domain name in AWS:

 [https://www.freecodecamp.org/news/a-beginners-guide-on-how-to-host-a-static-site-with-aws/](https://www.freecodecamp.org/news/a-beginners-guide-on-how-to-host-a-static-site-with-aws/)

[https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html#root-domain-walkthrough-add-record-to-hostedzone](https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html#root-domain-walkthrough-add-record-to-hostedzone)

I did exactly what they said to do and then I checked with the GoDaddy support on what might be the error. They told me it has to do with the DNS from AWS or the IP address. I checked the DNS for my domain and subdomain and both come up as No Error on the DNS check. 

This is what I see when I check my domain and subdomain name:

&amp;#x200B;

https://preview.redd.it/jubbw7pq4qs51.png?width=1762&amp;format=png&amp;auto=webp&amp;s=8d314f9f5ec2cd047e3f635c7d5f3cdf8412f36f

I am not sure what the issue is. Can someone please help me out?",2,aws,2020-10-13
j9wywj,Creating an IAM policy with Cloudformation. Why is a Role/User/Group compulsory?,"Hello, 

I am creating an IAM Policy document that (at the moment) I need to attach to different roles for EC2 instances depending on the account (and the instance actually). 

Why do I need to specify a role/user/group (one of three) when creating the policy document through CF, but not through the console? Or (casting my mind back to when I was scripting some stuff with PowerShell), is this because there is a role required, but the console just creates it without you kind of knowing anyway?",2,aws,2020-10-13
j9vvjx,AWS IAM Overview," 

# IAM Introduction :

* [**AWS Identity and Access Management (IAM)**](https://docs.aws.amazon.com/iam/index.html) **is a service that allows you to manage users and their access to AWS.**
* **Your whole AWS security revolves around IAM.**
* **IAM is a global service. It’s not bound to any specific region.**
* **IAM allows you to control**
   * **Identity: who can use your AWS resources (Authentication)**
   * **Access: what resources they can use  (Authorization)**
* **When you create AWS account, that very first account is called as Root account.**
* **When you first start with AWS you get a root access key. Root access key provides complete access to your AWS account and you should never use them.**
* **So you basically need to delete your root access key.**
* **You can set up MFA (Multi-Factor Authentication) for extra security.**

## IAM Features :

* **IAM provides you with centralized control of your AWS account.**
* **It provides shared access to your AWS account.**
   * **You can grant other people access to your AWS account.**
* **It allows you to set Granular permissions.**
   * **You can grant specific permission to specific users. For example, you might allow a user access to only some set of services only.**
* **Identity Federation (including Active Directory, Facebook, LinkedIn etc)**
   * **Users can log in by using their company credentials without having an account with AWS.**
* **Multifactor Authentication**
   * **This is two-factor authentication. User not only need to provide password or access key but also code from physical devices such as android or ios smartphone.**
* **Support PCI DSS compliance.**
   * **IAM supports the processing, storage, and transmission of credit card data**
* **Provide application running on EC2 access to AWS resources.**
   * **You can have your application running on EC2 secure access to AWS resource.**
* **IAM integrates with almost every AWS service.**
   * **Since it’s a fundamental or core service that provides security, IAM integrates with almost all AWS services.**
* **Free to use**
   * **IAM is a completely free service to use.**

## IAM Components :

### Users

* **User is an AWS identity.**
* **User is basically a physical person.** 
* **This physical person will get an account in IAM.**
* **New users have no permission when first created.**

### Group

* **Group basically means a set of IAM users.**
* **Groups are basically defined by their function for example Developers, Admin etc.**
* **Instead of assigning the same permission to every other user individually, you can assign that permission to group and add all the users in that group.**
* **All the users in this group will inherit the permission of the group.**

### Roles

* **Role is an IAM identity that has specific permissions.**
* **Role is similar to User.**
* **But it is not associated with a specific person, a role is assumed by anyone who needs it.**
* **Role doesn’t have standard long term credentials such as password, it works on temporary security credentials for a session.**
* **Roles can be assigned to :**
   * **An IAM user in the same account as role in.**
   * **An IAM user in a different account than the role.**
   * **An AWS service such as EC2.**

### Policies

* **Policies are JSON documents.**
* **They define what User, Group and Roles can do or cannot do.**
* **Permissions are governed by policies.**
* **AWS provides you with many built-in policies that already have permissions according to use cases. This is the AWS managed policies.**
* **You can also create your own policy as well.**

## IAM Federation

* **IAM Federation is a feature which can be used by big enterprises.**
* **Generally, big enterprises have their own repository of users.**
* **Using IAM Federation such enterprises can integrate their own repository of users with IAM.**
* **This way users of such enterprises can log in into AWS using their company credentials.**
* **Identity Federation uses the SAML standard**

**read rest of the tutorial here** [**http://cloudforgeeks.com/aws-identity-access-management-iam-overview/**](http://cloudforgeeks.com/aws-identity-access-management-iam-overview/)",0,aws,2020-10-13
j9wl03,How do you reference a specific Lambda version from API Gateway HTTP API?,"How (on earth) do you reference a specific Lambda version from an API Gateway of type HTTP API? It's not possible to suffix the Lambda function name with :1 etc. when setting up the integration as you can with a REST API, and I can't see where else you're supposed to do it.",2,aws,2020-10-13
j9tkwd,Route 53 geolocation block,"Hello,

I'm creating Route 53 geolocation based records for my website. In my case, I just need to block access from 1 or 2 countries.

So I thought about creating a a type A record with 'standard' location (it will redirect to my website for all locations) and another one with the country I want to block to act as an 'exception rule'. However I have two questions:

\- Does the country-specific dns record overrides the standard one?

\- What would be the country-specific record endpoint (since I want to simply not resolve the dns in this case)?

Basically what is the easiest and cheapest solution for this?

PS: I read about using cloudFront for this, maybe to have a custom error page, but I'm not familiar with this product and what would be the cost of that?

Thank you",2,aws,2020-10-13
j9tl68,SNS Explained,"Hi folks, 

I recently put together a video talking about SNS (Simple Notification Service) and it's role in PubSub systems.

Video is here: https://youtu.be/bktTomENEX8

Thought I would share, and hope someone finds this helpful.",10,aws,2020-10-13
j9tpp9,How to learn about using AWS inside an enterprise environment?,"I'm trying to learn how to create/use AWS services inside a corporate/enterprise environment, where all the resources must only be accessed while on the corporate network. For example, here's a few situations where I have some idea of what to do, but I don't think I'd actually know enough to implement it (and I definitely don't know best practices):

&amp;#x200B;

* The company has an ALB that must use a DNS subdomain using the company's name (e.g. [my-alb.company.com](https://my-alb.company.com)) and this URL must only be accessible to users who are on the company's VPN
* An S3 bucket or CloudFront distribution that can only be accessed from the corporate network

&amp;#x200B;

I'm thinking that for something like S3, I could add something to a resource-based policy to lock down access to the corporate network. But I don't know what to add or where to start.

Are there networking services I should start learning at a deeper level (e.g. Direct Connect, Site-to-Site VPN, Transit Gateway)?  I don't want to go overboard and spend too much time on low-level networking concepts (e.g. the Advanced Networking certification) if I don't need to.

I have a few years experience with AWS and two certs, but until now all my AWS experience has been creating publicly-accessible resources.   Are there courses or resources that would help me get a better grasp of understanding how to ""use AWS in a corporate network""?",2,aws,2020-10-13
j9uaia,"Opinions on Logging Stack - ELK, Cloudwatch, etc.?","I'd like to solicit folks' working opinions on log shipping/centralization pipeline for an infrastructure entirely provisioned within AWS.  For context, I've spent the past 3 years running an ELK + Filebeat stack at two different firms with great results.  A critical feature of that stack is that I have some logstash configuration that selectively raises a PagerDuty incident for any issues in real-time.  It works very well; we suffer few false positives and no missed alerts ::knock on wood::.  That result was achieved after a bit of fine-tuning the various groks, throttles, and meta data stamping.  So, that's a hard requirement for me.

Neither of those infrastructure stacks were deployed in the cloud, although my current one is provisioned as Docker containers running in my firm's own infrastructure in anticipation of moving all of our infra to AWS.  I'm at that point now.  I've spent the past 6 weeks provisioning our new infrastructure entirely within AWS.  I feel good that our multi-account environment, VPC, and ECS configurations are in good shape.  And I want to deploy this stack as micro services in ECS.  In fact, the vast majority of this new stack will be running as micro services (currently in ECS, but eventually moving to EKS or a self-managed Kube environment).

However, in the course of doing some research this weekend it appears that:

* There are a number of new options: Filebeat straight to Elasticsearch, Fluentd, etc.
* Logstash seems to be falling out of favor?
* I really need to learn more about Cloudwatch and the logging solutions/kit native to AWS

What does your solution look like?  What was your rationale?  Does it handle alerting (and specifically PD)?  Thank you in advance for your help!",8,aws,2020-10-13
j9uybz,Does SSM document run before or after Userdata script?,"Hello,

I’m hoping this question makes sense. I’m using Terraform to deploy an Windows EC2 instance. I want to use my userdata script to configure the instance and change the name then use my ssm document that’ll be associated with the instance to join it to my Domain. I’m wondering if this is possible or if anyone has a better solution.

Thank you.

EDIT: I moved my PS script from userdata to SSM document.",3,aws,2020-10-13
j9v425,"How ""expensive"" is it for me to sync data to S3 Glacier Deep Archive?","I've started setting up a backup solution for my Google Drive on S3. Currently, I'm using Rclone to sync it as deep storage since I don't see myself actually needing to pull down data. I have around 78GB backed up but I plan to upload up to 2TB in the future.

If I sort by usage type, I can see that around 66% of my costs so far have gone to requests and ""early deletions"" (which is me accidentally uploading large, redundant files and deleting them from GDrive and restarting the sync).

Ideally, I'd like to sync this data every week or so with Rclone, which will include deleting older files. Note that Rclone also asks S3 for the hash of each file when syncing. I currently have around 6000 files. Will this cost loads to do or should it be fine?

Here's a screenshot of my costs: https://imgur.com/a/jjE2emh",2,aws,2020-10-13
j9uxfv,Lambdas,"I'm using AWS Lambda functions and I have removed the rules hoping to stop the function from invoking.  However, the Lambda is still invoking and I am having trouble finding out from where? Short of deleting the lambda, how can I find where the Lambda is being invoked? Thank you!",3,aws,2020-10-13
j9syoz,How do you migrate Appmesh services to new Cloudmap namespace,"Hi, I’m trying to migrate appmesh services from once namespace to another. Finding the following issues:

1. Appmesh doesn’t allow you to create a new virtual node(pointing to a different namespace) for the new virtual service
2. If I create a virtual node with a different name, envoy doesn’t seem to discover it - it fetches the service in the old namespace (this shouldn’t happen?)

Any suggestions on how to properly do this without any downtime?",3,aws,2020-10-13
j9s9te,Graph database prototype — working around Neptune being not in the Free Tier?,"Hey folks,

I am working on a few prototypes to validate ideas to propose in my company, and I’d like to use a Neptune as everything else I am using is Lambda, API Gateway, and a lot more with AWS.

I’ve an AWS free tier account, but Neptune seems it is not included.

So, I am thinking to externalize only the graph on Neo4j Aura or Mongodb Atlas, then migrate to Neptune in future, maybe.

Have anyone succeeded with this approach before, or can you recommend an alternative?

Thanks.",2,aws,2020-10-13
j9s5xz,Week of Oct 12th - What are you building this week in AWS?,Share what you are working on,3,aws,2020-10-13
j9s4pm,Create EKS cluster from yaml and specify the Kubernetes version,"I currently create an AWS EKS cluster using a cluster-spec.yaml. After the cluster is created, by default it is using Kubernetes version 1.15.

How can I specify to use a newer version of Kubernetes - like1.17?

I have looked around and do not see how to specify this in a yaml.",2,aws,2020-10-13
j9rh4z,AWS SSO VS Cross-account role-based IAM access. Why and how to use roles?,"Are you using AWS SSO to manage your AWS multi-account environment?  
If yes why?  
If no, what is your environment and how did you come to the idea of managing your account in that way?

[https://www.itscava.com/aws-sso-vs-cross-account-role-based-iam-access-why-and-how-to-use-roles](https://www.itscava.com/aws-sso-vs-cross-account-role-based-iam-access-why-and-how-to-use-roles)",1,aws,2020-10-13
j9oj4k,AWS as the backend for a live streaming app,"Hey there, I'm an iOS Developer and I recently got a client who wanted a Rugby live streaming app, (they are the ones who will take care of videoing the games and the legal stuff). The client wanted to know if this is possible and to know the technologies I'll be using. I'd like to know if this would be possible through AWS and if so, how much it would cost. Appreciate the feedback!",1,aws,2020-10-13
j9nm1z,Timestream: CloudWatch Positioning; Counters and Other Rough Edges,"I've been playing around with Timestream. I really like it conceptionally, but there's a few things that tripped me up.

First and foremost: Positioning of Timestream against CloudWatch: AWS advertises Timestream for ""DevOps applications"" (sic). But how does it relate to CloudWatch, the prime address for ""DevOps applications"" on AWS? I have read a lot of coverage on Timestream, but this topic hasn't been discussed anywhere. Specific examples:

* How would I get the AWS-provided metrics (especially EC2 metrics) into Timestream? If I'm collecting my own instance data in Timestream (like, say, memory usage or connection count), I'd sure love to correlate it with the AWS metrics (like CPU Credits balance).
* There's already the Cloudwatch Agent to collect many of the metrics used in the advertised examples; do I need to reinvent the collection of these metrics?
* How would I go about alerting? CloudWatch Alarms might be obvious, but there's no way to link those at all.

Apart from Timestream being (much) cheaper and allowing sub-second precision, it's not clear to me why TS would be an improvement over CloudWatch.

I mean, I could roll my own data exchange between CW/TS, but that would defeat the purpose *and* be really expensive. Or I could plug it into some frontend like Grafana and move all my alerting there and pay obscene `cloudwatch:GetMetrics` charges.

Am I missing something?

Then there's counter values. The TS docs promote that Timestream is intended for recording *measurements*, i.e. *gauge* values like room temperature, load average, fuel available etc. But in ""DevOps applications"" it's common to have a lot of *counter* values like ""network interface bytes transmitted since boot"" that are monotonously increasing -- you're interested in ""bytes per second"" for a certain interval, which is calculated as ""value difference to previous reading, divided by seconds since previous reading"". Prometheus and Influx for example provide a RATE function to calculate this.

I've tried doing this in Timestream and broke a finger. I can easily get the value difference using Window Functions, but getting the 'time' delta is ~~impossible~~ very impractical. Also there's annoying corner cases like counter resets.

Again, am I missing something?

And while I'm here, some very minor nuisances:

* No IAM Managed Policy for writers (easily created, but an odd omission)
* Time is always UTC (good); but the Console doesn't show that
* Pricing for ingestion is by ""KB"", but no definition of that. Is that the text representation of the metrics data? Wire (compressed) or raw? Or is it storage size in internal representation (even less predictable)? Does batch-`WriteRecords`'ing reduce the charges?
* Similarly, no way of knowing how much data is being stored and/or queried (afaict)
* Massive marketing push on the awesome Graviton instances, but no `grafana-datasource-timestream` available for ARM
* Documentation on available type casts would be helpful
* No way to ingest historical data (see also [here](https://www.reddit.com/r/aws/comments/j8oy8d/amazon_timestream_doesnt_allow_us_to_store_any/))

That being said, TS looks great so far, especially having a rich set of SQL to analyze the data and the ""serverless"" aspect of it. I'm really looking forward to see how Timestream will evolve.

P.S.: I'm new here, so if there's something wrong with this post, please let me know.",4,aws,2020-10-13
j9r1nl,What would you use to automate update of security groups of bastions with new client IPs?,We cannot use VPN. Therefore access is restricted through up rules on a bastion. What would you use to update those rules automatically for a group of remote devs?,2,aws,2020-10-13
j9qktx,Best security practices for a WordPress website on AWS?,"So, I want to host my WordPress website on AWS EC2. Just wanted to know how can I make my Instance secure from exploits and hackers.

I know how a self-managed Dedicated Web Hosting server works, In dedicated server they provide with a empty machine and then I need to take care of it's maintenance, updates, security etc. 

But it looks like AWS doesn't work this way. In AWS I don't get a dedicated server/PC, instead I have to use their cloud applications to create instances, etc.

So, as someone who's using AWS (or any other Cloud Computing service) for the first time, what security measures can I take here? 
Other than keeping my account passwords secure.",0,aws,2020-10-13
j9nlew,Storing EC2 programs and their secrets in CodeCommit?,"I am new to AWS. I am sure there are hundreds of possibilities to achieve what I want, I am just trying to figure out if what I am doing is dangerous or problematic in any way.

- What I want: I have EC2 machines that run various small jobs (shell scripts, python scripts, docker-compose files). 
- The scripts also include parameters/secrets such as database passwords, API keys, etc.
- I want to be independent of the machine, i.e. I want to be able to terminate it and start with a fresh one whenever I want. 
- I have AWS backup enabled for the machine, however I feel that backing up the whole machine does not allow me to track changes in files well enough.
- So I put the whole scripts-directory into a CodeCommit repository. This way I can track all my changes in detail.
- When writing programs, putting secrets into a repo is not desired, however I know that other people put their whole server /etc-dir into a repo, so it is probably not wrong.

Is this good practice? Any other method you would recommend? Thanks!",0,aws,2020-10-13
j9mmbf,"How to serve latest content from S3 via CloudFront, with minimal involvement of developers?","Helping  out an organization with their infrastructure. It's a really small team  (&lt;5), and I am the new ""cloud guy"". First thing I took up is hosting  their website which keeps going down because they keep doing all sorts  of things on the EC2 in which they were hosting the website. I am  putting it up on S3 and CloudFront.

What  would be the best way to serve the latest content without asking the  developers to do special stuff in their development process? I say that  because I cannot ask them to do versioning and stuff with their files.  Because... let's just say they are people who would argue with you for  years before making the tiniest change in their development process. So  object versioning is simply not possible.

I see three options here.

1. Set CF cache TTL to zero and always serve content from S3.
2. In  the CI pipeline, I could make folders inside the bucket on each  deployment. And use a lambda@edge on viewer request to ask for content  from the latest folder. The only thing I'm worried about here is the  lambda cold starts. If it appears like ""I made the website slow"", then  there would be another argument.
3. Cache  invalidation (/\*) on each build. I don't know if they would ever go  above 3k objects. But this should work until then. How do I handle  &gt;3k objects?

What approach would you suggest from these three? Any other suggestion is also appreciated :)",2,aws,2020-10-13
j9p5qq,How we use EBS and Auto Scaling groups to move database state during updates,,27,aws,2020-10-13
j9oxeb,[Free Tier] Will I be charged for egress traffic ?,"Hi,

Just created an account to benefit a 12-month free vps ( t2.micro ), but before using it I'm wondering if the egress traffic is also free of charge or not ? I can't seem to find the information

Thank you",3,aws,2020-10-13
j9mb7g,AWS + Serverless Framework + OpenAPI 3.0/Swagger generation,"Hi everybody,

I am using serverless framework with AWS and TS and have to add an (semi-autogenerated) Open API specification of it. I have been scouting the possibilities to do so but all of them seem pretty meh.

I found [serverless-aws-documentation](https://github.com/deliveryhero/serverless-aws-documentation) plugin which in theory does the trick + it would be great to define it insiede `serverless.yml` where our validation layer is already defined. However, it looks dead + does not seem to support multiple serverless.yml files (basically it loses context of the paths etc.).

Another possibility would be to adopt a framework like Nest, Koala or Express and but I'd rather not move our project to mono-lambda realm. The frameworks seem to support autogenerated swaggers/OpenAPIs but mostly for containerized types of work.

There is also Amazon API Gateway Export which I don't really know how would fit here. (Is that mechanism any good, even without the serverless?). Perhaps somebody could give me a hand.

We also could try finding/writing our own decorators and webpack build, so that CI/CD walks recursively through `.ts` files and automatically generates OpenAPI 3.0 spec from the annotations. This is handful of work though and my team has only few developers, already overwhelmed with amount of work. :P

Lastly, manual writing of `.yaml` files. Duh.

I'd love to hear your stories about similar issues. Have you ever generated a Swagger or OpenAPI 3.0 in serverless realm? If yes, how was it?",4,aws,2020-10-13
j9iong,Best practice for caching with Cloudfront and EC2,"(I appreciate there's a separate discussion going on here at the moment about TTL lengths, but this is a little wider and I hope might be a useful permanent record).

Let's say I'm running an RSS feed for a jobs board at `https://example.com/rss`. It's about 300KB, so is pretty large. I'd like this to be as cached as possible: I really don't want to be reproducing the entire RSS feed all the time if I can be avoiding it.

Versioning don't work for this, of course. The RSS feed needs to live at `https://example.com/rss` all the time.

I don't know when I might get a new job to list in this RSS feed. It might be tomorrow; it might be in two minutes. So I can't plan ahead and say `max-age: 3600` for an hour, since if I get a new job in two minutes I'd like that to be visible.

So I *think* I have a few options here:

**Invalidation:** Set `max-age` to be six hours. When I get a new job, invalidate the page in Cloudfront. This is what I do now. However, as far as I can work out, any caches upstream won't be invalidated - so they'll continue to be served for six hours. That's probably not good.

**ETag:** Set `max-age` to be five minutes. Set an etag on the EC2 server. While the etag is the same (i.e. the page content hasn't changed), I can stop the page from generating the entire RSS feed, and instead just return a ""Not Modified"" header. Every five minutes, Cloudfront will come knocking to check if it's changed. If it hasn't, it gets cached for a further five minutes. Is that right?

**Static ETag:** Produce the RSS feed as a file and place it on S3, with an ETag. Set the `max-age` to be five minutes, and point Cloudfront to S3, not EC2, for this. That should achieve the same as the above, but not place any additional stress on the server. An ETag should mean that S3 replies with a ""Not Modified"" header, and it shouldn't cost anything in terms of bandwidth. Is that right?

Is there something else I should be considering?

I've been using Cloudfront for more than ten years, but this issue has always stumped me. I'd welcome any best practices - I can't really find any documentation that helps me here.",3,aws,2020-10-13
j9odvz,AWS Server Migration Service and Veeam (snapshots) question,Will running the AWS SMS that snapshots and sends deltas to AWS every few hours interfere with Veeam backups (or vica-versa)  Or would the two products track their own snapshots and CBT in vSphere?,3,aws,2020-10-13
j9mm80,Opening paegent,"I own a windows machine. I’ve downloaded the PPK file to connect with my insurance on ssh. But whenever i try opening the paegant app in windows, it shows “The service is already running’’. How should I open the app ?",5,aws,2020-10-13
j9mde7,What are the most legendary AWS Keynotes?,I'm collecting the most memorable keynotes of AWS on this page: https://awsvideocatalog.com/general/legendary - are there more worth adding?,52,aws,2020-10-13
j9lagd,Amazon EMR integration with AWS Lake Formation is now generally available,,1,aws,2020-10-13
j9jaic,Custom domain confusion with Next.js/Serverless/AWS Amplify Application,"Hey everyone I’m trying to set up my serverless.yml file to host my Next.js app on a custom domain. I asked this question in the Serverless forum but I didn't get any response. For those who aren't familiar with why I'm using Serverless, I have to use the Serverless framework with Amplify because my application has API routes which run on the Server. Amplify doesn't have support for running these API endpoints with Next.js yet, so their team suggests I use Severless. Right now my domain is hosted on Route 53 and I also have a certificate and a hosted zone in US-East 1 generated for it in AWS. My domain is sekndapp.com and the domain and certificate are just for that domain, no www or http on the certificate or the registered domain. Right now the serverless.yml file looks like this.

myNextApplication:
  component: ""@sls-next/serverless-component@1.17.0""
  inputs:
    domain: ""sekndapp.com""

The error that was coming up said this.

InvalidViewerCertificate: The certificate that is attached to your distribution doesn’t cover the alternate domain name (CNAME) that you’re trying to add. For more details, see: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/CNAMEs.html#alternate-domain-names-requirements.

I'm confused because serverless wants me to input the domain in .yml file as such, but this error is saying that there is an alternate, which there isn't. The domain is literally just sekndapp.com. Is there something I have to do within Route 53 to further configure my domain? If that's not the case does anyone have an idea on how to configure my serverless.yml file?",6,aws,2020-10-13
j9j7ua,connecting to aws postgres database,"Noob question here. I've been having a hard time connecting to a postgres database instance in AWS.

    import psycopg2
    
    myConnection = psycopg2.connect( host=hostname, user=username, password=password, dbname=database)

Under  connection and security I copied the endpoint and  passed it to the `host` parameter, the default port for posgres is is 5432 and `psycopg2.connect`.

Under security i modified my public accesssability ot yes.

The parameter user is my `Master username` which is available under configuration and Availability. I copied my password.

for dbname, I've tried both  `DB instance id`  and `DB name`. Both available under configuration and  Configuration.

It's unable to connect. I tried both Pycharm database connection tool and `psycopg2`

What did i do wrong?",3,aws,2020-10-13
j9it3i,Stalling ELB instance / dropping socket connection,"I have elastic beanstalk instances that stream data and on a regular basis I have to reset the connection and or restart the instance.  Could be due to data load, but nothing really in the logs.  Using Node.  

Upsizing instances doesn't seem to offer much help.  Is this a common occurrence, ideas on how to fix ?",1,aws,2020-10-13
j9idd7,Spot instance freezing/crashing 1-2 times per day,"Hello, I run a discord bot on an AWS spot instance (a t2.micro IIRC), and for 5 months straight, I've never had a single issue with it. However, for the past approximate week, I have noticed slow response times, freezing of the bot, and sometimes all-out crashes. Typically when this happens, the second status check ""Instance reachability check"" fails and I am not able to connect to the instance via putty. Usually rebooting the instance and waiting will solve the problem. 

I have not received any emails from AWS about them needing to shut my instance down (since it is a spot request, and they did say they'd warn me about this if it ever was to occur). Does anyone have any ideas as to why this may be happening? Thank you!",1,aws,2020-10-13
j9hsu4,Handing off RDS instance from personal account to client account,"I'm considering to use AWS RDS for a client project.  I'll have to create the instance on my personal account, but when it's time to handoff the instance to the client, can I migrate/transfer to a new account?  What's the process for something like this?",0,aws,2020-10-13
j9htp4,Processing an xml file,"Hi All - Just picking up AWS and trying to study. Have a requirement to process an XML file which has emails within it and convert this back into email format messages. How do I start any help is appreciated.
TIA",0,aws,2020-10-13
j9help,IllegalLocationConstraintException error when using sagemaker,"I am following the AWS docs to deploy a Sagemaker instance.

[https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/](https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/)

&amp;#x200B;

`# import libraries`

`import boto3, re, sys, math, json, os, sagemaker, urllib.request`

`from sagemaker import get_execution_role`

`import numpy as np`                                

`import pandas as pd`                               

`import matplotlib.pyplot as plt`                   

`from IPython.display import Image`                 

`from IPython.display import display`               

`from time import gmtime, strftime`                 

`from sagemaker.predictor import csv_serializer`   



`# Define IAM role`

`role = get_execution_role()`

`prefix = 'sagemaker/DEMO-xgboost-dm'`

`containers = {'us-west-2': '`[`433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest`](https://433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest)`',`

`'us-east-1': '`[`811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest`](https://811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest)`',`

`'us-east-2': '`[`825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest`](https://825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest)`',`

`'eu-west-1': '`[`685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest`](https://685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest)`'} # each region has its XGBoost container`

`my_region = boto3.session.Session().region_name # set the region of the instance`

`print(""Success - the MySageMakerInstance is in the "" + my_region + "" region. You will use the "" + containers[my_region] + "" container for your SageMaker endpoint."")`

&amp;#x200B;

This is the first code block I run and I get a success message.

&amp;#x200B;

`Success - the MySageMakerInstance is in the us-east-1 region. You will use the (url here)`

&amp;#x200B;

I then run this code block and get an error:

&amp;#x200B;

`bucket_name = 'your-s3-bucket-name' # &lt;--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET`

`s3 = boto3.resource('s3')`

`try:`

`if  my_region == 'us-east-1':`

`s3.create_bucket(Bucket=bucket_name)`

`else:` 

`s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })`

`print('S3 bucket created successfully')`

`except Exception as e:`

`print('S3 error: ',e)`

&amp;#x200B;

I then get this error:

&amp;#x200B;

`S3 error:  An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.`

&amp;#x200B;

What am I doing wrong? My region is us-east-1.

I do",1,aws,2020-10-13
j9g3z1,Newbie Lambda and JWT Question - Where to put my JWKs?,"I'm working on a Lambda authorizer for my API Gateway project. I'm working with JWTs for the first time, and not sure how to handle the JWK part of this.

My OAuth source provides a JWK endpoint ([https://xxxxxxxxx/jwks](https://xxxxxxxxx/jwks)) that spits back a JSON list of about 500 JWKs. I'm using a library to decode the JWT, and my understanding is that I pass in the JWT and the list of JWKs, and if valid the library decodes it. However, the OAuth source has of course asked that we cache these JWKs since they're infrequently rotated. Therein lies the AWS problem:

Where should I put this data? My choices so far are:

1. Save them in a JSON file and package it up with the Lambda deploy, then read the file on execution.
2. Save them in a JSON file and stuff it in an S3 bucket somewhere, then pull it into the Lambda on execution.
3. Use AWS Parameter Storage (?), which seems weird to use with 300KB worth of JSON data.
4. Say ""screw you!"" to the OAuth source and call the JWKs endpoint every time.

O great AWS experts, what are your opinions on this?",2,aws,2020-10-13
j9eby8,Best AWS Service for handling bulky CSVs,"I need to design a service which takes in large CSVs(of millions of records) and does some sort of user specific transformations on it, like changing header names etc, and also enrich that with some extra columns by deriving some data from the existing columns by hitting external APIs. This functionality is currently being run on AWS Lambda, but due to its memory limitation and 15 min timeout, it isn't able to handle large files. What can be the best alternative for AWS Lambda here? I have tried considering AWS Glue, EMR and AWS Batch, but not able to decide firmly upon any one of these. Please recommend what could be the best possible options over here, and also suggest anything else apart from these three, if needed.",4,aws,2020-10-13
j9dsgu,Issue with creating directory DNS IP address - aws workspace,"Hi, I've been trying to setup workspace but first I would need to create directory. So I started with AD Connector type and currently having issues with setting proper DNS IP address. Getting failed messages: https://i.imgur.com/0J1pcUJ.png

Can I use any dns ip or should I create new one? Appreciate any help since I'm newbie with all of this.",2,aws,2020-10-13
j9bb4l,How do you output processed JSON from AWS Glue to DynamoDB?,"```
{
            ""adult"": false,
            ""backdrop_path"": ""/example.jpg"",
            ""belongs_to_collection"": null,
            ""budget"": 350000,
            ""genres"": [
                {
                    ""id"": 18,
                    ""name"": ""Drama""
                }
            ],
            ""homepage"": """",
            ""id"": 123,
            ""imdb_id"": ""a3f4w4f4"",
            ""original_language"": ""en"",
            ""overview"": ""Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum"",
            ""popularity"": 27.298,
            ""poster_path"": ""/example.jpg"",
            ""production_companies"": [
                {
                    ""id"": 60,
                    ""logo_path"": ""/example.png"",
                    ""name"": ""example 1"",
                    ""origin_country"": ""US""
                },
                {
                    ""id"": 10212,
                    ""logo_path"": null,
                    ""name"": ""example 2"",
                    ""origin_country"": """"
                }
            ],
            ""production_countries"": [
                {
                    ""iso_3166_1"": ""US"",
                    ""name"": ""United States of America""
                }
            ],
            ""release_date"": ""1970-04-10"",
            ""revenue"": 1000000,
            ""runtime"": 97,
            ""spoken_languages"": [
                {
                    ""iso_639_1"": ""en"",
                    ""name"": ""English""
                }
            ],
            ""status"": ""Released"",
            ""tagline"": ""Lorem ipsum."",
            ""title"": ""Example name"",
            ""video"": false,
            ""vote_average"": 8.5,
            ""vote_count"": 5004
        }
```

I am new to AWS Glue. From what I know it creates a zeppelin notebook that flattens json you throw at it using relationalize transform. Then it normally allows writing to RDS/s3 etc.   

I didnt find any good information on directly exporting to dynamodb from AWS glue.  
Above is one of the json items in a collection I want to store in dynamodb.   
 
The json fields and keys are identical and consistent with the other json items, albeit some have fewer or more subitems.   

If the dynamodb table and schema mapping exists, I want AWS Glue to insert or update this json information into dynamo.   

How do I do that?",3,aws,2020-10-13
j9dp95,An open-source tool that generate Terraform for AWS,,57,aws,2020-10-13
j9d70z,Accessing my ECS container?,"I have a Node application hosted in a Fargate ECS Cluster with a Load Balancer in front of it to be accessed by a React / Relay front end. When I start the task in the ECS Service and access the route using the given IP address, I get the expected result. However, when I try to access it through the security group and loadbalancer, it fails the health checks with a 400 and the frontend breaks. 

What am I doing wrong?

EDIT: For anyone wanting plot resolution, I got the health checks working by using the Apollo-provided health check endpoint (which I didn’t know existed). As for the routing, it seemed like security groups were the real culprit. Thanks everyone for the help!",10,aws,2020-10-13
j9awzd,Newbie Question - Can you horizontally scale a CPanel or Plesk server?,"Our small hosting company is looking to switch to AWS and I'm tasked with doing the research.  I've been digesting a lot of info for a few weeks now but there doesn't seem to be a lot of info on this subject specifically.  Currently, we host about 200 sites through CPanel on a few dedicated GoDaddy servers at the moment.

If I understand scaling and AWS EC2's correctly - because CPanel and Plesk are licensed per server - I can't use the Auto Scaling service for EC2 instances, is that correct?  Well, I guess I could - but I'd have to pay a license fee per EC2 that spins up, right? - Which doesn't seem very cost-effective.

If that's all accurate, I believe my only option is to buy a good fit with an EC2 and when I need to upgrade, I'm assuming I can detach the EBS and re-attach it with a new EC2 instance? If so - is that process pretty simple?

Any advice is appreciated on the best route to take.

Thank you in advance!",1,aws,2020-10-13
j9bb08,Is there ANY real reason one would not want to lower the TTL for caching with CloudFront? Noticed I have to wait 24 hours for changes to fully update on Cloudfront from S3 bucket...but don't see why I wouldn't want to set it to 1 hour instead. Help?,"So, basically the title. I have a static website I have on S3 and use cloudfront with it. Basically, with the URL, it goes through cloudfront, but the files are held in S3 bucket.

One thing I recently noticed is I changed the CSS for the static website and also added an image. The image change went through, but the CSS change did not. I verified that the CSS changes do work when I access the content directly from the S3 bucket, so I know this really is a caching issue with cloudfront.

When checking the TTL policy, it seems the max is some super high number, but the default TTL is 24 hours (it is shown in seconds).

Given this...why wouldn't I say want my max and default ttl to be 1 hour instead? I mean I'm guessing this means cloudfront makes a request to S3 bucket once an hour instead of every 24 hours. 

But I just don't really see the major downside to this? Is there additional costs I should be aware of if I did that? Is there a reason one would really want the default to be 24 hours? 

Just curious as now I have to wait 24 hours for the change to reflect in the cloudfront now. Thanks for any information.",16,aws,2020-10-13
j990bj,Is there a MFA cmd line tool?,I've been using authy for awhile and it works fine but I don't always carry my cell phone with me. Authy (desktop version) is in beta review and i'm not a fan of snap installs. I was wondering is there a MFA cli tool that would allow me to manage AWS accounts (basically just like authy but from the cli).,4,aws,2020-10-13
j97kwb,Deleting old ECR docker images easily across the account,"Just added an option to delete old ECR docker images to cut down storage costs.

Just run `awsctl delete ecr -r eu-west-2 --keep 20 --yes`

[https://github.com/omerh/awsctl/releases/tag/v0.0.7](https://github.com/omerh/awsctl/releases/tag/v0.0.7)",4,aws,2020-10-13
j956g7,Can SNS deliver to Kinesis Firehose?,"Without the use of Lambda?

Or is there a better service that achieve a fan out approach to data received?

Something like this

```
API Gateway =&gt; (x) =&gt; Kinesis Firehose =&gt; S3
                                  =&gt; Lambda
                                  =&gt; Kinesis =&gt; Lambda
```

Where (x) is a service that fans out to other services?",1,aws,2020-10-13
j922g7,Is the cloudfront distribution automatically setted on the free trial when created? Can't find something related to the on the distro creation page,,0,aws,2020-10-13
j93djz,Permissions error trying to run Prometheus on EKS (Fargate only) with EFS,,1,aws,2020-10-13
j8zfwh,Do I have to pay to change name servers in was,I want to know if I can change my name servers to cloudflare if I buy a domain from route 53 and if it would cost me to change nameservers or not?,2,aws,2020-10-13
j8y6a0,How to query a local server using aws lambdas through web sockets,"Scenario: I have an aws gateway that needs (for development purposes until module is ready for production) to call a local server on my machine. There is of course ngrok and localtunnel but I am looking for a totally integrated system (without resorting to hosting lt on ec2 etc) with minimal fuss for developers. Now aws offers websocket support unfortunately this is event based and not a request/response - so I cannot query the local web server, get a response and send it on. The best I can come up with is to have the web socket client (the local server) send a message to another lambda to either put on sqs or dynamodb and have the original lambda (the one that called the local server) poll for a response. I hate polling but I can't think of another way to get a response in the same cycle. Anyone have a clever idea that can be done without launching complex ec2/dockers etc and does not require using \`setInterval\`? I should add that I am unclear as to whether one can get a response from the client using sockets. The \`postToConnection\` command seems to imply it can return data but I have not succeeded in finding a library that can do so - I am using NodeJS and the \`ws\` library. I did see a comment on StackOverflow about SocketIO being able to respond but this may be a proprietary thing and socketIO does not appear to work with API gateway.",1,aws,2020-10-13
j8z1gp,Serverless Ddos security?,"I was thinking how people secure their public serverless API's. Is rate limits good enough for this? The fear of suddenly getting a Bill on 10.000$+ is scaring me. So far I have choosen to use lambdas only for private workloads. Using EC2 for API, which is not doing much most of the time. 

I am using T3A family to reduce price. But would love to switch to fully serverless and only pay for what I need. 

Would love if there was some AWS best practices for public Lambda API's. 

Hope someone out there has some good advice",1,aws,2020-10-13
j8xfwq,"Stitching Terraform and Ansible together via AWS SSM, plus Graviton2",,3,aws,2020-10-13
j8xd2w,"Any samples of completely serverless websites on AWS that include all the bells and whistles (dynamic content, user authentication, etc.)?","I've read about how it's possible to create a website that includes dynamic content, backend processing, user management, etc. all 100% serverless by leveraging things like S3, API Gateway, Lambda, Cognito, etc. with the web content being some popular JS framework like React.

Does anyone know of any tutorials or guides that could be used to set up such a site (even if the content is essentially blank/hello world) that preferably includes all necessary components as CloudFormation, the web content code to call the backend services, etc.?",2,aws,2020-10-13
j8wnbo,"This AWS Quck Start tutorial needs to know my ""Allowed Bastion External Access CIDR"" but I don't know what to put here.","As the title goes i'm following this aws quick start tutorial [__here__](https://docs.aws.amazon.com/quickstart/latest/linux-bastion/step2.html). I'm at a point in the tutorial where I have to fill out [__this option here__](https://imgur.com/a/AYgsHUL). AWS defines this as...  

&gt; CIDR block that’s allowed SSH external access to the bastion hosts. We recommend that you set this value to a trusted CIDR block. For example, you might want to restrict access to your corporate network.  

My understanding is I need to provide a range of ip's but in CIDR format that I'll be ssh'n into from. Is this my public facing ip of my home network (e.g. the one showed by sites like [__ippig.com__](http://ippig.com/)? Or is this the internal gateway of my modem?  

Lastly, whether its the public facing ip or the internal gateway of my modem this is just one ip. So I'm still confused how I'm to put this in CIDR format as a range of ips. Can someone clear my confusion up with an example of what I'm to put here please.",2,aws,2020-10-13
j8w9l9,This is what happens when you move from Intel to Graviton2,,207,aws,2020-10-13
j8w2h1,RCA for SA-EAST-1,"Hello,

Does AWS publish public RCA’s for issues like the one that happened today with EC2 in South America?",4,aws,2020-10-13
j8e19s,Solving AWS Aurora RDS load balancing problem with Pgpool,"My colleague and I wrote an interesting piece on how we solved challenges we faced with DNS loadbalancing with AWS

https://medium.com/talabat-tech/solving-aws-aurora-rds-load-balancing-problem-with-pgpool-ii-d54897c95176",2,aws,2020-10-13
j8vchn,[Long] Completely lost after following AWS Amplify tutorial,"I have a React Native project that's just 1 screen and the user can get an image from an S3 bucket on a button click.

I partially went through the [Amplify Tutorial](https://docs.amplify.aws/start/getting-started/installation/q/integration/react):

`sudo npm install -g u/aws-amplify/cli`  
`amplify configure` \- created `amplify-user` with `AdministratorAccess` in `eu-west-2 region`, got `accesskeyId` &amp; `secretAccessKey` pasted correctly  


then I `cd` into my app, did `expo start` (just a 'Hello World' bare app) and everything seemed normal. then

`amplify init`

`? Enter a name for the project AWSvideoApp`  
`? Enter a name for the environment dev`  
`.`  
`.`  
`.`

`Using default provider awscloudformation`

`? Do you want to use an AWS profile? Yes`  
`? Please choose the profile you want to use default`

`Adding backend environment dev to AWS Amplify Console app: d3r4bzwznjjdck`

These commands ran successfully and I had an S3 bucket in my console: `amplify-awsvideoapp-dev-42789-deployment`

I installed some Amplify libraries and put this in my code:

`import {Amplify, Storage, Auth} from 'aws-amplify';`  
`import awsmobile from './aws-exports';`  
`Amplify.configure(awsmobile);`  


Then I went through the [Storage with Amplify](https://docs.amplify.aws/lib/storage/getting-started/q/platform/js) tutorial so I can easily create and interact with an S3 bucket. I did 

`amplify add storage`

`Please select from one of the below mentioned services: Content (Images, audio, video, etc.)`  
`You need to add auth (Amazon Cognito) to your project in order to add storage for user files. Do you want to add auth now? Yes`  
`Using service: Cognito, provided by: awscloudformation`  
`The current configured provider is Amazon Cognito.`   
`Do you want to use the default authentication and security configuration? Default configuration`

`Warning: you will not be able to edit these selections.`   
`How do you want users to be able to sign in? Email`  
`Do you want to configure advanced settings? No, I am done.`  
`Successfully added auth resource awsvideoapp879a96b2 locally`  
`Please provide a friendly name for your resource that will be used to label this category in the project: amplifyS3storage`

`Please provide bucket name: amplifys3test`  
`Who should have access: Auth users only`  
`What kind of access do you want for Authenticated users? create/update, read, delete`  
`Do you want to add a Lambda Trigger for your S3 Bucket? No`

`Successfully added resource amplifyS3storage locally`

then I did `amplify push` and got 

`Current Environment: dev`

`| Category | Resource name  | Operation | Provider plugin  |`  
`| -------- | -------------- | --------- | ----------------- |`  
`| Auth     | awsvideoapp879 | Create    | awscloudformation |`  
`| Storage  | amplifyS3storage | Create  | awscloudformation |`

`? Are you sure you want to continue? Yes`

At this point I thought everything was setup and I tried accessing my bucket from App.js:

`const data = await Storage.get('icon_1.jpg',{download: true,contentType: ""image""})`

Console logging data got me a huge URL that, when pasted into the browser, read:

    InvalidAccessKeyIdThe AWS Access Key Id you provided does not exist in our records.undefined5540B...

I Googled this and tried stuff but I can't seem to figure out what's wrong.

What should I do?",2,aws,2020-10-13
j8udhz,Proxy with URL rewrite?,"Howdy,

I'm trying to leverage AWS to rewrite url of a web service.

I have used Azure Enterprise Application Proxy before and I am looking for something similar within AWS.

Any help is greatly appreciated.

Thanks",2,aws,2020-10-13
j8tlj5,Slow and laggy (12MB/s) NFS write to AWS Snowcone; NFS seems to be bottleneck,"I got a Snowcone delivered to help me migrate my old home server ZFS pool into the cloud. I've got two 3TB Hitachi SATA HDDs in a USB 3.0 dual disk dock, plugged in with USB 3.0 to my old ThinkPad X230 with 12GB of RAM running a fresh install of Ubuntu 20.04.1 LTS. It's plugged in with Gigabit Ethernet to a Netgear switch, as is the Snowcone.

I've set up the Snowcone to provide an NFS share so I can copy files over. What I've noticed is that over time, the NFS share becomes really slow. Network traffic drops down to 12MB/s, and it can take a minute for another terminal to list the contents of the mount point.

I thought the issue could be with ZFS and the two hard drives, but I was able to do `zfs send` to `/dev/null` at &gt;200MB/s for the 3 hours and 20 minutes it took to generate the 3.1TB snapshot.

I can send `/dev/urandom` to `/dev/null` at a similar 200MB/s. However, when I send `/dev/urandom` to my NFS mount point, it starts out maxing out the gigabit connection before falling back down to the consistent 12MB/s.

I can see that the NFS backlog wait increases to 17 seconds.

    $ mountstats
    Stats for 192.168.0.55:/buckets/snowcone-bucket/ mounted on /mnt/snowcone:
      NFS mount options: rw,vers=3,rsize=131072,wsize=131072,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=192.168.0.55,mountvers=3,mountport=20048,mountproto=udp,local_lock=none
      NFS server capabilities: caps=0x3fcf,wtmult=4096,dtsize=4096,bsize=0,namlen=255
      NFS security flavor: 1  pseudoflavor: 0
    
    NFS byte counts:
      applications read 0 bytes via read(2)
      applications wrote 95529026482 bytes via write(2)
      applications read 0 bytes via O_DIRECT read(2)
      applications wrote 0 bytes via O_DIRECT write(2)
      client read 0 bytes via NFS READ
      client wrote 95529026482 bytes via NFS WRITE
    
    RPC statistics:
      730168 RPC requests sent, 730168 RPC replies received (0 XIDs not found)
      average backlog queue length: 0
    
    WRITE:
            728830 ops (99%) 
            avg bytes sent per op: 131271   avg bytes received per op: 136
            backlog wait: 17449.278832      RTT: 233.379521         total execute time: 17682.687959 (milliseconds)
    COMMIT:
            649 ops (0%) 
            avg bytes sent per op: 192      avg bytes received per op: 128
            backlog wait: 42.784284         RTT: 170.322034         total execute time: 213.144838 (milliseconds)
    LOOKUP:
            187 ops (0%) 
    

nfsiostat shows it maxing out at \~100 NFS RPC op/s of about 128K each. There don't seem to be any network issues like dropped packets.

    $ nfsiostat
    
    192.168.0.55:/buckets/snowcone-bucket/ mounted on /mnt/snowcone:
    
               ops/s       rpc bklog
              88.238           0.000
    
    read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
                       0.000           0.000           0.000        0 (0.0%)           0.000           0.000
    write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
                      88.076       11302.624         128.328        0 (0.0%)         233.380       17682.688

Does anyone know what might be going on? This can't be normal, surely?

What I'm trying to do is to send a `zfs send` snapshot. By default this goes over stdio. Dumping it straight to a file on the mount point seems to have the same performance as using the `split` command to divide it up into sequentially numbered files.

I've also tried setting the client NFS version to 3 without it making much of a difference. The fact the avg RTT is low but the avg `exe` time is high suggests to me that there's something wrong with the NFS stack config.",1,aws,2020-10-13
j8um1t,What kind of database allows tools like AWS X-RAY to function?,"I was testing AWS X-RAY out and it learned how to setup annotations for filtering and group by. I haven't collected a lot of data yet as I'm just toying around with a dozen traces or so. However I wouldn't doubt that x-ray can scale to millions of traces.
The main functionality I'm curious is the ability to have what seems to be a schemaless structure (50 arbitrary annotations) capable of offering filter (like a where clause) and grouping (like group by clause) and aggregation (avg, sum, count, etc).
I imagine building this on top of DynamoDB could be extremely challenging and a relational database would have performance issues on scale.
What's the ideal type of database to power applications like X-RAY and what makes it a great database for this type of process?

Thanks for any input!!",8,aws,2020-10-13
j8tfna,AWS hackathon abruptly cancelled?,"A few months ago [this](https://dev.to/willydavidjr/aws-mission-to-mars-hackathon-10lp) was announced. AWS Mission to MARS hackathon - build whatever you want using AI, ML or Robotics services to win 65k in prizes. But yesterday I'm suddenly getting an email it was cancelled. Here's the [website](https://www.awsmissiontomars.com/#/event).

Anyone know anything? Kinda sucks to make noise about such a cool hackathon competition and then just cancel without giving a good reason.",10,aws,2020-10-13
j8tzh0,Amplify deploy diff in code,"First off, please excuse my lack of knowledge on the specific terminology of this.

&amp;#x200B;

I know some services offer a feature where you can push code diff to clients when they open the app. I am not sure what this is called, but effectively it enables the developer to deploy changes quickly without requiring a new submission to the app store.

&amp;#x200B;

Does amplify offer this kind of frame work, and if so, what is the specific term for this kind of feature?

&amp;#x200B;

Thank you!",3,aws,2020-10-13
j8oy8d,Amazon Timestream doesn't allow us to store any history and any forecasts for the future?!,"AWS has finally released Amazon Timestream, a serverless Timeseries Database. I was super excited about it because, in the end, we had to wait for over 2 years for the service to be released. 

However, today I wanted to ingest my last month's data and play around with the system and I found out that the service doesn't allow to store any historical data!!! I was at first surprised that I got an error: ""RejectedRecordsException"" when I was trying to ingest my historical time series data via Boto3. But then I read in the documentation:

""Ensure that the timestamp of the incoming data is **not later than** data retention configured for the memory store and **no earlier than** the Future ingestion period defined in Quotas (p. 160). Sending data with a timestamp outside these bounds will result in the **data being rejected by Timestream**""

So this means that if I would ask AWS to increase the service quota for ""Future ingestion period in minutes"", I would probably be able to store some forecasts for the future. But it seems impossible to store any time series data for the past.

To me this sounds absolutely ridiculous. How can anybody migrate to this service if it's impossible to move their historical data so that it can be analyzed together with new data?! 

If there is some way of configuring this time series database service to use historical data, please let me know, as it would be really helpful. Without it, this service is useless to me.

Thanks a lot in advance!",10,aws,2020-10-13
j8mjbl,Instance for ml.p2.xlarge,"Hello Everyone, so I'm doing this udacity nanodegree and there is this last assignment that I have to complete through AWS but the problem I'm facing is that I have to request for the instance change of ml.p2.xlarge, I've applied it today but my nanodegree subscription end in 3 days and it is said that it takes 48 for the query to resolve so I don't really know that if my request will be accepted in time or not and I also want to know that is there any other method to change that instance?",1,aws,2020-10-13
j8kl1q,I wrote a script to remove an AZ from an ASG to avoid launching new instances in the faulty AZ.,,7,aws,2020-10-13
j8p4ip,Thinking of beginning a VSaaS (Video Surveillance as a Service) curious about what to do for storage?,Let's say a client has 100 cameras that need video archived retention of 10 days. That could be up to 20tb's of data. What method should I go for using this?,0,aws,2020-10-13
j8my83,(New to AWS) Hosting a public website on Windows Server - where should I start?,"Hello there,

I'm planning (more like, in the midst of) to host a [ASP.NET](https://ASP.NET) web application globally on a virtual Windows server (Amazon Lightsail - EC2) for a school project.

Things I've done:

* Set up IIS in the virtual Windows server (the localhost/wwwroot stuff)
* Installed some required dependencies for my web project (most notably Python) in the server
* Created a database instance on RDS and connected it to my web app (web.config)

Things I will do:

* Use FileZilla to copy my website files to the server

&amp;#x200B;

With regards to using a domain, I'm looking at using Route 53 for registering.

As a beginner, where should I start to focus/look at, in order to host my website to the Internet? Any suggestions/tips would be extremely helpful.

&amp;#x200B;

Thanks in advance.

&amp;#x200B;

Technical information:

* Windows server uses the free tier of Amazon Lightsail instance
* Database uses free tier of Amazon RDS",0,aws,2020-10-13
j8m3np,AWS Security for beginner?,"Hey y’all 
Just wondering if it’s feasible to jump straight into AWS security Cert without any of the beginner AWS certs? Not sure if my job would expose me to cloud in the future. I currently have Network+, security+, and working on CCNA. without any cloud experience, is AWS security too far out?",3,aws,2020-10-13
j8jces,How can a AWS server be secured properly?,"Note: This is a hypothetical question, just for informational purposes.

So, Let's assume that I have a server with EC2 instance running on AWS.

My root password is a long combination of - characters, numbers, weird special characters, etc. , which is theoretically impossible to crack.

My account has only 1 user, that's me.

The API keys and root password are locked in a unbreakable vault.

I only access my AWS server/account on Mac, in safe-boot mode.

Now, if an experienced Hacker want's to hack my website. 
What are some ways he/she can do so?
(Assuming that he can't trick me into installing a malware or clicking on some malicious link).",0,aws,2020-10-13
j8laxb,I wrote a tool to blacklist an AZ from ASG to avoid launching new instances in the faulty AZ. Followup to current issues with AWS Availability zones.,,47,aws,2020-10-13
j8gxqu,AWS Site-to-Site VPN Routing Configuration Trouble,"Hello everyone,

New to AWS and trying to configure a VPN connection to our data centre and wanted to confirm something. I have the following subnets and VPC configured in AWS and in the Data Centre

* VPC - 192.168.0.0/16
* Local subnet- 192.168.100.0/24
* Data centre subnet - 192.168.200.0/24

Neither subnets overlap and I have successfully brought up both the IPsec tunnels. I have added the propagated route to the route table and for the moment have all traffic allowed on the NACL and Security group. Route Table as follows:

* 192.168.0.0/16 - Local
* 192.168.200.0/24 - VGW (propagated)
* 0.0.0.0/0 - Nat Gateway

Now I can see traffic hitting my ENI in VPC flow logs, but nothing is being received back at the data centre. Upon doing some research I found the following documentation on routing that states the following:

* If propagated routes from a Site-to-Site VPN connection or AWS Direct Connect connection overlap with the local route for your VPC, the local route is most preferred even if the propagated routes are more specific.

[https://docs.aws.amazon.com/vpn/latest/s2svpn/VPNRoutingTypes.html#vpn-route-priority](https://docs.aws.amazon.com/vpn/latest/s2svpn/VPNRoutingTypes.html#vpn-route-priority)

So if my understanding is correct, I cannot use a remote subnet that is in the range of my VPC. If this is the case why is that not something that in enforced when creating the subnet? Given that most VPNs to my knowledge only care about conflicting Subnets. Which in this case aren't conflicting.

Can someone confirm that my understanding is correct? Or if there is a way that I can configure the routing to allow the traffic to my data centre without recreating the entire subnet?

Many thanks. Really appreciate any help possible.",1,aws,2020-10-13
j8dync,"Lambda using EFS getting ""permission denied"" when trying to open a file","I followed this guy's tutorial:  [https://www.youtube.com/watch?v=4cquiuAQBco](https://www.youtube.com/watch?v=4cquiuAQBco&amp;ab_channel=SrceCde)  And HIS example works, but mine does not.

What are the common problems, and is there a better way to debug the issue than to stare angrily at the ""Permission Denied"" message I get when running my lambda test?  


FWIW, when I try to open a file (for write, since my EFS fs is empty), I get the permission denied message, but if I try to copy a file from an S3 bucket over to EFS, it just times out.  (Moved the timeout up to 30 seconds, and it gladly exceeds that every time while trying to copy a 50 byte file).  


Any help GREATLY appreciated!",2,aws,2020-10-13
j8i5st,How to Develop Serverless Vuejs Application with AWS Amplify?,,19,aws,2020-10-13
j8eht7,"I am attempting to get Create an SSL Certificate using AWS Certificate Manager. My DNS is google domains. Followed steps and it won't get out of pending status for status, help?","So, basically, I have a domain name already bought and the DNS is google domains. With AWS Certificate Manager, I followed the steps here:

https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request-public.html#request-public-console

Also, I both did *.example.com and example.com for domains when following the steps (not example.com, but the domain I have. The * is for wildcard).

For step five in that step by step, I selected DNS validation. At the end, I was given instructions to add a CNAME record to the DNS configurations.

So I went to google domains, entered the NAME section given as it was written in AWS, Selected CNAME for type, and for the Value I entered the exact value given in AWS. They literally give you all the values and I copied and pasted them in and verified they are correct.

Yet, after all of that, I am still given the validation status in AWS as ""Pending Validation"". It has been over 12 hours now and the status is still pending validation.

What is going on? To be clear, I have used this domain before with github pages and it worked fine and linked up fine. I since removed those records from google domains and so they should no longer interfere with anything. Also verified that nothing shows anymore either when going to my URL.

Does anyone have any suggestions as to how I can get the validation status to go from ""pending validation"" to ""success""?

Just very confused and frusterated, as I am basically following what the documentation says. Hopefully someone can help, thanks.",13,aws,2020-10-13
j8bjjb,Managed Service SLA's,"Does anyone self-measure managed service SLA's for AWS services like RDS or MSK. They claim 99.95% but how do you (1) know you are receiving the advertised uptime, and (2) detect/measure outages to submit claims for reimbursement?

Is this automated by AWS? Is there an un-biased 3rd party service/auditor that does this?",4,aws,2020-10-13
j8ah2k,DynamoDB streams to ingest to Elasticsearch,"We have an elastic search domain which we put some of our dynamodb records into to facilitate with queries.  Right now we have a dynamodb stream that insert  or updates records to elasticsearch when the dynamodb table changes, but we are seeing it take 1 minute to move over 40 records.  We also have parallelism set to 10, but the performance is still bad.  Is there a special configuration that needs to be made to make dynamodb streams processed at a faster rate?",3,aws,2020-10-13
j87omh,Prerequisites for learning AWS + Terraform?,"I've started my first job in the industry which includes AWS (Servers and Serverless) and Terraform. I've started learning both, which hasn't been the easiest as someone with mainly JS experience. I have a CS degree in which I slept through the networking module. I've been trying to understand VPCs, subnets, LBs and how private/public IPs work with the companies existing policies. Are there any specific topics that I should delve into to that will aid me in my pursuit of learning AWS? Any resources would be appreciated too, thanks.",3,aws,2020-10-13
j8844j,"Several attacks on EC2 instance, and docker logs is outputting weird format","I have a docker contianer running on an ec2 instance, where there have been several attacks and exploits (mostly, trying to access admin URLs)

Now when I run my docker logs, my terminal is filled with this input afterwards.

&amp;#x200B;

https://preview.redd.it/khfwtncl35s51.png?width=1645&amp;format=png&amp;auto=webp&amp;s=85abdafc0d06c4b93630ae5bd8db30c45731c237

I'm kinda new to working with EC2 instances, but has everybody experienced something like this before. Has my application been compromised?",2,aws,2020-10-13
j86d9u,AWS Cloud Map simplifies service discovery with optional parameters,,11,aws,2020-10-13
j866x7,WAFv2 rules disappeared,"Yesterday, Using WAFv2 Automation CloudFormation template I created  a new WAFv2  Web ACL with additional custom rules in the eu-central-1 region. Everything was working fine and I associated some load balancers. shockingly, the Web ACL is back to defaults with all  my new rules and customizations gone. I don't know where they have disappeared to? Anyone experienced this before?  pinging  [/u/JeffBarr](https://www.reddit.com/u/JeffBarr/)",0,aws,2020-10-13
j85jzc,AWS Canada Beanstalk routing through US,"We have an elastic beanstalk setup in the Canada region (I am located in Canada as well), when I run a trace route command (tracert) on the IP of the associated EC2 instance, one of the IPs it goes through is located in Ashburn, Virginia ( 52.95.219.214 ).

Does anyone know why the packets are being sent through Virginia, and is there any way to make it so packets are only sent within Canada?

Thanks!",1,aws,2020-10-13
j85gj1,Serverless Help Needed,"Hey there,

I am building a web platform using a Serverless Architecture.

I am unable to find any docs/solutions for the following problem, any help is appreciated :)

So, I want to build a static User ID URL for every user. Like an Instagram profile link. Example : A user sets their username as ""HappyBoy55"". I want my website to incorporate xyz.com/user/HappyBoy55 to be there.

I had an idea :

Create a new folder in my S3 bucket everytime someone sets up their profile. Given the cacheing mechanism in CDNs and the general absurdity of the idea, I decided to drop it.

Any suggestions ? Thanks !",6,aws,2020-10-13
j85acv,"CloudWatch alarm which monitors spike in number of files created on an S3 bucket (or better yet, folder) within a certain time period","The only thing which I have seen so far are the 2 metrics: \`numberofobjects\` and \`bucketsizebytes\`  
The first one simply monitors if # of files in total are above or below a certain threshold...   
The other option is simply going to look at the total size of the bucket, which isn't what we want either.  


The scenario which we experience is a spike in the # of files which are created within a 1 minute period... How do we catch this?",3,aws,2020-10-13
j84ld1,Amazon EMR integration with AWS Lake Formation is now generally available,,27,aws,2020-10-13
j84h22,Why Doesn't AWS Have a Cloud Run Equivalent?,"Does anyone know why AWS doesn't have something similar to [Cloud Run](https://cloud.google.com/run) where you run your container and are billed only when your container receives incoming requests? It is similar to Lambda but instead of FaaS, it is CaaS but with the billing model of FaaS, unlike ECS and EKS where your container runs all the time. I would think that this would be an attractive option for companies that are still building traditional apps that can be containerized but don't want the complexities of ECS or EKS and want to move to the cloud and benefit from the auto-scaling, per second billing, etc. In Lambda, AWS is already running a full container but to serve a single request at a time. Using Cloud Run, you can serve dozens or more concurrent requests using the same processing footprint",88,aws,2020-10-13
j83zbq,How does a private image built from a marketplace AMI get billed ?,"I'm using an AMI that has a $0.02/hr subscription fee. I have added additional configuration on top and created a private AMI that I use in a CloudFormation template.

I'm assuming that using my new private AMI built off the marketplace AMI will still incur the subscription charges (which it should, I'm not trying to circumvent that). Does anyone know if this works the way I think it does and what mechanism is used to know that an EC2 instance is using an AMI derived from a subscription based AMI ?",1,aws,2020-10-13
j82dai,AWS kinesis react native,"Hi smart people, I am trying to implement video calling in a React Native app through AWS Kinesis. but I was unlucky to find anything related to this on the web. do you have any example code I can use? or any tutorial? if not, how easy it is to bridge the native library to RN?Thanks",1,aws,2020-10-13
j82j9b,"New AWS user, 502 Bad Gateway","**What I'm using:** Laravel 8, CodeCommit + CodePipeline, Elastic Beanstalk.  
**What I've done:** set up code pipeline, pushed initial source w/ .env, pipeline updated EB, and then I set root as /public  
**What I've got:** 502 Bad Gateway, no errors in EB log file

I'm reading the documentation for resolving 502 errors using EB. It says I need to enable S3 error handling so I can get more information, but my application/environment doesn't have an S3.

I've been going nuts for weeks trying to fix this, and only now did I think about asking for help on reddit...   ¯\\\_(ツ)\_/¯",1,aws,2020-10-13
j80gsj,Canon Imageclass mf644cdw printing doublesided when printing through AWS,"Hello, I am using Amazon WorkSpaces on Windows 10 and just installed a new Canon printer to replace an older Brother printer that never produced the issue above.  All settings locally and within WorkSpaces seem to indicate single sided printing but it continues to print double sided.   It uses the teradici client render xps

Any suggestions would be much appreciated

https://preview.redd.it/fyatgwf023s51.png?width=606&amp;format=png&amp;auto=webp&amp;s=3cc16c271ecfdf62aa6c6593795f424c6e1af9ce

https://preview.redd.it/9rue86dc13s51.png?width=1080&amp;format=png&amp;auto=webp&amp;s=763422d5039fd0e8edcb26a84a7c171bcde3ee7c",1,aws,2020-10-13
j7zcvq,Boto3 filtering on ec2 tags,"I'm trying to apply filtering on EC2 tags and get ONLY  the instances which have two specified tags. For example 'foo' and 'bar'

So far, I'm tried with the instances.all() method and got only the tag values.

But I need the instance id along with the tag names as well.

     for instance in ec2_client.instances.all():
        print(instance.tags)",1,aws,2020-10-13
j7xvib,US-EAST-1 outage use1-az2,"We are facing issue for new instance scaled from 1:20 AM PDT having netowrk connectivity issue.. Tried other 2 AZ B,A &amp; D..

Is anyone else facing problem? What other service.. I believe all major service calling EC2 or network interface will be affected..

&gt;Network connectivity issues  \[01:48 AM PDT\] We are investigating networking connectivity issues for a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. We have identified root cause and are working towards resolution. Network connectivity for existing instances is not affected by this issue. For newly launched instances that are affected, relaunching a new instance may resolve the issue.  
&gt;  
&gt;\[02:47 AM PDT\] We continue to work toward recovery for the networking connectivity issues affecting a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. Network connectivity for existing instances remains unaffected by this issue. For newly launched instances that are affected, relaunching a new instance may resolve the issue.  
&gt;  
&gt;\[04:53 AM PDT\] We are still working toward recovery for the networking connectivity issues affecting a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. Network connectivity for existing instances remains unaffected by this issue. For newly launched instances that are affected, launching a replacement instance may resolve the issue.

We beleive the issue started well before this.. probably 3-4 hrs before,  when we started noticing the higher error rates but were not sure of incident then. Can AWS come clean on when it actually started &amp; how soon can this be resolved??  sitting ducks for now!.",48,aws,2020-10-13
j7xi2z,AWS Pen Testing and Whitelisting,"I manage security for an organization that has an AWS environment that is used for eCom as well as some other functions. We are required to have a pen test annually and to have the segmentation of the network tested (technically two separate tests) as well. 

The setup for the test includes placing pen test jump hosts at various places within the environment so that the tester can simulate having a foothold inside various internal networks in the environment. We have a standoff over a configuration point: I say that while we modified the test platforms and applied a SG that will allow all outbound traffic from the test platforms/jumphosts to all IPs in our environment, we have not modified the SGs on any other hosts in the environment. Similarly, we have not modified the NACLs in any way. It is our position that these modifications would invalidate the test since the test should be to determine whether or not our controls are effectively protecting and/or segmenting the environment. 

The tester's position is that his test platform should be whitelisted everywhere so that he can reach all the hosts and access anything in the environment. 

I'm looking for any sort of references that are out there that would provide guidance on which position would be considered best practice. I've looked all over and all I can find is references that call out physical networks and the practices there. Can anyone point to cloud-specific guidance on this? I'd prefer AWS specific, but I'll take what I can get.

&amp;#x200B;

Thanks",3,aws,2020-10-13
j7uxk0,Insufficient capacity for lambda + API Gateway,"Hi there, I’ve started receiving this 5xx error in the API Gateway + Lambda in the Sydney region a day ago.

“We currently do not have sufficient capacity in the region you requested. Our system will be working on provisioning additional capacity. You can avoid getting this error by temporarily reducing your request rate.”

As a result, our services were down for hours. Is it possible for lambdas to fail to start because of capacity issues? If so, how can I prevent this from happening by routing requests to backup instances say an ECS or EC2.",5,aws,2020-10-13
j7uopz,Issue with my (Lambda + EFS) backend. Any help would be appreciated!,"Greetings!

I have an AWS Lambda function running some code that requires tensorflow as a dependency. Since the tensorflow library is much larger than the lambda layers allow; I loaded the library onto an Elastic File System and connected that to my lambda function. This created this setup in late August and it was working great for \~5 weeks.

A couple of days ago the error rate for the lambda jumped through the roof. Error message:"" This function instance was stopped because Lambda detected an IO process that was taking too long likely due to low throughput in an EFS file system"".

I have more than enough bursting credits at the moment, but compared to the past 5 weeks, my EFS Bursting credits fell off a cliff. Lastly, when the lambda does not throw an error, it takes 10 times longer than it used to before this error started appearing. (It used to take \~7 seconds, now it takes 70 seconds)

I have attached images of the lambda error rate and EFS Bursting credits  with this image. What really blows my mind is that I haven't changed anything in the backend since. **I am out of ideas and would really appreciate any help.**

Thanks,

https://preview.redd.it/v0fnk3kav0s51.png?width=1607&amp;format=png&amp;auto=webp&amp;s=2ff1bfc08ed6c5e1d53f58c39d0e595198fad6e2",1,aws,2020-10-13
j7rot4,Up your AWS CloudFormation testing game using TaskCat | Amazon Web Services,,6,aws,2020-10-13
j7vcup,Oneliner to map aws AZ ids to AZ names,"[https://gist.github.com/awsiv/bc7a2a28679a7bfc59b988134a8597bd](https://gist.github.com/awsiv/bc7a2a28679a7bfc59b988134a8597bd)  


\`aws ec2 describe-availability-zones --region us-east-1 | jq -r '.AvailabilityZones\[\] | .ZoneId + ""="" + .ZoneName'\`   


With the current DNS issue in \`us-east-1\` - this has proven quite helpful",8,aws,2020-10-13
j7uu19,AWS CodePipeline Docker on existing EC2 Machine,"Hi everyone. I'm pretty new to AWS concepts and DevOps concepts, I'm currently studying both of them worlds. Right now I'm free tiering for everything since we're trying to bootstrap a startup here in my country.   
I'm trying to make a Pipeline that deploys a Docker image on an existing EC2 machine that we are already using for other things ( such as Redmine ). 

I think that the Pipeline should be something like this:  


1) CodeCommit for the source code   
2) CodeBuild that run the build process for the docker images  
3) ECR that contains the built docker image  
4) CodeDeploy that runs a script which pull the new built image and run it on the existing EC2 machine  


Up until point 3 everything is cool and fine, I don't really know how to make point 4. I was thinking of maybe running a remote script from a ssh shell ( I don't even know if it's possible on CodeDeploy ) with the .pem that pull the images and run them.  
Have you ever came across a problem like this?",2,aws,2020-10-13
j7se01,Timestream Query Engine Pruning (with Tableau),"Not a lot of community resources out there for Timestream yet (but hopefully soon!) so this might be a bit of a long shot.

**Quick Context:** I have machine data streaming into timestream and would like to query it with Tableau.  I have the JDBC driver setup and it appears to work fine to connect to the db, let me choose my table, and shows the dimensions.  However when it goes to query the data, it throws an error which appears to be because the table name isn't prefixed with the database name:

    timestream jdbc Invalid database/table name

e.g. it uses

    SELECT * FROM table 

instead of

    SELECT * FROM database.table

I have replaced the table object in my data connection with custom SQL that is just

    SELECT * FROM database.table

and everything appears to work just fine now.

**My question is,** since this is essentially running my tableau queries as a subquery inside a SELECT \* does this mean my entire table is getting scanned for every query (holy $$$ batman)? or is it passing through all the filters I've applied to scan a much smaller chunk of the database (Query Engine Pruning)?  Better yet, is there a way to check query logs to see GB scanned for each query?

TIA",1,aws,2020-10-13
j7rfiq,How to point root record to an ALB?,"The ALB’s IPv4 constantly changes, and I can’t use a CNAME for the root record (@).  

I’m on Google Domains and managed to forward the @ record to the www subdomain. The problem is the client doesn’t want the www as the root is the perfect name for their brand.  

Now I’m thinking, would it be possible to point the root record at an NLB, then have the NLB point to the ALB? It’s a lot more rerouting than I was expecting, so wondering if it’s possible and if so, is it a good practice to do so?",3,aws,2020-10-13
j7r0kv,Problem with AWS Fargate and Elastic ALB's - Any Help Would be Appreciated...,"Hey Friends!

I have a question/problem with AWS Fargate and Elastic ALB's that I could use some help with. I've been banging my head against a figurative brick wall all day and now I have a massive headache. I feel like this should be simple but for some reason it's just not. Here's the problem I have a node js application that I've just finished putting into a docker container. I exposed a port in the Dockerfile, login into ECR, build the container, tag it and push into ECR and then finally deployed the container into a Fargate task. When it's deployed I can access the container using both the Public IP and the port that I exposed. Ideally I would like to have this container behind an Application Load Balancer (ALB) so I create a cname with Route 53 and point it the ALB's Public DNS name. This is my though process of how It should go:

1. deploy container into AWS Fargate
2. open port xxxx in security rule attached to eni-xxxx that is attached to AWS Fargate.
   1. Test to see if I can access the container by using both the public IPv4 address and port.
3. create target group that has private IPv4 Address of container hosted in AWS Fargate.
4. create security group to be attached to the ALB that accepts traffic from everywhere but only on port 443.
5. modify security group attached to eni-xxxx that's attached to AWS Fargate to only accept traffic from the security group attached to the ALB. 
6. deploy ALB attaching security group and target groups created.
7. copy Public DNS name from ALB and create a cname that points to the ALB's public DNS name.

When I test... I get 502 bad gateway... which doesn't make that much sense to me. This in my mind should work for me, it looks like the ALB is rejecting my packet... so r/aws what am I doing wrong? I've added logging to the ALB and nothing of useful is displaying in the logs, as well I've used both dig and traceroute... I can confirm dig is pointing authoritatively to the ALB and traceroute confirms packet is being dropping off at the ALB...",3,aws,2020-10-13
j7qnw4,Auto Accept Resource Shares,"Hey guys, just throwing this out there. Anyone have a better way to automate accepting a cross account resource share - other than a CLI script?
Any existing custom resource Lamba that will accept it for you?",1,aws,2020-10-13
j7qzxe,SNS Message to Trigger CodePipline,"Is there an easy way to have SNS trigger a CodePipeline (or CodeBuild)? I want my CodePipline for Packer to run whenver a new update is release for the public Amazon AMIs.

&amp;#x200B;

[https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/aws-windows-ami.html#subscribe-notifications](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/aws-windows-ami.html#subscribe-notifications)",2,aws,2020-10-13
j7qv5f,DNS issues,"I just purchased a domain name and fixed up everything using route 53 and all.
But when ever I try to assess my site using the domain name I bought, It works, but after loading, it changes to the IPV4 addresses. I’ve tried googling the problem but can’t seem to find a solution.",2,aws,2020-10-13
j7pync,Deploying application configuration to serverless: introducing the AWS AppConfig Lambda extension | Amazon Web Services,,14,aws,2020-10-13
j7poz0,Consuming RDS postgres database,"I'm new to AWS and just finished setting up my database.

What would be the best/easiest restful API to implement with an RDS database?",2,aws,2020-10-13
j7oxkf,One big lambda doing parallel tasks or many lambda functions?,"This might be a no-brainer for some, but for me, this is the first time I worked with Lambda backends at scale (previous experience is smaller single independent lambdas).

I've built a backend for a chatbot and I now want to expand with dashboard functionality. I'm gathering stuff like (all these can be filtered per day/week/month/year individually):

* Average No. chats
* Average chat length
* Average feedback
* etc

We don't plan on making the dashboard flexible yet, so it will almost always retrieve all of these metrics. Would it be better to have a small lambda for each of them or to have 1 big lambda doing everything?

p.s. I imagine I have to start thinking about the caching side of this solution as well?",1,aws,2020-10-13
j7o47b,PeopleSoft Database Migration to AWS RDS Oracle,"Hi Guys,

Just wondering if anyone has used AWS DMS to migrate the whole peoplesoft oracle database to AWS RDS? We have database around 1/2 a TB and wanted to use this service to migrate the whole database. However the worry I have is the long list of limitations on aws documentation regarding using oracle as the source database:  

[https://docs.aws.amazon.com/dms/latest/userguide/CHAP\_Source.Oracle.html#CHAP\_Source.Oracle.Limitations](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.Limitations)

Will these limitations impact a peoplesoft oracle database?

Many thanks!",2,aws,2020-10-13
j7o37m,IaC comparison for a simple API,"Thanks for all the great feedback on my [previous post](https://www.reddit.com/r/aws/comments/j5uch5/iac_comparison_for_eventdriven_architecture/) comparing [event-driven architectures built the AWS CDK](https://outwiththeold.info/posts/event-driven-iac/).

In my next post, I define a [simple API infrastructure with the CDK](https://outwiththeold.info/posts/simple-api-iac/) using both a serverless approach and containers and compare the resources needed by each.

As before, I would love your feedback and suggestions for use cases you would like to see implemented using serverless technologies (or don't think can be handled in a serverless way).

You can read the new post here:

* [https://outwiththeold.info/posts/simple-api-iac/](https://outwiththeold.info/posts/simple-api-iac/)",1,aws,2020-10-13
j7ngkb,Amazon ElastiCache Day,,25,aws,2020-10-13
j7kgkk,how far into a multi account strategy are you and whats you're biggest cost?,"Hello, 

we are quite far into a 'scaleable account architecture' design with our client and could essentially 'scale forever' in a self service consumable model for the client, but there is a bit of an allergic reaction to the perceive cost of 'lots of accounts'.   


Anyone running 'a lot of accounts' and care to share what the biggest costs are?   


we centralize networking and use VPC sharing so NAT gateway costs are mitigated, biggest concern from the customer are 'security stuff' with specifically 'lots of checks' being mentioned.   
I'm looking to track down data to support or discredit the idea that those kinds of things would be volume based, so you should end up paying for 'total checks against thing' rather than getting penalized for a model that aws seems to promote. Basically, are there large financial penalities for making an account structure too granular?",1,aws,2020-10-13
j7kxan,Do you login root user?,"[Non enterprise - startups, SMB, hobby, personal] Account owners / admins

AWS strongly recommends the that you use the root user only to create the first admin IAM user.  So how many of you follow that recommendation? What do your use to administer your account?

Edit: most enterprises have the framework to support saml or other enterprise identity manager, So narrowing the question to non enterprise users - startups, SMB, hobby, personal account

[View Poll](https://www.reddit.com/poll/j7kxan)",0,aws,2020-10-13
j7k98m,Sagemaker Studio billing after deleted,"Hello!

I've created a Sagemaker Studio (the new one) yesterday for try and after this i have deleted following this doc ([https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-delete-domain.html](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-delete-domain.html)). All fine.

Today CloudWatch send me a Billing info saying that i have a SageMaker Studio Notebook Instance ml.t3.medium running. Anyone known how to solve this?

i've checked for running instances, notebooks, etc. Nothing.

&amp;#x200B;

Thanks all.",1,aws,2020-10-13
j7m9ba,Where to serve web content from? S3 or EC2?,"I've got a background in general operations and maintenance on a few lift and shift AWS accounts. Don't really know much about web applications/web site hosting, but my client wants to start hosting their own site. Doing a bit of research and trying to understand the pros and cons of serving content from S3 or EC2? Or do I do both depending on what it is?",2,aws,2020-10-13
j7m2ju,3rd Party AWS Support Plans,"My company is entirely on AWS and currently on their business plan for our production accounts. Unfortunately I don't feel that the Business plan gives us the support that we need for our accounts but I can't justify the cost of the Enterprise plan to leadership.

I'm trying to be a bit creative here but are there any third party vendors that can provide the middle ground between Enterprise and Business support?",1,aws,2020-10-13
j7lzlr,should i get a cheap RDS or aws?,"I'm not going to need much space. just to track the customer id and some api keys, and a usage counter for a couple thousand users.

less than 1gb data is fine but will access potentially hundreds of times a second. 

any reason i shouldn't cheap out with something like a2 hosting (5$) vs amazon (13$) per month?",0,aws,2020-10-13
j7lpa2,"Multiple MFA for root, when?","6 months ago at the beginning of UK lockdown, I asked AWS support about this, and it’s still not solved. They suggested it was a good idea and could see the use case for it.

We keep our root yubikeys in an office safe. Well, thanks to Covid, we’ve closed our offices and are perm remote workers now. I’ve ended up with the keys, but this isn’t very good for resilience.

When can I add multiple yubikeys to my root accounts?

Anyone else have this issue?",5,aws,2020-10-13
j7jx7r,Announcing Amazon CloudWatch Lambda Insights (preview),,11,aws,2020-10-13
j7lcbe,AWS Lambda Extensions (preview),,5,aws,2020-10-13
j7l9b1,Is it possible to attach a Lambda to Kinesis Firehose?,"We have...

API Gateway =&gt; Kinesis Firehose =&gt; S3

Where Kinesis Firehose outputs to S3 every 15 minutes.

Is it possible to add one or more Lambdas to this process?

For example, if I wanted to add a Lambda that ran every 1/5/10/15 minutes to send or analyze the data in the stream?",1,aws,2020-10-13
j7jx08,AWS ELB custom 502 bad gateway page.,"Hello,

I would like to replace the ELB 502 page with a custom ""We are down for maintenance"" one :

[ELB 502 Page](https://i.stack.imgur.com/OaRNM.png)

There are two ways I see myself doing this :

*  Setup a custom HTML page for the 502 error in ELB 
* Upload a ""We are down for maintenance"" HTML page to a free static hosting like netlify.com and redirect ELB traffic to it when the load balancer isn't able to get a response from any instance.

I don't know if there is a way to do any of this. I would like some suggestions or some guide to set this up.

Thank you!",1,aws,2020-10-13
j7iht9,Recovering access to EC2 instance,"I have 0 experience with AWS so forgive me if I'm not explaining things well.

A couple of guys on my team created a Windows server in an EC2 instance and used it as a repository to store Veeam backups.  They are no longer with the company so I'm trying to gain access to this server in order to continue supporting it.  However it seems like a unique Key Pair was generated for this server and I cannot find that file anywhere on our network.  I've looked up some guides and it seems like I'm pretty much screwed as far as getting back into this server without that file.

I did see some guides that basically have you create a copy of the instance, assign a new Key Pair for it, and then log in using that.  I'd have to rebuild some connections which is annoying but at least I won't lose the existing restore points.  I did that but when I try go to connect to the server, the ""get windows password"" section says ""Password is not available.  Please wait at least 4 minutes after launching an instance before trying to retrieve the auto-generated password.""  The instance was generated nearly 20 hours ago and still says the same thing.

Further research says this is possibly being a result of ""EC2Config"" being disabled.  It also sounds like I have to be logged in to enable it?  So does anybody have any idea how I access either of these servers?  Thanks.",0,aws,2020-10-13
j7ifu3,Multiple angular apps deployed in one S3 bucket behind Cloudfront with some advanced routing behavior?,"So I'm trying to work through this and I'm thinking I have to add a Lambda function but the existing Q&amp;A i've seen by way of StackOverflow don't reference any need for a Lambda function so I thought I'd stop by today to see if anyone has any tips!

I have several Angular apps that exist as subdomains:

* [www.example.com/app1](http://www.example.com/app1)
* [www.example.com/app2](http://www.example.com/app2)
* [www.example.com/app3](http://www.example.com/app3)

Currently, I have these apps residing in an S3 bucket under unique paths:

* app1/
* app2/
* app3/

Each Angular app has its' own index.html (therefor I cannot use a common index.html in the bucket root) and currently I'm able to get one app working at a time.

&amp;#x200B;

I have 3 behaviors defined in Cloudfront depending on what subdomain is provided. My thinking was I needed to have 3 behaviors so that if someone goes to example.com/app1/auth/login then the behavior will recognize /app1/\* and will point the request to the origin I have defined which has /app1 as the origin path

What I'm seeing is, unless I define an error page that points to a specific path index.html in the event of a 403/404 (i.e. /app1/index.html) I cannot get advanced routing to work:

* example.com/app1/tenant/auth/login won't correctly resolve if I set the error page to /app2/index.html
* if I set the error page to /app1/index.html then the app2/tenant/auth/login doesn't work correctly because it won't resolve the correct index.html

I feel like my configuration has gone off the rails as I feel like this isn't the correct way to do it.

Anybody else here maintain multiple angular apps in a single s3 bucket behind a cloudfront? Does your Angular app have advanced routing? I would love to hear from you!

Thanks for stopping by and taking the time to read this post! Have a great day

\*edit: if you're downvoting I would love to get some feedback! If this is a dumb question, that is important to know as well!",0,aws,2020-10-13
j7hj9e,AWS IAM and Organizations Account Question,"Hi! I'm trying to create an aws organization, and invite accounts to join. The login requires a unique account ID, or otherwise I just log in as the root. The problem is that when an account is invited, even when it has its own aws sign-in, does not have an IAM account in the environment. Therefore, when the invitation is sent, it just gives the root ID as the login, and I don't know how to provide credentials that link a created iam account with the email invited by the organization.

 I am a beginner, but I do not how to assign IAM accounts to invited users in AWS organizations, even using the switch role and applying the organizational policies outlined in the documentation. How do I invite a user to an AWS organization, and also give it the IAM credentials to login?",0,aws,2020-10-13
j7hzmp,What will break if I remove the AWS SDK for .Net from an EC2 instance?,"Windows servers.

None of my applications use that SDK. Some of them use Powershell, but not the AWS SDK. 

Does the instance stop communicating with AWS if that SDK is removed?",0,aws,2020-10-13
j7gqsy,Introducing AWS Lambda Extensions,,129,aws,2020-10-13
j7ghjv,How to combine lambda and api db update?,"So I have an application where users can trigger a specific external api, for which I have set up a lambda function. The function holds the external api secret key. So a user will have a balance of say 10 units, he can trigger the lambda function once, which should check his balance, reserve 1 unit from his balance and trigger the function. After the action is confirmed the balance should be updated to 9 units. So as ypu can see there should be some conditional interaction between the lambda function and the db/api. 

If this was any express backend this would be easy but I am not sure how to go about doing this? Do I need to set up multiple api's? Is there maybe another service I am overlooking?

I hope I am explaining this situation correctly, let me know if I need to clear up anything. Thanks for your help.",0,aws,2020-10-13
j7evs7,Most cost effective serverless tracking endpoint,"I'm looking for the most cost effective way to create an endpoint that allows me to write a web request to S3.  

My current endpoint is just a listening endpoint implemented using Lambda and Kinesis Firehose, and works well, however the PutRecord to Kinesis call takes ~200ms, which is costing my more money than I would have liked.

Since I do not need much transformation in the actual request(Looking to record path, query string, and headers), I am wondering if there is another way to implement this in a more cost effective way.  I would prefer that the requests are batched somewhat, as the endpoint is seeing about 1 billion requests / day.",0,aws,2020-10-13
j7f5jj,Route53/AWS Gov Cloud access from private subnets,"Hi,

I have a VPC setup in govcloud with private subnets. Whenever I run terraform from one of these nodes its timing out when trying to hit the api endpoint for route53 (it's trying to reach the public endpoint). We have created the endpoints for the other necessary services (ec2, elb, sts, and s3) but I don't see where I can do this for r53.",0,aws,2020-10-13
j7ei2d,How are you guys trying to send notifications after Patching?,"So right now, we have a use case to send notifications after Patching is done and I'm using cloudwatch events to trigger lambda which will modify the message body for sns and sends and notification email to subscribed users.
But moving on we may have to do it dynamically so I was thinking of using ses either in sandbox production or verifying the domain(which I'm not sure how feasible it would be with current situation in the org)

I was curious as to know how you guys are managing it?",10,aws,2020-10-13
j7eb89,Extreme bandwidth usage worry,What is stopping (by default) a user from download a 100mb file thousands of times and giving me a big bill? Couldn't I find AWS customers and mess up their bill if I was a bad actor?,0,aws,2020-10-13
j7cwx0,Testing tomorrow,"I’m testing for the Practitioner tomorrow. I’ve been studying for the past 3 weeks for this and every practice test I find I fail. Admittedly, I would be able to recite everything I’ve studied and I expect to get some answers wrong but failing these practice tests is starting to give me doubts. Anyone have any tips or advice I can use in my last 24 hours before the test?",1,aws,2020-10-13
j7bypr,403 error when uploading file to S3 from browser,"I'm trying to upload file from browser direct to S3 presigned url.  And uploading always gets stuck at \~80% and throws 403 Forbidden error (without response body).

I use PHP (Laravel) backend to generate presigned url:

    $s3 = Storage::disk('s3');
    $client = $s3-&gt;getDriver()-&gt;getAdapter()-&gt;getClient();
    $expiry = ""+1 hour"";
    
    $command = $client-&gt;getCommand('GetObject', [
        'Bucket' =&gt; config('filesystems.disks.s3.bucket'),
        'Key'    =&gt; Str::random(10),
        'ContentType'    =&gt; 'video/mp4',
    ]);
    
    $request = $client-&gt;createPresignedRequest($command, $expiry);
    
    return response()-&gt;json([
        'uri' =&gt; (string) $request-&gt;getUri(),
    ]);

And then execute PUT request with axios from browser:

    const options = {
        headers: {
            'Content-Type': file.type,
            'x-amz-acl': 'public-read',
        },
    };
    
    axios.put(signedUrl, file, options);",6,aws,2020-10-13
j7bltp,GraphQL Tools &amp; libraries pt.3,,16,aws,2020-10-13
j7b25h,Has anyone used ChatQL?,"I want to use AWS AppSync. I wanted to have Chat functionality in my app so my friend suggested me to use ChatQL. The feature I am looking most in ChatQL is to notify the sender that msg has been delivered or not, whether it is read by the receiver or not, along with caching functionality. I am using React Native on client side. Does anyone has experience with ChatQL? Is there any other service from aws which provides real time msg sending along with informing sender whether msg delivered to receiver or not, caching etc. It could be GraphQL or REST architecture. I was using Nodejs socket io lib for real time chat but I needed some trigger for whether msg was send correctly or not. Of course I can create those triggers on my own but I want to stay away from it as there is a high chance I might make some mistake on my part",1,aws,2020-10-13
j7azky,Why isn't my s3 bucket secure?,,0,aws,2020-10-13
j73m8u,No internet access from one particular public subnet...,"Context: I'm running web-scraping jobs within ECS Fargate, configured to run inside one of my three possible public subnets. Running into a repeated, apparently random issue where jobs run inside one particular subnet -- the one in AZ  'us-east-2b' --  are complaining about no internet access. Jobs run inside the other two subnets have no such issues.

&amp;#x200B;

I've confirmed all three subnets have an identical VPC setup as far as route table, security group, ACL. 

I am assigning a public IP to each job.

Any thoughts as to the cause / a fix? Much appreciated!",0,aws,2020-10-13
j753h1,Which AWS Certification Should I get First?,"Hi, for someone who is just starting in cloud services, what would be the first certification for AWS? Should I go for Certified AWS Cloud Practitioner or something else? Any help will be appreciated.",1,aws,2020-10-13
j764h1,How do I use AWS Educate,"Hello! I'm taking an Intro to Programming class in college, I am very new to learning tech. We're being told to get AWS certification, and I chose Software development. Maybe it's just because I have ADHD, but I'm very overwhelmed and confused. I’m getting a lot of information at once, and there are links to other websites that start even more courses on other websites. How do I go through this effectively? It’s all very confusing and a bit overwhelming in how it’s formatted, with each piece of the module being several links to other courses that are several weeks long, or 8 45 minute videos that I may or may not need to watch. I would really appreciate some help, thank you!",0,aws,2020-10-13
j77rfk,Probably a pretty basic CloudWatch question...,"Hey Guys!

Let me know if this is in the wrong place and I will move it.

Hey guys sort've newer to AWS (have a couple certifications and a small amount of experience) and I was tasked with setting up CloudWatch Monitoring Architecture for my medium/large sized company (est. 4000). At the moment our footprint is pretty small and is heading in the direction of what sounds like most of our infrastructure will be in the cloud in a few years.

I set up a dashboard showing CPU Usage/Ram/Disk/Network/StatusChecks.

So I am trying to set up SNS notifications based off certain events/alerts that are triggered in this test environment I built. The things I want to alert for are atm when an instance is stopped/terminated/created which I know will be set up as an event and when a system failed a status check (System and Instance level) which I believe needs to be set as an alarm.

So I set up an alarm for the system check on a single EC2 instance and was able to publish that to an SNS topic and subscribe myself just fine but how would I do this in groups/AutoScalingGroups or in mass? When I try to select other instances into the alarm it will not let me. Seems like 1 Alarm = 1 EC2 Instance. I saw something about Composite Alarms but that doesn't seem to be what I am looking for. 

What are the ways around this? Do I need to created a separate alarm for every EC2 instance and subscribe them all to the SNS topic? How do I ""group up"" instances for easier monitoring/alerting? Is this the right way to do this?

Let me know if I need to clarify anything or provide more info.

Thanks guys!",2,aws,2020-10-13
j717s9,Extracting a CFN template from an existing architecture,"So I've made a bit of research to try **to avoid writing down a CFN template for an architecture I had already set up** and which I want to reproduce elsewhere. Here are my findings:

* [CloudFormer](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html)
* [Former2](https://github.com/iann0036/former2)

Alternatively, in a similar spirit, there is also (but you have to manually recreate your whole architecture in the Console while it's recording your actions):

* [Console Recorder](https://github.com/iann0036/AWSConsoleRecorder)

---

I do, however, **want the result to be reusable by other people**.

Are you guys aware of whether or not there will be a layer of abstraction for the location of the deployment, for example? Something that would allow, say, a `us-west-1` parameter rather than directly using the `ca-central-1` being used by my stack.

---

Considering that the `CloudFormer` warns about the fact that it'll temporarily create resources, and because I'm running on a Free Tier that is already maxed out, I'm kind of on the hold for trying this stuff.

Both `Former2` and `Console Recorder` avoid mentioning whether or not they create resources in order to generate a template.

... so for now I'm trying to avoid running anything, and would love to get some community insight. :)

---

**EDIT**: Because someone asked for it, you can understand a bit better where I'm coming from with [this reddit post I made yesterday](https://www.reddit.com/r/aws/comments/j6k8z9/where_to_find_templates_of_iac/).

The actual stack is:

### My setup:
* ECS (for EC2, not Fargate)
  * Cluster
  * Service
  * Task Definition
  * IAM Role (created automatically by ECS)
* EC2 (On-Demand, t2.micro, AMI: ECS-Optimized Amazon Linux 2) : mostly managed by the ECS Cluster
  * Auto-Scaling Group (desired 1, min 0, max 1)
  * EBS Volume (Prevent deletion, 30GB, gp2)
  * Key Pair
  * Security Group
  * Launch Configuration
  * Elastic IP (on re-create if your instance dies, configured to be associated automatically!)
* Cost Explorer -&gt; Budget Alarm

### ECS configuration
Initial set up was done through following [this tutorial](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started-ecs-ec2.html).
 
 Then, to customize the set up to work with GitHub Actions, the following changes were made:
 
   1. Changed the ``containerPort`` of `PortMapping` from ``80`` to ``80``.
   2. Removed the ``commands`` that was setting up a default HTML page to land on.
   3. Updated the ``entryPoint`` to match a jar execution: `java, -jar, /app.jar`.
   4. Edit the ``build.gradle`` file so that jars do not include version numbers and include Shadow plugin.
   5. Add a ``Dockerfile`` which extends our JDK (alpine version) and copies the generated jar.
   6. Added a ``workingDirectory`` value of `/`.
   7. Changed the ``image`` to point toward our Docker Hub repository.
   8. Enforced the ``requiresCompatibilities`` to be `EC2`.
   9. The minimum healthy percentage modified from ``100%`` to `0%`, and maximum from `200%` to `100%`: this is to ensure that we can kill a running task and then start a new one, and never have more than 1 task in the Container. See [this](https://stackoverflow.com/a/40741816/9768291) SO answer to understand better.",0,aws,2020-10-13
j7a4qc,"reuse cloudfront for dev, test and prod?","Due to company policies, our global security team runs an uninformed Qualys scan on every CloudFront we have. This results in roughly 50k requests every other day (it scans for everything you can imagine, even WordPress and Joomla even though we don't use that).

To avoid doubling or tripling our costs, can I reuse a cloudfront that points to [dev.example.com](https://dev.example.com) and [test.example.com](https://test.example.com) ? Do I need to reuse my bucket for that as well or can those be separate?",2,aws,2020-10-13
j79j8e,Any lamda sample/tutorial to serve a static website from S3?,"This is what I post earlier:

[https://www.reddit.com/r/aws/comments/j6imbx/tried\_to\_build\_my\_personal\_blog\_using\_aws\_static/](https://www.reddit.com/r/aws/comments/j6imbx/tried_to_build_my_personal_blog_using_aws_static/)

I want to serve folder with HTML files on S3, Couldn't find on google

Please help, thank you",0,aws,2020-10-13
j78pyb,What DB for Lambda to work with GEO SPATIAL data?,"Hi Guys,

What would you recommend to use to be able to store and query (efficiently) spatial data with a serverless app on AWS?  
I am working with Node and there is a lib for geo hash for DynamoDB but its support is pretty uncertain, so I am not sure about it and unfortunately, DynamoDB does not support spatial data out of the box.  
I thought about using Mongo Atlas hosted in same DC, but network peering is available from M10 which is $70/month which does not go well with cost scaling and without it, first of all, will be slower and it will incur fees for data transfer out of AWS (pretty stupid...).  
Maybe someone has some experience with similar serverless app on AWS?",1,aws,2020-10-13
j77q6n,Making a Virtual classroom in AWS?,"We are basically wanting to set up a virtual classroom environment where every one has their own instance to log into and work from.Every machine would be the same and would not need to be connected together. It's to learn a program in windows. 

At the end the instances are reset back to a default state (either via deleting and creating new ones or some other snapshot method).

Has anyone got advice on how to go about it? 
My idea was to create my own image then deploy 10 of the same image but then i am wondering what's going to be left behind if i blow them away as sometimes NIC's and other configs are still left.

Thanks",1,aws,2020-10-13
j76tq2,Start / stop MediaLive channels with schedules?,"I’m confused whether it’s possible to start and stop channels with a Schedule in MediaLive.  

The documentation says “The schedule is a list of actions that a channel performs as it is running.”

https://docs.aws.amazon.com/medialive/latest/ug/x-actions-in-schedule.html

So I’m guessing that a channel cannot be started with a schedule, but it seems weird not to have that feature. Am I missing something?",0,aws,2020-10-13
j75id9,Confused on cost of instance and how it's billed,So I currently have a server set up for running Foundry VTT (table top rpg program like roll20) and right now I'm on the free tier for 12 months which is awesome. My question is if I only need to server to run maybe 40hrs or less/week do I only get billed for those hours? I've looked through the pricing on the website and I feel it isn't clear. I don't need to instance running 24/7 so can I just pay for the hours I use? Also if this is the wrong place to ask let me know.,6,aws,2020-10-13
j72p53,Should I expect capacity issues with AppStream 2.0 on-demand fleets and G4 instances?,"I am considering AppStream 2.0 for a project that requires g4dn.xlarge instances. Wait times of 1-2 minutes are acceptable and I would like to use on-demand fleets with a few stopped instances. I am well aware of InsufficientInstanceCapacity errors with EC2 and G4, and recently had to switch from us-east-2 to us-east-1 since I was not able to launch any G4 instances for a week in all AZs.

Should I expect similar issues with AppStream 2.0? I am asking about a situation in which there is available capacity in the fleet but stopped instances cannot be started due to (internal) resource availability issues.

I read the user and API docs and could not find any mention of such issues. In fact, it looks like ""Insufficient Capacity Error"" has a completely different meaning - it refers to scaling policies.

Edit: pinging /u/AppStreamAtAWS :-)",3,aws,2020-10-13
j711cu,Unable to Locate AWS TCO Calculator,"Hello everyone,

I would like to state that I am finding trouble locating the AWS TCO calculator, while every link and every button always redirect me to the [https://calculator.aws/#/](https://calculator.aws/#/)

Where can I find the  calculator that can provide me with the on premise vs aws costs?

&amp;#x200B;

&amp;#x200B;

Thank you.",0,aws,2020-10-13
j6yt8p,How to attach an AWS EBS storage volume to your Docker container,,0,aws,2020-10-13
j70ns7,Direct Memory Access,I am doing research on linux splice and I came across DMA. It is said that DMA must be enabled. Is this something already enabled on AWS servers?,0,aws,2020-10-13
j70d5r,New – Redis 6 Compatibility for Amazon ElastiCache,,43,aws,2020-10-13
j6xhk8,[HELP] EFS or S3?,"I'd like 3 of my servers to share a 200 GB filesystem. Looking at AWS solutions, EFS seemed like a good choice, I'd just put the 200 GB of data on the EFS and attach it to all 3 servers. The data is AI models which are about 5 GB each and all frequently accessed.

I am just wondering if anyone can confirm this is a good choice since I'm not sure it throughput and access and be a problem with EFS? Alternatively I could just have all of the data on S3 and keep syncing the servers to it.

Really appreciate any input!",0,aws,2020-10-13
j6zndf,Is an instance aware if it's an ASG?,"Simply put, is there any way on an instance (without AWS CLI) to identify if that instance is a member of an ASG? I've looked through meta-data and followed an AWS dev thread requesting tags be available in meta-data that's at least 10 years old, but I can't find anything. 

The long story: I'm looking to bake ansible pull into my on-prem and aws images in order to have a new system come online and answer some kind of ""here I am"" script in order to have the system logged in a cmdb/ansible inventory. But I want some instances to then pull other playbooks/whatever but not if they're a member of an ASG. As this is happening across lots and lots of accounts with different roles and permissions, I don't see AWS CLI as a solution. If tags were available locally that would work, but, alas... the only other idea I can think of is the in the user-data during launch have a file touched if ASG or something, but that seems pretty hacky and maybe a little unreliable.",1,aws,2020-10-13
j6yt23,Creating Linux instance with IDE for multiple people to connect to at the same time,"Hi everyone! I would love to work together with some strangers on code in WebStorm from JetBrains on the same machine, and since I don’t want them to be ablehnte access my machine, I want to set up something on AWS with a visual user interface and an IDE where we all can log in at the same time.
What should I set up on AWS, how would everyone best connect to it and is there any other security details I should be aware of?",1,aws,2020-10-13
j6yim8,Tools and best practices for user and activity auditing at AWS,"Pretty new to AWS and wondering what tools oand/or practices to make the environment secure specifically on user access, activity and login auditing, mfa, and anything revolving under IAM? Hoping for some guidance from you all.",0,aws,2020-10-13
j6xaqw,CloudFormation ECS ServiceDefinition Model validation failed - expected number found string,"Hi all,

I'm having an issue that I don't understand. I have a CloudFormation template for deploying an ECS cluster with Service- and TaskDefinitions. The ServiceDefinition creation fails with:

    Model validation failed (#/HealthCheckGracePeriodSeconds:
    expected type: Number, found: String #/DesiredCount: 
    expected type: Number, found: String #/DeploymentConfiguration
    /MaximumPercent: expected type: Number, found: String
    #/DeploymentConfiguration/MinimumHealthyPercent:
    expected type: Number, found: String)

I tried two variants, with and without quotes but they both give me the same error:

      DeploymentConfiguration:
        MinimumHealthyPercent: '100'
        MaximumPercent: '200'
      DesiredCount: '1'
      HealthCheckGracePeriodSeconds: '60'

and

      DeploymentConfiguration:
        MinimumHealthyPercent: 100
        MaximumPercent: 200
      DesiredCount: 1
      HealthCheckGracePeriodSeconds: 60

What's going on here?

EDIT: Turns out the issue was a wrong mapping in the service definition and the error message was just wrong.",3,aws,2020-10-13
j6xvi7,Best way to get data across regions?,"I have DDB tables in many regions around the world (NA, EU, etc.) and need to aggregate those tables into a service in 1 region.

I can't use Global Tables as I will be needing only a small portion of each table, so it's not worth having the entire table replicated globally.

The data in each region will go through an ETL process regardless, and the data will end up in S3 anyways, so an idea I had is to use S3 CRR to replicate each region's bucket into a centralized bucket.

Is this the best approach? Or is there a better time to replicate the data somewhere between DDB -&gt; S3?

Thanks!",1,aws,2020-10-13
j6qvju,Working with S3 pre-signed URLs,"A great way to secure access to your S3!!

Grant temporary access to objects in AWS S3 buckets without the need to grant explicit permissions

[https://www.altostra.com/blog/aws-s3-presigned-url](https://www.altostra.com/blog/aws-s3-presigned-url)",0,aws,2020-10-13
j6s8xl,"[HELP] Multi-AZ EKS - Instance ""OutOfService"" on Load balancer Ingresses",,1,aws,2020-10-13
j6sgw9,Who manages AWS accounts in your organization?,"In your organization, who's in charge of the management of AWS accounts? By management, I meant things like...

* Provisioning new accounts
* Handling IAM of accounts (e.g. AWS SSO &amp; managing associated AD groups)
* Enabling opt-in regions
* Defining, provisioning and maintaining (human) roles
* Defining, provisioning and maintaining (service / instance profile / ... ) roles
* Turning on governance/security service which are not integrated organization-wide such as Config, GuardDuty in all regions",2,aws,2020-10-13
j6tlfv,WAF does Override rules action require Enable count mode ?,"* i have my default action as allow
* i have  AWS-AWSManagedRulesCommonRuleSet  enabled but i wanted to  SizeRestrictions\_BODY to overide
* and bellow AWS-AWSManagedRulesCommonRuleSet is my custom rule
* my custom rule is body size if greater 100000 bytes block it.

does enabling count mode make everything under AWS-AWSManagedRulesCommonRuleSet as count?

&amp;#x200B;

basically i want to disable  SizeRestrictions\_BODY and do it my self.

also i need to upload an image ",1,aws,2020-10-13
j6u9az,Amazon SageMaker Continues to Lead the Way in Machine Learning and Announces up to 18% Lower Prices on GPU Instances,,23,aws,2020-10-13
j6wvv8,WorkMail without any users,"Has anyone used WorkMail for the ability to create/use a domain without actually adding (and paying for) any users?

I'm looking at using WorkMail+SES+S3+Lambda to create an ETL automation pipeline. Configure a third-party system to email a report to a WorkMail user email address, have an SES rule for incoming emails that PUTs the email to S3, and have the S3 PUT trigger a Lambda.

If I don't need to store the email in WorkMail, it seems like I don't need to pay for any users. I really just need the domain. I can have SES forward emails that are sent to foo@mydomain.awsapps.com even though there is no foo user.

Am I missing something? It seems like I can skip the $4/month user fee and rely on SES rules for inboxes that don't exist (foo in my example).


Edit: The answer is no, you must have at least one enabled user. And even then, AWS recommends using a dedicated domain.

From the documentation:

""You can start using your Amazon WorkMail organization with the provided test domain created during setup. The test domain format is example.awsapps.com. You can use the test mail domain as long as you maintain enabled users in your Amazon WorkMail organization. However, the test domain cannot be used outside of Amazon WorkMail. Also, the test domain might become available for registration and use by other customers if your Amazon WorkMail organization does not maintain at least one enabled user.""

https://docs.aws.amazon.com/workmail/latest/adminguide/howto-start.html",1,aws,2020-10-13
j6vfuy,SQS Queues - More Traffic in Less Queues or Less in More?,"Currently exploring architecture options in our AWS platform. Apologies for being vague but the basic principle is this:

We pass events through a pipeline of Lambda functions, where multiple endpoints can receive the final data load asynchronously. We are utilizing SQS FIFO queues in order to dedupe and sequence the events.

Currently, for each endpoint, a separate SQS queue is created and maintained. However, multiple endpoints may be calling different (or the same) APIs inside a single consuming application. So I was curious about building our SQS queues based on the consuming application, rather than each individual event. We have an abstraction layer over the runtime to control individual events already, so user experience would see no change in functionality.

The basic question is, is it better practice to utilize a large number of queues with a smaller throughput, or limit the amount of running SQS queues and increasing overall throughput? The queues all trigger the same event source mapping to call a lambda to dequeue the records on arrival.",1,aws,2020-10-13
j6vfie,AWS ES missing features?,"In Kibana it allows you to have multiple projects as seen [in this link](https://www.elastic.co/guide/en/kibana/current/xpack-spaces.html). After spinning up my own ES cluster I do not see that feature enabled. I even tried in the dev tool to run this API to create a new space, which did not work either. Any tips on how to do this?  


    POST api/spaces/space
    {""id"": ""test"",""name"": ""test"",""description"" : ""This is the test for multiple spaces"",""color"": ""#aabbcc"",""initials"": ""Test""}",1,aws,2020-10-13
j6ux85,How to protect from DDOS costs when using Cloudfront &amp; S3 for personal website?,"I have a personal website that I want to protect from costs if I get ddosed. I don't even really care that it survives a ddos, as much as I don't want to wake up to a huge bill if I find out that this happened.

  
I know that Cloudfront has AWS shield standard by default and that shield advanced will give cost protection but for a resume website that seems unnecessary. Is there anything else I can do that's fairly low cost which will protect me from this?",1,aws,2020-10-13
j6uhbq,Can passwords stored in AWS configuration profiles be secured ?,"We need to automate copying local files to an S3 bucket from a folder on an AD domain-joined Windows server on our internal corporate network (NOT an EC2 instance).

Is there any way to accomplish this using configuration profiles or any other method that doesn't require saving clear text AWS credentials  (secret access key etc.) in a file on the server?",1,aws,2020-10-13
j6ty4x,Unit/Integ test for lambda in Typescript,"Our team has been using and lambda/nodejs for some time. Here is how we do unit/integration test on the lambdas.

Later I will also showcase how to do integration test on DDB.

[https://link.medium.com/QfFpgy7ukab](https://link.medium.com/QfFpgy7ukab)

Thanks.",3,aws,2020-10-13
j6tvj9,Constrain Cloudformation Parameters with ImportValue,"If I have two private and two public subnets but I only want to be able to pick the two public ones, can I constrain the Allowed Values from an import? These do not work, fails validation because CF wants a string:

    AllowedValues:
      - !Select [ 0, !ImportValue PublicSubnets ]
      - !Select [ 1, !ImportValue PublicSubnets ]
    
    or
    
    AllowedValues:
      - !ImportValue PublicSubnet1
      - !ImportValue PublicSubnet2
    ",1,aws,2020-10-13
j6rjfl,ECS: Fargate or EC2?,"We are trying to move away from cloudwatch rules to provide scheduling because it lacks concurrency control at the schedule level (i.e. don't trigger the next execution if the previous execution is still running).

To do this we are running an always on scheduler that checks a schedules table and creates and manages what we call ""jobs"". Some jobs may run in batch, others may run in lambda. As part of management, the scheduler checks the running status of batch jobs and updates the ""jobs"" table. For lambdas, the scheduler waits for the invocation to complete with a RequestResponse invocation before marking the job as completed or failed. It must use a RequestResponse here, because an Event invocation can't be later queried to determine the result of the execution. We originally went with having lambdas manage their own job status, but with dozens of lambdas that got out of hand.

I thought a good place to run this scheduler would be Fargate, but I just learned there is a maximum of 2m configurable shutdown timeout when a sigterm is sent. This won't work for us because lambdas may run up to 15 minutes. Stopping the scheduler before lambdas have returned will result in orphaned jobs.

So Fargate doesn't seem like the right solution here. But I really don't want to have to manage EC2s. It looks like ECS can manage EC2s and I'm confused by that. What's the difference between an ECS managed EC2 and an ECS managed Fargate? They're both ECS managed, they both have a long provisioning time, why wouldn't I always choose EC2?",2,aws,2020-10-13
j6tlnf,The Complete AWS Lambda Handbook for Beginners (Part 1),,139,aws,2020-10-13
j6qpb0,IAM policy inconsistent implementation,"I always thought all IAM policies (identity based, scp, resource, ..) would be evaluated by the same ""engine"", but the more you work with IAM this does not seem to be true.

Two recent examples I've seen is that not all policy elements are available in [SCP policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html), I was trying to use NotResource, but it is not supported. 

When trying to put a CloudWatch Destination Policy, you cannot use PrincipalOrgID in the conditional section, which is really annoying when trying to set up a centralized logging function.

Has anybody ran into some of these inconsistencies or does anybody know why they exist in the first place?",1,aws,2020-10-13
j6rp8q,Does any one know if it's possible to customize retry strategy for S3 based on the operation?,I'm using the C++ sdk for s3 and would like different strategy based on if its a write or a read. Does any one know if it's possible to do this with the sdk? Or would I have to do my own implementation of retries?,1,aws,2020-10-13
j6roiq,New IAMCTL tool compares multiple IAM roles and policies,,18,aws,2020-10-13
j6rgpi,Port Forwarding on an EC2 Instance,"Hello I have an EC2 Instance that I primarily use for gaming, and I wanted to switch my remote desktop app from Parsec to Moonlight, but moonlight requires me to forward some ports which I have very little idea of what to do",1,aws,2020-10-13
j6rdhw,Cloudformation - AutoScaling Group,"Hello,

I'm fixing some drift on one of my CF templates, and one part in particular has me stumped. The Autoscaling group.

     ""ECSAutoScalingGroup"":{
          ""Type"":""AWS::AutoScaling::AutoScalingGroup"",
          ""Properties"":{
            ""VPCZoneIdentifier"":{
              ""Ref"":""InstanceSubnets""
            },

I need to reference more than just ""InstanceSubnets""  how do I do it? I can add the new subnet in as a parameter like ""InstanceSubnets"" but  I can't seem to get the correct JSON syntax for ""ref""

Thanks",1,aws,2020-10-13
j6qso5,Cloudwatch Event Pattern Event Rule Limit,"We are trying to setup our cloudwatch to monitor 250+ servers. However, there are ec2 instances that are only started on-demand basis. This doesn't allow us to use ""Any resource"" as the it will include the instances that are intentionally stopped. Is there a way to exclude those stopped instances? Or is our only choice is to create multiple pattern event rules?",1,aws,2020-10-13
j6pf36,What's the proper way to troubleshoot site-to-site VPN (GRE stage)?,"Hello,

I'm configuring subj from Customer Gateway (CG) to Virtual Private Gateway (VPG).

Ipsec tunnel as it is starts normal, on VPG side both redundant tunnels are in state ""IPSEC IS UP"", from CG side (strongswan) - ipsec status also in state established.

But when i try to ping inner GRE address from CG to VPG - no luck, no reply packets. Tcpdump shows no activity from VPG side.  
No specific firewall rules on CG side. This machine keeps other ipsec tunnels with no problem, and new endpoints added as new items in ipset lists.  
What options do i have to troubleshoot the problem?  
Because aws troubleshooting guide has only one advise for this kind of situation ""review your tunnel interface configuration to make sure that the proper IP address is configured"" :)  
[https://docs.aws.amazon.com/vpn/latest/s2svpn/Generic\_Troubleshooting.html](https://docs.aws.amazon.com/vpn/latest/s2svpn/Generic_Troubleshooting.html)",2,aws,2020-10-13
j6q06v,Revisiting CDK project,"It's been 6 months since I last looked at my first CDK project that is now running in production. However, I have a slight problem.

We want to expand with a development setup that is identical to production. I made everything into reusable components back in March (with DTAP tags and everything). I haven't made any changes to the code, but when I run

    cdk diff

it shows that it wants to add a bunch of resources. I'm a bit scared it will completely fuck up my production environment or that it will create a new production environment. Does anyone have any tips or guidelines that could help me? Or is it better if I clone this repository and start fresh to make sure my Dev deployment won't accidentally mess with production?",1,aws,2020-10-13
j6mmxn,S3 or DynamoDB,"Hey guys! 

I’ve just finished my first Amplify project. It’s a static Gatsby website that was a ton of fun to make. Amplify is awesome!

So now that I’m comfortable with the JS SDK, I thought I’d try CPP out. I’ve currently got a camera system using a few raspberry pi’s. I’m currently trying to figure out the cheapest way to upload the recorded images.

Is S3 even an option? I plan on recording 100’s of thousands of pictures a day. I can store the RGB data as digits to upload to DynamoDB, but I don’t even know if that would be an option. 

What do you guys think?",1,aws,2020-10-13
j6n51s,GuardDuty Cloudwatch events deployment using SAM,"Hi 

&amp;#x200B;

I would like to deploy Guard Duty + Notifications using Cloudwatch events triggers.

&amp;#x200B;

I am new to the SAM template, and wonder if someone could help me understand what sections i need to configure in the template.

&amp;#x200B;

Thanks in advance!",1,aws,2020-10-13
j6orzo,Feature request: target S3 static site with an ALB,"Hi everyone,

I want to preface the following by noting that this is not a support request.

I'm finding myself in the situation where I have an ALB with a path prefix that proxies to some Docker containers in Fargate. However the rest of the site is static so I would like to just make a static S3 site, offload TLS at the ALB, proxy what doesn't go to Fargate to the S3 site, and Bob would be your proverbial uncle.

I am surprised to find that that is not possible out of the box. I can set up an EC2 instance, or something like Fargate, or redirect to another site, but if I want to have:

- a single domain - one (1) domain,
- with an S3 site
- and an API endpoint of some kind in ECS
- and cheap or free TLS
- and I want to do all of that serverlessly,

...then I need to spin up CloudFront in addition to the ALB I need for the ECS stuff. For various reasons I prefer an ALB for this over CloudFront.

I know folks who work for AWS read this, so I'd like to submit the feature request that directly adding a static S3 bucket as a target to an ALB is something I for one would very much appreciate - as a matter of fact I actually totally expected it to be a thing, because it honestly seems like a super obvious feature to me. Of course, that may just be me because apparently it's not obvious to AWS. :)",3,aws,2020-10-13
j6mgre,Cognito SignUp and GDPR consent?,"How do you guys handle GDPR with Cognito?  
The consent has to be explicit, so ""by continuing you agree"" cannot be the case here.  
It also includes issues with social signup.  
As far as I understand, I cannot even create ""locked"" account (until they accept the T&amp;C) because I would already have user's email, so technically I have their data without their consent...   


I am trying to develop free community product, so I cannot afford an lawyer consultation for that :/",1,aws,2020-10-13
j6mgfc,[Need help] AWS SAM templates,"Hi everyone.

I am trying to use AWS::Serverless::Function and i want to create a function/lambda

in order to export findings to a S3 bucket. (long term saving)

※Optimally i would also want to have the ability to send a email notification of new AWS GuardDuty findings using SNS.

I am a bit lost to what parts of the AWS::Serverless::Function template i need to use to create this.

Basicly this is what i want to acchive, and i am unsure what parts i need in the template.

&amp;#x200B;

GuardDuty (notifie via email and save Findings in a S3 bucket) 

\--&gt; Notification using cloudwatch events

\--&gt; SNS topic for Email notifications.

\--&gt; attach required IAM roles.

\--&gt; trigger events to be exported to a S3 bucket

&amp;#x200B;

Any help would be greatly appricated",1,aws,2020-10-13
j6nyxe,SQS - fifo queues and retention behaviour,"Hi all,

We are looking at utilising SQS for our pipeline. 

I was hoping somebody would be able to answer a quick hypothetical question.

If we wanted to guarantee the order in which messages are processed using fifo queue type how does message retention period play a part in this process?

For example, we have a few messages come into the queue and we have the first message fail to be handled by the application properly and the message is not removed.

Will this result in the other messages waiting till the failed message reached the end of its retention time?

I have had a look online and the aws document and couldn’t find anything referencing this sort of behaviour. 

I could setup some tests to check this behaviour but I thought maybe someone who has had dealing with this before maybe able to advise to save me sometime.",1,aws,2020-10-13
j6m6w0,AWS COST INFLATED ?,"Can you guys please assist me in understanding AWS costs for RDS :

db.m4.large is :  $0.342 / Hour ([AWS billing here](https://aws.amazon.com/rds/mysql/pricing/)), but final total for db.m4.large is $717.12 for EU-InstanceUsage:db.m4.large (Hrs) 720 hrs ( $0,996 / hour)  


Please find image here showing reports &amp; filters : [https://ibb.co/wNx7zsC](https://ibb.co/wNx7zsC)

Is this normal ?",0,aws,2020-10-13
j6k8z9,Where to find templates of IaC ?,"So I have an infrastructure I would like to get more people to use, something really basic, and I've never really messed with IaC.

While I feel like I'd benefit from writing my own code, there must be some place where a whole bunch of quality IaC templates are available for to me inspire myself with.

***Is there a resource for Open-Source IaC templates?***

What I like about my setup is that it is entirely free for 1 year. And I'd like to have it as IaC because it would permit more people to use it to spin up and break down the server in no time, and at no cost. (I'm not sure if it's possible to also provide a boolean which would automatically break down the server after a year, once the Free Tier expires.)

The other advantage is that I have a CI/CD (GitHub Actions) workflow already written which deploys to the server automatically, too.

### My setup:
* ECS (for EC2, not Fargate)
  * Cluster
  * Service
  * Task Definition
  * IAM Role (created automatically by ECS)
* EC2 (On-Demand, t2.micro, AMI: ECS-Optimized Amazon Linux 2) : mostly managed by the ECS Cluster
  * Auto-Scaling Group (desired 1, min 0, max 1)
  * EBS Volume (Prevent deletion, 30GB, gp2)
  * Key Pair
  * Security Group
  * Launch Configuration
  * Elastic IP (on re-create if your instance dies, configured to be associated automatically!)
* Cost Explorer -&gt; Budget Alarm

### ECS configuration
Initial set up was done through following [this tutorial](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started-ecs-ec2.html).
 
 Then, to customize the set up to work with GitHub Actions, the following changes were made:
 
   1. Changed the ``containerPort`` of `PortMapping` from ``80`` to ``80``.
   2. Removed the ``commands`` that was setting up a default HTML page to land on.
   3. Updated the ``entryPoint`` to match a jar execution: `java, -jar, /app.jar`.
   4. Edit the ``build.gradle`` file so that jars do not include version numbers and include Shadow plugin.
   5. Add a ``Dockerfile`` which extends our JDK (alpine version) and copies the generated jar.
   6. Added a ``workingDirectory`` value of `/`.
   7. Changed the ``image`` to point toward our Docker Hub repository.
   8. Enforced the ``requiresCompatibilities`` to be `EC2`.
   9. The minimum healthy percentage modified from ``100%`` to `0%`, and maximum from `200%` to `100%`: this is to ensure that we can kill a running task and then start a new one, and never have more than 1 task in the Container. See [this](https://stackoverflow.com/a/40741816/9768291) SO answer to understand better.

---

***EDIT***: For future reference, I recently found [this GitHub repo](https://github.com/awslabs/aws-cloudformation-templates) and [this other one](https://github.com/widdix/aws-cf-templates) which contain a few official and community-submitted templates. It doesn't have what I'm looking for, but maybe it'll help you. :)",1,aws,2020-10-13
j6m06m,Weird issue... Can't send AWS4 signed request... but only in tests?,"This is a bit of an odd one and I'm stumped. I'm making identical Axios calls in a test and in a script I wrote to hit an API gateway endpoint protected with IAM authentication. The script works fine, but when the test tries to do the same call it fails with a network error. There appears to be a ""preflight 403"" failure that's logged out at some point although the actual failure from axios just says ""Network Error"". 

I even replaced the values in the axios call within the test with the hard coded values in the script, hoping that it would at least get past the API call and fail the test case but it still threw the networking error.

I saw something online about maybe having to enable CORS? But why would this *only* happen during my tests?? Does anyone have any ideas?",1,aws,2020-10-13
j6k8te,What are CloudFront CDN redirects? can't find answers on google.,"&amp;#x200B;

https://preview.redd.it/3i30wdryklr51.png?width=532&amp;format=png&amp;auto=webp&amp;s=3a494255980294f9c473863faf3a9843ef382b59

Source: [https://piyushkashyap.net/images/architecture.png](https://piyushkashyap.net/images/architecture.png)",1,aws,2020-10-13
j6iih9,Creating Visualization in Amazon CloudWatch,Hey Guys! I'm trying to create a visualization in Amazon CloudWatch and want to plot multiple lines. My scenario is that I'm parsing the logs and creating 3 fields. I want to group by one of the field and plot the count for each group over a period of time(1h). Is it possible to plot multiple lines? If yes then how to do that?,1,aws,2020-10-13
j6imbx,"Tried to build my personal blog using AWS + static websites, is it good enough?","&amp;#x200B;

https://preview.redd.it/ql97smph0lr51.png?width=1362&amp;format=png&amp;auto=webp&amp;s=e2f410ba67f51a2fa775a419462a655e56632b82",1,aws,2020-10-13
j6hhb4,How can I guarantee performance globally?,"I've been given the task of making sure our Website/Mobile application performs well globally.  All our backend infrastructure runs in AWS (with Cloudflare sitting infront of everything). We are using Lambdas and API Gateway to run our api and backend. Our frontend application run in ECS. And the bulk of our content sits in S3 (\~5TB), with \~500gb in databases

There seems to be a suggestion from colleagues of duplicating infrastructure (e.g. RDS, DyanmoDB, Compute platforms) and running it in different global regions, so that requests can be processed closer to the user/client . And then try and sync data between the different geographical regions to keep things up to date. But to me this sounds like a headache, not to mention expensive. 

The easier option (and my more preferred option) would be to serve media content (e.g. images, large files) via a CDN, which can serve our content from different nodes around the world. The data processing would still be happening in the primary region (e.g. us-east-1) but large files would be served from a CDN located close to the clients location.

Would be great to get some recommendations.

Thanks",8,aws,2020-10-13
j6gqgb,Need help with creating a CloudWatch Insight Visualization,"Hey Guys! I'm trying to create a CloudWatch Insight Visualization and add it to a dashboard. What I want is to plot multiple lines in the visualization. I'm creating 3 new columns by parsing a log message. The columns are ApiName, ApiType, URL. Now I want to group by api type and plot a count of them and see how many times they are called every hour. Is it possible to do that? Any examples or help would be great.",1,aws,2020-10-13
j6f58g,What was your AHA moment using AWS?,Either the moment when you understood its full potential or when you knew that AWS was the right set of tools for your path?,13,aws,2020-10-13
j6gehe,Savari leverages AWS Wavelength to make roads safer and more predictable | Amazon Web Services,,6,aws,2020-10-13
j6gw7s,SSM Parameters in CFN templates,"So AWS recentlyish (?) released a feature that allows you to use SSM parameters directly in your CFN templates using the following format:

`'{{resolve:ssm:parameter-name:version}}'`

We have been doing this for years using the older parameter reference method and were keen to move to this less verbose method until I saw [this](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html):

&gt; You cannot currently specify that AWS CloudFormation use the latest version of a parameter. For more information, see [Working with parameter versions](https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-versions.html) in the *AWS Systems Manager User Guide* 

What good is that to anyone? I need to track what version each parameter is up to now and keep that in sync with the templates I deploy? May as well just enter the value manually...

WTF!",3,aws,2020-10-13
j6ga4c,Introducing Distributed Load Testing Solution from AWS,,94,aws,2020-10-13
j6fwy6,Identify when to upgrade processing size,"Currently working on a small project and will be getting it ready to release to production. During my development, I’ve been using a db.t2 micro. How do I know how large to make it when I push to production? Do I watch utilization and scale it when it hits 20% cpu utilization?",1,aws,2020-10-13
j6dsbt,"Reference Architecture Examples, Diagrams, and Best Practices from AWS",,3,aws,2020-10-13
j6dy1s,DynamoDB Overloading,"We are designing our DynamoDB table and considering using a single table with standard columns, even though it may involve more GSIs, against using an GSI Overloading pattern.

Using overloading, where we reuse GSIs for multiple column types, it makes the data access patterns much more complicated. At least that's what I can see from researching it. However, it does reduce costs because we only need one GSI.

I just wondered if anybody has used an overloading dynamo pattern and is it still relevant?  Was it only suitable when ""Pay Per Request"" wasn't an option?",2,aws,2020-10-13
j6di1h,Emacs/TRAMP to AL boxes?,"I recently setup an AL1 box for work. I have existing CentOS boxes which I've used for ~2 years, doing most of my development with emacs via TRAMP (i.e. `x-f /ssh:hostname`) with no issues.

New AL1 box won't accept TRAMP connections from emacs from my laptop. I get no error. I get no messages, no timeout, nothing. It just hangs forever. I followed the same usual setup, setup ssh keys and aliases, I can ssh to the box fine with simply `ssh &lt;alias&gt;` but from inside the emacs GUI I get nothing.

I googled around, found nothing. Someone suggested it was a ip routing/firewall issue. But i can't find anybody any posts or docs related to the issue. If anybody has had this issue before, any direction would be appreciated.",1,aws,2020-10-13
j6c7r0,Implementing NFS in AWS with ACL support?,"I'm working on an application 'server' consisting of an autoscaling group and wanted to use a remote storage solution for user data. Initially I tried using EFS, but the application running on these autoscaling servers requires ACLs, which EFS does not support. I don't think EFS access points will work either since there are dozens of users who all would theoretically have their own permissions. 

Have any of you run into this challenge and found a way of implementing NFS with proper ACL support? As of right now, I'm considering just running my own NFS server on EC2 instances, but it will be a challenge to maintain high availability and multi-AZ deployment, so I'm desperate for an alternative. Any advice is appreciated!",1,aws,2020-10-13
j6cuqj,How do you Implement Encryption with Direct Connect.,Direct connect does not support encryption. How have you incorporated encryption into your connection for a transit VIF? It is a dedicated connection but compliance still wants encryption.,2,aws,2020-10-13
j6d3v7,IIS autoscaling - spin up time,"Hello, 

Is anyone else running IIS for webserver autoscaling? Typically, it takes 20 minutes from an instances being launched, to being healthy (being created from image, running userdata, being syspreped, region settings applied, joined to the domain, and the website data uploaded from S3 (webdeploy used too), then IIS starting and the instance passing healthchecks). 

That still seems like a long time to me. 

Out of interest, has anyone managed to get this significantly better, and any tips?",2,aws,2020-10-13
j6cmb0,AWS IAM role impersonation to HashiCorp Vault vulnerability,,74,aws,2020-10-13
j6cfjw,[MSK] For anyone else needing a rolling reboot script for their clusters,,4,aws,2020-10-13
j6ar4w,How to approach resource-based together with context-based authorization with AWS Lambda?," I have recently got the AWS Activate credits (didn't expect that), so I've started rethinking if I may use AWS Lambda together with DynamoDB, SQS, SNS to build my project instead of full-blown app using NestJS.

However, in terms of the authorization, I have pretty thought through solution for Nest, which I cannot get so far for AWS stack.

The application will be mostly focused around ""groups"". Groups can be managed by some owner, but can be also managed (through ""change requests"") by the community.  
Most endpoints will be built like that: \`[api.example.com/groups/:groupId/information\`](https://api.example.com/groups/:groupId/information%60) etc.  
User can be member (as manager) user of multiple groups and in each group can be either admin who has access to all modules (like information) or have access only to some of the modules.  
""Normal user"" can access most of the GETs, but on POSTs they will have paths like \`[api.example.com/groups/:groupId/social/foo\`](https://api.example.com/groups/:groupId/social/foo%60) where they can send some changes asked by the community, which can be approved either by group admins or system admins.  
I would not like to do the check if user: foo can access module: bar in group: xyz in each and every function as it will be nothing but painful and code repetition in each and every lambda and also perf penalty can hit hard.

Would any of you recommend any good approach for that in the serverless world, preferably on AWS?  
Maybe some of you had similar requirements?

I will appreciate your help and suggestions!",1,aws,2020-10-13
j6bkbo,Comparing AWS and Azure Partner Resale,Does AWS require you to use a distributor for resale of EC2 etc like Microsoft CSP?  Do they require the partner to bill the client and then be billed by Amazon or does Amazon handle the billing on behalf of the partner?,1,aws,2020-10-13
j69scr,Help required in understanding the s3 bucket policies,"Hello there,

Could someone please clarify if there is a difference between the two policies mentioned below?

1 -

            {
                ""Action"": [
                    ""s3:ListBucket""
                ],
                ""Effect"": ""Allow"",
                ""Resource"": ""arn:aws:s3:::my_bucket_name*""
            },

2 - 

    
        {
                ""Action"": [
                    ""s3:ListBucket""
                ],
                ""Effect"": ""Allow"",
                ""Resource"": ""arn:aws:s3:::my_bucket_name*/*""
            },

Is it like, first one just let you see if the bucket exists but not its contents while the second one also allows you to see the objects in the bucket?

My main objective is to allow the user to see the bucket as well as list its objects.

Thanks.",8,aws,2020-10-13
j69eve,Cloudwatch monitor disk space on EBS volume attached to windows instance.,"Hi everybody.

It maybe has been asked a ton of times but I cannot get my head around it, nor stup a proper CLoudwatch alarm for it. I have a windows EC2 instance with a database on it, the   E: drive is a 300gb EBS volume attached to the instance used as TempDB and we had recently transaction issues because this E: was full. 

I need to setup a cloudwatch alarm for the E: drive for a threshold of 10 minutes with &gt;=80% usage. I cannot move the database system to RDS.  I've followed these guides:

https://www.reddit.com/r/aws/comments/b13281/cloudwatch_alarm_for_disk_space_for_windows/
https://www.youtube.com/watch?v=xKVrJJyG-4I&amp;t=55s
https://volkanpaksoy.com/archive/2018/12/19/Monitoring-EC2-Instance-Disk-Space-with-AWS-CloudWatch/

I have setup perfect monitoring for C: but I cannot setup E:  Almost all documentation that I stumble upon is for Linux instances with mountpoints and perl scripts. Hope you guys could give some guidance.",2,aws,2020-10-13
j69ki8,How are dependencies shared across Lambdas with Layers?,"If I am deploying many lambdas using serverless, and I:

*  add a dependency in a layer (ex. chromium)
* use the layer across many lambdas

Is there a single chromium binary being shared behind the scenes or is there a copy of chromium with every lambda?",3,aws,2020-10-13
j698gt,CAC Cards through AWS govcloud,"I currently have a few organizations that I manage where we are able to use CAC cards through zero clients by using a Teradici server rather than amazon workspaces direct connect on the zero client. 

Now this new organization is set up using AWS Govcloud and the big issue is that we cannot use Teradici servers anymore through Govcloud. Only the direct connection. 

So my question now is: Is there a way to redirect a CAC card plugged into my physical zero client to my AWS workspace without using a Teradici server?

I have a built in card reader on this zero client as well as a USB card reader that I have went into the device and added the PID and VID for in the bridged device section to no avail.",1,aws,2020-10-13
j694as,OAuth for AWS Console Access,"I have a customer that wants to use OAuth (rather than SAML) for users to access the AWS Console.  My understanding is that this possible but will take some custom code.  [https://docs.aws.amazon.com/IAM/latest/UserGuide/id\_roles\_providers\_enable-console-custom-url.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html)

&amp;#x200B;

My understanding is once a user is authenticated, temporary credentials are generated from an assumed role and then those credentials are used to create temporary ""pre-signed"" url that a user can browse to access the console with the assumed role.  

At this point, I am just trying to plan out the CFN templates to create the pre-defined roles that users will end up using to log-in.  What goes in the AssumeRolePolicyDocument aka the trust relationship?  

The identity broker that will be generating the URLs will be hosted on EC2. I'm assuming that it will need an instance profile of its own to interact with STS.  Would the roles have a trust relationship with the instance profile of the identity broker? Or am I missing something?",3,aws,2020-10-13
j68q73,How to create a custom metric for SQS?,"Hello there,

I have created an SQS queue and an ASG where based on 'ApproximateNumberOfMessagesVisible' metric instances are launched or terminated. This is how my current policy looks:

    ASG-scale-out-policy
    
    Policy type:
    Step scaling
    Enabled or disabled?
    Enabled
    Execute policy when:
    ASG-scale-out-alarm
    breaches the alarm threshold: ApproximateNumberOfMessagesVisible &gt;= 1 for 2 consecutive periods of 60 seconds for the metric dimensions:
    QueueName = my_sqs_queue

I want to use the **sum** of 'ApproximateNumberOfMessagesVisible' &amp; 'ApproximateNumberOfMessagesNotVisible' as a condition for scaling. How can I do it?  
Can I create a custom metric for this? Or is there any other method?

Thanks",3,aws,2020-10-13
j68cj4,Step functions for ASP.NET Core?,"I am using [asp.net](https://asp.net) core 3.1. 

I have made step functions with just using .net core 3.1 without the [asp.net](https://asp.net) framework, but I do not get the benefits of using EFcore and Automapper among other things the framework offers. 

So with this in mind I have came across a use case for step-functions, and to implement the logic in the [asp.net](https://asp.net) core application seems to be verbose. Step functions seem like the best option.

The issue I am facing is it seems there is only one entry point for the lambda requests when using `apigateway` this seems to be handled by the `serverless.template SAM`, and this one lambda function just passes the input to be handled by [asp.net](https://asp.net) routing internally to the controllers.

Now I dig through some AWS Docs coming across this blog post TITLE: (ASP.NET Core path base)

[https://aws.amazon.com/blogs/developer/updates-for-serverless-asp-net-core/](https://aws.amazon.com/blogs/developer/updates-for-serverless-asp-net-core/)

This post TITLE (ASP.NET Core path base) shows a way to declare different memory values per controller in the `SAM template`, so trying this as it seems to make another `apigateway` route that links to only that controller, which will possibly allow me to utilize this to set it in `apigateway` to use `services step functions.` Well trying this causes a error of

**Resource with id \[Customer\] is invalid. property roll not defined for resource of type AWS::Serverless::Function**

So I followed exactly what was shown in that blog, and was returned with an error on deployment. 

&amp;#x200B;

* **Does anyone have any idea what that error relates too? or why that error is caused?** 
* **As well anyone have any information or tips on how to make step functions with** [**asp.net**](https://asp.net) **core? ( or is the only way currently is to make them outside the** [**asp.net**](https://asp.net) **framework**?)",3,aws,2020-10-13
j67bhu,Multipart uploads with s3 pre-signed URLs,"Strikes exactly in the spot which aws doesn't have any documentation about and there are so many developers who try to guess how to use it!  
[https://www.altostra.com/blog/multipart-uploads-with-s3-presigned-url](https://www.altostra.com/blog/multipart-uploads-with-s3-presigned-url)",0,aws,2020-10-13
j67sds,Providing feedback on AWS Containers,"I was wondering if there was a good location to provide feedback for Amazons SAM Docker images? 

One thing that I've noticed is that images such as the `amazon/aws-sam-cli-build-image-python3.8` for building the lambdas is over 2Gb, which is a bit rubbish for CI/CD pipelines but especially so for local SAM on home internet, and after a bit of analysis there looks like there is potential for shrinking that down.

I didn't see anything on Github, is there a better method of communicating feedback for these images than via the Support Portal?",2,aws,2020-10-13
j688u5,Does a spot fleet work better than a single instance type?,"Hi,

I've got an ASG in 2 zones. With a single instance. It should only ever run one and should start a new one if that one fails.

Today, the spot capacity disappeared in eu-west-2b and the instance stopped. Fine, I expect that, which is why I have an ASG. I'd expect it to take a minute or 2 to notice it die and another couple of minutes to launch a new instance.

However, today some time passes without a replacement instance. Long enough for users to start asking questions.

So, I investigate : After the termination, the ASG tried to start a new instance in eu-west-2b. Tried 5 times, every 2-3 minutes. Then started again, still in eu-west-2b. Another 5 times. Eventually about 20 minutes later (about 2 minutes after I get logged in and see what is going on and am still head scratching) it tries eu-west-2a. And surprise, there is capacity. Instance launches.

\_Thanks\_ for the delay.

IF instead of a single spot type, I use a spot fleet with multiple instance types, will it sensibly realise straight away that a certain zone/instance combo isn't going to work. Or will it sit there mindlessly trying to launch m5.large instances in eu-west-2b when there is no capacity. And keep doing that for 20 minutes before deciding to try m4.large in eu-west-2b and if there is none of that, another 20 minutes, before trying c4.large etc..... ??

(Will what should make it better actually maybe make it worse?)

Is autoscaling just not going to work right for a single instance deployment and spot? I'd be better off not using ASG and building my own autoscaling rules, with counting 503's on an ALB and if I see any, terminate and launch new. (I might not even need a lambda for that, not 100% sure if metric based autoscaling is that clever.)

tl;dr : Compared to a single instance type launch config, will using a spot fleet with multiple spot types decrease or increase my recovery time from capacity exhaustion when I want only a single instance to run?",4,aws,2020-10-13
j67bfd,How y'all mocking Redshift,Or do you just spin up the cheapest redshift cluster and use that?,0,aws,2020-10-13
j65w1g,How to sync data to AWS DynamoDB using Amplify DataStore?,"I've setup a React Amplify project.I successfully have Auth working using Cognito User Pools but can't seem to figure out DataStore.

I have the models generated and can locally save and read posts but I'm wondering how it actually syncs to the cloud?

I tried following [this guide](https://docs.amplify.aws/lib/datastore/sync/q/platform/js) but with no luck.",9,aws,2020-10-13
j65ip0,Issue trying to use DeepAR in SageMaker: ModelError when calling the InvokeEndpoint operation,"Hey,

I wonder if someone here went through this issue. I am getting the same error using my own data, or the data from the official examples from AWS SageMaker (with synthetic data and the electricity dataset).

I successfully train but on inference I get:

`---------------- ModelError Traceback (most recent call last) &lt;ipython-input-114-4820dcc52bfb&gt; in &lt;module&gt; ----&gt; 1 list_of_df = predictor.predict(df_list_train[0]) &lt;ipython-input-110-2874930fdfe5&gt; in predict(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles) 22 print(""req:"") 23 print(req) ---&gt; 24 res = super(DeepARPredictor, self).predict(req) 25 print(""res:"") 26 print(res) ~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant) 111 112 request_args = self._create_request_args(data, initial_args, target_model, target_variant) --&gt; 113 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args) 114 return self._handle_response(response) 115 ~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs) 335 ""%s() only accepts keyword arguments."" % py_operation_name) 336 # The ""self"" in this scope is referring to the BaseClient. --&gt; 337 return self._make_api_call(operation_name, kwargs) 338 339 _api_call.__name__ = str(py_operation_name) ~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params) 654 error_code = parsed_response.get(""Error"", {}).get(""Code"") 655 error_class = self.exceptions.from_code(error_code) --&gt; 656 raise error_class(parsed_response, operation_name) 657 else: 658 return parsed_response`

`ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message ""Unable to evaluate payload provided"". See` [`https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/deepartest-2020-10-05-12-23-39-128`](https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/deepartest-2020-10-05-12-23-39-128) `in account 462474520888 for more information.`

Any hint on how to solve this would be very appreciated.  


EDIT: I tried also the official examples from the notebooks below and got the same error:  
[https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction\_to\_amazon\_algorithms/deepar\_synthetic/deepar\_synthetic.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb)

[https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction\_to\_amazon\_algorithms/deepar\_electricity/DeepAR-Electricity.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_electricity/DeepAR-Electricity.ipynb)",0,aws,2020-10-13
j65mgp,Is there any way to create a CloudWatch event that triggers whenever an action has a specific errorCode?,"I'm basically trying to create an event that triggers whenever an 'AccessDenied' errorCode occurs.  

I could probably just create a rule that takes into account ALL actions and then parse through those events for an error code, but I was looking if there any other possibilities that would be less resource/data intensive.

&amp;#x200B;

Any ideas?  Thanks",1,aws,2020-10-13
j5nh4w,I've deleted my account but Amazon keeps billing me for Route 53 charges.,"I deleted my AWS account because I wasn't using it and apparently I forgot to terminate the Route 53 service.

Now Amazon won't let me access the account proper to finish it and their ""customer support"" form is utter garbage so, what should I do? Is there a direct e-mail address? Am I just screwed?

Update that should have been qrittrn yrsterday but forgot to: I put a case in for the recovery of the account and I'm now waiting a response/update on it

Update 2: They've gotten back to me, they're reopenning the account to let me fix it. Thanks for the help, everyone! Once it's done I'll officially mark this as closed.",62,aws,2020-10-13
j64g2i,Most efficient way to connect to RDS without an own instance,"Hi everybody!

We are starting to use AWS in our company, but we are still newbies, so I hope I don't bother if any question is too obvious, I have tried to document as much as possible as with the rest of the things we have already done.

We have a Datalake in S3 made up of 3 buckets (in a dev account):

* In the first bucket, we have the raw data, which we filter and normalize through Glue, to save it in the second bucket.
* In the second bucket, we have the filtered data that is sent to EMR to perform several actions (data merge, variable creation and transformation)... and later saved in the third bucket.
* In the third bucket, we have the final data that is consulted through Athena and sent to Quicksight.

We need to incorporate data that is stored in an Oracle RDS instance into another AWS account. To incorporate it into our datalake, the data needs to be stored in S3 in the first cube. 

I have seen that is possible to export RDS snapshots to S3 from another account, but this RDS database will be updated frequently. You can automate the daily creation of snapshots in S3, but only in the same account in which the RDS instance is placed. To send them to another account, I think it must be done manually.

Therefore, I thought of creating an RDS instance in our account and connect that instance to the instance in the other account, and automate the generation of snapshots from our account. However, the creation of the RDS instance in our account has a very high cost in relation to the rest of services.

Do you know if this task could be done in an efficient way without having to export the snapshots manually or without having to create an RDS instance in our account?

Thanks a lot for your time!",1,aws,2020-10-13
j63sbr,"How to change a variable in a JavaScript file, depending on the build project used in CodeBuild?","I am building a serverless website using HTML, CSS and JavaScirpt that is hosted on an S3 bucket.

I have a CodeCommit repository, with two branches: develop and master. I do all my development work in develop, and when I'm happy with the changes I merge into master, which is my production environment.

I have two CodeBuild projects, one for the development environment and one for the production environment. I have a build spec file, which tells CodeBuild to sync the CodeCommit files with the S3 bucket that is hosting the website.

There is a variable inside the JavaScript file, that contains the URL for a HTTP endpoint. I need to dynamically change this variable, depending on which environment I am deploying to - development or production. For example, if I deploy to develop, I want this variable to automatically be ""[http://develop](http://develop)"". Alternatively, if I deploy to master, I want it to be ""[http://production](http://production)"".

How can I do this? I know there is something about environment variables, but I am not sure on how to tie this in with everything else.",1,aws,2020-10-13
j63ior,AWS S3 Browser-Based Uploads Question,"Hi Everyone,

I'm relatively new with very little experience in using AWS. I was wondering if it was possible to allow public users to upload files to a bucket I've setup using just a public URL? 

What I am currently doing is extracting the files people upload to a dropbox then manually uploading the file to the s3 and setting up folders within the bucket.

Thanks in advance!",1,aws,2020-10-13
j62rue,How should I deploy my AWS for a simple app?,"Hello,

I am going to make a simple web app + Android app that contains a MySQL (Or MariaDB) database with not too many records. The app does not have registration, but it stores information provided by users. Both the web and the Android interface should send the data from the app to the database.

I'm not expecting too much traffic, so I want to know how and what should I deploy, and make it scalable, if some miracle happens and the traffic grows. And also how can I secure my deployment?

I was thinking about a single EC2 instance and a single RDS. But there are many instances, and my budget is not too big right now, so I was thinking which is the best machine to deploy?

Also, in order to save costs should I install the database on the EC2 itself and get a larger SSD space, or it's will be more expensive and less secured?

Thanks",1,aws,2020-10-13
j60ze6,Hosting thousands of static websites?,"What would be the best way to host a lot (thousands) static websites with different domains using AWS? Ideally with instant updates when content changes and without having to handle scaling ourselves. 

I’ve looked into just using S3 and CloudFront via API, but that would mean thousands of CloudFront distributions, as I understand it, since one distribution only can be associated with one certificate. This approach feels a bit hard to manage in the long run, but maybe it’s the way to go?",1,aws,2020-10-13
j5yk5f,Re-modelling existing dynamoDB,"Hi all, we strategies / AWS services can I use to remodel existing dynamoDB data? 

From what I read on online resources, it's best practice to know first the access patterns before modelling the data.  The problem I want to add new access patterns to an existing table. Need your advise on this. Thanks!",1,aws,2020-10-13
j5yhzy,AWS EventBridge: How to calculate even payload size,"hello everyone, as the title says, I want to know if there's an easy to understand method written somewhere or a calculator that can tell the size of an event payload. I know that each 64 KB chunk of a payload is billed as 1 event  &amp; that's why the sizing question. If I am passing custom events, I'd like to know the size of it.   
Say here is an AWS event, I want to know the size of it. How can I do that?

    {
       ""id"":""7bf73129-1428-4cd3-a780-95db273d1602"",
       ""detail-type"":""EC2 Instance State-change Notification"",
       ""source"":""aws.ec2"",
       ""account"":""123456789012"",
       ""time"":""2015-11-11T21:29:54Z"",
       ""region"":""us-east-1"",
       ""resources"":[
          ""arn:aws:ec2:us-east-1:123456789012:instance/ i-1234567890abcdef0""
       ],
       ""detail"":{
          ""instance-id"":"" i-1234567890abcdef0"",
          ""state"":""stopped""
       }
    }",1,aws,2020-10-13
j5xkqv,AWS development environment/workflows for large teams,"Just curious how different development teams working with aws design their development environment when needing to test changes against real aws resources.

For example, does each developer on your team create their own version of resources to test their changes against (ex: using cloudformation)? Does software like localstack usually work as an alternative?",10,aws,2020-10-13
j5xvqj,Diagram Maker: Open sourcing IoT visualization by Sid Ilangovan and Sameer Goyal,"[__Diagram Maker: Open sourcing IoT visualization__](https://aws.amazon.com/blogs/opensource/diagram-maker-open-sourcing-iot-visualization/)

by Sid Ilangovan and Sameer Goyal

__Diagram Maker__, an _open source graphical user interface library_ for _IoT application developers_. With Diagram Maker, IoT application developers can define their own user interface and user experience, or suppress it completely and build their own custom behavior using Diagram Maker APIs.

__Diagram Maker__ is an _open source client-side library_ that enables IoT application developers to build a _visual editor_ for IoT end customers. With this visual editor, IoT customers can create and modify any graph-like data, such as state machines or workflow definitions, in a visual manner with the help of graphical UI.

__Diagram Maker__ comes loaded with features, such as:

+ Built-in node dragging

+ Drag to create edges

+ Canvas panning &amp; zooming

+ Panel dragging

+ Context-based menu

+  Keyboard shortcuts for selecting all and deleting

+  Modes: dragging mode, selection mode for multi-select, read-only mode for restricting edits

+ Declarative interface for defining any drag and drop interfaces, such as panel headers or node connectors


__Github__

awslabs / diagram-maker",3,aws,2020-10-13
j5wgfi,Lightsail data transfer (egress) - what's the catch?,"If I wanted to download 2TB from S3, I could proxy it through a lightsail instance for $5.

Downloading it directly would cost $180. That's 36x more expensive.

Is this true? What's the catch? Why does this exist?

I was reading https://forums.aws.amazon.com/thread.jspa?threadID=253162 to find out, and it appears that the data transfer is prorated to the month - so if you have a lightsail instance for 15 days (half a month), you only get half of the month's data transfer limit (quite reasonable!). So it isn't even more ridiculous than that, you need the full month's $5.

Even if this is technically possible, is this the sort of thing that will cause AWS to terminate me for abusing their pricing model / services? I'm considering spinning up about two dozen of these $5 instances to download about 25 terabytes from S3.",2,aws,2020-10-13
j5w67k,Switching website on s3 bucket to new domain,I know this isn’t directly related to r/aws but I have my website hosted on an s3 bucket under .xyz domain. I want to switch it to a .org or .com now that it is starting to pick up steam and no longer is a side project. How easy is it to switch all my assets and code to a new domain?,1,aws,2020-10-13
j5uch5,IaC comparison for event-driven architecture,"Hey everyone!  I've been running production workloads on AWS for a long time (started when S3 was announced in 2006) and have been silently lurking on r/aws for over a year.  After many discussions with colleagues debating the future of cloud architectures, I decided to start [my own blog](https://blog.outwiththeold.info/) where I will explore new ways to solve old tasks.

[In my first post](https://blog.outwiththeold.info/posts/event-driven-iac/) I compare two architectures, both built with the AWS CDK, that process a file dropped into an S3 bucket. One architecture uses ECS/Fargate and the other Lambda.

I'd really like to hear what you think about this post and get suggestions for additional topics you would like to see covered.

You can read my first post here:

* [https://blog.outwiththeold.info/posts/event-driven-iac/](https://blog.outwiththeold.info/posts/event-driven-iac/)",53,aws,2020-10-13
j5twnl,Helm style deployments/rollbacks of multiple processes on ECS?,"I’m curious how you achieve helm style deployments/rollbacks to ECS for apps with multiple processes. Helm abstracts away atomically updating multiple processes of an application, ex a python web app may contain:

- Web API
- Celery message queue
- Celery beat cron scheduler


Each of these processes scale independently. In Kubernetes, a helm upgrade/rollback can upgrade or rollback all of these to a new image in parallel. In ECS, we have to:

- Copy the latest task definition for each process to a new one
- Update the service to use said task definition

The only way I’ve been able to replicate this workflow is to write my own tool to do the above parallel deployment/rollback process which adds overhead.

Is there an easier way/something I’m missing?",5,aws,2020-10-13
j5q6fm,AWS Career Advice,"Hoping to pick someone's brain about AWS careers.  Long story short, I have been working in corporate/internal IT for ~15 years working from desktop support to systems engineer.  Looking at open AWS jobs it seems the two most likely paths for someone like me is either Solutions Architect or Support Engineer.  Does anyone have any experience with either of those two positions at AWS?

Sorry, I know kind of a broad question but any help is appreciated.",4,aws,2020-10-13
j5tqxj,AppStream 2.0 shared FSx files with individual users?,"The documentation for connecting AppStream to a multiple user shared FSx share is to make a batch script with the username and password included.

Is there a way to automatically connect to an FSx share using the credentials of the user running apps? (mainly for auditing purposes to see who is accessing what files)  

My initial thinking is directory bound AppStream with SSO, and use a GPO to map the share. Is there a better way?",1,aws,2020-10-13
j5sd8b,How long does AWS automatic certificate renewal take?,"I added the cname record to my DNS provider's records to cover one of my subdomains ([blah.foo.com](https://blah.foo.com)). How long will it be until amazon checks that the entry is there, and autorenews the certificate? I'd like to be able to manually force the process so I can check to make sure I got the cname entyr right, but I don't see anything in the interface to let me do that.

The certificate I'm trying to force renew expires on  2020-11-11T12:00:00UTC",3,aws,2020-10-13
j5npd6,"Host a Laravel 8 App on AWS with CI/CD, SSH, DB, Crons, Email, Logs, Domain, and SSL",,2,aws,2020-10-13
j5rb8t,Link sub domain to S3 bucket,"Good day, i’m trying to deploy static websites using directories in S3 buckets and linking them to a subdomain.

I’m using lambda@edge to get the sub main from the request and match that with the directory inside the bucket, i create the lambda function and run it inside the `Test` from AWS and seems to be working but when i try that using the request the lambda function does not get triggered.

I check the settings in the distribution and the lambda function resource appears but nothing is running.

There is some step that i’m missing creating the lambda@edge function?",1,aws,2020-10-13
j5po5n,S3 usage by folder,"Is there any tool that can calculate size of all items by path in S3. We store data for our customers in some path like this

/data-type/version-ma/version-mi/account-{id}/{Subfolders &amp; files}

&amp;#x200B;

I want to sum the size for each account-{Id}.",1,aws,2020-10-13
j5mqx0,aws cognito invalidate token,"I wasn't able to find information as to whether aws cognito supports a method for logging out a user and invalidating their jwt tokens. Is this possible?

Typically, with JWT it is good to have some blacklist of expired or invalidated tokens, so wondering how aws handles such.

Thanks!",3,aws,2020-10-13
j5luk7,"Could a Windows 10 desktop be ""CloudEndured"" with its pre-installed digital cert?","Trying to save some time, by migrating a Windows 10 desktop, with a lot of customization, in AWS, maybe by using [CloudEndure](https://docs.cloudendure.com/Content/Getting_Started_with_CloudEndure/Supported_Operating_Systems/Supported_Operating_Systems.htm), while this machine also has a digital cert installed for access to a particular resource, which demands this second FA. Would this work?",3,aws,2020-10-13
j5lhhn,Limiting the S3 PUT file size using pre-signed URLs,"I am generating S3 pre signed URLs so that the client(mobile app) can PUT an image directly to S3 instead of going through a service. For my use case the expiry time of the pre signed URL needs to be configured for a longer window (10-20 mins). I want to limit the size of file upload to S3 so that any malicious attacker can not upload large files to the S3 bucket. The client will get the URL from a service which has access to the S3 bucket. I am using AWS Java SDK.

I found that this can be done using POST forms for browser uploads but how can I do this using just signed S3 URL PUTs? I have found conflicting answers for this on different platforms.",18,aws,2020-10-13
j5laql,"I have a backend that is entirely API Gateway -&gt; Lambda, using serverless, and I need to enable video streaming when the client requests an mp4.","How can I do this?
Do I need to redirect to cloudfront? I’ve spend a lot of time investigating kinesis and can’t quite connect the dots.
This needs to be multi-region too. The data files live in s3.
What are my options?",1,aws,2020-10-13
j5ktbt,Questions around IAM limitations with Aurora (MySql),"Using IAM credentials for our Aurora DBs would simplify a lot of management around credentials. However, I have some concerns on some of the limitations IAM seems to bring according to this [AWS link](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.html), under *Limitations for IAM Database Authentication.*  


&gt;The maximum number of connections per second for your DB cluster might be limited depending on its DB instance class and your workload.

Then under *Aurora MySQL Recommendations for IAM Database Authentication*, it specifies:

&gt;Use IAM database authentication when your application requires fewer than 200 new IAM database authentication connections per second.  
&gt;  
&gt;The database engines that work with Amazon Aurora don't impose any limits on authentication attempts per second. However, when you use IAM database authentication, your application must generate an authentication token. Your application then uses that token to connect to the DB cluster. If you exceed the limit of maximum new connections per second, then the extra overhead of IAM database authentication can cause connection throttling. The extra overhead can cause even existing connections to drop. For information about the maximum total connections for Aurora MySQL, see [Maximum Connections to an Aurora MySQL DB Instance](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Performance.html#AuroraMySQL.Managing.MaxConnections).

It isn't clear to me if this is saying that by enabling IAM permissions it creates an even stricter connections per second limit than what is normally imposed? It seems like the connections per second limit isn't just for IAM users, according to the ""Max Connections"" link above.

Ideally we'd just use IAM for everything, including the roles that need access to the DBs. However, minimally, we'd like to use IAM for employees/contracts, and we could still use traditional ""MySQL DB users"" in the DB for the applications themselves to access. With spikes in the system, we could certainly surpass the 200 connection per second mark.

Am I misunderstanding the connections per second restriction? And are there any other opinions on using IAM for Aurora?

(Side note: Several months ago when our team looked into IAM permissions and limitations, the documentation used to say that enabling IAM permissions on a DB created a hard 250 connection per second limit, but this seems to no longer be the case)",2,aws,2020-10-13
j5ilt2,Adding SSH users to AWS Lightsail,"I have a CentOS instance on Lightsail, I wanna add additional users and give them ssh access. So I did the sudo useradd and then tried generating ssh keys but that didn’t work, just gives me this error: “Permission denied (publickey,gssapi-keyex,gssapi-with-mic).” So then I copied the authorized_keys file from the CentOS user to the new user and tried SSH with the .pem key and still go the same error.

Has anyone tried this and been able to get it to work? Most examples I see just seem to deal with EC2.",1,aws,2020-10-13
j5jxvg,Script to list what each instance in AWS can access to or be access from at a network level?,"Is anyone aware of any (preferably opensource) scripts or tools that can query an AWS account, and essentially list out what each instance can access or be access from at a network level (i.e. IP and Port)? I presume it would need to query security groups, NACLs and routing tables at a minimum.",5,aws,2020-10-13
j5k9hy,AWS Backup on running EC2 instances,"Hi, 

I'd like to automate the EC2 backups using AWS Backup but I haven't found any info about backing up a running EC2 instance.

It's possible without losing data?

Anyone is doing this in a production environment?

TIA &amp; best wishes",0,aws,2020-10-13
j5jsap,Week of Oct 5th - What AWS questions do you have? (but were always afraid to ask :-) ),Ask your AWS questions here and help others - Also don't forget that there are almost 2500 members in the /r/aws chatroom!,2,aws,2020-10-13
j5jm8h,Deadling with unused workspaces...,"we're using the AWS Workspaces Cost Optimizer to flip between monthly and hourly instances, it's working great.

one issue tho is that we have many units that are just sitting there at about $10/mo and costing us thousands. I'd like to terminate them going into a new month. 

I can use the csv output the optimizer creates to do this but is there a simpler API driven way to get this info?",6,aws,2020-10-13
j5ii8m,Scheduling/Autoscaling Pet instances on AWS EC2,"Hi all,

I’m on a Move-to-Cloud project for one of our costumer. It is a very big migration that needs to be executed fast to go out their existing DC  ASAP (both for deadline DC closure reason and avoid double run costs  Cloud/On-Prem). So, refactoring all the applications is not an option due to the short  timeline. Currently, they are sized to the peak and migrating as is, could be  inefficient from a cost perspective. They can’t use AWS AutoScaling groups  yet due to their existing ‘Pet’ model on OnPrem VMware.

I’m looking for a solution that will do some kind of autoscaling on AWS EC2 instances  but orchestrating ‘Pet’ ones (so start/stop instances only and not create/terminate ones) based  on some custom metrics or CPU...

Thanks for any advice!",1,aws,2020-10-13
j5ifbr,Is it correct to use Serverless Lambda functions to render templates?,"I was transitioned to project using Serverless/Lambdas and needed to create a PDF generator.  It's a node project, using tsoa (TypeScript api), so I went with Puppeteer and Handlebar.js.

I'm running into an issue where locally the template directory exists and resolves but on the Lambda server it doesn't exist.

Our set up is, we have a parent directory in the root and every child directory to it is a service.  In a child service, I'm trying to access and read template files into memory to build a template in Handlebars.  Locally, the it finds the paths - but upon deploy - it cannot find the templates directory.  First templates was a sibling to `/proj_root/api`, but then I moved it to a shared folder under `api`.  Again, works locally but on server I get an error saying "" `/var/task/api/shared/templates` could not be found.""

I followed [this guide](https://www.serverless.com/framework/docs/providers/aws/guide/packaging/#examples) to try to include the directory with:

    package:
      include: api/shared/templates/**

Still to no avail.

Is this a misuse of serverless to include static files with a function?

Is it ""best practice"" to use an S3 bucket and make an api call for every static template file?

Is there a serverless way to include static files with a function? (AWS Layers?)

\----------------------------

Edit: 

Solved the issue.  Had to use webpack to copy the static folder to the dist folder.",3,aws,2020-10-13
j5ikem,CI/CD for Elastic Beanstalk,"I will be creating a beanstalk app and need guidance on best practices for CI/CD.

I am well versed with the AWS CLI, and Cloud Formation, and have a bit of experience with the EB CLI. The last project I worked on that used beanstalk had an environment that was set up manually then updated via pre-written scripts that used the EB CLI. Unfortunately this made it difficult to stand up clones of the environment due to the manual step.

The environment will consist of a multi-instance java app and will be using RDS. Standard public web app stuff like CNAME, redirect HTTP to HTTPS, putting the ec2 instances in a private subnet while having the load balancer public, etc.

What is the best way to do this? In my serverless environments i'm able to stand up a new env front/backend, s3, api gateway, dynamo, route53, etc. in about an hour using cloud formation/SAM. I'm looking for something similar with beanstalk. I haven't found any great docs on how to do this for more advanced setups other than using the console. Or is the best way to stand up new beanstalk environments actually to use the console, then make modifications with the CLI/config files as I was doing in my other project?",1,aws,2020-10-13
j5hpyb,Slow AWS Client VPN Performance,"We are testing AWS Client VPN before we completely migrate from using Windows Server as our VPN server. We're using certificates generated from easyRSA as authentication and  everything seems to be working as expected. I tried using the ovpn config through AWS' own VPN app and OpenVPN client.  


However, I  noticed VPN clients are getting significantly lower speeds compared to our old setup. I'm talking about around 6Mbps where it should be 50Mbps.  


To  troubleshoot, I ran the speedtest on a Windows Server EC2 instance on  the same region as the Client VPN endpoint (Tokyo). The speeds are upwards 100Mbps as expected.  


I've read about disabling ""Hardware Checksum  Offloading Hardware TCP Segmentation Offloading"" but I can't find a way  to configure this on AWS Client VPN. I checked laptop's NIC and I can't  find that option in 'advanced' tab either. Help?",2,aws,2020-10-13
j5cg3h,What to use if i need a long running application being serverless?,"as title. sorry for being ignorant... as far as i know lambda has a time limit, but my application is connecting to an external party via websocket and processing data fed by it.",1,aws,2020-10-13
j5ghkc,How to properly use security group to manage accesses ?,"Hi there

TL;DR : I've been pondering a change in the way I use security groups. From ""SG allow this subnet here"" to ""allow this (ASG A) SG to this (other ASG B) SG"". Good idea, bad idea, how do you do it ? 

Currently I've VPCs with a bunch of subnets : for each AZ I've ""public"", ""private"", ""database"".

I don't want to discuss that specific point which could be improved, but for clarity's sake : there's one VPC by environment (production, staging, dev...), in production's VPC you've both IT stuff like the wiki, mattermost, etc. And our apps for our clients. 

I'm managing accesses for my different services through ingress security group stating ""from this subnet, you can access this port on the ASG's instances the SG is attached to"". A real life example being ""rabbitmq-from-vpc-private"", meaning anything in the VPC's private subnets can access the rabbitmq ports on the rabbitmq ASG's instances. 

It's been good enough to begin with, but I feel it's lacking, for example to allow access from the SSH bastion in public subnet, I need a rule ""allow SSH from VPC public"" everywhere, which isn't exactly secure if one of my HAProxy get compromised for example.

So I'm thinking about switching to a ""allow this access from this security group"". The terraform module I'm using refer it as ""Access from source security groups"", I don't know if that's the word for it. I can see the gain, I'm even wondering if keeping subnets is useful once properly set up. But I've no idea of the downside except for the time spent.

Hence here I am asking for your opinions, experiences in the matter, any interesting article or piece of doc is welcome too.

Thanks guys &amp; gals, take care !",4,aws,2020-10-13
j5g80w,AWS EventBridge Rule complains that target is not a valid JSON format,"hello, I am following AWS tutorial &amp; trying out input transformation on rule.  
[https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-input-transformer-tutorial.html](https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-input-transformer-tutorial.html)

I am not changing anything &amp; copying straight from the tutorial but when I click on create to create the rule, I get `Input for target Idbda77dd1-bec3-4408-9e1f-9d154933b733 is not a valid JSON text.` 

Am I missing something? Is there a working example out there that I can use, maybe console or IaC?

&amp;#x200B;

https://preview.redd.it/363gw5dst8r51.png?width=774&amp;format=png&amp;auto=webp&amp;s=18dc3b49b66bfc2db62598df195361c88dcb20a2",2,aws,2020-10-13
j5daf2,Reasons to use EKS over ECS (2020 Oct) ?,"Is there any reason to use EKS over ECS if:

1. We are not planning to manage EC2; means we only want to use Fargate
2. We are fully invested on AWS. No plans to migrate to other cloud or on premise
3. We have enough AWS skilled people in general (not specifically ECS), but have fewer K8s skilled people (but possible to hire or train)
4. The applications we are planning to deploy are regional. Will only be used by users in the region. We will only deploy in one AWS region. The peak usage could be around 10k concurrent sessions. Will go down to few hundred users in non peak periods.
5. We need to have full control on the VPC and networking in general.

6. We will be using terraform exclusively for the deployment.

7. Our CICD pipeline will be using gitlab.",61,aws,2020-10-13
j59d4m,Question about securing access to a S3 bucket,"Hi. Looking for guidance/confirmation on securing access to a S3 bucket.

So I have a S3 bucket that has public access turned off (S3's default). The only place I want to access this bucket from is a specific EC2 instance. I tried to find the best way to do it and found various different combinations but finally settled on creating an IAM role and then attaching it to the EC2 instance, selecting a profile that gives S3 access.

My question is if this makes sense? Is there a more recommended way of doing this? Do I also need to apply some settings on the S3 side to further secure it?

Thanks for your time.",1,aws,2020-10-13
j57mza,Does it cost extra charge if we install and run extra applications on a single ec2 instance?,"Today I signed up for aws free tier account for learning. And created tier2.micro account running ubuntu which was eligible for tier 2 free.

But fear it doesn’t cost me anything for keeping up all the time I stopped the instance.",0,aws,2020-10-13
j57o5b,Looking for a book for AWS + .net core,"Hi everyone!
So, my company builds an app based on AWS. We're primarily using .net core for the BE and a broad set of AWS features, such as S3, Api Gateway, Lambda, ECS, Fargate, DynamoDB, etc. I was learning some of these by reading the documentation, watching Udemy video courses and a bit of YouTube. What I am looking for right now is a good book that would connect everything for me in one piece and would give me the confidence in working with these technologies. 
Any suggestion would be very much appreciated. Even if the examples within the book are not on .net core but are good and easy to grasp, I'll consider it for study.
Thank you very much.",3,aws,2020-10-13
j57qyq,Pearson onvue online proctoring help,"Hi, 
I have an upcoming exam using onvue online proctoring 
I've read a few horror stories online, so was wondering if anyone could guide me through the check in process? 
I understand that I need my phone for this? 
Is there any way I can just use my webcam? , As I'm frightened about having my phone with me during the exam
If anyone could shed some light on the process I'd really appreciate it 
Thanks 
mo",2,aws,2020-10-13
j582ic,Auto Extending an EC2 volume,Is AWS auto extending a volume partition automatically now when you extend a volume with an EC2 instance? I added more to my volume and when I went into my EC2 Ubuntu 18.04 instance the volume was already extended!!!,3,aws,2020-10-13
j57iwt,S3 static website bucket policy,"Can someone tell me what I’m doing wrong?

My buckets are private and I’ve applied the cloudflare recommend bucket policy to allow cloudflare IP to access my static website. 

https://support.cloudflare.com/hc/en-us/articles/360037983412-Configuring-an-Amazon-Web-Services-static-site-to-use-Cloudflare

Still getting access denied. Do I need to make the bucket public?",4,aws,2020-10-13
j5363l,Lambda beginner project with python,"Hi, I've been learning python and AWS (got my CCP!) since the pandemic started.  

I want to do a bot on telegram and point it to an AWS Lambda function that returns the metacritic rating of game, or rotten tomatoes score of a movie, haven't decided which.

Does this seem like a feasible project for a beginner?  What would be the basic steps?",8,aws,2020-10-13
j555qh,Using SSM Parameter Store with CloudFormation,,4,aws,2020-10-13
j5525n,Redshift/Quicksight: Most performant way to handle querying arrays within json?,"Hi all,

I currently have a large set of json data that I'd like to import into Amazon Athena for visualization in Amazon Quicksight. each json contains two fields: one is a comma separated string of ids (orderlist), and the other field is an array of strings(locations). What is the best way to handle/store this data to produce the most performant queries when generating dashboards on Quicksight?

 Because Quicksight doesn't support array searching, I'm currently resorting to creating a view where I generate crossjoins across the two string arrays:

    select id,
     try_CAST(orderid AS bigint) orderid_targeting,
     location
    from advertising_json 
    CROSS JOIN UNNEST(split(orderlist, ',')) as x(orderid)
    CROSS JOIN UNNEST(locations) t (location)

I've also considered flattening out the data beforehand, would that make a difference in query performance? Considering that I have two crossjoins currently, that could 30x my storage space if I explode out the two arrays, I'm hesitant to go down this route if it doesn't make a significant difference in performance.",6,aws,2020-10-13
j54mo6,Best way to store logs for analysis,"Slightly silly question, maybe, but doing silly projects is how I teach myself stuff.

I run an IRC bouncer in AWS that saves logs in JSON format. I want to archive said logs somewhere that will make for easy searching/analysis.

My original thought was Elasticsearch, but a managed cluster seems like overkill. Standing up an ES container in my Nomad cluster is problematic because EBS doesn't play well with ephemeral EC2 hosts (that may not always be in the same AZ) and EFS has too much latency to host ES.

Thoughts?",28,aws,2020-10-13
j53koq,"Cloud computing (AWS, G cloud) vs Web Hosting (WPX, WP Engine). Which one is better for Hosting a WordPress website?","Let's say I have a WordPress website with 1M visitors/month.

So, which is a better option for hosting this website.

Cloud computing (AWS, G cloud) or Web Hosting like (WPX, WP Engine).

**and I am not talking about the performance/speed **

I want to know that which one will be more convenient / easier to use.

For example: If I use WPX or WP Engine, then if I ever face any technical issue, I can simply contact them via live chat and they'll solve my problem in minutes.

But if I choose to host my website on AWS or G cloud, then will I get the privilege of contacting them via live chat, if I ever face any problem?
If so, then how much will I be paying for their Support?

Also, which option is more cost effective?",1,aws,2020-10-13
j4yluy,Create and host a static website on AWS S3 - Part I,"In this article we will see how to create a very simple webpage (just one page) and use the **AWS S3** to host it. 

[https://dusaitis.com/posts/create-and-host-a-static-website-on-aws-s3-part-i](https://dusaitis.com/posts/create-and-host-a-static-website-on-aws-s3-part-i)

&amp;#x200B;

[my awesome web page hosted on AWS S3](https://preview.redd.it/fcvqj7rkt2r51.png?width=2030&amp;format=png&amp;auto=webp&amp;s=b423ed80cd5fcc9fa9b358d5e8e22b23f20a1dca)",0,aws,2020-10-13
j4u0ur,Workspaces no longer free,Just a reminder to everybody who tried Workspaces because its was free for a while thats over now.,25,aws,2020-10-13
j4whk6,AWS CloudFront route traffic through private DNS name,"About 10 years ago, I setup AWS Cloudfront origin pointed at my EC2 private DNS so that I can close off all outside traffic from the internet to my EC2 instance directly using security groups.  In other words, all traffic had to go through CloudFront.

I tried doing the same and it won't let me.

Does anybody know how I might able to get this to work?

Thanks in advance.",8,aws,2020-10-13
j4vzw5,Can Transit Gateway ENIs be configured as Traffic Mirror sources?,"As per title - there's an archived thread with the same question but no confirmation.

This seems like a much cleaner approach to traffic monitoring in our VPC than setting up each individual EC2 host/eni.",2,aws,2020-10-13
j4s7y4,AWS Lightsail Deep Dive: What is it and when to use it,,46,aws,2020-10-13
j4r4lh,Question: AWS Amplify Graphql S3 Integration,"Reading through the documentation I see this line:

""The GraphQL Transform handles creating the relevant input types and will store pointers to S3 objects in Amazon DynamoDB. The AppSync SDKs and Amplify library handle uploading the files to S3 transparently.""

From what I understand, this means by running a graphql request with the described S3Object definition I should be able to do a mutation and by running that mutation upload the data to S3. However, that doesn't happen. I'm curious to know if this functionality exists and how to do it or if I am misinterpreting what's being said, thanks!

[https://docs.amplify.aws/cli/graphql-transformer/storage#basics](https://docs.amplify.aws/cli/graphql-transformer/storage#basics)",3,aws,2020-10-13
j4ohcp,Aim for recovery from regional failure or zonal failure? TRICKY EXAM QUESTION,"So since AWS wants us to build while keeping fault and failure tolerance in mind, should we do that on a regional scale or a zonal scale?   


for example i can duplicate my resources in different zones in the same region right? but then what if a disaster hit the whole region or a regional outage happened? in this case i should have built my infrastructure to rely on regional availability no? so incase of one region outage accord then at least i have a different region serving my users?  


The question in this picture is from jon bonsos practice test for CCP

  


https://preview.redd.it/nmpwf8n2myq51.png?width=770&amp;format=png&amp;auto=webp&amp;s=6744f06c85f980082377602313d496bc5e08502d",12,aws,2020-10-13
j4nua5,Utterly confused by Cloudwatch custom metrics GUI,"Let's say I have an app that simulates a train, I want to collect metrics on the number of passengers, and also their names, I use commands such as this to add metrics:

`aws cloudwatch put-metric-data --namespace MyTrainApp --metric-name passengers --value 2
--unit Count --dimensions ""passengers=abe;bob""`

However, over in the console, when I try to look at a graph of the passengers, the data is sorted up by the passengers list (`abe;bob;carl`, etc), so I need to manually select each unique passengers list in order to have Cloudwatch graph the whole thing.  This is very counter intuitive to me as I'd expect it'd be sorted by metric-name.

Is there something I can do to make it do what I expect?",1,aws,2020-10-13
j4kmnq,How Do I Get Graphics Output From My EC2 Instance To My Local Machine?,The EC2 instance is running the Ubuntu 18.04 Deep Learning AMI and my local machine is running Ubuntu 20.04. I have some Python scripts that output Matplotlib graphs and I would like to run the scripts on my EC2 instance and then see the output graphs on my local machine. Is there a good way to do this? Some googling turned up a bunch of things about X11 and Windows machines but I couldn't find a good resource concerning Linux local machines. Any help is appreciated. Thanks in advance.,10,aws,2020-10-13
j4gm7o,Protect S3 videos from being downloaded by Add-Ons or Plugins,"Hi! I'm new to this community :) Let me explain my problem... I'm running a website where I sell my courses and the students streams the videos in the website. I host every video in a s3 bucked, I already converted the mp4 to hls using Media Converter and I add the embed video in the website using the .m3u8 file. Everything works perfectly but... If someone use a plugins or a download manager they can grab the video and download it without any problem. For example videodownload helper (a mozilla addon) can download these videos without any problem. How can I prevent all these plugins using an aws? Thank to everyone! And sorry if I made any mistake posting this here :)",0,aws,2020-10-13
j4gezr,Cracking my head over proper routing for multiple API gateways &amp; cloudfronts,"I've been struggling all day long how to best do routing for a new (non-technical) client. The project is small enough to use it as a test bench for our standardization.

They have a main website, hosted by a third part, on [website1.com](https://website.com) \+ a few sub websites also on different domains. They asked us to develop a chatbot for [website1.com](https://website.com) and since the third party could become a bottleneck in terms of DNS and support we decided to register [client.com](https://client.com) in route 53 and let the third party load it through an iframe.

So now I have [client.com](https://client.com), loaded through an iframe, serving the chatbot and [api.client.com](https://api.client.com) to act as the API gateway with websockets.

The client has said that they want a dashboard for the chatbot1 + expand into the other websites that they have with new bots. I think the best would be to expand the current API gateway to accommodate the dashboard API calls as well for chatbot1. Before we continue though I want to make sure that we have a proper plan in place for expanding, including DTAP (dev, test, acceptance, prod).

Taking the chatbot1 as an example, what would be the best course of action?

&amp;#x200B;

|frontend|API gateway|
|:-|:-|
|client.com/web/chatbot1 |client.com/api/chatbot1 |
|chatbot1.client.com |chatbot1.client.com/api |
|client.com/chatbot1 |api.client.com/chatbot1 |

&amp;#x200B;",3,aws,2020-10-13
j4eebh,How to set up WordPress website on AWS free tier?,"I want to set up a WordPress website on AWS free tier, I'll be only expecting upto 1000 visitors/month in the website's initial days.

I looked up for articles on the internet, explaining that I'll need to use EC2 for creating WordPress website.

But I am confused about few things:

1. On Amazon EC2 page, it says 750 hours/month, but a month only have 720 hours (24x30=720). So, what does 750 means?

2. How much storage will I get with EC2? Where my website's files/media will be stored? Do I need to install another software (except EC2) to store my files?",0,aws,2020-10-13
j492fw,AWS Associate Solutions Architect TECH U Program interview rescheduled,"Hi Everyone,

I had a virtual onsite interview scheduled with AWS. Recently, I got a mail from them saying that they have stopped interviewing for this position and they will reschedule the interview in the coming days. I tried to reach out to the recruiters but no luck. Is there someone with the same situation?",1,aws,2020-10-13
j4aqur,Cognito reset password returning NotAuthorizedException instead of PasswordResetRequiredException,"Hello, I have imported a user to cognito. The user state is in Reset Required as expected. However, when I try and login, using amplify library, I get back a NotAuthorizedException. As I understand, the docs say cognito should return a PasswordResetRequiredException instead. 

Any idea of what I'm doing wrong? When signing in, I'm entering a random set of letters for the pw but that shouldn't matter since the user is in reset required. Is that correct?",2,aws,2020-10-13
j496wl,Production OpenVPN access server recommendations?,"I'm currently working on setting up access to a multi-account AWS structure. We have dev, stage and prod VPC's in different accounts. All are peered to a shared-services account which will host OpenVPN access server to allow access to the network.

Has anyone set this up with high availability? Or do you have any general security recommendations for a production-grade setup?",6,aws,2020-10-13
j46szl,Parima: Launch Your Website using AWS in Minutes : License : Apache 2.0 Github,"[Parima: Launch Your Website using AWS in Minutes](https://github.com/formkiq/parima)    

… and leave any server and infrastructure complications behind

    ** What is Parima?

Parima is a free, open source project that has been created to allow easy publishing of static web sites and JavaScript web applications (like Angular or React apps) using Amazon Web Services. It's ideal for designers and front-end developers who want to launch a new web site without having to worry about hosting outside of creating an AWS Account.

Parima creates a simple, serverless architecture for your web site, using Amazon S3 to store the markup, code, and asset files, and AWS CloudFront to distribute the site using HTTPS. It can be installed with or without a custom domain (for custom domain requires domain DNS to be using Route53).


    ** How Parima works?

[Parima on AWS](https://github.com/formkiq/parima/blob/master/images/aws.png)

Parima installs into AWS Account using CloudFormation (through the Console or AWS CLI).

CloudFormation sets up an S3 bucket to hold the artifacts of the website, as well as CloudFront to host the website in Amazon's content delivery network (CDN). Optionally, if a Domain/Hosted Zone is specified, a Certificate and Route 53 DNS entries are created for your custom domain.",12,aws,2020-10-13
j44ydu,Allowing external access to a specific S3 bucket,"I have a use case with my clients where they export their data from our applications. This is done via a mongo dump that then uploads their data in zip files to S3. Currently a public URL to their data is created for a period of time. However, the issue is that anybody with the URL can download their content. Obviously this isn't anywhere near ideal, and the process was created before my time.

What I'm looking to do, is provide some sort of authentication to the S3 bucket(s) and then the client can download their content. The process/script I have put together now creates an S3 bucket \[client-name\]-\[id\], and in that S3 bucket a series of zip files are uploaded with their content. The roadblock I'm at is how to allow them to access their data and prevent anyone else. I did debate on using Amazon's SFTP service as I use this elsewhere for content uploading. I'm not sure that's necessarily the best path or at least not anywhere near as convenient as just clicking a link and your files start downloading.

Any thoughts on how to go about this? My goal is to have some sort of authentication prior to being able to actually access those S3 object URLs. But any other suggestions are certainly helpful and welcome.

Edit: So out of ""pure laziness"" I used Amazon's SFTP service and wrote a script to upload the zip files to an S3 bucket, create an IAM role and policy for that bucket, create an SSH key, and then add the role to the SFTP server. I didn't have much time to devote to this and while its not the most convenient way to do it, it certainly works.

Thank you all for the replies.",2,aws,2020-10-13
j44l6e,Client VPN or custom OpenVPN setup,"Hi,

I’m fairly new to AWS, and the sheer number of services can be a little complicated! We are a startup handling sensitive patient data. We have two services, an Internet facing API that collects data coming from our mobile app - and a backend management web interface that lets us see the status of patients in our pipeline.

We are all working remotely so we have no static IP’s and what we want to implement is a VPN system where we can connect our machines to the VPN to have access to the backend (while still having an internet connection) and leave the internet facing API to be accessible from the internet. The purpose of this is to physically remove the backend system from the outward facing internet to try to reduce attack surfaces.

My plan is to use Client VPN with I guess two separate VPCs (let’s call them VPC-web and VPC-internal for front and back end). Then set up some network rules that prevents internet traffic from all but the CIDR specified in the Client VPN settings for the VPC-internal VPC. I think I could also use split tunnel to give us internet access while connected to the VPN.

Is this the best way to do things or am I glossing over something super important?

Best wishes
Tom",1,aws,2020-10-13
j42i75,So what's best practice with multi-account now?,"I'm currently looking after around 16 accounts, all of which are standalone. 

I want to split out environments and probably support a split by business unit too. Of course, every engineer wants their own account. Clearly automating auth, guardrails and account 'furniture' is critical. 

Organisation is created, all features enabled (so too late for control tower). We tried ADF last year but tbh felt too, erm, heavy.

There's a lot of options now. Both in tech choice and general approach. 

Anyone set out on this in the last 6 months?
What's good? Snog/marry/avoid?",6,aws,2020-10-13
j415wf,Query Dynamodb function using gsi with more than 1MB of data,"I'm trying to find a timely and cost-effective way to query a dynamodb table using a lambda function that will scan the table and return 1MB+ values using a sort key to determine which values to return. I read that parallel scans are a thing but since I have to use a sort key + GSI to narrow it down I'm not sure how much time that would save. I also saw that I could query the table into batches(by finding the LastEvaluatedKey) but considering that would be 10+ calls to the database every time(function is called every 10 mins) I don't think that would help as well!

If anyone has alternatives/advice it would be much appreciated!",3,aws,2020-10-13
j422w2,S3 Update – Three New Security &amp; Access Control Features,,125,aws,2020-10-13
j421hr,Amazon S3 Update – Three New Security &amp; Access Control Features,,26,aws,2020-10-13
j417un,QuickSight with tables in RDS as source?,"Looking to use (and learn) QuickSight with RDS tables as source. Looks like when I go to add RDS as a source, it wants me to choose a single table to be the dataset. If I have 10 tables, is it better to add them individually, or query the data myself to join them and form a single, denormalized data set across all 10 tables?

Otherwise, should I use something else in the middle, like Glue/Athena, to make this process easier?

Thanks for any advice!",5,aws,2020-10-13
j3ue5u,How much roughly for a month of EC2 windows vm for a virtual assistant.,"I currently have a VA that works for me and I would love to have a virtual machine for her to log into and have everything configured to assist with some training

&amp;#x200B;

What would be the cost roughly of a Windows vm running on AWS 24/7, does anybody have some rough numbers for US east region?",1,aws,2020-10-13
j3ylnu,MAC OS on aws ec2 instance,"Hello

Is there any option to deploy mac os on aws ec2 instance. I did search the ami lib and couldn't find one for mac os x. Any thoughts or suggestion would be of much help",0,aws,2020-10-13
j3xpsq,Transit Gateway with SSM?,"I'm doing a proof-of-concept for an architecture that allows us to keep each of our customers in separate VPCs - some of our customers require this because they work in highly sensitive industries, and our 'classic' app is a fairly standard LAMP stack that we deploy to an EC2. We want to be able to access the EC2s by SSM, as we do currently, and the EC2s require outbound internet access (i.e. a NAT).

My problem is: how do I give each customer an isolated VPC, but still allow them outbound access. I could create a NAT on each VPC, but since each NAT requires an elastic IP and we have only 5, this does not seem like a scalable design.

My solution: Create an egress VPC with the NATs and use a Transit Gateway to route outbound traffic through the egress VPC. I followed the instruction [here](https://aws.amazon.com/blogs/networking-and-content-delivery/creating-a-single-internet-exit-point-from-multiple-vpcs-using-aws-transit-gateway/) to do this. I created two test EC2s - one in the egress VPC and one of the application VPC. I can use SSM to connect the EC2 in the egress VPC, but not to the EC2 in the App VPC. 

Things I've already tried: 

* I've created a VPC Endpoint for SSM in the application VPC. Now instead of instantly telling me it can't connect, the busy spinner on the Connect button never goes away.
* Checked that both EC2s have the same IAM Role (with the SSM policy attached)
* Checked the security groups on both to allow inbound and outbound traffic on all ports (this a POC in an isolated account - I wouldn't go live with this configuration!)

Questions:

* Is there a better way to isolate customers? (Maybe one AWS account per customer? This could become a pain to manage though...)
* Is there a better way to share NATs (or some other way to get outbound connections)?
* What could I be doing wrong that SSM doesn't work?

Thanks!",6,aws,2020-10-13
j3z9zs,How to stop DMS replication ?,"A few POC's was done a few months back, but the dms replication was not stopped or deleted. Right now I'd like to stop the replication and stop the compute cost.

I dont see an option to stop the DMS service, the only option is to delete, or modify. I dont want to delete it.",1,aws,2020-10-13
j3vqxp,Document Empty after uploading a SVG to S3 bucket and trying to access to it via URL," 

So, i'm trying to upload my web images into a s3 bucket. I send them via HTTP Request to a lambda in AWS. It executes correctly and the image appears in my bucket, but when i tried to serve it in my frontend it doesn't load, and if i open the url in my browser i get this error:

***This page contains the following errors: error on line 1 at column 1: Document is empty Below is a rendering of the page up to the first error.***

My lambda is develop in **nodejs v12**, im using the **aws-sdk \^2.765.0**. And the code in the server side is this:

    let buf = new Buffer.from(image.replace(/^data:image\/\w+;base64,/, """"),""base64"");
    
    let type = image.split("";"")[0].split(""/"")[1];
    
    let params = {
        Bucket: process.env.BUCKET_IMAGE,
        Key: `${name.toUpperCase()}.${type.replace('+xml','')}`,
        Body: buf,
        ContentEncoding: ""base64"",
        ContentType: `image/${type}`,
        ACL: 'public-read'
    };    
    
    s3.upload(params,function(err,resp){});

 I tried sending the complete base64 without buffering too but i doesn't work either.

Any idea what could be wrong? The previous code works perfectly with any jpeg/png image.

Thanks in advance.",0,aws,2020-10-13
j3yjkg,API Gateway to API Gateway policies,"Hi,

I'm creating an API Gateway (in Account A) which will be the centralized gateway for API calls between many accounts

Let's say I have, on Account B, another API Gateway which exposes a lambda.

  
For consistency reasons, I want the centralized gateway to call the target endpoints through HTTP Integration (not directly the lambda in act B), so it will be like this:  


HTTP --&gt; API GW (A) --&gt; HTTP --&gt; API GW (B) --&gt; Lambda (B)  


Is it possible, in this scenario, to restrict API GW in Account B so that it only accepts requests coming from API GW in Account A?

I tried adding a Resource Policy in API GW B similar to the \*\*AWS Account Whitelist\*\* example but since it's being calling by HTTP I think it's not providing account information.

Thanks!",4,aws,2020-10-13
j3y1x4,AWS APM Solutions,"Am I just being thick, or does AWS not have a browser telemetry solution?

Azure handles it within AppInsights (both client and server) [https://docs.microsoft.com/en-us/azure/azure-monitor/app/javascript](https://docs.microsoft.com/en-us/azure/azure-monitor/app/javascript).",0,aws,2020-10-13
j3xu2i,Cognito: Any need for Lambdas to verify contents of JWT?,"Assuming one is using Cognito for authorization of an API Gateway which is connected to a Lambda, is there any need to verify the token that arrives at the Lambda?

We use custom attributes in Cognito, so they are included in the JWT which makes their way to a Lambda through API Gateway. I was about to code-up verification of the JWT when I realized that Cognito has likely already verified it, meaning that its contents can be trusted by the Lambda. Amiright?",2,aws,2020-10-13
j3xpjn,AWS Directory Service: patched against CVE-2020-1472? (Windows ZeroLogon),"We're an AWS customer and have been using AWS Directory Service for about 2 years.  Does anyone know if it's been patched against  [CVE-2020-1472](https://portal.msrc.microsoft.com/en-US/security-guidance/advisory/CVE-2020-1472)**?**

I'm not too concerned because ours is in a private VPC, and a Palo Alto firewall limits traffic to DNS and LDAP.  There are no domain-joined instances.  We do use AWS Single Sign-On though for authentication to AWS Console and Slack.",3,aws,2020-10-13
j3x16d,Rotating HTTPS Credentials for CodeCommit,"Is there any way to set up an automatic rotation of HTTPS credentials used to access CodeCommit repos through git? For security purposes, I want to set up a rotation of HTTPS credentials every 90 days but as far as I can tell, AWS doesn't have any support for rotating those credentials through the CLI or in BOTO3. IAM access key rotation is easy to do and well documented but I can't find any information about HTTPS credentials, as if no one's tried to do it at all. Is there some reason those credentials don't need to be rotated?",1,aws,2020-10-13
j3ui41,Setting up AWS Healthchecks,"Hi there

Im setting up a couple of healthchecks that is either checking HTTPS (443) or LDAPS (636) and they are working great, they also give alarms when its gone offline. Now im trying to setup a notification for when a healthchecks goes back to OK, for example if a healthcheck has failed and it comes back to 100% for 5 minutes it should notify that the service is online again.... but cant seem to find the correct way to do this?

Thanks.",1,aws,2020-10-13
j3wq9u,Cognito: How do you add metadata/custom attributes to a user?,"I've added a custom attribute, but I can't find out how to add actual data into that attribute for a given user.

In Auth0 you could add metadata and it would come into your web app on the idToken, does Cognito have this kind of functionality, or do I need to roll my own solution using Lambda/DDB and a pre/post auth hook?",1,aws,2020-10-13
j3wluv,IAM Role Path,Curious to hear from folks if and how they leverage the IAM path for a given role you've created?  We've been creating roles up until now under the default path / and am curious how changing that strategy might help or not??,1,aws,2020-10-13
j3wfm3,What way would you recommend me to host my MEAN stack website?,I was thinking on going for Lightsail with Bitnami's MEAN stack blue print but I'm not sure if hosting the fronted backend and database on the same instance would be good or secure. What do you guys think?,1,aws,2020-10-13
j3w0ws,AppStream and User time limits,Is there a way to set time limits so that users can only use a set number of hours per day?,1,aws,2020-10-13
j3ugsn,Clear Amplify Appsync cache form node.js Lambda,"I have an Amplify Appsync graphql api.

I have a bunch of the default crud mutations and queries and some custom mutations that are resolved with a node.js Lambda function (gqResolver).

Here is an expert from my graphql schema which shows a custom mutation.

    type Mutation {
      buildCompetition(id: ID!, params: CompetitionParamsInput!): Competition @function(name: ""gqResolver-${env}"")
    }

I want to enable server side caching ([https://aws.amazon.com/blogs/mobile/appsync-caching-transactions/](https://aws.amazon.com/blogs/mobile/appsync-caching-transactions/))

But some mutations, when run, should clear the server cache (certain resolver caches would be nice or all would also be ok).

Does anyone know if this is possible?

Alternatively, if it would be possible to specify in the client api call to use the server cache or not (at the moment I can only turn it always on or always off per resolver). That may also help.",1,aws,2020-10-13
j3vcsg,AWS AD and On Prem authentication question,"I have no windows admin experience, so apologies in advance if I am asking something basic or incorrect. So, we have on Prem ad and AWS ad with trust relationship. We are using direct connect. From my local system, I connect to an RDP and then RDP again into the ec2. My question is , how do I login (or create login) to the SQL server rds inside of ec2 using my on Prem ad user. This is a poc and intention is that we do not want to create new users but rather use on Prem users to authenticate using the trust relationship. Also, the ec2 is manually joined to the AWS AD Please help",1,aws,2020-10-13
j3tonj,AWS::CertificateManager::Certificate Automatic DNS validation?,"So, I was thinking about migrating away from the lambda to generate my certificates - and to jump straight in the new Cloudformation feature for the AWS Certificate manager, that looks like this:
```
  MyCertificate:
    Type: AWS::CertificateManager::Certificate
    Properties:
      DomainName: ""stuff.mydomain.com""
      SubjectAlternativeNames:
        - ""morestuff.mydomain.com""
      ValidationMethod: DNS
      DomainValidationOptions:
        - HostedZoneId: XXXXXXXXXXXXXXXXXXXXX
          DomainName: mydomain.com
```
I thought that this would put the required entries straight in the Route53 Zone config, but I ended up waiting for about 2hrs.

Is the above example somehow wrong, or is the AWS::CertificateManager::Certificate resource *not* supposed to actually input the records to the supplied Route53 zone?",2,aws,2020-10-13
j3tdvg,How are you providing access to S3 data in EMR,"Hey everyone I have an AWS account that stores tons of large datasets using parquet on S3.

I have a separate account that will leverage EMR and Spark to process this data.

I am wondering how people are providing access to their data. S3 bucket policies? Lake Formation (not supported on spark submit or cross account), etc.

Thanks in advance!",2,aws,2020-10-13
j3ssqy,AWS workflows to set up services automatically via GitHub CI/CD: just fork &amp; reuse!,,97,aws,2020-10-13
j3r7o7,S3 NextContinuationToken question,"Hey, guys, i was wondering about how will nextContinuationToken behave in the scenario where i listObjects, say a 1000, get a nextContinuationToken and delete those 1000 objects i got through listObjects call from s3, will the token be invalidated and fail next call to listObjects using the token? Or will it work correctly and list other objects in the bucket?",2,aws,2020-10-13
j3q3nr,How to securely share resources in bulk with other accounts?,Our AWS resources are organized by tags for different projects. Looking for a secure way to share all resources tagged “ProjectName=Alpha” with another AWS account.,2,aws,2020-10-13
j3opqz,AWS Aurora - What issues have you had?,"We're considering moving from RDS Mysql to AWS Aurora. The promise is great - but in reality we've found that the devil is in the details with a lot of AWS services. 

Has anyone migrated from RDS Mysql to AWS Aurora? Are there any gotchas or things to watch out for?

If its relevant, our application is using Ruby on Rails v5.0.7 - soon to upgrade to v6 where we can leverage different database connections (ie utilizing read only nodes)",11,aws,2020-10-13
j3onn9,"Keeps getting “Error: A problem occurred while uploading to S3"" when deploying spring boot app to elastic beanstalk. What to do?",Please help,0,aws,2020-10-13
j3nue4,AWS SSO with Just-in-time Privileges,"Azure has a great concept called PIM or Privileged Identity Management

[https://docs.microsoft.com/en-us/azure/active-directory/privileged-identity-management/pim-configure](https://docs.microsoft.com/en-us/azure/active-directory/privileged-identity-management/pim-configure)

It allows specific users to elevate privileges, with customizable expiration, authorization (currently unfortuntely only email no mobile push), notifications etc.

It's a great way for admins to have only basic permissions for day-to-day admin, but consciously (with authorization, explicit reason, for limited duration) elevate privs when required.

I'm looking at moving our company's AWS account auth to using a single identity account (with AWS SSO &amp;. Azure as IdP) and was wondering if anyone had built similar? Just-in-time privesc.

In terms of the assumable roles - I guess you could update AssumeRole principles, or probably better use ABAC [https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction\_attribute-based-access-control.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html)

While we could try to leverage Azure PIM, it's limited to Azure Roles ... so you're need e.g. one Azure Role per Assumable AWS Role ... then based on that role membership, add SAML Assertion .... reauth ... kind of yuck.

Probably best would be to deploy a system in the Identity Account to manage it, dynamically add/remove tags on users' Principal roles (the ones they sign in to, from which they'll assume into others), and the target roles could add a Condition based on the principle's tags to the Assume role policy (and require the assuming principal be in the Identity Account(!))

I did find this excellent writeup on the area, by our previous Solutions Architect Louay (legend btw), it's related, but not 100% what I'm after [https://aws.amazon.com/blogs/security/attribute-based-access-control-ad-fs-simplify-iam-permissions-management/](https://aws.amazon.com/blogs/security/attribute-based-access-control-ad-fs-simplify-iam-permissions-management/)

(also now I notice this isn't necessarily about AWS SSO but the more general concept of ""just-in-time approved/audited privesc in AWS"")

Just spitballing ideas and thought it might be interesting to discuss here, cheers",13,aws,2020-10-13
j3n1fo,build website for high traffic first time (i'am rookie who want to be millionaire),"i'm rookie in this field and solo founder, my budget is approximately 5690 us dollar, i want to save my money as much as possible and i want to build website that have 2 functions are

1.putting some keyword to button and then press it for analyzing to show statistic,graph of specific information

2.sign-in/out that users have to put their email/hotmail and create password

so i was searching many information about website this is my tech stack but i have a little bit questions for all of you

could you give some suggestion for me

🌲frontend ; bootstrap

🌳backend ; node.js

🌵webtools ;

[1.google](https://1.google) cloud run + docker (for web hosting and i like billing in 100ms)

2.s3 (for cloud storage)

3.cloudfront for cdn (or should i use cloudflare ?)

4.gitlab (is it the best for ci/cd ?)

5.dynamoDB (for high traffic)

6.elasticsearch (for creating my function in website)

7.okta (for sign-in/out ,should i use ?)

8.redis run on docker on ec2 (should i use ?)

[9.google](https://9.google) analytics

10.mailchimp

should i prepared for high traffic initially ??

if i have any mistake for some valuable information or should i use.... for more efficiently and save my time to do unnecessary          please let me know       thanks for all of you guys",0,aws,2020-10-13
j3my4j,lambda vs ec2 (codedeploy),"Hello,

Here is my use case:

I have a nodejs app that uses

* postgres db
* redis (elasticache in aws)
* sagemaker to run a ML algorithm and can be accessed through a nodejs endpoint
* vuejs for frontend

Do you guys think using Lambda is sufficient for this use case? I am looking to maximize cost efficiency. Can anyone walk me through the implementation for each one of these items?

Thanks",2,aws,2020-10-13
j3muke,Staging cost in S3 - how to find the source,"Dear all,  
When we setup our offsite backups, Glacier was not yet ""S3"" so our backups were in ""regular"" S3.

Since Glacier has become a S3 type, our backup solution can now write directly in Glacier so I've change the storage policy on these buckets. As expected, our costs have gone down tremendously (Purple to Orange) .  
I tried at first everything through the console  Before learning that the console could only do it for smaller objects - so I completed the transition then ""the right way"" with a Life0Cycle rule. As you can see below all that was completed  on the 22nd of sept. Ever since, there is a bunch of cost that is ""stuck"" in staging (yellow). That is the exact same cost ever since - I cannot trace the source of this cost to get rid of it??

The corresponding buckets have a "" Clean up expired object delete markers "" rule as well .

Where should I look to identify what is creating this ""Staging"" cost ?

THX

  


    


https://preview.redd.it/jz0s0uf0elq51.png?width=2226&amp;format=png&amp;auto=webp&amp;s=6983a87afc9c8470b21920881a82e57db13488fb",1,aws,2020-10-13
j3mg1e,Can IAM permissions prevent read/write access to S3 objects after stored in glacier?,"I'm currently in a situation where certain objects in an S3 bucket need to be archived after 30 days.

After this point, I want them stored in Glacier (or equivalent long-term storage) + deleted from the bucket, and need my API access keys used to manage the S3 bucket (s3:GetObject, s3:PutObject) to have no permissions to view/modify the archived objects.

Is this possible with a single S3 bucket/IAM permissions + Glacier? Or what is a good solution to achieve this with AWS?

&amp;#x200B;

Thanks!",0,aws,2020-10-13
j3luvy,Can anyone tell me or send me documentation on how AWS has enough IPv4 addresses for all of the publicly routed instances?,"I'm assuming they have tens of thousands of instances running at all times(or more) which are publicly routed with IPv4 addresses. 

I understand that they recycle IPs when not in use, but it seems that they'd run out at some point at the rate they're growing.",7,aws,2020-10-13
j3kl1z,Upgrading environment platform help.,"Hello,  
  

I recently had to upgrade a beanstalk environment because the platform on my current one is deprecated.  
  

I checked out the AWS guide on how to integrate it to a new upgraded platform but I got stuck. Mind you I am completely new to AWS and most of the lingo goes over my head so if anyone can help through this problem in a ELI5 sort of way that would be greatly appreciate. Also, I even tried getting dev support and made a ticket in AWS but so far I've gotten no response from their support team.  
  

Anyways, onto the problem,  
  

I created a new environment with the updated platform and pointed it to the same instance as my current environment. However the new platform health condition says ""Severe"". The reasons it states are:  


&gt;""100.0 % of the requests are failing with HTTP 5xx.""     
&gt;  
&gt;""ELB processes are not healthy on all instances.""     
&gt;  
&gt;""ELB health is failing or not available for all instances.""  

And one instance on the environment has these issues:   


&gt;""100.0 % of the requests are failing with HTTP 5xx.""     
&gt;  
&gt;""Process default has been unhealthy for 12 days (Target.ResponseCodeMismatch).""

&amp;#x200B;

Now I tried googling and going through AWS resources for awhile, but like I said, I'm pretty tech illiterate when it comes to this so the resource solutions go over my head. Best that I could guess form stuff I googled is there's an error in the code for the instance, but I have no idea how to resolve that.

The site I'm trying  to host runs on wordpress if that makes any difference.",2,aws,2020-10-13
j3jz9s,"How to ""disconnect"" a database from a ElasticBeanstalk environment?","Hey folks, I'm migration an application from Python 3.4 (to be deprecated) to python 3.7, however, the application is connected to an RDS postgres instance and I believe that if I delete the old environment it will delete the database as well. Is there any way to delete the environment while keeping the DB?",2,aws,2020-10-13
j3i20m,Where to get more practical experience with AWS?,"I have taken the CSAA course from LA so now im looking for ways I can do more practical experience either from labs or guides. Are there any resources out there with more practical labs, etc.?",2,aws,2020-10-13
j3jmby,How do I set up reverse DNS on AWS EC2?,"I have an Elastic IP address set up, and a DNS record that works 1 way (name2ip). Now I just need help with a PTR record??? (ip2name) but it is not working, even if I copy the tutorials exactly! One tutorial claims I need **TWO Hosted Zones** and this seems **WRONG**! Has anyone ever set this up before and do you know what to do?

1. I have one hosted zone, and it's **NAME** is my domain.
2. It contains 4 Records: A, NS, SOA, and PTR.

I don't know how to create the PTR record so I brute-forced every combination that made sense and it never worked. Help!",1,aws,2020-10-13
j3hub3,How do you use RDS MySQL with AWS AppSync,"I have been trying to wrap my head around this for a while now. I have my MySQL instance in RDS and I have tried to to add it as a \`datasource\` with \`amplify api add-graphql-datasource\` however I get an error:  


    hutber@hutber:/var/www/unsal.co.uk$ amplify api add-graphql-datasource
    Using datasource: Aurora Serverless, provided by: awscloudformation
    ? Provide the region in which your cluster is located: eu-west-2
    No properly configured Aurora Serverless clusters found.
    TypeError: Cannot destructure property 'selectedClusterArn' of '(intermediate value)' as it is undefined.
        at serviceWalkthrough (/home/hutber/.nvm/versions/node/v13.8.0/lib/node_modules/@aws-amplify/cli/node_modules/amplify-category-api/src/provider-utils/awscloudformation/service-walkthroughs/appSync-rds-walkthrough.js:55:11)
        at processTicksAndRejections (internal/process/task_queues.js:97:5)
        at Object.executeAmplifyCommand (/home/hutber/.nvm/versions/node/v13.8.0/lib/node_modules/@aws-amplify/cli/node_modules/amplify-category-api/src/index.ts:188:3)
        at executePluginModuleCommand (/home/hutber/.nvm/versions/node/v13.8.0/lib/node_modules/@aws-amplify/cli/src/execution-manager.ts:161:3)
        at Object.executeCommand (/home/hutber/.nvm/versions/node/v13.8.0/lib/node_modules/@aws-amplify/cli/src/execution-manager.ts:26:5)
        at Object.run (/home/hutber/.nvm/versions/node/v13.8.0/lib/node_modules/@aws-amplify/cli/src/index.ts:86:5)
    

This makes me think, I am doing it wrong way?",2,aws,2020-10-13
j3hjvi,Using EC2 instance to run Moodle (open source LMS). How do I launch it so end-users (students) can access?,"Sorry, I know this is really basic, but I'm still lost. Moodle is an open-source learning management system (similar to Blackboard). My team has it installed on an EC2 instance running Windows Server 2019 (I think). We're optimizing Moodle for our particular needs, accessing it Microsoft Remote Desktop to make our changes. MRD is fine for developers, but I need to figure out how I'm supposed to make it accessible to my end users (students). 

I naively figured that I'd just have to open a couple ports to HTTPS and HTTP access from any IP address, and my end users/students would then be able to access Moodle. No such luck. Looking back I realize that was probably stupid. There's got to be some intermediate steps I'm missing here. But I don't know what they are. 

In short, when you have an application installed and running on a server, what do you have to do to launch it into the real world? I know this is a big question, so I don't expect a simple answer. Links to long articles, AWS documentation, tutorials, etc. would be helpful.",2,aws,2020-10-13
j3hd64,How to alert on port 3389 use via Cloudwatch events?,"Hi all,

Is there any way to query VPC flow logs via Cloudwatch events? I'd like it set up so that whenever port 3389 is used in our infrastructure an alert is sent out via SNS.",4,aws,2020-10-13
j3erzj,AWS Workdocs,"Regarding workdocs. We have reached a limitation we were unaware of We have a file that exceeds the limitation per  [https://workdocs.thinkfree.com/hc/en-us/articles/360003070954-File-and-Data-Limitations](https://workdocs.thinkfree.com/hc/en-us/articles/360003070954-File-and-Data-Limitations) / 

Does anyone have a workaround they know of?",1,aws,2020-10-13
j3gync,IAM having timeout issues?,"Errors from Terraform complaining about the connection being reset, and getting this from the CLI: 

    Connection was closed before we received a valid response from endpoint URL: ""https://iam.amazonaws.com/"".

Console shows this error:
&gt; Http request timed out enforced after 999ms

Not happening with all my accounts, strangely enough.

EDIT: Just resolved?",31,aws,2020-10-13
j3gkyz,This is my first time trying to set up domains/DNS and I am stuck.,"I bought a domain from GoDaddy, [here is the zone file I can edit](https://i.imgur.com/lMOt1bm.png). I was under the impression when I own a domain I can tell it what IP address it represents but apparently not, and now I am stuck. Assume I own mydomain.com and the IPv4 address is 111.222.333.444:

1. What do I type on that screen (see picture).
2. I am having trouble finding any link to any tutorial on what to do after step 1. The first 100,000 search results are not relevant to what I want to do, and I lack the ability to know what to search for.

**I want to do something extremely simple**, when I type ""nslookup mydomain.com"" in any computer console I want it to list the IP address of my currently running EC2 instance.

** **
** **
** **

**EDIT**: I just found this tutorial right now, it seems like it'll work. I'll post back here in a bit if it does for anyone else looking for similar help in the future:

https://medium.com/progress-on-ios-development/connecting-an-ec2-instance-with-a-godaddy-domain-e74ff190c233",1,aws,2020-10-13
j3ct3s,[super noob question] How do I actually build the AWS tutorial code?,"I’m working on the Amazon Kinesis tutorial and there’s a repo with some Java code in it 

https://github.com/aws-samples/amazon-kinesis-learning

Traditionally I’d just do “javac MainFile.java” with java projects, but I don’t see an obvious main file for this project. How would I compile and run this?",1,aws,2020-10-13
j3cokd,tag policy,"how to config Every resource must have a ""**Project**"" tag when creating a new resource?

apply for all users",2,aws,2020-10-13
j39lhq,Billing Third Party AWS WAF Managed Rules,"Hello,

I'm having trouble finding the information.

I'm using AWS WAF and I understand the billing around AWS WAF itself. But, you can get other rule packages from 3d party vendors to use in your Web ACL. This vendor charge you for using the ruleset that he is providing for you. And the price will depend on the package, each vendor sets his price.

This is an example:

 F5 Rules for AWS WAF - Common Vulnerabilities &amp; Exposures (CVE) Rules:

Charge per month in each available region (pro-rated by the hour $20 / unit

Charge per million requests in each available region$1.2 / unit 

To use tha rules, you will have o subscribe for the product, once subscribed you will have it as 'SubscribedRuleGroup', then you can use this group of rules in a WebACL.

My question is: when exactly you are charged for the above prices? is it when you subscribe for the product, or when associated with a WebACL?

Cheers,",1,aws,2020-10-13
j38ppb,Error when trying to create a new record within hosted zone,"I'm trying to follow the steps in this [article](https://dev.to/arswaw/create-a-subdomain-in-amazon-route53-in-2-minutes-3hf0) to create a subdomain for my parent domain. After trying to add the namespace values for my subdomain to the parent I get the error below.

    (InvalidChangeBatch 400: ARRDATAIllegalIPv4Address (Value is not a valid IPv4 address)
    

I'm new to AWS and my goal is to have two separate servers, one for Jupyter Lab and another for my web server without having to register two separate domain names.",1,aws,2020-10-13
j3cj33,What is the best way to automatically and consistently convert CSV files to a database.,"I’m trying to set up a system where I can drop a CSV file into, say, a directory. Then have that CSV automatically read at uploaded to a DB. All within AWS.  The CSV files will all follow the same structure and will convert to the dB in the same way. 
I’m planning to us RDS MySQL or Aurora, but I have flexibility. 
Thanks!",3,aws,2020-10-13
j3chto,Athena vs EMR with Presto,"What are the differences between using these 2 services for queries on data in S3?

I know both use presto as the engine, why would one choose one over the other?",10,aws,2020-10-13
j3c62a,How to disable private key (pem file) and create a new one,"Hi,
I just finished work with some contractors that helped set up my app and instance. They have the pem key and I feel like that means they can access my instance at any time they want. I will like to remove their access to both the instance and my RDS instance. Are there any steps I can take to create a new key and disable the old one?

Also how do I change the password to the RDS? I tried to change it but my app stopped working. I assume the instance needs the password to connect to RDS? How to I change that and ensure they still connect and the app still works?

Thanks for the help.",0,aws,2020-10-13
j3c4j8,Which AWS service am I looking for?,"Some of my project’s code is hosted on AWS, so I know we have some AWS support. I’m working on a Martech platform. Due to constraints of that platform, I need to utilize an external API action in a workflow where I send some data in the body, then another service receives that call, takes the body param, and uses it to implement a different API call back to my my martech platform. 

Is there some sort of middleware/micro service available in AWS or would I just need an EC2 instance?",1,aws,2020-10-13
j3blej,"We're building 7777, a tool to simplify connecting to RDS databases in VPC, feedback is welcome",,53,aws,2020-10-13
j3aijs,SES authentication SMTP authentication failed,"Hello,

I'm trying to test an SES configuration with SMTP.  I've followed the AWS docs to retrieve my SMTP credentials, I've verified a domain and email address.  I also am following the directions to test using openssl, including converting the username and password to base64.  When I convert the password it looks a little weird, the password is split out on two lines, but I've been removing the space and just combining it into a single password.

However when following the directions to test using openssl, I can connect to SES but I keep getting authentication failed.  I also followed a forum where someone said there was another step to add an identity policy to the validated domain, but that didn't work either.  Still just says authentication failed.  Created another IAM User/SMTP credentials, same thing.

Anyone get this working successfully?  What am I missing?",1,aws,2020-10-13
j38jw9,Looking for AWS-based Architecture Solution/Suggestions For Third-Party Docker Container + self-written program,"Hey Guys,

I have a piece of software I wrote in Java and it connects via TCP/IP socket to a third-party program for specific data streaming. The third-party program runs in a Docker container and I can successfully run it on Windows with my program connecting to it on a specific port.

My question is this: I'd like to deploy this Docker container and my application out on a hosted environment.  What would you guys recommend my architecture set up be if I decide to go the AWS route?  Would Fargate be beneficial in my case?  Both programs are not resource-intensive, mostly moderately network usage as the Docker program is a datafeed for analyzing data which is what my program does.

I'm also open to other solutions or providers (ie Vultr) if they provide a cheaper alternative to my simple setup.

Would love to hear what you guys would recommend if I go the AWS route though.  Thanks so much.",2,aws,2020-10-13
j396da,Help a noob out... what tool/product do you think will become way more relevant in the future?,"The main examples I can think of are snowflake and spark. It's very clear to me that it is worth it to spend lots of time learning these things because they will be a bigger deal in the future.

What else should I spend time learning? What should noobs align themselves with? What will be a huge deal in the future?",0,aws,2020-10-13
j38qda,RDS / VPC Setup Review,"I have taken over an RDS setup from a DevOps guy who left me in a bit of a bind. Reviewing his setup, I'm not comfortable with some of the way it's setup. Most importantly though, some of the RDS instances have extra Security Groups.

1. default (sg-9bdXXXXX) - I believe this one is safe. It shows inbound traffic is allowed on all ports from the security group that is on all our EC2 instances (so if RDS and EC2 are both on this security group, it will work by default?)
2. I have some RDS-Launch-Wizard security groups on one of our databases, but it appears to be wide open (Mysql/Aurora - TCP - 3306 - [0.0.0.0/0](https://0.0.0.0/0) )

I am looking to remove all public access, but my only fear is that #1 above won't be sufficient and products will lose database access. I've been working in AWS for years and am fairly comfortable, these security groups are just the one area I've never worked with because I typically ran my own EC2 database servers and just setup restrictions to access on the box itself, not using Amazon tools.

I guess what I'm trying to confirm is that if an EC2 container and an RDS instance are both attached to the same security group, will they by default be able to communicate? We also only have one VPC which all resources are part of.

Thanks in advance. I promise I've RTFM, but when working with this existing setup I'm a bit nervous.",13,aws,2020-10-13
j3860g,AWS Web Scraper architecture advice,"  I have an EC2 instance with a Web Server which reads from a local MongoDB. I'd like the data in the database to be updated on a weekly basis by performing web scraping (100k+ rows). For that I've set up 2 SQS queues, Lambda functions for doing scraping in parallel, and 3 scripts: one is reading from the DB and inserting data in SQS, another one is checking for the job to be completed and the third one is reading from SQS and updating the database. Since the database is located on the same instance and not externally available I had to put all 3 scripts on the instance as well, but in order to separate them somehow I ran them inside docker containers. This is the solution I ended up with, but I think it's far from perfect, and would like to hear any advice on how this can be improved. Thanks. 

&amp;#x200B;

https://preview.redd.it/wkejeswoehq51.png?width=1800&amp;format=png&amp;auto=webp&amp;s=6e9c4ebb1518c727fa9aad7f271c9799765afd62",2,aws,2020-10-13
j37ujd,How can I mass audit user access for an environment using federated on premise Active Directory?,"I would like to be able to show auditors a report of all of my AD users that have access into AWS, what they have done and when. 

Cloudtrail is on but how can I get everyone's access at once?",1,aws,2020-10-13
j37hsj,Managing long running scripts,"I have a series of node scripts which listen to different sources (websockets or long polling mainly) for the purpose of data collection. The actual script is modified slightly depending on the customer it is being run for. The script is basically a while(true) which runs for 6 to 72 hours. The scripts have grown and gotten pretty convoluted over time, rewriting them isn't on the table at the moment. 

Currently, we are just ssh'ing into an ec2 instance and running them manually. This is quickly becoming something we can no long handle manually. We've also had issues with our ec2's underlying hardware being degraded. 

We'd like to have a web interface where we can ""launch"" these scripts. I have no concerns for the web interface, but I do have some about how to manage running these scripts. 

*It seems like ec2 or ecs are the only real options for running this sort of workload. Am I missing any others?* I'd like to avoid managing ec2 instances as much as possible because of the previous degradation issues, but *I wonder if the same sort of degredation can happen with fargate?* If it can (or even if ecs isn't susptable), *what is the recommended way or pattern for running a process in the cloud which needs to listen for incoming data when you don't want to miss any data?*

Because these scripts are tweaked on a regular basis, we'd like to store them on S3 so we can replace them easily.  Based on that, *how can we automate taking a script from S3 and running it somewhere*? If we go the ecs approach, I guess we'd have to have our web interface create a docker container, deploy it to ecr, deploy that to ecs? I am not familiar with ecs, is that approach recommended, is there a more standard approach? And if there is hardware degradation, what are the recommendations for restarting a process like this?",1,aws,2020-10-13
j3630o,How do you manage your Cloud Credentials?,"It's something that I and my teammates struggled a lot with.  


Today I launched my open-source tool to help us managing credentials in a secure way.  


How do you manage them? It's the AssumeRole and AssumeRoleWithSAML the better method?

[https://github.com/Noovolari/leapp](https://github.com/Noovolari/leapp)  


To me save credentials from an AWS user to the \~/.aws file is insecure and I preferred to store them in the Keychain. It is the right way?",2,aws,2020-10-13
j34hpt,My EBS volume is frozen and when I attach it to an instance the instance is frozen. How can I unfreeze the EBS volume?,,1,aws,2020-10-13
j33oey,Anyone have any luck with the certification talent community?,AWS offers to add you to their talent communities when you get a certification with them. I was wondering if anyone had a positive experience with them worth mentioning.,0,aws,2020-10-13
j32gpz,VPC endpoints vs NAT gateway.,"I have a private VPC setup, and I've been trying to figure out the benefits of walling it off completely from the internet and using VPC endpoints or just using a NAT gateway and getting to my services that way.

I'm ignoring the two AWS gateways (S3 and DynamoDB) as I believe those are free (correct me if I'm wrong?) and focusing on the interfaces through AWS PrivateLink. I'm also only focusing on AWS services at the moment.

So if I use a NAT gateway on 2 availability zones, and say 1GB a month each, the calculator comes in at 65.78 USD a month (N Virginia). If I use 4 VPC endpoints (SQS, Secrets, Logs, EC2) in two availability zones as well, it comes to 58.48 USD, but I'll get charged for data transfer across AZ as well, which isn't included in that calculation. Not a huge difference. It doesn't change a whole lot with data increase.

So the question is: Is it worth the trouble of all those VPC endpoints, which could increase if I start to use more AWS services, or should I just stick with a NAT gateway?

I can understand the security implications, having a completely private network would be great, but outside of that?",3,aws,2020-10-13
j34f1v,EventBridge isn't pushing events to cloudwatch logs using EventBridgePutEventsPolicy,"EDIT: re writing the question as I don't think I was asking it right.

Manually creating an event bus and an event rule, then selecting target as cloudwatch logs (creating during the process) makes events show up in cloudwatch.

When I am deploying the same resources using IaC then resources create but events don't make it to cloudwatch logs. If I keep the same event bus &amp; event rule &amp; edit the rule to create &amp; point to a another cloudwatch logs then it works.

My workflow looks like this

event  --&gt; lambda  --&gt; cloudwatch

If I create the cloudwatch log group using console while I am creating the rule then events flow to the cloudwatch logs when lambda is fired but if I deploy the code to create the resources &amp; specify target as cloudwatch logs then it doesnt work.

Has anyone encountered this before?

EDIT: For anyone having same issue, read [https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap/issues/351](https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap/issues/351)

&amp;#x200B;

&amp;#x200B;",2,aws,2020-10-13
j32n0l,Getting Information from ECS Events,,1,aws,2020-10-13
j32am0,"Why in CloudFront, requests to S3 origins will skip regional edge cache locations","Regional edge cache looks like a pretty good solution, why it ignores requests to S3 origins?",1,aws,2020-10-13
j3006q,Route53 DNS AWS Organizations with child accounts,"Historic context: AWS Root account - all dev, QA and prod are in the same vpc. doman name registrations are [dev.](https://dev.companyname.io)&lt;companyname&gt;[.](https://dev.companyname.io)com, [qa.](https://qa.companyname.io)&lt;companyname&gt;[.](https://qa.companyname.io)com, and &lt;companyname&gt;.com. Now with new architecture, we have AWS Organizations and AWS Account for Dev and one for Production environment. Now the question I have is, do I need to setup the hosted zones in the parent (root) account and give child's load balancer account? Do we see any latency in that case? or each child account should have its Route 53 configuration? If so, how do we handle the certificate? as we have one certificate for \*.companyname.com",3,aws,2020-10-13
j30fkt,Code Deploy vs Lambda vs Elastic Beanstalk and Sagemaker,"Hello,

For the past few days, I have been researching these 3 AWS platforms and I am still quite confused as to what the distinctions are. 

My use case:

I have a Nodejs app that currently uses a Python Flask server to send and receive data. I would like to host this app on AWS. I don't know if I should continue using this Flask server with AWS or use Sagemaker.

So my two questions are 

a) Should I use Sagemaker or the Flask server?

b) What deployment platforms do I need to maximize cost efficiency while maintaining solid performance?

Thanks",1,aws,2020-10-13
j2zzib,Re:Invent 2020...what products/updates are you hoping for?,For me: Aurora MySQL 8.0,2,aws,2020-10-13
j2zgqh,Suggestions for a better reading experience,"Hello Everyone and Mods,

Happy to see that r/aws is growing every day and the amount of posts we are getting is very high. Sometimes, people wanted to help on a specific topic(for eg: my skills are databases and BigData), if we want to help someone who posted questions about RDS, RedShift, EMR, Lambda, its seems a bit difficult on the front page, We have to scroll down many questions to find the question which is relevant to me.

**Question for Mods** \- Its like a suggestion(or a feature request  **😄** ) to have the dedicated subreddit for most commonly uses services like ec2, rds, eks, vpc, analytics and etc.(I know we have flairs already)  So it will be easy to find more relevant questions and we can answer to more questions.",3,aws,2020-10-13
j2zfm3,How to force redirect to HTTPS with Application Load Balancer?,"I have a very basic Python Flask app in Elastic Beanstalk and I would like to redirect or force HTTPs but it does not seem to work when I follow the documentation. I have [followed this guide](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https-elb.html) with no luck.  
I added the rules in ELB, I added the secure listener file in .ebextensions, but users can still visit the HTTP version of the site. Does anyone have any ideas? I thought this was pretty straightforward.",2,aws,2020-10-13
j2yz2v,Amazon Timestream is now Generally Available,,18,aws,2020-10-13
j2ycqx,AWS User data,"I feel like a noob asking this.  But I'm trying to use EC2 to pre-install a few packages on AWS EC2 RHEL server.  Does this not work for RHEL?

    #!/bin/bash
    yum -y update &amp;&amp; yum -y upgrade
    yum install -y pcre-devel expat-devel
    groupadd dev
    useradd devuser
    usermod -aG dev devuser",1,aws,2020-10-13
j2y0gu,How do i protect access to EC2 instance with a password?,I'm hosting my website on Vercel. They use dynamic IP addresses so I can't protect my server domain based on IP. How do my server route with a password?,0,aws,2020-10-13
j2uikn,Authorization in Serverless Applications (IAM and Cognito),"I am building a serverless web application on AWS and could use some insight around best practices with IAM policies throughout my application.

My app has a fairly typical stack: Cognito for Authentication, API Gateway to expose my API, Lambdas to execute my business logic, DynamoDB for my data store and S3 for file uploads.  I am building this with the help of the Serverless Framework.  

I'm having trouble wrapping my head around how to use IAM policies throughout my application.

So far, I have the following:

* Cognito (User Pools and Identity Pools) to manage users and provide IAM roles to guest users and authenticated users.  Currently, the IAM policies associated with these roles only deal with S3 uploads from the client.
* JWT Authorizers protect my API endpoints (Cognito provides the tokens), which allows me to control which API endpoints are accessible to different user roles (guests vs logged in users).
* API Gateway proxies to Lambdas, which have their own IAM roles that give them access to my other AWS resources (e.g DynamoDB).

&amp;#x200B;

Using this setup, it appears that Lambdas don't have any idea of their execution context (e.g. which user is on the other end of the API call and what they are authorized to do).  To be clear, I *do* have access to *who* made the call from within the lambda, but I'm not clear about how to use that information to get/enforce IAM policies.  This seems problematic if I ever want to have finer grained control over authorization within my Lambda. 

For example, I may want to allow ALL authenticated users to issue a POST request to the /profile endpoint, which will ultimately updates a users profile in DynamoDB.  However, I want to make users can only update *their* profile (a specific key in DynamoDB).    


I hope this question is clear.  I'm not sure if how I'm using IAM is appropriate, or if there's something I'm missing about making sure a users role follows the request all the way through my stack.",2,aws,2020-10-13
j2x9n0,Can CloudFront be counterproductive to performance for low-traffic websites?,"I have several static websites with very low traffic (each generally having no more than 10 users/day, from no country in particular) in an EC2 instance, and wanted to migrate them to S3 for improved maintainability/performance/savings/availability/etc. Since S3 website hosting doesn't support HTTPS and CloudFront seems to be the suggested solution, I tried it and it works like a charm, but I'm concerned about cache misses with this CDN approach.

I noticed each CF cache appears to evict files if they haven't been requested in \~24h even if Cache-Control time is longer. Considering the nature of the traffic I'm working with, it is likely that for half of the visitors their nearest CF cache will be cold, and so CF will harm performance instead of helping. Is there a better approach to do this with CF? Or perhaps a better suited service? I wasn't really looking for a CDN, but S3's lack of custom-domain HTTPS was a deal-breaker and people seemed to unanimously recommend CF as the solution.",3,aws,2020-10-13
j2wz4x,Store and Access Time Series Data at Any Scale with Amazon Timestream – Now Generally Available,,88,aws,2020-10-13
j2vto0,How do you get visibility over cloud workloads?,"I oversee our cloud infrastructure and we have hundreds of EC2 instances, S3 buckets, other cloud resources. How do you know the *purpose* or the *business reason* for the cloud resource? Tagging is the only way I know how, but tags can be inconsistent and unreliable.  It's pretty much all a black box for me, but I want better visibility into the box.  For example, which are SFTP servers, which are pipelines, which are web servers, etc.  Also, even with cost explorer I'm not able to pinpoint where any cost increases are coming from - it only gives me the AWS service which is not helpful if I'm running hundreds of EC2 and the workload is ephemeral. Does anyone else have this problem? What do you use? Do I just need to shore up my tags?",7,aws,2020-10-13
j2vkja,Amazon S3 on Outposts Now Available,,89,aws,2020-10-13
j2uwu5,Looking for a way to export failing CIS benchmarks within Security Hub,"As the title states, I'm looking for a way to export passing, failing and other benchmarks that are a part of the ""CIS AWS Foundations Benchmark v1.2.0"" within AWS Security Hub.

I thought Security Hub&gt;Findings would allow me to export to CSV (or other) through the ""Actions"" menu, but there is no export option there.

Has anyone dealt with this issue before? I'm trying to find a way to go about this through the console and to avoid writing a custom script. Seems like a great compliance feature that AWS should support.",2,aws,2020-10-13
j2und0,Multi-Layer Users,I want to make a user system where there is one company with multiple users underneath on a mobile app with react native. Is there any easy way to do this? There would be one Admin account with multiple accounts under it. The admin account can then track their employees and manage them.  Is there any easy way to do this?,0,aws,2020-10-13
j2ugxv,Input transformer modeler?,"Is there a modeler I can use to take a sample of event data and put it through against input transforms, so I can see how it looks?

Otherwise I just have to keep generating events for every change.",1,aws,2020-10-13
j2ug8d,How to save AWS commands as a file and run as a script?,"I want to automate copying files from a folder on an on premises physical Windows server to an S3 bucket.

I can create an IAM user account and assign permissions to the S3 bucket and then type the copy commands into AWL CLI to copy the files, but how do you save these commands as a script file so it can be added as a scheduled task to run with no user input?

When you do it manually, you have to respond to prompts asking for AWS access key ID, secret key, region etc. and then enter the copy commands.  How do you put everything into a file in a format that works?

Is it saved as a .bat file or is there some AWS CLI specific extension?

&amp;#x200B;

This link has the title of ""How to script the backup of files to S3."" [https://aws.amazon.com/getting-started/hands-on/backup-to-s3-cli/](https://aws.amazon.com/getting-started/hands-on/backup-to-s3-cli/)

However, it only shows you how to enter commands interactively.  I see nothing on how to make this a script.",0,aws,2020-10-13
j2tmds,Reserved Instances vs Cost Savings Plans for EC2,"So I've been using Reserved Instances for my clients for the past couple of years. No biggie there. 


I've got a new client and I'll be doing the same type of projects I always do, but this time Cost Savings Plan exist, so I'm at the point where I don't know what to choose.  Some facts:


1) I'll need six t3 instances that we will be using long term (3+ years).


2) EC2 in same, non-changing region, nothing special there.


3) Whatever I chose, RI or CSP, I'll need to be able to upgrade my agreement if I need to upgrade my instances. On RIs I used convertible RIs, I don't know if CSPs support that. I'll still be using the same family of instances (t3) but I made need to upgrade from medium to large or to xlarge or 2xlarge, etc. 


So based on my use case, what's the best option for me?  Thanks Reddit!",5,aws,2020-10-13
j2tqt4,AWS Site to Site VPN and NAT'ing,"It seems that NAT'ing traffic over a site-to-site VPN in AWS is not a supported feature (really wish it was..), with that in mind how is everyone handling their VPN connections?  


We have dozens of VPNs to clients where we only control our side (AWS) of the network and therefore run into overlapping subnets pretty consistently, in the on-prem world we'd just have to set whatever network appliance (ASA, PA, f5, etc) to NAT the VPN traffic before transmitting it, with site-to-site that is not possible.  


Is our only option to spin up an ec2 and host our own VPN appliance?",3,aws,2020-10-13
j2pc0v,Using DMS across GovCloud and Commercial,"I have to move data from RDS in GovCloud to S3 in Commercial. Will DMS (running in Commercial) be able to reach the RDS instance in GovCloud? No Direct Connect is in place.

Edit: typo",2,aws,2020-10-13
j2lmq7,Have an API gateway with JWT Authentication in front of two microservices. I am wondering if I would put a load balancer in front of the API gateway or the Microservice instances.,"Relatively new to AWS, at least building this type of architecture. An ALB would go in front of my instances, but would the NLB go in front of my API gateway? Also, I built the API gateway with Amazons service to save time, but I've seen you can do this with NGINX, does anyone recommend this?

&amp;#x200B;

Edit: appreciate. Saved me from being stupid",4,aws,2020-10-13
j2oiud,Any advise on where to get help with DynamoDB Data Modeling?,"I'm newish to DynamoDB and looking to use it in a new application that I'm building. I've watched all the videos and read all the blog posts but I'm still having difficulty designing what the attributes and indexes should be for my table (assuming a single table design). I have the following models and access patterns. 

**Models**

**Forwarding Number -** The phone number that a call will be forwarded to. 

**Incoming Number -** This represents a purchased number that a user can call and will trigger the application logic. 

**Campaign -** Refers to the resource that brings both numbers together. A campaign has a single incoming number associated with it and a single forwarding number and single incoming number. 

&amp;#x200B;

**Access Patterns**

\- List Each Campaign (w/ corresponding Incoming Number &amp; Forwarding Number)

\- List all Forwarding Numbers

\- List All Incoming Numbers and whether they are assigned to a campaign already

\- Given an Incoming Number, get Campaign and Forwarding Number 

&amp;#x200B;

Would love any advice on how to structure my table, or any places/services I can go to get advice about how to structure this table to take full advantage of DynamoDb's functionality and scalability.",1,aws,2020-10-13
j2rs2y,Possible to request spot instance but fallback to on-demand if none are available?,"I'm new to AWS. I want one instance which will just run a web api and a discord bot, for my own use, so nothing at all critical here. For this reason, I think a spot instance would make the most sense.

In particular, it seems like the ideal situation would be to:

* request a single spot instance
* keep/maintain a single instance (whether spot or on-demand)
* with a price-cap being the on-demand price
* if no spot instance is available for less than the on-demand price (can spot prices even exceed on-demand prices?), fallback to an on-demand instance so that I'm not left without any instances

Is this doable in a straightforward 'official' manner? Or does anyone happen to have a better idea? Or what gets me closest to this?

I'm mainly worried about being left without any instances, so in such a case I would rather just fallback to an on-demand one.",2,aws,2020-10-13
j2sbzq,Advice on which AWS resources I need to use,"I am incredibly new to AWS. So far I have been running a few Lightsail instances for some small Wordpress sites.  


I now need to build a small site that needs to be able to handle 2000 concurrent visitors all watching a livestream video for 60 minutes.

I understand that EC2 is a better option than Lightsail. But I really do no know where to begin in selecting instance features that will support 2000 concurrent visitors all watching the same hour-long livestream.

Also, what else can I do to improve the performance of the livestream or not get hammered with huge bandwidth costs? Is a CDN applicable to something like a livestream in order to reduce the origin server bandwidth?

  
I'm very nervous about what I do not know at this point. I've done three similar 60-90 livestreams from the Lightsail sites I'm hosting but they were only for about 110 visitors max. And that maxed out the CPU of the Lightsail instance that is close to the top one that is available.

Any advice you can share on how to estimate the EC2 instance size I might need and ways to reduce the overhead of the livestream on the instance would be greatly appreciated.

The livestream will actually be pre-recorded and streamed ""live"" at the prescribed time using something like Vimeo, [Restream.io](https://Restream.io) or [onestream.live](https://onestream.live).  This would be a WordPress site most likely running on Ubuntu for the instance.

&amp;#x200B;

Thank you!",2,aws,2020-10-13
j2skyt,S3 Bucket Invalid Canonical ID,"Hey All,

New to AWS. I created a bucket, the root account already has access but when I try to add my account to give permission to access the bucket, it keeps telling me my canonical ID is invalid when im adding the correct one.",1,aws,2020-10-13
j2n8e9,"Hi community , we have been struggling with an problem for a couple of years. We are trying access to 192.168.241.2 ( pritunl client ) from 10.174.1.200 . Can anyone help with the solution to this problem ? Our vpn server is Pritunl installed on 10.182.6.10. Thank you",,0,aws,2020-10-13
j2n7sd,How to find the exact cost of all the services which includes data transfer and others utilized by a respective CloudFormation stack?,"Hi, 

I tried using cost allocation tags of each CloudFormation stack but I am able to get the cost of only explicitly defined resources in CloudFormation template..

But what I want to know the exact cost of data transfer, API calls made by the particular CloudFormation stack..

Is there any work-around or a proper way to do that in AWS?

Thanks in advance!",3,aws,2020-10-13
j2qqot,Manual SSO Migration,I've recently been looking into enabling SSO for my org but it seems like I'd have to recreate every IAM role and user. Is that right?? That seems crazy tedious.,1,aws,2020-10-13
j2miph,Beanstalk support for m5a and m6g instance types,"We use Elastic Beanstalk for our workloads basically PHP Servers. Currently we use M4.large instance type. Recently we have been thinking about moving these workloads onto either M5a or M6g instance type for cost and performance benefits and we have decided to proceed with testing our workloads in these instance types. But unfortunately we found that there is no support for both these instance types in Elastic Beanstalk in Mumbai Region. 

can anyone confirm whether we have support for any of these instance types in Beanstalk for Mumbai Region ??

If NO, then any idea about when to expect support for these instance types in Mumbai?

Thanks in Advance :)",2,aws,2020-10-13
j2ogvq,Connect AWS Glue to SQL Server," Hello!

For a project, I have to move data from SQL Server to AWS Redshift.

For the ETL process, I'd like to use AWS glue and connect to my source (SQL Server), but I always get the following error message when testing the connection

&gt;Check that your connection definition references your JDBC database with correct URL syntax, username, and password. The TCP/IP connection to the host, port 1433 has failed. Error: ""Connection timed out: no further information. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall

Port are correctly opened on my server and did sever test with other tools (PowerBI and SQL Workbench can connect to it)

Here is the connection string used :

    jdbc:sqlserver://host:1433;databaseName=AdventureWorksDW2012 

I'm not sure what's the issue at this point

if anyone has an explanation, thanks for sharing

Thanks!",1,aws,2020-10-13
j2nz75,Got an email with subject: [Action Requested] S3 Bucket Security,"&gt;\&gt;Your security is important to us. Your AWS account \[OMIT\] has one or more S3 buckets (list follows this paragraph) that allow write access to data from any user on the Internet, and we strongly recommend you change this configuration.  
&gt;  
&gt;\&gt;S3 buckets are private by default, however, you (or someone in your organization) have overridden the default private settings and configured this bucket with an Access Control List (ACL) that permits public write access. This means that anyone with Internet access can write, over-write, or delete objects in your bucket. To prevent public access or writes to your bucket, we recommend changing your ACL to remove public access, and enabling Block Public Access (BPA) to override current and future ACLs permitting public access to S3 buckets and objects.  
&gt;  
&gt;\&gt;BPA is a free feature and only takes a minute to enable. For step-by-step instructions on setting up S3 Block Public Access via the S3 management console, see Jeff Barr's blog \[1\], or the S3 documentation \[2\] \[3\].  
&gt;  
&gt;\&gt;Your affected S3 buckets are the following:  
&gt;  
&gt;\&gt;\[OMIT\]892374jkhkl.execute-api.us-east-1.amazonaws.com

&amp;#x200B;

My question: Being an AWS noob. I have like 20 buckets for little websites I put together. Most are public because they are public websites, but they shouldn't be public write (only read). Is there any easy way to tell which bucket matches up to the affected URL that they sent me?",13,aws,2020-10-13
j2m973,Can I use API gateway to manage connections between RestAPI server and websocket server?,,1,aws,2020-10-13
j2n2oo,Use s3 to host multiple static website,"I have a s3 bucket called `itsappdev-common` I want to host multiple static websites in folders in this bucket. Each of the folder would contain their own website with their own index.html and error.html. How can I achieve this with single s3 bucket? I know this can be accomplished using multiple s3 buckets.

***Solution:*** ***If you add a trailing slash at the end it will reference the index.html of the folder it is in. For e.g.*** ***http://******&lt;bucket-name&gt;.s3-website-us-east-1.amazonaws.com/&lt;folder-name&gt;/***",7,aws,2020-10-13
j2llwh,Step Functions - ECS/Fargate integration,"Fargate has the ability to scale based on alarms, often times based on the load volume in it's SQS (I guess?).

What would you all use to scale the number of Fargate tasks when orchestrating the data input to the Fargate task with Step Functions?

EDIT: Revised question to specifically ask about scaling out the number of Fargate tasks.",2,aws,2020-10-13
j2kspp,"Using Lambda to perform get requests, but only getting part of the response, solution?","Hello there strangers, im having an issue with my Lambda function where I use it to run multiple GET requests, but one of the requests are only returning part(roughly half) of its response, which is a problem as the field i need is allways the part of the response missing //sigh... As the exact code used locally does not experience this issue, im thinking the problem is with Lambda and its limitations, but its not clear to me what exactly goes wrong and if i can just change it with a configuration change or maybe some code compression. Anyone who has a tip on how to fix this?  


The python code:

from urllib.request import Request, urlopen   

def some\_method():

   url = ""some url here""

req = Request(url )

req.add\_header('authorization', mykey) 

content = urlopen(req).read()

print(content) 

print(""Bytesize of response: "" + str(len(content))) 

some\_method()  


The response is allways cutting off at the same position in the JSON response, and the lenght of the response is allways the same.",1,aws,2020-10-13
j2kxp9,Extracting email template variables. Useful for SES.,"So I thought I'd share the regex I use to pull email template variables so that I can test SES emails.

`{{[\s]*[#/]?[a-zA-Z][a-zA-Z0-9\.]*[\s]*[a-zA-Z0-9\.]*[\s]*}}`

This regex isn't perfect like I've explained [here](https://zeer0.com/b/extracting-variables-from-email-templates.html) but it does get all the  handlebar style strings out. Hope it helps someone googling this.",2,aws,2020-10-13
j2iwpk,RDS Performance,"Hello guys,   


what's the best way to run a load test on my RDS database to analyse performance?",1,aws,2020-10-13
j2j590,aws-cost-saver - A tiny CLI tool to help save costs in development environments when you're asleep and don't need them!,,19,aws,2020-10-13
j2fvfs,Offensive Terraform Modules - Automated multi step offensive attack modules with Infrastructure as Code(IAC),"[\#AWS #CloudSecurity](https://offensive-terraform.github.io/)   
[https://offensive-terraform.github.io/](https://offensive-terraform.github.io/)

[Offensive Terraform Modules](https://preview.redd.it/7orm7zrdv7q51.png?width=1720&amp;format=png&amp;auto=webp&amp;s=684dda9295ba4e237449b427cef8973bb48247e0)

* **Cross Account Persistence**
* **EC2 Instance Credential Exfiltration**
* **S3 Subdomain Takeover**
* **EBS Snapshot Publicly Exposed**
* **Lambda Function Credential Exfiltration**
* [More](https://offensive-terraform.github.io/)",2,aws,2020-10-13
j2fqqq,Should't I wait for ECS container instance to be registered when I launch a new instance in ASGs?,"I found articles on draining instances automatically when terminating instances, but what about launching?
Wouldn't it be better to wait for container instances registration as well?
I couldn't find any example on this.

 https://github.com/aws-samples/ecs-cid-sample

This example only have lifecycle hook for draining.

I've never experienced cases where instance registration fails, though in theory the instance should not be considered healthy if it's not shown in container instances list.

EDIT: After searching with different keywords, I've found this. https://github.com/awslabs/ecs-cluster-manager

The python script seems to have the ability to check if new ECS container instance is healthy.",2,aws,2020-10-13
j2euog,AWS CloudFormation now supports StackSets Resource Type in the CloudFormation Registry,,55,aws,2020-10-13
j2e2ak,codedeploy vs docker,"Hello,

I am currently developing a nodejs app with some ML that I plan to put on sagemaker. I was planning to host the app on aws. I was wondering what the best strategy for deployment was.

I do not have much capital so cost-efficiency is a priority. I am trying to decide between codedeploy and docker. 

Can someone explain me the pros and cons of choosing either service and how I would be able to use it?

Thanks",3,aws,2020-10-13
j23vbl,AVC Website - all the official Amazon videos related to each individual AWS Service,,2,aws,2020-10-13
j2b39n,Where can I locate older Win10 Workspace Client?,"Hey gang.. I looked.. I looked everywhere.. And I cannot find the 3.0.8 or 3.0.9 Win10 Workspace client anywhere. The recent one 3.0.10 suffers from graphics corruption and is basically unusable for our entire fleet.  Seeing similar issues to this forum posting: [https://forums.aws.amazon.com/thread.jspa?threadID=328234&amp;tstart=0](https://forums.aws.amazon.com/thread.jspa?threadID=328234&amp;tstart=0)  


Does anyone have a link to the 3.0.8 or 3.0.9 Win10 client for me?  


Thank you!",2,aws,2020-10-13
j2aak9,Can't see lambda function pushed from ASK,"I've been struggling with this for the past few days. I am learning how to make Alexa skills and have set up locally. I used `ask new` to create a new node project, I also selected lambda. It was working fine and I did `ask deploy` and everything worked correctly it said. During the deploy it said: 

`Updating current Lambda function with ARN arn::aws::us-west-2::xxx`

(xxx has info just not sure if that's safe to share publicly) and it didn't give any error messages or anything. I then went to go check for the lambda function on my AWS account, and I couldn't find anything. I made sure that I was in the right region (us-west-2 Oregon) but it's not there! The alexa skill pushed and I was able to test that and it worked but there is no function I can see (I need to see the logs since my skill is now breaking with some code I added)

I'm not sure if this is important but in case it is I'll put it here as well but inside .ask/ask-states.json I have   


    ""default"": {
      ""iAmRole"": ""arn:aws:am::xxx,
      ""lambda"":{
        ""arn"": ""arn:aws:lambda:xxx,
         ...
      }
    }

  
The `arn` property is the same as the one that pops up when running `ask deploy`  


I also tried the minimum responses that were here: [https://www.reddit.com/r/aws/comments/49ag62/cant\_see\_all\_my\_lambda\_function\_in\_aws/](https://www.reddit.com/r/aws/comments/49ag62/cant_see_all_my_lambda_function_in_aws/) it doesn't look like the question was ever answered.

Any help would be appreciated a lot!",1,aws,2020-10-13
j2a1lu,"On-Demand Capacity Reservations - ""capacity is available immediately""","According to the AWS docs ([source](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html)) -

&gt;You can create Capacity Reservations at any time, without entering into a one-year or three-year term commitment, and **the capacity is available immediately**. When you no longer need it, cancel the Capacity Reservation to stop incurring charges. 

So if I run into insufficient capacity issues in a region can I just create a capacity reservation and the instances I was trying to create will somehow be available immediately? Sounds too good to be true...",4,aws,2020-10-13
j29vsr,AWS Serverless Application Model : Guide to writing your first AWS SAM Application,,10,aws,2020-10-13
j278ob,AWS CodeBuild to GitHub authentication,"I'm trying to set up an AWS CodeBuild project to build/test the code in a GitHub repo in a GitHub organisation (it will be many repos in the future). However, I'm slightly confused on how AWS CodeBuild gets access to the GitHub repo. It seems that I can only set up CodeBuild to access the repo on my behalf, i.e. using a token that represents me. This means if I were to leave to GitHub org that the CodeBuild would fail at that point?

Is there a way, other than creating a dedicated GitHub user for this purpose, to authenticate AWS CodeBuild to GitHub. I.e. access GitHub on an org level rather than a person/account level?",2,aws,2020-10-13
j28dqt,Inbound mail bouncing on SES when JSON is present in body,"spam and virus scanning is disabled on the SES inbound rule.

When I send email with text in the body, it works fine.

When I send a 2000+ line JSON format it bounces like:

    {
      ""Properties"": {
        ""duration"": ""0"",
        ""estimatedDevices"": ""10000"",
        ""month"": ""JUNE"",
        ""dateRange"": ""Jun 11, 2020 - Jun 11, 2020"",
        ""year"": ""2020"",
        ""companyName"": ""My Fish Inc"",
        ""seMAIL"": ""sxere_fish.com""
      },
      ""Policies"": {
        ""Blizzard Instance State"": [{
          ""key"": ""Instance Running"",
          ""value"": ""6""
    ....
    

The bounce message is ""The sender's email address is invalid""

This only happens when I send the JSON.  I don't see anyway to get a Cloudwatch log on these inbound SES rules.

Any ideas?

&amp;#x200B;

Many thanks!",5,aws,2020-10-13
j288iz,Will AWS support .ai domains?,"I'm using an .ai domain for my web app, and at the moment it's unsupported. Can this be fixed?",5,aws,2020-10-13
j288d3,Learning plan for AWS + Terraform,"Hi,  
  
I'm working on a personal project which basicly consists of scrapping websites.  
In order to make the scrapping more efficient, I'd like to run multiple scrappers/workers in parallel.  
For now I run a single scrapper/worker application locally but that's about it.  
The application is not dockerized yet but that shouldn't be too hard to do.  
I would also like to schedule some jobs e.g. trigger a scrapping every 24 hours etc...  
  
I don't know much about AWS but I'm pretty sure I would need to feed some data to a queue like SQS with say 10 websites I want to scrap and it would spin up 10 workers but I have no idea where to start.  
Ideally I would also like to use Terraform to do infrastructure as code rather than doing everything manually on the AWS UI since we are multiple people working on this project on GitHub.  
  
What would be a good learning plan, preferably free?  
Is Terraform too much for a beginner?  
  
Thanks!",6,aws,2020-10-13
j26vis,"On-prem managed instances and IAM roles, how do they work?","I'm trying to come up with a good way to setup some Windows credentials for auto domain join.  So far the best way I've come up with for my project's scenario is to have credentials generated (and routinely refreshed) and stored in SSM Parameter Store, however we do not currently have any AD servers hosted in AWS.  I'm testing out deploying the SSM agent to an on-prem system and it shows up in SSM as a managed instance, and when I look at the managed instance I see my IAM role associated to it, however I don't understand how the IAM roles work with managed instances.  I can take an IAM role with similar policies assigned and it will connect to the Parameter Store just fine, but with the service role attached to my managed instance I just get, ""No credentials specified or obtained rom persisted/shell defaults.""  Searching on the subject I see lots of things about how to use AssumeRole , but that also seems to give me the same error.",3,aws,2020-10-13
j26tw9,AWS Virtual Meetup: API Discovery and Real-Time Analytics at Scale with DynamoDB,,2,aws,2020-10-13
j267hm,Cannot login to AWS Elasticsearch using Nginx proxy,"Hi All,

I have created an ElasticSearch service on AWS and trying to create a reverse proxy to connect to the internal VPC endpoint.

I have been trying to follow this guide: [https://medium.com/@k.ashu403/aws-elasticsearch-nginx-reverse-proxy-for-accessing-kibana-86292edc6f14](https://medium.com/@k.ashu403/aws-elasticsearch-nginx-reverse-proxy-for-accessing-kibana-86292edc6f14)

Also have tried other nginx configurations which I can all get to the same place.

I found a stack overflow post explaining the exact same issue with no responses ([https://stackoverflow.com/questions/63144494/kibana-redirects-back-to-login-after-successful-auth-on-aws-elasticsearch-servic](https://stackoverflow.com/questions/63144494/kibana-redirects-back-to-login-after-successful-auth-on-aws-elasticsearch-servic))

One thing I have noticed is if you follow the network trace it tries to resolve this URL after you successfully login which is then 'Cancelled'. **Request URL:** http://\[nginxIP\]/\_plugin/kibana/api/ui\_metric/report. . Once this cancels, it reloads the login page.

Any help from anyone in this community would be fantastic?

Thank you in advance",1,aws,2020-10-13
j260kr,Can I disable xray tracing?,"Hi guys,

So, I'm having a problem with xray: I need to disable the tracing for my health check endpoints, but I don't seem to be able to do it. I've tried configuring sampling rules using the aws console and tried putting the same rule in my x-ray api configuration, but it doesn't work. I've reading the documentation and can't find a way to do it.

Does any of you guys know how i would do it?

**My configuration:**

&gt;{  
 ""version"": 2,  
 ""rules"": \[  
        {  
 ""priority"": 1.0,  
 ""description"": ""Todas chamadas."",  
 ""host"": ""\*"",  
 ""rate"": 0.05,  
 ""http\_method"": ""\*"",  
 ""url\_path"": ""/cliente/v1/\*"",  
 ""fixed\_target"": 0.0  
        },  
        {  
 ""priority"": 1.0,  
 ""description"": ""Disabling health check endpoint"",  
 ""host"": ""\*"",  
 ""rate"": 0.0,  
 ""http\_method"": ""GET"",  
 ""url\_path"": ""\*health"",  
 ""fixed\_target"": 0.0  
        }  
    \],  
 ""default"": {  
 ""rate"": 0.1,  
 ""fixed\_target"": 1.0  
    }  
}",2,aws,2020-10-13
j24h55,market research interview for IoT developers,"Would anyone care to donate 15 minutes of your time for a quick chat? I'm looking to speak to developers who actively work with IoT devices. Preferably on the firmware side, but it's not necessary.",3,aws,2020-10-13
j23s3t,Is AWS the best for hosting the kind of website I want to do?,"I have created some webpages with HTML, CSS, JS, and jQuery.  They connect to a Microsoft Access database to run some queries and present the user with information based on their selections.

I acquired a domain, but I need someone to host my website.  However, I'm too dumb to figure this part out.  The AWS documentation is far too complicated, as is the ""AWS for Admins for Dummies"" book I got from the library.

All I want to do is have some server hold my website files and my Access database, and serve up my default.aspx page when someone types in the domain I bought.

Is AWS suitable for this?  Is there some ""How to"" out there that would help an idiot like me accomplish this task?  Is there a better subreddit for my questions?",2,aws,2020-10-13
j234uf,Amazon Work Spaces: How can I use two monitors?,"Hi,

I'm using two monitors, one is 17-18 inches long and the other is 13 inches because I have a large monitor hooked up to my laptop. The screen resolution of both is 1920 x 1080. When not using AWS, I can move things around across both monitors. But when using AWS  it only lets me use one monitor at a time. 

Someone made a similar post here and didn't find a solution but my coworker is able to use both of her monitors in AWS. She has two matching-sized monitors and had to install some sort of software.

I asked my company's I.T. for help and they only suggested the setup I already had. Is it impossible to use two monitors with my current set up?

&amp;#x200B;

Thanks!",0,aws,2020-10-13
j2201o,AWS VPN subnet routing question,"So I have the standard 10.0.0.0/16 address space allocated to a VPC.  I've broken my subnets in that VPC into simple /24 address spaces. 


I have a 10.10.10.0/24 subnet on one of my on-premise networks(I have no control over this network and did not pick it's addressing scheme).  I recently found out I'll need to route traffic from a particular AWS subnet to this on-premise subnet over AWS VPN.  None of the subnets I have in AWS are 10.10.10.0/24.


The question is, if I setup AWS VPN and set a static route to the on-premise 10.10.10.0/24 network, will traffic be properly routed there?  Even though I don't have that particular subnet in AWS, the VPC is still set to be allocated with the standard 10.0.0.0/16 address space, which of course 10.10.10.0/24 would be a part of.  My question stems from not knowing how AWS VPCs handle that allocation in its own backend system. 


So will there be a conflict on the AWS networking stack, or will the static route I add work and route that traffic over the VPN?

Edit: So I'm embarrassed, I had a major brain fart and was thinking of a /8 not /16.  Duh, 10.10.10.0/24 is fine.  But if it WAS a /8 the correct answer would be to do NAT.  Thanks everyone!


Thanks Reddit!",2,aws,2020-10-13
j21njr,Is there any alternate approach to cost explorer API and tags for CloudFormation stacks to find the cost of usage by each stack?,"Hi I am from the DevOps team of a fast growing startup..

I am looking for tools or ways which can help me know the cost and usage of resources of an individual CloudFormation stack without using tags or cost explorer API.

I explored the option of using cost explorer API to display in Grafana dashboard..

But once our product is deployed in the client-side.. we need admin access to create tags for our CF stacks and access cost explorer API.. which is a dependency on our client and we are looking forward to eliminate this dependency to improve the experience for our clients..

If there is any alternate approach which you have come across.. please share your thoughts about it..

Open-source is preferred but other alternatives are also fine..

Thanks in advance..",5,aws,2020-10-13
j20cut,Cross-account CloudMap resources,"Having some trouble getting access to a CloudMap account from a child account.  Has anyone tried this before?  
The docs refer to a link that helps manage CloudMap, but not how to set up permissions to consume resources listed in CloudMap.",2,aws,2020-10-13
j2070k,aws-quickstart bastion host failing to create - eu-north-1,"Hoping someone here can help,   
[https://github.com/aws-quickstart/quickstart-linux-bastion/blob/main/templates/linux-bastion.template](https://github.com/aws-quickstart/quickstart-linux-bastion/blob/main/templates/linux-bastion.template)  


I have been trying to spin this template up in eu-north-1 region and it keeps failing with:

&gt;BastionAutoScalingGroup	   
&gt;  
&gt;\*CREATE\_FAILED\*   
&gt;  
&gt;The requested configuration is currently not supported. Please check the documentation for supported configurations. Launching EC2 instance failed.

I have tried the exact same setting in us-east-1 and it works fine.

Anyone got any ideas whats wrong with and why it would fail in eu-North-1??

I've also created an issue here: [https://github.com/aws-quickstart/quickstart-linux-bastion/issues/104](https://github.com/aws-quickstart/quickstart-linux-bastion/issues/104)

but not entirely sure how active that place is.",2,aws,2020-10-13
j206jz,AWS enabling some big tech for little attractions - a drive thru haunt,,8,aws,2020-10-13
j1ymwz,Introducing AWS cost anomaly detection (preview),,106,aws,2020-10-13
j1ze5y,File from a thing to S3,"How would it be possible to transfer a file from a thing to an S3 bucket?   
The thing is in Greengrass group. With a lambda with a IAM role that has an access to the bucket?",0,aws,2020-10-13
j1z3ch,Best way to automate ECS setup + deployments?,"What do people suggest as a good way forward when it comes to having ESC infrastructure as code and deployments (blue/green) automated?

I have set up a couple of clusters / tasks / services (via the console) as sandbox to learn ECS but am now at the stage where I would like to deploy apps, built by jenkins in an automated fashion. It would also be interesting to use aws Code Deploy.

At the moment my focus is on terraform for the cluster configuration, is terraform also a good option for setting up and operating code deploy? I see some tutorials that use cloudformation but i'm really not a fan and would rather avoid it if at all possible. Another  alternative i could consider is ansible

Edit: images are already stored in ECR",1,aws,2020-10-13
j1yfox,Is there an Amazon WorkSpaces client for OpenSuse?,"Hi, I have been trying to find an Amazon WorkSpaces client for OpenSuse but no package seems to be available anywhere. I know amazon only officially supports deb but I was hoping that there would be some repackaged client at a community repo, or snapcraft, or somewhere. Why wouldn't amazon support RDH and SUSE as well? It wouldn't be that much of a task for amazon to repackage it, even as non-supported client.",5,aws,2020-10-13
j1xlhs,SUSE SAP BYOL image,"Hy guys,

We have a plan to migrate our existing SAP to AWS, because of our existing subscription. We want to BYOS option when migrate to AWS.

I tried to search for the procedures, but didn't get a clue for that.

Like

\- Which AMI should we use for BYOS

\- Which is better use BYOS or On-Demand SUSE subscription on AWS

\- How to migrate with minimum downtime. We get suggestion for using cloudendure, but we considering too to use fresh installation and using system copy.

&amp;#x200B;

Maybe someone can help me get some enlightenment? BYOS is so confusing for us.

&amp;#x200B;

Thank you",3,aws,2020-10-13
j1xhk1,What is the best approach to convert XML to PDF via XSLT,"We have a team in our project who are responsible to convert 100000+ XML files to PDF via an XSLT. Currently they are running the task on 4 desktops but are looking for a quicker turnaround.  
This made me think to use some service in AWS to do this but I have no idea what AWS service we could deploy. I was thinking Lambda could be a candidate but not sure about the allowed running time per call (some XML's are quite large, resulting in over 20MB PDF files) and the cost of calling Lambda for so many files might not be cost effective.  

  
  
&amp;nbsp;  
Edit: we asked our TAM also for some input. Once we have some conclusion from them or perhaps via the inputs from the great people here we will paste the route and method we have chosen to go with.
  
If anyone here has some idea or suggestions then that would be great!",5,aws,2020-10-13
j1wyzx,SNS doesn't deliver SMS,"Hi,  
I'm using Amazon SNS to send SMS text messages to verify and notify my users. But these several days the SMS never came. There's no error at all, on the dashboard it says 100% delivery rate all green but in reality the SMS never came. I tried on several phone provider (SEA region Singapore) with SDK and directly from dashboard all got same result, it says successful but SMS never came. I also get billed.  


Anyone can help? Like where to check or report or anything? Or SNS has always been unreliable?",1,aws,2020-10-13
j1t8a4,"CloudFront has not support to contents compression by a HTTP 1.0 request, this is right?","Hi guys

a few moments ago, I have read out the document for about CloudFront contents compression feature.

well.. in this docs have an explanation that CloudFront do not support compression when a request by HTTP 1.0.

But I've tried it then got a respond brotli and gzip compressed from Edge when I request with HTTP 1.0

I am fully aware, Accept-Encondig header is not on the  HTTP 1.0  specification.

&amp;#x200B;

is this feature has been wrong?",1,aws,2020-10-13
j1qvdk,master node hidden in eks?,"master node not shown when running `kubectl get nodes`

```
$ k get node
NAME                                                    STATUS   ROLES    AGE   VERSION
fargate-ip-192-168-116-129.us-west-2.compute.internal   Ready    &lt;none&gt;   21m   v1.17.9-eks-a84824
fargate-ip-192-168-124-93.us-west-2.compute.internal    Ready    &lt;none&gt;   21m   v1.17.9-eks-a84824
```

it this by design on all managed kubernetes service like aks,gke?",1,aws,2020-10-13
j1p4nh,Error while deleting objects from AWS S3,"Hey guys, I'm creating a funcition in my app to delete multiple objects from S3. But it returns me this error: \`The request signature we calculated does not match the signature you provided. Check your key and signing method.\` I wrote correctly the key and the secret key. Here's the code

&amp;#x200B;

try{  
 AWS.config.accessKeyId = ""key""  
 AWS.config.secretAccessKey = ""secretKey""  
 AWS.config.region = ""region""   
   
 const s3 = new AWS.S3();  
 //Generates Objects with \`Key\`  
 var objects = this.state.variants\_s3\_urls.map((object)=&gt;({Key: object}))  
 var params = {  
 Bucket: 'bucket',  
 Delete: {  
 Objects: objects  
}  
}  
 s3.deleteObjects(params,(err,data)=&gt;{  
 console.log(err)  
 console.log(data)  
}).promise();  
}catch(err){  
 console.log(""CATCH"")  
 console.log(err)  
}",1,aws,2020-10-13
j1w6aw,AWS IoT and device states,"If the state of a thing changes (eg. temperature sensor value changes) I can use an IoT Rule to update that state change directly to DynamoDB, right?  
So I can keep track of thing state in DynamoDB and query their current sensor values with lambdas? 

How about Fleet Indexing, it seems that it is designed to do just that.   
Should I rather be using Fleet Indexing and use it to keep track of each thing and query their state and sensor values?",6,aws,2020-10-13
j1ul2g,Live speech recognition," 

Hello guys,

first  of all thanks in advance for the help. I want to bring to your  attention a problem i've run into. I have right now a program which,  after running it, open the computer's microphone and actually listen to  what i am saying and then (using some python libraries) translate it  into text. I would like to do this operation also with AWS services; i  know that for Speech to text option i can use Amazon Transcribe. But for  the microphone ""live"" acquisiton (basically without saving the sentence   i want to transcribe as a mp3 and then save it into a bucket) which  service i could use?

I already have a script which calls the remote Transcribe service on a mp3 file stored in a bucket and get the output and print it into the console. I just want to do a step forward to avoid saving the mp3 file in a bucket if possible.

&amp;#x200B;

Thanks for the help!",2,aws,2020-10-13
j1uckc,How to share DynamoDB access between accounts?,"We have multiple accounts in the same region under the same organization. What would be the best way to go about sharing DB access to DynamoDB between VPCs in different accounts? We want to access this database between EC2 instances in different accounts.

My guess is PrivateLink? Thanks in advance!",1,aws,2020-10-13
j1u9rg,Automated help on quota increases?,"I've found the quota system pretty frustrating. I dislike waiting for my quota request and find it infuriating when my request is declined. Is my money not green enough?

I'd like a website that automatically requests maximum quota for all services. When quota requests are denied or limited, I want insights into what I can do to get that request approved.

Is this something that would be useful for you as well? Curious how you guys handle quotas and if this website idea is worth building.",0,aws,2020-10-13
j1sr0r,Moving data from SQL Server to S3 Bucket,"What is the quickest way to do this? I’m not migrating a whole database, but a few select Tables.

The sql server is on Prem.

Thanks for the help!",1,aws,2020-10-13
j1rtze,"Newbie question, cheaper alternative than SFTP for something akin to FTP access for EC2 instance's filesystem?","Hi there,

Total AWS newbie here. I'm using an EC2 instance to host Foundry VTT, a virtual table top application for playing games like Dungeons and Dragons. 

After successfully following a tutorial to get it set up, I wanted to access the Linux filesystem to upload my ""world"" (dnd campaign) files to the proper directory so that Foundry would be able to see them, so I wouldn't have to rebuild everything I had already made from scratch. I tried using rclone, but at least from the tutorial I followed it didn't seem to expose the actual Linux filesystem, instead getting me to an S3 bucket. After some more searching I found that I could setup SFTP access pretty easily, and that worked perfectly-- I was able to connect to the EC2 instance and navigate throught he filesystem to the proper directory and upload my data. However, I did not see that SFTP access costs $.30 an hour until I found I had a $60 bill waiting for me when I logged into the billing section.

Fingers crossed Amazon will take pity on me and rescind the charge, but going forward, do any of you have a suggestion of a cheaper way to accomplish what I am after? I have some very basic linux and ssh skills, basic Python knowledge, and a background in web development from forever ago hence my inclination to get something akin to FTP going for myself.

Thanks for any help that you can give!",2,aws,2020-10-13
j1raxw,Tokenized “magic” login links that can be emailed to users at an interval.,"I’m wondering if this is possible to achieve with Cognito:  we have a product that sends surveys to employees. In order to maximize participation we send the users pre-generated login links with tokens in them. They get these links in an email once a week, and the tokens have a lifespan of about 24 hours so that they have a little lee-way to complete the survey. 

We’re thinking of moving to cognito to manage our users, but I’m not sure if there’s any way to replicate this process. Is there any way to generate a jwt on behalf of a user without their username or password?  We would like to avoid the user being sent to a login screen when they click on the link.",3,aws,2020-10-13
j1p9ul,EventBridge processing pipeline,"I’m designing a custom event-based processing pipeline and want to use EventBridge as the bus.  This gives us a single endpoint to use across the application (multiple event types) and provide to customers for customer-specific events, route to targets based on the payload (instead of attributes), etc.  All the reasons why EventBridge exists.  Except for the cross-region limitation.  

I want to deploy the processing pipeline into 2 regions for either active/active processing (load balance the two) or active/passive processing (1 region acts as a failover for the other).  

Regardless, I know I can have the bus in the primary region target another bus in the secondary region, but I’m wondering if there is any other pattern that’s elegant and makes sense?  

How can I either failover to the secondary region or load balance between the two?

Note: I’m not necessarily trying to solve for a regional outage (the EventBridge bus itself would be down as well).  

But then that raises another question...how are people solving for a regional outage where the bus itself is down?  Just provide a list of endpoints in different regions?

Hope to hear how other people are architecting around this.",6,aws,2020-10-13
j1i7iz,When / why to use ARM Graviton2 EC2 instead of Intel/AMD instances?,"I understand that the Graviton2 instances are roughly 20% cheaper than the comparable Intel ones. For example *m6g.large* is 20% cheaper than *m5.large*. However because it's a different CPU architecture there may be significant costs associated with porting, maintaining builds for  2 different CPUs, extra testing, presumably worse upstream packages support, etc. The true cost analysis may be tricky, but doable.

Anyway, the cheaper pricing aside - when would you consider using  Graviton2 instances? Are they better, faster, more effective for some specific workloads? More secure? Have better networking? Can they do  something that Intel/AMD instances can't?

I'm more interested in the technical reasons to use ARM Graviton2 EC2 rather than the cost perspective.",5,aws,2020-10-13
j1fl9w,SES complaint@simulator.amazonses.com handled as bounce?,"Hi Amazonians 🤓

# TL;DR

complaint@simulator.amazonses.com unexpectedly calls bounce topic &amp; lambda instead of the expected complaint.

# In detail

So i've got 2 SNS Topics set up, 1 for bounces &amp; 1 for complaints. 2 separate Lambda functions, which are subscribed to the Topics respectively. The complaint Lambda deletes newsletter subscribers straight. The bounce Lambda only deletes if `bounceType !== ""Transient""` or `bounceSubType === ""AttachmentRejected""` otherwise it increments the bounce count for this subscriber for later removal.

I'm about to go live (production access has just been granted this morning 🎉) but wanted to test everything again when I noticed that subscribing `complaint@simulator.amazonses.com` would result in a soft bounce instead of a complaint as expected. As I understand the [simulator docs](https://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-email-simulator.html#send-email-simulator-how-to-use) it should be as ""marked as spam"" by receiver, which I expect to receive as a complaint to my complaint handler.

In my logs I can see that only the bounce Lambda has run and the `bounceType=Transient` &amp; `bounceSubType=General`.

**Edit**: I should mention that I know for sure that it worked before, like a week ago or so.",1,aws,2020-10-13
j1jk7u,Virus / malware scan for S3 - What do you use?,"I see a couple of options in the AWS Marketplace and via a Google search. What do you use? Has anyone tried a couple of them and found one to be better than another? I'd love to hear your experiences.

My use-case is fairly simple. I upload images, resize them, and store them in S3 for delivery to multiple clients (web, mobile). I also upload Documents (PDF, Word). I just need to make sure they are free of malware prior to processing the files. 

In the future, I'd also like to use Rekognition to see if the document passes some threshold to be acceptable (i.e. not offensive, etc.). So, I'll need to work out a sample workflow between the systems. Thanks for your insight.",4,aws,2020-10-13
j1ly8o,Deploying Lambda from Cloud9 Issues,"I have a couple python 3.6 lambda functions that I created in the Cloud9 IDE.  I have been able to develop and deploy updates from Cloud9 without issue. However, now I am attempting to deploy another update, and am receiving an error that says ""UPDATE\_FAILED &lt;lambda function name&gt; already exists"". I'm following the same steps I always was (open the lambda from the right hand pane, right click the lambda, select ""deploy"").  Does anyone know why that wouldn't work anymore, and how I can deploy the lambda again? I want to keep the name so my current triggers and reporting still work. Thanks!",1,aws,2020-10-13
j1e9s3,eBook delivery with DRM using AWS,"Hi,

&amp;#x200B;

I was looking at different options to deliver ebooks with DRM using the AWS platform. The [Media Services](https://aws.amazon.com/media-services/) and [SPEKE](https://docs.aws.amazon.com/speke/latest/documentation/what-is-speke.html) look interesting but seemed to be directed at protecting video content.

&amp;#x200B;

Can someone please recommend a solution suitable for eBook delivery with DRM?

&amp;#x200B;

Thanks!",1,aws,2020-10-13
j1gy2o,Is there any downsides to using cognito for authentication rather then just handling it myself?,,2,aws,2020-10-13
j1g4c5,Multiple Windows VPS - Same password on launch possible?,"I have a simple question I think, but can't seem to find it:

I'm running multiple Windows VPSses those: ""Microsoft Windows Server 2019 Base with Containers"".

But what i want is that they all have the same password from launch on. Is that possible? I'm running a lot of servers for the same cause, would save a ton of time. 

I'm connecting with RDP only.  And at the RDP connect page i see this, I think this is the solution but don't know where to find those directories:

""If you've joined your instance to a directory, you can use your directory credentials to connect to your instance.""

Hope you guys can help me out here :)",2,aws,2020-10-13
j1mcae,How to create a public web-server AND share login credentials for remote work?,"So, I'm following this tutorial: [https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP\_Tutorials.WebServerDB.CreateVPC.html](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Tutorials.WebServerDB.CreateVPC.html)

(Please share a better one if you know) and I think I have created a public web-server. What I don't know is what to do next. I have to share this server with people working remotely. I have no idea how to do that. 

Please help me.",2,aws,2020-10-13
j1m1l7,Can't load 2gb PDF file from s3 in html5 object?,"For some reason I cannot get large files to work in my file preview. Everything works fine if the file is smaller. It also works if I download the file and then open it in the browser (or adobe reader). The metadata is set correctly to ""application/pdf"". It loads for a while and then just says ""failed to load pdf"" in chrome. In firefox it just never loads and I don't even get an error. It seems like it's downloading the whole file, I see the network traffic.

What am I missing? The only difference between this and a working PDF seems to be the file size?",1,aws,2020-10-13
j1lpto,FYI: CDK Day 2020 Coming on September 30th,,92,aws,2020-10-13
j1fqdv,"Understanding AWS SageMaker's ""Local Mode""?","Hello,

I am new to AWS SageMaker. My college is looking to use it for a course on cloud computing and machine learning. 

We have been looking into using SageMaker offline through Jupyter NoteBooks for the first part of the course, just so students can get used to the API without having to worry about logging into AWS and navigating the console.

I see that I can install the boto3 and sagemaker Python packages, but when I try to run a job in local mode on my laptop, I keep running into issues. I finally got everything to work on a small regression project and called [model.fit](https://model.fit)()...but the Jupyter cell hangs indefinitely. 

Even if I do get this to work on my Linux machine, I'm thinking it won't be worth the effort of trying to get students up and running with Mac OS, Windows, and who knows what else.

TL;DR:

Is it possible (or reasonable) to run SageMaker in offline mode on a laptop?",2,aws,2020-10-13
j1f3cp,how to make boto3 s3 file upload use IAM role for authentication,"I made a code to upload the files to S3 using boto3. The code runs in docker using cron job.
Initially I've set the AWS credentials in the Dockerfile using `ENV`, and later switch to binding `/home/$USER/.aws/` to the container to `/root/.aws/`. 


```
FROM python:3.7-alpine

WORKDIR /scripts

RUN pip install boto3

# ENV AWS_ACCESS_KEY_ID=
# ENV AWS_SECRET_ACCESS_KEY=

COPY s3-file-upload-crontab /etc/crontabs/root
RUN chmod 644 /etc/crontabs/root

COPY s3_upload.py /scripts/s3_upload
RUN chmod a+x /scripts/s3_upload

RUN mkdir /root/info/
RUN touch /root/info/max_mod_time.json
RUN touch /root/info/error.log

RUN mkdir /root/.aws/
RUN touch /root/.aws/credentials
# RUN touch /root/.aws/config


ENTRYPOINT crond -f
```

```
version: '3.8'
services:
  s3-data-transfer:
    image: ap-aws-s3-file-upload 
    build:
      context: ./
    volumes:
      - ../data/features:/data
      - ./info:/root/info
      - ~/.aws/credentials:/root/.aws/credentials
      # - ~/.aws/config:/root/.aws/config
```

At this point the code is using my credentials (AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY) for authentication and works perfectly.

I'm trying to switch the authentication to IAM roles. I've created a role in AWS called `Upload_Data_To_S3` with the `AmazonS3FullAccess` policy. 

I'm reading the [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#assume-role-provider) on how to set up boto3 for IAM roles. I've set my `~/.aws/config/` as follows

```
[default]
region=ca-central-1

[profile crossaccount]
role_arn=arn:aws:iam::#######:role/Upload_Data_To_S3
source_profile=
```

I don't have AWS CLI installed so no profile, besides my user on AWS account. My python code contains no code to do with authentication. 

```python
#!/usr/local/bin/python3

import boto3
from botocore.errorfactory import ClientError
import os
import glob
import json
import time

# TODO: look into getting credentials from IAM role
s3_client = boto3.client('s3')
s3_bucket_name = 'ap-rewenables-feature-data'

max_mod_time = '0'
file_list = glob.glob('/data/*.json')  # get a list of feature files
file_mod_time = None

# get mod time for all files in data directory
file_info = [{'file': file, 'mod_time': time.strftime(
    '%Y-%m-%d %H:%M:%S', time.gmtime(os.path.getmtime(file)))} for file in file_list]

# sort files my mod time (min -&gt; max)
timestamp_sorted_file_info = sorted(file_info, key=lambda f: f['mod_time'])
# print('File Info Sorted by Time Stamp:\n',timestamp_sorted_file_info)

# check if the file exists and not empty -&gt; set max_mod_time from it
if os.path.exists('/root/info/max_mod_time.json') and os.stat('/root/info/max_mod_time.json').st_size != 0:
    with open('/root/info/max_mod_time.json', 'r') as mtime:
        max_mod_time = json.load(mtime)['max_mod_time']

# upload the files to s3
mod_time_last_upload = ""0""
for file in timestamp_sorted_file_info:
    file_mod_time = file['mod_time']  # set mod time for the current file
    # file_mod_time = '2020-09-19 13:28:53' # for debugging
    file_name = os.path.basename(file['file'])  # get file name from file path

    if file_mod_time &gt; max_mod_time:  # compare current file mod_time to max_mod_time from previous run
        with open(os.path.join('/data/', file_name), ""rb"") as f:
            s3_client.upload_fileobj(f, s3_bucket_name, file_name)

            # error check - https://stackoverflow.com/a/38376288/7582937
            # check if the file upload was successful
            try:
                s3_client.head_object(Bucket=s3_bucket_name, Key=file_name)
                mod_time_last_upload = file_mod_time
                print(file_name, ' is UPLOADED')
            except ClientError as error:
                # Not found
                if error.response['ResponseMetadata']['HTTPStatusCode'] == 404:
                    # save error to log file
                    open('/root/info/error.log', 'w').write(str(error))
                    print(""error: "", error)
                break

        print('File Mod Time: ', file_mod_time)
        print('Mod Time Last Upload: ', mod_time_last_upload)


# save max mod time to file
# https://stackoverflow.com/a/5320889/7582937
# create JSON object to write to the file
object_to_write = json.dumps(
    {""max_mod_time"": mod_time_last_upload})

# write max_mod_time to the file to be passed to the next run
if mod_time_last_upload is not ""0"":
    if object_to_write:
        open('/root/info/max_mod_time.json', 'w').write(str(object_to_write))
```

When I build and run the container I get the following error:

```
Traceback (most recent call last):
  File ""/scripts/s3_upload"", line 40, in &lt;module&gt;
    s3_client.upload_fileobj(f, s3_bucket_name, file_name)
  File ""/usr/local/lib/python3.7/site-packages/boto3/s3/inject.py"", line 539, in upload_fileobj
    return future.result()
  File ""/usr/local/lib/python3.7/site-packages/s3transfer/futures.py"", line 106, in result
    return self._coordinator.result()
  File ""/usr/local/lib/python3.7/site-packages/s3transfer/futures.py"", line 265, in result
    raise self._exception
  File ""/usr/local/lib/python3.7/site-packages/s3transfer/tasks.py"", line 126, in __call__
    return self._execute_main(kwargs)
  File ""/usr/local/lib/python3.7/site-packages/s3transfer/tasks.py"", line 150, in _execute_main
    return_value = self._main(**kwargs)
  File ""/usr/local/lib/python3.7/site-packages/s3transfer/upload.py"", line 692, in _main
    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
  File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 337, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 643, in _make_api_call
    operation_model, request_dict, request_context)
  File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 662, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File ""/usr/local/lib/python3.7/site-packages/botocore/endpoint.py"", line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File ""/usr/local/lib/python3.7/site-packages/botocore/endpoint.py"", line 132, in _send_request
    request = self.create_request(request_dict, operation_model)
  File ""/usr/local/lib/python3.7/site-packages/botocore/endpoint.py"", line 116, in create_request
    operation_name=operation_model.name)
  File ""/usr/local/lib/python3.7/site-packages/botocore/hooks.py"", line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/botocore/hooks.py"", line 228, in emit
    return self._emit(event_name, kwargs)
  File ""/usr/local/lib/python3.7/site-packages/botocore/hooks.py"", line 211, in _emit
    response = handler(**kwargs)
  File ""/usr/local/lib/python3.7/site-packages/botocore/signers.py"", line 90, in handler
    return self.sign(operation_name, request)
  File ""/usr/local/lib/python3.7/site-packages/botocore/signers.py"", line 160, in sign
    auth.add_auth(request)
  File ""/usr/local/lib/python3.7/site-packages/botocore/auth.py"", line 357, in add_auth
    raise NoCredentialsError
botocore.exceptions.NoCredentialsError: Unable to locate credentials
```

That's understandable since I don't have the credentials in the container. What do I need to add to the code or the `~/.aws/config` file for it to use the IAM role I've set up? Unfortunately the docs aren't very clear in this regard.

Thanks in advance.",6,aws,2020-10-13
j1j9yk,Using Workspaces to secure remote workers,,4,aws,2020-10-13
j1ecdy,S3 public access block: Precedence between per account and per bucket settings?,"[S3 public access blocks](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) can be applied per S3 bucket, or per AWS account.

Suppose I have an **account-wide** public access block:

    resource ""aws_s3_account_public_access_block"" ""block-public-access"" {
      block_public_acls       = true
      block_public_policy     = true
      ignore_public_acls      = true
      restrict_public_buckets = true
    }

And, at the same time, a per bucket public access block:

    resource ""aws_s3_bucket_public_access_block"" ""example"" {
      bucket = aws_s3_bucket.example.id
     
      block_public_acls   = true
      block_public_policy = true
      ignore_public_acls = false # Different
      restrict_public_buckets = false # Different
    }

What's the end result? I would expect that the account-wide setting takes precedence, but I can't find a clear confirmation in the docs. Also, I'm not sure if the order in which they were created has any impact

The closest is:

&gt;You can apply these settings in any combination to individual access points, buckets, or entire AWS accounts. If you apply a setting to an account, it applies to all buckets and access points that are owned by that account

(from [https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html#access-control-block-public-access-options](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html#access-control-block-public-access-options))

Thanks!",1,aws,2020-10-13
j1dw3m,dual monitors for workspaces,"i have an old macbook air (2013) and am running 2 monitors attached to it, one via the minDisplay port and one from a USB3-&gt;hdmi adapter.  all is right with the world.

when i open workspaces and select ""Enter Full Screen on all displays""  aws workspaces moves to a new mac desktop, and puts the display on the left monitor, where the right monitor is black (although i can move my mouse from left-&gt;right(black) and see the pointer.

the 2nd aws window is opened on a different mac desktop (i can hit ctrl - left/right arrow) to get to it, and i see it in its own full screen window (with a title bar)  i can even drag aws worksapce windows from left into the black and have them appear in the other window on a different desktop

but i obviously want the aws workspace desktop to span both monitors right to left, not one left on one desktop and one right on another.

&amp;#x200B;

any idea why this is happening ?",0,aws,2020-10-13
j1f6ux,Global Lambda scope and thread safety,"I was looking into whether the Python Boto3 library was thread safe today and it turns out that it isn't at the resource level. This made me wonder if declaring DynamoDB Table variables in the global Lambda scope is a safe thing to do.

I wasn't able to find a response to this question, so I'm wondering if anyone here has experience with this?",5,aws,2020-10-13
j1f6uj,Does ALB remove the need to put a NGINX server in front of my app servers?,"I have a server for chat that handles websocket connections and a server for the core of my application with is just a rest API, users make posts and stuff. I was planning to put and NGINX server in front of both, to balance the load and act as a reverse proxy. However, now I am thinking the ALB might handle that for me. Is this assumption correct?",41,aws,2020-10-13
j1en8h,How do you allow/deny access to private resources across environments/AWS accounts with remote employees?,"I just finished asking the question about RDS connectivity. Some great answers on that so thanks!

What I’m wondering now is where to manage the entrypoint/restrictions of the entire network. Gruntwork.io suggests a “management” account/VPC where you peer VPC’s to dev stage and prod, add a VPN or bastion host, then restrict developer access to specific resources through the VPN/bastion.

I’m curious what setup/best practices you guys are using for this when all employees are remote.


My current setup is a bastion per account (which is nice since I can see sessions for each env), but managing 3 extra servers seems overkill if I can converge the entry point further.",1,aws,2020-10-13
j1dtjr,MediaPackage .SMIL files with S3 source file?,"Hi, I created my .SMIL file for MediaPackage and placed it in an S3 bucket near the original .mp4 file. I set the src in the smil file to ""myvideo.mp4"" as the name of the file and even tried many other solutions.

Does someone know how to stream files from S3 bucket with MediaPackage?",3,aws,2020-10-13
j1dpah,Network Issues Running SQL 2017 on Windows 2019,"I'm testing deploying SQL 2017 on a z1d instance running Windows 2019 and I'm running in to an issue. I've got a ticket open with AWS Support, but thought I'd run it by the group in case any of you have dealt with this before.

I've found that if I set tempdb up to run on local storage, if I then perform any operations that are tempdb heavy (my test transaction is an ALTER on an 800 GB table), I will lose network connectivity to the instance after a few minutes. No matter how long I wait, the only way I can get back in is to reboot the box. I've disabled RSS, as recommended by the AWS docs. And TCP offloading has been deprecated out of 2019, so I don't think that would be it. I've also confirmed the instance is using the latest and greatest drivers for NVMe and ENA. Any ideas on other things I should be looking at that could cause this behavior?",1,aws,2020-10-13
j1dndd,Week of Sept 28th - What are your favorite AWS Tips?,Share your AWS Tips,11,aws,2020-10-13
j1dddo,Initial setup for EKS along with dB and reporting,"Hello guys,

I have deployed the complete application on my old organisation kubernetes instance and I want to do the same at my new organisation but they have told to go with AWS EKS. I don't have the faintest idea about it.


I have to build my image every week with my files and push the data from the scripts to a dB and from the dB push to a reporting toll to create a dashboard.


Data from Scripts running on kubernetes cluster - &gt; postgres/sql dB - &gt; Reporting tool

Is everything possible on AWS?

Current setup in Old organisation-
 2gb memory for each pod and 50 pods atleast.
Send data to postgres dB and then to power bi.


I am not sure of the ec2 instance I should buy or if reporting is possible like what I want. I want a dynamic reporting tool which keeps taking data from the dB.",1,aws,2020-10-13
j1d70t,Email change automatically confirms the email without verifying.,"I'm using Amplify with Cognito for my user authentication, User can sign to my application with email/password. Auth flow User register an account with email/password Cognito sends a verification code email address and user verify the email by the code and then the user can sign into the app. 

But after sign When the user changes the email address Cognito automatically verifies the email address without verifying with the code.",1,aws,2020-10-13
j1d515,Control tower created account is messed up,"I created an account for a new application through the control tower account factory. The account was created successfully and I can login as administrator, but I don't seem to have any permission to do anything, I can't even view the VPC for instance. I can only guess it got messed up during creation, but I don't see any errors. I can't even create a support case for this, because I don't have the necessary permissions.

Any ideas on how to troubleshoot this?",1,aws,2020-10-13
j1cdkr,Using Google Domains Email Forwarding without transferring?,"I registered a domain `example.com` with AWS Route53 but I'd like to use Google Domains Email Forwarding to forward emails from `@example.com` to my `@gmail.com` account.

Can I point the domain name servers to Google Domains and use Google Domains Email Forwarding, without transferring the domain to Google Domains?

Basically: billing and registration would be managed by AWS, but the DNS configuration would be managed by Google Domains.",0,aws,2020-10-13
j1c20c,How can AWS security group changes be applied immediately?,Why it doesn't take time to propagate across different AZs?,6,aws,2020-10-13
j1bwy2,Advice on using RDS with db-migrate,"I have set up a RDS instance and need to make it publicly accessible to make it play nice with my db-migrate process that is called during my deployment and locally during testing.   


However I understand that making an RDS instance public is bad practice. I now want to know if I set my RDS instance back to being publicly inaccessible, how would I trigger my build process against it to execute the database migration scripts.",1,aws,2020-10-13
j1bjty,Session Manager,I am staring to use Session Manager.  I understand that it will allow us to manage resources centrally. Anyone have a bad experience or advise to offer?,1,aws,2020-10-13
j1ax6u,Using S3 as a storage in Angular app,"I am developing a multiuser web app using Angular for frontend + PHP for backend. I want to use S3 as a storage of users' files.

Each user should have the abilty to operate with his files (list all files / download / upload), but must not see other users' files - security is very important. Eventualy, I should introduce user groups and file sharing, so I was thinking of create IAM user for each user in my app.

I have a conceptual question should I communicate with S3 on frontend (Amplify?) or backend (AWS SDK for PHP)?",1,aws,2020-10-13
j1aluk,Can lambda connect to RDS using IAM authentication?,"Hello there,

  
I am using a lambda function to run a sql script on a Postgre RDS DB. At the moment I am connecting to DB using username and password. Is it possible to connect to DB using RDS IAM Authentication?

Code for lambda is written in .NET core.

Thanks in advance.",1,aws,2020-10-13
j1abon,Public SQL server to S3 parquet files: Best practice?,"# The Scenario 
There is a publicly accessible SQL database, with data going back several years. 
Each day, new data are appended in the form of 1 minute snapshots of several sensors.

**Each day, I would like to download yesterdays data, and save it as a daily parquet file to an s3 bucket.**

# My currently solution
Use AWS Lambda with python 3.7, and a pandas and pyodbc layer to give me access to those modules.
The function runs a query on the server, then saves that data in parquet format to the S3 bucket.
Code is below.
I plan on adding in an SNS topic that gets pushed to in the event the function fails, so I can get an email letting me know if it's failed.

It does seem to work, but as I am very very new to all of this, and I'm not even sure if Lambda functions are the best place to do this or whether I should be using EC2 instances isntead. I wanted to ask **Is there a better way of doing this and is there anything I should watch for? Several stackoverflow posts suggest lambda might auto-retry on fails continuously, which i'd like to avoid!**

Thank you for being patient with an AWS newbie!

best,

Toast


        BASESQLQUERY = ""SELECT * FROM TABLE""


	def getStartAndEndDates():
		"""""" Return yesterdays and todays dates as strings """"""
		startDate = datetime.now() - timedelta(3)
		endDate = datetime.now() - timedelta(2)
		datesAsStrings = [date.strftime('%Y-%m-%d') for date in [startDate, endDate]]
		return datesAsStrings 


	def runSQLQuery(serverAddress, 
				databaseName,
				username,
				password,
				datesAsStrings):
		"""""" Download yesterdays data from the database """"""
		with pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+serverAddress+';DATABASE='+ databaseName +';UID='+username+';PWD='+ password) as conn:
			yesterday = datesAsStrings[0]
			today = datesAsStrings[1]
			fullSQLquery = BASESQLQUERY + f""WHERE TimeStamp BETWEEN '{yesterday}' AND '{today}';""
			dataReturnedFromQuery = pd.read_sql_query(fullSQLquery, conn)
		return dataReturnedFromQuery



	def lambda_handler(event, context):
                """"""Download yesterdays SQL data and save it as a parquet file in S3""""""

		datesAsStrings = getStartAndEndDates()
		startDate, endDate = datesAsStrings

		logging.info(f'Downloading data from {startDate}.')
		try:
			logging.debug(f'Running SQL Query')
			dataReturnedFromQuery = runSQLQuery(serverAddress=SERVER_ADDRESS,
										    databaseName=DATABASE_NAME,
										    username=USERNAME,
										    password=PASSWORD,
										    datesAsStrings=datesAsStrings)
			logging.debug(f'Completed SQL Query')
			
			filename= startDate.replace('-','') + '.parquet'

			wr.s3.to_parquet(
				dataReturnedFromQuery ,
				f""s3://{BUCKET_NAME}/{filename}"")
		except:
			logging.info(f'Failed to download data from {startDate}.')
			raise
		
		logging.info(f'Successfully downloaded data from {startDate}.')
		return {
			'statusCode': 200,
			'body': ""Download Successfull""
		}",1,aws,2020-10-13
j19dyu,download webpage,"I have a home page web url. I want to execute the following by Lambda function.

1. Save the web page(this includes dependent css,js etc)
2. Upload it to S3

Which language/library support would help to make it easy. 

I have these concerns:

1. I think a http get on home page may download the home.html  but  not  dependent resources. 

2. The same language/library to support uploading the same files to S3. 

Is there any easy solution.",1,aws,2020-10-13
j19twk,Service agreement on getting shut down,What level of service agreement does AWS provide for this type of scenario where American government asks Amazon to stop doing business with a certain country? Would the account holders of that particular country be blocked and their accounts be frozen so that they can no longer access to servers or data on AWS cloud?,0,aws,2020-10-13
j19ugd,why sagemaker use jupyter notebooks?,"I was just trying to understand how SageMaker is working from a high level point of view.

It seems that SageMaker launch JupyterNotebooks. Why? 

I'm new to JupyterNotebooks too. After a quick search it seems to me that JupyterNotebook is a kind of ""environment"" for several languages....it seems a nice web application and you can do nice things, but I don't really understand what it has to do with a programming environment to run machine learning models...

It is an alternative to build a Machine Learning systems on EC2 installing all the environment yourself?

But still, why a notebook application?

Really I miss something. Maybe some high level idea about that will enlighten me",1,aws,2020-10-13
j14vrb,AppStream client app authentication?,"
How can I authenticate the client app within AppStream without having to have user enter their credentials twice?

There is loads of documentation on how to access appstream using saml authentication. However this doesn’t do anything for authenticating the app WITHIN AppStream.

My situation is running a FileMaker client within an AppStream instance. I want to user to only need to login with Okta that is federated with AppStream. However, once logged into AppStream instance, still requires FileMaker login.

FileMaker is also federated with Okta but requires signing in again since this is technically a new instance of the operating system and doesn’t know I already logged in.

Can I pass token from user’s browser through AppStream so we don’t require two logins?

Any suggestions on a thread I can pull on this one would be appreciated.",1,aws,2020-10-13
j17bhu,How to prioritize EC2 instances in a target group for ELB?,"Suppose I have a load balancer and the request gets forwarded to this target group that consists of 3 EC2 instances. Those instances are exactly the same, same web servers and are meant to be a back up server for when their other instances die. 

&amp;#x200B;

But, is there a way to prioritize the requests being sent through? So if the 1\_instance health check fails, then the requests gets redirected to 2\_instance....etc",1,aws,2020-10-13
j16oio,Contacting Amazon SES support as a non customer?,"I'm currently in the middle of the wonderful experience of trying to get DMARC and DKIM set up properly.  We only have one SaaS product that we're aware of that use Amazon SES to send their email and we don't use them ourselves.

DKIM signing has been set up fine for this application, but there's a bucket load of other emails coming from Amazon SES which include a DKIM signature *not* supplied by our SaaS provider.  As a non-customer, how can I contact Amazon to try to get an idea of who is sending email on behalf of our domain and whether it's legitimate or not.  However, Amazon say they don't have a telephone number for support for security reasons (one of the more rubbish reasons I've seen for not providing telephone support) and insist you log all support requests online.  I can't do this as I don't have a login for AWS, and I'm not about to create one just to log one single enquiry.

Does anyone have any suggestions on where I might be able to go from here?",0,aws,2020-10-13
j15mil,Certificate Manager Public SSL Cert with ECS?,"If I have an API hosted on ECS and a static front-end hosted in an S3 bucket, how can I serve them both at a domain name with the same public ACM SSL cert?

If it makes things easier, I don't mind having the static part served at www.site.com and the API at api.site.com

I got the S3 part set up with CloudFront, but I'm not sure how to get SSL for my API on ECS. I read something about CloudFront for dynamic content but it's not clear to me if that's relevant.

This is for something tiny with a handful of users max, so I'm trying my best to avoid things like load balancers or nat gateways which would spike my costs for something that isn't all that critical (not business-related). I do have a nat instance though.

Thanks!

**EDIT**: If I can't do this without a load balancer (not even with ECS service discovery?), what is the closest cheapest alternative for hosting an API behind the ACM public cert? I would even use let's encrypt but I've heard it's much easier to use the ACM cert if possible.",2,aws,2020-10-13
j13bd7,"For Ubuntu Instance, should I configure ufw or will security groups handle the same thing",Just want to know if the ufw configuration is a waste of time.,1,aws,2020-10-13
j12tvm,How are you handling inter/hybrid cloud connectivity from AWS?,"Hey all,

For those of you who are managing resources within AWS VPCs and other places like GCP, office networks, on-prem datacenter, etc. - How are you managing? A PITA or pretty seamless?

Just curious to see how others are handing their specific use cases. Are you standing up your own IPsec VPNs or DirectConnects? Or are you using partner providers like Megaport, etc.? ...Or are you just using public internet?

My current company has the full gambit - GCP, AWS, VPN users and multiple offices and on-prem DCs, and we have internal network connectivity between all of them (including 200Gb worth of GCP Interconnect links). We've actually got a datacenter cabinet deployment in Equinix for switching between providers, but that's probably because we're pretty heavy on the physical datacenter side. Plenty of dark fiber and wavelength transport terminated there.

How about yours?

Cheers",1,aws,2020-10-13
j129il,I'm trying to define how the AWS docs are so bad..,"But I can't put it down to any one thing. They're too verbose, I can't actually find out what I have to do and a lot of things dealing with the console aren't there anymore.

Is it just me or is this some of the most difficult documentation to force your way through? Not because it's necessarily complicated, it just never seems to get to the point.

But its like more than that. I hate it. Why do I hate their documentation so much?",69,aws,2020-10-13
j0x10p,"I want to deliver videos on a website im creating, does the combo aws + cloudfront is enough ? is there any tips you can give me ? like how can i convert video in a better format or any thing that cross your mind ?",,1,aws,2020-10-13
j0x53t,First time deploying to AWS and completely lost,"I normally use Heroku but I was wanting to learn AWS so I thought I would deploy my MERN stack application there. Now AWS seems way more convoluted than Heroku but [this](https://jasonwatmore.com/post/2019/11/18/react-nodejs-on-aws-how-to-deploy-a-mern-stack-app-to-amazon-ec2) tutorial almost got me there. I am on Windows so I am using Putty as my SSH to connect to my EC2 instance. I got all the files in there but I think the issue might be with my proxies. So first off here is my [frontend](https://github.com/TDonnally/finstagram-frontend/) and [backend](https://github.com/TDonnally/finstagram-backend) repositories. On my local host the way I connected the stacks was with this line in my package file:  `""proxy"": ""http://localhost:5000"",`  I think this is the difference between my project and the tutorials as he has all sorts of files for the frontend and Nginx steps.

The error I get when I go to my  Public IPv4 DNS is 500 Internal Server Error. Now in the tutorial before I did the Nginx step it said ""welcome to server"" and it was running the default Amazon code.

If anyone could help thank you very much!",2,aws,2020-10-13
j0w4s0,CI CD for Django rest API. Has anyone done this successfully?,Please let me know I am struggling to set this up!!!,0,aws,2020-10-13
j0uzww,Accessing Reports from a linux command line?,"Specifically, I'd like to pull a copy of the CloudFront Popular Objects Report (in csv format) to my local linux server so a script I have can parse the data for me.

The parsing is done (that was the easy part). But I need to be able to pull the .csv report. Can it be done?",0,aws,2020-10-13
j0ot6y,Simple web application design,"Hi all

I'm currently learning AWS, I think this would be a cool little project to help. 

In my community there's a bridge that's often closes to poor weather, construction etc. They have a Twitter feed which provides updates but also a REST API, so I can query the status and it comes back with 'active' etc.

I want a simple web page that states if the bridge is open or closed.

Would you do this in elastic beanstalk? Lambda? Just an EC2 Instance with a simple website running?

Would love to nail the architecture down before I start working on this 
Thanks!",3,aws,2020-10-13
j0pmhc,AWS Newbie,"hey all,

&amp;#x200B;

I'm working on an application and well, I'm new to this entrepreneurial journey. 

I don't have much knowledge on AWS, and I was wondering if you could help me with my questions below:

My monthly charges are:

&amp;#x200B;

* Elastic Compute Cloud US West (Oregon)

\- Amazon Elastic Compute Cloud running Linux/UNIX

$0.0208 per On Demand Linux t3.small Instance Hour 

(626.000 Hrs) 13.02$

\- EBS

$0.00 per GB-month of General Purpose (SSD) provisioned storage under monthly free tier 

(25.792 GB-Mo) 0.00$

* Simple Notification Service US West (Oregon)

\- Amazon Simple Notification Service USW2-Requests-Tier1$0.00

First 1,000,000 Amazon SNS API Requests per month are free

(83.000 Requests) 0.00$

\- Amazon Simple Notification Service USW2-SMS-Price-ROW$0.98

Variable Pricing for Amazon SNS SMS Notifications to Rest Of World (non-US)

(0.976 Dollars) $0.98

**Can you help me understand EC2 a bit better?** 

**How is AWS incorporated in app development? Like, not to sound extremely dumb but how is hosting connected to the website or app?** 

**And how am I using my app/website that would lead to 626 hours** 

&amp;#x200B;

https://preview.redd.it/7egk6piqeop51.png?width=2408&amp;format=png&amp;auto=webp&amp;s=f346960600044021a338f415311247e2581ecadd

Also, SNS service:

**How do you know how much your limit should be increased to in order to have enough transactional and promotional sms?**

**How can I increase the number of OTP's I'd like to send out?** 

for example, for every sms, I'll be charged: 0.03195$

&amp;#x200B;

https://preview.redd.it/lxpl3t9reop51.png?width=786&amp;format=png&amp;auto=webp&amp;s=a0df9ba63ce42de2c0fb782e4ea08183c55ccad3

thank you so much guys, it feels good to know that I have a platform that could support me with this.",1,aws,2020-10-13
j0sbbb,How to route a Route53 subdomain to Google Cloud Platform?,"I have a domain registered in AWS (*mydomain.com*) and I want to create a subdomain (*gcp.mydomain.com*) which will route to any services I have in GCP.

[AWS Setup](https://imgur.com/a/Luoa6mF)

[GCP Setup](https://imgur.com/a/alz1IPp)

Using [DNS Lookup](https://dns-lookup.com/) I can see *mydomain.com* returns the AWS name servers just fine but I get no response for *gcp.mydomain.com*.

Does anyone know what I'm doing wrong?

*mydomain.com* is not the real domain.",1,aws,2020-10-13
j0sb9d,AZ for my subnets in US East 1,"Does it make a difference which AZ I put my subnets in? Also, should I start with all my subnets in one AZ and then start putting them in different AZs? Feel like I’m exposing myself to single point of failure. Is any one AZ in Us East 1 more performing then the rest?",1,aws,2020-10-13
j0rxvq,Do I need to allow traffic coming from Route 53 health checkers in my EC2 security group?,"In Route 53, there about 15 health checkers under the hood that will constantly ping our EC2 instances when we create a health check endpoint. Are the request and port automatically configured by AWS internally? Or do we need to explicitly allow these public IP addresses (shown in the health check endpoint settings) in my SG?

Compared to ELB health checks, I think we normally reference the ELB SG to be allowed on the EC2 SG. I am wondering how Route 53 health checks differ with ELB.",3,aws,2020-10-13
j0qmby,Caddy or Nginx ?,"We need to automatically and programmatically generate domain names and certificates for customers (potentially 10-100Ks of customers) in a scalable, reliable and responsive way.

We have a serverless infrastructure (cloudfront / S3 / with dynamodb + lambda + api gate way serverless backend), so ideally we would have liked to use route 53 and AWS certificate manager and route the domains to our cloudfront distribution but there is no way to attach the customers' certificates.

Hence, we've been thinking about nginx or caddy as alternative. What are your thoughts ?  Is there a way to do this serverless ?

Or should we go for nginx or caddy proxy that generates domains and certificates on the go behind an  ELB ?

&amp;#x200B;

Edit: We're not a hosting provider. We're a SaaS platform that create content for users, and some might want to use their own domain names, so we need to be able to point those to our cloudfront distr (Angular frontend), but also have their certificates working as well.",16,aws,2020-10-13
j0qki4,How to create lambda applications with many functions? (CloudFormaton limit),"Hi, I'm creating a website that needs many functions. Currently, we have multiple repositories (users, processing, management, etc.), each repo with multiple functions each serving a specific logic.

We just hit the CloudFormation limit of 200 resources (tho we have only 25 functions) in one of the repos. As it's not logical to separate the same type of service (e.g. two user repos), any ideas on how to handle this? A repo for each lambda (e.g. EditImageDetails)? Separate the services further? We do need the lambdas to access a shared general code and that's a problem too.

Thanks for the help!",5,aws,2020-10-13
j0qdkf,Custom Servers for Games?,"Hi all,

Obviously I'm a noob in AWS but I want to leverage the cloud. I'm planning to setup a custom server for this game (Left 4 Dead 2) to play with my friends and I'm using EC2. When I tried to ping it from my game to do a test, I can't seem to connect to the server I created. TCP and UDP ports have been opened under security groups and I also enabled it from the Windows Defender Firewall. Now my real question, is there anything I may have missed? Or perhaps hosting a custom server in AWS is not possible?",21,aws,2020-10-13
j0osrv,Cognito: Static S3 website or API gateway/lambda?,"Hi all.

I’m working on a personal project, and looking to secure access to an “admin” site that allows authorised users access to certain functionality (sending SMS through Twilio etc). I’ve read a bit around using Cognito to restrict access but I’ve a couple of questions regarding using this with S3 and Lambda.

Currently I’ve a static website in S3 using CloudFront. My initial thought was to create my static webpages, and restrict access to the lambdas they call using Cognito, similar to the “wild rydes” tutorial. However, webpages with restricted functionality are still viewable before the redirect kicks in, although obviously these pages won’t allow unauthenticated users to use the functionality. I’d prefer to not allow these pages to be viewed.

I’m now considering a completely serverless approach. My thought is to create a registration page and login page on the static website, retrieve the JWT and redirect the user to a serverless drive sub domain, admin.website.com. This would be fronted by API gateway calling lambdas to build the pages and run the functionality for the admin section. Initially I’ll pull the webpages in from a private S3 bucket and build the pages myself, though I’d maybe look to move to React for building the customised webpages.

Is there anything I’ve overlooked in this approach? If this is an appropriate solution, do you have any thoughts on how I can improve this approach? I’d appreciate your thoughts on this!

Thanks!",12,aws,2020-10-13
j0pij6,Out of my depth here. Got asked to set up a Windows VM.,"I'm totally out of my depth and got asked to set up a Windows VM with .net, SQL, IIS to allow someone to RDP onto it and set up a web application. 

I guess this can be done with EC2? But how do I do all the other things? I don't even know what to google and whether this could just turn into an expensive waste of time. Any help?",1,aws,2020-10-13
j0l1eh,AWS API Gateway: HTTP vs REST,[https://www.learnaws.org/2020/09/12/rest-api-vs-http-api/](https://www.learnaws.org/2020/09/12/rest-api-vs-http-api/),2,aws,2020-10-13
j0ocmx,AWS CDK - Validation Error; Codepipeline,"Morning all, 

I'm running into some problems with AWS CDK, I'm trying to create 2x AMI's in a single build job. Here is my code:

```typescript
let jenkinsNodeTypes = [""master"", ""slave""]
    for (let value of jenkinsNodeTypes) {
    const jenkinsDeployment = new codebuild.PipelineProject(this, `JenkinsDeployment-${value}`, {
      buildSpec: codebuild.BuildSpec.fromSourceFilename(`jenkins-${value}/buildspec.yml`),
      projectName: 'JenkinsInfraStack',
      environment: {
        buildImage: codebuild.LinuxBuildImage.AMAZON_LINUX_2_3
      }
    })
      
    const jenkinsSourceOutput = new codepipeline.Artifact();
    const jenkinsBuildOutput = new codepipeline.Artifact(`jenkinsBuildOutput-${value}`);
    
    const jenkinsPipeline = new codepipeline.Pipeline(this, `jenkinsPipeline-${value}`, {
      pipelineName: 'jenkinsPipeline',
      restartExecutionOnUpdate: true,
    })

    jenkinsPipeline.addStage({
      stageName: 'PullFromGit',
      actions: [
        new codepipeline_actions.GitHubSourceAction({
          actionName: `PullFromGithub-Jenkins${value}`,
          owner: '&lt;OWNER&gt;',
          repo: '&lt;REPO&gt;',
          oauthToken: cdk.SecretValue.secretsManager('GitHubToken'),
          branch: 'master',
          output: jenkinsBuildOutput,
          trigger: codepipeline_actions.GitHubTrigger.WEBHOOK,
        }),
      ],
    })

    jenkinsPipeline.addStage({
      stageName: 'Build',
      actions: [
        new codepipeline_actions.CodeBuildAction({
          actionName: `buildJenkins${value}`,
          project: jenkinsDeployment,
          input: jenkinsBuildOutput,
          outputs: [jenkinsBuildOutput],
        })
      ]
    })
  }
```

but I'm getting this output :/ 

```
{path}/node_modules/@aws-cdk/core/lib/private/synthesis.ts:151
    throw new Error(`Validation failed with the following errors:\n  ${errorList}`);
          ^
Error: Validation failed with the following errors:
  [JenkinsInfraStackGBP/jenkinsPipeline-master] Both Actions 'PullFromGithub-Jenkinsmaster' and 'buildJenkinsmaster' are producting Artifact 'jenkinsBuildOutput-master'. Every artifact can only be produced once.
  [JenkinsInfraStackGBP/jenkinsPipeline-slave] Both Actions 'PullFromGithub-Jenkinsslave' and 'buildJenkinsslave' are producting Artifact 'jenkinsBuildOutput-slave'. Every artifact can only be produced once.
    at validateTree ({path}/node_modules/@aws-cdk/core/lib/private/synthesis.ts:151:11)
    at Object.synthesize ({path}/node_modules/@aws-cdk/core/lib/private/synthesis.ts:25:5)
    at App.synth ({path}/node_modules/@aws-cdk/core/lib/stage.ts:175:23)
    at process.&lt;anonymous&gt; ({path}/node_modules/@aws-cdk/core/lib/app.ts:112:45)
    at Object.onceWrapper (events.js:421:26)
    at process.emit (events.js:314:20)
    at process.EventEmitter.emit (domain.js:486:12)
    at process.emit ({path}/node_modules/source-map-support/source-map-support.js:495:21)
```

Any help would be much appreciated. 

Thank you.",2,aws,2020-10-13
j0k4p1,Create CloudWatch Alarm for EC2 running over 2hr?,"Is it possible to create a CloudWatch alarm to send an SNS when an EC2 is running over 2hr? Currently I have this (in yaml):

    F1RunningAlarm:
        Type: AWS::CloudWatch::Alarm
        Properties:
          AlarmName: F1LongRunning
          AlarmDescription: Alarms when the F1 instance is running for a long time
          AlarmActions:
            - !Ref 'F1SNSTopic'
          MetricName: CPUUtilization
          Namespace: AWS/EC2
          ComparisonOperator: GreaterThanOrEqualToThreshold
          EvaluationPeriods: 1
          Period: 7200
          Statistic: Average
          Threshold: 0
          TreatMissingData: notBreaching
          Dimensions:
          - Name: ""InstanceId""
            Value:
              !Ref EC2

Currently it Alarms immediately as the CPU utilization passes 0 in the 2 hour period. I'd like it to alarm after the CPU has been running for more than 2 hour. Is this possible?",9,aws,2020-10-13
j0ieri,Python Boto3 SNS with Lambda &amp; bucket,"Can anyone give me a code example of a Lambda Python script that will scan through a S3 bucket, if no files there within a specified timeframe it will send an alert to an SNS Topic?  Cheers.",2,aws,2020-10-13
j0gkte,Python Lambda: How to convert DynamoDB json from stream into regular json?,"Using Python Lambda as a trigger for DynamoDB stream, how do I convert the provided DynamoDB json into regular json?",4,aws,2020-10-13
j0fzam,Is it possible to spin up and spin down CloudHSM in order to save money?,"It looks like it's possible to save and restore backups of the HSM... Does this contain all the private keys contained within it? Could I programmatically spin up the HSM, sign a few documents with it, then spin it down again?",9,aws,2020-10-13
j0fvt7,"How to set up ""push"" notifications with Duo Mobile on AWS MFA","Hi all, 

I'm not super familiar with how MFA works, but I set it up on my personal AWS account by scanning the QR code presented by AWS with Duo Mobile. I was hoping that this would allow my to authenticate in a similar way that I do with LastPass (LastPass sends me a ""push"" notification that I simply need to click on my iPhone). However, with AWS, I need to enter in a 6 digit code that Duo Mobile provides me for my AWS account. How do I make it so that my AWS sign in using the ""push"" method instead of requiring me to manually enter in a code?",6,aws,2020-10-13
j0droo,How are you securely tunneling to RDS instances in a private subnet?,"I was curious what process people are using to securely access their RDS databases in a private subnet.

My current process is:

* Generate &amp; send temp ssh keys to the bastion in public subnet using ec2-instance-connect
* Bastion is firewalled off to only allow ingress from my ip
* SSH tunnel to the RDS instance through the bastion to use psql

Is there a more secure way to do it? I'm not fond on having to patch/maintain a bastion host. Perhaps a managed solution that doesn't require a bastion host to get the tunnel? I checked out Secure Session Manager but that seems to still require an EC2 server in the middle.",42,aws,2020-10-13
j0cakk,Complex AWS EKS / ENI / Route53 issue has us stumped. Need an expert.,"Context:

We are working on dynamic game servers for a social platform ([https://myxr.social](https://myxr.social/)) that transport game and video data using WebRTC / UDP SCTP/SRTP via [https://MediaSoup.org](https://mediasoup.org/) 

Each game server will have about 50 clients

Each client requires 2-4 UDP ports

Our working devops strategy

[https://github.com/xr3ngine/xr3ngine/tree/dev/packages/ops](https://github.com/xr3ngine/xr3ngine/tree/dev/packages/ops) 

We are provisioning these game servers using Kubernetes and [https://agones.dev](https://agones.dev/) 

Mediasoup requires each server connection to a client be assigned individual ports. Each client will need two ports, one for sending data and one for receiving data; with a target maximum of about 50 users per server, this requires 100 ports per server be publicly accessible.

We need some way to route this UDP traffic to the corresponding gameserver. Ingresses appear to primarily handle HTTP(S) traffic, and configuring our NGINX ingress controller to handle UDP traffic assumes that we know our gameserver Services ahead of time, which we do not since the gameservers are spun up and down as they are needed.

Questions:

We see two possible ways to solve this problem.

Path 1

Assign each game server in the node group public IPs and then allocate ports for each client. Either IP v4 or v6. This would require SSL termination for IP ports in AWS. Can we use ENI and EKS to dynamically create and provision IP ports for each gameserver w/ SSL? Essentially expose these pods to the internet via a public subnet with them each having their own IP address or subdomain. [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html) We have been referencing this documentation trying to figure out if this is possible.

Path 2

Create a subdomain (eg gameserver01.gs.xrengine.io, etc) dynamically for each gameserver w/ dynamic port allocation for each client (eg client 1 \[30000-30004\], etc). This seems to be limited by the ports accessible in the EKS fleet.

Are either of these approaches possible? Is one better? Can you give us some detail about how we should go about implementation?",13,aws,2020-10-13
j0akc1,"Saving $200,000 in a year on RDS - ConvertKit year in review",,13,aws,2020-10-13
j0a8hy,Possibilities to get DevOps engineer job with CSE experience.,"I have accepted CSE-2 devops recently and I’m glad that I got this role. I have 7 plus years System admin, devops experience. I’m 30m. 
My question is will this job help me get higher pay devops engineer or software developer roles in the future in AWS or outside ? 
I’m reading from Glassdoor reviews (they are a bit harsh) that this can be a dead end job and the CSE will not get any hands on experience that a typical devops engineer needs.",10,aws,2020-10-13
j09r56,A question about ACM fees,"Hi folks, I'm trying to evaluate use of a [terraform module](https://github.com/squidfunk/terraform-aws-cognito-auth) for serverless authentication for a web app  and am a little confused about ACM (AWS Certificate Manager) pricing. I've looked at AWS's [webpage on ACM pricing](https://aws.amazon.com/certificate-manager/pricing/) which seems to show that running the module above with ACM will cost $400/month + certificate issuance, but I read on [a blog](https://dpron.com/ssl-with-aws-certificate-manager/) that it's free and now I'm extremely confused.

This is just for a hobby web app so I absolutely cannot afford the pricing if true, please help!",5,aws,2020-10-13
j062ni,"Serving product photos from S3 public bucket, good or bad idea?","Hi all, I'll try to describe my situation succinctly, here goes:

We are building a (Wordpress) stock photography website that uses S3 to store the actual image files ie. *products,* file sizes vary between 10-25mb. We are currently at the stage where we must decide the best route to take regarding our future workflow when adding new products and serving the watermarked, downscaled *product photos,* average size for one is 60kb. I'll present options A and B:

Option A: Our website hosting has good amount of disk space, but I was thinking about how will things work out when we add thousands of products along with the product photos. By default, Wordpress stores the product photos in the Media Library, I disabled the automatically generated thumbnails because they are really not needed here and would take extra space in *uploads-folder*. By serving product photos from our server, the uploads-folder would soon have a large amount of unorganized (apart from filenames) product photos. 10,000 files would take \~600mb? That's not too bad...

Option B: I installed a Wordpress plugin that allows us to set product photos from URL, and by making a watermarked-bucket with public access (public files as well), I can use the aws-urls and our site now shows product photos straight from the public watermark-bucket. When I visit our site, I can browse products with pagination, 30 products per page and I see the aws-hosted product photos that are slightly downscaled with CSS. Each page I see has 30\*60kb= 1,8mb worth of photos.

Finally the question: **Does Option B make any sense**, will it become expensive when visitors browse our site and the product photos are presented from S3? In Wordpress dashboard the thumbnails are also served from the S3 bucket.

I personally am not the one that researched the AWS pricing for this project, but I would like to learn the practicalities as well, if you could provide me some insight (maybe with examples with X amount of visitors) about how the GETs etc work.

A big thanks to anyone who could help me out! :)",3,aws,2020-10-13
j06a5t,Anyone know and good courses on networking in AWS?,"I recently tried Amazon ECS &amp; Fargate Master Class on Udemy but found the use of CloudFormation to automatically create and configure a lot of the networking components, (VPCs, security groups, etc) confusing as those are the components I would like to learn more about. Ideally looking for something that starts in the AWS dashboard and then perhaps uses those tools perhaps towards the end of the course. The end goal is to be comfortable setting up ECS and other AWS services that can integrate with ECS clusters.",1,aws,2020-10-13
j05uk0,AWS is adopting Google deprecation policies and want your feedback :),,86,aws,2020-10-13
j05huf,AWS Service Catalog | Feedback and Experiences Using?,"Looking into using AWS Service Catalog for a large enterprise.  Wanted to hear feedback, experiences, pros/cons, challenges, gotchas, missing features, use cases?

Thanks in advance for any help!",1,aws,2020-10-13
j03qek,What is the largest AWS environment you have worked with? How do you manage it!???,"Hello,

  
I work for a SaaS company and our infrastructure is hosted in AWS. For most part until now we have been managing our cloud with homegrown scripts, terraform, and AWS dashboards.

However, we are rapidly increasing our cloud infrastructure both in EC2 as well as server less side.   
I'm looking for the pointers from the community about when to start looking for third party solutions for management, if at all. 

Also, I'm looking for some general information about AWS environment that you have personally worked with. How many resources (EC2 Instances, Lambda instances, RDS, ECS cluster sizes - number of containers) etc? How many accounts? Did you use any third party solution,? If yes, could you please mention those too. Idea is I want to compare it with our environment and see what fits to us.

We recently started experimenting with Cloud Custodian - but at times we are facing API throttling issues. This worries me as it may impact our workloads too.

Thanks in advance.",9,aws,2020-10-13
j04134,Can we make EC2 instances in the web tier as Private?,"We have Typical 3 tier architecture having Web, App and DB. Can we make EC2 instances in the web tier as Private? and allow incoming traffic only through ALB? AFAIK we can apply an SG only allowing connections from the SG of the ALB. But What if our Private EC2 instance has to return response back to the client? How it'll be routed through ALB as ALB is mostly used for managing incoming traffic. Also for outgoing traffic can we configure something like Private EC2 instance -&gt; ALB -&gt; Internet? If yes then how? So, is there any way for private EC2 instances to communicate to internet without assigning them public IP?",1,aws,2020-10-13
j041h6,Transitioning from SysOps to DevOps,"I am currently employed as a Systems Engineer for a consulting company which serves many clients here in Italy.
I'm mainly a Windows Admin, due to exposure, and have no formal training. I work with all the usual hassle (vmWare, networking, WS, some Linux machines, security, AD,....) but due to personal reasons I would like to relocate to a different country. 
I see many job offers as DevOps and after having a look around, I got interested in moving my focus into cloud based infrastructures, mainly AWS.
I grasp OOP concepts and have some personal experience in programming or scripting tools for my job (VBA and Powershell).
If you were in my position, how would you move ahead in order to improve your knowledge of DevOps and show a future employer that you have the skills he requires in order to work in this field?
Would you go with certs such as (AWS SysOps engineer)?
Which (paid if necessary) training would you undergo?

Thanks.",34,aws,2020-10-13
j04atj,Is it possible to reset AWS?,"I used my personal email to kickstart my company a few years ago, before moving it to its own account. Now I want to use the account for a few personal projects and, although I removed all I could see related to my previous company, I just want to make sure that I'm starting with a clean AWS account.

I thought of deleting the account and creating a new one, but apparently you cannot open new accounts with email addresses that are associated with closed accounts.

Is there a way of resetting all of AWS to defaults, or to open a new account with my personal email?",5,aws,2020-10-13
j04cpc,Best practise for deployments,"Hey all 👋

I’ve been asked with getting various applications up and running in our new AWS Environment.  I’m looking to build Jenkins, Vault etc (I know, why not use Codepipeljne/KMS etc, it’s a migration at the moment, well be swapping to these in the future).

I’m looking to build this up via CDK, but unsure on the best way to approach, which of these would you suggest?

1. Building the instances and configuring with Ansible (either tower, or Systems Manager - playbooks / roles will be stored in GitHub)
2. Build the AMI with Jenkins Master/Slave baked into the image? 

Ideally I’d like this to be immutable, slaves with be in a ASG to auto scale when needed. 

Many thanks,",1,aws,2020-10-13
j0462w,Question on S3 lifecycle policies,"I have a small collection of stuff (few hundred GB) that I keep on a Synology in my house, and I want to keep an offline copy in AWS. I don't expect to access it unless the Synology goes kablooey, so Glacier Deep Archive is fine with me.  I'm confused about lifecycle policies, though.

I have an S3 bucket and upload music to it via Cloud Sync.  I want to transition stuff out of this bucket and into glacier, so I have a policy which transitions everything in the bucket into deep archive after 7 days.  I know this might be a (relatively) expensive way to do it, but even the expensive way is like $3 so I don't care.  But after I've moved everything to deep archive, I want to delete all versions in the S3 bucket so I'm not storing stuff there permanently. 

If I create a lifecycle policy to transition to deep archive, do I also need to expire current and previous versions from the S3 bucket, or does the transition to deep archive remove things from the bucket as part of the policy?  I assume ""transition"" means ""remove from one place and put into another"" but ass-u-me and all that.

Thanks!",2,aws,2020-10-13
j014hj,S3 pattern for web &amp; mobile application.,"The pattern I've come up with is:


(1) Send user a presigned url to post/get - assuming database reads/writes are not public



(2) When user uploads to s3, trigger lambda which posts to our own server, and then creates a database reference for its path.


My problem with this is an edge case where the presigned url hasn't expired, our server went down, and the lambda either kept running or stopped trying to contact our server - which means our database isn't in sync.



How do you guys use s3 :)",3,aws,2020-10-13
j000oa,Need Advice on using Amazon Lex,"Hello, I'm a college student about to talk to recruiters soon and wanted to do another AWS project to put on my resume before then(4 days left). I think creating a chatbot with Amazon Lex is a pretty good idea. Maybe one that'll give you recommendations on Games, Anime or something else depending on how you answer questions. If someone has a better idea to put on my resume I'm all ears, just keep in mind my skills with python aws are limited, but if I had more time I could learn. The problem is I've never worked with Lex and have a ton of questions. Googling them is taking forever and some I can't even find, so I thought I'd ask the experts here. If you can help, that would be great.

I would like to know if I can link the bot to a twitter account or would I have to host a static website and integrate the bot there?

What would be the most cost effective way to show off the bot that will still impress recruiters?

Is this even a good project to focus on?

Can I set the bot to learn from it's users or is that type of bot one you have to start from scrap?

If someone can answer even a few of these it would same me a lot of time, thank you!",1,aws,2020-10-13
izykq3,Appstream fleet setup,"I’m setting up appstream for our school so that Remote users can run Autodesk software while completely virtual. Where I’m getting confused is when setting up a fleet is desired capacity the amount of total unique users that will be using it or the max amount of concurrent instances I want running? 

There will be a total of 300 unique users but never more than 50 on at a time. I know I will have to request quota increases but I do not want to make a costly mistake.",1,aws,2020-10-13
izxeke,AWS Amplfy Flutter update?,Any idea on when AWS will on include the API attributes to Amplify? Or other features?,6,aws,2020-10-13
iztydd,Advertised EBS bandwidth for a given ec2 instance type does not match the bandwidth listed on my bill,"Hello,

What is the canonical source of the EBS bandwidth allotted to an ec2 instance type?

Check the r5 marketing page here, and you’ll see for example that 8x large instances are allotted 6800 Mbps of EBS bandwidth.

https://aws.amazon.com/ec2/instance-types/r5/

But on my bill, in the EBS section, I see the following line:

“$0.00 for 5000 Mbps per r5.8x large instance hour (or partial hour)”

This discrepancy exists for other instance types as wel. I have asked AWS support about this, but I have not received an answer for 25 days, so I’m not sure what’s up. Hoping someone here knows the answer.

(I could of course try to find the max bandwidth empirically, but I’d rather not have to do that, since I’m not sure yet which instance type is best for my needs!)",2,aws,2020-10-13
izrt8b,Helper tool to access API Gateway APIs using IAM authorizers with Python+requests,,1,aws,2020-10-13
izvi0w,"Watch out - The filters on NoSQL Workbench are buggy... ""contains"" in this case...","I've had this happen to me multiple times while using the NoSQL Workbench with DynamoDB...  While the console will actually return a record, the Workbench will actually return zero results - and this happens consistently when filtering using ""contains.""

So be careful when you use this software... it's not completely reliable.   


What about the other options? Well... the JDBC drivers offered by 3rd-party vendors are outrageously priced....($1000 per year license for one of them!) or RazorSQL (a very sub-par, but functioning client..) so I was hoping for NoSQL Workbench to be at least reliable, if not pretty... but... nope - it's neither.",2,aws,2020-10-13
izq3jn,Error in AWS AppStream documentation for Adobe Creative Cloud build,"Howdy - I work on AppStream and am doing a lot of implementation and testing work around the Adobe Creative Cloud suite. One very handy document is the AWS white paper on installing the suite. [https://d1.awsstatic.com/whitepapers/running-adobe-creative-cloud-on-amazon-appstream.pdf?did=wp\_card&amp;trk=wp\_card](https://d1.awsstatic.com/whitepapers/running-adobe-creative-cloud-on-amazon-appstream.pdf?did=wp_card&amp;trk=wp_card) 

This document calls for the use of stream.graphics-design.xlarge instances for the suite. Doing a lot of testing and this does NOT work with the Premiere Pro app. This app really requires a stream.graphics.g4dn.4xlarge instance to run well. I have also tried it using a g4dn.2xlarge and have had some intermittent issues. The issues revolve around audio / video lag. On smaller instances there is a very visible, perceptible lag on the sync between voice and video. 

Just wanted to share. -- Kevin",3,aws,2020-10-13
izu5bi,"Being asked for ARN by customer, delivering video files to S3 bucket","I’m an editor and one of my customers is asking for my ARN to grant me access to their S3 bucket to upload some video files. 
I’ve delivered files to S3 buckets before and always received the bucket info with access key id and secret key id to connect via a client like cyberduck. 
Am I not getting the right info or am I missing something here.",1,aws,2020-10-13
izpybb,AWS Managed vs Customer Managed Key,"Hi , 

When using KMS for most of the services we see that AWS managed keys are used by default to encrypt/decrypt the data then why do we have customer managed keys for these services ?

I understand that we xan use CMK in our apps to encrypt data.


For example, all secrets in secret manager are encrypted by default using AWS managed key, if I use a customer managed key (without using ClousHSM or any other feature; just a CMK) is it going to more secure or make any difference at all ?


Is replacing AWS managed keys with CMK just for legal/compliance reasons ?

Thanks for the help!",3,aws,2020-10-13
izpq9v,Interesting AppStream Image Builder bug when using / testing microphone input.,"So, doing a lot of work with AWS AppStream and specifically with installing multiple components of the Adobe Creative Cloud suite. One application, Premiere Pro requires microphone input. In testing the Image Builder using websites like  [www.onlinemictest.com](https://www.onlinemictest.com) I discovered that even with Enable Microphone ""on"" if you change user personas (admin to template, or test, etc.) you lose microphone input capability. The only way to fix it is to reload the Image Builder browser and then re-enable the Enable Microphone setting.

This is a known bug and is on the ""fix it"" list. 

Just wanted to share in case anyone ""lost"" microphone input in the Image Builder and did not know why. -- Kevin",3,aws,2020-10-13
izt4ok,EC2 autoscaling instance lifecycle and user data execution,"I've configured a launch configuration and autoscaling group. The launch configuration's user data is a script that runs on startup, and takes some time to execute. The AMI is a standard copy of Windows Server (not customized for AWS in any way).

When autoscaling spins up a new instance, when does it consider the instance as InService? When the operating system on a new instance boots? When the AWS software is installed, but prior to the execution of the user data script? After the user data script is launched, but without waiting for it to finish? After the user data script finishes or upon the next reboot, whichever comes first?",4,aws,2020-10-13
izsd21,Where would you store a secret shared key?,"If I want to break my app up using Lambdas, and I want to share a secret key to authenticate jwt tokens, I'm assuming I'm better off keeping it in one location rather than having it stored in each app on Lambda.  Does AWS have a built in tool for this?  I've heard others talk about something I think was called Zookeeper, but I believe that is a third party program.  I'm wondering if AWS has a simple solution for this, where when the Lambda awakens, it can fetch the secret key and authenticate the jwt.",33,aws,2020-10-13
izmzqv,How to add a new key to a Dynamodb map without overwriting the existing key?,,1,aws,2020-10-13
izr1sb,"ECS Task Placement Strategy ""binpack"" --- what does this word for?","The binpack strategy will minimize the number of EC2 instances used in the EC2 launch type of ECS by placing tasks to maximize the utilization of available CPU and memory.  Does anyone know what the term ""binpack"" stands for or actually means?  I'm looking for a way to help remember this term, but ""binpack"" just seems like a gibberish word.",7,aws,2020-10-13
izq6rm,Configuration pushed to IOT Core,"We are building a solution were an edge device captures data and processes it before pushing out via mqtt the results. There is a descent amount of config that is required that at the moment we are pushing down as shadow docs. Issue is that we are getting close to the size of the shadow doc. 
Can you see an alternative to share a json file to the edge, safely, that doesn't rely on having the edge knowing auth and endpoints to download?
I'm considering checking if ssm could do something since we use it to maintain the device os.",9,aws,2020-10-13
izq2db,Access network loadbalancer from different VPC?,"I have some internal application and network load balancers in one account.  I can access the application load balancers from a different account/vpc connected through a transit gateway.  I cannot connect to any of the network load balancers however.

Is there some limitation I can't find for internal network load balancers being restricted to the VPC?  All my networking and flow logs seem fine for all other communications across VPCs and transit gateway.",11,aws,2020-10-13
izmzfz,Multicast in a single VPC to multiple VPS,"I'm relatively new to AWS and am learning about how AWS (doesn't) handle multicast natively.

&amp;#x200B;

I use a software suite that moves realtime audio (RTP) around via multicast subscriptions and discovery protocols. In the physical world, it requires IGMP and cisco switches (basically set for VoIP) to handle subscription traffic and such, and there's a multicast clocking system to sync all of the audio between hosts (PTP1588). 

Is there a way to semi-easily allow this to traverse between EC2 instances without requiring a lot of multicast transit gateway stuff where IGMP does not exist? I can generate the PTP clock in software no problem, but allowing the other EC2 instances to ""see"" the mcast stuff seems to be obfuscated from the AWS side without jumping through a LOT of hoops, and the discovery protocol is very key in this system.

I can run it on a single instance no problem, but it does not traverse the network.",1,aws,2020-10-13
izmts4,Network Drops Across EC2 in Public Subnet? Tools to diagnose the cause?,"Hello! Looking for some help, tips, otherwise on a tricky issue.

We have a fairly complex set-up of g4dn instance types running vMix with NDI, TeamViewer, and several other lighter streaming protocols to operate a live stream. The set-up is brand new and set up fairly quickly in the past two weeks so we haven't yet had time to effectively set up sysops tools like VPC flow logs and otherwise. The problems seemed to begin to creep in after we added some additional EC2 instances due to late arriving request. 

We've had a several occasions in which TeamViewer becomes inaccessible for a short amount of time however the machines appear to continue operating normally which makes leads me to believe it's some sort of network congestion. When I look into our CloudWatch dashboard I'll see occasional drops across all instance types that will then spring back.

&amp;#x200B;

https://preview.redd.it/0tacmww5mbp51.png?width=2481&amp;format=png&amp;auto=webp&amp;s=d07a9bcf81557bf16f91b5ac42bf08e6b7e5a379

&amp;#x200B;

https://preview.redd.it/v9t18ey6mbp51.png?width=2489&amp;format=png&amp;auto=webp&amp;s=a39972264eb1a8b7addba2d7546d88e0e9e7ea11

We also experienced a hard crash on a machine running OBS which could have been related to a network timeout but still unclear. Any ideas of the most likely cause of that sort of behavior / congestion? Is there a particular component that could be the weak link culprit? What tool would be the best to diagnose the cause of this issue? Would the [AWSSupport-SetupIPMonitoringFromVPC](https://docs.amazonaws.cn/en_us/systems-manager/latest/userguide/automation-awssupport-setupipmonitoringfromvpc.html) tool be ideal? VPC flow logs? Local tools on the Windows instances?

We have the following all in a single public subnet: 

1. c4.2xlarge
2. g4dn.12xlarge
3. g4dn.12xlarge
4. g4dn.2xlarge
5. g4dn.2xlarge 
6. g4dn.8xlarge 
7. g4dn.8xlarge 
8. g4dn.8xlarge 
9. g4dn.8xlarge 
10. g4dn.8xlarge 
11. g4dn.8xlarge 
12. g4dn.8xlarge 
13. g4dn.8xlarge 
14. g4dn.xlarge 
15. g4dn.xlarge
16.  t2.micro 
17. t3.micro

Thank you in advance for any tips or suggestions!",5,aws,2020-10-13
izjuuu,Can I use RecordIO files as PIPE input channels for the built-in SageMaker Semantic Segmentation?,"I tried the following cases:

\- JPG/PNG as FILE input: works, but requires to download the whole dataset to the instance.

\- JPG/PNG as PIPE input: failed

\- RecordIO as PIPE input: failed

I have no clue what I'm doing wrong when trying PIPE input channel. Any help is appreciated.

[View Poll](https://www.reddit.com/poll/izjuuu)",1,aws,2020-10-13
izlvjk,Oracle Database and WebLogic on AWS? Setup help needed...,"Does anyone have any experience setting up and using Oracle Database ([19.4.0.0](https://19.4.0.0)) and their WebLogic server ([12.2.1.4](https://12.2.1.4)) on an AWS EC2 instance?  I'm just looking for some setup assistance and best practices guidance.  


Thanks",0,aws,2020-10-13
izloz7,Difference between EC2 vs containerized on ECS?,"My understanding is that if I want to deploy something on the aws cloud, I can spin up an EC2 and run the app on that. Or I could containerize my app and run the container itself on ECS. Is that true?

What are the benefits or drawbacks of both ways?",1,aws,2020-10-13
izjtft,Creating Thumbnail on S3 bucket using a Lambda Function,"Greetings World!

&amp;#x200B;

 I was studying Lambda for the first time and I was using the tutorial &gt;&gt; [https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html](https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html)  


and it does not even work for me for a file size of 60Kb. This is as far as I have made it:  
\-Created the 2 buckets, uploaded the source file in the source bucket (67KB), named it HappyFace.jpg.  
\-Created the Lambda role  
\-I was able to create the Lambda function running the CLI command  
\-I was able to create the output.txt, BUT in this step, the output.txt file gets created but its void (0 KB), also the manual invoke does not create the thumbnail in my dst bucket.  
since the manual invoke test does not work, whenever I upload a new image in my src bucket, nada again.  


I am using a cygwin Linux emulator in my windows 10 laptop, using the latest node.js version 12.x and using CLI ver1.x (tried to upgrade it to ver 2.x but was not successful, I am a Linux newbie).  


I just passed the AWS arch associate exam and I have the AWS fever hehe, I just can't get this bug out of my head, I want to make this work. I suspect that there must be an issue with my index.js file  


Can you help?  


Thanks!  


Chilldingo",1,aws,2020-10-13
izihe8,Cross region Cloudformation. Now you can do it with a single file!,"A few days ago I wanted to use a single file (ie a single thing for people to update in a single place) to create some resources in different regions. I started down a few dead end ideas like SSM parameters and the like. They're all region locked.

I could have scripted it, but people struggle to set up roles/etc. on their CLI so console access was preferred.

Eventually I found that AWS have added StackSets as Cloudformation objects a few days previously.

To me this is a game changer. We can now supply code inline in a template for a second template that includes variables defined in the upper level template. I'd be interested to hear other uses for that. My first thought was a template that creates an update failed alarm on the sub-stack. BUT you can't yet get the IDs of the sub-stacks to know what that alarm needs to look at. (without a lamdba/macro).

BUT now, with one file you can create resources that reference other region's resources. If you've ever tried to Cloudformation ACM and Cloudfront, you know what I'm talking about. (""But the cert needs to be in us-east-1 even though all my other infrastructure isn't in us-east-1.. Wut?"")

If you want more detail I made a blog post with a simple example and a run down of some of the other things you can do to achieve this (the StackSet doesn't quite work for all scenarios, I have some alternative ideas that \_might\_ help which I still need to test! At the moment it is ""parent can pass values to children only"". You can't pass values between children or back to the parent.)  


[https://surevine.com/creating-cloudformation-stacks-in-multiple-aws-regions-with-common-resources/](https://surevine.com/creating-cloudformation-stacks-in-multiple-aws-regions-with-common-resources/)",53,aws,2020-10-13
izicyu,Using AWS cognito to deal with user authentication and api key token creation,"Hello all,

We have the following use case. We have an application running on AWS where we do the authentication of users manually^[1]. We are looking to migrate to using AWS Cognito to handle the user authentication and authorization. So far this all seems pretty easy and doable. The only roadblock is the generation of api_keys. When users login into our application they have the option to generate api_keys so that they can use our developer API from their own application. Picture something like [stripe](https://stripe.com/en-gb-nl) where you can make an account and login and within the application lets you generate api keys.

Is it possible to leverage Cognito to handle the creation of api keys (or something similar like client credentials in Oauth2) as well? The thing we tried are User Pool App Clients for every user but there is a limit of 1000 clients per user pool so it doesn't seem like this is meant to be used for every single user.

Another thing we looked at is the client credentials flow on a single app client. So we create a single app client for our application and turn on client credentials and let users login using that. However a cursory glance makes it seem like client credentials are for our own machines and not so much third party developers?

[1] With manually I mean that we have an endpoint where people sign up with a username and password, save those in an RDS and when people login we simply check if the user exists and give them a JWT token

UPDATE:

We have decided to use the client_credentials flow of oauth2. This means we will create an App Client for every user that wants to give their application access to our API.",8,aws,2020-10-13
izcp0w,Dedicated hosts Vs dedicated instances,"Hi guys are dedicated hosts basically an entire physical server rack that customers can select and can choose which individual physical server in rack that the instances can be deployed on ? Is dedicated instances basically instances that are running on the same physical server on a shared rack ? Each time a dedicated instance is stopped started , it goes onto a new physical server which only the company can spin up new vms ?",4,aws,2020-10-13
izfiiq,Someone uses the AWS App?,"Every time I use the AWS App I see it really useless, you can hardly do anything.

you feel the same?",10,aws,2020-10-13
izg0v0,New EC2 Experience - need New Permission to control resources?,"Hello,

One of our IAM user has permissions only for starting and stopping tagged EC2 instances.

(based on this document : [https://docs.aws.amazon.com/IAM/latest/UserGuide/reference\_policies\_examples\_ec2\_tag-owner.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_ec2_tag-owner.html))

I found out that in new ec2 experience, the error  occured while changing instance state saying that I need describe vpcs permission and others.

It doesn't give me any error when I switched back to old console and instance is started successfully

I had to put those action in the user's policy if I want to use new console.

Is there any API changes when it's operating on new console?

I couldn't find any documents about this.

Thank you.",9,aws,2020-10-13
izgcay,Using lamba to replace bash scripts in theory.,"Looking at the possibility on moving a legacy solution up to the cloud

Current situation:
Multiple times a day a dmz linux vm rsyncs some files from a third party ftp share. It then does some remote renaming operations to tag the files previously downloaded and unzips downloaded files and stores them locally. 

At different multiple times a different linux vm inside our main network running an oracle database then downloads and processes these files from the first vm to oracle readable files, injests the updates then deletes the files from the first vm

Lock files are used to stop the two processes conflicting. Honestly yes it's as bad as it sounds. 

This is all triggered by cron using bash scripts. 

I was considering the benefits of migrating the oracle database to oracle rds but would then need something to replace the function of said bash scripts. 

Very new to aws, but was thinking in theory could this be achieved with a couple of lambda functions. 

The logic would need some reworking and I am guessing it would need to be written in python and attach some s3 storage to hold the files but in theory is it possible and a good idea?",1,aws,2020-10-13
izfkte,API Gateway : disable AWS Signature,"I'm trying to make my API accessible from the Internet access without any Auth necessary, yet even though I don't configure any Auth on API Gateway, apps like Postman require AWS Signature.

Is there a way to make my API publicly open?

&amp;#x200B;

Thanks in advance",0,aws,2020-10-13
izewvv,Most Commonly Used AWS Services And How We Are Using Them,,0,aws,2020-10-13
izbx55,"Mix x86 and Arm-based instances into one spot fleet or ASG, possible?","With the newly launched Arm-based Graviton CPU, I wanted to try it with my existing applications which are currently inside some spot fleets. I could not find any related resources, hope some experts here can help me answer this.",2,aws,2020-10-13
izbghi,Can I use Lightsail for this dynamic website or should I use EC2?,"I'm creating a website that will allow a user to enter variables. Those variables will then be used to perform calculations and retrieve info from a database. Then the data retrieved will display graphs and charts to the user. The back-end code is written in Python.

Can this be developed with Lightsail? Or should I just go straight to EC2?",1,aws,2020-10-13
izbcu8,I'm losing trust in AWS. SNS is broken for 24 days.,,0,aws,2020-10-13
izb2xj,CloudWatch Alarms Created from a CloudFormation Macro?,"I need to create a bunch of simple EC2 and ALB alerts in Cloudwatch and looking for something simple to handle it for me that has a small footprint, is AWS native if possible, and wouldn't be a nightmare for my successors to investigate and maintain.


I'm intrigued by a blog post I found that describes writing a custom CloudFormation macro to automatically inject alarm entities into a template based on the resources it contains.  
https://aws.amazon.com/blogs/infrastructure-and-automation/automating-amazon-cloudwatch-alarms-with-an-aws-cloudformation-macro/  
https://github.com/aws-quickstart/cloudwatch-alarm-macro

But I'm a little suspicious. This just looks like one of those things that could introduce a subtle dependency that makes it impossible to update or rebuild my stacks later. Has anyone ever tried this before, any issues or gotchas I should watch out for?",2,aws,2020-10-13
izb7nk,How would you move legacy JBoss EAP AS workloads to AWS?,"Our company is on a big drive to onboard to AWS as soon as possible. As such, many application teams are opting to build like for like on AWS as they had on-prem, which for me seems like a sure way to bleed money and feels like a cloud anti-pattern.

These teams are not using any automation (not cloud formation, nor Terraform, nor Ansible) so they are building Snowflake servers.

I am part of the middleware team and would like to propose to the application teams, that we support, that they should containerise their applications using Wildfly first, before moving to AWS. Our traditional middleware admin team, would then become an EKS admin team and help the application teams transition their applications (as monoliths) to containers, using source control for both the generation of the container images and for the provisioning and maintenance of the EKS cluster.

I was thinking of using one private subnet for a single application (the 200 subnet limit per VPC will be more than enough), and using cloudwatch to monitor both the EKS cluster, the application servers and the worker nodes. 

I would like to know what the community thinks of the above proposal, if I am on the right track and how would you do it if you were in my position?

TLDR; How would you migrate JBoss EAP workloads to AWS, from on prem, in a manageable and maintainable way while not bleeding money?",1,aws,2020-10-13
iz9tzj,"What is mean that CloudFront ""x-amz-cf-pop"" header values format?","Hi Guys,

I just wondering what is mean that ""x-amz-cf-pop""  response header by CloudFront.

I thought might be mean this header value to AWS edge infra.

e.g &gt; header value ICN54-C1

""ICN"" is IATA code is meaning region country

""54"" is I have no idea, but it might be this is data-center code.

""C"" is perhaps Cluster, so if this correct, CDN terms as BAND

and then I guess  ""1"" is numbering of ""C"" cluster.

&amp;#x200B;

conclusively, in my guess this defined to this header value:

{IATAcode}{Data-center code}-{Cluster}{Cluster number}

&amp;#x200B;

please let me know this is whether or not correct.

Thanks",1,aws,2020-10-13
iza42q,"Multi-Region, Multi-Account VPC Communications","I'm a bit new to the complex networking side of AWS. I was hoping someone could shed some light on how to go about this:

I have some VPCs (and external VPNs) connected together in multiple accounts under the same org by a shared TGW in US-West-2, but I need to connect another VPC in another account of the organization that's located in EU-Central-1.

I've tried to search around, but I'm not finding good information for this issue. I tried sharing the TGW, but the other account/VPCs aren't an option. I tried creating a TGW attachment via VPC, but it's only showing the US-West-2 VPCs in the other accounts. It appears I can peer connect them, but that means I need to create a new TGW in the EU-Central-1 region?

I really need some clarity on this. I feel pretty lost :[ Thank you very much for any help!",1,aws,2020-10-13
iz9tn3,Best Storage Solution for Google Forms / SurveyMonkey clone?,"A few things to consider:

* There will be millions, not billions of survey answers.

* Answers are grouped by survey (some key), no more than 25-100k answers per survey at most. Realistically, most surveys will have less than a thousand submissions, with maybe 10-20 questions per survey.

* Every survey is different, so survey answers will be unstructured JSON in all likelihood.

A few ideas I've had:

* **Athena + S3** - The problem is how to get it into S3. I've looked into firehose but it seems I'd have to batch my answers with the KPL so as to not get 'hosed' by the 5kb pricing round up (hard, what to do if server goes down in middle of batch?). Also, I'm not sure how good presto is at querying JSON/full-text.

* **Postgres** - Supposedly Postgres has good JSON support. No idea if it has good full text search capabilities. 

* **Dynamo** - NoSQL, I'm sure it'd have good support for JSON. I'm not sure how well it do with aggregating all of the survey results under a certain partition key, and again, full text search.

* **Elastic (AWS or self-managed)** - I have zero experience with elastic so I can't really add to this, other than I'm sure it's the best at full text search. 

Really it all comes down to ingestion, aggregation query support, and full text search support I think. 

I'm a newbie at this as you can probably see.

edit: formatting",1,aws,2020-10-13
iz7pes,Amazon Lex - Optionally provide slot values?,"I'm building my first bot, so forgive me if this is a stupid question. My first intent I'm building is supposed to simply return a list of names, but I'd like to optionally provide some kind of filter. The Lambda function fulfilling the request will simply take the filter and return a subset of the bigger list that contains the filter string. E.g. if I pass ""Dan"", I want a list that's something like [""Danny"", ""Daniel"", ""Daniela""]. But when no filter is passed, return full list.

I created a slot and unchecked the required box. My utterances are as follow:

- show me a list of group member names.

- get list of group member names

- get names of members in the group

- show a list of group member names with name {nameFilter}

- get list of member names with name filter {nameFilter}

I specifically ask for the response to be a list of names because later on I will ask for a list of numbers, addresses, etc.

However with these utterances, the slot value is never filled. Even if I provide the exact text of one of my sample utterances, substituting the {nameFilter} with an actual one, nothing happens. I just get the full list.

I can set the slot to be required, but then it still doesn't pick it up if I provide the slot value in my initial request. It keeps asking for the slot value in response to my initial request.

Is there a way to achieve my desired behavior, or is that a limitation at this point?",1,aws,2020-10-13
iz8nq2,What's your CICD process for API Gateway backed by Lambdas using Versions &amp; Aliases?,,27,aws,2020-10-13
iz6qv9,Api gateway v2 authorizers (jwt or lambda): what's the pricing? Can't find anywhere.," Do they charge the usual lambda pricing for the authorizer executions? If so, my cost per request would be doubled since I'd get charged for 100ms twice (authorizer lambda and endpoint lambda). 🤔

What about the JWT authorizer?

Really interested in this because I'd like to avoid paying for unauthenticated requests.

Thanks for your time.",1,aws,2020-10-13
iz6895,AWS ECS encrypts attached volume,"I need to create ECS cluster where there would have a key file (which contains access token that kind of sensitive data) read by a third party library. Though I can store the key file content in aws secrets manager, however the third party library only reads the key file specified by path in the config file . The library does not have ability to read from S3 or to read key file content as parameter, or other means. 

The solution I can think of is to store that key file as local file in volume attached to the docker (defined as task definition in terms of ECS). So far I find that aws's doc \[1\] mentions that after PV 1.4 ECS' attached ephemeral volume by default is encrypted. 

&amp;#x200B;

Now my problem is how do I know or test if the ECS cluster launched its volume is encrypted? After searching on the internet, I can't find related doc talking about this. The only one is \[2\] where the command line can specify platform version. But the way how I deploy the cluster is through CloudFormation yaml file (create a stack in CloudFormation UI, and hook a Git repo in which a yaml file defines the cluster setting), and I do not find related explanation. Any way to achieve such purpose? I appreciate any suggestions. Thank you. 

&amp;#x200B;

\[1\]. [https://aws.amazon.com/about-aws/whats-new/2020/05/aws-fargate-now-encrypts-data-stored-on-ephemeral-storage-by-default-in-platform-version-1-4/](https://aws.amazon.com/about-aws/whats-new/2020/05/aws-fargate-now-encrypts-data-stored-on-ephemeral-storage-by-default-in-platform-version-1-4/)

&amp;#x200B;

\[2\]. [https://aws.amazon.com/blogs/containers/introducing-server-side-encryption-ephemeral-storage-using-aws-fargate-managed-keys-aws-fargate-platform-version-1-4/](https://aws.amazon.com/blogs/containers/introducing-server-side-encryption-ephemeral-storage-using-aws-fargate-managed-keys-aws-fargate-platform-version-1-4/)",0,aws,2020-10-13
iz5z0s,Document Signing on AWS,"I'm working a service that fills/generates/signs PDF documents.

Users will draw their signature (or type their name) on a web interface, and then my servers will generate the signed PDF. I would like to add a ""Verified by {Service}"" signature to the PDF, to show that the document was generated by my service.

I have searched through [the list of Adobe Approved Trust List members](https://helpx.adobe.com/acrobat/kb/approved-trust-list1.html), but these companies don't seem to offer what I'm looking for. They sell a USB token or HSM that is meant to be used by an individual or an organization. I'm looking for a document signing certificate where I can store the private key on my AWS servers, and use this key to sign/certify the generated PDFs.

I am able to sign PDFs using a self-signed certificate, but this shows an error in Adobe Reader:

&gt;Signature is INVALID. ... The signer's identity is invalid. There were errors building the path from the signer's certificate to an issuer certificate.

How do other electronic signature / document generation companies do this?

Would it be possible to use [AWS CloudHSM](https://aws.amazon.com/cloudhsm/) for this?

Also, I know this is the AWS subreddit, but are there any major differences with other cloud service providers when it comes to price or usability?",1,aws,2020-10-13
iz5oio,AWS Route 53 Privacy Protection question,"Hi everyone,
I've been looking into purchasing my first domain on AWS Route 53, but I am a little confused about the Privacy Protection (specifically, what it will and will not protect). This is my first foray into web hosting, so I appreciate any insight.

I've done some reading:
https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-privacy-protection.html

https://www.reddit.com/r/aws/comments/a4e216/route_53_privacy_protection/

So I understand that, depending on any limitations placed on the TLD, privacy protection should hide all of my personal contact information from WHOIS requests. However, when I fill out the form in Route 53 and select 'enable' for Privacy Protection, I get a message saying that it will protect *some* of my personal information.
Does anyone know what it won't hide? The Amazon docs seem to indicate that it hides ""all of your contact information"", but the wording on the message in the form makes me a little dubious.
The same message seems to appear for various other TLDs that I've tried so maybe it's just a standard AWS disclaimer?


Edit: it looks like my question was answered [here](https://www.reddit.com/r/aws/comments/gldo5a/route_53_costs_for_studying/) in a follow-up question near the end of the post:

if the registrar is AWS, all information is hidden. If it's their affiliate Gandi, then everything *except* org (if present) will be hidden. This likely the reason for the ""some of your information will be hidden"" disclaimer.",1,aws,2020-10-13
iz51lk,Code that refuses to let the AWS SDK initialize itself and breaks if I don't feed it access keys,"I keep running into code written by devs who just assume that I'm going to be feeding in a key/secret pair to a library that wants to talk to AWS. I set `AWS_PROFILE` to test locally and the whole library just throws a fit and tells me to go RTFM.

Right now, I'm dealing with Ruby code that does:

     AWS::EC2.new(
        access_key_id: fetch(:ec2_access_key_id),
        secret_access_key: fetch(:ec2_secret_access_key),
        region: region
      )

And when those are `nil`, the whole thing dies trying to talk to the metadata service. On my home network.

Am I doing something wrong that I keep seeing this out in the wild, or am I doomed to keep forking source and fixing it by removing code?",13,aws,2020-10-13
iz48gt,Amazon Elasticsearch Service now offers T3 Instances,,88,aws,2020-10-13
iz4ipd,Anyone using new Cloudfront-Viewer-Location-* headers?,"Back in July, there was an [announcement of new Cloudfront Location Headers](https://aws.amazon.com/about-aws/whats-new/2020/07/cloudfront-geolocation-headers/) that you can turn on.  Specifically, I'm trying to turn on the `Cloudfront-Viewer-Location-Longitude` and `-Latitude` headers.  The CF User Guide is, as usual, clear as mud without concrete examples &lt;sigh&gt;.

I already know that Terraform doesn't support the new OriginRequestPolicy object that this feature requires.  I found the PRs to add this, but it hasn't been merged yet.

So, I wrote a golang script to create the OriginRequestPolicy (which names the headers I want), and attach it to the CacheBehaviors array (which refers to the OriginRequestPolicy), and update the Cloudfront distribution.  The results of this script look like what I expect in the AWS console, but when I check my lambdas@edge, I'm not receiving the Lat/Long headers.  I've tried cache invalidation, and toggling the enabled flag, and no dice.  I'm about to try deleting/recreating the distribution.

(to close the loop, I'm intending to have terraform run my go script with the local-exec provider)

Because it's a new feature, I've not been able to find anything detailed on how to turn on the features.  Has anyone here had any luck with these headers?

Edit: clarification that I am naming the headers I want in the OriginRequestPolicy object.",3,aws,2020-10-13
iz2v2y,How can I programmatically setup multiple certificates,"Hi

Our service generates web spaces for users. If they have domain names, however, they need to be able to reroute their domain name DNS so they point to our Cloudfront distribution (s3), while also setting up their corresponding certificates so https works.

What would be the best way to do this? Anyone has any idea ? 

We thought about using Aws certificates alongside route 53 alongside an nginx proxy . Though Route 53 seems to have some limits, that may be increased up to a certain treshold. 

Is there an easier way to do this ?

Thanks",3,aws,2020-10-13
iz2mmf,AWS instance shortage in US-EAST-1,"Recently we ran into big difficulties to obtain some type of instances in us-east-1 (whatever the az).

m5.xlarge, r5.4xlarge etc.. very basic one. 

Are we alone in this situation ? are all customers already reserved all instances for holidays seasons ?",32,aws,2020-10-13
iz2bqz,Lambda MSK Kafka Destination,"I've seen that its possible to use MSK Kafka as a trigger for a lambda function, but what about sending a message from lambda to Kafka?

&amp;#x200B;

I'm new to lambdas, and I see that MSK Kafka does not appear as a destination. Is this still doable?",0,aws,2020-10-13
iz24t7,Save Environment values in Quotes in Elastic Beanstalk Environment,"I am trying to get key value pairs from my environment by running:

    cat /opt/elasticbeanstalk/deployment/env | tr '\n' ',' | sed 's/export //g' | sed 's/$PATH/%(ENV_PATH)s/g' | sed 's/$PYTHONPATH//g' | sed 's/$LD_LIBRARY_PATH//g' 

However, this doesn't return the values in quotes. I want to save all key value pairs into a variable in which the values are quoted. For example, environment=KEY1='value', KEY2='value2'

How can I do this? I use these key value pairs in a script for my superviord which supervises a celery worker.",1,aws,2020-10-13
iz1twp,Real-Time Analytics on Data Lakes: Indexing Amazon S3 for up to 125x Faster Queries,,5,aws,2020-10-13
iz1jno,"I forgot to check the ""change ownership"" box for S3 replication - any way to update existing replicated objects?","I am using S3 replication to copy a bucket across AWS accounts (both are in the same region).

When originally I set up the replication I didn't notice that I had to check the ""change ownership"" box in order to allow access to the objects in the destination bucket (the original bucket is using AES-256 encryption).

&amp;#x200B;

I realized this after I have a couple of Terabytes already copied over to the other account.

Is there any way to update the existing copied objects in the destination bucket to allow access so that I do not have to copy them all over again?

&amp;#x200B;

FYI -  To perform the copy I used  cross-region replication with the copy to itself method as shown here in an AWS knowledge center page (see the "" Use cross-Region replication or same-Region replication "" section)- 

[https://aws.amazon.com/premiumsupport/knowledge-center/s3-large-transfer-between-buckets/](https://aws.amazon.com/premiumsupport/knowledge-center/s3-large-transfer-between-buckets/)

&amp;#x200B;

If anyone knows of a faster or better way I am all ears!!",4,aws,2020-10-13
iz0yxv,Is anyone making good use of VPC sharing? Curious what your use case is and how everything is working.,"I'm thinking about using VPC sharing for some network boundary use cases, but wanted to see if anyone is finding it useful in their environment.  Thanks in advance!",7,aws,2020-10-13
iz01gr,RDS session count goes till 8 and becomes unresponsive,"Node (NestJs - Express) App deployed on AWS,

1. code deployed on EC2 (t2.micro)
2. Redis for caching
3. RDS( Postgres) t2.small

Even though we get very few hits (less than 10), but our RDS becomes unresponsive due to max sessions (8 sessions)

Most of the time its update query on one particular table. 

Can you please help me diagnose an issue",1,aws,2020-10-13
iyzlop,What tags do you typically assign to AWS resources?,"I'm curious about what tags you typically assign to AWS resources?  I'm thinking that these tag keys make sense, but I wanted to get some feedback from the /aws community.

Name ***(duh)***

Environment

Project

Team

Owner/Administrator

&amp;#x200B;

Any other good tag ideas?

Thanks!",5,aws,2020-10-13
iyykqi,Looking for advise on how to 'end-to-end' x-ray my architecture,"Hello everyone, please allow me to ask for advise.

I have a solution here which on-demand produces content for an API Gateway caller to be delivered via CloudFront. I intend to be able to x-ray a single call from the time the request arrives until it is available in CloudFront and I'm looking for inspiration. Please let me describe the architecture first.

The forefront is pretty straight forward. A Lambda, written in Node.js is exposed via API Gateway. It does a bit of business logic and then posts a message into an SQS queue. This Lambda and the API Gateway are X-Ray enabled. 
A back-end process, written in native C++ which does much heavy lifting then receives the message, processes it and uploads an artifact to S3, which is then available via CloudFront. All this works fine but want to get a closer understanding of bottlenecks.

So, the Lambda gets x-rayed just fine. Inside it, I get the environment provided trace header and put it into the SQS message's MessageAttributes. The backend is then able to retrieve it. In the C++ process, I use the x-ray SDK to assemble and post a trace document, providing the same trace id and a number of subsegments. Those show up in the x-ray service map OK. So I got the basics in place I believe. Here's the problems:

1) The Service Map (and trace timeline) displays the C++ parts and the Lambda parts as separate. They seem to be in one trace and both visible underneath but not connected.
The polling from the Q is displayed as another Client. Even though they have the same trace id. AWS has a [image here](https://docs.aws.amazon.com/xray/latest/devguide/xray-services-sqs.html) which explains how I want it to look. But no matter what I try, I see two clients. Two separate lines of tracing. I have tried instrumenting the xray SDK in the Lambda manually when I post the SQS msg but that doesn't help. I have to add the AWSTraceHeader in MessageSystemAttributes or the (C++) receiver has no idea what the trace id is. How can I connect this semantically and make the 'second client' disappear to reflect above image? My hunch is that it's got something to do with the parent_id fields in the trace document and I tried many different ways of setting them but that never changed the picture at all. Yet they are poorly documented and perhaps I am misusing them.

2) Given I can already send x-ray documents in the C++ backend, how can I trace that further? To S3? Using the C++ SDK, can I also trace the S3 upload call somehow by instrumenting the client like the JavaScript SDK can? Or should I just measure it like any other 'external' call and include it into my measurements? This would seem non-native somehow and I would prefer a deeper integration. But I have not seen any indication in the C++ SDK that this is possible.

3) Perhaps I'm missing some obvious building blocks that could help achieving what I want. Fell free to whack me over the head with an entirely different approach.

Thanks!",3,aws,2020-10-13
iyy4bh,AWSLogs agent &amp; date-stamped log filenames,"I have an application running on an EC2 instance that date-stamps it's log files (ex: log-2020-09-23.log, log-2020-09-24.log). I'm attempting to stream these log files to CloudWatch using the AWSLogs agent (not the unified CloudWatch agent.... yet). The agent allows you to specify a wildcard for the log filename (/path/to/logs/directory/log\*). However it only seems to want to read the log file for the day I (re-)configure the agent. It won't automatically move on to the new log file on following days.

Does anyone know if this is expected behavior? (Specifying a wildcard doesn't actually look at all log files with that prefix). If not, suggestions on what I may be doing wrong?  If so, does the Unified CloudWatch Agent support rolling date-stamped log files?",1,aws,2020-10-13
iyxgjo,What permissions do i need to set for boto3 rds.restore_db_instance_from_s3()?," 0

I am trying to restore a rds on my AWS cloud from a .bak file on my S3 bucket.

I keep runing into the following error:

    botocore.exceptions.ClientError: An error occurred (InvalidParameterValue) when calling the RestoreDBInstanceFromS3 operation: IAM role ARN value is invalid or does not include the required permissions for: S3_SNAPSHOT_INGESTION

I guess the error lies within my value of the field:

    S3IngestionRoleArn='string'

I can't find any ressources what exact permissions are needed or what exactely this field wants to have.

There is no information concerning the error with: ""S3\_SNAPSHOT\_INGESTION""",1,aws,2020-10-13
iyxkv6,Redshift Materialized View Incremental Refresh,We have created materialized view in redshift using Select statement only but it's always showing state = 0 which is full load. Can someone here help me how to make refresh in materialized view incremental instead of full load? Thanks.,1,aws,2020-10-13
iywyxm,IAM Console us-east-1 not working?,Is anyone else having problems bringing up IAM in us-east-1?,0,aws,2020-10-13
iywh6s,WAF rules,"Hey Guys,

I've recently been upgraded from a junior system administrator to a cloud engineer and in need of some help.

&amp;#x200B;

So I need to implement some WAF rules to better protect against DDoS, XSS and the usual fun you get on the web, but I am struggling to find some good resources on some 'Good starter rules'

Suffice to say I am slightly in over my head

Does anyone have a good document or suggestions I should look in to?

Thanks,

\-Chin",6,aws,2020-10-13
iyw81b,Sharing Access to Elastic Beanstalk Application,"Sorry if this a stupid question, but I'm not sure how to do it. I'm working with a group and have used Elastic Beanstalk to create an web application. Now I would like to give everyone else in the group the ability to go in and edit the environment's configuration. How do I do this?",2,aws,2020-10-13
iyw5hu,Setting Up a Redundant VPN Tunnel Between AWS &amp; CheckPoint,"Hi all. I've been asked if it's possible to setup a redundant VPN tunnel between AWS and our office. I'm not familiar with AWS so have been looking at documentation and from what I've read, AWS VPCs can be setup with 2 gateways. On the CheckPoint side, these AWS gateways can be put in a star VPN community with the on-prem gateways.

What I would like confirming is, if our on-prem CheckPoint fails over to our backup line, would the tunnel to AWS still stay up? Is there a way of defining a primary and a backup IP for the CheckPoint side on AWS?",1,aws,2020-10-13
iyuxc5,Connecting to Oracle DB using Python via Lambda,"I am trying to connect to Oracle RDS from my python code using lamda handler but the due to huge size of  my code owing mainly to Oracle library that's coming upto 250 MB, I am unable to deploy it. 
1) I tried using layers for libraries but they also have a size limitation.
2) zipping the files is also not helping. It's still huge.

Is there any way to make it work? A different library I can use? Please let me know.",1,aws,2020-10-13
iyux0b,[EFS] Trouble mounting EFS Access Point to ECS Volume,"Hi guys,

I ran into a problem trying to mount an ECS Volume to EFS through an EFS access point. 

The task role is set up with ClientWrite, ClientRead, and ClientRootAccess to that file system.

The access point is setup with posix userid 1001 and groupid 1001 with permission 755.

The cluster and the file system are in the correct VPC. 

But ECS failed to spin up a task with this error:

Error response from daemon: failed to copy file info for /var/lib/ecs/volumes/{{task-name}}: failed to chown /var/lib/ecs/volumes/{{task-name}}

I was able to spin up the task if i set the access point's POSIX userid and groupid  to 0 as in root. But i feel like its not the best choice for security reason in a shared FS.",1,aws,2020-10-13
iyu9zb,[Question] CDK security group,"Hey everyone,

I have been trying out the TypeScript SDK of Amazon's CDK service for the last few days in the context of an ECS based infrastructure.

Most things seem to work as expected but I have encountered some issues related to security groups.

I'm trying to achieve two things:

1) Create a security group that accepts incoming traffic from another security group located in a different VPC connected by a VPC peer connection.

2) Create a security group that accepts incoming traffic on whichever random host port ECS chose. This corresponds to setting a container's host port to 0 which means that a random port will picked by ECS so as to be able to run multiple copies of the container on the same host.

3) If (2) is not possible somehow reference an ECS service (since I'm in `awsvpc` network mode) instead of security group to automatically allow a container and port to send ingress traffic.

By looking at the CDK GitHub repository it seems that some of these features mights still be in development however that is not very clear to me and seem weird since these are pretty basic features that would be required by very simple ECS stacks.

Hope that some of you might be able to help on this one!",1,aws,2020-10-13
iyuhzd,CloudFormation Stack Drift,"Hi guys, 

I have a small amount of stack drift on one of my CF templates, is there an easier way of adding the drift into my template other than manually? The ""drift"" are changes that I want to be in the template going forward. 

Thanks",1,aws,2020-10-13
iyqfuu,AWS Social Values,"I am looking at applying for some roles at AWS and wanting to hear from people working there.

I am obviously aware of the Leadership Principles but I want to hear more about their principles when it comes to people, society and general charity work done by the organisation.

There has obviously been a lot of bad press on Amazon and how it treats its employees.

Does AWS do a lot of positive work though?
Are they avoiding discussing and voicing a stance on the issues currently being seen in the US?
Are employees encouraged to give back through volunteering, mentoring, etc. through internal programs?",0,aws,2020-10-13
iytnll,Scalable CRON/task scheduler architecture on AWS?,"Is anyone successfully running a scalable CRON/task scheduler architecture on AWS, possibly with just AWS services? Would love to hear your thoughts

I'm talking about thousands of user defined CRONs. (CloudWatch Events have quite a low limit. I know you can increase it, but not to these levels)

Also interesting to hear whether you guarantee exactly-once invocation, to safeguard non idempotent operations? I know Kubernetes CronJobs or CloudWatch Events can trigger twice on same invocation",4,aws,2020-10-13
iytkxq,"Why AWS DocumentDB charges hourly for instances and DynamoDB only charges for read, write and storage?","I'm new to AWS and little confused on Dynamo and Document pricing?


Thanks in advance",3,aws,2020-10-13
iyree9,Can AWS bring back the pinned favorites option in the navigation bar?,It was a good feature. Now there's a lot of wasted space in the nav bar :(,68,aws,2020-10-13
iyt4va,Can I force CloudFront to cache all of my resources at all POPs to avoid cache misses?,"Is there a way to tell CloudFront to immediately cache all of my resources in every POP (and keep them in cache for a long time, until I manually invalidate something) so that after that there are no cache misses at all?",0,aws,2020-10-13
iyrhni,How to create an account for copying files from Windows to S3 bucket?,"I found a AWS CLI reference that list cp commands that don’t seem too difficult, but there is more to it than just running the command.

The plan is to create a scheduled task in Windows that runs a script that copies files from a local folder to a specified S3 bucket daily at a specified time.

However, for this to work, we need to authenticate to the S3 bucket every time the script runs.

How can we create an AWS account that has access to write to and replace files in the S3 bucket, but doesn’t have other unnecessary privileges?

Our AWS accounts generally use MFA.  Since a script cannot run automated with MFA, how would we exclude the account from needing MFA?",0,aws,2020-10-13
iyr4yr,Security issue in CloudFormation was leaking role session credentials for customer accounts,,80,aws,2020-10-13
iyp0m2,Trouble launching g4dn instances in US-East-2.,"So at the moment I am trying to launch an instance to customize an AMI. It's a g4dn.xlarge in particular (or 2xlarge) so that I can install Nvidia drivers and the Nvidia container runtime. Most of today I haven't been able to launch the instance (US-East-2). I receive a ""We currently do not have sufficient g4dn.xlarge capacity in the Availability Zone you requested (us-east-2a).""

It doesn't matter which availability zone I choose as it goes, and I can come back later and it will eventually work.

My actual question revolves around elastic beanstalk. I have an app that uses this same instance type and I run into this same error when auto scaling kicks in, except it cycles through all the availability zones and tries both spot and on demand (to no avail). How do I account for this? Do I need to make the app in another region, and if so, what happens when this occurs there at some point in the future? Scaling up through the instance types probably works, but the lack of extra GPUs (until the g4dn.12xlarge) means I'm not scaling anything except my budget.

Thoughts?",3,aws,2020-10-13
iyodmf,S3/Cloudfront vs Cloudflare for Media Streaming,"Hello there. Currently I’m exploring two options for media streaming optimising for cheapest cost and best experience for end user. 

Option 1
Use ffmpeg to transcode video into different resolutions and formats.
Upload them to S3
Use cloudfront in front of them to deliver it to users

Option 2
Use a service like Cloudflare Stream.

Option 1 seems like a cheap solution because it’s basically you setting up the whole infrastructure, but let’s say if I have a 2-3TB of data transfer per month, then I believe it might be a lot more expensive compared to Cloudflare stream which charges completely on video watchtime and not on bandwidth?

Has somebody used both and can share their experience?",2,aws,2020-10-13
iyoabc,What do you use to orchestrate automation across AWS accounts?,"My organization has many AWS accounts, and it is becoming clear that there will be benefit in having a centralised automation solution to execute and visualise various forms of automation (e.g. AWS SDK/CDK scripts, CloudFormation, Terraform etc.)

I am wondering what everyone else uses / recommends as a way to centralise this automation, rather than everyone running scripts from their own machines using local credentials. 

Could a CI/CD server fill this need (like CircleCI?) which could assume an ""Automation"" type role in one or more AWS accounts and execute a script or IAC deployment? Are there other tools to do this job?",2,aws,2020-10-13
iyo67c,Amazon DynamoDB - Can I schedule nightly SQL script?,"Hi friends,

I’m hoping to use DynamoDB to host my database (consisting of about 8 tables) for my Power BI practice project.

I was just wondering how I can schedule a SQL script to run at a specific time every weekday (Monday to Friday). This script will do things like: create new records in one table based on another table; and update some records using the WHERE and IF clause. Is this possible within DynamoBD or do I need to involve other tools/apps to auto-run this scheduled SQL script?",1,aws,2020-10-13
iym28v,Does SES log failures due to DMARC rejection?,"Earlier today I had some trouble with mail delivery from SES. It was my fault, as I was trying to send mail from a domain that had GSuite SPF/DKIM records, with DMARC set to reject all failures, rather than the sub-domain that was configured for SES.

However, it threw me off for a bit because I would get emails with the subject ""Delivery Status Notification (Failure)"" saying ""An error occurred while trying to deliver the mail to the following recipients: &lt;my email&gt; &lt;contents of the email I was trying to send&gt;"", but in the SES dashboard, all emails were accounted for in Deliveries and I had 0% Rejects, Bounces, and Complaints. So the emails I was getting indicated a problem (which the SES mailer daemon could detect), but the SES dashboard did not.

I would put in a support ticket since this seems like a bug but I'm on the basic plan (so no tech support).

Has anyone else encountered this? Is this a bug? I cannot seem to find documentation to support that this is intended behavior.

If anyone wants to try to reproduce this, try to send email through SES from a domain setup with DKIM for any non-SES provider and your DNS DMARC record set to ""v=DMARC1; p=reject; pct=100;"" (you could also throw in ""sp=reject"" to match mine completely but I suspect this would happen regardless). You should get emails from the SES mailer daemon (sent to the address you are attempting to email from) saying your emails cannot be delivered, but no rejects will appear in the dashboard.",3,aws,2020-10-13
iylfqt,Workspaces - queries,"Hi all

Is anyone using Workspaces for users primary device ? 

Is it easy to deploy applications ?

Has anyone been able to enroll into Intune ? We are looking at how we manage OS updates and possible App deployment, issue might be that the underlying OS is server - which Intune won't fit this brief",1,aws,2020-10-13
iylbrl,CFN and Secrets,"Im trying to create a database connection string for an environment var like this:

    - Name: ConnectionStrings__TenantConnection
                  Value: 
                    Fn::Join:
                    - ''
                    - - 'Data Source='
                      - '{{resolve:secretsmanager:${SecretKey}:SecretString:host}}'
                      - ','
                      - '{{resolve:secretsmanager:${SecretKey}:SecretString:port}}'
                      - ';User Id='
                      - '{{resolve:secretsmanager:${SecretKey}:SecretString:username}}'
                      - ';Password='
                      - '{{resolve:secretsmanager:${SecretKey}:SecretString:password}}'
                      - ';Database='
                      - !Ref 'DBName'
                      - ';Connection Timeout=15'
                      - ';Min Pool Size=50'
                      - ';Max Pool Size=200'
                      - ';Pooling=True'

(Yes yes, I know this will be visible in the console, but happy to just get this working for now.  It's better than hard coding the password :)

Trouble is, I get an unhelpful error message:

https://preview.redd.it/dlpvni7h9zo51.png?width=993&amp;format=png&amp;auto=webp&amp;s=97a0ea1ea090f0c9dac7373cbe030592814209c4

Ive looked at lots of posts about dynamic things, and transforms and macros but it just does my head in.  Is there a simple way to do this?",1,aws,2020-10-13
iyjc02,Shutdown Order Control for Amazon Linux 2?,"I have a couple of services (GitHub Enterprise and a Harbor registry) in a separate VPC that I need to operate on from a given autoscaling group to tweak relevant security groups to

* White-list nodes for ingress on boot
* Remove SG rules on shut-down

This has been working pretty well for a long time, but recently, I've started having security group rules piling up on me.  My latest attempt at controlling the shutdown order is the following `systemd` unit.

    [Unit]
    Description=Service for dewhitelisting instance IP
    DefaultDependencies=no
    Before=shutdown.target
    After=network.target
    
    [Service]
    Type=oneshot
    ExecStart=/bin/bash /root/security_whitelist/stop_security_whitelist.sh
    TimeoutStartSec=0
    
    [Install]
    WantedBy=shutdown.target

I lifted most of what's above from the middle of the page [here](https://www.golinuxcloud.com/run-script-with-systemd-before-shutdown-linux/), and I got the `After=` portion from [here](https://unix.stackexchange.com/questions/294047/how-to-write-a-systemd-unit-that-will-fire-before-networking-goes-down).  I switched it to `network.target` instead of the original `network.service`, because the latter wasn't actively running on my test instance.

Could I get some guidance on how to achieve the desired behavior?  I just want a shutdown/terminate event to trigger running my `systemd` unit while the node is still online and able to do networking.  Thanks.",0,aws,2020-10-13
iykloz,Reusing RDS connection in Lambda,"[https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html](https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html)

This tutorial has an example of re-using a db connection, but it seems it is lacking a check and then reconnect if the connection is gone?

This post has an example

[https://medium.com/capital-one-tech/best-practices-for-aws-lambda-container-reuse-6ec45c74b67e](https://medium.com/capital-one-tech/best-practices-for-aws-lambda-container-reuse-6ec45c74b67e)

How do you re-use your RDS connection?",1,aws,2020-10-13
iyk0ol,EC2 Instance vanished (not a region issue),"Hello! I set up a Windows Server-based EC2 instance on the AWS free tier a few weeks ago in order to remote host a node server for a (low-demanding) game. It's not the optimal set-up, but its very easy to maintain the odd weekend or so when I need to do anything.

Anyway, this weekend I tried to Remote Desktop into it to restart it, when I hit an internal error. I logged onto AWS to see if there was anything wrong with the instance (I could tell it was still running as I could connect to the server through my browser), but it had disappeared:

[\\""You do not have any instances in this region\\""](https://preview.redd.it/bblrkxlzwyo51.png?width=956&amp;format=png&amp;auto=webp&amp;s=8d86135395da325b3a8d2120edb78189c3757fb7)

I tried remote desktop connecting again today, and it worked. You can clearly see that I'm connecting to an instance in the eu-west-1 region (Ireland):

[\\""eu-west-1\\""](https://preview.redd.it/91tcjmvaxyo51.png?width=667&amp;format=png&amp;auto=webp&amp;s=6abef76c69a0e66335e6dba29fb0a7d04ef6ef1f)

Even though I definitely have an instance in Ireland, which I can connect to with Remote Desktop, which is still running the application I can connect to in browser, it does not appear in the list of instances in its region, or in any region. As I'm on the free tier, I cannot open a case for technical support. I don't suppose anyone has any ideas what might be going on here? Thanks very much!",2,aws,2020-10-13
iyjzql,Static website hosting - optimal stack in 2020,"What is a good stack to serve a static website with ssl, custom domain and minimal latency in all major regions?
I am using Cloudfront at the moment but I don't like the fact that I can't force it to cache all my resources in every cache location with 1 click and keep them in cache forever until I invalidate them. Cache misses make me sad.
How do YOU do it?",1,aws,2020-10-13
iyitna,Official Language Training Courses from AWS,"These are official training courses from AWS for Node, Python, and Java. 

Standard Disclaimer: I am consultant at AWS. I do not speak for the company nor do I get any kickbacks if you decide to get the “certificates”. I thought they would be interesting to the community. 

Java:

https://www.coursera.org/learn/building-modern-java-applications-on-aws

Node:

https://www.coursera.org/learn/building-modern-node-applications-on-aws

Python:

https://www.coursera.org/learn/building-modern-python-applications-on-aws",2,aws,2020-10-13
iyj80f,Need to connect VPC to client through a Elastic IP,"Security for my client requires that my (AWS) hosts access said client through a dedicated IP.  My AWS hosts autoscale up and the IP change frequently.  Our VPC is a stock VPC consisting of 4 subnets all public with a route table that looks like:

    172.31.0.0/16   local    
    0.0.0.0/0       igw-eXXXXXX  

All hosts can directly connect to the internet.

What I attempted to do was to add a [NAT Gateway](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html) with an Elastic IP. My thinking was that this would route the traffic to this host through this gateway. The new route table looks like this:

    172.31.0.0/16     local  
    0.0.0.0/0.        igw-XXX 
    50.XXX.XXX.XXX/32 nat-XXX 

Here you can see that any traffic bound for that specific IP is supposed to use the NAT Gateway. Alas it doesn't work. The traffic does not appear at our customer and I'm at a loss. Can someone explain if this is correct or not.  I set up flowlogs and it appears that its not correctly routing it - it's still attempting a direct attempt.",1,aws,2020-10-13
iyi1ka,KMS Key Policy in CFT: Cart before horse issue?,"Hi all! I am trying to create a KMS key that will be used to encrypt objects in an S3 bucket among other things. However, the \`Principal\` section of the key policy references Arn's that don't yet exist as they are created in a stack that gets created AFTER my KMS stack.

Is there a way to work around this? Can I create the KMS key in a stack called foo-key, then launch other stacks like foo-web, foo-bucket, etc., and then have a foo-key-policy stack that applies the KeyPolicy to the key created in foo-key?

Scratching my head on this one.",1,aws,2020-10-13
iyha9f,[SES] Is there a way to list all emails and their status,"Im using SES and I'm trying to configure something like this (img). Does AWS have a dashboard too see the sent emails and their status? If not, where I can find tool like this? Is there a way to request all bounced emails?

https://preview.redd.it/5k5yih0e9yo51.png?width=1026&amp;format=png&amp;auto=webp&amp;s=de9233fde66ac7755531b5ee296e575e1001a8fa",1,aws,2020-10-13
iygerr,"Building Scalable GraphQL APIs on AWS with CDK, TypeScript, AWS AppSync, Amazon DynamoDB, and AWS Lambda",,15,aws,2020-10-13
iyghoy,AWS and Azure connectivity via Internet Gateway,"Hello Gents, can AWS Internet Gateway be used to connect AWS to Azure? If yes, what is the best approach? 

Thanks!",2,aws,2020-10-13
iyg4ta,how to create an EC2 instance running Windows that can also run Docker Desktop?,"I'm working at a new company where the developers must use EC2 VMs as their primary development workstations.  We're finding that none of them can run Docker, which is a crying shame.

So, we need EC2 VMs that:

* run Windows (so that we can use our normal tools like Visual Studio, not VSCode)
* can run the Docker Desktop for both Windows and Linux containers

Is anyone doing this?  Can you please point me to some instructions or walk me through it?  I'm guessing there is a specific AMI or some such to pick when creating the instance.

Thanks!",1,aws,2020-10-13
iyf8v6,"How to do basic log audits for an instance that went unresponsive, required reboot?",,3,aws,2020-10-13
iyfh88,AWS Landing Zone Template v2.4.1,,0,aws,2020-10-13
iyf47q,route53 resolver with eks troubleshooting,"In account A I have  an EKS cluster in a VPC created with eksctl, we will call  this  VPC0. In account B I have two VPC's. VPC1 in Account B has an elasticache cluster. VPC2 has a private  hosted zone that has a CNAME that points to the DNS for the elasticache cluster in VPC1.   


All of these VPC's are peered together and I have set up the route53 resolvers to allow VPC0 to resolve the DNS in VPC1.  


I am able to connect to the elasticache  cluster in VPC1 from inside of a container in my cluster in VPC0 using the AWS assigned DNS, but not the CNAME. I set up query logs for the EKS cluster. In the cloudwatch logs I can see the DNS query, and the  CNAME is resolving to the correct address (the AWS assigned address). However, I cannot connect to the elasticache cluster using the CNAME. The error is:  


 ""Could not connect...temporary failure in  name  resolution""  


Do I need  to make change to the alb ingress or a security group somewhere? Unsure why the  logs show things resolving correctly, but I still cannot resolve from the container. All my containers are running on fargate.",1,aws,2020-10-13
iyergh,Best practise for uploading multiple objects from Lambda to S3?,"Hi,

I have the following setup:

File arrives in S3 -&gt; Lambda downloads file and creates ~500 new files from it (None of them are larger than maybe ½mb but we need the info to be in different files) -&gt; Lambda uploads those 500 files to another S3 bucket

This happens roughly every minute. 

What’s the best practise recommended here?

I wrote a small function in python using boto3 but looping through 500 files and uploading them sequentially started to take 30-40 seconds which doesn’t seem like the best possible solution, since uploading everything as a single file takes less than a second. 

I know that Lambda can do some sort of multithreading but boto3 isn’t multithreading safe so I’d have to [create a new session](https://github.com/boto/botocore/issues/1246) for each thread. 

AWS created a guide for [multithreading in Lambda](https://aws.amazon.com/blogs/compute/parallel-processing-in-python-with-aws-lambda/) but that’s not about S3 and I’m not sure if there is a difference here that I’m overlooking. 

Thanks!",3,aws,2020-10-13
iyei1f,Can you negotiate 2% off your bill by switching from credit card payments over to invoice payments?,"Hello!

Just read this Wednesday excellent newsletter by Corey Quinn and this bit caught my eye:

&amp;#x200B;

&gt;Once you get into enterprise territory—which is somewhere in the $1 million/year range for cloud spend—both Amazon and companies that are paying Amazon switch over to invoice payments, which are paid via check, wire, or ACH. A customer might be able to negotiate 2% off of their bill while Amazon avoids getting hit with higher credit card processing fees.  
 

Assuming we are already doing invoice payments, does anybody has been able to negotiate that 2% off? What would be the correct channel for doing it?",7,aws,2020-10-13
iyehk7,POIP Certificate help,"\*Solved. Read comment below\*\*

&amp;#x200B;

Hi all,

I recently set up a Teradici server through AWS (I needed to configure with a server rather than the AWS connect since I need the ability to use smart cards) and I am almost done with my set up, but have one issue I cannot seem to bypass. I have uploaded our Godaddy wildcard certs to the correct location inside of our Teradici Linux server. When I go to log into the PCOIP Wyse box, I get the notification: "" **Cannot verify the identity of the server you have contacted**. Your credentials **will** not be secure. **Contact** your administrator **to** ask if this **server can** be trusted "". I see a few forums on other sites about this but have been unsuccessful in getting the issue solved. When I click the button to view the certificate, I see that it is, in fact, the correct certificate loaded into the system. Does anyone have any thoughts on how to fix the issue?

&amp;#x200B;

\*Edit for context\*

I have tried reaching out to Teradici support and no one has gotten back except to say I need to buy support and when I asked about pricing no on responded.",0,aws,2020-10-13
iyedhc,Question about AWS SDK for Cognito,"I've been trying to automate the user creation for my user pool, this is what I have got:

`response = client.admin_create_user(`  
 `UserPoolId = user_pool_id,`  
 `Username = email,`  
 `UserAttributes=[`  
            `{`  
 `""Name"": ""email"",`  
 `""Value"": email`  
            `},`  
            `{`  
 `""Name"": ""phone_number"",`  
 `""Value"": phone_number`  
            `}`  
        `],`  
 `TemporaryPassword = ""123456""`  
    `)`

The problem is, when I create a user manually, I get the option to not send any invitation message whatsoever, but I don't see any parameter for that in the admin\_create\_user function. I don't want the users getting a welcome message once I create their account.

How do I achieve this?",1,aws,2020-10-13
iydvl2,Question about resolving private network load balancer DNS to clients connected via OpenVPN.,"Hello.  My question is if there is an easy way to get each of my OpenVPN connected clients to resolve my private network load balancer.  Here are the details to my setup.  

I have a VPC that has 1 public subnet that hosts our OpenVPN instance.  I have 3 private subnets that each host varies nodes of various clusters.  I have a private network load balancer created that has 3 healthy registered targets.  I used a network load balancer because all of the ports that I use for our API's are all 8000 or higher.  All the instances can resolve the DNS name of the private network load balancer to it's 3 associated IP addresses just fine.  The OpenVPN server can resolve the IP addresses of the load balancer just fine.  Of course my clients can not.  

If I use any 1 of the 3 IP addresses associated with the network load balancer and connected to the vpn then everything works fine.  I don't want to use any 1 of 3 when using a name that points to all 3 would work so much better. 

I checked what DNS server the OpenVPN server was using and in resolv.conf it shows 127.0.0.53.  I noticed a setting in OpenVPN that lets the clients use the DNS server of the OpenVPN server but I doubt that would work.  If it's that simple then that would be great.  I can't currently test that as users are logged in.

Is there a way I can have the clients connect to the load balancer either through it's name or a new IP that points to that DNS name?  Currently they are hitting the clusters with just a single node's IP and that is not going to work in a production environment.  Also, that IP may change if an instance fails and gets rebuilt via Auto Scaling.  That's why I want to utilize the network load balancer.  Thanks in advance for any info!!!",2,aws,2020-10-13
iydvbf,Best resource to learn how to use python in AWS?,"Could be labs, documentation, etc.  ranging from any difficulty

Willing to spend money if that matters",2,aws,2020-10-13
iyd89r,S3 Path Deprecation Plan Updated,,18,aws,2020-10-13
iyd84u,S3 Path Deprecation Plan Updated,,131,aws,2020-10-13
iyce3k,How can I see the details of each bounced email in aws?,Trying to validate users/email addresses of bounced emails. Any advice on how I can accomplish this?,1,aws,2020-10-13
iybxux,SES and Emptying the Account-level Suppression List,"Been using SES for a long time now. Just recently discovered the [account-level suppression list](https://docs.aws.amazon.com/ses/latest/DeveloperGuide/sending-email-suppression-list.html#sending-email-suppression-list-enabling) -- this will save some hassle with managing our own list so I'm a big fan and will enable this very soon.

As near as I can tell, stuff never ages out of that list though (right?). Is there a simple CLI script to empty the list?  (yes, I have a use case :-) )",1,aws,2020-10-13
iyavat,"We are building a ""Spotlight for DevOps"". What do you think&gt;","Hello!

Two weeks ago, I shared here a [post](https://www.reddit.com/r/aws/comments/ir0ln7/im_building_a_search_engine_for_terminal_commands/) showing the [project](https://getsidekick.app) I'm, together with my friend, working on - a search engine for the terminal. Since then we received a lot of feedback and a lot of requests for access. Thank you! This feedback helps us a lot.

A lot of you requested to be able to execute commands right inside the app. We also talked to DevOps engineers and other infra developers. We learned how often painful is to interact with the big cloud platforms and understand them. This pain gets multiplied in teams even more.

We want to show you a new version of our app that is trying to solve all these problems for DevOps. We like to describe it as ""Spotlight for DevOps"".  It lets you control &amp; inspect your infra in just a few seconds.

Here's a video of a demo where I change my server machine and allow HTTP traffic in just a few seconds. No need to know any CLI commands or go to the cloud platform's dashboard.

[Spotlight for DevOps - control &amp; inspect your infrastructure](https://reddit.com/link/iyavat/video/3vgmahecnwo51/player)

&amp;#x200B;

I would love to hear your feedback!

**If the app interests you, here's the landing page where you can request early access -** [**https://getsidekick.app**](https://getsidekick.app/)**.**",40,aws,2020-10-13
iyaauy,Black screen when connecting to AWS Workspaces,"We've configured AWS Workspaces for use by some of our contractors and for the most part they seem to be working well. However, randomly one of the contractors will lose the ability to connect to their workspace. Once it's broken it seems to be permanent until we rebuild the workspace.

They are connecting with the latest version of the workspaces client and they see a black screen when connecting.

I can RDP into the workspace and I see an active session for their user account.

Things we've tried:

- rebooting the workspace
- disconnecting their active session
- [this](https://superuser.com/a/1138946)

Has anyone else run into this?",1,aws,2020-10-13
iy9q9y,AWS Org testing accounts,"Hi,

In our company we use an AWS org with Azure AD SSO for IAM. We have developers looking to test things like Control Tower, which means they need access to a master account of an AWS org. Obviously giving devs access to our prod master account isn't something we're going to do but creating new accounts solely for testing takes far longer than just spinning up a new sub account, as we still want some semblance of access management and billing access.

&amp;#x200B;

What are you guys doing for testing requirements like this? Do I just have to grin and bear it, or is there a solution I'm missing?",1,aws,2020-10-13
iy8vck,"Careful, ELBs are relatively expensive",Just a reminder to watch your billing stats as ELBs can get very expensive even with the most basic configuration.,0,aws,2020-10-13
iy8jvr,"I want to do a security audit on my company AWS account, not sure where to start",where do I start and check/test security vulnerability of our AWS account,7,aws,2020-10-13
iy2trt,Applying Savings Plans in large scale AWS environment,"Hi folks,

I have shared some thoughts on our approach applying savings plans in our organization which has multiple accounts spread across business units and AWS regions in this [blog](https://medium.com/@prn1982/making-the-most-of-aws-savings-plans-42b9879c2f37?source=friends_link&amp;sk=e1ae373eec28493fdf6beb293011433a).

Happy to get your feedback and ideas.

cheers!",1,aws,2020-10-13
iy788q,Need help with Kinesis video streams for facial analysis,"* I have to process the video stream to identify emotions. For which I need to use AWS Rekognition/custom model on SageMaker. With Kinesis WebRTC javascript sdk, currently video can be streamed only into the kinesis signalling channel. Signalling channel data is available for streaming only and not processing (ML). So, how can I get real time data for processing into Kinesis Streams from the frontend?
   * For streaming the video from frontend to backend into the Kinesis Video Streams for processing, I tested with Kinesis webRTC Javascript SDK and I am facing issues while implementing as mentioned above, so would Chime SDK serve as an alternative to this?
   * In Rekognition, ""create-stream-processor"" has a settings parameter. This currently only supports FaceSearch. I am looking to Detect and analyze faces. Is that possible with “create-stream-processor"" in the Python SDK? Or do I have to use the Java SDK?",1,aws,2020-10-13
iy78oo,Do you think AWS will open new regions in the US?,"Hello, currently there are Oregon, N. California, Ohio and N. Virginia. (not counting GovCloud). However, I was wondering if it would be possible to add more regions, like Texas for example. Azure has regions in those places.",1,aws,2020-10-13
iy5rb9,Stupid question regarding AWS Aurora Serverless pricing,"Hi,  


My understanding of the Aurora Serverless offer/pricing was that if I provision one of these then I only have to pay if I run any workload/query on the DB (+ of course storage and backup cost).  


But it seems to me that even for an idle workload I have to pay. Or did I mis-configured/misunderstood something?",2,aws,2020-10-13
iy3r5n,"Hi, I'd like to create separate organizations for different IAM users under my AWS root account without an IAM user having access to another organization's service such as a Lambda function. Is this possible?","Is the following setup possible?

 I am the root account and there are two organizations with the following services. 

* Organization A
  * Lambda function X
  * DynamoDB Table Z
* Organization B
  * Lambda function Y
  * DynamoDB Table Q
  * Cloud9 Instance G

IAM Users Joe and Kevin belongs to Organization A
IAM User Sally and Mitch belongs to Organization B

Can I arrange it so that Joe and Kevin cannot see what Organization B has created?

If anyone is wondering, the reason why I want them separated is because I have two different group projects for two different courses at my University and I don't want one member to potentially just tamper with another group's work",1,aws,2020-10-13
iy6yzd,My experience after 4 months of backing up Synology to Amazon Glacier,"I created a small project to [setup AWS to host Synologies backups to Glacier](https://github.com/0x4447/0x4447_product_synology_backup). And after 4 months of using it I must say I'm happy with the results. As of now I stored 31.2 GB and the total costs of this 4 months is of $5.01 where the first month was the most expensive due to the initial backup. The backup is set to be daily at 1AM and I normally add one or two documents a day. Nothing major. And then the occasional few GB video file once a month. Bellow is the full brake down. 

### June 2020 - $3.69

```
    Amazon Glacier EarlyDelete-ByteHrs  
    $0.012 per GB - Early Delete 2.036 GB-Mo $0.01

    Amazon Glacier Requests-Tier1
    $0.050 per 1,000 Requests 72,578.000 Requests $3.63

    Amazon Glacier TimedStorage-ByteHrs$0.06
    $0.004 per GB / month - Storage 13.994 GB-Mo $0.06
```

### July 2020 - $0.61

```
    Amazon Glacier EarlyDelete-ByteHrs
    $0.012 per GB - Early Delete 5.920 GB-Mo $0.02

    Amazon Glacier Requests-Tier1
    $0.050 per 1,000 Requests 9,250.000 Requests $0.46

    Amazon Glacier TimedStorage-ByteHrs
    $0.004 per GB / month - Storage 29.967 GB-Mo $0.12
```

### August 2020 - $0.15

```
    Amazon Glacier EarlyDelete-ByteHrs$
    $0.012 per GB - Early Delete 5.968 GB-Mo $0.02

    Amazon Glacier Requests-Tier1
    $0.050 per 1,000 Requests 63.000 Requests $0.00

    Amazon Glacier TimedStorage-ByteHrs
    $0.004 per GB / month - Storage 31.091 GB-Mo $0.12
```

### September 2020 - $0.56

```
    Amazon Glacier EarlyDelete-ByteHrs
    $0.012 per GB - Early Delete 3.572 GB-Mo $0.01

    Amazon Glacier Requests-Tier1
    $0.050 per 1,000 Requests 31.000 Requests $0.00

    Amazon Glacier TimedStorage-ByteHrs
    $0.004 per GB / month - Storage 18.748 GB-Mo $0.07
```",16,aws,2020-10-13
iy6wrq,"When using Step Functions, I find that the interaction between different paths is fiddly and hard to get exactly right. I made a web page where you can type in a payload, ResultPath, OutputPath etc and see the final and intermediate steps instantly",,22,aws,2020-10-13
iy6ev8,How to setup replication slave(On-premises MySQL) for MySQL server which is running in RDS.,Currently we are planning to create replication slave in on-premises MySQL for a MySQL server which is running in RDS. Do we need to take downtime to setup replication slave. If it's possible then please share documents.,2,aws,2020-10-13
iy4lvc,REST API using AWS Java SDK,,2,aws,2020-10-13
iy4huj,S3 html,"Is there any way for me to edit the html for my bucket so it displays a certain set of meta tags in the head? 

https://preview.redd.it/nmhbzmfr5uo51.png?width=516&amp;format=png&amp;auto=webp&amp;s=247bbb773e649075139ea5a1483cf55323b53c56

This is all I see when I go into inspect element on all of the objects in my bucket.",1,aws,2020-10-13
iy4cqw,How does changing EBS volume type from gp2 to io1 done on AWS backend without stopping instance?,"So, recently I had to change volume type for gp2 to io1 for one of my instance volumes which is like 5TB. How come they can do this shift without needing to stop the instance or stop the volume usage?

Or is it just a software lock like gp2 250MBps read/write is just a layer of AWS software limiting the usage to 250 MBps and when shifted to io1 it'll increase the limit to 1000MBps?",3,aws,2020-10-13
iy2w1o,Security September: Cataclysms in the Cloud Formations – One Cloud Please,,4,aws,2020-10-13
iy2ldg,How to use conda environments with a cluster notebook on an Elactic Map Reduce Cluster,"So I need to run some pyspark code for a project and I want to do it on an EMR. I also would like to use the notebook feature bc I already have the code saved locally as notebook files so the transition would be easy.

I need to use conda however to get the right package versions for what I am doing. Locally, I would activate the environment and type the jupyter command to open a notebook in that environment but that can't be done on an emr from the CLI. The localhost is inaccessible.

Is there a way to use the cluster notebooks but specify the conda environment? The regular amazon linux pip won't let me get a specific version of one package.  I need a conda env with python 3.7.",1,aws,2020-10-13
iy2kyn,DynamoDB and Quicksight connectivity ?,"Is there a way to connect DynamoDB with Quicksight. If not, is there an alternative BI tool that works well with DynamoDB and isn’t super expensive? 

Thanks in advance!",1,aws,2020-10-13
iy1la0,Anyone know how much local storage space a pythonshell glue environment has either with 1 dpu or 0.0625 dpus?,"I assume a 1 dpu pythonshell glue job has the storage space listed for 1 dpu on the normal glue docs (I can't remember off the top of my head but 20+GB IIRC) but am not sure if thats really the case, or how much local storage space there is on a 0.0625 DPU Pythonshell job.

I'm planning on using a pythonshell environment to pull files off of SFTP and load them into s3 so another normal spark glue job can process those files, but some of the files may range up to 1GB in size and some of them may need to be converted (e.g. xls to csv or xml to csv) so they need to be pulled into the local environment before being uploaded to s3 rather than streamed directly into s3.

But, I can't find any document anywhere that states how much local storage space a pythonshell job has.  Anyone know?  If not I guess I'll have to try setting up a pythonshell job that just prints the remaining available space on '/' and see what it says.",1,aws,2020-10-13
ixznh1,NACL question - wouldn't allowing 10.0.0.0/16 permit all further subnetted traffic?,,1,aws,2020-10-13
iy0ibg,"For the love of all that is holy, how can I resize/start/stop an EC2 instance inside Elastic Beanstalk on a schedule?","Hello! I've been fighting with trying to find answers for this for multiple days now but all I can find is a way to do this with plain old EC2. Here's my problem:

I have an Elastic Beanstalk environment that runs up to four t3.micro instances (usually no more than 2) but I've got a new nightly job I need to run which the micro can't handle with only 1GB RAM. I have tested and found the smallest instance the job will run on is a t3.large. ok fine - but a t3.large is 4x my cost so running it all the time just to handle a 10 minute nightly update is wasteful and unnecessarily expensive. 

So, my goal is to start a t3.large (or resize one/all of my t3.micros) a few minutes before the cronjob kicks off the nightly update, let it run the update, then shut it down after it's complete. 

I've been poking around with the instance scheduler which suggests setting up a whole stack including EC2, RDS, DynamoDB and a Lambda function that runs based off a CloudWatch alarm  - cool - I get that ... but I don't need all that. I've already got everything configured and connected and set up within my Elastic Beanstalk environment and I need the instance to be able to talk to the RDS instance inside that environment. I've also tried setting up time based scaling with a launch template and tried mixing the instance types and playing with the capacity percentages and all that stuff suggested for how to have an autoscaling policy with a mix of on-demand and spot instances, but all I was able to do was get it to scale up another t3.micro at the scheduled time, and I couldn't figure out how to tell the autoscaler I want to scale to a specific type of instance at a specific time or even based on memory utilization. I thought maybe that setting up that policy would allow me to run the job and when the memory consumption got too high it would scale up a t3.large for me but it just hung the t3.micro and nothing happened, lol. 

I just feel like I'm stuck in the mud with this and I've been burying my face in docs, youtube videos, tutorials, and even posted on the AWS forums two days ago but I've never once gotten a response there on any question I've asked so I'm not surprised I haven't gotten one there for this. 

Is there anyone who can point me in the right direction? This doesn't seem like it should be that difficult and it's killing my ability to release a really awesome update to my website which could turn into significantly more revenue if I could just figure this out. If I can't find a solution soon the only option I'm going to have is going to be increasing my EC2 costs 4x over just to run a stupid 10 minute nightly update so even though this update will probably bring in more revs I'd really rather not do that if I don't have to :) 

On another note, if I can figure out a way to resize/upgrade one of my instances on a schedule, is there a way for me to tell a cronjob to run on a specific instance (the t3.large)? I know for a fact it will hang a smaller instance so I just want to run it on a larger one and then shut it down.

Someone PLEASE help me before I throw myself/my computer/both off a high rise :-D

Sincere thanks to anyone who might be able to point me in the right direction!!",0,aws,2020-10-13
ixzs8r,AWS Optimizations,"Is anyone here looking for AWS cost/provisioning optimizations?   


I'm working to learn more about how and why people are optimizing their cloud deployments.  


Is anyone interested in learning more about your AWS resources and recommendations to decrease costs?  


I'm interested in chatting with just about anyone that has AWS deployments (small/medium/large etc) and would be interested in talking more about making your deployments better. We want to help you!",0,aws,2020-10-13
ixzfho,Why were the pins removed from the navigation bar?,"I had pins setup to 1 click navigate around the console. EC2, R53, Cloudwatch,cloudformation etc etc. Middle click the pin create a new tab to check some logs etc. 

Who in their right mind thought, you know what lets take away users ease of access to our services and make them click 2x as much. /rant",138,aws,2020-10-13
ixym4m,Should I split up my API from my workers for AWS deployment?,"I have a flask api that currently puts jobs on python-rq queues to be processed. The api and the ""processing engine"" is all part of the same flask code base and I launch it all together in a single docker container (with docker-compose). Let's say I have this app deployed on an EC2.

I am confused on how to scale this when there is a bottleneck on jobs and not the API. If I scale the EC2 instances directly, I'll have multiple APIs running, but the bottleneck is in the queues.

So should I instead split my API and Queues into separate docker instances? And then deploy each container on separate EC2 instances? That way I can load balance the queue servers separately but the API can still run on a single instance.

Just want to make sure I am thinking about this correctly.",1,aws,2020-10-13
ixyane,Live Stream using IVS save stream to a bucket,"Hi,  


I want to use the IVS live stream but also save the video simultaneously to a bucket to audit it. Is this currently not possible? Latency is actually not a big issue, should I use another service? Looking for an easy setup if possible.",2,aws,2020-10-13
ixw6cn,Am lost?,"I have a server I can't access as root with key pairs just with an user without sudo group and its password.

I did an image &gt; launch&gt; create new key pem&gt; created private key, login with ubuntu with putty and still get "" the server refuse your key"". does the security groups or vpn have anything else to do with this?

I can even login in the site and enter the mysql! whoever I can't access ubuntu user or root with key pairs.

&amp;#x200B;

The instance was created in January  in lightsail and I never used ubuntu with winscp. if I needed to use sudo I clicked the ""connect button"".  it clearly says: You configured this instance to use default (us-east-1) key pair.

However, I download that key pair from lightsail and is refused too.  the ""connect button"" works fine but I need to use it on EC2 not in lightsail anymore...

\*UPDATE\*

 ok I still don’t know why I can’t use Key pairs  in this server. even creating an snapshot and assigning a new Kp it does  not work. Even exporting to ec2 and changing every setting, KP do not  work on it. I might have changed something in the system half asleep and  now am  paying the price idk.

**The problems:** The magento user can not run setup upgrade with an external Database in AWS even if is working with magento.

Created external DB and   lightsail give you a master user. However  that user Can’t give grant all access to magento user. it lacks full  roles too, somehow. Therefore, I can't add the magento user full rights to DB so it can run setup:upgrade..............

A server that can’t be accessed with key pairs.

**How I got it fix.**  
 created 2 users and added to sudoers with password. 2 just in case.

Created an snapshot and exported to EC2.

Create external Db and its masteruser.

Login to ssh with sudoer, switch to root.

create an empty db in the external DB and leave \\q

mysqldump the DB from local. 

**So here is the trick:**  if you don’t do this you can’t import the  DB to external DB in AWS. and  in lightsail you can import but won’t have full access even as  masteruser:

*sed -i -e ‘s/DEFINER=root@localhost/DEFINER= aws DB master user@%/g’ DB.sql*

**THIS WAS IT.**

NOW export the DB to External DB:

mysql -h  [rds.amazonaws.com](http://rds.amazonaws.com/) \-u masteruser -p db &lt; db.sql

Now change env.php to assign the DB

Have a drink and thank God.",2,aws,2020-10-13
ixwr1x,How to properly handle S3 keys with a plus sign?,"`s3.copyObject` in the JS SDK is causing me all sorts of headaches when dealing with source objects with a plus sign (`+`) in the key.

If I have a key like: `path/to/object/foo+bar.ext` and I'm trying to copyObject it from mySourceBucket, normally what I'd do is:

`params = { CopySource: encodeURI('/mySourceBucket/path/to/object/foo+bar.ext');`

That would work with any other key I have thrown at it, including keys with (string literal) `%20`, or with spaces (and get encoded to `%20`)

Converting the `+` to a space or `%20` does not work, which somewhat contradicts what's said in this [thread](https://forums.aws.amazon.com/thread.jspa?threadID=55746) about S3's non-standard handling of pluses.

The only thing that works is to replace the `+` with `%2B`. But if I do that before the `encodeURL` it becomes `%252B`. So I guess I'll do it after.

(`encodeURIComponent` does not work here, because it encodes the `/` in keys, which does not work)

Is that the only way to handle this? Are there any other characters that need to be handled as a special case?",1,aws,2020-10-13
ixvqrf,How do I know if I need the AWS developer plan? (Beginner),"Hi,

I'll try to get the point as fast as I could but I feel I should give some context. 

So we are working on our bachelor's graduation project and the training dataset is fairly large (50 GB). We have never used AWS before because we have never worked on an ML problem with this scale. I have consulted some professors and they suggested starting with the EC2 c5a.24xlarge instance (96 cores, 192 GB RAM). 

I have read what the developer account ""can provide"" but I'm not sure what most of what's written actually means... So in layman's term, in what cases will one need the developer account?

&amp;#x200B;

Thank you",7,aws,2020-10-13
ixvgek,Is NLB part of the Free Tier?,"In the [ELB official FAQ](https://aws.amazon.com/elasticloadbalancing/faqs/), it claims that Free Tier is offered on NLBs for new AWS accounts, however when I browse on [AWS Free Tier](https://aws.amazon.com/free/) page it only states Classic and Application Load Balancers (shared 750 hours). Can someone confirm?",1,aws,2020-10-13
ixv6a8,Roadmap for RDS Aurora PostgreSQL,"Hi,

We have an application that is going to force to use PostgreSQL 12, however we don't know when the RDS Aurora PostgreSQL Team will update the engine to 12.

We love RDS Aurora but when we can't control the dependencies of an application, then we're in problems.

We're going to contact AWS support but I wonder if anyone in the community has some inside info about the roadmap for upcoming releases.",10,aws,2020-10-13
ixuyb0,Why can't use OpenCV-Python in AWS Lambda?,,3,aws,2020-10-13
ixv3mw,REST API using AWS Java SDK,,4,aws,2020-10-13
ixufi8,RHCSA/RHEL or CompTIA Linux+?,Which would be more useful in terms of getting a job as either a solution architect or dev/ops?,2,aws,2020-10-13
ixrzkc,Using AWS Config from another account than the organization master,"EDIT: Solved, found the answer here: [https://aws.amazon.com/about-aws/whats-new/2020/05/now-deploy-aws-config-rules-and-conformance-packs-across-an-organization-from-a-delegated-member-account/](https://aws.amazon.com/about-aws/whats-new/2020/05/now-deploy-aws-config-rules-and-conformance-packs-across-an-organization-from-a-delegated-member-account/)

I'm setting up AWS Config across an Organization, the end goal being:

* AWS Config aggregates data for all regions in all the organization accounts
* AWS Config S3 bucket is in a '*logging*' account
* Operationally, people use a '*security*' account to work with AWS Config (creating rules, accessing the AWS Config dashboard, etc.)

I understand that the Config Aggregator, Config Recorder and Delivery Channel, must be created in the organization master account. Storing AWS Config data in a S3 bucket in a separate '*logging*' account is pretty easy. However, once the data is there, I can't find how to *""link""* another account to AWS Config so it can display the dashboard, manage rules, etc.

Is this actually feasible? If not, do you all use AWS Config from your organization master account? I'm a little reluctant to give a role to all security people in the organization master account given how privileged it is...

Thanks for your insights!",3,aws,2020-10-13
ixt6b9,Why is Google hitting my CloudFront?,"So I finally resolved the hotlinkers/leechers (no thanks to AWS Paid Support), but now I'm finding this in my logs:

    2020-09-22	14:56:32	ATL52-C1	1270	66.249.88.146	GET	[mydistropoint].cloudfront.net	/movie/A/[first movie in that origin].mp4	403	-	Mozilla/5.0%20(X11;%20Linux%20x86_64)%20AppleWebKit/537.36%20(KHTML,%20like%20Gecko)%20Chrome/56.0.2924.87%20Safari/537.36%20Google%20(+https://developers.google.com/+/web/snippet/)	-	-	Error	EQjHMNCyWjWJDUX92gxoGOWI-8Y9oa_yvvxxGWcN0-5bqOan-QYZ9Q==	[mydistropoint].cloudfront.net	http	396	0.000	-	-	-	Error	HTTP/1.1	-	-	53876	0.000	Error	text/html	919	-	-
    2020-09-22	14:57:14	IAD79-C1	1270	66.249.88.143	GET	[mydistropoint].cloudfront.net	/movie/A/[first movie in that origin].mp4	403	-	Mozilla/5.0%20(X11;%20Linux%20x86_64)%20AppleWebKit/537.36%20(KHTML,%20like%20Gecko)%20Chrome/56.0.2924.87%20Safari/537.36%20Google%20(+https://developers.google.com/+/web/snippet/)	-	-	Error	PhCF5H5rW8uzZZSpsRKTe778p-y0QMpKyLbrQCVxFKhaYGnwxVuI7Q==	[mydistropoint].cloudfront.net	https	396	0.005	-	TLSv1.3	TLS_AES_128_GCM_SHA256	Error	HTTP/1.1	-	-	47810	0.005	Error	text/html	919	-	-

It does this over 2 minutes, then nothing for 5 minutes, then 2 more hits over 2 minutes.

I know from the error message that it's being blocked (as it should be), but why is it doing this, and should I add a blocking rule to my WAF as a ""just-in-case""?",1,aws,2020-10-13
ixspb7,Transferred Domain from GoDaddy to Route 53 - Unable to get A Records working,"On Sep 15th (7 days ago) I transferred an inactive domain from GoDaddy to Route 53.  

I've created a hosted zone which contains the 2 default NS and SOA records, as well as A records for {{domain}}.com and www.{{domain}}.com.

Both of those A records for testing purposes are pointing to 172.217.14.206 (Google's ip address).

Neither of those A records are working and I can't figure out why.  The same exact record set does work for one of my other domains (http://internationalclimatechangeday.com).  So this leads me to believe something is up with this domain or hosted zone as a whole.  Is there a way to test/debug that?

Has anyone experienced something like this and is there typically a very long lag time between transferring domains and getting them up and running?

Running the bash command `nslookup {{domain}}.com` gives me an error:


    ❯ nslookup my-domain.com
    ;; connection timed out; no servers could be reached


Running the bash command for my working domain returns this:


    ❯ nslookup internationalclimatechangeday.com
    Server:		172.16.0.1
    Address:	172.16.0.1#53

    Non-authoritative answer:
    Name:	internationalclimatechangeday.com
    Address: 172.217.14.206


Thanks for all your help!",3,aws,2020-10-13
ixsiqf,Not responding alarm of ec2 instance?,"I have bunch of ec2 instances and sometimes with instances that consume more cpu/ram than its instance type, they get stuck.

I can’t login to the instance via ssh and must reboot or stop/start the instance. 

Is there a way with aws to create an alarm for this situation?",2,aws,2020-10-13
ixrw03,Can anyone help a noob with certificate renewal?,"I keep getting emails from AWS that I need to renew my certificate and I simply don't understand all the jargon in it:

&gt;AWS Certificate Manager (ACM) was unable to renew the certificate automatically using DNS validation. You must take action to ensure that the renewal can be completed before Sep 29, 2020 at 12:00:00 UTC. If the certificate is not renewed and the current certificate expires, your website or application may become unreachable.  
&gt;  
&gt;To renew this certificate, you must ensure that the proper CNAME records are present in your DNS configuration for each domain listed below. You can find the CNAME records for your domains by expanding your certificate and its domain entries in the ACM console. You can also use the DescribeCertificate command in the ACM API\[1\] or the describe-certificate operation in the ACM CLI\[2\] to find a certificate’s CNAME records. For more information, see Automatic Domain Validation Failure in the ACM troubleshooting guide\[3\].

Can anyone walk me through what I need to do? Or just parse this into plain english?  I did some googling and checked the AWS forums but there didn't seem to be a clear list of steps. My website is registered with Namecheap, I use Route53 for DNS, I use Cloudfront for CDN, and S3 as static website bucket storage. I remember requesting a certificate to get HTTPS to work, but that's about it. Thanks in advance for your kind help!!!!",9,aws,2020-10-13
ixp10f,INSTALLING MOODLE 3.9.2 - HELP!,"Hello, r/aws community,

 I am a novice AWS customer, who has been trying and failing to install Moodle 3.9.2 on an EC2 Instance or on Lightsail. I always get so far, then I get stuck. I have build 4-6 servers already using YT tutorials and/or blogs. But they all have two things in common, they are either too old (out of date), or just don't work. Tutorial after tutorial, server after server, I am ready to throw in the towel. It's not AWS, it's either me or it's Moodle.

Before anyone says it, I have tried Bitnami, and yes I did get it to work. But, my future student base is government and I will not get past having a 'middleman' in my installation. I have tried Kindle ebooks and the Moodle help pages too with the same result (out of date or simply don't work \[missing detail\]).

I'm on my knees... Can anyone either point me towards a tutorial that is up-to-date and step-by-step? Or, is this me on a quest for a Holy Grail?",1,aws,2020-10-13
ixqzeq,How to Get Past the 15 Minute Delay Limit in Amazon SQS,"Hi! Scheduling an SQS message for delayed delivery doesn't exist in AWS. Anything past 15 minutes requires you to build your own scheduler.

To solve this, we've built an API for an SQS task scheduler. Call the API with your delay and message, and it will publish to your queue at the scheduled time. Supports delays up to a year, with production level SLA's and traffic. You can see how it works here:

[https://medium.com/swlh/how-to-get-past-the-15-minute-delay-limit-in-amazon-sqs-fba3c50daf0b?source=friends\_link&amp;sk=f0015b683f9df96e4974bb1c37b2d785](https://medium.com/swlh/how-to-get-past-the-15-minute-delay-limit-in-amazon-sqs-fba3c50daf0b?source=friends_link&amp;sk=f0015b683f9df96e4974bb1c37b2d785)

Let me know what you guys think, and if anyone would find this useful!",19,aws,2020-10-13
ixqddt,Using AWS for on-premises Veeam's secondary backups,"Hi

We have a customer that needs a secondary backup repository on the cloud. We were thinking on using VTL Storage Gateway and using tapes, but the customer just wants 1 month retention and the files to be as close as possible (so, if we went to VTL Gateway we should use a 1 month retention media pool and don't eject tapes so they don't fall on Glacier).

We are thinking if there's a better approach, like for example using Storage Gateway on ISCSi volume format, presenting a volume to Veeam and using it as a backup repository for doing backup copy.

Will it work? Will it work better than VTL? Is there any simpler solution?

Any help is welcome. Thank you so much in advance.",5,aws,2020-10-13
ixqc6t,Automating start and stop of AWS instances on some process completion,"Automating start and stop of AWS instances on some process completion or python script line.  
I have to move data from compute instance to GPU based instance on a process completion to carry forward the next script execution on the new GPU instance.  
What kind of AWS service can help in solving this kind of problem?  
Is AWS Lambda helpful in this kind of problem?",10,aws,2020-10-13
ixocei,AWS Perspective - Visualize Workloads as Architecture Diagrams,,61,aws,2020-10-13
ixn9wb,ELB Redirect to different path issue,"Hi 

I'm having an issue configuring my ELB to redirect to a different path. 

I've configured a rule on my ELB HTTP listener to redirect traffic to a different path if it originates from a specific IP range.

I'm wanting to keep the traffic HTTP to HTTP and redirect to a path: fixtures.html 

However i'm getting an error in chrome when i browse of too many redirects.

Can anyone help please?",1,aws,2020-10-13
ixm619,anyone have ever done - docker vault image hosted in AWS ECS,"if so, a little help of guidance may be a great of help  
requirement  
vault docker image in AWS ECS (EC2)",1,aws,2020-10-13
ixmv93,nab3: An async boto3 ORM,"[https://github.com/WillNye/nab3](https://github.com/WillNye/nab3)

nab3 provides an easy to use interface for searching and inspecting a service and its related resources within AWS. It also creates useful relationships that aren't returned within boto3. An example of this is returning the Security Groups for an ASG or an ECS Cluster (outlined in the README).

More AWS services still need to be added so any contributors are welcome. There is a dedicated doc on contributing because as you may imagine trying to normalize boto3 can get hairy.",8,aws,2020-10-13
ixmumm,Managing AWS infrastructure using Ansible Tower,,9,aws,2020-10-13
ixmifw,Amazon Athena Custom/Dynamic Front End?,"I know building a front end for Athena is fairly common, but I’m looking for some resources on a previously executed frontend for Athena with user generated query inputs.

I want to use some kind of Intellisense to help the user out too, so if you have any library recommendations pls let me know.

Thanks!",4,aws,2020-10-13
ixmeso,cloudwatch dashboard - account costs single widget,"hi,

i've created a single custom cloudwatch dashboard for my product with relevant metrics, what i wouldn't mind is a single number shown in the widget with cost estimate, ideally for the tagged stuff i'm using but for the whole account is fine. 

i think the answer is no, but doesn't anyone know if this is possible and if so how?

thanks.",5,aws,2020-10-13
ixlskc,Status check of stopped ec2 instance in Ok state?,"I have aws ec2 instance and I created status check alarm in “Status Checks” tab of the instance. 

I expect when instance have fulldisk or overload cpu, the alarm will be triggered as [aws doc](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html). 

But I also expect when the instance in stopped state, alarm should also be triggered as status check should fail?

Is there a way to test or manually test this alarm of ec2 instance? As I want to trigger alarm and send mesg to sns topic for lambda to process. 

Is there a json object for this sns come with this kind of alarm?",2,aws,2020-10-13
ixliza,Which serverless framework is used more in AWS related jobs?,"There is SAM, Serverless, Claudia, etc.   Which one seems to be more prevalent in the workforce?",14,aws,2020-10-13
ixh908,Deploy CloudFormation stacks and manage AWS Organizations with Takomo,"I've built a tool to deploy CloudFormation stacks across multiple accounts and regions. It also integrates with AWS Organizations and lets you manage member accounts, organizational units, and policies.

Here's a link to quick start guide [https://takomo.io/docs/getting-started/quick-start](https://takomo.io/docs/getting-started/quick-start)

Any feedback and suggestions are welcome.",3,aws,2020-10-13
ixk814,How to throttle SES from Lambda,"Even the AWS documentation has an example where an email is sent from Lambda directly via SES. But SES also has some limits of messages/time-unit. It would be super nice if SES would just queue and throttle the emails automatically - but IIUC that is not the case.

Now the question is how does everyone builds this throttling mechanism?
Lambda -&gt; SQS -&gt; Lambda -&gt; SES?
Or is there a simpler way?

Would be great to hear some production stories and get some pointers.",1,aws,2020-10-13
ixin3k,Multi-region implementation for AWS KMS,"I am trying to implement a multi region strategy for my lambdas which use dynamodb global tables.

I want to use KMS to do client side encryption for the userId and pin. How can I implement a multi region strategy for KMS. I found this example which talks about the same 

https://aws.amazon.com/blogs/security/how-to-use-the-new-aws-encryption-sdk-to-simplify-data-encryption-and-improve-application-availability/

Does that mean I have to create KMS keys in all the regions I deploy my lambda (for latency)?

Question: If so, if I provide multiple regions in MultipleProviderFactory, which of the keys does it use to encrypt data and which region does it use to decrypt data?

Bonus question: How will this change if I have to encrypt data bigger than 4096Kb?

PS: Would be super happy with an answer for first question.",2,aws,2020-10-13
ixjh7u,Security September: Cataclysms in the Cloud Formations – One Cloud Please,,15,aws,2020-10-13
ixhlvd,API Gateway consistently inconsistent delay trouble shooting,"When making requests through the API Gateway service I keep getting a 4-5s delay on about 80% of the requests. Even when I just hammer the same endpoint over and over. I’m trying to figure out how to find out where this delay is coming from. 

So, the architecture is nodejs app/services in containers on ECS Fargate behind the regional API Gateway, no lambda. API Resources are proxy type, using a VPC link to an NLB. 

From what I can tell the delay doesn’t appear the be the nodejs code as watching console logs and db activity is showing no such delay. I’ve enabled X-ray for the API but all I see is that the delay appears to be happening after the NLB. But I’m at a loss for what to do next. 

Anyone experience something similar, or have suggestions for how to narrow down the search? Still learning how to use X-ray.",1,aws,2020-10-13
ixgafg,AWS billing is like the Babadook,"tl;dr: I cancelled an AWS account months ago. Had some hassle with inflexible billing policies already. Now Route 53 is auto-renewing a domain name on the account unless I log in to prevent it. I cannot log in because the account is closed.

Sorry for the long rant. Just blowing off steam here.

I opened an AWS account for my own research and teaching needs while at a former employer. The bills were handled through our department, but the account was my responsibility. All was good at the time, but I no longer work there, and terminating an AWS account turns out to be nearly impossible!

Two months after leaving that job, I got a message from the accountant in my former office. It turned out I was still being billed for a reserved EC2 instance, and there was no possible way to just pay the remaining charges all at once. Instead, I will keep getting bills until the instance's yearly agreement has run out. It would easily be possible to pay off the remainder via institutional funds now, but I can't keep sending tiny bills to my former employer months after I've left that job. So, now I'm paying those out of pocket until January.

Dealing with that was a hassle, because I no longer have access to the institutional email address with which I created the AWS account, and contacting customer support from another address doesn't fly, for obvious reasons. When I finally did get things sorted out, the customer service person assured me that those were the only remaining charges.

But now it turns out that a domain name I started via Route 53 will auto renew this month unless I log in and change it (which I can't do, because the account is closed). Now I'm going to have to go through all the same rigamarole as last time to get an agent to disable the Route 53 domain registration.

Why in the world can I not just say ""I no longer want this service. Please stop taking money from me."" Why do I have to comb through every one of AWS's hundreds of surprisingly non-integrated services to make sure there are no hidden automatic renewals? And why is it possible to ""close"" an account to the point that I cannot make changes, even if they will keep billing me for stuff unless I make changes?

Every time I think I've ridden myself of this cursed account, it shows back up like a goddam Babadook!

Edit: I think I should have been more clear about my complaint here. I understand what a reserved instance is. It would be nice if they would bill out the remaining 3 months at once, but that's not the biggest problem. The two things that are truly frustrating are:

  1. **Nothing should auto renew on a closed account!!** As it stands, Route 53 will auto renew a domain unless I log in to stop it. I cannot log in, because the account is closed.

  2. When canceling an account, there should be clear information about what remaining charges exist on the account. The account should not truly lock until there are no outstanding charges to be billed.",97,aws,2020-10-13
ixeunq,how to properly use WAF AWS Manage rules? which rules to choice that is cost effective?,"* I have node js
* Beanstalk
* RDS
* No 3rd party services
* linux

i have this enabled is it necessary to have all of them?

* AWS-AWSManagedRulesAmazonIpReputationListUse
* AWS-AWSManagedRulesKnownBadInputsRuleSetUse
* AWS-AWSManagedRulesCommonRuleSetUse
* AWS-AWSManagedRulesLinuxRuleSetUse
* AWS-AWSManagedRulesSQLiRuleSetUse
* AWS-AWSManagedRulesAdminProtectionRuleSet",3,aws,2020-10-13
ixexpb,What language for good paying AWS based jobs?,"I've been a .NET developer my whole career, primarily in large corporate environments.  Several years ago I started embarking on a journey with AWS out of personal interest.   I'm now thinking I want to venture out of the corporate world and into more modern cloud based full stack development, with a focus on AWS services. 

However, one thing has become very clear to me in looking at job postings for the past several months.  There seems to be a fairly big pay disparity between .NET developers and Python/Node developers in the context of cloud based development, with Node/Python generally showing higher salaries. 

I've had some experience with Node, but zero with Python.  I know this puts me at a disadvantage but I'm hoping my umpteen years as a senior developer would give me some leverage. 

My question for this post is, what is the more popular language for AWS based development?  Node or Python?",11,aws,2020-10-13
ixcpwx,"CloudFormation Conditions, dependency issue vs multiple condition not allowed.","In my CloudFormation template I have two conditions. The one checks to see if the template is run in the prod environment or in any other environment. The other condition checks a combination of wither the stack is run for the prod environment and whether the stack is for the Data or Ops team.

The conditions are called ""isProd"" and ""isProdAndForDataTeam"".

I create a few resources in the template. One of which should only be created if the stack is for the prod account, regardless of whether it's for Data or Ops team. The ""Condition"" I specify for this resource is therefore the ""isProd"" condition. So far, so good.

Now I have another resource that should only be created if it's both a prod stack and for the data team. Naturally I use the ""isProdAndForDataTeam"" condition for this resource. However this fails. CF complains about a resource dependency. This second resource references an Output from the previous resource. Because of the reference, I have to use the same condition, otherwise it fails. But one condition doesn't satisfy my use case. I also can't specify multiple Conditions on a resource.

What am I to do?",2,aws,2020-10-13
ixe4up,Kinesis VS Kafka OR EventBridge vs Kafka,"hello everyone, as the title says, I am seeking some clarity. First off, what AWS product is the rightful contender to Apache Kafka? Is it EventBridge or Kinesis. AFAIK EventBridge is for routing events based on rules &amp; Kinesis is for ingesting large amount of data, so a fair comparison should be Kinesis vs Apache Kafka. Am I right?

Second, apart from the managed component of Kinesis, why should one choose Kinesis over Apache Kafka. What are the benefits of using Kinesis over Apache Kafka?

I am coming from AWS mindset but I'd like to understand which product comparison, EventBridge vs Apache Kafka OR Kinesis vs Apache Kafka, is valid &amp; why/which AWS product is better than Apache Kafka, if any.

Thanks in advance.",2,aws,2020-10-13
ixbqpv,AWS Workspaces Windows LAN,"Reddit AWS gurus

has anyone ever setup a couple of aws windows workspaces along with a windows ec2 server and created a aws private lan? i want to be able to map a shared drive off the windows ec2 instance to the workspaces for testing. thanks again for your valuable input, i appreciate it.",5,aws,2020-10-13
ixb9vp,"My RDS instance went into ""storage optimization"" status","I have a customer instance that has used 3.75 TB of 100 TB allocated. (Oracle RDS). Was exporting data using data pump and the export failed due to lack of space on device and the instance went into storage optimization status.

Why?",5,aws,2020-10-13
ixbu2n,How to create a Step Function to move records from dynamodb to SQS?,"I've found an example in amazon that says it does exactly that:

[https://docs.amazonaws.cn/en\_us/step-functions/latest/dg/sample-project-transfer-data-sqs.html](https://docs.amazonaws.cn/en_us/step-functions/latest/dg/sample-project-transfer-data-sqs.html)

&amp;#x200B;

The first part involves a lambda function that does seeding, idk why. if I remove that step, I got this error:

&amp;#x200B;

{

  ""error"": ""States.Runtime"",

  ""cause"": ""An error occurred while executing the state 'Read Next Message from DynamoDB' (entered at the event id #2). The JSONPath '$.List\[0\]' specified for the field 'S.$' could not be found in the input '{\\n    \\""Comment\\"": \\""Insert your JSON here\\""\\n}'""

}

What I just want to do is to take each item in dynamoDB and move it to a sqs to later processing.

&amp;#x200B;

Thanks!",7,aws,2020-10-13
ixafai,Using Amazon Inspector with Workspaces?,"Has anyone successfully used Amazon Inspector with Workspaces? It's unclear to me whether it's compatible or not, as inspector documentation specifically states it's for EC2. It does seem like the only thing it would need is the right IAM permissions and the agent installed. 

Anyone done this successfully or gotten an official ""not supported"" answer?",4,aws,2020-10-13
ixaom3,"Use VPN to access AWS instances, pricing, solution?","I would like to protect further access to my AWS instances with a VPN.

If I understand correctly I have 3 options:

1. AWS managed VPN client
2. use a VPN provider
3. use an EC2 instance to create and manage my own VPN (?)

My feeling is that 1. is easy but more expensive, 2. is cheaper and 3. not sure about all the costs (elastic ip, instance size?) ad how complex it will get.

Also 1. cost will be based on hours and data transfer in?

I'd like to elaborate more and have some suggestions.",3,aws,2020-10-13
ixabet,"""Malformed Lambda proxy response"" error blablabla...","Hey guys.

I've been struggling all day to understanding why I have this error, and yes I've read [this documentation](https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-output-format).

&amp;#x200B;

Probs someone with better eyesight can see where the issue is :

`api.post('/database-manager/create-request', async (lambdaRequest, lambdaResponse, callback) =&gt; {`  
 `await provideConnectionInfo()`  
 `let connection = mysql.createConnection({`  
 `host: mySQLHost,`  
 `user: mySQLUser,`  
 `password: mySQLPassword,`  
 `database: mySQLDatabase`  
 `}`  
`)`  
 `requestNb = lambdaRequest.headers[""request-id""]`  
 `pipelineId = lambdaRequest.headers[""pipeline-id""]`  
 `console.log(lambdaRequest.headers)`  
 `console.log(""am here"")`  


`connection.connect(function (err) {`  
 `if (err) throw err`  
 `let query = ""INSERT INTO ec2_request(request_id, pipeline_id) VALUES (${mysql.escape(requestNb)},${mysql.escape(pipelineId)})"" //changed the quotes so text formatter can work`  
 `connection.query(query, function (err, result) {`  
 `if (err) {`  
 `connection.end()`  
 `console.log(""insertion did not work : "" + err)`  
`lambdaResponse.json(`  
`{`  
 `isBase64Encoded: false,`  
 `statusCode: 404,`  
 `headers: {},`  
 `body: JSON.stringify(""record not inserted"")`  
`}`  
`)`  


`} else {`  
 `connection.end()`  
 `console.log(""1 record inserted"")`  
`lambdaResponse.json(`  
`{`  
 `statusCode:200,`  
 `headers:{},`  
 `body: JSON.stringify(""tired of this bs"")`  
`}`  
`);`  
 `}`  
`}`  
`)`  
`}`  
`)`  


`})`

Either way, I'm too exhausted to see where the error is. I'll really appreciate any help I get from you geniuses",1,aws,2020-10-13
ixafjf,Can't execute Step Function from within Lambda,"Having a heck of a time getting a Step Function to execute from Lambda which is triggered by DynamoDB.  I'm not getting any errors.  It seems like it's just skipping over the code for step functions as the Lambda is completing successfully otherwise.  What am I missing??

    const AWS = require('aws-sdk');
    const stepfunctions = new AWS.StepFunctions({ region: 'us-east-1' });
    
    exports.handler = async(event, context, callback) =&gt; {
        //console.log('Received event:', JSON.stringify(event, null, 2));
        const stateMachineArn = 'arn...';
        const params = {
            stateMachineArn: stateMachineArn,
            /* required */
            input: '{""value"": ""hello!""}',
            name: 'test'
        };
    
        console.log('Execute step function...');
    
        stepfunctions.startExecution(params, function(err, data) {
            // This code not executing
            if (err) {
                console.log(err);
                const response = {
                    statusCode: 500,
                    body: JSON.stringify({
                        message: 'There was an error'
                    })
                };
                callback(null, response);
            }
            else {
                console.log(data);
                const response = {
                    statusCode: 200,
                    body: JSON.stringify({
                        message: 'Step function worked'
                    }),
                };
                callback(null, response);
            }
        });
    
        // This code executes
        for (const record of event.Records) {
            console.log('Event ID: ' + record.eventID);
            console.log('Event name: ' + record.eventName);
            console.log('DynamoDB Record: %j', record.dynamodb);
        }
        return `Successfully processed ${event.Records.length} records.`;
    };

&amp;#x200B;",5,aws,2020-10-13
ix9ynw,"AWS CodeDeploy, Blue/Green for EC2. Failed deployments don't get removed.","I have CodePipeline and CodeDeploy configured with a blue/green deployment type to manage a launch config/auto scaling/ec2.

I've noticed that when a deployment fails, code deploy appears to just abandon the the process and leaves behind the failed autoscaling group and EC2 instances. I have CodeDeploy configured to auto-rollback upon failure but this doesn't make a difference. The failure happens and I have to manually go find which autoscaling group instances are in load and delete the non-live autoscaling group/instances before I can attempt to re-deploy.

Has anyone else run into this kind of issue? I've gone over the config multiple times but, nothing seems to trigger a full rollback with a deletion of the failed deploy autoscaling group and it's respective instances.",1,aws,2020-10-13
ix9o15,How to stop AWS from charging me without closing the account,"Hello all,

I have a AWS account, I've disabled everything and they still charge me around $3 a month. Can't find what is still running. Is this normal?",6,aws,2020-10-13
ix9u7k,"ASG not behaving as intended, Any suggestions on the policy?","Hello there,

I am trying to create a system where if there are any messages in an SQS queue, an alarm will be triggered using number of visible messages in queue which in turn will trigger an ASG to scale in or out.

To do this I created an ASG which has one step scaling policy as shown below-

    ASG-step-policy
    
    Policy type:
    Step scaling
    Enabled or disabled?
    Enabled
    Execute policy when:
    ASG-step-policy-alarm
    breaches the alarm threshold: ApproximateNumberOfMessagesVisible &gt;= 1 for 2 consecutive periods of 60 seconds for the metric dimensions:
    QueueName = my-sqs-queue
    Take the action:
    Set to 0 capacity units when +infinity &lt;= ApproximateNumberOfMessagesVisible &lt; 1
    Set to 1 capacity units when 1 &lt;= ApproximateNumberOfMessagesVisible &lt; 2
    Set to 2 capacity units when 2 &lt;= ApproximateNumberOfMessagesVisible &lt; 3
    Set to 3 capacity units when 3 &lt;= ApproximateNumberOfMessagesVisible &lt; 4
    Set to 4 capacity units when 4 &lt;= ApproximateNumberOfMessagesVisible &lt; 5
    Set to 5 capacity units when 5 &lt;= ApproximateNumberOfMessagesVisible &lt; +infinity
    Instances need:
    60 seconds to warm up after each step

The problem with this is even if all the messages are processed and deleted from the SQS queue, ASG do not change the desired capacity to 0 immediately. It takes hours to do that &amp; sometimes doesnt do that at all, leaving instances running for no reason.

For example - I sent 4 messages to queue last time and following actions happened:

     changed the desired capacity from 0 to 1 
     changed the desired capacity from 1 to 2 
     changed the desired capacity from 2 to 4 
     changed the desired capacity from 4 to 2 

And since then its stuck at 2 instances even though there are no messages in queue since past 2 hours.

What am I doing wrong here?

Thanks in advance.",1,aws,2020-10-13
ix9j2r,New AWS quick-links menu,2 clicks to get to my favorites instead of just one?    What a leap forward.,5,aws,2020-10-13
ix99od,What is the proper method of syncing the AWS Cognito user pool and RDS database (Postgres)?,"I'm creating a new web app project (Vue / Node.JS / Express.JS backend) and I need to migrate over the existing Postgres database that holds all the user credentials.

The user table holds their email and their encrypted password + salt. 

I initally tried Cognito by creating a user pool and migrating the users emails over via .csv upload. **I'm fine with the users having to reset their passwords if that is the only option**, but I'm having a hard time figuring out if Cognito is the right approach. 

I don't see a lot of examples where Cognito is synced with an RDS database to persist user information. I need to be able to do two basic features:

1. Allow users to log in (email mandatory, FB/Google option), change passwords, reset their passwords
2. Allow users to purchase products via Stripe and persist information (Stripe customer ID, product purchased, etc.)

So as you can see, I need my app to do the following logic:

""When a user signs in, determine if there is already a user account made for them and see if they have purchased a product.""

Cognito seems to do the 1st one, and the database should handle the 2nd.

But from what I'm reading in the docs, the user pool information is separate from the RDS database. What is the proper method of syncing the user pool + database?",11,aws,2020-10-13
ix7wki,How do I launch a Windows Instance with 2 PowerShell Userdata Scripts?,"I'm trying to set up a Windows EC2 using a PowerShell userdata script, but I am unable to run it as one large script because it requires I both install and import a DSC module (if this is done in a single PowerShell script, this causes a compilation error because it can't evaluate a DSC module that does not exist).

I'm trying to solve this problem by providing two PowerShell scripts to the instance's userdata. I thought this would be possible by specifying two sets of `&lt;powershell&gt;&lt;/powershell&gt;` tags, but doing so seems to accomplish nothing; the user data doesn't run and there are no visible errors or output in the logs.

How do I run two PowerShell scripts in the userdata?",1,aws,2020-10-13
ix6mrl,What are the best open source security assessment tools for AWS?,"Hello!

My bachelor thesis topic will be about the evaluation of open source security assessment tools in the AWS Cloud. In my case the security assessment is defined as scan that reveals misconfigured security groups, broad IAM permissions, public resources, etc. Please tell me if you have a tool that should be covered in an evaluation like this.

I already have this tools on my list:

- ScoutSuite - https://github.com/nccgroup/ScoutSuite

- Prowler - https://github.com/toniblyx/prowler

- CloudCustodian - https://cloudcustodian.io/

- CloudMapper - https://github.com/duo-labs/cloudmapper

Thanks for your help!",8,aws,2020-10-13
ix85cy,Publishing to sns topic,"I have an angular ionic app and i am planning to use aws sns for pushing notifications.
My question is how do i publish notifications to sns so that subscribers can get notifications?
I read on their website and it says to use the Amazon management console to creat a topic and send a push from there. How do i automate this process that when there is a notification it creates a notification/ topic in aws console and pushes it to devices.",1,aws,2020-10-13
ix8l52,What is the status on doing blue/green deployments with Cloudfront distributions?,"Hello !

**edit: Apologies for not describing the title correctly, I meant to say Cloudfront origin**

I've been doing some research because I have the need to be able to do blue/green deployments of an Angular site running behind Cloudfront on some EC2s:

User -&gt; Cloudfront -&gt; External ELB -&gt; 3rd Party WAF appliance -&gt; Internal ELB -&gt; EC2 instance with Angular binaries

Alot of the information I'm finding is from before Lambda@Edge existed and I wanted to see if any new options were available? So far I'm seeing a few suggestions for using Lambda to modify requests but I havent really seen any examples where people are dynamically choosing origins

Anybody have any luck with this? Thanks for any feedback!",7,aws,2020-10-13
ix61ho,Questions About AWS CloudFormer,"Hello,
I am going to be attempting to replicate the architecture of an existing AWS Enterprise and AWS GovCloud Account in another Enterprise/GC Account. What are some general problems you have had in the past? The services involved will be things like S3 buckets, ec2 instances, lambda functions, all the iam/security/routing, etc. 

My understanding of the service is that I will be able to create a cloudformation stack based off the current architecture, export the script, import into fresh/unused account, spin up resources based off the script that I migrated over and placed in to cloudformation on the new account. Am I off on this understanding? Thanks for your time!",5,aws,2020-10-13
ix6cky,What do I do with AWS credits?,"If you had $2,000 in AWS credits, what would you use them for?

I recently received some for a class project and didn't end up using most of them, so I am torn on how to use them before they expire in a year. Between creating a personal website and launching a small web application for fun, I wasn't sure how to make use of my credits. I figured why not ask the internet and maybe get some inspiration.",4,aws,2020-10-13
ix5zhc,AWS REGION which one should I choose?,"Hi, I'm currently doing the amazon cloud practioner course on Linux academy, during the lectures I have been asked to make a console, during the lectures the speaker recommends changing my region to North Verginia as it is the oldest and always has the latest updates later on in the lecture he recommends that I should choose the region closest to me, I am currently living in Saudi Arabia temporarily (1 year) then return to Europe my concern is if I choose the middle Eastern servers will that effect my studies during this course or anything else longerm negatively and if so should I choose a European server (London) as I will return in around a year or do I stick with the lecturers first recommendation (North Virginia) for the reasons mentions at the beginning.

Thanks in advance.",1,aws,2020-10-13
ix5zfn,Monitoring webpage for a keyword,"Is it possible to use CloudWatch to monitor if a web app (ec2 http) is broken via keywords? I was thinking the best way would be to check if the login page presents a ""Login"" button and if the web app is broken/misconfigured it will return an error page. I make use of cloudformation.",0,aws,2020-10-13
ix5gcc,Re-platform an EC2 instance from Amazon Linux 2 to Red Hat 8,"I have an Amazon Linux 2 instance, but I want it to be running on Red Hat 8 instead.  I want to use the same IP address it is currently on.  I was thinking I could go about this one of two ways:

1. Create a new RHEL instance, detach the root drive from it, and replace the root drive on the Amazon Linux 2 instance with the RHEL root drive.

2.  Detach all drives from the Amazon Linux 2 instance, terminate it, launch a RHEL instance with the old IP, re-attached data drives.

I tried method 1, but it wasn't able to connect to the RHEL repos.  I'm not 100% sure why, but I'm sure it is due to its instance attributes.
Is anyone familiar with a method of replacing the root drive to run a different OS, or does this just not work?",1,aws,2020-10-13
ix4zs5,Looking for sugguestions on backing up File Server using SMB 2.0 with DataSync,"We have a file server running Windows Server 2008 SP2. It's the version that runs SMB 2.0. SMB 2.1 was introduced in the R2 version of Server 2008. From what I'm seeing and have read, the oldest SMB version that DataSync  can utilize is SMB 2.1. The original intent was to backup this file server to AWS FSx and be able to migrate users over gradually without them losing any of their data.  


What are my options for backing up this server with an AWS service so it can be used as a file share for my users? We have approx 1.75 TB of data on this file share.",1,aws,2020-10-13
ix4rdg,[HELP] Connect to Aurora RDS from AWS Lambda,"Hi everyone.   
So, I have an RDS instance running with `engine=aurora` and `engine_mode=serverless.`

Now, when I try connecting to the database from a lambda function via Aurora Data API. Both the lambda and RDS are in the default VPC, but I am unable to connect to the database, using the following python snippet taken from [here](https://github.com/chanzuckerberg/sqlalchemy-aurora-data-api)

&amp;#x200B;

&gt;from sqlalchemy import create\_engine   
cluster\_arn = ""arn:aws:rds:us-east-1:123456789012:cluster:my-aurora-serverless-cluster"" secret\_arn = ""arn:aws:secretsmanager:us-east-1:123456789012:secret:MY\_DB\_CREDENTIALS"" engine = create\_engine('postgresql+auroradataapi://:@/my\_db\_name',   
echo=True,   
connect\_args=dict(aurora\_cluster\_arn=cluster\_arn, secret\_arn=secret\_arn))    
with engine.connect() as conn:  
for result in conn.execute(""select \* from pg\_catalog.pg\_tables""):  
print(result)

I am assuming the connection issue is caused due to either the VPC, or the security group attached to the lambda function. I have edited the security group to permit all inbound traffic and outbound traffic, but it didn't help.

But, if I create a new lambda function which is not in the VPC, I am again able to connect to the database. Also, I am able to successfully connect to the database from my local computer using the same snippet above.

But, I can't remove my original lambda function from the VPC, because it is accessing an AWS EFS instance, and thus needs to be in a VPC. 

Please suggest me a way to connect to my database. Thanks for your help.",0,aws,2020-10-13
ix2r2s,Favorite scenarios for testing a Disaster Recovery Plan in AWS?,"I'm in charge running the tests for our Disaster Recovery Plan (DRP). As always, I'm looking to do this in a way that does not piss off my dev team. Let's be honest, they would rather be doing almost anything else. My goal is to be well prepared enough for emergencies that we will survive no matter who is on vacation. Most of my experience is testing DRP for physical servers, which is a pretty clear set of scenarios (flooding, power/wifi outages, fire, etc). 

For AWS, we're running scenarios like system outages, successful phishing attacks, but I feel like I'm missing some key things we need to test.  I've been doing research, but there are not a ton of resources about testing cloud-based DRP, especially for smaller teams. Additionally, these resources are not written from the perspective of developers. Have you had any testing exercises that you thought were really effective? Have there been any that you hated or felt were unnecessary? 

Thanks for your perspective!",3,aws,2020-10-13
ix2bsi,Outgoing internet access over NAT Instance problems,"Hi everyone, I am encountering the following problem here is my scenario; I have a VPC with two private subnets and one public subnet. Inside the public subnet I have a NAT instance to give outbound internet access to the private subnets. Inside the private subnets I have lambda functions that send time sensitive emails over SES. The problem is that each time an email is sent I receive it 4 minutes later. But here is the thing, the time on the email is the time that the email was requested. What I mean is that if I requested the email at 10:00 AM I will received it at 10:04 AM but the email says that it was delivered at 10:00 AM.  

Any help will be appreciated, thanks!",1,aws,2020-10-13
ix217g,Is it okay to RESIZE a linux volume while the Snapshot is pending?,"I know its okay to USE a volume during a snapshot, but can you resize it? Per AWS docs its best to snapshot before resizing. But this is 300GB and going to take a while. I can't find any Doc that answers this.

&amp;#x200B;

UPDATE: Well I went ahead and did it anyway, because that's how we crazy AWS guys roll. You CAN make the change while its taking the snapshot. You CAN start the instance with the modified size when the volume is still optimizing. And the OS even recognized the change without me having to expand it manually. So yeah... coffee time. ",1,aws,2020-10-13
ix20ug,Cloud9 with multiple accounts,"I have been using cloud9 for a few weeks now. My only issue is that when I change accounts in the console I get logged out of the tab with my cloud9 console.   


Example: cloud9 is in account X,  I use console to assume role to look at resources in account Y. I get logged out of cloud9 console in Account X after 1-2 mins of being in an account that the cloud9 machine does not reside in.  


Is there any way around this issue?",1,aws,2020-10-13
ix0dmq,Week of Sept 21st - What are you building this week in AWS?,Share what you are working on,7,aws,2020-10-13
iwzrqz,"Dear AWS, Please stop spamming me","Dear AWS, 

Like many, I'm trying to follow 'best practices' - using AWS Control Tower and Multi-Account configuration.

However you're making this rather painful - every account created results in me getting yet another subscription to your marketing emails. 

Today I got a dozen more ""Welcome to your Getting Started series"" emails.   
As if that wasn't painful enough, un-subscribing is made unnecessarily difficult - I need to copy the email address over and tell you, once again, that I really don't want more marketing material. 

Given the amount of tracking in all those links, the least you could do would be to provide a one-click unsubscribe.  

Better still - how about for every account created from AWS Orgs / Control Tower you opt them out of marketing emails. 

Sincerely Frustrated, 

L. Extension.",159,aws,2020-10-13
iwzgj7,How do i validate a Cloudfront signed url in Lambda@Edge?,"Pretty much my scenario is as follows, right now i have a Cloudfront distribution where access is controlled by a Lambda@Edge function. Basically it will validate a token in the Authorization header. This has worked fine. 

Recently i ran into a use case where i need to use cloudfront signed urls for certain distributions protected by the above. 

The problem is the Lambda@Edge function prevents signed urls from working:

* If i remove the lambda@Edge function the signed urls work fine.
* If i add logic to allow any requests with Expires/Key-Pair-Id/Signature params in the Lambda and act as a simple pass through they will return 200. HOWEVER it doesn't seem like Cloudfront is actually validating the signed url. 
* I cannot find anything in the AWS-SDK Cloudfront stuff that indicates how to validate these urls.
* I really don't want to create duplicate distributions for this.

I have spent the last few hours digging for anything and i cannot find anything. Does anyone have an idea?",1,aws,2020-10-13
iwz5g1,Can you receive emails from an external domain on an Amazon SES domain?,"Hi,

Haven't really worked much with SES in the past but I have some integrations I need to implement. I have used SES before to send automated emails but now I'm trying to set it up so it can receive emails.

I am able to configure it so it receives emails from inside the domain, but I can't receive from outside the domain.

Is this due to how SES is setup or is there something I'm missing, can you only receive emails on SES from inside the domain?",2,aws,2020-10-13
iwylqu,Tool for managing parameters in AWS SSM Parameter Store,"[https://github.com/kevinglasson/goss](https://github.com/kevinglasson/goss)

I developed a tool to help managing config for various environments in AWS SSM PS. I though some people here might be interested.

I am also interested to know how everyone is managing their own config? We use SSM PS to manage local dev config as well as prod / stage (under different paths of course)",4,aws,2020-10-13
iwwjwl,HELP: Is uploading to S3 with the Amplify GraphQL client possible?,"I want to include S3Objects in my schemas like this example from the documentation:  [https://docs.amplify.aws/lib/graphqlapi/advanced-workflows/q/platform/js](https://docs.amplify.aws/lib/graphqlapi/advanced-workflows/q/platform/js) 

According to this, the feature isn't supported yet  [https://github.com/aws-amplify/amplify-js/issues/2706](https://github.com/aws-amplify/amplify-js/issues/2706) but there appears to be someone at the bottom who was able to do it. Anyone have any luck with this? Am I missing something?",2,aws,2020-10-13
iwxmor,Route53 Geolocation Routing Policy returning values in random order,"I set up 4 records with geolocation routing policy on them: one record for my instance in London (I set Europe region), one for Ohio (North America region), one for Sydney (Oceania region), and one for default. On the default record, I put the London instance IP address on it. So there would be two records with the London Instance IP address.

When I refresh every 5-6 minutes, I'll get my Ohio instance, and then my Sydney instance, and then London or something. Just like a simple routing policy.

I tried redoing the records just in case and it's the same.

Is this the correct setup?

(My instances are stopped, so the IP's don't work atm)

https://preview.redd.it/z026ugs77ho51.png?width=850&amp;format=png&amp;auto=webp&amp;s=3644a5fda630c599ba94fc5d797c7232c7134f57",2,aws,2020-10-13
iwrnot,MINI Project : Building a serverless application which uses Web Identity Federation,"&amp;#x200B;

[Make this 👆, details below 👇🏽](https://preview.redd.it/b3cswf24ueo51.png?width=2534&amp;format=png&amp;auto=webp&amp;s=8e6b807f2de7f796526db440ca72301c335e8890)

**TL;dr I try and include real-world-like demos within my courses and many of my students have troubles with Web Identity Federation and Cognito so I created this mini project where you build a simple serverless app (Google IDP + Cognito Credentials) to access an S3 bucket using presignedURL. I hope it's useful to everyone !**

**There is a video guided version in my AWS Certified Solutions Architect Professional Course (**[**https://learn.cantrill.io/p/aws-certified-solutions-architect-professional**](https://learn.cantrill.io/p/aws-certified-solutions-architect-professional)**) . And my other courses at** [**https://learn.cantrill.io**](https://learn.cantrill.io/)

**🆓And a freely available version .... GitHub REPO and Details at the bottom 🆓 👇🏼**

# The Background (might be interesting)

Some of you might know that I create AWS training courses - **but that isn’t what this post is about**.

As part of my course development - I’ve been working for a while on what I call \[**Advanced Demo**\] Lessons. These go beyond the usual DEMOS found in most courses, tackling more complex scenarios, they're longer and often involve Hybrid AWS and On-premises tech stacks.

I have a few of these advanced demos now in a [repo I manage](https://github.com/acantril/learn-cantrill-io-labs) (they are all freely released from my courses which have the full video guided versions in)

\- [BGP VPN](https://github.com/acantril/learn-cantrill-io-labs/tree/master/AWS_HYBRID_AdvancedVPN)

\- [Hybrid Directory](https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-hybrid-activedirectory)

\- [Hybrid DNS](https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-hybrid-dns)

\- [Hybrid Systems Manager](https://github.com/acantril/learn-cantrill-io-labs/tree/master/aws-patch-manager)

# The Detail (more interesting)

🟢 1-Click Deployment of a private bucket and application bucket

🟢 Setup a GoogleAPI project

🟢 Setup Cognito ID Pool for credential SWAP

🟢 Configure the S3 hosted App

🟢 Test &amp; See Cat Pictures

# The Bits &amp; Pieces (the good stuff)

(*This is all MIT Licensed - use it as you please, I have no liability - and its Release Candidate level ... so please report any issues - there are videos which make this easier than just text based instructions, but these are on my course only for now*)

All available in my [DEMO/ADVANCED DEMO Github repo](https://github.com/acantril/learn-cantrill-io-labs)

🟢Instructions ... for each step ( [1](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE1%20-%20Provision%20and%20Discuss%20Architecture.md) [2](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE2%20-%20Create%20Google%20APIProject%20and%20Client%20ID.md) [3](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE3%20-%20Create%20Cognito%20Identity%20Pool.md) [4](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application.md) [5](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE5%20-%20Cleanup.md) )

🟢Architecture for each step [here](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/ARCHITECTURE-STAGE1.pdf), [here](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/ARCHITECTURE-STAGE2.pdf), [here](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/ARCHITECTURE-STAGE3.pdf) and [here](https://github.com/acantril/learn-cantrill-io-labs/blob/master/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/ARCHITECTURE-STAGE4.pdf)

🟢AWS Base Infrastructure deployment link [here](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/quickcreate?templateURL=https://learn-cantrill-labs.s3.amazonaws.com/aws-cognito-web-identity-federation/WEBIDF.yaml&amp;stackName=WEBIDF)

I hope this helps people - As above, my aim in this community is to genuinely help people, yes i sell AWS courses at [https://learn.cantrill.io](https://learn.cantrill.io/) but this is mainly to help me give back to the community and help people change their lives ... (emo but true).

/Adrian",2,aws,2020-10-13
iwxs1q,AWS product suggestion,"Hello

I might have a new client who wants to work in cloud instead of onpremise. They have a Windows app which runs on a Windows server (and old accounting app) and everyone is connecting to it via a shortcut in their own computers. Something like shops used years ago. They will be uploading files to this server too. Not like as pure file server but only to specific folders which this app will use. They also want to use their local printers too.

How can I achieve that? App DB size is around 5 GB, and user count is around 80ish",1,aws,2020-10-13
iwsh3s,How to install SSL (Verify by Amazone) for AWS Lightsail not using Load Balancer,"As the title I cannot find the documentation for installing SSL AWS ACM for Lightsail.

I have only found the SSL verify by let's encrypt installation documentation. Anyone who has successfully installed please help me.  
Thank you.",2,aws,2020-10-13
iwwtvn,AWS RDS - Connection problems!,"Hi everyone,

I've been trying to connect to an RDS database using SQLAlchemy but I'm running into some problems.

Essentially, I am using Lambdas to make some writes to the database, and I am explicitly closing the SQLAlchemy sessions at the end of each Lambda. However, even after the Lambdas finish executing, the RDS database still registers the connections as open for quite some time, which causes me to reach the maximum number of allowed connections very quickly.

When this happens, all future Lambdas start crashing (I also assumed they would fail gracefully like waiting for a connection to be available, but unfortunately it just crashes).

Any insights on this, and how to use RDS in production? 

Thanks a lot for reading.",2,aws,2020-10-13
iwwgr9,MediaConvert Best Practices - Small clips vs single large video,"I'm tasked with developing a web app where the user can create short (few seconds to several minutes) video clips from a larger video (1-2GB \~2hrs) I'm looking for looking for some experienced advice on best practices with MediaConvert.

**Option A**

Create video clips using the original massive video file.

&amp;#x200B;

**Option B**

After initial upload, use MediaConvert to split the large video into smaller 5 minute video files.

Afterwards when a user sends a request to create a clip, it would create a clip using one or more of these smaller video files.  For example:

`| Clip 1 0:00 to 4:59 | Clip 2 5:00 to 9:59 | Clip 3 10:00 to 14:59 |`

`..................| Requested Clip 4:39 to 10:18 |`

MediaConvert would take small portions of Clip 1 and Clip 3 and then combine those portions with clip 2.

In my mind this would have lower operational costs and processing would be faster.

&amp;#x200B;

For those of you who have experience with MediaConvert:  Yea or Nea?",1,aws,2020-10-13
iwu57p,Multi AWS Account Manager,"I have just created about 30 AWS Organizations for a project I am working on.   
is there a application I can use to manage all of the 30 accounts without logging into each one?

Basically I want to view how many EC2 instances are running at any given time and turn them on or off on demand without having to manually switch to the organization account. 

Any recommendations welcome.",4,aws,2020-10-13
iwtjxm,Getting into the field,I have obtained my AWS CCP and SAA certs but I don’t have any professional experience with AWS. I just got interested in it about 9 months ago and learned a lot and want to transition to that field but don’t know how to go about it. Anybody have any suggestions?,3,aws,2020-10-13
iwst9o,How valuable is the AWS Security cert?,How valuable is the AWS Security cert? Could this help me land a position in cloud security? Does it hold a lot of weight?,1,aws,2020-10-13
iwsg84,Non-async nodejs sdk requests? Poor ux in cdk,"I'm writing my stacks using CDK which is done in typescript. In my class that extends a deployment stack I need to make a couple of aws sdk s3 requests but this is not possible as the request must be done in the constructor and the constructor cannot have async/await. 

How do I go about making these sdk s3 requests synchronously as you would using boto3 in python?",2,aws,2020-10-13
iwru2w,Deeplens stuck in endless setup update loop,"When I try to connect to my deeplens during setup, it tells me to update the software. However, after I update the software and connect to [deeplens.amazon.net](https://deeplens.amazon.net) like the setup guide says, it tells me to complete another update, and the loop goes on and on. Can anybody help?",2,aws,2020-10-13
iwrgmu,How to point a domain *subdirectory* (not subdomain) to a Lightsail instance?,"I see plenty of tutorials on how to point lightsail instance to a subdomain. i.e. [blog.example.com](https://blog.example.com)

But I don't see any on how to point the lightsail instance to a subdirectory i.e. [example.com/blog](https://example.com/blog)

Can someone please tell me how this can be accomplished?",0,aws,2020-10-13
iwrcbj,How are you versioning &amp; aliasing Lambdas that back different API Gateway stages?,"Just inherited some CloudFormation templates that create some Lambdas, Versions, and Aliases, along with multiple API Gateway Stages. The API Gateway Stages each have a Stage Variable corresponding to different Aliases, so a /ver1 stage points to ver1 alias, /ver2 stage points to ver2 alias, etc. There are many references to `AWS::Lambda::Version` resources, which is difficult to manage and I'm having trouble visualizing how to setup a less-intrusive CICD Pipeline.

I was looking at SAM for this -- it seems that it can auto publish versions &amp; aliases, but how would I integrate this with API Gateway? What would be the CICD Pipeline process?

What are you folks doing?",1,aws,2020-10-13
iwqhpi,How does AWS SQS manage offsets of different consumers?,"Hello,

I came from experience in Kafka so tried to understand SQS in the way similar to Kafka. From my understanding, standard queue should support multiple consumers, right? How did AWS SQS manages which consumer has consumed to which message? Is it similar way like offset in the Kafka?

Also, is there a place I can view the offset of each consumer in AWS console, and potentially modify the offset? (in case of a bad message stuck the queue, etc).

Thank you so much!

&amp;#x200B;

\-- some updates to avoid confusion:

I meant ""multiple consumers"" like different services who are listening to this topic and each of them needs to do some updates (based on the message) in their own context.",1,aws,2020-10-13
iwot8b,AWS Quicksight connection to RDS,"I have an RDS instance using MySQL trying to use it to connect to Quicksight to provide a new data source. Both are in the same VPC.

The security group attached to my RDS instance allows for traffic to my personal machine (I use workbench to connect and it works) and the CIDR block to the region where Quicksight is being hosted. 

The security group attached to Quicksight I allowed for all traffic inbound and outbound. 

When I try to create a new datasource from Quicksight and connect to RDS, I get a timeout.

What could be causing this?",1,aws,2020-10-13
iwnk2z,industry standard ways for hosting a scalable django app on AWS?,what are some common ways or industry standards for hosting scalable backends on AWS? im comfortable and have experience with EC2 but im learning a bit about lightsail and load balancers that seem to be better suited for having scalability out of the box. im really comfortable hosting frontends with S3 + cloudfront but trying to understand what are some of the best ways to go about hosting the backend. thanks!,3,aws,2020-10-13
iwmji8,One cloudfront for both site + data VS 2 separate cloudfront distributions?,"So I have a static website in an S3 bucket that needs to access files in another S3 bucket. Which option would be better (performance, cost, etc):

1. sharing 1 cloudfront with both s3 origins
2. using 2 separate cloudfront distibutions connected to their respective s3 (one for serving site to users, one for serving data to the site)

&amp;#x200B;

Thank you!",1,aws,2020-10-13
iwgs3o,Beginner Q: what's the difference between Red Hat and AWS?,"I think I mostly understand AWS (I've used Lambda and Elastic Beanstalk before), but I always see a lot of talk about Red Hat and I have no idea where that fits into things. 

At the previous startups I interned at, AWS is just the platform where you deploy your code -- press ""upload"" and it spins up all the servers you need. Is Red Hat just the flavor of Linux that the EC2 server is running? Why would you need Redhat instead of the default Linux flavor?",0,aws,2020-10-13
iwgap1,Trying to create S3 bucket through node error,"Returning the following error. I am passing my credentials (ID and secret) properly through the aws-sdk. Do I need to pass through a CA too?

    Error: unable to get local issuer certificate
    at TLSSocket.onConnectSecure (_tls_wrap.js:1492:34)
    at TLSSocket.emit (events.js:315:20)
    at TLSSocket.EventEmitter.emit (domain.js:485:12)
    at TLSSocket._finishInit (_tls_wrap.js:935:8)
    at TLSWrap.ssl.onhandshakedone (_tls_wrap.js:693:12)",1,aws,2020-10-13
iwfi95,"I passed the AWS solutions architect exam about an year ago, wanted to share my journey and my notes. It has already helped a lot of people, hope it also helps some of you.",,277,aws,2020-10-13
iwc30u,Does VPC can handle VLAN tagged traffic?,"Hi All,  


We are in the stage of developing a SDN component within an EC2 instance(preferably metal instances ) and have some concern regarding the traffic from this SDN part to the VPC.

1- If we send VLAN tagged packets from an EC2 instance through ENI, is there any possible way VPC can handle this traffic and route accordingly?

2- Is it possible to configure the ENI from VPC more similar to a trunk port from switch?

3- While setting subnets in VPC is it possible , each of these subnet bounding to specific vlan? 

4- Does the VPC subnets can be configured with complete isolation(no inter-subnet routing) 

Please shed some light to VPC backbone.  


Thanks and Regards,",2,aws,2020-10-13
iw6mql,Can ACM be used for client certificates on physical devices?,"Is ACM only for devices hosted in AWS?

I’d like to see if ACM could be used a complete substitute for an on premises Microsoft PKI or is it only useable for AWS-related services. 

For instance is there any way to effectively distribute (such as push certificates to devices with an MDM like Intune, SCCM,or ADCS auto-enrolment SCEP etc.) client authentication certificates to AD users and devices for uses such as EAP-TLS authentication for WPA2 Enterprise 802.11x WiFi, email digital signatures, certificates for identifying a device as a trusted device to access an Amazon Workspace etc.?

What about SSL web server certificates for internal servers and intranet sites not hosted in AWS?",0,aws,2020-10-13
iw69k6,Having trouble debugging an issue with table_import_from_s3,"I have an RDS instance with an attached IAM role which allows it to sts and talk to the ""athena-results"" bucket listed below. It's in a public subnet with a security group that allows outbound 443 and 80. I'm running the following query:

    select aws_s3.table_import_from_s3(
                   'etl.stating_table',
                   '',
                   '(format csv)',
                           'athena-results',
                           '0cae18bb-6fb5-4b23-a735-aede565bc345.csv',
                           'us-east-2'
               );

And it's returning this

    [XX000] CURL error code: 28 when attempting to validate pre-signed URL, 1 attempt(s) remaining
    [XX000] HINT: make sure your instance is able to connect with S3.
    [XX000] CURL error code: 28 when attempting to validate pre-signed URL, 0 attempt(s) remaining
    [XX000] HINT: make sure your instance is able to connect with S3.

It's pretty clear this is some sort of network routing issue and not a credentials issue, as when you make a bad request to the API it tends to give you a 401 immediately, not time out. However, im just not sure how to continue debugging because i've checked and rechecked the RDS instance and its subnet group is three \*public\* subnets with an internet gateway attached and a security group with sufficient outbound TCP / UDP rules such that it shouldnt interfere with cURL.

Can anyone help? I feel really up the creek without a paddle here.

&amp;#x200B;

( I did [post on the forums](https://forums.aws.amazon.com/thread.jspa?messageID=957175#957175) about this but i see so many unanswered questions on there I'm not gonna take my chances.)

&amp;#x200B;

***\*EDIT\**** I figure it out but im not happy and i'd really like some feedback if anyone feels like it.

When i change the RDS instance ""Public accessibility"" option to ""yes"" I get a different error immediately:

    [XX000] CURL error code: 51 when attempting to validate pre-signed URL, 1 attempt(s) remaining
    [XX000] CURL error code: 51 when attempting to validate pre-signed URL, 0 attempt(s) remaining

From there I took a wild guess, after working with AWS for 8 years, i removed the ""."" characters from my bucket name and it worked. Sometimes I really wish i could write a stern letter to some of the people who maintain this crap.",2,aws,2020-10-13
iw67wi,"Receiving GET 400 when trying to implement socket.io communication. Node.js, ELB, Route 53, CloudFront, and s3","Wasting all my days away :-( Please send help. 

https://stackoverflow.com/questions/63964233/receiving-get-400-when-trying-to-implement-socket-io-communication-node-js-elb",1,aws,2020-10-13
iw4x20,How to host a Node js program on AWS,"I am new to AWS as of today so please bare with me.

I successfully set up a mssql RDS database today. Then I created a node js program that creates a local host to host an API for the database. When hosted locally, I was able to connect to my AWS RDS database and query it.  What is the best option in the free tier to now host this node js program so that I can access it from any device? I tried using elastic beanstalk, but have not had any success with that yet.",1,aws,2020-10-13
iw3z7h,What are the Biggest Issues You've Had With AWS Resource Management?,"I've been working with AWS for about a year now in some environments that had less than ideal controls in place, and I'm curious what other people have ran into when it comes to managing things in AWS on a metadata perspective.

Biggest thing for me has probably been tag enforcement, especially with app teams setting up tags that had typos or capitalization issues. 

Also a lot of garbage snapshots/AMIs left over, people making changes to resources that were deployed by CFTs instead of updating the CFT, etc.

Any other big problems that people have run into that I should be on the lookout for so I can setup controls to prevent/remediate?",3,aws,2020-10-13
iw3i24,Aws internet,"Hello aws members, 

Is there a aws service where I can get internet service I heard of some rumors in the past about aws satellite or something like that does anyone know?",0,aws,2020-10-13
iw1wgx,T3 EC2 + T2 RDS?,"Hi all, I've got a couple quick questions:

I'm running a nightly update that is hogging all the CPU on my t2.micro EC2 instances so I wanted to enable unlimited mode to see if that fixes it, but since I'm running in elastic beanstalk I think I can only do it permanently by changing my instance types to t3.micro (where unlimited mode is enabled by default). 

My questions are:

1. Is it ok to connect a t3.micro EC2 instance to a t2.micro/medium RDS instance?

2. If I need to change my RDS instances to be t3 as well, is there anything special I need to do or can I just switch them over? I seem to remember there was nothing important when I upgraded from a t2.micro to a t2.medium a while ago, but I'm always a little paranoid about accidentally obliterating my database :-D

EDIT: one more question: if it turns out this isn't a CPU utilization error, is there an equivalent solution for ""burstable"" memory? The micro instances only have 1GB RAM which I'm assuming is definitely not enough considering I'm creating a big giant pandas dataframe once a night, but I don't need more than a gig or 2 at most for normal utilization of the app. Is there some sort of option to upgrade to a medium or large instance once a night for a few minutes or something?

Edit2: ok I've answered my first question cause it worked fine with that combination during my testing just now. My second question I'm pretty confident I know the answer already but if anyone has any insight on that that'd be good.

Finally I realized that this is actually a memory issue. I spun up progressively larger instances til it finally worked with a t3.large, but that's 8x my current cost if I have to run them all the time. I only need the extra capacity for about 10 minutes each night. Would the best option be to set up a scaling policy to scale the instances up to large each night on a schedule? It seems like I might be able to do that using launch templates instead of launch configurations? Does that sound right? Otherwise, what other options are there for something like this? 

Thanks!!",0,aws,2020-10-13
iw0fz2,Confused by the way IAM roles work,"I feel like AWS keep changing the way IAM Roles work, so how do they work...?

Scenario 1) If I'm in Account 1 and I want to read from an S3 bucket called ""Hulk"" in Account 2, I would create read permissions resource police on the bucket ""Hulk"" or create an IAM role in account 2 which I can **assume** from a user in account 1.

&amp;#x200B;

Scenario 2) But if I want to write to CloudWatch Logs from an EC2 instance I have to create Instance Role (which holds the Temp Security Credentials created from the STS) for a CloudWatch Agent to write into CloudWatch Logs.

&amp;#x200B;

Scenario 2 looks like inconsistent design on behalf of lazy AWS engineers... Or I'm just out of mind (which I think I am)

Anyways...

(Scenario 3): I would design things to make an IAM Role ""Batman"" on CloudWatch Logs (Not the EC2 instance), which I can **assume** from he CloudWatch Agent to write to the logs. The action is happening on CloudWatch Logs so the IAM Role should be on CloudWatch logs. Can someone explain why AWS does not implement it in this way Given Scenario 1 is correct in AWS?

&amp;#x200B;

Thanks",0,aws,2020-10-13
ivzrmd,Question for integrating Node app with ML,"Hello,

Just a general question because my startup is new to using AWS. Our backend is written in Node and the frontend is in Vue. Currently, we have a Python file for our ML. The plan was to run that on its own server and have the Node backend send data to it so that it can give an output to display in the frontend. Is this possible with AWS? Also, would it be better to use Sagemaker for this task? The algorithm is a basic probability calculator based on a dataset input.

Thanks",2,aws,2020-10-13
ivzqui,AWS Student - Why did I get less credits?,"I was recommended this by my friend and he has 100 credits but I just got my account approved and I have 30 credits ;( We go the same university, basically everything is the same? I think...",0,aws,2020-10-13
ivyg4f,A Cloud Guru slowly removing all Linux Academy courses!,,17,aws,2020-10-13
ivz7mn,Newbie question for EC2,"I need an EC2 instance with 12 cores and 16gb ram that’s going to host an api and a computation engine. I need the instance to be up 24/7 so when my app sends a compute request, it’s picked up right away and I can’t afford time lost in spinning the server up on demand.

Does the EC2 Savings Instance plan support this? Or do I need a dedicated host/instance in this case? There’s a massive price differential between the 2 so I’m a bit confused. Is the savings instance not up 24/7?",0,aws,2020-10-13
ivxoon,"Is it possible to ""share"" a lambda function between accounts?","As the title states, I'd like to give development access to a third party for my lambda function. Is it possible to utilize AWS STS for the third party to assume a role and upload their code to my lambda function? I wasn't able to find any info on this on Google... Thanks!",2,aws,2020-10-13
ivwj2w,Acloudguru is scamming people. Secretly removed Linuxacademy courses and replaced it with their inferior content,"**Acloudguru is scamming people and going back on their promise.**

When Acloudguru took over LinuxAcademy they assured us that we will have access to both catalog of courses. This was a lie.

I paid for Linuxacademy yearly subscription to access their AWS Architect Pro and Devops Pro courses. 

**When I logged in a few days ago I found out that ACG removed 50 hour Aws Architect Pro Linuxacademy course by Adrian Cantrill and replaced it with their ACG inferior 14 hour course by Scott Pelter**

**ACG removed 32 hour Devops Pro course and replaced it with their garbage 6 hour course. In actuality it’s only 4 hours!! Because they sneakily marked each section quiz as 4 hours long and added it to course total.**

This is clearly not what I and other Linuxacademy members paid for. We would like the content that we paid for. Ryan Kroonenburg should be ashamed of himself for scamming people.

I opened a ticket and was told by ACG rep that if I didn’t watch any video from Linuxacademy AWS Pro courses before then I won’t have access to them. Which is completely the opposite of what we were told when ACG took over.

They are slowly replacing all LinuxAcademy courses with shorter, vomit inducing ACG products. 

**Also they sneakily inflate course length by making their quizzes as 4 hour long each. For example there are 6 quiz for AWS Devops Pro exam. So 6 x 4 is 24 hours. The total length of AWS Devops pro course advertised by ACG is 27 hours. So there is only 3 hours of content. No really, go check!**

Linux academy had such great courses and content. Acloudguru is completely destroying all of its credibility and scamming people on top of it. I advise not to get any subscription with them. 

Rather support people like Stephen Maarek, Adrian Cantrill, Eissa Sharif, Neal Davis etc.",542,aws,2020-10-13
ivwqa5,"Blocking ""outside"" traffic on a CloudFront Distribution?","I help run a Roku channel that streams movies 24/7. We use S3 to store the video files, serving a CloudFront distribution, which then sends it to our CDN for distribution to our viewers. Roughly 30,000 actual viewers per week.

BUT, over the last couple of weeks we have been getting a VERY LARGE amount of traffic on the CloudFront distributions from other sources (mostly web browsers according to our logs), bypassing our CDN completely, on the order of 140,000+ viewers per week.

How can we stop this ""outside"" traffic while allowing our CDN traffic to continue? I tried setting up a Web ACL, but since I am a complete novice at it, I can't seem to get it working right.

Please, can someone help me with this? It's costing us $400+ this month alone, up from under $100/month.

I can post logs if needed, and more details if you tell me what you need.",1,aws,2020-10-13
ivwoyy,Serverless WebSocket client on AWS,"I have a new weather station for my backyard whose data is accessible via a WebSocket API. I would like to connect to this API and then add new records it produces into DynamoDB in real-time. I would like this system to be completely serverless.

I am new to the WebSocket protocol and exploring the options in AWS:

* [AWS IoT](https://aws.amazon.com/iot/) seemed like an obvious choice, but looks like it involves installing software on the IoT device itself. This weather station is a black box (the WebSocket API is exposed by the company's website, not the device itself), so this seems like a non-starter unless I use a Raspberry Pi as a proxy.
* [Amazon API Gateway supports WebSocket](https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api.html), but only as a WebSocket server and I would be a client in this case. I think there is a clear distinction between client + server in the WebSocket protocol even though it is bidirectional communication, but not totally sure on this point yet.
* Running a WebSocket client on EC2 seems viable, but does not meet the criteria of being serverless. I'm guessing I could do it with Lambda, but that would be hokey since Lambda is intended to be triggered by an event and not running 24 / 7 as a client.

I would be interested to hear any other ideas on how I might accomplish this.",2,aws,2020-10-13
ivv21y,S3 keeps zipping files?,"I push a few logs from a remote host to S3. Rather than it being stored in its JSON formate, it gets compressed. It does push several files after each job, but each file is a few MB. I tried searching online, but couldn't find other posts with my scenario. Have any of you experienced this? What workaround did you use?",0,aws,2020-10-13
ivvm71,AWS API Gateway Security Question,"We have a security requirement to only allow a website to load if the user clicking the link is coming from a specific website.

&amp;#x200B;

Is that possible to guard against within the system? 

&amp;#x200B;

I am using API Gateway + Lambda",5,aws,2020-10-13
ivut7f,Sending mail from an EC2 instance end-to-end need help,"I'll start with my requirement. I have a web site hosting asynchronous game (eurogames) but if your not familiar just assume its chess. A job runs periodically on the node server to see if its your turn and will send a mail to let you know. So not sure the best way do this but What i am trying to do is this.

I really only have a single EC2 instance, I'm running the mail on that instance. I will have the node service invoke an os call or aws api to send a mail. I'll use SES as the gateway.

What I have so far

- postfix is running and seems configured correctly on ec2. with a couple users defined

- ec2 instance is allowing incoming traffic on port 25 for SMTP.

- DNS has an MX record for mail pointing to ec2 dns name

- SES has domain registered and verified by use a txt DNS record.


What's working and not

- I can send mail from one user to another user on the ec2 instance.

- Trying to send mail from ec2 to an outside domain fails with unable to connect (as expected amazon blocks this)

- ISSUE: I have tried to send mail from a gmail account to the myaccount@mydomain. and nothing. the mail doesn't bounce but it doesn't show up on the ec2. I think I need this to register the user@domain in ses.

what am I missing, how do I debug this?",1,aws,2020-10-13
ivud9k,Amazon Aurora MySQL 5.7 vs RDS MySQL 8. What would you choose and why?,"So, we are currently on MySQL 5.7.x through a third party service (which is a wrapper over aws),   and we are planning to move to AWS soon.
We have 2 options, RDS and upgrade to MySQL 8 or use Aurora with an older release of MySQL 5.7

MySQL 8 benefits that we can definitely use :
1. Atomic DDL migrations
2. The new INSTANT algorithm
3. Functional indexes
Eventually we can use the new JSON features, window functions, CTE and others. 

We have plenty write operations happening on tables with secondary indexes, but we have much more read operations as well with 4-5 joins on avg.
I recently found out that RDS also supports auto storage scaling, which can be pretty useful.

MySQL 8 has these new cost query models and various other optimization as well. 

I believe it should be possible to give additional memory and CPU to RDS MySQL 8,and make it perform at par or somewhere near to Aurora's better read performance.

What are your thoughts?",4,aws,2020-10-13
ivths0,Need help: outpost vs wavelength,"I am going through the definitions at AWS and Google but it's a bit confusing to me on the differences. From AWS,

Outpost: AWS Outposts is a fully managed service that extends *AWS* infrastructure, *AWS* services, APIs, and tools to virtually any datacenter, co-location space, or on-premises facility for a truly consistent hybrid experience

Wavelength: Wavelength Zones are AWS infrastructure deployments that embed AWS compute and storage services within communications service providers’ (CSP) datacenters at the edge of the 5G network, so application traffic from 5G devices can reach application servers running in Wavelength Zones without leaving the telecommunications network.

From the description it looks like almost the same except Wavelength is targeted at 5G CSP and Outpost can be any Customer. 

Am I right in my understanding that

1. Underlying hardware is the same for both?
2. Capacity / Physical dimensions are the same or any differences exist?
3. Services / Capabilities ?
4. Fault Mgmt (Outpost faulty, AWS replaces it) differences?

Appreciate any insights.",2,aws,2020-10-13
ivt9p6,No way to restore a backup that was exported from RDS to S3?,"Just got informed by AWS support that there is no way to restore a backup that you have taken from RDS (Aurora MySQL) to S3. It's a Parquet backup, which they say they can't restore. If that's the case, what is the point of backing up your RDS DB to S3? Also what can you do with a Parquet backup? And how do you backup your DB outside of RDS to be restored in the future? Help me guys, my world has been shaken.",12,aws,2020-10-13
ivru4i,Microsoft Teams optimizations on Amazon Workspaces," Hi, does anyone have Teams optimizations for Amazon Workspaces? I haven't seen much out there so i figured i'd ask. Workspaces is not even listed as being supported on Microsoft teams website....",3,aws,2020-10-13
ivruji,A question on data security in transit between instances,On the docs it says AWS provides secure and private connectivity between EC2 instnsces of al types. What exactly do they mean here though? Got a client with a requirement that all traffic internally and externally is TLS encrypted. So my question ishow do instances within a VPC communicate. do we need to set up TLS ourselves or is aws handling this already?,3,aws,2020-10-13
ivqpg9,Why some regions are disabled by default?,"The following 4 regions are disabled by default:

* Africa (Cape Town)
* Asia Pacific (Hong Kong)
* Europe (Milan)
* Middle East (Bahrain)

What is the reason for not having enabled by default why the rest of the zones? Is it due to higher costs or different legislation in these countries?",2,aws,2020-10-13
ivpay2,AWS DFx and on-prem. FS,"Hi all,
I'm evaluating if AWS FSx is an option for my company, but still would like to offer the full power of the LAN connection with an on-prem. fileserver for the office users.

So I was thinking DFS with on target on-prem. and one in AWS. But I don't find anyone on the internet who has done this setup. Probably because DFS-r is not supported.

My questions: 
- Can I replace DFS-r with AWS Data sync?
  - two sync jobs(back and forth)?
- Did anyone try this out? Does it work as expected (fast everywhere and admins happy)?",2,aws,2020-10-13
ivo92v,How to get the most out of parallel processing in aws?,"I have a c++ program that i'd like to run across many different workers / threads. None of the workers need to interact with eachother, but they should all be able to signal success / keep their own logs of their best results. This is a one off simulation i'd like to run, and there seem to be quite a few options for how to achieve this in aws.

One option is to spin up an EC2, but this seems wasteful and would have many extra steps. https://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html seems closer to what i'm looking for, but then there's also https://aws.amazon.com/hpc/.",2,aws,2020-10-13
ivny3v,Help! ftp upload files to wp -&gt; cdn -&gt; s3,"Hi..  I'm still new to wordpress and aws so forgive me if the question is dumb. I successfully configured wp-&gt;cloudfront-&gt;s3 via offload media. I upload the files ftp to wp but that's not actually upload to cdn. How do i upload the files via ftp like the usual wp UI upload which are directly available on media library and cdn links attached on it? I tried manual upload to S3 but then i'm not sure how to embed the videos. 

Really hit a major stone here guys.. please help, thanks!",0,aws,2020-10-13
ivk15p,Why is ECS capacity provider not available when Blue Green deployment is in use?,"https://github.com/aws/containers-roadmap/issues/713

Perhaps because of complexity around removing older deployment?
ASG instance count should match with one of blue/green service, but not the other one.",2,aws,2020-10-13
ivjhde,AWS Pricing - Reserved Instances - Standard vs Convertible,"Hi,

We've been using AWS since 2012 and we've never actually switched to a reserved instance because we don't really understand how it all works...

We have:

- 1x t2.micro
- 1x t2.small
- 1x t3.large

Our t2.micro instance changes now and again to t2.small and occasionally a t2.large, we probably make a change every 2~ months

Our t2.small rarely changes but every 6~ months or so it may get an increase or a decrease.

Our t3.large sometimes goes up to an t3.xlarge or even sometimes a c2.xlarge

We don't plan on leaving AWS anytime soon, we'll likely stay with aws for at least the next 3-5 years.

If we look at the t2.small:

- A `Standard 1 Year Term, no Upfront` means I am contracted to pay a minimum of $12.92 per month over 12 months which means I can only use that $12.92 a month on a t2.small but cannot upgrade or downgrade the instance?

- A `Standard 1 Year Term, All Upfront` means I am contracted to pay $145 up front, which means I can then use a t2.small for 12 months but cannot upgrade or downgrade the instance?

- A `Convertible 1 Year Term, no Upfront` means I am contracted to pay a minimum of $14.89 per month over 12 months but if I want to upgrade or downgrade the instance I can? ***What happens if I upgrade? What happens if I downgrade?***

- A `Convertible 1 Year Term, All Upfront` means I am contracted to pay $167 up front... the same above questions I have for this one.

Anyone able to explain it better? Thanks",3,aws,2020-10-13
ivj53c,Considering AWS - How will AWS help us transition / train staff,"We have a company of 250 people, and we are growing quite quickly. We are considering AWS because we worry about security and scaling issues. My partner is worried about the transition as our engineers don't have much / any experience with AWS and it is important that we don't have down time due to our lack of knowledge. I looked at their site, but I'm not sure how they will help us train staff / assist us in migrating, and what that will cost us. I have seen solutions architects mentioned before, but do they charge for access to them, or is that part of the support plan?",3,aws,2020-10-13
ivhs91,"2 services, 1 instance?","Say I want to have a chat service and a post service, like twitter or IG. The chat with 1 &gt; 1 &amp; 1 &gt; \* capabilities if that matters. For the post service, I am using rest API with django, and was planning on building out the chat with either node js using socket IO or using flask socket io to keep my server side all in python. My question is, would I have to start up a separate instance to host the chat service? Could I host them all on the same instance? Is this where I would begin to learn docker containers? lol please help, not experienced.",0,aws,2020-10-13
ivg5ff,question about AWS Workspace directory,"I created a directory for Amazon Workspaces and it only added the \_controller security group but not the \_workspaceMembers security group.  so when i go to add the \_workspace to the directory i only get an option to add default or create new security group.   what do i do on this screen?  add new security group or select default.  I don't have right to try it myself and have to tell someone else to do it so thats why i am asking you guys.   the only thing i really found on amazon documentation was the below.  i didn't find anything on youtube about it either. 

 [https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-security-groups.html#security\_group\_existing\_workspace](https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-security-groups.html#security_group_existing_workspace)",0,aws,2020-10-13
ivhk63,MP3 Database,What the best AWS database for MP3 files?,0,aws,2020-10-13
ivf22d,HTTP Api Gateway issues in eu-west-2 (503 Service Unavailable) Why?,"Hi, 

We run an API Gateway that calls Lambda. 

Today, for \~1h30m it was returning us 503 Service Unavailable for most (99%) of the requests. 

I don't see invocations in lambda - neither in logs nor in metrics. I don't see any AWS notifications about API Gateway issues in health status dashboard. 

Do you have any idea why this can happen?",1,aws,2020-10-13
ivf14s,AWS Intellectual property protection,"Hello,

If I put my code on EC2 or similar code deployment method on AWS.

Is my code protected ? can AWS take it and use it ? do they have any right to view and copy data on my servers like code, dataset ..ets ?


Thank you !",0,aws,2020-10-13
ivclhz,Resources for hands-on training on AWS Serverless development,"Most of the available courses fill-in forms on AWS Console which I am pretty sure is not how people develop serverless applications.

Can I get pointers to some resources that are hands-on for real and fast paced, specifically for AWS Serverless development? That involves settings things up on your system in IDE and deploying it from there.",1,aws,2020-10-13
iveja2,AVC Website - all the official Amazon videos related to each individual AWS Service,[AWS Video Catalog](https://awsvideocatalog.com/) is a website that collects all the official Amazon videos related to each individual AWS Service and categorizes them in a way that makes it easy to find what you are looking for. No more fighting YouTube search or relying on an algorithm to find what you are looking for. Everything is in one place. – This means that AWS Video Catalog will allow you to discover older videos that are still invaluable.,162,aws,2020-10-13
ivdy39,System manager windows server upgrade failing,"So I'm trying to upgrade a EC2 instance running server 2012 r2 to 2016 using AWS System Manager.

I read up on the documentation for both System Manager and the Execution document I'm trying to use ( [awsec2-CloneInstanceAndUpgradeWindows](https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-awsec2-CloneInstanceAndUpgradeWindows.html) ) and meet all the pre requisites, yet the execution is getting stuck after completing step 9. 

Has anyone else ever had an issue getting this execution to finish? I tried multiple times (completely fresh 2012 server, updated ssm agent etc) and always get stuck right before step #10 starts. 

My input parameters are quite basic. I leave everything default and only change what is required (instance ID, subnet, profile, etc). 

Is there a logging system where I can check why the status failed? I only get success status messages and pending.. none of them say failed..",2,aws,2020-10-13
ivbyxf,How to give access to AWS resources without creating 100s of IAM Users?,,0,aws,2020-10-13
iv8q0d,Automatically creating AWS GovCloud Accounts and Linking them,,1,aws,2020-10-13
ivaydr,"S3 hosted static website with CloudFront and ""*."" certificate issue","Hello All,

below my configuration:

* requested and registered in route 53 wildcard certificate **\*.mydomain.com**
* **S3 static website** configured
* **CloudFront** distribution **configured** with wildcard certificate
* Created DNS record (**A record**) to point the CloudFront in **Route 53**

When I open the website, I get a certificate error message ""your connection is not private""... it is like the cloudfront distribution does not like the wildcard certificate..... (before I have the specific [mydomain.com](https://mydomain.com) certificate and there was not issue there....)

Anybody can suggest how to fix my problem, please...

Thank you",1,aws,2020-10-13
iv9q9i,AWS AppSync (Amplify) — Things I Wish I Knew Before Starting,,10,aws,2020-10-13
iv8q41,Create a ECS VPN task to connect to RDS?,"I'm trying to see if I can avoid creating a bastion EC2 instance to connect to my private RDS instance. Is there a way to connect to RDS instance through an ECS VPN container? This way the VPN container acts as the tunnel to RDS?

```
PC -&gt; VPN ECS Task -&gt; RDS
```",1,aws,2020-10-13
iv6d4n,Can a service account/role be used to give an EC2 a role?,I have Linux server with the CLI installed. It's used to launch EC2 instances. The EC2 instances need access to an S3 bucket. Can the service account on the Linux CLI server provide the role to the EC2 which it needs to communicate with S3?,1,aws,2020-10-13
iv15s0,"LAMBDA - A Serverless Musical (Hamilton ""My Shot"" Parody)",,20,aws,2020-10-13
iv5k1q,Features of a proper pipeline service - my improvement roadmap for AWS CodePipeline,,4,aws,2020-10-13
iv5h7p,Curated AWS SSM Scripts,,3,aws,2020-10-13
iv4gen,Congrats to r/aws on 140k subscribers!,Great to see a growing and vibrant community,113,aws,2020-10-13
iv0yyr,Need Help on AWS RDS(MS SQL - DB Schemas)," Hi AWS Experts,

First, I want to thank you all in advance for a very helpful forum. Sorry a newbie here, I needed some help regarding RDS DB Schemas in MS SQL. These past days, ***my DB Schemas on my RDS Instance just randomly getting offline for an unknown reason.*** Any idea why is that happening?

Thanks again.",1,aws,2020-10-13
iv4ex9,Api Gateway now supports mTLS,,31,aws,2020-10-13
iv0td2,"Build a code-based data pipeline on AWS ECS, with monitoring and alerting built-in","Hi r/aws!

I just wrote a guide on how to deploy a code-based data pipeline to AWS ECS that extracts data from a PostgreSQL source, and loads into Snowflake. [Read it here](https://dev.to/iamsimonyu/building-a-code-based-pipeline-to-move-data-from-postgres-to-snowflake-1kf1).

Once deployed, it's easy to monitor and manage the task thanks to CloudReactor, a product I've been building. Via CloudReactor, you can stop / start and schedule tasks; see the current status of all tasks (running, failed, succeeded etc.); set alerts; and link tasks together into workflows via a drag &amp; drop editor.

&amp;#x200B;

[CloudReactor dashboard showing current status of all tasks](https://preview.redd.it/19hl69dumun51.png?width=930&amp;format=png&amp;auto=webp&amp;s=221a3e34fe63ec20161897f857dde0207045a5d2)

**Who should read this guide?**

* You want to move data from PostgreSQL to Snowflake (actually you can adapt the script to any source / destination... guess this isn't a hard requrement!)
* You want to do it using code -- as opposed to replicating data via e.g. Stitch / FiveTran etc.
* You don't want to have to monitor the underlying compute hardware (the task runs via AWS ECS Fargate)
* You want to be able to easily monitor and manage the pipeline after deployment

&amp;#x200B;

Thanks for reading and would love to hear your feedback!

Simon",3,aws,2020-10-13
iuz9by,Can someone please ELI5 automating DNS + TLS offload?,"Hey all, 
I’ve now had a total of 90 minutes as an admin of some AWS EC2 instances and I’m hoping the community is as friendly as I’ve been told. 

I’m building out a training lab environment and I would like to automate the creation of a subdomain and a TLS offload. My training instance is a web server that runs on port 5000 and it works great. 

Registering a student I can pass in variables so I would like to do something like this:

1. Register firstname-lastname.demo.cloud (or whatever) on route 53 pointing to (presumably) an AWS load balancer of sorts then

2. Create/Update a rule on listening rule of first name-lastname on the load balancer to forward port 80 to 443 and proxy port 443 to 5000 of the public hostname or IP of the ami instance with a valid wildcard cert. 

3.  Some sort of heartbeat or timer to clean up and remove stale mappings every N hours/days. 

I know *exactly* how to do this by giving students a USB stick with a VM, a Kemp / F5 and CloudFlare API. I’m trying to keep up with you kids and I can’t even figure out the right terms to research. :(

Thanks for your help reddit!",1,aws,2020-10-13
iuyyvp,How do I check how much data I have analyzed?,"Well, the title's pretty vague, so I'll try to explain my situation here. I'm attempting to use Rekognition to do some simple facial emotion recognition. The problem is that I do not want to spend any money. Good news is that Amazon allows for up to 5,000 images to be processed a month for 12 months for free on Rekognition. Bad news is that I can't figure out how many photos I have analyzed so far. I can't seem to find a dashboard, or webpage, on Amazon Web Services that states how many images or how long I have (in terms of images left to analyze) until I'm out of free images for the month. Would appreciate any help, thanks!

&amp;#x200B;

(also, this is my first time using AWS, its frickin awesome)",2,aws,2020-10-13
iv2a12,AmazonLinux2 package repos | S3 VPC endpoint sufficient or Internet access required? seems needed,"Hi all,

I was under the impression that the Amazon Linux package repo's were hosted in Amazons [own s3 buckets](https://aws.amazon.com/amazon-linux-ami/faqs/) ...but that article is old and referencing the Amazon Linux 1 AMI, not 2. Im not sure why AWS wouldn't host a mirror in their own S3 buckets, that would seem to 'finish off' Systems Manager capabilities in not requiring instances to leave private link, but allowing package updates via repos in S3 via S3 endpoint. Seems like this would be a easy win for them to satisfy customers egress concerns.

[Amazon Linux AMI basics](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-linux-ami-basics.html#package-repository) AWS page shows that EC2 instances 'must have internet access'. To test I setup NACL and SG to only allow traffic to/from S3 VPC endpoint. I do see in the yum configs for Amazon Linux 2 that there are a few repo files in /etc/yum.repos.d/, but for brevity there is:

amzn2-core.repo which has \[amzn2-core\] enabled with mirror list:

http//amazonlinux.$awsregion.$awsdomain/$releasever/$product/$target/$basearch/mirror.list

If I run a yum update I can see traffic timing out to:

http//amazonlinux.us-east-1.amazonaws.com/2/core/2.0/x86\_64/7bd66e56187a1363571cafcbc92d6f3a57fa4ac69ed6e5859f3f5rd3b166c8/repodata/repomd.xml?instance\_id=i-*instanceId*&amp;region=us-west-2:

Which If i hit from an internet connected machine I receive the xml which shows its a public hosted mirror at Duke University: [http://linux.duke.edu/metadata/repo](http://linux.duke.edu/metadata/repo)

Now I could setup a proxy like Squid, host a mirror locally or in S3, or just allow my subnet out to the internet, but it seems like AWS hosting their own mirrors in their own S3 buckets wouldn't be that much farther to go, I mean they have their own distro flavor and a repo named 'amzn-\*'. Thoughts? Its 1:30 am and I may not be thinking clearly, I was working on setting up patching for AmznLinux2 instances today and ran into this since we dont allow outbound internet from this particular environment.",1,aws,2020-10-13
iv20gc,Availability of t4g instances in zone eu-est-1 (Ireland),"Following the announcement of new t4g instances I updated my autoscaler launch configuration to use it as *""*[*T4g instances*](https://aws.amazon.com/ec2/instance-types/t4/) *are available today \[14 Sept.\] in US East (N. Virginia, Ohio), US West (Oregon), Asia Pacific (Tokyo, Mumbai), Europe (Frankfurt, Ireland).""*

But when launching a new instance I got the message :

&gt;Launching a new EC2 instance. Status Reason: The requested configuration is currently not supported. Please check the documentation for supported configurations. Launching EC2 instance failed.

Is there something I missed ?",1,aws,2020-10-13
iuzjwd,Use Custom API Gateway Domain across accounts,"I've got a root account as the manager of my organization, an infra account which holds SES identities and a regional custom domain name in API Gateway.

How can another account create an API mapping to that infra domain?

If that's not possible I can create separate domains per service, just thought there should be a way to have one central \`api.domain.com\` 🤔",1,aws,2020-10-13
iuyqkh,App architecture,"Hey guys, I’m new to the dev side of AWS. Long time on AWS infrastructure, have a idea for an app but unsure what services to go with. 

The general flow is user logs in, fills out a form, based on their choices it will generate a pdf with dynamic content. The user could then view their content and share it. 

I’ve heard good things about Amplify, but am not sure if it would work here. As I’ve read it is for static websites. 

I’m thinking - Some combination of S3, api gateway and lambda and Cognito. 

Any advice??",2,aws,2020-10-13
iuvhk9,AWS QuickSight - How to filter by individual text within a string of multi-select submission,"Hi All,

I'm trying to filter my data by individuals submissions but the data is inputted under a multi-select field. Here's an example:

**Raw Data Submissions**

* Sally
* Bob, Sally
* Sally, Jim, Harry

&amp;#x200B;

**Current Count**

Sally - 1

Bob, Sally - 1

Sally, Jim, Harry - 1

&amp;#x200B;

**What I need**

Sally - 3

Bob - 1

Jim - 1

Harry - 1

&amp;#x200B;

Appreciate any help on this! I assume it would be a calculated field or somehow to filter similar to Excel (i.e. ""\*word\*"") which would pull only that word from a set of text dvidied by commas. Thanks!",2,aws,2020-10-13
iuxyxo,How to Host a Certificate Revocation List In AWS?,"We need to copy our internal certificate authority certificate chain files and CRL files to an internet accessible website such as [www.ourdomain.com/certs/](https://www.ourdomain.com/certs/)

What's the easiest way to redirect that specific URL (not the entire [www.ourdomain.com](https://www.ourdomain.com), but only [www.ourdomain.com/certs](https://www.ourdomain.com/certs)) to an AWS hosted server accessible from the internet?

We already have an AWS account used for Workspaces, and would like to just add a simple webserver.

I assume such a low demand site with nothing on it other than small crl and crt files, the lowest level server would work.

What kinds of costs would we be looking at and what would be the most simple method to automate copying CRL files from our internal CA not hosted in AWS to the AWS web server?  Would the server be fully managed so that we don't need to deal with any management or patching of the web server?  All we need to do is be able to host and the files using the existing http CRL path URL that's already coded into the certificates.

We are not familiar with using Linux.  The internal certificate authority is a Microsoft Windows Server CA generating the CRL files.",1,aws,2020-10-13
iuv2r4,Looking for default value of a RDS parameter,"Hi there,

I am currently working on creating a few PostgreSQL RDSs in different environments using terraform. There RDSs uses the log\_min\_duration\_statement parameter and I am wondering what should be its default value. When I create an RDS using the AWS console, there is no value set for this particular parameter (Its shown as an empty null value). I have tried setting a null value to the parameter in my terraform code but it fails saying that it's invalid.

Any help would be appreciated",1,aws,2020-10-13
iutvr5,AWS console intermittent errors,"Is anyone else having intermittent issues with the AWS console loading?

I see the error below in the browser console and have cleared my browser cache multiple times.

&amp;#x200B;

    Uncaught TypeError: Cannot set property 'dependencies' of undefined
        at Object.115../EventConstants (newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1209)
        at s (newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1)
        at newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1
        at Object.78../BeforeInputEventPlugin (newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1076)
        at s (newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1)
        at newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1
        at Object.64../ReactCurrentOwner (newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1008)
        at s (newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1)
        at newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:1
        at Object.53../Object.assign (newhome_Prod_eb957b4c5c24e6e2a8dd8e8bf7ec29aff6914420.gz.js:958)

TIA",3,aws,2020-10-13
iuucnp,Using aws for selling digital goods,Any options in aws to setup digital downloads? Anyone currently doing this? Is there an aws service to do this?,0,aws,2020-10-13
iut8i2,Beanstalk Ruby with Puma Issues,"This afternoon we had our staging environment go down with the instances in beanstalk health reporting:  Following services are not running: application. 

This spread to our production environment as well on the next deployment.  Reverting to a previous application version did not help.

We are on the Puma with Ruby 2.6 running on 64bit Amazon Linux/2.11.x platform.  We were on an older version so I tried the newest but still no luck.

After much troubleshooting I found that puma 5.0.0 was released this afternoon and it was included in the new deployments.  /opt/elasticbeanstalk/support/conf/pumaconf.rb has a line ""daemonize false"" in it that has been deprecated  in puma 5.0.0.  Removing that line fixed our issue.  

We have used an ebextension to remove that line from the file and things are back to normal.  Wanted to throw this out there in case someone else runs into this issue.",4,aws,2020-10-13
iutppj,Provision snow cones instead of OpsHub?,Is it possible to pre configure  snow cones before shipping from AWS? I have a client that needs to send me a lot of data and I don’t want to have them go thru the effort of setting up the snow cone in OpsHub.,2,aws,2020-10-13
iusmks,Network outage sa-east-1??,"different services (EC2 &amp; RDS) are suffering to communicate here. São Paulo (sa-east-1).

Someone else with the same problem?

Edit: the incident is now registered in the aws status page: [https://status.aws.amazon.com/#SA\_archive\_block](https://status.aws.amazon.com/#SA_archive_block)

Thank you all",7,aws,2020-10-13
iurihj,Made a Free Streaming AWS Flow Log Monitor,"Made a Free Streaming AWS Flow Log Monitor. Check it out on [GitHub](https://github.com/d10nets/dupi-lambda).

[LIVE DEMO](https://dupi1.d10nets.com/dupi/view/flow)

Monitor and analyze Amazon AWS Flow Logs from EC2 network interfaces, VPC subnets or entire VPCs on a dedicated AWS cloud server streaming network traffic statistics in real-time to your browser via DUPI Streaming AWS Flow Log Monitor (**DUPI** = **D**eep-**U**niversal-**P**rotocol-**I**nspection).

An AWS lambda function exports all relevant flow data to a dedicated cloud server to enable observation of aggregate protocol statistics across multiple virtual sites to view network traffic in your business as a 'whole' or at individual sites.

Analyze your network traffic in real-time for deep visibility into actual traffic patterns. Define detailed alerting rules per specific protocol field on various metrics as request rates, bps, pps, lengths, counts and geo-location info.

Explore network traffic at your sites easily and interactively from the comfort of your browser. Quickly switch protocols, fields, intervals and apply specific filter conditions in the web application for instant streaming results.",2,aws,2020-10-13
ius2m6,Can't Regain Access to AWS Console,"After attempting to sign in this morning to update an SSL certificate, the AWS login wouldn't accept the password I had for it.

After a couple of attempts to reset the password with the one time password never showing up (OTP) I called the support number. After an hour of trying different things and about 15 verification texts, the woman on the line was able to switch the email address to another one at the office and I was able to change the password. However after so many attempts to reset the password we were locked out and she told me to wait 15-30 minutes and try again.

So I did, to no avail. I was on the phone several different times trying to gain access to the AWS console and after about 4 hours the 3rd person I talked to had me make a new account with the same address and select to override the existing account, and then she was going to transfer me to the AWS department to re-enable my account so I don't lose everything.

During the transfer my call got dropped and I then spent 20 minutes on the phone again explaining everything to the 4th person I've dealt with and just after she was talking about transferring me the call dropped again. Tired of reading off account information and getting new two-factor codes I finally found a form to submit a problem, maybe it was billing, because I don't know what else to do.
All the support ticket stuff requires me to log in, when I am unable to. I try logging in with the original email we had for the account and it says it doesn't exist. I try logging in with the new email address and it was me to setup a new AWS account.

Our websites are still up but I have no clue how to access the console for them and I'm not calling that useless reset password number for it again. Any suggestions? I need a number or email or some way to contact AWS support.",2,aws,2020-10-13
iuqw4q,Printing through workspaces using PCOIP,"Hi all,

I am a new Sys Admin at my place of work. for some context, I work for company A but they subcontract to companies B,C,and D. Companies C and D are both newly formed.

&amp;#x200B;

For The older companies, the previous Admin set up the networks including the printer setup, but was gone before we got to that part of training. I am still learning a lot about AWS since my training was fairly basic but this has me a bit stumped.

The situation is as follows:

I want to set up the two new companies to network printers. If we we are using real computers this would not be a big deal, but we are using PCOIP clients to connect to AWS for all users. The only notes I have on the process from the previous admin is that he used a server and a GPO to do it, but I have been unable to find more. I am looking for any resources you may have on this, advice, or even expatiation/step-by-step. 

Thank you so much for any help you can provide in pointing me to the right place.",0,aws,2020-10-13
iuramb,What would It cost to create a music streaming app for 100k users in AWS? I'll post the services I'm looking at using.,"Hi! So apologies id this is a dumb question, I don't really know how or where to ask a question like this. 

I'm looking to build a scalable music app probably with Docker on EC2 Servers. What I was thinking was I was gonna create

A 3 node cluster in Ec2 each node running docker containers for an N tier app, maybe even use an ELB

Maybe use S3 to store a song catalog and make my webapp get the song.

Then a simple web app that acts like a Spotify ui. 

Is that a reasonable start as far as AWS services go?

Yes this idea is very green but i figured id let you all to scrutinize me :).",1,aws,2020-10-13
iuomsp,EC2 Reserved Instance expired - Choosing a new instance,"Hi, our EC2 Reserved Instance expired after one year.

Currently, we're running a c5.xlarge, this one expired. But if we now choose a different C5 instance, like the smaller c5.large (not the X!) for the new year, would that instantly replace the old c5.xlarge that's now retired?

Of course the new c5.large RI would match the Availability Zone, platform, and tenancy of the currently running (but retired) c5.xlarge. So, as long as I keep those things the same and select an EC2 C5 instance, even though it's a different C5 instance, the retired C5 instance will be replaced without me having to create images of the old C5 instance or anything right?

I really appreciate any help!

Thanks",2,aws,2020-10-13
iupj8z,Noob Question - Pinging Google Through Nat Gateway.,This should be possible right?  Even if I have a private NAT Gateway I should still be able to ping sites like google.com... right?,1,aws,2020-10-13
iuojz9,"Yikes: AWS Aurora PostgreSQL versions vanish from the mega-cloud for days, leaving customers in the dark","Surprisingly, I saw this via /u/quinnypig's Twitter account. Has anybody here been affected by this? 

[AWS Aurora PostgreSQL versions vanish from the mega-cloud for days, leaving customers in the dark](https://www.theregister.com/2020/09/16/aws_aurora_postgresql_versions_disappeared/)",97,aws,2020-10-13
iunyj5,Invoking AWS Lambda via Unsigned POST to REST API,"I want to learn the nuts and bolts of how to invoke an AWS Lambda via an unsigned POST to a REST API. I want the simplest possible setup, and do not need any security. Given that the AWS Lambda is already created, and it just needs to be wired up to receive the POST, I expect that 3-4 API calls should be sufficient. I say this because the YouTube video [Create a REST API on AWS in 8 clicks ](https://www.youtube.com/watch?v=bYkjYojgccY&amp;feature=youtu.be) shows how to do what I want - but using the web interface.

I will only truly understand the mechanics when I can see the method calls laid out. For those people who would like to suggest there might be better ways to solve this problem, thank you, but I am not asking for the 'best' approach according to some (unspecified) metrics, I just want to know the API Gateway method calls necessary to perform this task.

I want to have a form on my Jekyll website that visitors can fill out, and the `form` `action` should `POST` to an AWS Lambda function. No JavaScript is allowed on the website, so the `POST` must not require signing.

I want to use command line commands exclusively (not a web browser) to work with the AWS API. This allows for a scripted solution.

The bash script below uses [jq](https://stedolan.github.io/jq/) to parse JSON.

```script
# Please assume the following environment variables are already set:

GATEWAY_NAME=addSubscriberTo_eb3852a34f
LAMBDA_IAM_ROLE_NAME=lambda-ex
LAMBDA_IAM_ROLE_ARN=arn:aws:iam::031372724784:role/lambda-ex
LAMBDA_NAME=addSubscriberAwsLambdaSample
LAMBDA_RUNTIME=python3.8
LAMBDA_HANDLER=addSubscriberAwsLambda.lambda_handler
LAMBDA_ZIP=../_package/function.zip

echo ""Attach the AWSLambdaBasicExecutionRole managed policy to $LAMBDA_IAM_ROLE_NAME.""
aws iam attach-role-policy \
  --role-name $LAMBDA_IAM_ROLE_NAME \
  --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole


#### Integrate with API Gateway for REST
#### I don't know what I am doing here
#### Most of the following code is probably not required

API_GATEWAYS=""$( aws apigateway get-rest-apis )""
if [ ""$( jq "".items[] | select(.name | contains(\""$GATEWAY_NAME\""))"" &lt;&lt;&lt; ""$API_GATEWAYS"" )"" ]; then
  echo ""API gateway '$GATEWAY_NAME' already exists.""
else
  echo ""Creating API gateway '$GATEWAY_NAME'.""

  API_JSON=""$( aws apigateway create-rest-api \
    --name ""$GATEWAY_NAME"" \
    --description ""API for adding a subscriber to the Mailchimp list with ID '$MC_LIST_ID_MSLINN' for the '$DOMAIN' website""
  )""
  REST_API_ID=""$( jq -r .id &lt;&lt;&lt; ""$API_JSON"" )""

  API_RESOURCES=""$( aws apigateway get-resources --rest-api-id $REST_API_ID )""
  ROOT_RESOURCE_ID=""$( jq -r .items[0].id &lt;&lt;&lt; ""$API_RESOURCES"" )""

  NEW_RESOURCE=""$( aws apigateway create-resource \
    --rest-api-id ""$REST_API_ID"" \
    --parent-id ""$RESOURCE_ID"" \
    --path-part ""{proxy+}""
  )""
  NEW_RESOURCE_ID=$( jq -r .id &lt;&lt;&lt; $NEW_RESOURCE )

if false; then
  # Is this step useful for any reason?
  aws apigateway put-method \
    --authorization-type ""NONE"" \
    --http-method ANY \
    --resource-id ""$NEW_RESOURCE_ID"" \
    --rest-api-id ""$REST_API_ID""
fi

# The following came from https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#set-up-lambda-proxy-integration-using-cli
#   Instead of supplying an IAM role for --credentials, call the add-permission command to add resource-based permissions.
#   I need an example of this.
# Alternatively, how to obtain IAM_ROLE_ID? Again, I need an example.
  aws apigateway put-integration \
    --credentials ""arn:aws:iam::${IAM_ROLE_ID}:role/apigAwsProxyRole"" \
    --http-method ANY \
    --integration-http-method POST \
    --rest-api-id ""$REST_API_ID"" \
    --resource-id ""$NEW_RESOURCE_ID"" \
    --type AWS_PROXY \
    --uri arn:aws:apigateway:`aws configure get region`:lambda:path/2015-03-31/functions/$LAMBDA_ARN

  if [ ""$LAMBDA_TEST""]; then
    # Deploy the API to a test stage
    aws apigateway create-deployment \
      --rest-api-id ""$REST_API_ID"" \
      --stage-name test
  else
    # Deploy the API live
    aws apigateway create-deployment \
      --rest-api-id ""$REST_API_ID"" \
      --stage-name TODO_WhatNameGoesHere
  fi
fi

echo ""Check out the defined lambdas at https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions""
```

Thanks in advance!",1,aws,2020-10-13
ium988,Retries in aws-sdk,"I'm working with AWS secret manager js client SDK. The syntax to initialize the client is with retry is

`const secretsManager = new SecretsManager({region: region});`

From the documentation, it is not obvious to me if the SDK does retries itself or do we need to explicitly mention it? It does mention [here](https://docs.aws.amazon.com/general/latest/gr/api-retries.html) that each SDK comes with automatically reties requests, but I just want to be sure if that is what the `SecretsManager` will do too.

&amp;#x200B;

The [documentation](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#maxRetries-property) for `maxRetries` parameter doesn't mention that.

Also, does the `maxRetries` cover the list of errors when the mentioned [here](https://docs.aws.amazon.com/secretsmanager/latest/userguide/best-practices.html#throttling)?

&amp;#x200B;

My main question, would using the SDK as is without providing any special parameters have retries for the errors mentioned [here](https://docs.aws.amazon.com/secretsmanager/latest/userguide/best-practices.html#throttling)?",1,aws,2020-10-13
iunjnz,(How) can you run a TURN server on Ubuntu Server 18.04 EC2 with TLS on free tier?,"For helping run BigBlueButton. Certbot said 'An unexpected error occurred:
The server will not issue certificates for the identifier :: Error creating new order :: Cannot issue for ""&lt;number&gt;.&lt;other stuff&gt;.amazonaws.com"": The ACME server refuses to issue a certificate for this domain name, because it is forbidden by policy'
when I tried getting TLS. I guess I need Route 53 but not sure if that's in the free tier because I'm just experimenting.",1,aws,2020-10-13
iuna6n,Access Key Best Practice For Veeam Backup Integration,"Hey,

Im pretty new to Amazon AWS. I'd like to migrate our cloud backup to an S3 bucket instead.

So far, I've created a root user and than an IAM user.

In our Veeam Backup console, it's using the Access Key from the IAM user I created. Is this bad practice? Should I be creating a service account an generating a key from that or does it not matter.",1,aws,2020-10-13
iul5h3,"Limitations on a ""Preview"" instances?","In some regions some instances types are denoted as ""preview"". Does this mean it can only be launched via CLI? Are there any other limitations such that someone wouldn't want to run production loads on them?",0,aws,2020-10-13
iukld0,Is there a European version of AWS IQ?,AWS IQ sounds great but I am in Europe. Denmark and the UK. Would be awesome to have a similar service offered there.. note: it doesn’t have to be an AWS service itself,1,aws,2020-10-13
iukfky,Eventbridge pricing,"Hi guys, i need a confirmation on pricing for Eventbridge. Let's say, I create a rule and define a pattern to match all ECS events. Are those events free of charge? Accordingly to AWS pricing page 'AWS service events' published to eventbridge are free.",0,aws,2020-10-13
iuj0ww,S3 Replication RTC with CloudFormation?,"Hey Guys!

I've been writing a CF template that will create two S3 buckets and setup SRR (Same Region Replication) between them. Got everything working fine and the buckets replicate no bother. But when i try to add RTC (and get the 15 minutes replication time) to the template it all fails and i can't even deploy it.

I've followed along with the S3 CloudFormation docs and did exactly as it said. The problem seems to be from this document and the one it links to. I've found no examples online or anything and im beginning to think this feature barely exists lol.

Error im getting inside CloudFormation is :

*Encountered unsupported property ReplicationConfiguration*

My code is below that im using for the bucket creation that im adding RTC to (with the bucket names changed), any help would be so appreciated!

      OriginalBucket:
        Type: AWS::S3::Bucket
        Properties:
          BucketName: original-bucket
          VersioningConfiguration:
            Status: Enabled
          ReplicationConfiguration:
            Role: !GetAtt ReplicationRole.Arn
            Rules:
            - Destination:
                Bucket: !GetAtt DestinationBucket.Arn
                ReplicationTime:
                  Status: Enabled 
                  Time:
                    Minutes: 15
                Metrics:
                  Status: Enabled
                  EventThreshold:
                    Minutes: 15
              Prefix: """"
              Status: Enabled

Edit: For all those who are wondering, after scouring the Developer Forums, it turns out that RTC is currently not supported by CloudFormation. [https://forums.aws.amazon.com/thread.jspa?messageID=942241&amp;#942241](https://forums.aws.amazon.com/thread.jspa?messageID=942241&amp;#942241)",3,aws,2020-10-13
iuiuza,docker container - simplest way to host?,"Hi,

All my experience uptil now is on kubernetes.

I am prototyping an idea and need a cheap and simple way to deploy it on aws. 

Requirements:

\- supports container

\- needs RDS

\- needs internet access (for external apis)

\- should support basic ci/cd pipeline. (I use gitlab)

&amp;#x200B;

ECS seems the route but as soon as I put it inside vpc, it lose internet and nat gateway is way expensive for small prototype!

&amp;#x200B;

Thanks.",6,aws,2020-10-13
iugy1y,Starting with AWS SDK,"Going to build some application to integrate with aws, e.g. getting some cloudwatch metrics. Any way i can make request to aws and test the api free? Any sample data response?",1,aws,2020-10-13
iuf9uu,Following the Juli 2020 ruling of the EU court that the EU/US Privacy shield is invalid: Will we be able to use AWS for customer projects in the near future?,"I have heard several opinions on the EU/Privacy Shield Rulings and the german politics is pretty much silent on the specifics so far. Given that we (In Germany) have no real alternative to AWS here, the situation seems to be that AWS, Azure, Google Cloud, Digital Ocean etc. are not good for projects storing customer sensitive data (aka everything with a login).   
So fellow Europeans, Germanz and other people of the world: What do you think is the case?",4,aws,2020-10-13
iuenfc,How can I remove these duplicate amplify apps?,,1,aws,2020-10-13
iub770,AWS WorkSpaces Custom Image,"Hey All,

Anyone got any ideas to why a custom image with 3rd part software installed (installed as admin) bundled and new workspaces created prompts users to run the programs each time?

Any pointers would be great!

eg. 

https://preview.redd.it/3t9w2mkqkmn51.png?width=406&amp;format=png&amp;auto=webp&amp;s=caa079d487fbf370a2ca550a443a9151a0c1cf45",1,aws,2020-10-13
iugle3,How to have 2 Lambda functions communicate with each other.. when one of them doesn't have Internet access?,"Here's my thing:

I got 2 functions A &amp; B:

* A is in a default Lambda VPC, provided by AWS, that has open access internet inbound and outbound
* B is in a specific VPC with a RDS DB, with inbound access but no Internet access (no NAT Gateway).

The process is this : 

* A sends data to B while B inserts that data in a RDS Database
* B is supposed to send confirmation of whether it's okay or not but A keeps getting a 502 error (timeout I guess).

At first I thought it was a stupid Lambda proxy output format error but now I realised it's more serious than that.

I'm looking for some easy/cheap way out (i'm a student on a budget and this is a PoC). Is there some kind of easy solution to this problem?",1,aws,2020-10-13
iugisk,Help / Advise with AWS Elastic Transcoder,"Hi everyone! 

&amp;#x200B;

New to AWS but I'm stuck trying to figure something out....if anyone can help me that would be great! 

So I'm playing around with Elastic transcoder to convert an uploaded videos from my S3 bucket to a GIF.

At the moment, when I upload to S3, I have a basic lambda which triggers a pipeline to do the converting to gif.

Now whats happening here is I upload a 7mb mp4 and when its transcoded the output gif is 80+ MB's??

I've looked through the docs and I cant find anything that helps me. Can anyone shed some light...

Below is my presets in elastic transcoding

&amp;#x200B;

    Codec -- gif  
    Codec Options:  
    LoopCount: 5  
    Bit Rate -- auto 
    Frame Rate -- auto 
    Video Max Frame Rate -- 10 
    Max Width -- auto 
    Max Height -- auto 
    Sizing Policy -- ShrinkToFit 
    Padding Policy -- NoPad 
    Display Aspect Ratio -- auto

And here is my Lambda 

&amp;#x200B;

    'use strict';
    var AWS = require('aws-sdk'),
        transcoder = new AWS.ElasticTranscoder({
            apiVersion: '2012-09-25',
            region: 'eu-west-1'
        });
    exports.handler = (event, context, callback) =&gt; {
        let fileName = event.Records[0].s3.object.key;
        var srcKey =  decodeURIComponent(event.Records[0].s3.object.key.replace(/\+/g, "" ""));
        var newKey = fileName.split('.')[0];
        console.log('New video has been uploaded:', fileName);
    transcoder.createJob({
         PipelineId: process.env.PIPELINE_ID,
         Input: {
          Key: srcKey,
          FrameRate: 'auto',
          Resolution: 'auto',
          AspectRatio: 'auto',
          Interlaced: 'auto',
          Container: 'auto'
         },
         Output: {
          Key: getOutputName(fileName),
          ThumbnailPattern: '',
          PresetId: 'XXXXXXXXXXX',
          Rotate: 'auto'
         }
        }, function(err, data){
            if(err){
                console.log('Something went wrong:',err)
            }else{
                console.log('Converting is done');
            }
         callback(err, data);
        });
    };
    function getOutputName(srcKey){
     let baseName = srcKey.replace('videos/','');
     let withOutExtension = removeExtension(baseName);
     return 'gifs/' + withOutExtension + '.gif';
    }
    function removeExtension(srcKey){
        let lastDotPosition = srcKey.lastIndexOf(""."");
        if (lastDotPosition === -1) return srcKey;
        else return srcKey.substr(0, lastDotPosition);
    }",1,aws,2020-10-13
iug6j1,Τ-family instances,"After the announcement of the new T4g we are checking the wider T-family. Unpredictable pricing is a little awkward. On the other hand, the bursting concept seems convenient.

Interestingly enough their base price is lower than respective Ms. For example, in Ohio:

* T3.xlarge (4 vCPUs, 16GB RAM) is at $0.1664/hr
* M5.xlarge (4 vCPUs, 16GB RAM) is at $0.192/hr 

Have you used T-family instances? What do you think about them?",3,aws,2020-10-13
iufnk5,New AWS Console with favorites menu inside the services button,"Now i need two clicks to visit a service instead of right clicking my favorite small icon service and opening in a new tab.

I know this console is not at all used if you do IaC but sometimes this needs to be shown to others in a graphical way",54,aws,2020-10-13
iue6xt,S3 endpoint question,Is it possible to create an S3 endpoint with an domain name registered outside of R53?,1,aws,2020-10-13
iue10h,"HELP, Where this expired certificate is coming from?","I'm deploying Docker PHP-Apache to ECS ([https://hub.docker.com/\_/php](https://hub.docker.com/_/php))

But when I checked the certificate, one of the certificates chains is expired?

https://preview.redd.it/fprye2zgjnn51.png?width=626&amp;format=png&amp;auto=webp&amp;s=3b34255556216cc7abae9973f30a3e3c44386530

but I never install any certificate inside the container, since the load balancer only pointing to port 80 of the container. I only installed the certificated to Load balancer only with the below info:

https://preview.redd.it/c3ymqn0iknn51.png?width=283&amp;format=png&amp;auto=webp&amp;s=fba4c3e539134ff19e3dad3706bc83d389742ab9

When I tried curl from other EC2 instance:

    $ curl --verbose https://thedomain.com
    * Rebuilt URL to: https://thedomain.com/
    *   Trying &lt;IP.IP.IP.IP&gt;...
    * Connected to thedomain.com (&lt;IP.IP.IP.IP&gt;) port 443 (#0)
    * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
    * successfully set certificate verify locations:
    *   CAfile: /etc/ssl/certs/ca-certificates.crt
      CApath: none
    * TLSv1.2 (OUT), TLS handshake, Client hello (1):
    * TLSv1.2 (IN), TLS handshake, Server hello (2):
    * NPN, negotiated HTTP1.1
    * TLSv1.2 (IN), TLS handshake, Certificate (11):
    * TLSv1.2 (OUT), TLS alert, Server hello (2):
    * SSL certificate problem: certificate has expired
    * Closing connection 0
    curl: (60) SSL certificate problem: certificate has expired
    More details here: http://curl.haxx.se/docs/sslcerts.html
    
    curl performs SSL certificate verification by default, using a ""bundle""
     of Certificate Authority (CA) public keys (CA certs). If the default
     bundle file isn't adequate, you can specify an alternate file
     using the --cacert option.
    If this HTTPS server uses a certificate signed by a CA represented in
     the bundle, the certificate verification probably failed due to a
     problem with the certificate (it might be expired, or the name might
     not match the domain name in the URL).
    If you'd like to turn off curl's verification of the certificate, use
     the -k (or --insecure) option.

This is really causing me a headache, Any tips, please?",1,aws,2020-10-13
iub9l0,Noob in need of help : which OS should I choose for aws lightsail instance?,"Hi, I'm a noob in aws and I'm trying to host a small website for myself on aws. Since potential user is just me I'm going to go for the lowest plan (3.5$) and there are myriad of options and I can't make up my mind. I want to use node.js and python and maybe django. Which os should I choose?",1,aws,2020-10-13
iuab83,AWS CloudTrail question: Have I been pwned?!,"I noticed in my cloudtrail that there was a Decrypt event logged. It was not initiated by myself, and I want to know if this is normal or have I been pwned? The listed username appears to be a hash of some kind, an alphanumeric value (as opposed to ""root"" which is what I usually use for everything). My instance was rebooted too at the same time.

Event Source: kms.amazonaws.com

Event Name: Decrypt

User Agent: aws-internal/3

&amp;#x200B;

Have I been pwned?

&amp;#x200B;

&amp;#x200B;

EDIT: I have probably not been pwned. 

Upon inspecting the status check logs it looks like there was a status check failure at the same time when all of this happened. So I assume it was a normal operation conducted by aws to migrate the instance and get it up and running again. ",2,aws,2020-10-13
iuaxpl,Best way to scale ECS instances with on demand tasks?,"Here's my configuration/restrictions:

1. I have processing jobs that I want to run on demand, the load is unpredictable.

2. Unfortunately, the underlying Docker images are big, 3-5 GB and I have no easy way to make them smaller.

3. I would prefer to just use Fargate and not worry about instances at all but Fargate cannot [cache images yet](https://github.com/aws/containers-roadmap/issues/696) and I need to avoid the slow download times.

4. Regular ECS support image caching via the ECS_IMAGE_PULL_BEHAVIOR parameter.

5. With ECS though, I now  need to worry about instance scaling. Now, when I attempt to place a task, if it fails, I suppose that I can detect that failure and scale up my ECS cluster. I could probably scale down based off of CPU and/or memory fairly easily, but there will be a penalty in the scale-up case, I'll have to wait till the new instance spins up.

Thinking out loud here, I suppose that I can configure a capacity provider to scale appropriately. What I really want is to scale based on a formula. Let's say I restrict each image so that I can fit 3 on an instance, what I want is to add another standby instance as soon as I've scheduled 3 tasks, or maybe 2 tasks (n - 1) so that I'll always have enough capacity.

Any help/ideas are appreciated.",3,aws,2020-10-13
iuarvt,Got interviewed at AWS for a Security role - The interviewer refuse to accept privilege escalation examples I provided.,"So basically I was at a Technical interview with AWS recently for a Security Engineer role. The interviewer asked me about Privilege escalation. One of the examples I gave him is how if a current user is allowed to run vim with sudo permission, they can basically spawn a root shell.

I also explained to him a scenario where SUID can be exploited to escalate privilege to become a root user and spawn a root shell.

Here is what I provided  an explanation about,[https://medium.com/@gbmbalag/linux-privilege-escalation-by-using-suid-19d37821ed12](https://medium.com/@gbmbalag/linux-privilege-escalation-by-using-suid-19d37821ed12)

[https://gtfobins.github.io/gtfobins/vim/](https://gtfobins.github.io/gtfobins/vim/)  


I provided the scenarios, where you could only run one specific script using SUID (you are limited to only run that/ or it automatically run as a CRON job) and how it can be exploited to escalate privilege to spawn a root shell and do basically everything else as root (I have personally done it so I gave them those examples).

Please note that I have been using all these exploits and techniques on [HackTheBox.eu](https://HackTheBox.eu) so I had some good knowledge of how these things work, and I was aware of what I was talking about.

The interviewer only has one answer. ***It's not privilege escalation if you use the SUID technique or a technique where a command already has a sudo privilege and you use it to spawn a root shell.***

The consensus is, I ended up having an argument about it because he couldn't even explain to me what Privilege Escalation means if this is not what it is.

I got rejected for the role.

Can someone please let me know where I was wrong?",3,aws,2020-10-13
iu95vk,Data warehousing in AWS practical approach,"I am into data warehousing since past 10 years but very new to AWS environment. I have couple of questions related to DW in AWS.

* I started learning Redshift (I do have very good knowledge in SQL Server &amp; MySQL). Do I have to learn more databases in AWS to work in data warehousing projects?

* I have worked on SSIS and Talend, I know that columnar usually prefer ELT but based on my experience (project specific), I found that ETL help us to reduce data size, faster processing of data (in reporting) &amp; clean data. Ex: I can skip rows with incorrect data types, or apply business rule to process limited data (ex: my text file may contain data from multiple region but I have to load only USA related data). How big organizations with millions of rows do ETL on cloud? 
I can think up of two approach: 
1: Use Glue 
2: Use EC2 instance, use talend there and take source as S3 and destination as Redshift

Is there any other way to do it?

* How big organization handle agile development? I mean, let say I have developed a glue job in test environment. Now, in production my S3 folder could be different &amp; many things could be different. Usually, SSIS/Talend provides a deployment method with parametrization. How agile development happens in enterprise level projects?

* After Googling for long, I couldn't find a realistic DW implementaion discussed online about how companies use Redshift (or other AWS databases?) for DW. All I can see is AWS white paper which has tons of services involved even for a simple task. I personally think that using 7-15 services to develop a DW architecture is not a smart solution but I can be horribly wrong because I am novice in AWS. If you have worked on DW on AWS, can you please help me know how data processing happens on AWS? 
Ex: Storage Gateway &gt; S3 &gt; Glue &gt; Redshift &gt;&gt; reporting

Sorry for my bad English, it isn't my first language &amp; this is my first post on reddit so I don't know if this is how I should be asking help.",2,aws,2020-10-13
iu8nj6,Lightsail load balancer for Magento 2,"I have this M2 store up and running in lightsail, not bitnami fully configured by me in ubuntu. 

However, I haven't work in the redis, varnish part because I never done it before and was ""working fine"" but then the other day we got traffic over 1000 people and the CPU just couldn't handle it even with the highest lightsail tier 8core CPU 32 Ram. it just crashed for a minute.

So I notice the load balancer option, created an external DB linked M2 to it, made another instance from the latest snapshot. Added both instances to load balancer and got ""healthy"" status.

I thought everything was working fine. but then noticed in the metrics the new instance was not active at all.  maybe  the balancer was going to move traffic to it after the main instance started to raise traffic? but then one day we got high traffic and the main instance started to slowdown and the new stayed with cpu without activity so I realize there was no traffic moving to it. 

I followed all the steps I found, but all the guides only talk about wordpress. 

I need urgently to make my M2 stable for high traffic.

what step am I missing for M2 to have a stable load balancer?",2,aws,2020-10-13
iu819s,Cumulative sum drop to zero in QuickSight,"In AWS QuickSight, I have a line chart that displays cumulative value of a column over time. I'm using calculated fields with \`runningSum()\` function to get the cumulative sum, like so:

`runningSum(sum(COALESCE( NULLIF(col_one, '') , 0 )), [col_two ASC], [col_three])`

The cumulative sum that I get first rises, then drops to zero and then rises again. Now, assuming, or rather considering, that the values in col\_one will always be positive, the cumulative sum should never drop to zero once it goes up from zero, IMO. Any insights what could be causing this?",1,aws,2020-10-13
iu7lwt,How to prevent S3 bucket content to be modified by non-app IAM roles?,"I want to make it hard to delete S3 bucket or delete/modify objects inside it by mistake.
The problem is that occasionally we need to modify object for administrative reasons. We never delete the bucket and older objects manually in production, though. It's OK to allow reading objects by others.

I have considered restricting DeleteObject/PutObject by non-app roles, but doing so will deny all modifications for administrative reasons. I probably need another role that can be assumed for administrative reasons and add it to the policy as well. I'm not sure if this still allows S3 to delete older object version with lifecycle configuration.

Or, the middle ground would be only denying bucket delete and manual object version deletion, and instruct everyone to use users/roles with less permissions for daily use?",1,aws,2020-10-13
iu7j17,Deployment Bucket Rename (September 30 deadline - virtual-hosting style S3 URLs),"Going to try keep this short.

I use Serverless to deploy lambdas as APIs for a frontend Application

In serverless.yml my **deploymentBucket.name** is set to be **api.dev.xxxx.com**  (xxxx is the app name) which has been throwing me warnings lately about S3 bucket naming

&gt;WARNING: DNS incompatible bucket name detected ([api.dev.xxxx.com](https://api.dev.choirpal.com)).  
&gt;  
&gt;These will cause errors starting 30 Sep 2020 due to S3 API changes  
&gt;  
&gt;and SSL certificate mismatches for virtual-hosting style S3 URLs.

&amp;#x200B;

Unfortunately, I'm using the same bucket for all 4 stages of the app (DEV/TEST/STAGING/PRODUCTION)

\- I KNOW this was not a good move putting all APIs from all stages into one bucket hence me wanting to change it as well as complying with S3's new naming policy.

What's my next move how do I change my bucket name without deleting everything and possibly affecting users using the PRODUCTION APIs?

&amp;#x200B;

Apologies if this should be posted in r/serverless but I figured it was an AWS S3 issue more than a serverless issue.",2,aws,2020-10-13
iu7ab3,RDS Aurora MySQL: How is the database so expensive? Need to reduce bill.,"We're on Aurora MySQL and it's costing us a fortune. How can we bring this cost down?

Edit: Additional details below

- Fortune is thousands of dollars to me.
 
- Using db.r5.xlarge since Aurora doesn't give too many options 

- It's for roughly 200k users accessing a SaaS business application. It spikes in the morning at 7am, then dips a bit, peaks in the afternoon and drops after 9.30pm. 

- Got a writer and auto-scaling multi AZ readers",0,aws,2020-10-13
iu78yn,Anyone using Trend Micro Deep Security/Cloud One on their EC2 workloads?,"Is it any good for you? Is there anything better at the same price? We're using it but got mixed results.

Edit: adding more details below.

They keep sending alerts that our agents are not updated and then they get updated later. The logs analysis doesn't seem to work.

Protecting anywhere between 5 and 50 instances as they scale up and down.

Not using Cloud Connector.

Yes the agents are installed on our AMIs directly. We install the agents separately, not using the ready made AMIs offered by them.",1,aws,2020-10-13
iu77au,How do you monitor your logs?,"Edit: I'm asking for CloudTrail logs, S3 access logs, and HTTP request logs. Things like security violations, usage patterns, anomalies, etc.

What do you use to monitor your logs? How effective is it? How much does it cost?",2,aws,2020-10-13
iu71oa,How to screenshot a Remote Desktop screen and save it to your pc,"As the title says, I need to take a screenshot of the Remote Desktop, but when I do it saves it to the Remote Desktop and not my physical pc. Any help?",0,aws,2020-10-13
iu6pai,Boto3 : create_distribution - any good examples?,"Has anyone actually managed to create a basic CF distribution via Python/Boto3?

Just going through and trying to figure out which fields are required/optional.",2,aws,2020-10-13
iu686c,Another outage?,"Looks like this one might be IAM related.

&gt;\[03:17 PM PDT\] We are investigating increased authentication error rates and latencies affecting IAM. IAM related requests to other AWS services may also be impacted. 

My eb and aws cli are both affected, can't push anything.",96,aws,2020-10-13
iu5irw,Lake Formation Blueprint in Production?,"Has anyone used lake formation blueprints for daily database snaps from on premise databases?

I have a very nice spark jdbc codebase that does everything I need for 170 oracle tables but am being asked to investigate Blueprints.

I am a bit worried it’s going to have flaws and not provide the flexibility. The reason this codebase was created years ago was because of flaws of Apache sqoop.",3,aws,2020-10-13
iu2rrr,Using AWS Lambda for map and collect,"Basically, I want to execute an embarrassingly parallel map job using lambda (which I understand how to do), but I want to trigger the next step (Lambda or Batch) in my workflow only after all ALL lambdas from first step completed execution. Will AWS Step Function + `.sync` help me do this? Is there a way to do it without step functions?",3,aws,2020-10-13
iu0pt9,Working on a simple chrome extension for adding shortcuts to the console navigation bar!,,2,aws,2020-10-13
iu0v1n,AWS CLI Query for pulling image from ecs describe-task-definition?," So I'm trying to pull just the image from a task-definition. I'm running:

aws ecs describe-task-definition --task-definition nameoftaskdefinition --output text

And I'm trying add in the --query option so that result will look like

VAR = $(aws ecs describe-task-definition --task-definition nameoftaskdefinition --output text)

where echo $VAR would return image

 Any help would be greatly appreciated.",1,aws,2020-10-13
iu1hi0,What's your approach to operational security in AWS?,"Say you have a classical architecture in a production account with: EC2 instances, RDS databases, EKS clusters (Kubernetes), 100 % deployed Terraform via CI/CD, and you have the following typical requirements.

Operational requirements:  
\- Ability to SSH to any EC2 instance  
\- Ability to manage databases at the application-level, e.g.connect to MySQL using the mysql CLI client  
\- Ability to hit the Kubernetes API, e.g. using kubectl

Main security requirements:  
\- Nothing exposed to the Internet, everything is in an internal VPC  
\- All accesses to should be nominative, i.e. using your AWS (SSO) identity  
\- No long-lived credentials with privileged access  


How would you approach this? The quick wins seem to be:

\- Users in AWS don't have any permissions by default and need to assume a specific role depending what they need to do  
\- Use SSM to access EC2 instances  
\- Reuse AWS identity to authenticate to [K8s](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html) and [RDS](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html)  


What about the DB management and K8s API access requirements? Do you use a jump host and let Ops perform administrative actions from their laptop? Do you enforce the use of a bastion? But then, how do you propagate each user's AWS identity to the bastion and ensure distinct bastion users are properly separated?

Your inputs and returns on experience are appreciated!",5,aws,2020-10-13
iu2bio,Is the AWS Free Tier Really Free? (Spoiler: it is not.),,5,aws,2020-10-13
iu0c8d,We are the AWS EC2 Team - Ask the Experts - Sep 24th @ 9AM PT / 12PM ET / 4PM GMT!,"Hey r/aws! u/AmazonWebServices here.

The AWS EC2 team will be hosting an Ask the Experts session here **in this thread** to answer any questions you may have about **deploying your machine learning models to Amazon EC2 Inf1 instances powered by the** [**AWS Inferentia chip**](https://aws.amazon.com/machine-learning/inferentia/), which is custom designed by AWS to provide high performance and cost-effective machine learning inference in the cloud. These instances provide up to 30% higher throughput, and 45% lower cost per inference over comparable GPU-based instances for a wide variety of machine learning use cases such as image and video analysis, conversational agents, fraud detection, financial forecasting, healthcare automation, recommendation engines, text analytics, and transcription. It's easy to get started and popular frameworks such as TensorFlow, PyTorch, and MXNet are supported.

Already have questions? Post them below and we'll answer them starting at 9AM PT on Sep 24, 2020!

\[EDIT\] We’re here today to answer questions about the AWS Inferentia chip. Any technical question is game! We are joined by:

&amp;#x200B;

https://preview.redd.it/i44hnlgza4p51.png?width=1272&amp;format=png&amp;auto=webp&amp;s=8052a61f6728d646b0f5cecda5715a92b8c6c1dc

* Chetan Kapoor - Senior Manager, EC2 Product Management
* Gadi Hutt - Senior Director, Business Development, Annapurna Labs
* Monica Joshi - Senior SDM - Inferentia ML Applications
* Rich Heaton - Senior Software Manager, Annapurna Labs

We're here for the next hour!",36,aws,2020-10-13
iu01uj,Adding new AWS EBS Volume to ASG in same AZ,"ok, so I am trying to attach an EBS volume which I have created using Terraform to an ASG's instance using userdata, but now issue is both are in different AZ's, due to which, it failing to attach. Below is the steps I am trying and failing:

    resource ""aws_ebs_volume"" ""this"" {   for_each = var.ebs_block_device     size              = lookup(each.value,""volume_size"", null)     type              = lookup(each.value,""volume_type"", null)     iops              = lookup(each.value, ""iops"", null)     encrypted         = lookup(each.value, ""volume_encrypt"", null)     kms_key_id        = lookup(each.value, ""kms_key_id"", null)     availability_zone = join("","",random_shuffle.az.result) } 

In above resource, I am using random provider to get one AZ from list of AZs, and same list is provided to ASG resource below:

    resource ""aws_autoscaling_group"" ""this"" {   desired_capacity          = var.desired_capacity   launch_configuration      = aws_launch_configuration.this.id   max_size                  = var.max_size   min_size                  = var.min_size   name                      = var.name   vpc_zone_identifier       = var.subnet_ids // &lt;------ HERE   health_check_grace_period = var.health_check_grace_period   load_balancers            = var.load_balancer_names   target_group_arns         = var.target_group_arns    tag {     key                 = ""Name"" value = var.name     propagate_at_launch = true } }

And here is userdata which I am using:

    TOKEN=`curl -X PUT ""http://169.254.169.254/latest/api/token"" -H ""X-aws-ec2-metadata-token-ttl-seconds: 21600""`  instanceId = curl -H ""X-aws-ec2-metadata-token: $TOKEN"" http://169.254.169.254/latest/meta-data/instance-id  aws ec2 attach-volume --volume-id ${ebs_volume_id} --instance-id $instanceId --device /dev/nvme1n1

Above will attach the newly created volume, as I am passing output ${ebs\_volume\_id}  
 of above resource.

But, its failing because instance and volume are in different AZs.

Can anyone help me on this as a better solution than **hardcoding AZ on both ASG and Volume**?",1,aws,2020-10-13
ityafv,AWS to ASA ikev2 site 2 site vpn,"AWS config includes multiple peers, does the ASA not support this? (static/no VTI or dynamic routing)

Does Phase 1 and Phase 2 agreed parameters need to match (exact same enc/integrity/DF group for both Phase 1 and Phase 2). Seeing no packet decaps back so not sure if due to this due to ASA having ikev2 lower security profiles at the top.

&amp;#x200B;

Thanks",1,aws,2020-10-13
iu0qto,Beanstalk app keeps two ec2 instances running despite zero load,"I have a tiny app launched on Beanstalk, in my ec2 console I see two instances running with the app's tag name. Is this normal?

I'm also trying to figure out the proper way to ""right-size"" the ec2 instance to avoid unnecessary autoscaling. Curious how others establish a baseline for this.",2,aws,2020-10-13
iu0czg,Elasticache CloudWatch percentage metrics seem too low,"I'm performance testing my Ruby on Rails product, which uses Sidekiq. Sidekiq uses Redis as a job queue, and we host our Redis instance using ElastiCache. 

During performance tests, I can see that my CPU % seems to cap at 1% [(pic attached)](https://imgur.com/7CV7Svt), but one of the possibilties is that maybe CloudWatch is returning the ""ElastiCache - CPU utilisation (percent)"" metric as a fraction instead of a percentage (meaning that it's really at 100%). I have a similar suspicion for other ElastiCache metrics like ""Database Memory Usage Percentage"" [(pic also attached)](https://imgur.com/VPZUCBL).

Could someone please help me figure out whether these CloudWatch metrics are definitely percentages?",1,aws,2020-10-13
itz955,Tagging Policy Approach,"I'm trying to solve an issue where I want to enforce a set of tag KEYS (not values) for any new resources that gets created. I want to deny any resource creation if those keys are not present. As it stands today, you may not enforce such a policy on EC2 when using CloudFormation because of a known issue where the 'RunInstancesAPI' does not include the tags specified in the CloudFormation template and subsequently prevents EC2 creation even when the correct tags are present.

From what I know from AWS support is that the best case scenario here is to use AWS Config to check tags on EXISTING resources and then have a compliance report to use for enforcement some other way. Can anyone elaborate on some other approaches here that I might have missed that doesn't involve purchasing a 3rd party tool?",2,aws,2020-10-13
itynev,How much does AWS consume regarding internet data?,With the current work from home situation. How much does AWS consume on a 8 hour shift? Living in a 3rd world country with data pricing of 40 GB = 14 USD(converted)  to be consumed for 15 days. How many megabytes does it consume per day?,0,aws,2020-10-13
ity388,Web server is slow outside load balancer,"Hello fellow AWS users

from ec2 instance:

```
$ curl &lt;some ip&gt; (web server ip inside VPC), veryyy fast
```

and then
```
$ curl &lt;some domain&gt;, sloww, mostly timeout, but the domain is to the target group for the above IP
```
any tips?

edit:

the target group is pointing to ECS instance serving raw text ""welcome to my website""",1,aws,2020-10-13
itxf62,Whitelist IP through VPN?,"I started using a VPN (NordVPN) on my personal computer. My servers (RDS &amp; EC2) are locked down to only allow SSH or MySQL access from a whitelisted IP address... obviously this is a bit hard with a VPN since it changes periodically.  


Is there a way that I can keep using my VPN but also whitelist my main machine without having to constantly change the whitelisted IP?",1,aws,2020-10-13
itv80a,Commit Digest,How to get/compute CommitDigest when committing a transaction in AWS QLDB using php?,0,aws,2020-10-13
itrmrh,Deliver files stored in S3 to user,"Currently, we are using our rest-api backend to deliver files stored in S3 to our customers. This has the advantage that it's easy to authenticate users. However, we want to optimize those files for speed.

That's why we are thinking about serving files directly over S3 instead of going through our backend. The files have random file names, so we would only give authenticated users access to these filenames using our rest api. Should we just make our bucket public or use CloudFront in front of S3?",2,aws,2020-10-13
itwd8q,Anyone else having console issues in us-east-1 right now?,Getting a lot of errors in EC2 right now.,64,aws,2020-10-13
itvd0f,Fastest and most cost efficient way to copy over an S3 bucket from another AWS account,"I have an S3 bucket that is 9TB and I want to copy it over to another AWS account.

What would  be the fastest and most cost efficient way to copy it? 

I know I can rsync them and also use S3 replication.

Rsync I think will take too long and I think be a bit pricey.

I have not played with S3 replication so I am not sure of its speed and cost.

Are there any other methods that I might not be aware of?

FYI - The source and destination buckets will be in the same region (but different accounts).",22,aws,2020-10-13
ituisc,How to track RDS minor version upgrade in code?,"I have an RDS, created using a basic Cloudformation template.  My question is, how do you keep the version defined in the template and the actual version in sync, if I enable automatic minor version upgrade?


Should I use some sort of lambda function to keep that value in an SSM parameter, and have the CF template refer to the param?    Or is it better to disable automatic upgrade and do it manually through update-stack?

Or is this simply a problem not worth solving?",2,aws,2020-10-13
itraqb,"Please, return back the shortcuts in the top panel... I like the new looks but I want the top panel back",,26,aws,2020-10-13
itpmvw,How to unencrypt an EBS volume quickly?,"We need to migrate EC2 instances from AWS to azure.

Azure migrate doesn't support encrypted disks, hence we need to decrypt the EC2 volumes.

I found the support article in AWS  [https://aws.amazon.com/premiumsupport/knowledge-center/create-unencrypted-volume-cmk/](https://aws.amazon.com/premiumsupport/knowledge-center/create-unencrypted-volume-cmk/)  where it had the steps to uncrypt the EBS volume, we need to detach the encrypted volume, connect it to a new rescue instance, then run a command ""dd"" to clone the data encrypted disk to non encrypted disk.

That's the procedure. But, the challenge is, the disk size we have is 1.5TB, it takes over 24 hours to decrypt and move the data.

Its not possible to get such long downtime.

Is there anyway to speed up the decryption/unencrypting process or an alternate way around it??",1,aws,2020-10-13
itnilc,Best way to route certain traffic in VPC through an OpenVPN client?,"I have a remote database I need to access through a VPN connection. The issue is, I find myself repeating the same OpenVPN client connection configuration for multiple EC2 and Fargate instances. 

This is entirely outside my area of expertise, so I'm not sure what the best way of solving this issue is. Would creating a dedicated EC2 instance acting as an OpenVPN client to which I can route all traffic to the database be a good approach? The database is hosted off of AWS on a customer's business network, and I'm not permitted to configure that end, so it doesn't look like a Site-to-Site VPN would help me out here.

Any advice would be appreciated.",1,aws,2020-10-13
itszqh,Making a scheduled task communicate with other tasks in an ECS Fargate service,"I have a web service built in Java running in ECS Fargate with 3 tasks that have 1 container each. All 3 of these containers cache a table from a DB in JVM heap inside the container and I need to refresh this cache periodically. Ideally, I would just move the cache outside the container to something like Elasticache and try to use that, but for reasons that are beyond my control I am limited to storing and updating the cache inside these containers for the time being. There is an http end point in the service that accepts requests to refresh this cache. So where I am at right now is trying to figure out how I can make a request to the refresh cache end point in the service periodically and have that request routed exactly once to all 3 of these containers. I have come across scheduled tasks in ECS, so in theory I can spin up that scheduled task on a specified time interval in the same service and have it make that refresh cache request to all the other tasks in the service, but I am unsure about how to make it communicate with the other tasks. I am also aware of the risk of one of the containers restarting or being replaced and having it's cache out of sync with the other containers, but I am willing to accept that risk for the time being.",1,aws,2020-10-13
itmgm1,does promoting read replica sync both with same data?,,1,aws,2020-10-13
itsdaq,Statistical simulations in the cloud?,"Dear all,  
is it possible to run simulations in the AWS cloud using Stata for a person that is not an expert with cloud computing? I am looking for a machine with 16 to 64 cores and about 8GB of RAM. What are the approximate costs if this simulation runs about 12h? What type of machine can you recommend? Thanks for some info.",0,aws,2020-10-13
itlx32,Cloud Former,"Hi Everyone,

Just wondering if anyone has had any experience using Cloud Former.

I have some Instances to be decommissioned and intend using cloud former to capture the config by creating a Cloud Formation Template of the existing resource so it could be restored in the future should need be.

If anyone has used Cloud Former or any suggestions as to how to go about this. I would really appreciate the help 

Thanks.",2,aws,2020-10-13
itr6d6,RDS Proxy &amp; Lambda function - Target groups unavailable?,"Hi!

I'm trying to use RDS Proxy with my Lambda function for a Proof of Concept and I keep getting unavailable on my RDS Proxy:

&amp;#x200B;

[VPCs and inbound rules \(outbound rules are All\/All\/0.0.0.0\/0,::\/0\)](https://preview.redd.it/vxtuph9grgn51.png?width=2466&amp;format=png&amp;auto=webp&amp;s=5d3a71cc49f3ec7544a4ecc86b1aa7ee342d6183)

&amp;#x200B;

[RDS Proxy available!](https://preview.redd.it/4hwbcg7vqgn51.png?width=3060&amp;format=png&amp;auto=webp&amp;s=165eb9f3bb17e02e5ab6d7ba2a2dbaad7349b01d)

&amp;#x200B;

[Target group unavailable!](https://preview.redd.it/c1eyvid8rgn51.png?width=2674&amp;format=png&amp;auto=webp&amp;s=34b31944abbb07cfa0bd7e0030a3f3de6af77541)

I'm afraid I don't understand where the issue is :

* Same VPC and security groups shared by VPC and RDS DB instance
* 3306 port restricted only to members of the specific security group (therefore, the EC2 instances generated by Lambda)

Is there something I'm missing?

&amp;#x200B;

Thanks in advance guys!",1,aws,2020-10-13
itpkdq,Having a bit of trouble with with the php CoudFrontClient,"I recently upgraded the version of my cloudfrontclient to the latest one using composer and now the \`path\` that Im passing into it is being broeken. Ive already asked AWS support but I have a feeling they wont be able to help in this case and theyve been slow to respond.

This is the error:

     file_get_contents(php\\keys\\pk-APKAIB7HH2KTXIJW4WKA.pem): failed to open stream: No such file or directory in /var/app/current/stutor/php/vendor/aws/aws-sdk-php/src/Aws/CloudFront
    
    PHP Warning:  openssl_sign(): supplied key param cannot be coerced into a private key in /var/app/current/stutor/php/vendor/aws/aws-sdk-php/src/Aws/CloudFront/CloudFrontClient.php on line 190
    

Locally php can find the file fine but once uploaded to AWS I get this error. I turned on error logging on in PHP to find this problem, Before it was a silent crash.

My code:

        $cloudFront = CloudFrontClient::factory([
          'private_key' =&gt; ""php/keys/pk-APKAIB7HH2KTXIJW4WKA.pem"",
          'key_pair_id' =&gt; '{IDDDDDD}'
        ]);
    
      $expiry = new DateTime('+10 minutes');
    
    
      $url = $cloudFront-&gt;getSignedUrl([
        ""url""=&gt;""https://website.net/{$video[""INTERNAL_NAME""]}"",
        ""expires""=&gt; $expiry-&gt;getTimestamp()
      ]);
    
    -- File structure
    root
     - main-file.php
     - php
       - keys
         - pk-APKAIB7HH2KTXIJW4WKA.pem
    

How can I pass in the correct path to CloudfrontClient so that it finds my file?",1,aws,2020-10-13
itnnpg,My problem with hosting,"hey guys, i wanted to get an idea as to why heroku is not good for hosting. my use cases include a website and discord bot and since i dont expect heavy traffic i dont see any problem with this.",0,aws,2020-10-13
itmr9k,Aws cli command,"Hello aws members,

Would anyone know how to change aws access key id on the  aws cli command prompt using windows 10?",0,aws,2020-10-13
itmayo,I am sorry this is a dumb question,"I don’t know if this is the right place but I am currently looking into server options that allow for two python scripts (one of them involves tensorflow and stable_baselines) a pkl file and can maintain a connection (this connection is setup with a daemon thread) to an outside client application. 
The server needs receive info from the client application, use the model from the pkl file to decide on an action, and then send the action back to the client app. This needs to happen every minute for 5 days a week. I just want to know how much this can cost and if it is viable before I start learning how to do it.",0,aws,2020-10-13
itl66b,API Gateway Transforms,"So I created a Web API in ASP .net core using SAM templates, I am trying to transform JSON requests to a string. Usually I am able to achive this in API gateway when selecting the methods and intergration request, but it seems that since I am using an **ANY** Method with a greedy modifier **{Proxy+}** allowing my the ASP .net application handle the routing I do not have an option for body mapping templates to transform JSON to string in the console, and it seems it just passed through instead.

So usually this is not an issue when I create single lambdas with a (POST, GET) method, so I am confused as to why using the ANY method is not allowing me any type of mapping templates in the api gateway ( I do not have the option in the console in API Gateway). 

*Is there any solutions to string JSON in API gateway using the method* ***ANY*** *and greedy modifier ?* 

*I am using SAM templates, after searching online, I seen using swagger definations along with SAM templates may achieve what I am looking to do, I do not have much experience with swagger but if anyone does, does this sould correct?*",1,aws,2020-10-13
itkhk9,Are there any intentions of lowering the price of EBS volumes?,"An honest question, not complaining or anything, just wondering if it's on the ""roadmap"".

[2014 vs 2020 pricing](https://preview.redd.it/xblvi08ween51.jpg?width=595&amp;format=pjpg&amp;auto=webp&amp;s=44d97859d0d8f393c076e95255b29a2748023cd2)

A good article talking regarding the pricing of SSD drives over here years - [https://www.minitool.com/news/ssd-prices-fall.html](https://www.minitool.com/news/ssd-prices-fall.html) (not mine, just a good ref).

**Sources**

2014-Jun [https://aws.amazon.com/blogs/aws/new-ssd-backed-elastic-block-storage/](https://aws.amazon.com/blogs/aws/new-ssd-backed-elastic-block-storage/)

2020-Sep (Updated constantly) [https://aws.amazon.com/ebs/pricing/](https://aws.amazon.com/ebs/pricing/)",7,aws,2020-10-13
itjt4j,Security September: Racing against CloudWatch Synthetics Canaries – One Cloud Please,,8,aws,2020-10-13
iti4mj,Unable to access MongoDB Atlas Database from AWS BeanStalk,"  I made a (very) simple  NodeJS (Express based) app that connects with MongoDB Atlas database.

This app works perfectly fine on my local computer and I am able to see inserted records in the cloud GUI of MongoDB Atlas.

* But when I deploy it to AWS Elastic Beanstalk, the app is not working ( I am using a AWS CI/CD pipeline connected to a github repo). I pulled the logs and it is as follows ( NOTE: I am only printing wed.stdout.log):

&amp;#8203;

    ----------------------------------------
    /var/log/web.stdout.log
    ----------------------------------------
    Sep 15 21:12:50 ip-172-31-35-31 web: &gt; Elastic-Beanstalk-Sample-App@0.0.1 start /var/app/current
    Sep 15 21:12:50 ip-172-31-35-31 web: &gt; node app.js
    Sep 15 21:12:50 ip-172-31-35-31 web: Server running at http://127.0.0.1:8080/
    Sep 15 21:16:14 ip-172-31-35-31 web: &gt; docker-node-mongo@1.0.0 start /var/app/current
    Sep 15 21:16:14 ip-172-31-35-31 web: &gt; node index.js
    Sep 15 21:16:15 ip-172-31-35-31 web: Debugging (personal note): process.env.DATABASE_PWD is undefined in this environment
    Sep 15 21:16:15 ip-172-31-35-31 web: Server running at  3000
    Sep 15 21:16:15 ip-172-31-35-31 web: MongoDB connection error message:  connection 3 to bsrcluster1-shard-00-01.njwnt.mongodb.net:27017 closed
    Sep 15 21:16:15 ip-172-31-35-31 web: MongoNetworkError: connection 3 to bsrcluster1-shard-00-01.njwnt.mongodb.net:27017 closed
    Sep 15 21:16:15 ip-172-31-35-31 web: at TLSSocket.&lt;anonymous&gt; (/var/app/current/node_modules/mongodb-core/lib/connection/connection.js:275:9)
    Sep 15 21:16:15 ip-172-31-35-31 web: at Object.onceWrapper (events.js:422:26)
    Sep 15 21:16:15 ip-172-31-35-31 web: at TLSSocket.emit (events.js:327:22)
    Sep 15 21:16:15 ip-172-31-35-31 web: at net.js:674:12
    Sep 15 21:16:15 ip-172-31-35-31 web: at TCP.done (_tls_wrap.js:567:7) {
    Sep 15 21:16:15 ip-172-31-35-31 web: errorLabels: [ 'TransientTransactionError' ],
    Sep 15 21:16:15 ip-172-31-35-31 web: [Symbol(mongoErrorContextSymbol)]: {}
    Sep 15 21:16:15 ip-172-31-35-31 web: }
    
    

 I think it probably has something to do with Whitelisting IP address in Atlas, assigned an Elastic IP address to the EC2 instance associated with this beanstalk environment and then whitelisted this address inside MongoDB Atlas. But this somehow causes the health of the instance to deteriorate to a ""SEVERE"" state.

I have been sitting with this for the past 6 hours and am pulling my hairs out. Someone, please help me.  

P.S. I am a college student just starting out with exploring AWS. So, pls excuse me if I overlooking something trivial.",1,aws,2020-10-13
itisoy,Is Amazon S3 Considered a Managed Service?,"Looking to host landing pages using  S3. In this case, is S3 considered a managed serivce?

Thanks all!",3,aws,2020-10-13
itimpn,How can we restrict API Gateway Ciphers to PFS only?,"When creating API Gateway Custom domain. It does allow us to choose between different TLS versions, but even version 1.2 currently supports many ciphers without Perfect Forward Secrecy.

For compliant reasons, we need to restrict it to PFS ciphers only. While there are ways to achieve it by putting HAProxy or something in front of it, but its a stretch for us.

Is there direct way to use custom ciphers?",1,aws,2020-10-13
itimlj,What is the fastest way to migrate 14 million dynamodb records?,"We need to migrate around 14 million dynamodb records from one table to another.  Both of these tables are global tables with on-demand provisioning enabled.  A requirement is it has to be completed in under 6 hours.   


The current approach we are taking is to create a dynamodb stream, segment scan the source table via lambda functions which modify every record in the source table, setup a lambda to process the stream events to move the records to the sink table.  


We managed to migrate 1.4 million records in about 7 minutes, so our solution seems to work ok...  we are trying with 14 million records soon.  Is there an easier approach than to engineer all of this?",6,aws,2020-10-13
ite4im,"You can now share your Amazon CloudWatch dashboards outside the AWS console - by username/password, SSO and public URLs. Dark mode by default, can be embedded in other websites. With extremely long login/sessions timeouts perfect for TV dashboards.",,84,aws,2020-10-13
ithz5w,Amazon CloudFront announces support for Brotli compression,,101,aws,2020-10-13
ith3b3,eDiscovery tools on AWS?,"Hi, 

 
Are there any tools that can be used for a law firm's ediscovery (OCR, indexing, searching, tagging..etc)?",1,aws,2020-10-13
itgjek,How is RDS billed in this case?,"Hello, I feel stupid asking this but here it goes.

What I want to use:

* RDS for MySQL
* db.t3.micro (Single-AZ)
* Cost:  $0.017 per hour

From aws' pricing page:

&gt;For both Single-AZ and Multi-AZ deployments, pricing is per DB instance-hour consumed, from the time a DB instance is launched until it is stopped or deleted. Partial DB instance-hours are billed in one-second increments with a 10 minute minimum charge following a billable status change such as creating, starting, or modifying the DB instance class. 

Let' say we are at the start of the day and month, at 00:01 (HH:MM).  


My question is:

Let's say every 30 seconds a select and an update request are executed (1 row 1 table), let's say these request take a total of 10ms to complete, how much do I get charged in one day? Like, do I get charged in one-second increments or everytime someone performs a request I get charged the 10 minute minimum.

To make the maths easier, it's 4 requests per minute, 240 an hour, 5760 a day.",1,aws,2020-10-13
itgcy5,What condition keys would allow multiple values for SNS topics to enable cross account publishing?,NOT looking for any suggestion on using a principal OrgId for the condition. Looking specifically for a condition key whether its SourceArn or something else that would allow multiple values (arns) in a list for the values or any kind of atypical condition key that would allow storing the values in a list in a one-to-many kind of option.,1,aws,2020-10-13
itffhz,Difficulty pointing domains correctly (Lightsail),"I'm pretty noob (a recipe-follower) at this, please be gentle.

I am running a single Lightsail instance with the Bitnami LAMP stack. I have two domains being managed under Lightsail. Let's call them elephant.tld and aardvark.tld. Both point to the static IP address of my Lightsail instance.

I have a basic placeholder website currently accessible at elephant.tld. That website ""lives"" in the default htdocs folder.  

I also have a subdomain, baby.elephant.tld that points to a Wordpress blog in a Bitnami apps folder apps/baby/htdocs/ with a VirtualHost setup. The VirtualHost directs baby.elephant.tld to the correct folder. That part works well.

I am also trying to set things up so that aardvark.tld displays a Wordpress blog hosted in apps/aardvark/htdocs/ but no matter how I set up the VirtualHost, it doesn't work. Instead I see same basic placeholder website that I see when I visit elephant.tld.

For some reason -- the VirtualHost isn't properly directing the request. I can't figure out why.

I reached out to Bitnami Support about this -- but after much back-and-forth, they are stumped, are insinuating that there is a problem with the Lightsail setup.  

Anyone know a solution or a workaround?",1,aws,2020-10-13
itdcq4,Did AWS Amplify change their logging ?,"Hi,

Up until yesterday, when we pushed our apps to AWS Amplify (They work on GatsbyJS), the Deploy step of the deployment gave us a log that looked like this :

    2020-09-14T02:03:43 [INFO]: Starting Deployment
    2020-09-14T02:03:43 [INFO]: Skipping upload of existing file 404.html
    2020-09-14T02:03:43 [INFO]: Skipping upload of existing file about-us/index.html
    2020-09-14T02:03:43 [INFO]: Uploading airports-games/index.html
    2020-09-14T02:03:43 [INFO]: Uploading app-5585718ca69dc9efc672.js
    2020-09-14T02:03:43 [INFO]: Uploading app-5585718ca69dc9efc672.js.map
    2020-09-14T02:03:43 [INFO]: Uploading aws-faq/index.html
    2020-09-14T02:03:44 [INFO]: Skipping upload of existing file baba-operations/index.html
    2020-09-14T02:03:44 [INFO]: Uploading baba-juice/index.html
    2020-09-14T02:03:44 [INFO]: Skipping upload of existing file baba-alto/index.html
    2020-09-14T02:03:44 [INFO]: Uploading baba-platform/index.html
    2020-09-14T02:03:44 [INFO]: Uploading baba-milk/index.html
    2020-09-14T02:03:44 [INFO]: Skipping upload of existing file baba/index.html
    2020-09-14T02:03:44 [INFO]: Uploading banking-games/index.html
    2020-09-14T02:03:44 [INFO]: Uploading blog/some-blog-post-1/index.html
    2020-09-14T02:03:44 [INFO]: Uploading blog/some-blog-post-2/index.html
    2020-09-14T02:03:44 [INFO]: Uploading blog/some-blog-post-3/index.html
    2020-09-14T02:03:45 [INFO]: Uploading blog/some-blog-post-4/index.html
    2020-09-14T02:03:45 [INFO]: Uploading blog/some-blog-post-5/index.html
    2020-09-14T02:03:45 [INFO]: Uploading blog/some-blog-post-6/index.html
    2020-09-14T02:03:45 [INFO]: Uploading blog/some-blog-post-7/index.html
    2020-09-14T02:03:45 [INFO]: Uploading blog/some-blog-post-8/index.html
    ...
    ...
    ...
    2020-09-14T02:08:38 [INFO]: Updating Edge config
    2020-09-14T02:08:39 [INFO]: Deployment finished.

...and was some 7300 lines long.

Now, since yesterday evening, the whole Deploy log is something like the following, which is 6 lines long :

    2020-09-15T17:06:18 [INFO]: Starting Deployment
    2020-09-15T17:06:19 [INFO]: Updating Edge config
    uildId 0000000098
    2020-09-15T17:07:25 [INFO]: Got archive: 510986621 bytes
    2020-09-15T17:07:26 [INFO]: Archive unzipped: 7425 files
    2020-09-15T17:08:08 [INFO]: Deployment complete

It still seems to be pushing the new content, SOMETIMES. Sometimes it doesn't, and we get no error, no warning, nothing out of the ordinary. The app seems to just not deploy.

If it were only one app being affected, I'd blame it on our own code or something, but it seems to be affecting all our apps, including some that are deployed from repos we haven't modified in months !

Does somebody know if AWS changed something in their process ? Did we miss a memo ?

Thank you !",2,aws,2020-10-13
iteso8,Amazon Transcribe Now Supports Automatic Language Identification,,45,aws,2020-10-13
itciui,Migrating public S3 bucket data (either to another S3 or Azure) faster,"We're needing to migrate some 10TB of data out of a public S3 bucket following a service agreement cancellation. AWS CLI copy using the ""--no-sign-request"" is currently running but will take several weeks, which is not ideal. I've looked into Azcopy and Azure Data factory to get the data into an Azure Storage account, however both of these seem to require AWS access keys with permissions to the source S3 bucket, which the vendor will not provide since it's a shared bucket.

Is there any faster way to copy all data from a public S3 bucket to either another S3 bucket or Azure Storage that doesn't require authentication on the source S3 bucket?",0,aws,2020-10-13
itb1y1,Fargate Private IP discovery in Cloud Formation?,"I am building a Fargate stack with a database container and a UI container. The UI container needs to reference the internal IP of the database.  I haven't been able to find a method using cloud formation to retrieve the IP of the database container for reference in the UI environment variables.  I am working on integrating a load balancer that may solve this problem, but I think it would be better to directly reference the IP and eliminate the need for the load balancer in this case.  I have found some suggestions using the cli and grep to retrieve the IP, but this is outside of the cloud formation scripting.  Any suggestions on how this can be done?",1,aws,2020-10-13
it9vyx,TGW to TGW peering,"I need to connect 2 DX (10Gbps) of different account in a same region to multiple VPCs. Best Option I thought was of connecting TGW-TGW but TGW peering doesn't work in SAME REGION. 

NEED HELP!! ASAP

Both account VPCs should be interconnected, let's say we have 5 VPCs (increases by time) in each account with other services.",2,aws,2020-10-13
it9n8r,Student in need of help with VPC,"hello, so while practicing vpc last night i accidentally deleted my default internet gateway. I don't even remember detaching it from my vpc. so can anyone help me out? I'm not able to connect to my ec2 &amp; practice.",10,aws,2020-10-13
it99nc,AWS CDK and terraform &amp; cloudformation,"anyone know if you can take an existing AWS CDK and convert it to terraform CDK? I want to try out terraform and i have some AWS CDK. 

thanks.",6,aws,2020-10-13
it8ete,"[Glacier] Restoring a 130GB file from Glacier, using MultipartCopy to transfer back to standard returns an ""internal error"" after a while?","There is a file sitting in S3 under Glacier. We trigger a RestoreObject on this key. Once the Restore has completed, we trigger a MultipartCopy back to the Standard class - as the Restore will only remain for 2 days.

However - it seems like many large files are hitting errors during this MultipartCopy:

&gt; An exception occurred while uploading parts to a multipart upload. The following parts had errors:
- Part 6289: Error executing ""UploadPartCopy"" on ""https://mybucket.s3.amazonaws.com/projects/ID/source-files/ID/master/ID.mov?partNumber=6289&amp;uploadId=uploadIdToken"";

&gt; AWS HTTP error: Server error: ""PUT https://mybucket.s3.amazonaws.com/projects/ID/source-files/ID/master/ID.mov?partNumber=6289&amp;uploadId=uploadIdToken"" resulted in a `500 Internal Server Error` response:

&gt; &lt;Error&gt;&lt;Code&gt;InternalError&lt;/Code&gt;&lt;Message&gt;**We encountered an internal error. Please try again.**&lt;/Message&gt;

Is there anything obvious here?",11,aws,2020-10-13
it80r5,SES Not Delivering to outlook.com email,Anyone else having issues with Amazon SES not delivering emails to [outlook.com](https://outlook.com) mailboxes?  I just set the service up and cannot get emails to deliver to my two [outlook.com](https://outlook.com) email accounts.  Other accounts seems to be working.  SPF is passing.,1,aws,2020-10-13
it7fdu,"To ""lift-and-shift"" or not to ""lift-and-shift""?","Under the very high pressure of time constraints (migration out of data centers by mid-2021) a lot of application owners are engaging consultants specialized in their specific areas, who - of course - offer almost exclusively ""lift-and-shift"" solutions, i.e. acquire ""x"" EC2/AMI machines of sizes ""a"", ""b"", ..., databases (PostegreSQL, Oracle, SQL) of other machine sizes, and ""connect them together"" identically to how they look on-prem. The only area where I was able to ""insert"" something more cloud-native, was in the replacement of very crude LB functions, presently conducted on F5s, which could be easily addressed via simple ELB configurations.

Question: while an approach of ""like-to-like"" seems logical, is there anything in the world of tooling, for AWS, which would allow an analysis of an app architecture and/or data flow, with ability to recommend maybe some AWS native solutions, where the traditional server/DB/storage/connectivity package is suboptimal for that specific usage?

*Edit*: the above is an *infrastructure* view (bottom-up) and attempt to ""untangle"" the apps by leveraging a ""AWS service validation"" tool, if any such, rather than the ability to completely understand the intricacies of 100+ apps assumed to migrate in one year. Example: having leveraged AWS discovery agent, in association with some creative Athena queries, I was able to ""manually"" reverse engineer some on-prem apps, to determine connectivity in/out the environment, for example, but this is more of a validation attempt, than a scalable solution.",18,aws,2020-10-13
it7lmm,Wondering about S3 supplements/alternatives for the high bandwidth website,"Hi guys! I've recently watched a lecture on the differences between geo.distributed and centralized storage models ([https://youtu.be/TOnZ78ay3rs](https://youtu.be/TOnZ78ay3rs)). My personal project to create and deliver a 4k resolution stock video through the private subscription.  I am with S3 for 4+years now, but the transfer costs are killing me.

1. I was wondering if anyone used geographically distributed storage for the heavy files storage and delivery as a replacement or alongside AWS S3?
2. Talking about geo-distributed storage services, any recommendations? I've spent hours searching and found only two so far: JUCE ([https://juce.cloud/](https://juce.cloud/)) and Eco4cloud ([http://www.eco4cloud.com/](http://www.eco4cloud.com/)).
3. The first one offers 20tb storage and unlimited transfer for €1800 per month. It would cut my expenses more than 5 times, but I couldn't find any reviews on them online, and this kind of puts me off.  Do you think it worth trying out?

If you have any other suggestions/solutions not related to geo-distributed storage - I'd be more than happy to hear them. The main idea is to cut the transfer-related costs while delivering 4k videos to the USA and Asia (this is where most of my clients are).",3,aws,2020-10-13
it7u86,Lambda@edge returns JS as content-type: text/html,"I have a website (Django site) deployed with Lambda and CloudFront and it works fine with no problems. I decided to try Lambda@edge with origin request set and it deployed without errors but when I visit it my site it will load all my JS except one script.

[https://xxxx.cloudfront.net/js/main.js](https://xxxx.cloudfront.net/js/main.js)

**content-type: text/html**

# 502 ERROR

## The request could not be satisfied.

The Lambda function returned invalid json: The json output must be an object type.",2,aws,2020-10-13
it6rmw,"Starting stopped Gs.xlarge instance gives limits error, but allows me to start it.","We have one instance that we're working on and today I've got this error message.

But I was allowed to start it.

What does this mean actually?

&amp;#x200B;

https://preview.redd.it/10i7hd9span51.png?width=852&amp;format=png&amp;auto=webp&amp;s=a8d405688d7a5b8b07802e02f7c7be8da909b63b",2,aws,2020-10-13
it6fe9,Centrally enabling AWS Config across all accounts/regions with Organizations?,"We have a situation where we can't use the Organizations integration for AWS config, but want to easily enable it across a number of accounts and all of the regions in those accounts. Is there any easy/preferred way to do this with Terraform? The thought of having X number of providers for each region multiplied accounts seems completely out of hand.

We don't use much Cloudformation, but I'd consider it for the better stack support across regions/accounts.

Is there a good CLI example? Also, is it simply just creating an S3 bucket in a central account, granting the permissions to to the other accounts and then enabling the recorders from there?",3,aws,2020-10-13
it5opb,Two AWS organizations. What's the best way to move all the resources from one to another?,"Due to financial reasons we are now in a position where all AWS resources will need to be moved from one AWS master organization to another.

What's the best way to do it?",1,aws,2020-10-13
it5tcp,What is the preferred way to connect to RDS for manual editing?,"I am still kinda new to AWS and I understand there are many different ways to achieve certain goals...

I want to connect to a RDS MariaDB to manually edit some fields. What solution is the most elegant and does not cost much?

- open the port to the internet to access it from my computer's tools - does not seem to be best for security. Also, I don't have a static IP, so I would need to change it all the time or open to the internet which is not preferred.
- create a VPN to access it using my computer's tools
- in EC2 run phpmyadmin
- in ECS run phpmyadmin-Docker (haven't tried, but sounds easy)

Have I overlooked any other good solution? Is there is no built-in AWS tool for this, is there?

I don't want to spend a fortune as I will most likely be doing this not very frequently - like once a month. So with EC2 / ECS I would terminate the instances afterward...",8,aws,2020-10-13
it1lzo,Bitnami no longer free tier eligible?,"Hi everyone I've been looking all over the internet but can't seem to find any information on this. 

I currently host a couple of websites on multiple bitnami wordpress EC2 instances but just found out today that the bitnami wordpress stacks are no longer eligible for the free t2micro tier. 

Does anyone know why it is no longer eligible for the free tier? Did Bitnami make some changes that would invalidate the eligibility?",0,aws,2020-10-13
it0xtz,Volume Discount,I am wondering if there is a specific volume discount for using all the products in the AWS Elemental product line. On the website it states that for Data Transfer and Amazon S3 there exists volume pricing. Does this mean there is no benefit for using all the products in that specific line?,1,aws,2020-10-13
it5k8t,Best options for cloud storage outside AWS instances,"I have setup a virtual machine outside of AWS but I still have lots of AWS credits and am currently short on storage for my machine I was wondering which of AWS cloud storage options would work best for me. 

I am looking to store a variety of programmes and files, does anyone have any idea how I could go about doing this.",1,aws,2020-10-13
it3xye,Restoring large MSSQL backup file in Fargate,"Hi all,

I'm trying to figure out a clever cloud-based way to restore a mssql .bak file for a one-time use to extract some data out. The data would be saved in s3 as json for future use. The issue is that these .bak files can exceed 17gb. Initially I tried running a mssql server in a docker container, and another container restoring the file and querying it. This has proven pretty difficult however, as when the file is restored, the image is very large (17gb).

I thought Fargate would be a good option, but I'm unsure about the size limitations for the container and shared volume between the two containers. A bit of research revealed EFS can be used to help with this, but I have little experience with it and am pretty hesitant to dive into testing it's use with these larger files as cost is a concern. 

The other option would be to restore the file with RDS, which can directly reference a S3 bucket. The issue however is that the 17gb file would have to be uploaded, and therefore would need to be compressed. From what I've found so far, RDS can't extract zips as part of the restore procedure.

I know an ec2 server would work as well, either starting it up and shutting it down via lambda or leaving it running 24/7. This is my least favourite option, but can be always be a fallback if I'm out of luck.

I'd be super grateful for advice or direction with this.

Cheers!",1,aws,2020-10-13
it3s90,"Metric filter terms that include characters other than alphanumeric or underscore must be placed inside double quotes ("""").","I'm trying to create metric filter on a aws cloudwatch log group with pattern: ?fatal ?Fatal

But I get this errors: Metric filter terms that include characters other than alphanumeric or underscore must be placed inside double quotes ("""").

I was created the metric before and it created successfully but recently I want to edit the pattern but aws don't allow me to do that and echoed above error.

&amp;#x200B;

If I put pattern in """" as: ""?fatal"" ""?Fatal"" 

and try to test on some sample, it gives zero result even the text contains fatal for example.

&amp;#x200B;

I gged and there's no solution for this, anyone has this issue?",2,aws,2020-10-13
it26z2,Latency comparison. RDS or DynamoDB,"I have an OLTP system and I was wondering if I'll get lower latency with RDS or DynamoDB. The application is designed to be realtime so I am hoping for ""instant feedback"". The system will have only a few hundred users. The queries are quite predictable hence why the openness to dynamodb.  


I'm specifically wondering the latency performance.",2,aws,2020-10-13
isz4qd,How to build an api key service?,"I have a service foo that I deployed on eks. It uses mtls with certs from our own CA. We started to acquire other small companies and their workloads are running in a mixture of gcp/azure/aws. For a long list of reasons moving our CA and cert bits into their environments isn’t feasible. I want to expose my service’s apis but I don’t want everyone on the internet to use them. 

I was thinking of building an api key service. Our acquisitions can login to that service. Create a service account and then generate an api key. Then they can distribute that key and send it along with their requests. Foo would only accept requests with a valid api key. 


I’m looking for some help in designing this api key service, preferably using as many aws components as I can. Thanks!",2,aws,2020-10-13
isyenp,Any tips to having two AWS accounts (for the same project) and not paying double for Business support?,"I have two accounts for our business (same app/service), one is v1 of our product and one is v2. I don't think it makes much sense to pay $100/mo twice. Any advice here? Will AWS support allow me to use one account, and reference the other account # in my support tickets?",1,aws,2020-10-13
isy2u3,AWS Amplify SSL Configuration,"I deployed my app using AWS Amplify and it works. I'm trying to use my custom domain which I've purchased through Route53. Using domain management, Amplify gets stuck at the SSL Configuration stage with the message listed in the screenshot below.

https://preview.redd.it/zrv6hmbsj7n51.png?width=1166&amp;format=png&amp;auto=webp&amp;s=fad5244ff17b238ea2cc3792d3f6996c27367fa1

&amp;#x200B;

When I go over to Route53, I see that Amplify already configured the following CNAME records with my domain.

&amp;#x200B;

https://preview.redd.it/z75hj1p2k7n51.png?width=2431&amp;format=png&amp;auto=webp&amp;s=964d11195dd2cd32d1517c616f438ca61d543690

Previously, I hosted my app on an s3 bucket and went the manual steps of using ACM but would get stuck at this point as well. 

How am I able to fix this issue?",1,aws,2020-10-13
isw3ph,Amazon CloudWatch now monitors Prometheus metrics from Container environments,,3,aws,2020-10-13
iswuor,"Best way to extract text from small, but regular batches of PDF files.","Hi,

I am primarily a JS developer, but I want to use AWS Textract to take a file uploaded to s3, parse it and return the result to a web app. Most examples I find seem to only work with images.

I cannot seem to find any clear examples where I can take a PDF file hosted on s3 and parse it using Node (I cant seem to find a lambda one which reads PDF either but Node is preferable).

I did come across this cloud formation, but when I ran it the s3 uploads wouldn't trigger the lambda function which parses the file. [CDK Link](https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing)

The ultimate solution for this I want would need to handle a few hundred files every Friday afternoon; but I want it built so it can scale; so s3 -&gt; sns -&gt; lambda &gt; sns seems to be the pattern – I just cant find recent examples/tutorials.

Anyone had any experience with this? I feel like I have been going in circles.",2,aws,2020-10-13
iswozm,Managing a Largish? Number of Accounts,"I just inherited an env with \~30 accounts which is considerably more than I am used to (my old env we had 2, prod/non-prod).   There is SSO setup but I've had some issues getting sts to work correctly.  My colleague shared [https://github.com/99designs/aws-vault](https://github.com/99designs/aws-vault) which is looking promising and does seem to works withs sts.....but before I go too far down one path I want to find out what others do and if there is some obvious way to do this I'm missing.  My initial requirements are just for me (full admin access) to manage the accounts and resources in them using terraform and python.",6,aws,2020-10-13
isvuym,New EC2 T4g Instances – Burstable Performance Powered by AWS Graviton2 – Try Them for Free | Amazon Web Services,,117,aws,2020-10-13
isulkr,Seeing t4g instances going live,"Seeing them available for spin up in my accounts. The pricing information is also live in the usual places, but no formal announcement as at the time of this post.",17,aws,2020-10-13
istvzz,AWS Account to Account Migration,I am asking the reddit users out there if they have used any particular vendor to do AWS Account to Account Migration. Assume that the volumes are encrypted on the source account. Is it possible to use any particular commerical software out there to do this type of migration? thanks in advance.,2,aws,2020-10-13
istv97,"Visit Serverlessland.com: Learn to use and build apps that scale automatically on low-cost, fully-managed serverless architecture.",,13,aws,2020-10-13
ista3o,ELB: Classic TCP or Network TCP to kube-apiserver: OutOfService?,"I've setup a terraform / ansible kubernetes cluster using the first controller as the ELB up till now.  
Now I want to add the ELB to the setup, but no matter how I set it up, through terraform or the web UI, in classic tcp ELB or tcp NLB mode: All the instances end up in OutOfService and I can't curl the :6443 port from the LB like I can from the 3 controllers directly.

Is there anything obvious that I'm missing?",2,aws,2020-10-13
ist2v0,"EKS &amp; encryption, what is recommended? Is EKS volume encryption good enough?","I am not super familiar with EKS itself, but I was asked to give a recommendation about what level of encryption is appropriate for it. This cluster has a DB in it, but they would prefer not to encrypt it if the EKS volume encryption is good enough.

I would think that the DB itself would need to be encrypted even if the cluster itself is. Does anyone have any experience with this kind of use case?

Edit - Sorry, meant is EBS volume encryption good enough.",4,aws,2020-10-13
isrgmn,Can't access billing page,"I made a personal AWS account for learning's sake. I have one group (admin)  with one user (myself), which has admin permissions. The policies attached to this group are Billing and AdministratorAccess. I tried to access the billing service, and it says I don't have permissions. I followed the guide that the page said to follow in order to view the billing page: [delegating access to billing console](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_billing.html?icmpid=docs_iam_console#tutorial-billing-step1)\_, and I still can't access the billing console.

When I tried to search for tech support, most of what I found seemed to be AWS trying to sell services to help me. Seriously, wtf AWS. It's like mission impossible 3 trying to get someone on the phone to help me with this shit.  


This policy is attached to the admin group, which my user is a part of:  

```
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""aws-portal:*Billing"",
        ""aws-portal:*Usage"",
        ""aws-portal:*PaymentMethods"",
        ""budgets:ViewBudget"",
        ""budgets:ModifyBudget"",
        ""cur:*"",
        ""purchase-orders:*PurchaseOrders""
      ],
      ""Resource"": ""*""
    }
  ]
}
```

EDIT:
Solved! Thanks to `/u/frgiaws`. If you're having this problem as well, follow [this guide](https://aws.amazon.com/premiumsupport/knowledge-center/iam-billing-access/)",0,aws,2020-10-13
isqt0s,"AWS + Laravel Module + Protect 1 module from outside, while allowing access to another module","I have 1 server which uses Laravel for the php web application.  I make use of Laravel modules.

[example.com](https://example.com)\\module1

[example.com](https://example.com)\\module2

[example.com](https://example.com)\\module3

Each URL points to the same server in AWS.  Module 2 needs to be open to the entire internet.  Module 1 and Module 3 must be protected from the outside.  Is there a way to do this?  I'm open to anything; it could be an existing AWS service or not.",1,aws,2020-10-13
isrbo2,Not sure where to start on AWS? Check out these 25 services. (Getting started and video tutorial links included),"See a service you’re interested in learning about? Click the link below for the links to getting started: [Getting started with AWS services](https://www.allcode.com/top-aws-services/)  


Did I miss any? Let me know and I will add them to the list with links to getting started. Thanks for your help!

&amp;#x200B;

1. Amazon EC2 (Elastic Compute Cloud)
2. Amazon RDS (Relational Database Services)
3. Amazon S3 (Simple Storage Service)
4. Amazon Lambda
5. Amazon CloudFront
6. Amazon Glacier
7. Amazon SNS (Simple Notification Service)
8. Amazon EBS (Elastic Block Store)
9. Amazon VPC (Virtual Private Cloud)
10. Amazon Kinesis
11. Amazon Auto-scaling
12. Amazon IAM (Identity and Access Management)
13. Amazon SQS (Simple Queue Service)
14. Amazon Elastic Beanstalk
15. Dynamo DB
16. Amazon ElastiCache
17. Amazon Redshift
18. Amazon Sagemaker
19. Amazon Lightsail
20. Amazon EFS (Elastic File System)
21. Amazon Cloudwatch
22. Amazon Chime
23. Amazon Cloud Directory
24. Amazon Cognito
25. Amazon Inspector",59,aws,2020-10-13
isr0en,AWS WAF Classic rate-based rules on errors?,"Is there any way to restrict a WAF Classic rate-based rule on errors? For example, I want to restrict normal traffic to 1000 req/5min but stop traffic that returns errors (e.g. scrapers) after 100 errors in a 5min window.

Is there any way for WAF to become aware of the resulting return code from CF? Thanks!",1,aws,2020-10-13
iso0hj,Dynamic Resources for Lambda/SQS Service,"We've got a service that streams tags from EC2 instances to it's child resources (ENI/EBS/Snaps) and currently we've got this all being done via an ""audit"" script that finds any tags that need to be copied, this feeds into an SQS queue which then kicks off another ""process"" Lambda function. It's all very config driven and we're making it more and more flexible (i.e. tagging other services)

The problem is that it can take quite a while to run because of concurrency issues and also running operations against different regions (i.e. Asia).

So, I had an idea to essentially have an ""execute"" function that would take the config and then create the Lambdas/SQS in each region, in each account, and then when all the processing was done, would self delete themselves (this step is ideal but not necessarily mandatory). The idea behind splitting it out just means that you'd have less processing running in each region so much less risk of concurrency issues. The actual cost of this service has been about 50 cents per month, go serverless!!!!

Does this sound idea sound OTT? I've not done much with Step Functions as yet so thinking I might see if this would be an ideal tool to orchestrate the components in each account and then manage the deletion, not sure though if this is actually feasible inside SF.

Any feedback more than welcome :)",2,aws,2020-10-13
isnqsm,freaking aws ec2 .. what a poj,"Useful sometimes, but really? When you choose to give your centos disks 100gb, and they only reflect 15 gb on instantiation .. that just sucks. wth.",0,aws,2020-10-13
isnpee,The best serverless database combo on AWS,,1,aws,2020-10-13
isn3g0,Can I get a clarification regarding WorkDocs?,"Our business is currently a subscriber to AWS for Cloud Computing services and now I'm looking into WorkDocs as a document repository/collaboration space.  

It looks interesting, but can somebody tell me if I'm misunderstanding?

It sounds like, to migrate data from our current internal network into AWS WorkDocs, I would need to use both DataSync and S3.  Is that correct?  I'm a little confused on the distinction.

Do you require DataSync to put the data into S3, which then places the data into WorkDocs, or do I just need DataSync and I don't need the S3 Bucket?

Any info that anyone can provide will be useful I'm sure, I'm pretty new to AWS.

Thank you!",2,aws,2020-10-13
ism5ye,Advice for the new DevOps Professional exam?,"I've passed the DevOps Professional exam previously, and will re-certify it in the coming week or two. What subject areas are emphasized on the newer exam? Aside from new services, any substantial differences?",5,aws,2020-10-13
islzhd,Load balancer - For services that are required both external and internal,"Does it cost more to use a Public Application Load balancer for a service that is needed both externally and anternally, how does aws route the traffic for internal services if we go that route?

Would it be less/more costly and or secure to use  an additional internal load balancer for stuff that needs to get to it internally?",15,aws,2020-10-13
iskqbj,Forced to use ugly URL with Elasticsearch Service and Cognito?,"I'd like to use Cognito with Elasticsearch to create a cluster which exposes Kibana to public users that authenticate via Google.  Got it all working only to discover that there's no way change the callback URL that's passed to Cognito during auth flow from the ugly https://search-domain-name-identifier.region.es.amazonaws.com  to a cleaner name managed by Route53.  

Anyone know of any workarounds here?",3,aws,2020-10-13
isls8o,Week of Sept 14th - What are your favorite container/serverless tips in AWS?,Share your container/serverless tips,14,aws,2020-10-13
isllrp,Polly Lexicon tag for case sensitivity?,"Is there a tag I can use that removes case sensitivity for graphemes in a Polly XML lexicon? Polly doesn't seem to like:

    &lt;lexicon version=""1.0"" blah blah blah alphabet=""ipa"" xml:lang=""en-US"" case-sensitive=""false""&gt;

(case-sensitive is a tag from IBM TTS engine PLS XML specification)

In the following sentences, Matthew (en-US) reads ""Woods"" and ""Hills"" as  wiɑdz and  hilz :

&gt;I suppose, now I think about it, to hear him once again would be a good reason to venture into the Damos Woods. I traveled quick and careful until I got myself free of the Damos Hills.

if I say:

      &lt;lexeme&gt;
        &lt;grapheme&gt;Damos&lt;/grapheme&gt;
        &lt;phoneme&gt;deɪmoʊs&lt;/phoneme&gt;
      &lt;/lexeme&gt;  
      &lt;lexeme&gt;
        &lt;grapheme&gt;hills&lt;/grapheme&gt;
        &lt;phoneme&gt;hɪlz&lt;/phoneme&gt;
      &lt;/lexeme&gt;
      &lt;lexeme&gt;
        &lt;grapheme&gt;woods&lt;/grapheme&gt;
        &lt;phoneme&gt;wʊdz&lt;/phoneme&gt;
      &lt;/lexeme&gt;

the problem isn't corrected. I have to say:

      &lt;lexeme&gt;
        &lt;grapheme&gt;Damos&lt;/grapheme&gt;
        &lt;phoneme&gt;deɪmoʊs&lt;/phoneme&gt;
      &lt;/lexeme&gt;  
      &lt;lexeme&gt;
        &lt;grapheme&gt;Hills&lt;/grapheme&gt;
        &lt;phoneme&gt;hɪlz&lt;/phoneme&gt;
      &lt;/lexeme&gt;
      &lt;lexeme&gt;
        &lt;grapheme&gt;Woods&lt;/grapheme&gt;
        &lt;phoneme&gt;wʊdz&lt;/phoneme&gt;
      &lt;/lexeme&gt;

I'd like my lexicon to be a able to handle this problem, regardless of case.",2,aws,2020-10-13
isjwmu,Workspaces: switching usernames to a new instance,"hi there, i hope someone here can give me a runbook or best practice list to do this;

i want to switch to the new WPS instance types so i can run my webcam through my workspace, plus its been about 18 months since i set up my current instance and i think its just time for a wipe and new install.

&amp;#x200B;

so i want to create a new instance under a temporary user so i can continue work side-by-side and then once i have it all running kill my current instance under my name and then switch to the new one.

&amp;#x200B;

an alternative is i reassign the current one to a temp user and create the new one under my username so the users/ folders are correct while im installing apps and dev systems.

&amp;#x200B;

i just cannot seem to find any instructions on how to do this, most of my results are ""you cant"" which i frankly find bonkers, so i must be missing something.

&amp;#x200B;

any help and advice would be greatly appreciated...",3,aws,2020-10-13
iskyv1,Use of IP from other region?,"I want to have my servers running at `us-east-1` but we need to consume APIs from the Brazilian government. And some of these government servers block IPs from outside of the Brazil (and it is not updated, because today the range is bigger than what they accept).

At the moment what I have in mind is to have a NAT Gateway in `sa-east-1` with VPC Peering Connection to my `us-east-1` VPC. I am a bit concerned about the Data Transfer but I need to do the math.

Another strategy that I don't know if it would work is to buy IPs from Brazil and bring that to AWS.

I would love to have some help!",8,aws,2020-10-13
iskm73,AWS Load Balancing and Web App redirecting to SSL,"We have a [legacy] web application that redirects non-SSL requests (port 80) to SSL (port 443). The web server is IIS (if that matters).

Now I wish to put a load balancer in front of this app. That will be step one towards scaling in a future project phase.

However, the redirecting to SSL is causing problems that I can't quite get my head around.

I have the AWS Network Load Balancer (NLB) listening on both 80 and 443. Apparently it can only send to the target group (my web server) on **one** port.

* If I have it sending to port 80 then we go into a redirect loop because the web server assumes every request is non-SSL.
* If I have it sending to port 443 then we never redirect because, again, the web server assumes all requests are already secure

I chose NLB because it will allow me to use my existing web server's elastic IP which makes the cutover a bit more seamless (and, I'm told, some of our API clients have that IP address allow-listed already so a change is hard). I think that logic rules out using an Application Load Balancer (can't use elastic IP)? NLB is layer 4 though, so I can't play any games with headers.

I can't help but think this ""redirect to SSL"" thing has already been solved by smarter people than me... so how'd you do it?",8,aws,2020-10-13
isk7if,Learning Lambda - Suggest A Project?,"I was tasked with learning about Lambda, but not given any actual direction on what to do with it.

What task could I tackle in a test environment that would teach me good fundamentals about it?",7,aws,2020-10-13
isjzvc,How to setup Lake Formation to handle daily full datasets,"Hi everyone I am trying to use lake formation to register my tables. I have a process that will put my entire dataset into an s3 bucket daily. The structure looks like this currently.

Bucket -&gt; databasename -&gt; table name1 -&gt; dated parquetfolder1

Bucket -&gt; databasename -&gt; table name1 -&gt; dated parquetfolder2

Bucket -&gt; databasename -&gt; table name1 -&gt; json file telling which parquet is the newest

Bucket -&gt; databasename -&gt; table name2-&gt; dated parquetfolder1

Bucket -&gt; databasename -&gt; table name2 -&gt; dated parquetfolder2

Bucket -&gt; databasename -&gt; table name2 -&gt; json file tellling which parquet is the newest

But currently when I setup a glue crawler it literally flattens out database name and table name and just gives me 6 tables. On for tech dated parquet and two for the json config. Since they both have the same name one gets a hash.

In reality I want two tables and I want the path to swap when the json file changes. Or if I need to save the data in a different structure to achieve snapping of my data that is also fine",3,aws,2020-10-13
isjc8w,Charges in AWS Educate Account,"Hi all!

&amp;#x200B;

I have a AWS account, that came with Github Education, and I'm wondering why I'm being charged for a EC2 instance, using the free tier (**t2.micro**)

As I have a Educate account, I can't acess the billing console.

&amp;#x200B;

Thank you!

&amp;#x200B;

**EDIT**: answer from AWS Support:

&gt; If in case you are referring to the credits usage in your ESA, please be informed that regular AWS Account is different from AWS Educate Account and the 'Free Tier Promotion' is only applicable in regular AWS Account. In ESA any usage of services are covered by the pre-loaded credits and can be used until the credits are exhausted or expired. Educate Members receive a credit renewal once a year (after the first year) until they graduate from their institution.  ",2,aws,2020-10-13
isj6fp,AWS work question,"I'm using for work AWS, and I was wondering, can they see what device I connect with, like am I on my laptop or iPad? (Because it's strictly not allowed on tablets/phones etc just PC/desktop)  


Sorry if I'm not in the right place, but I didn't know where to ask this!

Thank you!",0,aws,2020-10-13
ishmja,Which AWS service to use for backing up office files?,"Use case:
My fathers company wants to backup all their office data on AWS from their in-house server as they are moving to a remote first model.
Their requirements are, CRD files and have access levels for which employees can access which files and folders. 

I have zero experience with AWS but I looked into S3 and it’s very easy to use from the console for managing the files. But I couldn’t see how to set access levels for storage system.

I want avoid creating a custom console and write all the APIs for accessing the storage system.

Help would be very much appreciated.

EDIT:

Thanks a lot for the help. I’ll look into the provided solutions 🙏🏻",7,aws,2020-10-13
isgvq4,AWS VPN Alert - You have new non-redundant VPN connections," Hi guys,  


I have several VPN connections from AWS to ASA's. They all have Active/Standby Connections and have done for as long as I've been in the company, however the last few days the following alert has sprung up out of nowhere:  


""You have new non-redundant VPN connections  
One or more of your vpn connections are not using both tunnels. This mode of operation is not highly available and we strongly recommend you configure your second tunnel. View your non-redundant VPN connections""  


I've seen elsewhere online that people say this is expected behaviour and just to ignore the notification because this will always be there when there are active/standby tunnels, however this is a bit irritating because I keep getting tickets generated everyday by the email that AWS is sending warning about this. Is there a way to get rid of this notification or at least stop it from emailing (preferably without changing the connection to Active/Active).  


Cheers,  
Paolo",1,aws,2020-10-13
isix59,Subnets: What CIDR block do they want from me?,"I am trying to setup DMS between two accounts and I think I have done everything except the subnet.

☑ VPC Peering

☑ Endpoints (destination)

☑ Route table (I think is right)

☑ Security group created for source and applied

Now I am trying the subnet which I think is my missing piece.

The problem is it doesn't seem happy with anything I input for the IPv4 CIDR Block.

If I enter the IP address of the other subnet:

&gt;CIDR Address is not within CIDR Address from VPC

If I enter the same range that the source VPC is in:

&gt;CIDR Address overlaps with existing Subnet CIDR

If I change the third, fourth, or fifth (range) octet/value:

&gt;Must be valid IPv4 CIDR

If I change the second octet/value:

&gt;CIDR Address is not within CIDR Address from VPC

Is there anything else I can try?  There's definitely something I am not understanding - perhaps fundamentally, so please be brutal if necessary :)

Thanks!",2,aws,2020-10-13
isgep8,Looking for CMP recommendation,"Looking for a cloud management platform tool that can work with multiple providers, appreciate any experience based recommendations/suggestion

Need is to manage environments on AWS, GCP, Azure. Operations &amp; tasks, resources, reporting, inventory, etc.",1,aws,2020-10-13
isigqo,Can you help with my Route 53 &amp; Lightsail redirect issue,"TLDR: Made a few config changes and now my *domain.tld” points to an elastic beanstalk page, instead of my actual lightsail instance that is in dns. Solved - see comments below.

So here’s what went down... I’ve been hosting my lightsail instance in AWS for a while now. Everything has been great. So I decided it was time to drink more AWS koolaid. I decide to 
migrate my tld registration. 

I setup my new DNS entries in Route53 mirroring my prior hosting company. Then repoint the nameservers to AWS. I wait, test, and verify all is well. My domain.tld and sub1.domain.tld work fine. Great. Well done.

Time to refill the koolaid cup and drink more. This time it’s email flavored, and I spin up WorkMail. It’s a tall glass, but I get it all setup. 

However, now when I visit my domain.tld I get redirected to a elastic beanstalk landing page. Happens on both domain.tld &amp; www.domain.tld. When I check R53, the A record is correctly setup pointing at the IP of my lightsail instance. It looks identical to my subdomains. So I’m lost as to what to change",1,aws,2020-10-13
isem6x,"Hi, I'm a UX designer and I was wondering if it's possible to utilize browser push notifications from AWS. Or is it possible to create our own notifications in the UI that can come from AWS? Thanks :)",,1,aws,2020-10-13
isdnpw,What factors to consider when choosing an architecture,"Asked on stackoverflow and got downvoted for asking for opinions.  I am looking for experienced based opinions.

I have maintained a large legacy SaaS app for years. It is an EMR, scheduling, document sharing, and video chat application. 

The core code and basic DB design were originally developed in 2006 using the Codeigniter MVC framework on a single EC2 LAMP server. It is currently running on load-balanced LEMP images as Fargate EC2 tasks using Aurora RDS with a few Lambda functions for cron tasks, SNS and SES features.

I have just received budget approval to create an updated greenfield version. Ordinarily, I would consider that good news, however, it presents an overwhelming number of interrelated decisions to be made. The goals for the new version are in descending order of priority:

* Functional parity with the current version (do not sacrifice any current functionality)
* Transparent migration of existing user data to the new version.
* Support multi-tenant and white labeling
* Multi lingual 
* Improve security and audit functions
* Improve application extensibility and reduce enhancement development time through reusable components.
* Support future or simultaneous mobile app development
* Revamp UI (currently responsive Bootstrap 3.3.7 and jQuery)
* Deploy within 8 - 12 months
* Improve current performance and support forecast growth
* Support oAuth and LDAP SSO

# The quandary:

I have narrowed my choices down to two options but my confidence is low that I am asking all the right questions. What else should I consider?  I am open to other solutions not specifically listed below.

## Option 1 React/Redux on AWS Amplify

Nodejs 
Lambda microservice API
Cognito
Aurora serverless RDS. 

## Pros/Cons. 
Pro - Popular modern language with lots of tools 
Pro - Easy to learn  
Pro - Completely serverless 
Pro - CDN edge delivery 
Pro - Large and active React community 
Pro - Available React developers 
Con - React is overkill for the APP - not really a SPA   
Con - Will require additional routing for backend business logic  
Con - Significant ramp-up time for team to become proficient with language and tools.   
Con - More difficult to debug  

## Option 2 Laravel/Vue  
LEMP images on Fargate EC2 tasks 
Laravel microservice API  
Aurora serverless RDS. 

## Pros/Cons. 

Pro - Established team skillset  
Pro - Faster ramp-up and development time  
Pro - Easier to debug 
Pro - Thriving Laravel community 
Pro - Available Laravel/PHP developers 
Con - Laravel is heavyweight and not known for performance. 
Con - Requires server image. 
Con - PHP is very old yet still popular. 

Edited for readability.",0,aws,2020-10-13
isfjpc,Best way to retrieve data with AWS GraphQL?,"Am using the default blog schema and can retrieve posts in the front end but I was wondering if there was a way to get specific data. Right now there is a listPosts query provided by the default schema but that retrieves everything in the database. How would I be able to filter? The default tutorial from AWS doesn't go into any further extant on more advanced quarry controls. Like I can do it in the front end by checking the BlogID but there must be a a better way and more efficient of the server does that.

I currently retrieve data like this

  const postData = await API.graphql(graphqlOperation(ListPosts));

but is there a way to get more specific information. I just need the syntax of how to do that. I know you can put a comma after ListPosts but am not sure how the syntax works and I keep getting errors.",1,aws,2020-10-13
ise8pq,How would you solve CloudWatch Logs email notifications with Message body,I have large number of remote assets that are logging into CloudWatch Logs. It is primarily simply syslog log format. The task is to have email delivery of particular pattern match. Email body should include full Message field of a log record that match. What is your suggestion?,1,aws,2020-10-13
isdvi0,Why do new accounts only get access to 2 AZs in us-west-1?,"I normally never have anything in the US regions (beyond whatever AWS has centralised) as I’m on the other side of the planet, however we are looking to expand into the US and I’m trying to decide on a region. 

For me, west coast will probably be best latency-wise so I’m trying to decide between us-west-1 and us-west-2 but I’m puzzled by Northern California having 3 zones but only 2 available to new accounts. Anyone know what that’s about?

Sorry if this has been asked before I tried searching and couldn’t find anything. 

See here, there’s an asterisks: https://aws.amazon.com/about-aws/global-infrastructure/regions_az/",20,aws,2020-10-13
isccg5,Can I communicate with EC2 with Lambda?,Can I run a MongoDB database on EC2 instance and do CRUD operations with Lambda functions?,1,aws,2020-10-13
isc1we,Can an EC2 be configured through a Service Account?,"Some documentation I read states “a staging server (an EC2 I assume) that can run CLI, preconfigured through a (IAM) service account” - parenthesis are mine. 

My understanding is IAM service accounts provide permissions to EC2 - can they also be used to configure an EC2?",0,aws,2020-10-13
isbg0q,"AWS Amplify, manual deployments, and S3 Buckets","I'm using AWS Amplify to publish a react web UI, and it's currently deployed via an internal company CI (gitlab).  The backend is built using the serverless package.

The way that the deployment is currently configured creates an S3 bucket for hosting the web UI and a second bucket for deployment.

I'd prefer for my manual deployment to _not_ create an S3 hosting bucket, and possibly share a deployment bucket with other CloudFormation deployments.

I've tried mirroring my gitlab repository to CodeCommit and using the serverless CLI to create the AmplifyApp and AmplifyBranch resources, but I was unable to get the Amplify Console to authenticate against the CodeCommit repository.  All said, it felt quite complicated for what I'm trying to accomplish.

Can anyone recommend something I might not have tried?",1,aws,2020-10-13
isaccz,Simba/AD FS Authentication Question,"I'm hoping to get clarification on an authentication approach for an application we're setting up. We're setting up a 3rd party BI application to use an AWS Data Lake. We were planning to use AD FS to map AD groups to IAM roles, but the approach has changed.

The BI application handles AD authentication and checks AD group membership to grant access to reports, and it will check an LDAP attribute for the user's preferred role, which it will pass to a Simba JDBC driver to authenticate to AD FS to get a temporary token from AWS. The preferred role will match an AWS IAM role. Since Simba will be using the preferred role when it authenticates, do we even need AD FS? Could we configure the Simba driver to connect directly to AWS to assume roles instead? I don't have much experience with AD FS/IdP's, so i'm curious about the best approach.

Thanks",1,aws,2020-10-13
isayfo,CDK Stack with predefined roles,"Howdy folks,

We are currently building various Pipelines with CDK.  We are not using the new Pipeline construct (yet, we haven't look at it and from the Github repo.. there's still some issues) so we rely on the CodePipelineAction construct, CodeBuild, etc...

By default, each of those ""Actions"" creates a role and CDK assign the proper IAM polices and most of the time, it does a pretty good job, even when doing cross-account.

Our security team would prefer to have predefined roles and their permissions / IAM policies in a different stack for auditing purposes and easier code reviews.

So... I've been tackling the issue by creating a single role for all those actions and assign the proper IAM permissions.  As I move forward, I had the impression that I kept fighting against what CDK was created for.............

I've run into a few  cases and it's been a rollercoaster:

\- CodeCommitSourceAction will create a role for Cloudwatch Event(bridge) to trigger the build when a commit is made.  The only way to use another role is the use CodeCommitTrigger.NONE and define your own event rule.  Now imagine if your CodeCommit is in another account than your Pipeline, you have to call the EventBus in the account where the Pipeline is and there's no public CDK API for that, but if you let CDK do it jobs, it will creates all the requirements.  (Note: You can probably fall back to CloudFormation) ... Bonus, you get Circular Dependencies if you reference the role from one stack to another by passing a variable.

&amp;#x200B;

[https://github.com/aws/aws-cdk/issues/10069](https://github.com/aws/aws-cdk/issues/10069)

[https://github.com/aws/aws-cdk/issues/9473](https://github.com/aws/aws-cdk/issues/9473)

[https://github.com/aws/aws-cdk/issues/8042](https://github.com/aws/aws-cdk/issues/8042)

\- If you create a roles with IAM policies and use it in another stack (iam.Role.rom\_role\_arn()), you have to make sure to set mutable=False.  If not, CDK will still try to add a Policy to the Role with the proper permissions.

\- If you use KMS key policies / S3 bucket policies (useful for cross account) and set those policies manually, CDK will add the required IAM Policy it needs anyways, so you end up having them in double.  You have to use ""without\_policy\_updates"" ....

[https://github.com/aws/aws-cdk/pull/9689](https://github.com/aws/aws-cdk/pull/9689)

&amp;#x200B;

All in all, I mostly got it working (except the for EventBus Cross-Account thingie) but it took me longer I would have expecting and during the whole process, I kept thinking that I was fighting against a opinionated framework and it was not the way it was thought to be used..

Anyone have run into similar train of thought?  What approach do you use when dealing with similar security requirements?

Cheers.",1,aws,2020-10-13
isa9wl,Do EC2 Spot instances persist EBS storage?,"Our EC2 instances need to use about 20-30gb of files (source code and assets) for each user (about 20 minutes of use), This isn't an issue when used On-Demand instances.

Been thinking about using Spot instances for the cost savings, but I am concerned that even though the assets are sync through S3 this will take some time to start. 

Is there a way to make sure that as many EBS volumes are reused as much as possible? I guess i could run something like EFS and connect each client to it

&amp;#x200B;",2,aws,2020-10-13
is9q5d,"What's the best way to ""pause"" SQS when using lambda triggers and step functions?","I'm experimenting with SQS to decouple 2 systems. System 1 is scalable. System 2 is not


**Requirement:**

1. Call Step Functions when a message is pulled from SQS and only delete it from the queue AFTER Step Functions has completed
2. Ability to ""pause"" SQS (by throttling consumer?)


**Desired work flow:**

1. System 1 puts message in SQS

2. Lambda pulls message from SQS

3. Lambda then invokes Step Function

4. Lambda finally returns (without waiting for Step Function to complete, since step functions can only be invoked asynchronously) 

5. Step Function runs its steps (pushing records to System 2 among other things) then deletes message from SQS when done

6. If Step Function fails, the message visibility timeout will run out and the message will be put back into the queue again


**Issue with the desired flow:**

1. If i setup SQS to trigger lambda to run on every message put into the queue, the lambda will automatically delete the message from the queue when it returns. But what if the Step Function fails? The message would have already  been deleted from the queue at this point


2. If I do setup a custom lambda (not triggered by SQS), whats the best way to launch a lambda and pull from the queue? I cant use SNS to listen for incoming messages on the queue and fire a lambda because I need to be able to throttle the lambda to ""pause"" the queue so not to overload System 2. Using SNS would cause the event to be ""lost"" if the SQS consumer is disabled (throttled to 0)

Im thinking of using CloudWatch Events to run the lambda every minute but that means only processing 10 messages per minute which seems a bit low and could cause pile up


Is using CloudWatch Events the best approach given the requirements?",8,aws,2020-10-13
is8pbb,Javascript updating a user's Cognito custom attributes.,"Here is the code I am currently trying to use:

&amp;#x200B;

&amp;#x200B;

`var cognitoUser = userPool.getCurrentUser();`

`async function changeProfileImage(link){`

`var userAttributes = {`

`""AccessToken"": cognitoToken,`

`""UserAttributes"": [`

`{`

`""Name"": ""custom:Company_Image"",`

`""Value"": link`

`}`

`]`

`};`

//Breaks here

`await cognitoUser.UpdateAttributesAsync(userAttributes);`

`document.getElementById('comapny_profile_logo').src=link;`

`}`

&amp;#x200B;

&amp;#x200B;

This is supposed to change the profile image of a user, as the URL to the company image is stored as a custom attribute to the user's Cognito account. I have done some research, and I thought this was how it should be done, but I get the error: cognitoUser.UpdateAttributesAsync is not a function.

Have any of you updated a user's custom Cognito attributes in javascript, and perhaps could push me in the right direction? Thanks!

&amp;#x200B;

&amp;#x200B;

UPDATE:

&amp;#x200B;

the correct solution was to use

&amp;#x200B;

	var data = { 

UserPoolId : 'us-east-1\_ZF#####y', // Your user pool id here

ClientId : '5sr83##########5oso4gd7'

};

var userPool = new AmazonCognitoIdentity.CognitoUserPool(data);

var cognitoUser = userPool.getCurrentUser();

&amp;#x200B;

var attributeList = \[\];

var attribute = {

Name: 'custom:Company\_Image',

Value: imageLink,

};

var attribute = new AmazonCognitoIdentity.CognitoUserAttribute(attribute);

attributeList.push(attribute);

 

cognitoUser.updateAttributes(attributeList, function(err, result) {

if (err) {

   

console.log(err);

return;

}

console.log('call result: ' + result);

});

}",2,aws,2020-10-13
is8k2e,Best way to move CloudTrail logs to Glacier?,"I can't simply use lifecycle rule for that, because for lots of smaller files Glacier just increases costs.

The only way to move log files to Glacier without incurring too much cost is concating and pushing to Glacier. As far as I know, there are no existing solution that allows this, so I would need to create lambda function that package log files in S3 and move it to Glacier.

I want to do the same for log files created by other services, such as CloudFront logs, CloudWatch logs exprorted to S3, etc.",17,aws,2020-10-13
is6wd3,Viewing logs we sent to Cloudwatch as custom events,"I haven't written the code to send the metrics to Cloudwatch but I do have the Amazon doc on how to send custom events to CloudWatch. Assuming my code has been sending these custom events to CloudWatch, how do I view the data as a graph or chart?",2,aws,2020-10-13
