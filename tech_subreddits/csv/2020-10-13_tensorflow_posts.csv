post_id,post_title,post_body,upvotes,subreddit,date
jaj9yk,What package am I supposed to put the msvcp140_1.dll file?,"Like, do I place it under the tensorflow package or the Python package or what?",1,tensorflow,2020-10-13
jaj4b6,Prince of Persia: The Sands of Time - All Cinematics (Remastered 8K 60FPS) Resolution increased using neural networks to 8K 60FPS,,1,tensorflow,2020-10-13
j9yt2m,How already worked with Masked Autoregressive Flow ?,"Hello community , I 'm working on MAF , and have some questions about it , I didn't find the documentation  very clear.

My question is about the way of adding Mask Autoregressive Flow to VAE",3,tensorflow,2020-10-13
j9wzy1,Does Tensorflow support CUDA 11.1,I've downloaded the latest version of CUDA 11.1 and I was wondering if Tensorflow supports this version or it requires version 10.1?,7,tensorflow,2020-10-13
j9vtn1,"TF Exam broke, now what? Has customer service been helpful?","**TL;DR:** TF exam broke to where I couldn't even see/work on questions, haven't heard back from support

&amp;#x200B;

So yesterday I started my exam with pycharm set up and ready to go per the guide. After installing packages and setting up the environment, a window popped up saying ""Sign in via the opened browser."" Nothing popped up on the browser, and I was signed in to my google account. I tried signing in and out, closing and reopening both Chrome and Edges, and nothing. I couldn't click anywhere in pycharm other than the cancel button which did nothing for 20 min. I had to kill the process and try again. That got rid of the ""Sign in via..."" window, but when I clicked on the exam button in the top right, it told me time was ticking down. There was no exam in sight, and when I clicked on the ""view instructions"" all the links didn't do anything. I couldn't do anything and the support team (at  [tensorflow-certificate-team@google.com](mailto:tensorflow-certificate-team@google.com)) didn't respond to my emails. 

&amp;#x200B;

&amp;#x200B;

Has anyone heard of this issue? I tried googling it and came up dry. Does anyone have experience with the support team? Will they be understanding whenever they get back to me?",8,tensorflow,2020-10-13
j9tevr,Anyone passed the tensorflow exam? What's your experience,"Forgive me if this is not the right subreddit.

I'm noobie to tensorflow and AI in general but almost 6 years of software development experience. I want to prepare for the tensorflow exam. What's your experience on preparing, any advice (even tensorflow is not good at this stage) is much appreciated.",12,tensorflow,2020-10-13
j9spdb,Neural Structured Learning (NSL): A TensorFlow Framework To Train Neural Networks With Structured Signals,"Neural Structured Learning (NSL) is a TensorFlow framework for training neural networks with structured signals. NSL can handle structured input in two ways: 

(i) As an explicit graph (for [Neural Graph Learning](https://arxiv.org/abs/1703.04818))

(ii) As an implicit graph (for [Adversarial Learning](https://arxiv.org/abs/1412.6572))

These techniques only affect the training workflow while the model serving workflow remains unchanged. This occurs due to these techniques being implemented as a form of regularization in the framework. 

Github: [https://github.com/tensorflow/neural-structured-learning](https://github.com/tensorflow/neural-structured-learning) 

Summary: [https://www.marktechpost.com/2020/10/12/neural-structured-learning-nsl-a-tensorflow-framework-to-train-neural-networks-with-structured-signals/](https://www.marktechpost.com/2020/10/12/neural-structured-learning-nsl-a-tensorflow-framework-to-train-neural-networks-with-structured-signals/)

&amp;#x200B;

https://preview.redd.it/hzcf0jtfkos51.png?width=2330&amp;format=png&amp;auto=webp&amp;s=05da1740ec499e6aa8c947ccd6f78c2dfa6c4099",8,tensorflow,2020-10-13
j9rmae,.data file in tensorflow checkpoint has repeats of variables which is correct?,"  

So inside my .data  
 file for a specific checkpoint I have  repeats of variables. I have a fully connected layer with weights  called w1, and inside the .data file I have the following three sets of  weights:

    linear_model/w1/Adam:0 

and

    linear_model/w1:0. 

and

    linear_model/Adam_1:0 

I have recreated my model in Keras as an experiment and want to load  the weights to my keras model directly from the .data file and I am not  sure which to use for the variable w1, is the linear\_model/w1/Adam:0  
 correct or the other?

Could someone explain the difference of these to me? Any suggestions would be appreciated.",1,tensorflow,2020-10-13
j9pevj,I published a video explaining frequency-domain audio features for machine learning,"In my new video, I introduce fundamental frequency-domain audio features, such as Band Energy Ratio, Spectral Centroid, and Spectral Spread. I explain the intuition and the math behind these acoustic features, and mention a few sample applications.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=3-bjAoAxQ9o&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=21](https://www.youtube.com/watch?v=3-bjAoAxQ9o&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=21)",5,tensorflow,2020-10-13
j9nf9e,Is there a way to pre-set my checkpoint monitor value when re-loading my weights?,"I want to resume training using the weights from my checkpoint and continue saving the best one. However, when I come back and start training again, it doesn't remember the loss from the last training and uses 'infinity' instead. I could just start a new checkpoint but I'd like to see if this is a possibility.",1,tensorflow,2020-10-13
j9m3il,Graph Classification," 

hello,

I have about 500 networkx graphs and I wanted to perform graph classification on them into 2 classes.

This is how a node looks like.

`{0: {'id': tensor(144), 'residue_name': tensor(8), 'h': tensor([0.0000, 0.0000, 0.0000, 0.0000, 6.0700, 0.1300, 0.1500]), 'coords': tensor([-21.1550, 23.3610, 1.9100]), 'ss': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]), 'asa': tensor([10752.]), 'rsa': tensor([128.])}`

This is how an edge looks like.

`(0, 2, {'id': 322, 'rel_type': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64), 'norm': tensor(1.)})`

If i try to use stellar graphs, it throws an error that it only accepts numeric features and i am not able to use DGL as none of their examples show to load our own dataset.

How do I use my data with these libraries or are there any other libraries I can use?",2,tensorflow,2020-10-13
j9kxhf,Can I pass the Tensorflow Developer exam with using just Google Colab? What about the Colab Pro?,"Last night, I almost hit the ""Start Exam"" button! Then a thought came up in my head: what if they enforce a specific version of TF2.xx??


Given my hardware specs, the latest I can have on my machine is TF2.2 and I read on Reddit [a statement](https://www.reddit.com/r/tensorflow/comments/itsfkx/tensorflow_software_version_for_certification_exam/g5n75pr/) (from the author of the exam @Laurence_Moroney) that TF2.0 is the version to be used in the exam... 


Unfortunately, I can't get TF2.0 to use GPU on my local machine, and the only solution on the internet for my case is to upgrade the TF version!! which is against the reason I downgraded to TF2.0 (or tried to!).

Long story short, now I plan to use the *""free version""* of **Google Colabs**, will it be enough? Does it take a reasonable time to train models during the exam, using Google Colab?
I am concerned that I might get cut off of resources if I use GPU runtime for hours!


Is it safer to pay for the  **Google Colab Pro**, or there is no need to?",3,tensorflow,2020-10-13
j9k4ow,"First, alpha release of TensorFlow Java!",,22,tensorflow,2020-10-13
j9414d,Rtx 30xx compatibility tensorflow,"I've read that the ampere series cards need Cuda 11.1 to work, while big projects like stylegan2 need tensorflow 1.14 with cuda 10.0. Does this mean it is not possible to run models like stylegan2 on rtx 30xx cards?",8,tensorflow,2020-10-13
j93kgg,Learn the basics of Tensorflow Extended in ~ 45 minutes ( mini video course ),,27,tensorflow,2020-10-13
j92l5y,[P] Monitor Keras model training on mobile phones,"App: [https://web.lab-ml.com](https://web.lab-ml.com/home)

Github: [https://github.com/lab-ml/app](https://github.com/lab-ml/app)

samples: [https://web.lab-ml.com/runs?labml\_token=samples](https://web.lab-ml.com/runs?labml_token=samples)

&amp;#x200B;

https://preview.redd.it/1zx8lugqrfs51.png?width=2880&amp;format=png&amp;auto=webp&amp;s=3a0ec8018609c247b6d0b03b54d211de7a269def",3,tensorflow,2020-10-13
j92ahm,I'm about to take the tensorflow dev exam ... I want to know some more details about it which i cant seem to find it on the web,"1. I've seen multiple posts where they use the tfds library for loading the dataset . I just want to know if this is mandatory or can i even use keras.datasets (in case the question is related to the dataset) ? 
2. Are there any minimum requirements to meet when training the model  ? (maybe something like : train a model to achieve val\_acc of 99.4% etc)
3. Apparently you dont have to type everything from scratch but rather just fill in some parts of the program during the exam , can I write whatever I want and  change the names of variables to what i want or will that mess with the backend of the plugin ?

I've finished the specialization course on coursera and practiced fairly well apart from course exercises but im still slightly doubtful about it",5,tensorflow,2020-10-13
j922ve,Copy .tflite file to another location while using the mobile application,I want to get a copy of the convereted model (.tflite file) in my mobile application and save it in another location when user performs a specific task in my mobile app. Is there ay way to do that? I know that .tflite files are not compressed but dont know how to get a copy of that while using the mobile application,7,tensorflow,2020-10-13
j8oi3q,How do you cast a Tensor&lt;TInt32&gt; in Java to a double and other native Java variable types?,"I'm new to TensorFlow and I cannot seem to find how to do this despite extensive searching online. I want to use the TensorFlow model I have built in Java to set some Java variable values, which in this case, is a double. Is there a good way of going about this?

Here is the code snippet:

    try(SavedModelBundle b = SavedModelBundle.load(""/somePath"", ""serve"")) {
        Session s = b.session();
        Tensor&lt;TInt32&gt; x = TInt32.scalarOf(1);
        Tensor&lt;TInt32&gt; y = TInt32.scalarOf(2);
    		
        Tensor&lt;TInt32&gt; result =(Tensor&lt;TInt32&gt;) s.runner().feed(""x"", x).feed(""y"", y).fetch(""ans"").run().get(0);
    
        // I know this won't work but do whatever is needed to convert to a double
        this.ExampleDouble = result;
    }",2,tensorflow,2020-10-13
j8hjdv,Masked Atuoregressive Flow (MAF) TF2.0,"Hello community , does anyone already add MAF Layer to an existing model ? I'm struggling into implement a solution based on MAF with TF-2.0 . I have a classic variational auto-encoder (VAE) ,and I want to add to it some MAF Layer . I didn't found the documentation very clear sincerely .

Does anyone know something about it ? It would be of a great help for me. Thanks",5,tensorflow,2020-10-13
j8ftoi,Jupyter Notebook to Convert TF/Keras model to TenosrRT with ONNX,"[riotu-lab](https://github.com/riotu-lab)/[tf2trt\_with\_onnx](https://github.com/riotu-lab/tf2trt_with_onnx) this Github repo has Jupyter notebook documents how to convert a Tensorflow or Keras model to TenosrRT using ONNX. ONNX is the official way to convert TF/Keras model to TRT. 

This is has been tested successfully on Facenet Keras model   
If anyone has a question please ask!",5,tensorflow,2020-10-13
j8cx8b,How to Train and Find Rotated Object in Image?,"Hello,

I have no experience with TensorFlow, but I have lots of python experience. I am trying to look for a target board in my images. All images will look very similar to [this](https://i.imgur.com/GnUke1W.jpeg) , where the target board takes up the majority of the image. We can assume that the camera will always be looking directly at the target board from straight ahead, as shown in the image. The board may however roll/rotate +/- 15 degrees.

I tried to follow [TensorFlow 2.0 Object Detection API Tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/) but when I went to train my model I was only ever able to get a bounding box that included a lot of the background image, and had a confidence of, at best, 60%. My training images were all cropped images of the board, like [this](https://i.imgur.com/MsAhpkZ.jpg) , and the associated annotation xml file for that image had the target area of the entire image width/height. My test images looked like the first image I linked in this post.

What am I doing wrong? Why does the model, after roughly 3000 steps, fail to detect the board anymore? I cannot find a sweet spot between 2000 (where it was approximately 60%) and 3000 steps that accurately finds the target board.

Lastly, how do I eventually incorporate object rotation into this? I would ideally like to rotate and crop to my target, such that the + is parallel/perpendicular to the X/Y axis.

Thanks.

&amp;#x200B;

EDIT: Machine specs: i9-9900K, 32GB RAM, NVIDIA RTX 2080 Super

EDIT2: I have roughly 20 actual image of my target board. I used the Python package [Augmentor](https://github.com/mdbloice/Augmentor) to apply a small amount of rotation, flipping, and distortion to the image. All target boards will look exactly the same. They will be a white background with 4 triangles surrounding a plus symbol. I used Augmentor to create 4,000 slightly different target boards.",1,tensorflow,2020-10-13
j86229,Any way to run tensorflow on RX 5500 XT,Hi guys. I have a RX 5500 XT that I'm trying to run tensorflow on. I have my pc dual booted so I can run ubuntu 2020.04 or Windows 10. Is there any way that I can run tensorflow using my card? I've looked at ROCm but that doesn't support the NAVI chipset.,1,tensorflow,2020-10-13
j85mzt,How do I replace original prediction with post-processing prediction in evaluate function?,"I have an image-classification model that has a pretty good accuracy but I have created a function that crops each image into 5 different images and then takes the most accurate prediction from those. I was wondering if there was an easy way to replace the original prediction with this new prediction in the evaluate function for a keras generator so that I can see the accuracy across my testing batch of around 50,000 different images.",1,tensorflow,2020-10-13
j7v4zc,Which losses to use in Variational Autoencoders,"Hello community. I'm currently implementing a VAE solution . In TensorFlow 2.0 examples , I saw that they used `sigmoid_cross_entropy_with_logits` as the reconstruction loss.

I didn't understand this choice , I would instead use MSE or RMSE errors to reconstruct the loss ,and use the `KL Divergence` for the latent loss.

I want to precise that in my task ,images don't have classes ,they are just a bunch of images that cannot be classified . I can simply say that I have one class.,what loss should I use instead of `sigmoid_cross_entropy_with_logits` does `MSE` would do the job ?",7,tensorflow,2020-10-13
j7tjju,TensorFlow beamsearchdecoder attentionwrapper with clone error,"I am trying to create an example for seq2seq.beamsearchdecoder

`vocab_size = 10`

`max_time = 16`

`batch_size = 2`

`emb_dim = 20`

`cell_dim = 5`

`attention_dim = cell_dim`

`beam_width = 3`

`hidden_size = 7`

`inputs = tf.random.uniform(`

`[batch_size, max_time, emb_dim],`

`maxval=1., dtype=tf.float32)`

`embedding = tf.random.uniform(`

`[vocab_size, emb_dim], maxval=1., dtype=tf.float32)`

`# make encoder`

`lstm = tf.keras.layers.LSTMCell(hidden_size)`

`lstmW = tf.keras.layers.RNN(lstm, return_sequences=True, return_state=True)`

`whole_encoder_seq_output, final_encoder_state, final_carry_state = lstmW(inputs)`

`tiled_encoder_output = tfa.seq2seq.tile_batch(whole_encoder_seq_output, multiplier=beam_width)`

`tiled_encoder_final_state = tfa.seq2seq.tile_batch(final_encoder_state, multiplier=beam_width)`

`encoder_initial_state = lstmW.get_initial_state(inputs)`

`tiled_encoder_initial_state = tfa.seq2seq.tile_batch(encoder_initial_state, multiplier=beam_width)`

`#make decoder`

`memory = tiled_encoder_final_state`

`attn_cells = tfa.seq2seq.AttentionWrapper(`

`lstm,`

`attention_mechanism=tfa.seq2seq.BahdanauAttention(units=hidden_size, memory=memory, memory_sequence_length=batch_size*beam_width),`

`attention_layer_size=hidden_size,`

`initial_cell_state=tiled_encoder_final_state`

`)`

`decoder_initial_state= attn_cells.get_initial_state(batch_size=batch_size*beam_width, dtype= tf.float32)`

`decoder_initial_state = decoder_initial_state.clone(cell_state=tiled_encoder_output)`

`#make predictions`

`decoder = tfa.seq2seq.BeamSearchDecoder(`

`cell=attn_cells,`     

`beam_width=beam_width,`

`output_layer=tf.keras.layers.Dense(hidden_size, name='output_proj')`

`) #second structure decoder`

`start_tokens = tf.zeros((batch_size,), dtype=tf.int32) decoder.initialize(embedding=embedding, start_tokens= start_tokens ,end_token= 1, initial_state=decoder_initial_state)#first structure decoder_initial_state`

but I am getting the following error

Entire first structure: AttentionWrapperState(cell\_state=., attention=., time=., alignments=., alignment\_history=(), attention\_state=.)

Entire second structure: AttentionWrapperState(cell\_state=\[., .\], attention=., time=., alignments=., alignment\_history=(), attention\_state=.)

the first structure is a sequence while other is not.

&amp;#x200B;

&amp;#x200B;

\\",3,tensorflow,2020-10-13
j7d72k,Supreme Commander: Forged Alliance - All Cinematics (Remastered 8K 60FPS) Resolution increased using neural networks to 8K 60FPS,,2,tensorflow,2020-10-13
j7bvvp,Stylegan2 first training 0 gpu utilization,"So i finally after a day or 2 got my first training going on a 3080. Have tensorflow gpu installed in my environment. But now the training just started. And my gpu stays at 0% somtimes goes to 1. Is this normal?

EDIT1: oke after a few minutes the training stops and i get this.

https://imgur.com/a/NafQfqo",4,tensorflow,2020-10-13
j7b8nc,I published a tutorial where I extract Mel-Frequency Cepstral Coefficients from audio data with Python,"In my new video, I explain how to extract MFCCs (and 1st and 2nd MFCCs derivatives) from an audio file with Python and Librosa. I also visualise MFCCs for a music piece.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=WJI-17MNpdE&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=20](https://www.youtube.com/watch?v=WJI-17MNpdE&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=20)",15,tensorflow,2020-10-13
j6zcgw,CUDA vs rocm,"Hi everyone,

last time I checked (3 years ago) rocm was absolutely useless.

Does anyone of you have a proper benchmark between consumer grade nvidia gpus with cuda and amd gpus using rocm?

I would also appreciate reports from rocm users about how they experience rocm.

Best wishes.",15,tensorflow,2020-10-13
j6nn2d,RTX 2060 Super slower than a GTX 1060 !,"Hello all,

I just got my hands on a brand new RTX 2060 Super to replace my GTX 1060.

I went straight to executing a few ML Tensorflow algorithms I've been working on and the RTX 2060 Super is on average 30% slower than my GTX 1060 !!!

I know there are some architecture changes under the hood ; I don't use mixed precision at all yet (so bye bye Tensor cores for now). 

Could it be because I use tf.float64 ?

Any help or links would be greatly appreciated, I'm not sure where to start.

Best regards,",16,tensorflow,2020-10-13
j6njzg,[Question] tf.saved_model.load in TF2.x is much slower than tf.import_graph_def in TF1.14,"Hi guys,

Anyone has experience on migration of tf1 frozen graph yo tf2 savemodel? Is there a corresponding method to frozen graph in TF2.x?

Our team want to move to TF2.X from TF1.14. 

Save model:
In TF1.14, we apply freeze_ graph approach to generate .pb file.
In TF2.x, in turn we apply tf.saved_model.save to save trained model to savemodel directory.

Load model:
TF1.14, it only take 3.4s to load model
TF2.x, it take 24s

Thanks in advance.",2,tensorflow,2020-10-13
j6fa65,Latest from Microsoft and Samsung researchers: State of the art in Face Attribute Editing with GANs,,0,tensorflow,2020-10-13
j69rsn,Performance drop migrating from standalone keras to tf.keras?,"Apologies in advance because I'm not super familiar with tensorflow itself, and just working with a library that makes use of it.

I'm working on a project that involves translating terabytes of English text into its phonetic representation. Luckily there's a fantastic library called [pincelate](https://github.com/aparrish/pincelate), that uses a tensorflow backend to guess pronunciations of words that don't appear in my main pronunciation dictionary.

Pincelate was written for tensorflow 1.15 and keras 2.2.5. I've got it set up with those dependencies in its own conda environment, and my multithreaded script can translate my source text at a speed of \~25-200 lines/second running on my CPU. The single-threaded GPU version is a lot slower because so much in the script is file i/o and text manipulation that isn't in tensorflow, which is why I'm running on CPU.

I thought it'd be worth seeing if there was any improvement to be had by using a more recent tensorflow (and tf.keras, since standalone keras is going away). I modified the import statements and a few model loading calls (pretty much exactly what [this person did in this pull request](https://github.com/aparrish/pincelate/pull/9/files)) but otherwise left the code untouched. 

Performance using tensorflow 2.3.1 and tf.keras has been pretty terrible in comparison, \~1.5-8 lines/second. Basically what I was getting without multiprocessing. But my CPU is still showing near-max usage across all cores, so I don't think it's a multiprocessing problem. It's still loading models trained under the original code, but the tensorflow docs indicate those models should still be compatible. 

I haven't come across any performance complaints related to tf.keras. If anything, I've seen reports of performance improvements. Is there something important I'm missing that needs to be done as I migrate the code?",1,tensorflow,2020-10-13
j68zsg,Top 10 Machine Learning Courses(Theory and Practical in 2020),,5,tensorflow,2020-10-13
j67yl7,Why you should write Algorithms for Projects that may not require them,,1,tensorflow,2020-10-13
j67fcf,Tensorflow js- where do i keep/serve my model?,"I have a keras model that i converted for tfjs and i would like to be able to run it on the user's device for my web app. After the conversion i'm left with a model.json and a shard1of1.bin file. Is it standard procedure to serve the model from the google cloud platform or is it possible for me to include the model in the assets of my app and access it that way instead of having to host it on google cloud or aws? 

I tried to just throw the model onto github and make a request to it there but that resulted in lots of CORS issues (understandably). I'm worried I'm convoluting this... I thought I would be able to include the model in the assets of the app and access it there but that doesn't seem to be the case.",1,tensorflow,2020-10-13
j65lw5,I wrote a blog on Federated learning. I hope you find it helpful.,,14,tensorflow,2020-10-13
j63ybu,Can't achieve advertised Model Zoo TF2 detection speeds,"Hello Tensorflow Community,

&amp;#x200B;

this weekend I started looking into Tensorflow for object detetction. After a day of trial and error I got my code working so far that I can test all the models presented on the [TensorFlow 2 Detection Model Zoo-Page](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)

&amp;#x200B;

My PC Specs:

i7 8700k 6 x 4,5GHz

RTX 2070 Super

32GB DDR4 3200MHz RAM

Everything running on a NVME Samsung 960 EVO

&amp;#x200B;

In this example I am running the [EfficientDet D7 1536x1536](http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d7_coco17_tpu-32.tar.gz) model with an advertised speed of 325 ms and a COCO mAP of 51.2 and the  [SSD MobileNet V2 FPNLite 640x640](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz) with an advertised speed of 39 ms and a COCO mAP of 28.2.

However in my example the model takes way longer (2-3x) for inference.

&amp;#x200B;

Timings corresponding to the code (EfficientDet D7):

Grab took 0.04 seconds

Inference  took 0.92 seconds

Boxes took 0.12 seconds

Frame took 1.09 seconds

&amp;#x200B;

&amp;#x200B;

Timings corresponding to the code (ssd\_mobilenet\_v2 640x640):

Grab took 0.05 seconds

Inference  took 0.12 seconds

Boxes took 0.05 seconds

Frame took 0.22 seconds

&amp;#x200B;

&amp;#x200B;

My full code here (with some debugging leftovers): [https://pastebin.pl/view/5647503b](https://pastebin.pl/view/5647503b)

Screenshot for further info (EfficientDet D7): [https://imgur.com/a/XyydPp9](https://imgur.com/a/XyydPp9)

Screenshot for further info (ssd\_mobilenet\_v2 640x640): [https://imgur.com/a/W7YD2Bv](https://imgur.com/a/W7YD2Bv)

&amp;#x200B;

All code is running inside a conda environment with Python 3.6, Tensorflow 2.3.1, CUDA 10.1 and Cudnn 10.1.

&amp;#x200B;

Now for my questions:

\- Am I doing something horribly wrong?

\- Are the advertised speeds even achievable (with a normal computer)?

\- Sometimes even the draw for the detectionboxes takes \~0.3 seconds (is that normal?)

(visualization\_utils.visualize\_boxes\_and\_labels\_on\_image\_array())

\- Is the inference-time the advertised speed on the model-zoo page?

&amp;#x200B;

I am grateful for any replys

&amp;#x200B;

&amp;#x200B;

Edit:

I updated the detect\_fn to a more TF2 approach:

        @tf.function
        def detect_fn(self, image):
            """"""Detect objects in image.""""""
            start_time = time.time()
            image, shapes = self.detection_model.preprocess(image)
            prediction_dict = self.detection_model.predict(image, shapes)
            detections = self.detection_model.postprocess(prediction_dict, shapes)
            print('Inference took {} seconds'.format(str(time.time() - start_time)[:4]))
            # return detections, prediction_dict, tf.reshape(shapes, [-1])
            return detections

&amp;#x200B;

but that sadly doesn't change the inference speed at all.

&amp;#x200B;

Edit:

EfficientDet D7

Step 0: Inference took 7.95 seconds

Step 1: Inference took 0.96 seconds

Step 2: Inference took 0.96 seconds

Call-Trace: [https://cdn.discordapp.com/attachments/462892567575658497/764139635701055539/efficientdet\_d7\_gpu\_cpu.png](https://cdn.discordapp.com/attachments/462892567575658497/764139635701055539/efficientdet_d7_gpu_cpu.png)

I don't really know if there are any anomalies. Need to do further investigation.

&amp;#x200B;

&amp;#x200B;

Edit: According to some more knowledgeable person there is a corresponding GPU Kernel for the  NonMaxSuppressionV5 Operation which is consuming my CPU Pipeline.

[https://github.com/tensorflow/tensorflow/blob/e8598ce0454c440fca64e4ebc4aeedfa7afd5c97/tensorflow/core/kernels/image/non\_max\_suppression\_op.cu.cc#L580](https://github.com/tensorflow/tensorflow/blob/e8598ce0454c440fca64e4ebc4aeedfa7afd5c97/tensorflow/core/kernels/image/non_max_suppression_op.cu.cc#L580)

I still need to figure out how to implement the GPU version

&amp;#x200B;",0,tensorflow,2020-10-13
j60hte,Install Tensorflow on Windows 10 Working,,6,tensorflow,2020-10-13
j5tvqk,How can I apply reinforcement learning in a Javascript environment?,"Hi everyone! I'm working on a project about creating an agent that plays a Mortal Kombat game written in Javascript. My problem is that I want to train a deep learning model in Google Colab (Python + tensorflow) so my first idea is to ""migrate"" the project from JS to Python to train the model in a ""similar"" game environment, but surely there are better solutions, any advice?

Thank You!",4,tensorflow,2020-10-13
j5kw10,What opensource programs for end user desktop include tensorflow?,"Most end users dont know how or wont follow manual install instructions. If they click it and it doesnt work, or at most click next next... next, they use some other program. I'm not looking for instructions how to in theory deploy tensorflow to desktop. I want to click someone's existing program then copy the parts of that already working deployment by removing their code and adding my own, then unzip it on other computers and it works instantly there too. Linux and Windows.",0,tensorflow,2020-10-13
j5h35b,I published a tutorial where I explain Mel-Frequency Cepstral Coefficients (MFCCs) easily,"Mel-Frequency Cepstral Coefficients have traditionally been used in numerous speech and music processing problems. They are a somewhat elusive audio feature to grasp. In my new video, I introduce the concept of Cepstrum, illustrate its intuition, and discuss how we can extract MFCCs.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

https://www.youtube.com/watch?v=4\_SH2nfbQZ8&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=19",12,tensorflow,2020-10-13
j595yj,Supreme Commander - All Cinematics (Remastered 8K 60FPS),,10,tensorflow,2020-10-13
j4zx3d,"Getting very low performance with two GPUs (Ubuntu 20.04.1, Ryzen 1950X, 2 Vegas)","Hi all,

I made a post last night where I believed I was not using my GPUs at all.  I now think that's not true, but I am still a little confused by what's happening.

I followed [this guide](https://towardsdatascience.com/train-neural-networks-using-amd-gpus-and-keras-37189c453878) and am running the code below.  A few things stand out to me.

1. I'm getting 6ms per step which is slower than I got without trying to use both GPUs, and about 120 times slower than what the guide got with a single RX-480.
2. I can see that the GPUs are being used:

&amp;#x200B;

    ========================ROCm System Management Interface========================
    ================================================================================
    GPU  Temp   AvgPwr  SCLK    MCLK    Fan     Perf  PwrCap  VRAM%  GPU%  
    0    57.0c  27.0W   852Mhz  945Mhz  14.9%   auto  220.0W   95%   51%   
    1    50.0c  23.0W   852Mhz  167Mhz  13.73%  auto  220.0W   99%   54%   
    ================================================================================
    ==============================End of ROCm SMI Log ==============================

&amp;#x200B;

but the output lines `""INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).""` I am not sure what to make of.

&amp;#x200B;

Code and complete output:

    import tensorflow as tf; #2.3.1
    from tensorflow.keras.datasets import mnist
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout
    from tensorflow.keras.optimizers import RMSprop
    from tensorflow.keras.utils import to_categorical
    strategy = tf.distribute.MirroredStrategy()
    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
    
    with strategy.scope():
        batch_size = 128
        num_classes = 10
        epochs = 10
    
        # the data, split between train and test sets
        (x_train, y_train), (x_test, y_test) = mnist.load_data()
        x_train = x_train.reshape(60000, 784)
        x_test = x_test.reshape(10000, 784)
        x_train = x_train.astype('float32')
        x_test = x_test.astype('float32')
        x_train /= 255
        x_test /= 255
        print(x_train.shape[0], 'train samples')
        print(x_test.shape[0], 'test samples')# convert class vectors to binary class matrices
        y_train = to_categorical(y_train, num_classes)
        y_test = to_categorical(y_test, num_classes)
    
        model = Sequential()
        model.add(Dense(512, activation='relu', input_shape=(784,)))
        model.add(Dropout(0.2))
        model.add(Dense(512, activation='relu'))
        model.add(Dropout(0.2))
        model.add(Dense(num_classes, activation='softmax'))
    
        model.summary()

Which yields:

    INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
    Number of devices: 2
    60000 train samples
    10000 test samples
    Model: ""sequential""
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense (Dense)                (None, 512)               401920    
    _________________________________________________________________
    dropout (Dropout)            (None, 512)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 512)               262656    
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 512)               0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 10)                5130      
    =================================================================
    Total params: 669,706
    Trainable params: 669,706
    Non-trainable params: 0

And then:

    model.compile(loss='categorical_crossentropy',
                  optimizer=RMSprop(),
                  metrics=['accuracy'])
    history = model.fit(x_train, y_train,
                        batch_size=batch_size,
                        epochs=epochs,
                        verbose=1,
                        validation_data=(x_test, y_test))

which yields:

    Epoch 1/10
    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use `tf.data.Iterator.get_next_as_optional()` instead.
    INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    464/469 [============================&gt;.] - ETA: 0s - accuracy: 0.9248 - loss: 0.2446INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
    469/469 [==============================] - 4s 9ms/step - accuracy: 0.9252 - loss: 0.2433 - val_accuracy: 0.9578 - val_loss: 0.1437
    Epoch 2/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9689 - loss: 0.1028 - val_accuracy: 0.9724 - val_loss: 0.0916
    Epoch 3/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9769 - loss: 0.0768 - val_accuracy: 0.9789 - val_loss: 0.0733
    Epoch 4/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9823 - loss: 0.0611 - val_accuracy: 0.9805 - val_loss: 0.0676
    Epoch 5/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9848 - loss: 0.0517 - val_accuracy: 0.9810 - val_loss: 0.0746
    Epoch 6/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9871 - loss: 0.0437 - val_accuracy: 0.9814 - val_loss: 0.0759
    Epoch 7/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9888 - loss: 0.0381 - val_accuracy: 0.9793 - val_loss: 0.0958
    Epoch 8/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9901 - loss: 0.0344 - val_accuracy: 0.9827 - val_loss: 0.0833
    Epoch 9/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9901 - loss: 0.0316 - val_accuracy: 0.9813 - val_loss: 0.0923
    Epoch 10/10
    469/469 [==============================] - 3s 6ms/step - accuracy: 0.9921 - loss: 0.0275 - val_accuracy: 0.9828 - val_loss: 0.0874

I am completely new to all of this so any help would be enormously appreciated.",3,tensorflow,2020-10-13
j4yfe2,Build your own Alexa with the ESP32,,29,tensorflow,2020-10-13
j4vbbm,How to install tensorflow using pip. I wanna import it in python.,"Edit: I did it. Thanks to everyone who helped 

I'm not sure if this goes here but I;m gonna try it out. I can't seem to install tensorflow using pip.  


I've tried using the command:

`pip install tensorflow`  
&amp;  
`pip install tensorflow-gpu`  
because that's what the internet said but i'm always getting an error saying:  
`ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)`

`ERROR: No matching distribution found for tensorflow`

I've tried writing the command on command prompt and python terminal and VSCode terminal but it's not working. Can someone help me please? I'm using python 3.8 (32 bit)",0,tensorflow,2020-10-13
j4rfs0,Tensorflow Developer Certification Hoodie,"I just passed the Tensorflow Developer Certification Exam yesterday, I've heard from others that there might be a hoodie to purchase after you passed the exam, is that true? It would be really swag if there is one.",12,tensorflow,2020-10-13
j4qfet,"Using Tensorflow/ROCM with Ryzen Threadripper 1950X and two Vegas, Ubuntu 20.04.1 - only CPU is being used, despite visible GPUs","EDIT: I now believe the issue is a little different than I wrote here.  I made a newer post [here.](https://www.reddit.com/r/tensorflow/comments/j4zx3d/getting_very_low_performance_with_two_gpus_ubuntu/)

&amp;#x200B;

Hi all,

I installed everything this afternoon and I thought I'd figured it out, but I am having an issue where I can see that only my cpu is being used by tensorflow.  For reference, I followed this guide:

[https://towardsdatascience.com/train-neural-networks-using-amd-gpus-and-keras-37189c453878](https://towardsdatascience.com/train-neural-networks-using-amd-gpus-and-keras-37189c453878)

When I run jupyter notebooks I see this, but I can watch my CPU usage go up when I run the example code given in the tutorial and also see that the time per step is abysmal compared to what it ought to be (3ms / step).  Any tips would be appreciated!

    Kernel started: dc0e3dd4-cb8b-4966-808c-275109d7d828, name: python3
    2020-10-04 00:55:54.504572: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libamdhip64.so
    2020-10-04 00:55:54.590711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
    pciBusID: 0000:0c:00.0 name: Vega 10 XTX [Radeon Vega Frontier Edition]     ROCm AMD GPU ISA: gfx900
    coreClock: 1.6GHz coreCount: 64 deviceMemorySize: 15.98GiB deviceMemoryBandwidth: 450.61GiB/s
    2020-10-04 00:55:54.590778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 1 with properties: 
    pciBusID: 0000:43:00.0 name: Vega 10 XT [Radeon RX Vega 64]     ROCm AMD GPU ISA: gfx900
    coreClock: 1.63GHz coreCount: 64 deviceMemorySize: 7.98GiB deviceMemoryBandwidth: 450.61GiB/s
    2020-10-04 00:55:54.594086: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so
    2020-10-04 00:55:54.595564: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so
    2020-10-04 00:55:54.602565: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so
    2020-10-04 00:55:54.602838: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so
    2020-10-04 00:55:54.603009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
    2020-10-04 00:55:54.609553: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3399600000 Hz
    2020-10-04 00:55:54.610598: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x58e1970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
    2020-10-04 00:55:54.610622: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
    2020-10-04 00:55:54.613108: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x594d650 initialized for platform ROCM (this does not guarantee that XLA will be used). Devices:
    2020-10-04 00:55:54.613127: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Vega 10 XTX [Radeon Vega Frontier Edition], AMDGPU ISA version: gfx900
    2020-10-04 00:55:54.613137: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Vega 10 XT [Radeon RX Vega 64], AMDGPU ISA version: gfx900
    2020-10-04 00:55:55.259440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
    pciBusID: 0000:0c:00.0 name: Vega 10 XTX [Radeon Vega Frontier Edition]     ROCm AMD GPU ISA: gfx900
    coreClock: 1.6GHz coreCount: 64 deviceMemorySize: 15.98GiB deviceMemoryBandwidth: 450.61GiB/s
    2020-10-04 00:55:55.259541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 1 with properties: 
    pciBusID: 0000:43:00.0 name: Vega 10 XT [Radeon RX Vega 64]     ROCm AMD GPU ISA: gfx900
    coreClock: 1.63GHz coreCount: 64 deviceMemorySize: 7.98GiB deviceMemoryBandwidth: 450.61GiB/s
    2020-10-04 00:55:55.259583: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so
    2020-10-04 00:55:55.259604: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so
    2020-10-04 00:55:55.259627: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so
    2020-10-04 00:55:55.259650: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so
    2020-10-04 00:55:55.259940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
    2020-10-04 00:55:55.259972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
    2020-10-04 00:55:55.259983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 
    2020-10-04 00:55:55.259991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N 
    2020-10-04 00:55:55.259998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N 
    2020-10-04 00:55:55.260233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15385 MB memory) -&gt; physical GPU (device: 0, name: Vega 10 XTX [Radeon Vega Frontier Edition], pci bus id: 0000:0c:00.0)
    2020-10-04 00:55:55.265708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7685 MB memory) -&gt; physical GPU (device: 1, name: Vega 10 XT [Radeon RX Vega 64], pci bus id: 0000:43:00.0)
    2020-10-04 00:55:59.028748: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:506] ROCm Fusion is enabled.",7,tensorflow,2020-10-13
j4ow4k,"Latest from USC researchers: Given a single neutral scan, researchers generate a complete set of dynamic face model assets, including personalized blendshapes and physically-based dynamic facial skin textures of the input individual!",,6,tensorflow,2020-10-13
j4o9fq,Quick question,"Does anyone know how to make an object detector form scratch? Preferably it trains from images with boundary boxes. Can you pls explain to me how it works, im quite new to tensorflow.",0,tensorflow,2020-10-13
j4m83g,Art student. and ex-IT student want to use a GAN," 

Hello. I am a art student. And in my profession I use a lot of new technology. 3d printing, But also unity and VR. But now I want to learn how to make a specific GAN. The GAN I would like to make. Is a smaller version of thispersondoesnotexist site but now with faces i present to the GAN.Im wondering if there is already a pretty basic premade version of that one. Or a guide on how to make one yourself. I know my way around code. atleast I can read it to a certain degree. Im not an expert. I cant write code out of the top of my head. But following a guide is not that hard for me.

Is there somebody who can send me a direction.

I would also like to make a gan that takes for example all Rembrandt Paintings. And then creates something new out of it. The end result doesnt need to be perfect.

I have a high end pc with a 3080 btw.",7,tensorflow,2020-10-13
j4f9c0,Installing a AMD and Nvidia gpu into same machine.,Is it possible to have an AMD card as the main driver for your system and have a Nvidia card in the spare pcie slot just for tensorflow? (Windows 10),3,tensorflow,2020-10-13
j42uwf,Learn to build Image classification API with TensorFlow. https://youtu.be/23R2eI95S30,,24,tensorflow,2020-10-13
j42nsd,TFProfiler to profile your .tflite model with one click,"[https://github.com/iglaweb/TFProfiler](https://github.com/iglaweb/TFProfiler)  


TFProfiler is an app to profile TensorFlow Lite model and measure its performance using FPS, model initialization time, model inference time, memory consumption, etc right on device. You can tweak model runs with different delegates (CPU, GPU, NNAPI, HEXAGON), XNNPACK option, number of threads, etc.",6,tensorflow,2020-10-13
j40nv3,Optimize runtime memory,,1,tensorflow,2020-10-13
j3uabw,"John Snow Labs Spark-NLP 2.6.2: New SentenceDetectorDL, improved BioBERT models, new Models Hub, and other improvements!",,6,tensorflow,2020-10-13
j3mwyk,(beginner) Using old TF version with RTX/CUDA10,"Hello, student here, I am in a class this semester which requires tensorflow and a GPU to use it (no problem)

However, after receiving the first TF assignment, the class is requiring us to use tf 1.10.

This does not seem to be compatible with CUDA10 and by extension I cannot use it with my RTX 2060.

I can get 1.15 working just fine.

Is there a guide anywhere to get this up and running?

(This 1.10 requirement is throwing the entire class into chaos for seemingly everyone)",4,tensorflow,2020-10-13
j3b5es,F.E.A.R. 3 - Trailer (Remastered 8K 60FPS) Resolution increased using neural networks to 8K 60FPS,,1,tensorflow,2020-10-13
j38b7j,Tensorflow JS import error and compatibility issue.,"I have created a conda environment in Linux and installed pip install tensorflowjs in it. When i tried to import tensorflowjs as tfjs, I got AttributeError: module 'tensorflow\_hub.tf\_v1' has no attribute 'estimator'. I have attached Pictures of the code and error. Please help me in resolving it.

https://preview.redd.it/jff05wjjghq51.png?width=1533&amp;format=png&amp;auto=webp&amp;s=5a08d3a35d8bb508d1e7f3122e00e1a6491417fc

https://preview.redd.it/z9hf60kjghq51.png?width=1533&amp;format=png&amp;auto=webp&amp;s=8fc4d75ab639ba1cba8f117ea342762b7211348b",1,tensorflow,2020-10-13
j37yt6,How to use LSTM,"I am beginner in tensorflow.

To learn about lstm I have a simple array  1,2,3,4,5 and job of lstm is  to predict 6

&amp;#x200B;

 

`model = tf.keras.Sequential()`  
`model.add(layers.LSTM(1,input_shape=(5,1)))`  
`model.add(layers.Dense(1))`  
`x=np.array([1,2,3,4,5])`  
`print(x.reshape((1,1,5)))`  
`x=tf.Variable(np.array(x.reshape((1,5,1))))`  
`y=tf.Variable(np.array([6]))`  
`model.compile()`  
`model.fit(x,y)`

&amp;#x200B;

I get  `ValueError: Failed to find data adapter that can handle input:`",1,tensorflow,2020-10-13
j34rt7,Is it still worth learning Tensorflow 1 for Deep RL in 2020?,"I am self-studying Berkeley CS285 (Deep Reinforcement Learning) and I want to work on the homework assignments. The official exercises of the 2019 version of the course are written in TF1 and there is an unofficial, modified version of these exercises written in PyTorch. Also, it seems that in the 2020 version of the course, that is currently running, they transitioned to PyTorch officially. However, currently only 3/5 exercises are available.

I have been using PyTorch for a while now, but did not really get my hands on TF. In light of the release of TF2, I was wondering whether it is still worth learning TF1, through working on the official exercises. I also thought that there might be a lot of legacy code written in TF1, especially in the field of Deep RL, and thus it might be beneficial to know it, at least to some extent.

What are your thoughts? Is TF1 still worth learning for these purposes?",2,tensorflow,2020-10-13
j32fo3,Creating Calculators in Mediapipe — Beyond The Documentation,,6,tensorflow,2020-10-13
j3268v,Garbage data/NaN with tensorflow on RTX 3090 (windows),"I've just upgraded to a 3090 FE from my old 1080 Ti, and installed the new drivers, etc. But now anything I run with tensorflow seems to output garbage data. Has anyone had anything like this happen to them before?

For example, just running inference on a pretrained stylegan2 model outputs random noisy images (oddly the first image it generates seems to be mostly ok). Examples: [https://imgur.com/a/tiqZVld](https://imgur.com/a/tiqZVld)

If I try to train any model the losses/weights end up going to NaN almost instantly (using FP32, not FP16). For stylegan2 it crashes when training due to NaN matrix values.

My first thought was some kind of hardware error, but video games/passmark seem to run fine. So I'm a bit stumped. Maybe it's an issue with conflicting cuda versions or something? I've tried a few different models with different tensorflow versions. Also reinstalled drivers/tried the studio driver version with no luck.

Thanks in advance if anyone can help me out.

EDIT: I think I got it to work. Steps below:

1. Create a python 3.8 conda environment and install tf-nightly-gpu via pip (thanks /u/kevso311)
2. Install cuda 11.0 and cuDNN 8.0.2
3. Install cuda 11.1
4. Replace ptxas.exe in the v11.0 bin directory with the v11.1 version (the 11.0 version was causing errors for me)
5. Make sure your path/cuda path point to cuda 11.0 (not 11.1)

I'm getting an issue now where some models just hang on the start of training. But the data corruption and NaN values seems to be fixed.",25,tensorflow,2020-10-13
j31upr,Propagate the style from a few selected keyframes to the rest of the sequence!,,3,tensorflow,2020-10-13
j2pbrc,UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.,"I'm hoping to learn more about a particular warning I'm seeing when running a Python API locally that queries ML models. Disclaimer: I'm not a Python programmer or a data scientist. ;) I'm involved with team management now but my background is in .NET. Our data scientist has built this API but I want to validate what I'm hearing from this person with the broader community as we're running into some memory management issues (roughly 2.5MB of memory is consumed on every API call and is never released/garbage collected, resulting in application crashes when max system memory is reached).

**UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.**  **This may consume a large amount of memory.**

This is the warning we're receiving on running the Python API locally. It appears to be caused by the bottom two lines of code:

    from tensorflow.python.keras.models import load_model
    from deepctr.layers import custom_objects
    
    params['model_cr']  = load_model('deepfm_cr.h5',custom_objects)
    params['model_wr']  = load_model('deepfm_wr.h5',custom_objects)

These are the only lines that leverage the imported Tensorflow module and removing them eliminates the warning. Is this a warning that we should be paying attention to or can it safely be ignored as I've seen in some other posts on StackOverflow?

Our data scientist is indicating that this warning (and the 2.5MB per call that is never released) is the result of a ""known memory leak in either Tensorflow or DeepFM"" and that in order to fix it, we'd need to patch one of those modules but from what I've seen that doesn't appear to be documented anywhere and I haven't seen any references to that in the research I've done on my own. I'm finding it hard to believe that Tensorflow or DeepFM would have such glaring issues, especially in our use case which is extremely lightweight compared to much more complex use cases.

Thanks for any light you might be able to shed on this!",1,tensorflow,2020-10-13
j2od43,"Yet another tensorflow tutorial - from ""scratch""","To help myself better understand TF 2.X I did a short tutorial building up and training a model from ""scratch"" starting with auto-diff and then adding the abstractions that come with Keras. Posting this in hopes that it will help someone else, as an alternative to the more abstract ""getting started"" MNIST tutorials.

[https://github.com/rbitr/blog/blob/master/tutorial.md](https://github.com/rbitr/blog/blob/master/tutorial.md)",4,tensorflow,2020-10-13
j2m5gy,Prototype 2 - Trailer (Remastered 8K) Resolution increased using neural networks to 8K,,15,tensorflow,2020-10-13
j2j3xz,what is the solution to this problem( PermissionError: [Errno 13] Permission denied)?,"The code:

import tensorflow as tf   
import os   
loaded\_model = tf.keras.models.load\_model('C:/Users/USER/Desktop/project-tst#1/my\_project\_model')  
\#Running the Model  
import numpy as np  
from tensorflow import keras  
from tensorflow.keras.preprocessing import image

\# predicting images

path = os.path.join('c:/', 'Users', 'USER', 'Downloads')

img = image.load\_img(path, target\_size=(300, 300))

x = image.img\_to\_array(img)

x = np.expand\_dims(x, axis=0)

images = np.vstack(\[x\])

classes = loaded\_model.predict(images, batch\_size=10)

print(classes\[0\])

if classes\[0\]&gt;0.5:

print(""It is a fox"")

else:

print(""It is a cat"")

&amp;#x200B;

and here are the errors:

&amp;#x200B;

Traceback (most recent call last):

File ""c:/Users/USER/Desktop/obj-classification/obj-class.py"", line 19, in &lt;module&gt;

img = image.load\_img(path, target\_size=(300, 300))

File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\keras\_preprocessing\\image\\[utils.py](https://utils.py/)"", line 110, in load\_img

img = pil\_image.open(path)

File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\PIL\\[Image.py](https://image.py/)"", line 2878, in open

fp = [builtins.open](https://builtins.open/)(filename, ""rb"")

PermissionError: \[Errno 13\] Permission denied: 'c:/Users\\\\USER\\\\Downloads'

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

Thank you",1,tensorflow,2020-10-13
j2i582,Bug with starting the Tensorflow Developer Certificate Exam.,"I attempted to write the Tensorflow Developer certificate Exam last week. So how the exam works is you have to pay for the exam and start the exam environment on the Trueability portal and then start the exam inside pycharm using the Tensorflow Exam plugin.   

I had done this before and it worked fine on the previous attempt but on this attempt i couldnt start the exam because the plugin gave a message saying "" We dont have your exam ready right now. Please try again later."" I contacted the support at Trueability and they said everything was fine on their end and it is a problem that has to be resolved by the Tensorflow certification team.   
It is not a problem with pycharm as i was using the latest version and had already given an attempt on that version before. I basically couldnt start the exam and the exam environment timed out on the Trueability portal. Even now if i click on the exam plugin it still gives me the same message about the exam not being ready 


Now I emailed them multiple times at their support email at tensorflow-certificate-team@google.com but i have not received a response for over a week now. I dont know how to proceed with getting my issue fixed and was wondering if anyone here could help get in touch with the team behind the certification exam.",1,tensorflow,2020-10-13
j2gn4k,Is there a way to create a tensor in tensorflow such that some elements are constants and some are variables?," I am implementing a simple SGD, and the tensor is used to calculate a custom loss function.",6,tensorflow,2020-10-13
j2fdoc,Google Open-Sources TensorFlow Recommenders (TFRS): Helping Users Find What They Love,"‘Recommendation’ is something you come across every day on almost every online platform, from the morning news stories to the late-night online TV shows. The personalized suggestions “Guess you like” or “You might like” are given using AI-powered technology, predicting a user’s general behavior and preferences. But technology is developing day by day and enhancing its productivity. Thus, Google, being one of the leading companies in recommender system research, development, and deployment, has recently introduced TensorFlow Recommenders (TFRS), a new open-sourced TensorFlow package.

Github: https://github.com/tensorflow/recommenders

Summary: https://www.marktechpost.com/2020/09/29/google-open-sources-tensorflow-recommenders-tfrs-helping-users-find-what-they-love/",20,tensorflow,2020-10-13
j21ocl,Neural Networks on Tabular data,"Hello, I had previously asked a question regarding the use of tensorflow and neural network architectures on tabular data. Tabular data meaning, structured excel or csv type format with columns and rows with numbers in each column. No images, no text, just simply data that I could be using for scikit-learn classifiers and regressors. I have a tabular dataset with 449,371 rows, and previously I had done a project with 316,800 rows, which ended up being horrific. My loss was “NAN” and my accuracy would stay at 0.25, ive sent pictures of this issue a few weeks ago, and I was told that maybe my problem was that the data wasn’t “deep enough” for deep learning. Is this true? I mean I have a 450k rows of data, is this truly not deep enough? Does sklearn solve this issue better than tensorflow?",2,tensorflow,2020-10-13
j20utn,Benchmarking tensorflow on NVIDIA GeForce RTX 3090,,4,tensorflow,2020-10-13
j1zhh9,How to predict images using a saved model?,"Hi

I'm trying to run the predict method using the saved model, so that the nn doesn't repeat training, 

but it's not working and the training is getting repeated every time I run the code.

I'm still new to TensorFlow, so any explanation would be helpful. 

Thanks

Here is the code:

 

import os   
\# Accessing the directory of our training cat pictures  
train\_cat\_dir = os.path.join(""c:/"", 'Users', 'USER', 'Desktop', 'fox-or-cat', 'cats')  
\# Accessing the directory of our training fox pictures  
train\_fox\_dir = os.path.join(""c:/"", ""Users"", ""USER"", ""Desktop"", ""fox-or-cat"", 'foxes')  
\# Accessing the directory of our validation cat pictures  
validation\_cat\_dir = os.path.join(""c:/"", ""Users"", ""USER"", ""Desktop"", ""validation-fox-or-cat"", 'cats')  
\# Accessing the directory of our validation fox pictures  
validation\_fox\_dir = os.path.join('c:/', 'Users', ""USER"", ""Desktop"", ""validation-fox-or-cat"", 'foxes')  
import tensorflow as tf   
model = tf.keras.models.Sequential(\[   
 \# Note the input shape is the desired size of the image 300x300 with 3 bytes color  
 \# This is the first convolution  
tf.keras.layers.Conv2D(16, (3,3), activation='relu', input\_shape=(300, 300, 3)),  
tf.keras.layers.MaxPooling2D(2, 2),  
 \# The second convolution  
tf.keras.layers.Conv2D(32, (3,3), activation='relu'),  
tf.keras.layers.MaxPooling2D(2,2),  
 \# The third convolution  
tf.keras.layers.Conv2D(64, (3,3), activation='relu'),  
tf.keras.layers.MaxPooling2D(2,2),  
 \# The fourth convolution  
tf.keras.layers.Conv2D(64, (3,3), activation='relu'),  
tf.keras.layers.MaxPooling2D(2,2),  
 \# The fifth convolution  
tf.keras.layers.Conv2D(64, (3,3), activation='relu'),  
tf.keras.layers.MaxPooling2D(2,2),  
 \# Flatten the results to feed into a DNN  
tf.keras.layers.Flatten(),  
 \# 512 neuron hidden layer  
tf.keras.layers.Dense(512, activation='relu'),  
 \# Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('foxes')  
tf.keras.layers.Dense(1, activation='sigmoid')  
\])

&amp;#x200B;

model.summary()  
from tensorflow.keras.optimizers import RMSprop  
model.compile(loss='binary\_crossentropy',  
optimizer=RMSprop(lr=0.001),  
metrics=\['accuracy'\])  
from tensorflow.keras.preprocessing.image import ImageDataGenerator  
\# All images will be rescaled by 1./255  
train\_datagen = ImageDataGenerator(rescale=1/255)  
validation\_datagen = ImageDataGenerator(rescale=1/255)  
training\_image\_dir = os.path.join('c:/', 'Users', 'USER', 'Desktop', 'fox-or-cat')  
validation\_image\_dir = os.path.join('c:/', 'Users', 'USER', 'Desktop', 'validation-fox-or-cat')

&amp;#x200B;

\# Flow training images in batches of 128 using train\_datagen generator  
train\_generator = train\_datagen.flow\_from\_directory(  
training\_image\_dir, # This is the source directory for training images  
target\_size=(300, 300),  # All images will be resized to 150x150  
batch\_size=128,  
 \# Since we use binary\_crossentropy loss, we need binary labels  
class\_mode='binary')  
\# Flow training images in batches of 128 using train\_datagen generator  
validation\_generator = validation\_datagen.flow\_from\_directory(  
validation\_image\_dir,  # This is the source directory for training images  
target\_size=(300, 300),  # All images will be resized to 150x150  
batch\_size=32,  
 \# Since we use binary\_crossentropy loss, we need binary labels  
class\_mode='binary')

&amp;#x200B;

\### Training  
history = model.fit(  
train\_generator,  
steps\_per\_epoch=7,    
epochs=50,  
verbose=1,  
validation\_data = validation\_generator,  
validation\_steps=7)

 

model.save('my\_project\_model') 

loaded\_model = tf.keras.models.load\_model('my\_project\_model')  


 \#Running the Model  
import numpy as np  
from keras.preprocessing import image  
   
\# predicting images  
path = os.path.join(""c:/"", 'Users', 'USER', 'Downloads')   
img = image.load\_img(path, target\_size=(300, 300))  
x = image.img\_to\_array(img)  
x = np.expand\_dims(x, axis=0)  
images = np.vstack(\[x\])  
classes = loaded\_model.predict(images, batch\_size=10)  
print(classes\[0\])  
if classes\[0\]&gt;0.5:  
 print(""It is a fox"")  
else:  
 print(""It is a cat"")",0,tensorflow,2020-10-13
j1yg2z,How to Run DeepSORT Object Tracking with YOLOv4 and TensorFlow in Google Colab,,21,tensorflow,2020-10-13
j1sjpz,What's the easiest way to setup some NER API?,"Found the named entity recognition services from Google and Azure way too expensive and wonder if and how I could do it myself. I'm a TS dev but don't have neither tensorflow nor Python skills. is the tensorflow js an opton, does it have a good ecosystem?",3,tensorflow,2020-10-13
j1rnaa,How many layers does my NN have?,"Hey guys! So I have this assignment to train a very simple neural network. Our dataset has 6 features that are fed into the network and we are required to train it and then predict one output number. The professor gave us the code and basically told us to learn by ourselves lol. So my doubt is, in the following code, in which the layers for the neural network are defined, does the first dense layer defined (the one with 50 neurons) corresponds to the input layer, or is it the first hidden layer?

Thanks in advance!

    def get_compiled_model():
    model = tf.keras.Sequential([      tf.keras.layers.Dense(50, activation='relu', input_shape=(6,)),     tf.keras.layers.Dense(30, activation='relu'),     tf.keras.layers.Dense(30, activation='relu'),     tf.keras.layers.Dense(1, activation='linear'),   
    ])

 ",2,tensorflow,2020-10-13
j1rfmr,#Excellent,,0,tensorflow,2020-10-13
j1pkgv,Tensorflow training total tasks stays at 1/1... instead of going through the data,"Hello! I just created a model and I am loading location data out of a CSV. I then put all of the location data into one list, with each index being another list of a pair of X and Y cords.  Finally, I made it into a NumPy array with np.asarray, and printing it out looks like how I want it, 2 columns (x, y), and many rows down.

&amp;#x200B;

\[\[0.03284732 0.71823503\]

 \[0.         0.80737603\]

 \[0.53450931 0.6075211 \]

 \[0.56680244 0.89441298\]

...

 \[0.75476707 0.57551977\]

 \[0.96351955 0.16136675\]\]

&amp;#x200B;

When I train the model, instead of going through the different data points, it just says 1/1. I set the Epoch to be high because it's the only way it actually trains. For example:

&amp;#x200B;

Epoch 992/1500

1/1 \[==============================\] - 0s 182us/step - loss: 0.3187 - accuracy: 0.8333

&amp;#x200B;

If there's anyone who knows how to fix this please let me know. This is my first real model not following a tutorial.",2,tensorflow,2020-10-13
j1id4b,Why is everybody using tf on Linux?,"Hey there,

I am kinda new to tensorflow, just started using it for a new project using rnn for text understanding, and I was wondering why everybody is using it with linux, and whether I should switch.

Now I know the dev environment is kinda biased for linux, but honestly I've always used Windows for most my stuff, and never had much problem with it. I am not here asking for an endorsement of linux for development, just wondering if there are any particularities or features of tf that I'd only get on linux, compared to windows.",11,tensorflow,2020-10-13
j1fmox,What is the best distro for tensorflow GPU,"Hello,

What is the best distro to be installed for TensorFlow GPU with NVIDIA GeForce 1050 Ti ?",0,tensorflow,2020-10-13
j1eqir,What would the best neural network type to use for a self driving car?,"I'm attempting to create a a self driving car AI using TensorFlow. The car itself is really simple its just a rectangle on a 2D track, it 'sees' its environment with 8 distances from centre of the car to the track these 8 distances are spread out 45 degrees between them. The information I plan on giving the network are these 8 distances, the speed of the car and the acceleration/deceleration rate. I was originally thinking of using a simple deep neural network, what do other people think to this idea? Thank you for any suggestions or improvements!",0,tensorflow,2020-10-13
j1aejp,What do I need to yield for a validation split in my own generator?,"I'm using my own generator for training and I want to do the shuffling and selecting of validation data from my main dataset without having to parse it into a separate validation set on disk. 

Does the fit method recognize some format as being data for validation? Do I just give it two tuples and it will take the second tuple as validation data?",2,tensorflow,2020-10-13
j19sae,"tf-agents: to replicate multi-agent environments, could an approach with two custom environments simultaneously interacting with game engine via twisted clients work? (Plus an AttributeError in utils.validate_pyenvironment(), time_step is None type)","Hi there,

I'm new to RL, and am looking at creating custom environments for two agents playing against each other in a game I made with tf-agents. I'm aware tf-agents doesn't seem to handle multi-agent environments out of the box (and I'm using windows so RLlib isn't an easy option). I've read a few tutorials about custom environments, and am doing my best to hash something workable together.

The game I'm trying to train agents on needs the moves of both players, which are essentially coordinates of the pieces they control, and then calculates the final position following those moves. The reason I'm using twisted is to handle this sort of asynchronous issue- the \_step() method can't just send a move to the game and return the result, as the result is only returned if the game has both sets of moves- my thought was with server-client stuff, the \_step() method can just send the moves, and the client can wait to receive the result.

In the below code you can see a custom environment, which connects to a server on localhost (which is running in the script which runs the game). The \_step(action) method would send the action to the game, which, once both players moves have been collected, calculates the result and returns a result and observation spec through the connection. Ideally the dataReceived() method unpacks the result, and ticks on the time\_step. 

Here a move will consists of a 6x2 array with values between 0 and 1 (which represent destination coordinates for the relevant players in the game as a proportion of the screen's pixels). self.\_state and the observation spec is a 13x4 array, which again represents the position of players as a proportion of the screen pixels, plus some other info.

Here's example of the basic environment I'm aiming for, the two custom environments will be the same save for the reward values based on the observation:

     class TrainClient(Protocol):
    
        def __init__(self):
            self.moves = []
    
        def connectionMade(self):
            pass
    
        def dataReceived(self, data):
    
            response = pickle.loads(data)
            
            if response[0] == ActionState.CHANGE_POSSESSION:
                return time_step.termination(response[1], 10)
    
            elif response[0] == ActionState.DOWN_INCREASE:
                return time_step.transition(response[1], reward=0.05, discount=1.0)
    
            elif response[0] == ActionState.TOUCHDOWN:
                return time_step.termination(response[1], -10)
    
            elif response[0] == ActionState.VALID_MOVE:
                return time_step.transition(response[1:], reward=0, discount=0)
    
    
    class TrainClientFactory(ClientFactory):
        def startedConnecting(self, connector):
            print('Started to connect.')
    
        def buildProtocol(self, addr):
            print('Connected.')
            return TrainClient()
    
        def clientConnectionLost(self, connector, reason):
            print('Lost connection.  Reason:', reason)
    
        def clientConnectionFailed(self, connector, reason):
            print('Connection failed. Reason:', reason)
    
    
    class ActionState(Enum):
        VALID_MOVE = 1
        ILLEGAL_MOVE = 2
        CHANGE_POSSESSION = 3
        DOWN_INCREASE = 4
        TOUCHDOWN = 5
        GAME_COMPLETE = 6
    
    
    class CrossEnv(PyEnvironment):
    
        def __init__(self, connection):
            self._action_spec = array_spec.BoundedArraySpec((6, 2), np.float, minimum=0, maximum=1, name='moves')
            self._observation_spec = array_spec.BoundedArraySpec((13, 4), np.float, minimum=0, maximum=1, name='field')
            self._state = [[0.9, 0.18759375, 0., 0.],
                           [0.125, 0.18759375, 0., 0.],
                           [0.5835, 0.18759375, 0., 0.],
                           [0.4165, 0.18759375, 0., 0.],
                           [0.5, 0.18759375, 0., 0.],
                           [0.9, 0.0939375, 0., 0.],
                           [0.125, 0.0939375, 0., 0.],
                           [0.5835, 0.0939375, 0., 0.],
                           [0.4165, 0.0939375, 0., 0.],
                           [0.5, 0.04696875, 1., 0.],
                           [0.5215, 0.04696875, 0., 0.],
                           [1., 0., 0., 0.],
                           [0., 0., 0., 0.]]
            self._episode_ended = False
            self.connection = connection
    
        def action_spec(self):
            return self._action_spec
    
        def observation_spec(self):
            return self._observation_spec
    
        def _reset(self):
            self._state = [[0.9, 0.18759375, 0., 0.],
                           [0.125, 0.18759375, 0., 0.],
                           [0.5835, 0.18759375, 0., 0.],
                           [0.4165, 0.18759375, 0., 0.],
                           [0.5, 0.18759375, 0., 0.],
                           [0.9, 0.0939375, 0., 0.],
                           [0.125, 0.0939375, 0., 0.],
                           [0.5835, 0.0939375, 0., 0.],
                           [0.4165, 0.0939375, 0., 0.],
                           [0.5, 0.04696875, 1., 0.],
                           [0.5215, 0.04696875, 0., 0.],
                           [1., 0., 0., 0.],
                           [0., 0., 0., 0.]]
    
            self._episode_ended = False
            return time_step.restart(np.array(self._state))
    
        def _step(self, action):
    
            if self._episode_ended:
                return self.reset()
            list1 = ['cross']
            list1.append(action)
            moves = pickle.dumps(list1)
            print(f'action: {list1}')
            self.connection.transport.write(moves)
    
    
    connection = reactor.connectTCP('localhost', 8001, TrainClientFactory())
    cross_footballEnvironment = CrossEnv(connection)
    
    reactor.run()

Trying to validate the environment with

    utils.validate_py_environment(cross_footballEnvironment, episodes=5)

currently returns an attribute error on the time\_step:

`if time_step.is_last():`

`AttributeError: 'NoneType' object has no attribute 'is_last'`

Strangely the same code ran last night without error. I can't find the error online anywhere, but I assume it's is something to do with the time\_step transitions/terminations being in the twisted protocol method rather than the \_step method?

Would be interested if anyone has any thoughts on that error, but also if the more experienced out there have general thoughts about whether this approach could/should work? Or if it's a bad idea and there are better alternatives? 

Any thoughts appreciated!",1,tensorflow,2020-10-13
j19qx8,I wrote a blog on getting started with tensorflow serving. Please let me know if it's any good.,https://medium.com/@cleanpegasus/getting-started-with-tensorflow-serving-b03c130bdb5c,11,tensorflow,2020-10-13
j18f1v,TensorFlow Developer Directory stuck on august 31?,"I submitted my exam on september 12. Notified that I passed the exam after 2 seconds and got my certificate 3 days later but I'm still not in the TensorFlow Developer Network (which is the most important part I guess). 

They said ""You are now eligible to be listed among fellow TensorFlow developers in the TensorFlow Certificate Network. \[...\] **you will be added to our directory within 2 weeks of your submission.""** but still nothing.

Also the directory seems stuck on august, the most recent developer was certified on august 31. It seems very unlikely that nobody passed the exam in september.

Any idea on what's going on?",5,tensorflow,2020-10-13
j18d1j,Tensorflow Releases New Package For Recommendation Systems: TFRS,[https://analyticsindiamag.com/tensorflow-tfrs-recommenders-package/](https://analyticsindiamag.com/tensorflow-tfrs-recommenders-package/),3,tensorflow,2020-10-13
j18a7z,"Hi #Devs, I registered to attend India's largest developer conclave - ""DevFest India"". There are some amazing sessions by expert speakers and you can join me now! Register at: https://devfestindia.com/ #DevFestIndia #DevFest @DevFestIndia",,0,tensorflow,2020-10-13
j0ubhc,Overfitting when restarting colab?,"So I built a CNN image classifier yesterday, and I did it in google colab. I initially did it without a validation set, and I was getting decent accuracy on my training set of a 0.887 and loss of 0.2, but i wanted to go back through and add a validation set to check for overfitting as my model was training. So essentially I went back added a validation set and retrained, model during each epoch showed little signs of overfitting, as I the training accuracy and validation accuracy were within 0.1-0.2 and loss was close to each other as well. No drastic differences, however colab distrupted the training process and the runtime stopped, so my model stopped training last night at 50/100 epochs. No worries I thought, I’ll retrain it this morning so it had longer time to run. So now this was my 2nd time retraining the model with a validation set but this time there’s a huge discrepancy between my loss and accuracy between validation and training set. I don’t know if this is because I’ve fed the same model now twice, and now it’s overfitting it because I had fed it data on 50/100 epochs yesterday, but now I don’t know what’s wrong. Can a model overfit just from retraining it a couple times? I really have no control fo this as my runtime always disconnects when I’m not watching the colab. And hence I have to retrain it. Any advice? Does overfitting get caused by this?",1,tensorflow,2020-10-13
j0s938,Does Tensorflow work with RTX 3000 series?,"I've just started with DeepFaceLab / Machine Learning and was using a 1660 Super but sold it in anticipation of the RTX 3070 but apparently TensorFlow doesn't yet work with Nvidia's Ampere architecture. 

Is it correct to assume TensorFlow will be updated sooner rather than later?

I'm trying to decide if I should just get a 2070 Super instead.",11,tensorflow,2020-10-13
j0rfa6,Editing MrBeast with StyleGAN2,,2,tensorflow,2020-10-13
j0qtlw,When does Google update their TensorFlow developer certificate network website?,"I passed the exam in first week of September and can't seems to see my name in there. If anyone gave this exam earlier, how long did it take for you to get your details updated? I filled that form on same day when I got my result email.",11,tensorflow,2020-10-13
j0q2fx,My First Mediom post on Towards Data Science using Tensorflow,"Hi everyone! I'm happy to announce that I writed my first post in towards data science community, my post is about a virtual steering wheel using posenet, if you find it interesting, let me some feedback.

Thank you!

[https://towardsdatascience.com/virtual-steering-wheel-with-javascript-and-posenet-12439712a68](https://towardsdatascience.com/virtual-steering-wheel-with-javascript-and-posenet-12439712a68)",0,tensorflow,2020-10-13
j0o8ug,Fine-Tuning DistilBert for Multi-Class Text Classification using transformers and TensorFlow,,5,tensorflow,2020-10-13
j0jn31,Eras and TensorFlow does not work anymore after attempting to install TensorFlow GPU,"I recently got a new machine and it has a GPU that can run TensorFlow on it (RTX 2070). So natrually I decided to upgrade to the tensorflow-gpu version and start using that instead. I followed this tutorial: [**https://www.youtube.com/watch?v=zTGrt1oyul4**](https://www.youtube.com/watch?v=zTGrt1oyul4)**,** and the installation was successful. But the problem now is that after I uninstalled the original TensorFlow version, keras refuse to start and claims that the TensorFlow version has to be 2.2 or later, while I have tensorflow 2.3. Any Ideas on how to fix this? I tried reinstalling keras and now not even keras will import.",4,tensorflow,2020-10-13
j03epy,How to divide a dataset into training &amp; validation sets?,"Let's say the dataset consists of 1200 images, how many images should in each the training  &amp; validation sets be?",8,tensorflow,2020-10-13
j03djb,How to divide a dataset into training and validation sets?,"Let's say the dataset consists of 1200 images, how many images should in each of the training &amp; validation set be?

Thanks",1,tensorflow,2020-10-13
j01iwa,How to load a model with ckpt files?, I was running a GitHub code and the github doesnt have a proper read me file. When I trained the model I have ckpt-13000.index and ckpt-13000.data-00000-of-00001 file. I have worked with .h5 and .pb files but I am bemused with these type of files. Can anyone guide me that how can I load the model with the help of these files? The code is in Tensoflow 2.0,3,tensorflow,2020-10-13
izty5t,How to use tensorflow with esp32 cam,I want the esp32 cam to detect an object. When it detects the object i want it to turn on an led. How can I use tensorflow for this?,1,tensorflow,2020-10-13
izln3c,Advanced Tensorflow Data Input Pipelines: Handling Time Series,,9,tensorflow,2020-10-13
izdw8j,Issue with image data generator when compiling with model.,,9,tensorflow,2020-10-13
iz5ovz,Looking for tutorial on installing TF or TFlite on Ubuntu running in a VM,"I've exhausted Google, trying a number of the instructions there - but I have not yet had any success. Can this community point me to a decent  beginners tutorial on installing TF or TFlite on Ubuntu running in a VM?

If it helps, my goal is to process still images for person detection. These still images are taken by a Raspberry Pi running [Motion](https://github.com/Motion-Project/motion), and saved. Motion produces a lot of false positives, so I want to process the images for people to reduce useless notifications. I did get this method working on the Pi, but it was overheating etc - so I would like to (I think?) run TF on a WIN10 I5 machine that I am already running VirtualBox on for other projects. Thanks!",7,tensorflow,2020-10-13
iz1mj0,How can I add to a dataset from another dataset?,"    ds_train.cardinality()
    &lt;tf.Tensor: shape=(), dtype=int64, numpy=2016&gt;
    for i in range(5):
        aug = ds_train.map(augment_data)
        ds_train.concatenate(aug)
    ds_train.cardinality()
    &lt;tf.Tensor: shape=(), dtype=int64, numpy=2016&gt;
    

I don't understand why the dataset is not getting bigger",1,tensorflow,2020-10-13
iyw977,TensorFlow Lite With Platform.io and the ESP32,,17,tensorflow,2020-10-13
iyrmg7,How to learn Tensorflow?,"Hi people. I have done the following to try and master Tensorflow. 
Tensorflow specialization in coursera
Tensorflow documents
Hands on with scikit learn, keras and tensorflow
Deep learning with MIT

But still, I am not comfortable with it. Each time I see a model defined differently, I get confused. And the fact that I can't personally ask doubts to anyone immediately does me no good. A lot of doubts pop up in my brain but all I could find in the internet is just the code and not what's under the hood. A lot of you people must be really good at TensorFlow. Can someone tell ne the right way to learn? If you don't believe there is any 'right' way in doing a task, please share the way you did it.
Thanks guys. Hope I find my solution here",9,tensorflow,2020-10-13
iyoc8i,State of the art in 3D dense face alignment!,,2,tensorflow,2020-10-13
iylh1m,[Question] Is there anything necessarily wrong with averaging gradients over batches to simulate a larger batch size?,"I've been trying to replicate some results I read in a paper, but it depends on a large batch size of 256 and my GPU can only handle about 32. I created a training loop which gets the gradients over the 32-batch sample, stores them, then after 8 steps (32\*8 = 256) I average the gradients over each layer, and then call on the optimizer to update weights with the 8\*32-averaged gradient list. The model is training fine and starting to match the results from the paper, but this feels like it's too easy to be right.",3,tensorflow,2020-10-13
iyfhq8,Twitter toxicity detector using Tensorflow.js,"Hi! I recently finished this project, I hope you find it interesting :)

[https://github.com/MCarlomagno/TwitterToxicityDetector](https://github.com/MCarlomagno/TwitterToxicityDetector)",2,tensorflow,2020-10-13
iyeejw,"Training AlexNet like structure on entire ImageNet in Tensorflow 2.0, Problem with flat loss"," 

I am trying to replicate AlexNet model on ImageNet (for learning purposes). The dataset is 1.2 million ImageNet dataset with 50K validation. This is my code :

model function:

&amp;#x200B;

https://preview.redd.it/bi1armh2kxo51.jpg?width=1172&amp;format=pjpg&amp;auto=webp&amp;s=241370dd82723a18951c2a328d649d0cc81a553f

 training function: 

&amp;#x200B;

https://preview.redd.it/6d2o6595kxo51.jpg?width=1406&amp;format=pjpg&amp;auto=webp&amp;s=75a6455d1ab11a2d646ba4dd0d8749db3d0cf2ed

 However, when I start to train this model (8 GPUs, Batch size=32\*nGPU) I get flat loss and accuracy: 

&amp;#x200B;

https://preview.redd.it/3wv57f38kxo51.jpg?width=978&amp;format=pjpg&amp;auto=webp&amp;s=64167df852aa303f0fcfc41cdb637f26daf254e6

 Any idea what's the problem causing flat loss? 

# Code can be found [here](https://github.com/anejad/Convolutional-Neural-Network-Champions/blob/master/AlexNet/AlexNet_Tensorflow_Full.py)",3,tensorflow,2020-10-13
iy9sol,Tensorflow 1.1x vs Tensorflow 2.x,"It seems Tensorflow 2 has been recieved well overall. I have personally enjoyed the convenience of eager execution and the added flexibility in tf.keras API in my recent experiments.

But I'm curious about the opinion of long-time Tensorflow users. Do you still prefer/like Tensorflow 1.1x over the new API? And if yes, what are the reasons?",12,tensorflow,2020-10-13
iy7mpo,Physically playing Google's T-Rex Offline Game with a PoseNet,,3,tensorflow,2020-10-13
iy6drm,interpolating between stylegan2 generated billionaires,,18,tensorflow,2020-10-13
iy0312,Why does pytorch preform better?,So I'm implementing a CycleGAN in tensorflow and I'm wondering why pytorch preforms better by miles? Also is it worth switching to pytorch for these reasons?,2,tensorflow,2020-10-13
ixu5c3,Use keras model from c++,"I have a trained a classic CNN(pre-trained mobile net) for image classification. I want to now use this model from c++. From my understanding, I need to create a library of the model, that can accept the input and return its outputs. I have the model saved in format .pb  
 (SavedModel).

I have already tried, [CppFlow](https://github.com/serizba/cppflow), where the error shows that it can't read my model. I assume it's due to incompatibility with TF 2.0.

I have also got the command line interface of [SavedModel](https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface) working, but I don't know how to use it in my cpp application. I'm not sure how connect the tensorflow C Api with my program and model.

I want to know how I can build a library of my model and use this library such that it can make predictions on the fly. Any guidance will be helpful. Please let me know if any additional information is required.

&amp;#x200B;

PS: I don't think connecting python with c++ is a feasible solution because I'll be performing classification many times on a single frame of a video. This will drastically slow down my program.",4,tensorflow,2020-10-13
ixpuh1,Statistics knowledge helps master deep learning?,"Hey everyone, I’m currently a sophomore at my college pursuing an undergraduate degree. I’m a statistics Major double minoring in computer science and economics, when originally I was a data analytics major. I decided to switch because I wanted a deeper understanding of statistics and a more solid foundation so I can further understand algorithms in depth when I intent to study/practice deep learning. Do any of you feel that having a deeper understanding of statistics helped you in anyway when you were learning deep learning or implementing tensorflow models? Thanks.",4,tensorflow,2020-10-13
ixpr5f,Formulating Loss as a Unit Test,,2,tensorflow,2020-10-13
ixpdcf,How to load a model from a locally saved .tar.gz file?,"Hey guys, I'm a newbie and using TF's object detection tool. The default code for loading a model looks like this:


 def load_model(model_name):

    base_url = 'http://download.tensorflow.org/models/object_detection/'
    
    model_file = model_name + '.tar.gz'
    
    model_dir = tf.keras.utils.get_file(
        fname=model_name, 
        origin=base_url + model_file,
        untar=True)

    model_dir = pathlib.Path(model_dir)/""saved_model""

    model = tf.saved_model.load(str(model_dir))

    return model

It looks like it's downloading a saved .tar.gz model from the internet but I have one saved locally that I'd like to use. Can anyone provide guidance on what that would look like?

Any help would be greatly appreciate! Thank you!",1,tensorflow,2020-10-13
ixn4me,Has anyone managed to make a tfx pipeline for kubeflow where the trainer objects autoscales,"I want to train a model with a tfx/kubeflow pipeline, the idea is when the model itself is being trained the cluster will auto scale to train the model faster and cheaper. 
Is there any examples of people doing this? 
Is it even possible?",8,tensorflow,2020-10-13
ixmv16,Neural Network Car Driving Game using Tensorflow.js,"Hi! I recently uploaded this github repo, please let me some feedback. I hope you'll find it useful :)

[https://github.com/MCarlomagno/CarDrivingResNet](https://github.com/MCarlomagno/CarDrivingResNet)",9,tensorflow,2020-10-13
ixjtqj,Clustering of simple data leads to degenerate results,"Hi, I attempted clustering the famous Iris data set with a custom TF layer. However, the clustering fails with the centers being pretty much on top of each other or completely away from the data. Since I have relatively little experience with Tensorflow, I'm not sure which is to blame, my implementation or the method itself. I know it is certainly not anyone else's job to find out which it is and why, but I'm truly stuck. I would appreciate any pointers if you have some time to spare!

The method is based on cluster hardening ([Theano source](https://github.com/ElieAljalbout/Clustering-with-Deep-learning), also contains research article), meaning that each update the network should enforce stricter cluster assignments and update cluster centers towards better values. A PCA-reduced (2D) representation in which the data still has clear clusters is used as an input to the clustering model. But as described above, this doesn't work. Here's the data and cluster movements.

&amp;#x200B;

[Data and cluster centers each epoch](https://preview.redd.it/rzksklk91oo51.png?width=500&amp;format=png&amp;auto=webp&amp;s=2656f1bcd23633c5e156b8751f983a2774e45ecd)

Here's my implementation. Bear with me, the code is a bit lengthy, because I wanted to leave lots of comments and a good visualisation of the results. The model definition itself with TF logic is quite short. In addition to TF 2, it requires `numpy`, `matplotlib` and `sklearn`. So, first the model definition.

    import numpy as np
    import tensorflow as tf
    
    from tensorflow import keras as k
    from tensorflow.keras.optimizers import Adam
    
    from matplotlib import pyplot as plt
    from matplotlib.colors import Normalize
    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA
    
    class ClusterHardening(k.layers.Layer):
        def __init__(self, n_clusters: int, **kwargs):
            super().__init__(**kwargs)
            self.n_clusters = n_clusters
            self.centers = None
    
        def build(self, input_shape):
            self.centers = self.add_weight(
                'centers',
                [self.n_clusters, int(input_shape[-1])],
                initializer='random_normal',
                trainable=True,
            )
    
        def call(self, inputs, training=False, mask=None):
            """"""
            Optimise for stricter cluster assignments.
    
            Tensor shapes:
            inputs: (n_inputs, 1, latent)
            center: (1, n_center, latent)
            dist:   (n_inputs, n_center)
            q, p:   (n_inputs, n_center)
            loss:   (1,)
            """"""
            inputs = tf.expand_dims(inputs, axis=-2)
            centers = tf.expand_dims(self.centers, axis=-3)
            dist = tf.norm(inputs - centers, axis=-1)
            q_ij = self.student_q(dist)
            p_ij = self.target_p(q_ij)
            kl = self.kullback_leibler(p_ij, q_ij)
            self.add_loss(tf.reduce_sum(kl))
    
            return q_ij
    
        @tf.function
        def student_q(self, dist):
            """"""
            Membership degree.
    
            Q: (n_inputs, n_center) / (n_inputs, 1)
            """"""
            # t-distribution with nu = 1, can be reduced by the common factor in division
            q_ij_num = (1 + dist ** 2) ** -1
    
            # Q normalised summing clusters for each point
            # -&gt; total membership of each data point = 1
            return q_ij_num / tf.reduce_sum(q_ij_num, axis=-1, keepdims=True)
    
        @staticmethod
        @tf.function
        def target_p(q_ij):
            """"""
            Target membership distribution.
    
            P: (n_inputs, n_center) / (1, n_center) / (n_input, 1)
            """"""
            # Squared: values close to zero reduce further
            # Normalised summing original data points for each cluster
            # -&gt; Forces some points to belong to a cluster anyway
            p_ij_num = q_ij ** 2 / tf.reduce_sum(q_ij, axis=0, keepdims=True)
    
            # P normalised summing clusters for each point
            # -&gt; total membership of each data point = 1
            return p_ij_num / tf.reduce_sum(p_ij_num, axis=-1, keepdims=True)
    
        @staticmethod
        @tf.function
        def kullback_leibler(p, q):
            """"""Difference between distributions.""""""
            return p * tf.math.log(p / q)
    
    
    class Clustering(k.Model):
        def __init__(self, n_clusters: int, **kwargs):
            super().__init__(**kwargs)
            self.cluster = ClusterHardening(n_clusters)
    
        def call(self, inputs, training=False, mask=None):
            return self.cluster(inputs)

Then training and visualisation.

    lr = 1e-2
    n_epochs = 100
    latent_dim = 2
    n_clusters = 3
    
    x, y = load_iris(return_X_y=True)
    x = (x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0))
    x = PCA(n_components=latent_dim).fit_transform(x)
    
    adam = Adam(lr)
    model = Clustering(n_clusters)
    model.compile(adam)
    
    h = {'loss': []}
    centers = np.zeros((n_epochs, n_clusters, latent_dim))
    for epoch in range(n_epochs):
        print(f'epoch {epoch} / {n_epochs}')
        h_ = model.fit(x, None, batch_size=x.shape[1], epochs=1, verbose=0).history
        h['loss'].extend(h_['loss'])
        centers[epoch] = model.cluster.centers.numpy()
    
    groups = model.predict(x)
    groups = np.argmax(groups, axis=1)
    
    plt.figure(figsize=(5, 3))
    norm = Normalize(0, 10)
    plt.scatter(x[:, 0], x[:, 1], s=1, c=groups, cmap='tab10', norm=norm)
    plt.scatter(centers[0, :, 0], centers[0, :, 1], s=20, c='k', marker='o')
    plt.scatter(centers[-1, :, 0], centers[-1, :, 1], s=20, c='k', marker='x')
    for c in range(n_clusters):
        plt.plot(centers[:, c, 0], centers[:, c, 1])
    plt.tight_layout()
    
    plt.figure(figsize=(4, 3))
    for v in h.values():
        plt.plot(range(n_epochs), v)
    plt.legend(h.keys())
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.tight_layout()
    
    plt.show()",4,tensorflow,2020-10-13
ixifkq,How to convert Tensor into NumPy array,"Hello everyone, I have trained `ResNet50` model on my data. I want to get the output of a custom layer while making the prediction. I tried using the below code to get the output of a custom layer, it gives data in a tensor format, but I need the data in a `NumPy array` format. I tried to convert the tensor to NumPy array but getting errors

Can anyone share some thoughts, any advice will be very helpful

    from keras.models import load_model
    import tensorflow as tf
    import numpy as np
    
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    tf.Session(config=config)
    
    model = load_model(model_path) # load trained model
    
    data = load_data(data_path) # load data for predictions
    result = model.predict(data)
    print(type(result_dev))
    #&lt;class 'numpy.ndarray'&gt; 
    
    result = model.get_layer('avg_pool').output
    print(type(result))
    #&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;

Things I tried

**Option 1**

    result = result.numpy()

&gt;AttributeError: 'Tensor' object has no attribute 'numpy'

**Option 2**

    result = result.eval(session=tf.compat.v1.Session()) 

&gt;2020-09-22 11:21:59.522138: I tensorflow/stream\_executor/cuda/cuda\_gpu\_executor.cc:983\] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  
&gt;  
&gt;2020-09-22 11:21:59.522343: I tensorflow/core/common\_runtime/gpu/gpu\_device.cc:1618\] Found device 0 with properties: 

Dependency Installed:

    tensorflow-gpu==1.15.0",8,tensorflow,2020-10-13
ixgm51,Push keras logs to smartphone for monitoring,,6,tensorflow,2020-10-13
ixccfh,Getting started with tensorflow,"Hey guys, after realizing my teachers don’t give a shit about teaching anymore since we are remote, I decided to learn machine learning, more specifically deep learning. Last few days I have been reading up on neural networks and tensorflow, and I was wondering what you guys think the best way is to get started with tesnorflow? In other words, what is most basic beginner project to do, to understand the fundamentals? Thanks",4,tensorflow,2020-10-13
ix9hew,Help with the MSG-GAN paper,"I am trying to implement the technique described in the MSG-GAN paper:

https://arxiv.org/pdf/1903.06048.pdf

But I am having difficulty understanding some things, for example, how are the connections made from the generator to the discriminator? These are Conv2D connections literally? (in that case, how would I insert the real images to train the discriminator?) Or does the discriminator have multiple outputs (one prediction for each resolution and the generator has to optimize the average loss of the resolutions)?",2,tensorflow,2020-10-13
ix4wox,Object detection api model to tf.keras model,"I want to convert a model from tensorflow object detection api to a keras model. Is there any way to do it?

I have provided some details regarding this issue in the below link.

[Coverting object detection api model to keras model](https://stackoverflow.com/questions/63990388/is-there-any-way-to-convert-a-model-from-tensorflow-object-detection-api-into-a)


Any suggestions on how to do this are really welcome.",5,tensorflow,2020-10-13
ix1ed3,Why TensorFlow?,,0,tensorflow,2020-10-13
iwzx05,Need advice on deep Q learning w/ utilizing GPU!,"Hey, so since on deep q learning, we train the model from memory array one batch at a time during one action cycle, it seems like the GPU isn't really used at all. My analysis on this issue might not be accurate, but

Any advice on efficiently using GPU for Deep Q Learning would be welcome!",8,tensorflow,2020-10-13
iwxmyz,Does GTX 1660S work for tensorflow-gpu?,title,0,tensorflow,2020-10-13
iwscxi,How to best use tf.gather/gather_nd to collect along inner dimension,"I have a 4d tensor output for an object detector that outputs per-pixel, per class box predictions, i.e. Shape H x W x C x 6, where the innermost 6 wide dimension is box parameters for that class. Now, when computing loss, I want to update only the predictions from the ground truth class. To do this, I'd like to have a tensor with shape H x W whose elements are the ground truth class index. This tensor is then used to extract the relevant class only from the input, outputting a tensor with H x W x 6. I know this should be possible using gather or gather\_nd but I can't get the parameters right to get the desired output. Plus I'm confused about the purpose of the batch\_dims parameter for gather\_nd, that may be relevant though to solving this. Any suggestions on how I can properly use these or some other tf function to achieve this result?

&amp;#x200B;

For a single element example, here's an example input/output I'm looking for:

`Prediction tensor: [[[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]]]] # 1 height X 1 width X 2 classes # 6 values/class`

`Ground truth class tensor: [[1]] # 1, 1 has class 1`

`Output:  [[[7, 8, 9, 10, 11, 12]]]`",2,tensorflow,2020-10-13
iwr39o,TF 2.0 object detection API data augmentation options,Is there a comprehensive list of all the data augmentation options available in the object setection api?,2,tensorflow,2020-10-13
iweqv0,How to get output of custom layer while making prediction,"Hello everyone, I have trained [ResNet50](https://keras.io/api/applications/resnet/) model on my data (having 5 classes). I want to get output of custom layer while making prediction

I am using below code for making predictions, It gives me last layer output as expected, but I want the output of other layers

    from keras.models import load_model
    
    model = load_model(model_path)
    result = model.predict(data) # data variable is of size 1000
    print(result.shape)
    #(1000, 5)

What changes I have to make in the code to get custom layer output, any suggestion would be very helpful",5,tensorflow,2020-10-13
iwekoy,interpolating between stylegan generated presidents,,4,tensorflow,2020-10-13
iw9wjy,Failed to convert a NumPy array to a Tensor,"Hello,

I am trying to do a machine learning task and in my current dataset I have featurized protein sequences, so for every protein I have 27 features which are float arrays of different length. So, I have a numpy array of shape (556,1) and for every sample it have 27 features so (27,1) and each feature has variable length and the same feature might have a different shape for different samples. Trying to use this data with sklearn gives the following error ValueError: setting an array element with a sequence  
and using it with keras gives the following error ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)  
. If i try to convert it with input = input.astype(np.float32),  
i get the same error ValueError: setting an array element with a sequence.

Any idea how to fix this or how to I get this data working? I thought about padding each feature to equal length but some features are too big about 10k length so Im skeptical about doing that. Any help would be appreciated.",6,tensorflow,2020-10-13
ivz4mq,"ICYMI: A new browser extension for finding code for ML research papers on the internet (on Google, Arxiv, Scholar, Twitter, Github)",,8,tensorflow,2020-10-13
ivxzpy,How to make a model output a list of numbers?,"Hey, so im pretty new to machine learning and tensorflow, and i try to create my own first application with ai: a bot that can play tetirs on a specific website. It all goes pretty well, the only thing i struggle with is how to make the model output a list of integer values representing actions which then get translated to keypresses. When I try to fit the model, i get the following error:

\`\`\`

TypeError: float() argument must be a string or a number, not 'list'

&amp;#x200B;

The above exception was the direct cause of the following exception:

&amp;#x200B;

Traceback (most recent call last):

File ""[train.py](https://train.py)"", line 47, in &lt;module&gt;

[model.fit](https://model.fit)(input\_data, output\_data, epochs=250)

File ""C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\_core\\python\\keras\\engine\\[training.py](https://training.py)"", line 727, in fit

use\_multiprocessing=use\_multiprocessing)

File ""C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\_core\\python\\keras\\engine\\training\_arrays.py"", line 675, in fit

steps\_name='steps\_per\_epoch')

File ""C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\_core\\python\\keras\\engine\\training\_arrays.py"", line 394, in model\_iteration

batch\_outs = f(ins\_batch)

File ""C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\_core\\python\\keras\\[backend.py](https://backend.py)"", line 3461, in \_\_call\_\_

dtype=tensor\_type.as\_numpy\_dtype))

File ""C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\\_asarray.py"", line 85, in asarray

return array(a, dtype, copy=False, order=order)

ValueError: setting an array element with a sequence.

\`\`\`

Here is the code:

\`\`\`

input\_data = np.asarray(input\_data)output\_data = np.asarray(output\_data)

&amp;#x200B;

input\_data = tf.keras.utils.normalize(input\_data, axis=1)input\_test = tf.keras.utils.normalize(input\_test, axis=1)

model = tf.keras.Sequential()model.add(tf.keras.layers.Flatten())model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax))

model.compile(optimizer='adam',loss='sparse\_categorical\_crossentropy',metrics=\['accuracy'\])model.fit(input\_data, output\_data, epochs=100)

\`\`\`

input\_data is a multi-dimensional array with lists containing lists containing 10 numbers of either 0 or 1, but this works fine and is not the problem

&amp;#x200B;

output\_data is also a multi-dimensional array containing list with values of 0, 1, 2, 3 or 4, ex.: \[\[0, 1\], \[3\], \[4, 1\]\] and tensorflow seem to does not like that.

&amp;#x200B;

Any ideas of how to fix that?

Thanks!",1,tensorflow,2020-10-13
ivve4y,What can I do with such a dataset? Can I use it to predict sales? Any Ideas?,,1,tensorflow,2020-10-13
ivln9o,"First model went horrible: Same person as before, with more info on my project",,4,tensorflow,2020-10-13
ivklwl,Transformers: Fall of Cybertron - (Resolution increased using neural networks to 8K),,16,tensorflow,2020-10-13
ivez7l,Configure tensorflow with AMD Gpu on Fedora,"Hello everyone,   
I'm new to Tenserflow and after installing it (CPU version) it was a bit slow with my Ryzen 5 3600 CPU. I read on internet that it exist a GPU version of tensorflow. How ever I didn't find anything concerning the installation on Fedora (32) with an AMD GPU (Navi architecture, 5700 XT) even the ROCm project does not seems to support Navi cards.  Also, I'm actualy running with the open-source free driver for my GPU, not the ""pro"" version, is it enough or do I need to upgrade?

&amp;#x200B;

Thanks for your help!",3,tensorflow,2020-10-13
ivanii,Uploading Trained Models,"Hey, 

I was wondering what's the best practice/method to upload Trained models. Suppose, I trained a few custom models or even finetuned a YOLO/InceptionResnet/other models for a specific purpose. I now want to upload these models so that others might use it, if they ever find a need for it. How should I do it?  
Till now, I have been using \`[git lfs](https://git-lfs.github.com/)\` to upload models &gt; 100MB, since normal \`git commit\` and \`git clone\` wouldn't upload files &gt;100MB. However, I recently trained a model on my custom dataset, and its \~280MB. I think it is a bad practice to upload large models using \`git lfs\`, just because \`git lfs\` allows it. 

Is uploading it to my GDrive and then sharing it through a link, a more appropriate solution (but then again, if in the future, I need to move the uploaded model, the link won't work anymore unless I go back and replace the old link with the new one)?   


Any tips or advices would be greatly appreciated.",11,tensorflow,2020-10-13
iv9lgk,How to make pos tagger using tensorflow's HiddenMarkovModel,What next after I initialize  HiddenMarkovModel. How to use it for pos tagging?,2,tensorflow,2020-10-13
iv2wrr,SuperAnnotate Desktop: A better alternative to free annotation tools,"In partnership with OpenCV, SuperAnnotate launched an all free-to-use desktop app. This is the fastest annotation software ever built. It closes the gap between free and commercial annotation tools providing CV Engineers with all the functionalities designed to increase the speed, the accuracy and the efficiency of their annotation projects.

I'm sharing an article all about the software and the behind-the-scenes of why [SuperAnnotate Desktop](https://blog.superannotate.com/superannotate-desktop-a-better-alternative-to-free-annotation-tools) was created.",11,tensorflow,2020-10-13
iuyv25,Retiming and duplicating people in video (using neural rendering)!,,8,tensorflow,2020-10-13
iuwrc7,"Find code implementations for ML/AI research papers directly on Google, Arxiv, Scholar, Twitter, Github, and more!!",,2,tensorflow,2020-10-13
iuvxyx,Tensorflow Not Seeing GPU (CUDA 11 and CUDNN 8),"Hi!

I'm 14 year old noob, so please bear with me here. I recently upgraded to an RTX 2080S from an AMD GPU, so I want to use Tensorflow now. I installed CUDA 11 and CUDNN 8, but I'm not being able to use my GPU with tensorflow - I've attached a screenshot of the error. I'm on Ubuntu 18.04. Any help is appreciated! I'm willing to provide more info if needed.

EDIT: Apologies for the title - I realize it might be a bit misleading.

UPDATE: It works! I had to downgrade and use CUDA 10.1 and CUDNN 7.6.5 - I guess Tensorflow support isn't great with 11 and 8. Here is the article that saved me: [https://medium.com/@stephengregory\_69986/installing-cuda-10-1-on-ubuntu-20-04-e562a5e724a0](https://medium.com/@stephengregory_69986/installing-cuda-10-1-on-ubuntu-20-04-e562a5e724a0)

And here is the comment that also saved me: [https://medium.com/@mattphillipsphd/hey-this-was-fantastic-fd7c98944b6f](https://medium.com/@mattphillipsphd/hey-this-was-fantastic-fd7c98944b6f)

Yay!

https://preview.redd.it/5nijmsrcxsn51.png?width=737&amp;format=png&amp;auto=webp&amp;s=455f942b1ae3d831b5ee7094071d858662fdb22b",2,tensorflow,2020-10-13
iupf66,"for GAN training, is it important that the Generator and Discriminator have the same/similar architecture?","I wonder if I can make the generator more complex to allow for more detail and variation, but use a less complex discriminator. Every trick to stabilize GANs seems to be to cripple the discriminator in some way, and I wonder if this is an appropriate way to address that?

I have been using very similar architecture for these so far, as in:

Current generator:

    def make_generator_model():
        model = tf.keras.Sequential()     model.add(layers.Dense(32*32*128, use_bias=False, input_shape=(100,)))     model.add(layers.BatchNormalization())     
        model.add(layers.LeakyReLU())
        model.add(layers.Reshape((32, 32, 128)))
        model.add(layers.Conv2DTranspose(64, (4,4), strides=(1, 1), padding='same', use_bias=False))     
        model.add(layers.BatchNormalization())     
        model.add(layers.LeakyReLU())
        model.add(layers.Conv2DTranspose(32, (5,5), strides=(2, 2), padding='same', use_bias=False))
        model.add(layers.BatchNormalization())     
        model.add(layers.LeakyReLU())
        model.add(layers.Conv2DTranspose(3, (5,5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    return model

Current discriminator

    def make_discriminator_model():
        model = tf.keras.Sequential()     model.add(layers.Conv2D(32, (4,4), strides=(2, 2), padding='same',                                      input_shape=[128,128,3]))         model.add(layers.LeakyReLU())    
        model.add(layers.Dropout(0.3))
        
        model.add(layers.Conv2D(64, (5,5), strides=(2, 2), padding='same'))     model.add(layers.LeakyReLU())     
        model.add(layers.Dropout(0.3))
        
        model.add(layers.Conv2D(128, (5,5), strides=(1, 1), padding='same'))     model.add(layers.LeakyReLU())     
        model.add(layers.Dropout(0.3))
        
        model.add(layers.Flatten())     
        model.add(layers.Dense(1))
    return model

Idea for more complex generator

    def make_generator_model():
        model = tf.keras.Sequential()     model.add(layers.Dense(8*8*512, use_bias=False, input_shape=(100,)))     model.add(layers.BatchNormalization())     
        model.add(layers.LeakyReLU())
        model.add(layers.Reshape((8, 8, 512)))
        model.add(layers.Conv2DTranspose(256, (4,4), strides=(2, 2), padding='same', use_bias=False))     
        model.add(layers.BatchNormalization())     
        model.add(layers.LeakyReLU())
        model.add(layers.Conv2DTranspose(128, (5,5), strides=(2, 2), padding='same', use_bias=False))     
        model.add(layers.BatchNormalization())     
        model.add(layers.LeakyReLU())
        model.add(layers.Conv2DTranspose(64, (5,5), strides=(2, 2), padding='same', use_bias=False))     
        model.add(layers.BatchNormalization())     
        model.add(layers.LeakyReLU())
        model.add(layers.Conv2DTranspose(3, (5,5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    return model

&amp;#x200B;",3,tensorflow,2020-10-13
iulacf,I published a tutorial where I extract Mel spectrograms from audio data with Python,"In my new video, I explain how to extract Mel spectrograms from an audio file with Python and Librosa. I also visualise Mel filter banks.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=TdnVE5m3o\_0&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=18](https://www.youtube.com/watch?v=TdnVE5m3o_0&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=18)",12,tensorflow,2020-10-13
iuc7gc,"TensorFlow Bounty, $500","Ambianic.ai has posted a $500 TensorFlow bounty at:

[https://www.bountysource.com/issues/92825863-people-fall-detection](https://www.bountysource.com/issues/92825863-people-fall-detection)

More bounties will be posted in the future.

Thanks",1,tensorflow,2020-10-13
iua2ap,TensorFlow Open Sources An End-To-End Solution For TFLite On-Device Recommendation,"TensorFlow [open-sources an end-to-end solution](https://www.tensorflow.org/lite/models/recommendation/overview) for on-device recommendation tasks to provide personalized and high-quality recommendations with minimal delay while preserving users’ privacy. Developers build on-device models using TFlite’s solution to achieve the above. When it comes to real-world applications, such as music, videos, merchandise, apps, news, etc., high-quality personalized recommendations are needed.

Summary: https://www.marktechpost.com/2020/09/16/tensorflow-open-sources-an-end-to-end-solution-for-tflite-on-device-recommendation/

Github: https://github.com/tensorflow/examples/tree/master/lite/examples/recommendation/ml",11,tensorflow,2020-10-13
iu9fc7,Why tensorflow versions installed in Ubuntu don't match?,"I want to use mixed precision in Ubuntu 16 which I read requires tensorflow 2.0+

I had tensorflow 1.4. I then attempted to install the latest version with `pip3 installl tensorflow` and `pip3 install tensorflow-gpu`. 

Even though `pip3 show tensorflow` and `pip3 list | grep tensorflow` both show version 2.3, when I go to Python in the terminal then try

    import tensorflow as tf
    print(tf.__version__)

it shows 1.4.0

Thus when I try `from tensorflow.keras.mixed_precision import experimental as mixed_precision`, it says `importerror: no module named tensorflow.keras`

anyone can help?",1,tensorflow,2020-10-13
iu6jos,Machine Learning Engineer Training ($130k+ average salary),"ML colleagues, according to ZipRecruiter Machine Learning Engineers earn $130,530 per year. Learn advanced machine learning techniques and algorithms -- including how to package and deploy your models to a production environment. Gain practical experience using Amazon SageMaker to deploy trained models to a web application, evaluate the performance of your models, A/B test models and update the models as you gather more data..Suggested prerequisites are intermediate knowledge of Python and Machine Learning algorithms. The four skill-based training modules (each with a unique training project) include: 1) Software  Engineering Fundamentals (Project: Build a Python Package), 2) Machine  Learning in Production with Amazon SageMaker (Project: Deploy a Sentiment Analysis Model), 3) Machine Learning Case Studies (Project: Plagiarism Detector), and 4) Machine Learning Capstone (Project: Select a machine learning challenge and propose a possible solution). 

Enroll today at: https://fxo.co/9glo 

Much career success, Lawrence E. Wilson - Artificial Intelligence Academy (AIA)",0,tensorflow,2020-10-13
iu5x6h,Teachable machine,Does anybody know a quick and efficient way to download a teachable machine model to a raspberry pi that can work WITHOUT internet connection? I’ve looked at plenty of YouTube’s but haven’t found one that works the way I want it to.,0,tensorflow,2020-10-13
iu1iiv,State of the art in Crop/Weed Segmentation!,,1,tensorflow,2020-10-13
ituuek,Crop and Save YOLOv4 Object Detections with TensorFlow and Python,,31,tensorflow,2020-10-13
itsfkx,Tensorflow software version for certification exam,"Hello,  
I'm planning to take Tensorflow Certification Exam by end of this month. 

I have a question regarding the version of TensorFlow, As per the Candidate Handbook, it says "" TensorFlow 2.x "" but I heard when we install the exam plugin it will auto-install Tensorflow 2.0 which is a pretty old version.

Does anyone update the version to 2.3 and attempted the exam?  Faced any issues with the v2.3?

The reason I'm asking this Is to configure my system with cuDNN and Cuda libs.

https://preview.redd.it/itkwyp9qdhn51.png?width=1349&amp;format=png&amp;auto=webp&amp;s=f3ddbb1fe71852e2bb06c56de851fa5123a68043",10,tensorflow,2020-10-13
itogpa,Used TensorFlow to Decrease Ambiguity in Texting,,5,tensorflow,2020-10-13
itlfzx,Role of BIAS in DCGANS: why is it set to FALSE in some cases,"Greetings!

Can someone help me understand the intuition behind not using BIAS in DCGAN(s) such as here

[https://www.tensorflow.org/tutorials/generative/dcgan](https://www.tensorflow.org/tutorials/generative/dcgan)

[https://pytorch.org/tutorials/beginner/dcgan\_faces\_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)

?",8,tensorflow,2020-10-13
itj92r,Free browser extension! AI/ML Code Implementation Finder,,1,tensorflow,2020-10-13
itiske,Why might my val. accuracy and loss be so variable? ~500 val. samples,,5,tensorflow,2020-10-13
itgv7c,How do you like my Remaster? Resolution increased using neural networks to 8K,,1,tensorflow,2020-10-13
itf0nb,Read Large CSV Dataset into Tensorflow Estimator,"Hey guys, 

I'm trying to learn Tensorflow and all the different ML models that I can make from it. I'm currently trying to make a Boosted Trees model using Tensorflow's built in`tf.estimator.BoostedTreesRegressor()` function. Right now, I have two very large CSV files holding my test and training data. I was following along with [the Tensorflow Documentation guide](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding#visualizing_model_fitting) for Boosted Trees, but their input function uses `tf.data.Dataset.from_tensor_slices()` in order to read in the dataset. When I try to run it this way, my computer runs out of memory very quickly and my Jupyter Kernel dies (I'm assuming that `tf.data.Dataset.from_tensor_slices()` creates copies as it goes along?) Anyways, I tried to change the input function to utilize `tf.data.experimental.make_csv_dataset()` instead. Now my input function looks like so: 

    NUM_EXAMPLES = 32
    
    def input_function_new(filename, n_epochs =None, shuffle= True):
        def input_fn():
            dataset = tf.data.experimental.make_csv_dataset(
                filename, 
                batch_size = NUM_EXAMPLES, 
                label_name = 'discountrate', 
                na_value = -1, 
                num_epochs = n_epochs
            )
            if shuffle:
                dataset = dataset.shuffle(NUM_EXAMPLES)
        return input_fn
    
    train_input_fn = input_function_new('train.csv')
    eval_input_fn = input_function_new('val.csv', n_epochs=1, shuffle=False)

And when I initialize the estimator like so

    params = {
      'n_trees': 50,
      'max_depth': 10,
      'n_batches_per_layer': 1,
      'center_bias': True
    }
    
    est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)
    est.train(train_input_fn, max_steps=100)

You may notice that I have feature columns being used, I just used one-hot for all my categorical features and left my numeric ones the same. 

    #Reading so that I can get the unique values 
    dftrain = pd.read_csv('train.csv')
    dfeval = pd.read_csv('val.csv')
    dataset = pd.concat([dftrain, dfeval])
    fc = tf.feature_column
    NUMERIC_COLUMNS = [NUMERIC COLUMN NAMES]
    CATEGORICAL_COLUMNS = [CATEGORICAL COLUMN NAMES]
    
    def one_hot_cat_column(feature_name, vocab):
        return fc.indicator_column(
            fc.categorical_column_with_vocabulary_list(feature_name,
                                                       vocab))
    feature_columns = []
    for feature_name in CATEGORICAL_COLUMNS:
        vocabulary = dataset[feature_name].unique()
        feature_columns.append(one_hot_cat_column(feature_name, 
                                                  vocabulary))
    
    for feature_name in NUMERIC_COLUMNS:
        feature_columns.append(fc.numeric_column(feature_name,
                                                 dtype=tf.float32))

When I try to train my model, however, I get the error 

     TypeError: Expected string, got -1 of type 'int' instead.

I think it has something to do with the way I'm handling NA values? I changed the `na_value` param in my input function to a string and the error became 

    AttributeError: 'NoneType' object has no attribute 'values'

but this is also confusing since running `df.isnull().values.any()` on my dataset returns `False`, which leads me to think that there aren't any null columns anyways so why am I getting an error? Also, do I have the right approach to making my input function? Would this even work?",1,tensorflow,2020-10-13
itboqc,Learning tensor flow,"Hi friends!
Could you recommend me adequate resources or bootcamps for learning tensor flow with colab?",1,tensorflow,2020-10-13
it7i8l,NEED ULTIMATE GUIDE/RESOURCES FOR TF 2.X OBJECT DETECTION ON COLAB.,"Hey , I am trying to do object detection with tensorflow 2 on Google Colab. I want suggestion for guide, resources or  tutorials that I can follow ! 

Thanks !",12,tensorflow,2020-10-13
it66qi,Using celery/huey to run a tensorflow model,"I am trying to create and make predictions using a celery/huey worker. What i wanna know is that, is this a good approach? Will this work?",3,tensorflow,2020-10-13
it61cl,[Semantic Segmentation]How do I match up my loss function and my output/labels to agree with one another?,"I'm new to semantic segmentation and am really struggling to wrap my head around how to construct my model so that the logits and labels are the same. And then so that the loss function is the correct loss function.

I'm using Keras 

    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_6 (InputLayer)         [(None, 256, 256, 3)]     0         
    _________________________________________________________________
    block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
    _________________________________________________________________
    block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
    _________________________________________________________________
    max_pooling2d_20 (MaxPooling (None, 128, 128, 64)      0         
    ..........................................................................................................
    up_sampling2d_11 (UpSampling (None, 256, 256, 512)     0         
    _________________________________________________________________
    block7_conv1 (Conv2D)        (None, 256, 256, 123)     566907    
    _________________________________________________________________
    softmax_4 (Softmax)          (None, 256, 256, 123)     0       


That's my current setup. RGB input and then 123 classes output.

My masks are also in RGB so that could be an issue because I think they should be one-hot, but the tutorials that I've been able to follow don't mention converting the masks or its over my head what they are saying. 

I'm using 'sparse categorical crossentropy' as I haven't been able to get any custom loss function to work. I get weird errors. 

I have no idea if this is the right loss function to use, some stuff on SO suggested it could work. But I dunno.

When I try to fit I get this:

    InvalidArgumentError:  logits and labels must have the same first dimension, got logits shape [1048576,123] and labels shape [3145728]
    	 [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at &lt;ipython-input-40-8ccdd719022a&gt;:3) ]] [Op:__inference_train_function_8590]

I don't really know where these numbers come from except for the 123. Neither of the other numbers seem to relate to the 256 of the image size. 

I don't know I'm feeling a bit overwhelmed. I've been stuck on this problem for like 30 hours now and it feels like it should just be a simple fix. But I can't find the right words or resources to explain how to go from RGB frame to RGB mask. Do I need to change my mask? How come none of the tutorials explain the loss functions they use? Why do I need to download another complete repository? 

Is there a better way to feed my images into the fit method? I'm using a generator

    from keras.preprocessing.image import ImageDataGenerator
    def image_mask_generator(image_data_generator, mask_data_generator):
        train_generator = zip(image_data_generator, mask_data_generator)
        for (img, mask) in train_generator:
            yield (img, mask)
    
    SEED = 100
    
    image_data_generator = ImageDataGenerator(
        rescale = 1./255,
        width_shift_range = 0.1,
        height_shift_range = 0.1,
        rotation_range = 30,
        zoom_range = 0.1
    ).flow_from_directory('/content/drive/My Drive/Thesis Pics/train_frames', batch_size = 16, target_size = (256,256), seed = SEED)
    
    mask_data_generator = ImageDataGenerator(
        width_shift_range = 0.1,
        height_shift_range = 0.1,
        rotation_range = 30,
        zoom_range = 0.1
    ).flow_from_directory('/content/drive/My Drive/Thesis Pics/train_masks', batch_size = 16, target_size = (256,256), seed = SEED)
    
    generator = image_mask_generator(image_data_generator, mask_data_generator)

IF anyone is knowledgable about Semantic Segmentation I'd realllllllyyy appreciate being able to pick your brain for specific information. 

Thanks.",2,tensorflow,2020-10-13
it3ivh,how to do custom pre-processing on data when using tf.data?,"I need some help help with tf.data.

I am doing a few experiments on SQUAD dataset. dataset structure given is like below:

    row-1] { conext: ""some big string"", question:""q string"", ""answer"": ""some ans"" }

I would like to make use of **tf.data for load and pre-processing**. After loading, it is loaded in foll. format:

    {   context: Tensor(""some big string""), 
        question:Tensor(q string),  
        answer"": Tensor(some ans) 
     }

Now we want to pre-process the data. Now here pre-processing is not straightforward because **values are Tensor objects.**

Tensorflow provides some apis for such kind of pre-processing but **what if I want to do my custom pre-processing or maybe I want to use spacy which just operates on raw datatypes like string and not tensors.**

Basically I want help with this snippet:

    def format_data(row): 
      # Now I can access individual data row here. But value of row is in Tensor form. 
    
      # Hence I can't use my custom function. How to use custom function or spacy 

# function which operates on string and not on tensor?

      # I can use only below tf functions return 
      tf.strings.regex_replace(row['context'],'some-regex',' ',True)   
      # I have also tried using tf.py_function, it doesn't work. 
    
    train = dataset.map(format_data).batch(2) 
    list(train.take(1))
    ",1,tensorflow,2020-10-13
isxdso,Tensorflow: libcublas.so.10: cannot open shared object file,"I was trying to run a Unet in Keras but got

    terminate called after throwing an instance of 'std::bad_alloc'

which doesn't make sense since I'm running the same Unet as before. I did make changes to CUDA, so I'm guessing that's the cause of this

Whenever I use tensorflow (I use version 2.3.0 in Ubuntu 16 with an NVIDIA GPU) and try
 
    gpus = tf.config.experimental.list_physical_devices('GPU')

it shows `gpus` as an empty list and says

	Successfully opened dynamic library libcudart.so.10.1
	2020-09-14 16:39:11.975096: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/lib64::/usr/local/cuda-10.0/lib64::/usr/local/cuda-11.0/lib64::/usr/local/cuda-11.0/lib64
	2020-09-14 16:39:11.975158: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
	2020-09-14 16:39:11.975197: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
	2020-09-14 16:39:11.975232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
	2020-09-14 16:39:11.975380: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/lib64::/usr/local/cuda-10.0/lib64::/usr/local/cuda-11.0/lib64::/usr/local/cuda-11.0/lib64
	2020-09-14 16:39:11.975436: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7

even though I set

	export PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}
	export PATH=/usr/local/cuda-10.0/bin:/usr/local/cuda-10.0/NsightCompute-1.0${PATH:+:${PATH}}
	export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}

and `which nvcc` shows

    /usr/local/cuda-10.0/bin/nvcc

and `$LD_LIBRARY_PATH `

shows

	bash: /usr/local/cuda-10.0/lib64::/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.0/lib64: No such file or directory

and `~/.bashrc` shows

    export PATH=""$PATH:/usr/local/cuda-10.0/bin""
    export LD_LIBRARY_PATH=""/usr/local/cuda-10.0/lib64""${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}

can anyone help?",1,tensorflow,2020-10-13
isv70s,Is it possible to pretrain GPT-2 (or another NLP algorithm) for a different task?,"I want to train a neural net that, given an image of someone, comes up with a roast (insult) about them. Originally, I figured that the amount of data and size of the network I would need to have the machine understand enough about pop culture and the human language and lots of that stuff was just too much, so I found a workaround.

I trained a variational autoencoder on human faces, then I went through /r/RoastMe and found the latent encodings of every post. Now, when I take a picture of myself, it encodes my picture through my VAE, find the closest post based on the euclidean distance of the encoding vectors, and just reads one of the roast from there.

That was a fun project that I just finished a day ago, but I'm realizing now I can achieve my original goal of just having a neural net that goes from a picture of someone straight to a new novel roast. I could just take something like GPT-2 that has a huge understanding about the world, and just make a dataset of /r/RoastMe posts -&gt; specific roasts.

I have done pretraining like this before, i.e. taking InceptionV3 (thanks to the built-in tensorflow package for it), freezing all but a few layers, replacing the output layer with my own, and retraining it very quickly.

Is this possible with something like GPT-2? I've been looking and I can't even figure out how to load the model. My guess is it's a simple solution, but the files I got from downloading one of the models just don't make sense to me. [Picture](https://imgur.com/pk24z4q).

I'm used to using .h5 files for saving and loading models, I don't know what to do with this. Any help would be appreciated.",1,tensorflow,2020-10-13
isqidm,Converting Tensorflow 2 model to tensorflow 1,I have a tool that is incompatible with tensorflow 2 but want to train using tensorflow 2.  How can I convert a model trained with tensorflow 2 to be a tensorflow 1 frozen graph?,2,tensorflow,2020-10-13
iskseu,"Building a “Motivation Bot” with TensorFlow.js, Face Detection, and Emotion Classification",,2,tensorflow,2020-10-13
isjnc4,I published a tutorial where I explain Mel spectrograms easily,"Mel spectrograms are often the feature of choice to train Deep Learning Audio algorithms. In this video, you can learn what Mel spectrograms are, how they differ from  “vanilla” spectrograms, and their applications in AI audio. To explain Mel spectrograms, I also discuss the Mel scale and Mel filter banks.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=9GHCiiDLHQ4&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=17](https://www.youtube.com/watch?v=9GHCiiDLHQ4&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=17)",16,tensorflow,2020-10-13
isjg1v,Train the GPT-2 model from scratch any langauage,"Train GPT-2 from Scratch on your own language . 

GPT-2 Training on non-English textual  


[https://gist.github.com/miladfa7/8668eee87a17da2e7d22f1b8df5648ba](https://gist.github.com/miladfa7/8668eee87a17da2e7d22f1b8df5648ba)",1,tensorflow,2020-10-13
isdzj4,Tired Of Tedious Pre-Processing Tasks? Try TensorFlow Lite Task Library!,"INTRODUCING TENSORFLOW LITE TASK LIBRARY

A set of powerful and easy-to-use model interfaces for popular ML tasks, it handles most of the pre and post processing and complex logic for you. Inference can now be done in just 5 lines of code!

**Summary:** [https://www.marktechpost.com/2020/09/13/tired-of-tedious-pre-processing-tasks-try-tensorflow-lite-task-library/](https://www.marktechpost.com/2020/09/13/tired-of-tedious-pre-processing-tasks-try-tensorflow-lite-task-library/)

**Resource:** [https://blog.tensorflow.org/2020/09/introducing-tensorflow-lite-task-library.html](https://blog.tensorflow.org/2020/09/introducing-tensorflow-lite-task-library.html)",17,tensorflow,2020-10-13
isdw1n,How do I crop detected faces from Blaze Face Model in Tensorflow.js?,I am able to detect the faces and draw the bounding boxes on them but now I want to crop these detected faces and then send them to another model.,2,tensorflow,2020-10-13
is5ua8,Conv Network predicts same output regardless of input variations," Hi guys!

I have a 1 D convolutional network that is designed to predict time series data.

It has 11 layers, as it is basically designed like an AutoEncoder, with a bottleneck in between.

My normalized input comes from \[-3, 3\] normal distribution which outputs time series data scaled from 0 to 1.

After training and achieving desired prediction performance, I tried random noise (-0.1, 0.1) as input and observed that my network had a decent prediction. Furthermore, I created an input of all zeros (shaped like my original input), and run it thought the NN model - it again predicted something reasonable even though it had all zeros as input.

My first question is: how is that possible that the network does not react much to the input data variability?",1,tensorflow,2020-10-13
iry241,most efficient distribution-strategy for LSTMs on 2 GPUs ?,"Which of Tensorflows distribution-strategy is most efficient for LSTMs on 2 GPUs ? ([https://www.tensorflow.org/guide/distributed\_training](https://www.tensorflow.org/guide/distributed_training))

Otherwise, how do you decide which distribution strategy to chose?

&amp;#x200B;

Float or double sequences (of around 1.000-15.000 rows &amp; around 10-100 columns), 1-2Million Samples

    [ # 1st sample
      [ 89.319787 1.329743 99.234670    ...    52.329743 0.319787 2.319787 ] 
      [ 84.319787 1.329743 49.329743    ...    52.329743 0.319    2.319787 ] 
      [ 12.319787 1.329743 33.329743    ...    52.329743 0.319787 2.319787 ] 
      [ 33.319787 1.329743 23.329743    ...    52.329743 0.319787 2.319787 ] 
    
            ...       ...       ...    ...          ...       ...     ...  
            ...       ...       ...    ...          ...       ...     ... 
    
      [ 23.319787 1.329743 45.234670    ...    52.329743 0.32721  2.319787 ] 
      [ 89.319787 1.329743 99.234670    ...    52.329743 0.319787 2.319787 ] 
      [ 84.319787 1.329743 49.329743    ...    52.329743 0.319    2.319787 ] 
      [ 12.319787 1.329743 33.329743    ...    52.329743 0.319787 2.319787 ] 
      [ 33.319787 1.329743 23.329743    ...    52.329743 0.319787 2.319787 ]   ],
    
    ...
    
    [ # n'th sample          (n = 500k or 2 million samples)
      [       ...       ...       ...    ...          ...       ...     ... ]
              ...       ...       ...    ...          ...       ...     ... 
      [       ...       ...       ...    ...          ...       ...     ... ]  ],

Batch\_sizes; 8, 16 or 32

LSTM with these layers: \[700,700,700,700,64,32\]

On 2 RTX 3080 or 1x3080+1x3090",5,tensorflow,2020-10-13
iru30u,Tensorflow.js Error “tfjsconverter.loadGraphModel is not a function”,"I would like to create an object recognition using the camera feed for dog breeds using tf.js.

I built an object recognition demo that works https://codesandbox.io/s/tensorflowjs-real-time-object-detection-om12e?file=/src/index.js:0-27 but it uses the cocoSsd model that doesn't have dog breeds.

So I know that mobilenet model does have dog breeds and I found a working example here https://codesandbox.io/s/o4l6pnlkzz (though it uses images upload rather than the camera feed).

When I tried to put the mobilenet model into the object detection example, I get the following error. ""tfjsconverter.loadGraphModel is not a function"".

https://codesandbox.io/s/tensorflowjs-real-time-object-detection-forked-6hxdb?file=/src/index.js:925-934

I imagine there's something wrong on line 31, but I can't figure it out.

If you know what I'm doing wrong, it would be really helpful for a few pointers.

I've also posted this question on StackOverflow, but so far, I got no response.",5,tensorflow,2020-10-13
irmxqh,What kind of output should I be using for Semantic Segmentation?,"I'm pretty new to Tensorflow and can make basic classification and detection models. 

I want to perform segmentation on satellite imagery but this isn't the same obviously as classification. I'm not quite sure how my model is supposed to output the data and how to use it.

My masks are simply RGB images, 3 channels. My question is does my output layer need to be the same dimensions as my masks? 

What's the correct workflow from going from simple RGB satellite images to RGB mask image? The models I've been looking at online are a bit hard to follow, but one of them does output to the same size as the image but with the classes for channels instead of RGB. So i'm not quite sure how it was output as an RGB image. 

Any guide for dummies on simple image to mask segmentation?",6,tensorflow,2020-10-13
irb1wj,Passing stacked layers to tf.keras.Sequential() with .add(),"Rather than passing layers in the following manner

    convs=tf.keras.Sequential()
    for i in range(y):
        self.convs.add(tf.keras.Conv2D(...))
        self.convs.add(tf.keras.layers.BatchNormalization(...)

Is it possible to combine the Conv2D layer and batch layer using a function, and pass the result to Sequential()?

For example:

    def func_a():
        convs=tf.keras.Sequential()
        for i in range(y):
            self.convs.add(func_b(...))
    
    def func_b(...):
        conv=tf.keras.Conv2D(...)
        norm=tf.keras.layers.BatchNormlization(...)
        x=norm(conv)
        return(x)

Although outside of ML I would employ trial-and-error, I'm not proficient enough to realise whether the program is working as it should, or not, at this point.

If this method is a possibility, how is the input passed from func\_a() (whether initial input, or output of previous layers) to func\_b(), and how should I accommodate it?

Thanks!",10,tensorflow,2020-10-13
ir801n,simple exercise for school," I am looking for simple implementation of loading few sets of pictures from different folders, and then after training, identify by single image.",0,tensorflow,2020-10-13
ir4rqs,Is Style GAN patented?,"I want to know **exactly** if it’s allowed to use state of the art neural network architectures to create commercial software.

This is the official source code of the Style GAN:

https://github.com/NVlabs/stylegan

On the license page it says “Attribution-NonCommercial 4.0 International“, but it refers to the idea or the source code?

Option 1 - If it refers to the idea, it means that nobody else can use a StyleGAN-based architecture to build a commercial software.

Option 2 - If it refers to the code on the page, it means that any person can use a StyleGAN-based architecture to build a commercial software, if the person implements its own code from scratch and uses its own dataset.

Now, if we navigate to this repository:

https://github.com/taki0112/StyleGAN-Tensorflow

We see that the license now says “MIT license”, which means anyone can use it commercially, but it uses the same idea from the Style GAN paper. So, it confirms the option 2, if you write your own implementation, you can use the style based architecture to build your own software. Is this correct? Or am I missing something here.",11,tensorflow,2020-10-13
ir0y0e,xception broadcast issue with custom size,"## System information:
Red Hat Enterprise Linux Server release 7.7 (Maipo)
Tensorflow: tensorflow gpu 2.1
GPU: Nvidia V100
Python: 3.6.10
GCC: 7.3.0
CUDA: 10.1 (I think)
Running through slurm

## The Error log:

```
ValueError: could not broadcast input array from shape (850,550,3) into shape (850,550,3,3)
```

## model.summary()
This model is the stock xception with a custom top (global max, dense layer, and softmax) used for image classification. 
                    

        __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to
    ==================================================================================================
    input_2 (InputLayer)            [(None, 850, 550, 3) 0
    __________________________________________________________________________________________________
    block1_conv1 (Conv2D)           (None, 424, 274, 32) 864         input_2[0][0]
    __________________________________________________________________________________________________
    block1_conv1_bn (BatchNormaliza (None, 424, 274, 32) 128         block1_conv1[0][0]
    __________________________________________________________________________________________________
    block1_conv1_act (Activation)   (None, 424, 274, 32) 0           block1_conv1_bn[0][0]


## Relevant Code    
    import numpy as np
    import pandas as pd
    import tensorflow
    from tensorflow import keras
    from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
    from tensorflow.keras.utils import to_categorical
    from sklearn.model_selection import train_test_split
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.models import load_model
    from tensorflow.keras.models import Model, Sequential
    from tensorflow.keras.applications import Xception
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.layers import Flatten, Dense, Input, GlobalAvgPool2D
    from tensorflow.keras import optimizers
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
    import matplotlib.pyplot as plt
    import random
    import os
    import time
    from datetime import datetime
    from IPython.display import SVG
    from PIL import ImageFile
    ImageFile.LOAD_TRUNCATED_IMAGES = True
    
    
    
    from pathlib import Path
    root_path = Path(__file__).resolve().parents[2]
    print(root_path)
    
    # verify gpus
    print(tensorflow.config.list_physical_devices('GPU'))
    
    
    FAST_RUN = False
    IMAGE_HEIGHT=850
    IMAGE_WIDTH=550
    IMAGE_CHANNELS=3
    IMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)
    
    input_tensor_def = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))  # unused for now
    
    
    NAME = f""{datetime.today().strftime('%Y-%m-%d')}-xception_850_flowering_keras_xception""
    
    
    model_core = Xception(weights = None, include_top = False, input_shape = IMAGE_SIZE)
    
    model_head = model_core.output
    model_head = GlobalAvgPool2D()(model_head)
    model_head = Flatten()(model_head)
    model_head = Dense(512, activation = 'relu')(model_head)
    model_head = Dense(256, activation = 'relu')(model_head)
    model_head = Dense(2, activation = 'softmax')(model_head)
    
    model = Model(inputs = model_core.input, outputs = model_head)
    
    model.compile(Adam(lr=.00005), loss='categorical_crossentropy', metrics=['accuracy'])
    
    print(model.summary())
    
    earlystop = EarlyStopping(patience=20)
    
    
    filepath=f""{root_path}/Models/xception/{NAME}.hdf5""
    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
    
    callbacks = [earlystop, checkpoint]
    
    
    # hard coded for now, replace later
    nb_train_samples = 33960
    nb_validation_samples = 8482
    batch_size=16
    
    train_path = '/gpfs/loomis/home.grace/teo22/project/Herbarium/Train_Cropped_850_8_8_Flowering'
    valid_path = '/gpfs/loomis/home.grace/teo22/project/Herbarium/Valid_Cropped_850_8_8_Flowering'
    
    train_datagen = ImageDataGenerator(
        rotation_range=15,
        rescale=1./255,
        shear_range=0.1,
        zoom_range=0.2,
        horizontal_flip=True,
        width_shift_range=0.1,
        height_shift_range=0.1
    )
    train_generator = train_datagen.flow_from_directory(
        train_path, target_size=IMAGE_SIZE, class_mode='categorical', classes=['Flowering', 'Not_Flowering'], batch_size=batch_size)
    
    validation_datagen = ImageDataGenerator(rescale=1./255)
    validation_generator = validation_datagen.flow_from_directory(
        valid_path, target_size=IMAGE_SIZE, class_mode='categorical', classes=['Flowering', 'Not_Flowering'], batch_size=batch_size)
    
    print(nb_validation_samples//batch_size)
    
    epochs=3 if FAST_RUN else 500
    history = model.fit_generator(
        train_generator,
        epochs=epochs,
        validation_data=validation_generator,
        validation_steps=nb_validation_samples//batch_size,
        steps_per_epoch=nb_train_samples//batch_size,
        callbacks=callbacks
    )

## My thoughts
I feel like normally with broadcasting errors, it's generally related to the sizes of the images, or it fails to broadcast from a 3-dim input tensor to the 4-dim (batch, height ,width,channels) tensor. However, here, it just seems like the code has forgotten about the existence of the batch dimension and confused height for batches, width for height, and channels for both width and channels. I have double checked my code, but to my (admittedly very limited) knowledge, everything looks okay.",1,tensorflow,2020-10-13
iqzfy4,"How would one go about building a sequence to sequence model where the input sequence can have variable length, but the output sequence must always have the same length as the input sequence?",,3,tensorflow,2020-10-13
iqxz0o,Is their a pre built non AVX binary? Running on non AVX vps.,I don't know how to build from source without AVX is their any up to date (ish) binary which I can install (ideally with pip)?,6,tensorflow,2020-10-13
iqxvxr,How to use different versions of cuda on same machine (Windows)?,"I guess I could also ask how can I link a specific version of cuda to my python virtual environment? I installed cuda 11.0 but my code still uses version 10. Even when I create a new virtual env, it still uses version 10 not 11. Please help with answers related to windows. Thank you.",8,tensorflow,2020-10-13
iqky1s,Tensorflow environment,"Hi

I take this course in [Udemy](https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/)

The title is Tensorflow but in Syllabus I do not see it as a topic, I passed Numpy and now learning Pandas, and I enjoy it.

But what is Tensorflow?  

#",3,tensorflow,2020-10-13
iqiaap,Tensorflow Object Detection API - Average Precision/Recall with max detections,"Hello, I've been having this question in my mind for quite a while.

When Evaluating the model with the COCO metrics, there are a few options for max detections (1, 10, 100).

Lets consider that there are 100 object in the image and we just calculate the Precision/Recall with max detections = 1. Are the other groundthruths not being considered in the confusion matrix? 

This is what makes sense to me, that they will not be considered, and only the prediction with best score and the respective groundthuth will be when calculating the metric.

If I did not make myself clear, I would very much appreciate if someone would clarify the max detections part in the metrics!",6,tensorflow,2020-10-13
iqg216,Why TensorFlow Lite Has Been Running Slower On Recent Linux Kernels,,13,tensorflow,2020-10-13
iq9tl5,Easy ML mobile development with TensorFlow Lite Task Library,,3,tensorflow,2020-10-13
iq4js3,Data Generators - TF &amp; Keras,"I'm trying to implement a generator in Keras, but my system is still running out of memory (36GB available). Each set of 64 images and masks totals \~100MB. I can't even get through one epoch without consuming &gt;30GB of memory.

Here is my generator code:

    import numpy as np
    import tensorflow as tf
    import keras
    import glob
    import math
    
    
    def getFiles(stack_dir, mask_dir):
        '''
        ** find all stack images in dir., along with matching mask image files **
        ~~~~~~~~~
        INPUTS:
                - stack_dir = directory containing all stack images
                - mask_dir = directory containing all mask images
        RETURNS:
                - 2d array of stack and mask pairs [[stack_image_loc, mask_image_loc], ...]
        '''
        stacks = []
        masks = []
        for stack_image in glob.glob(stack_dir+""/*.npy""):
            x, y, z = stack_image.split(
                stack_dir+""/stack_tile_"")[1].split("".npy"")[0].split(""_"")
            mask_image = mask_dir+""/mask_tile_{}_{}_{}.npy"".format(x, y, z)
            stacks.append(stack_image)
            masks.append(mask_image)
        return stacks, masks
    
    
    class dataset_gen(keras.utils.Sequence):
        ' See https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence'
    
        def __init__(self, stacks, masks, batch_size):
            self.stacks = stacks
            self.masks = masks
            self.batch_size = batch_size
    
        def __len__(self):
            ' number of batches per epoch '
            return math.ceil(len(self.stacks) / self.batch_size)
    
        def __getitem__(self, index):
            print(""\n\n\n{}\n\n\n"".format(index))
            indices = [index*self.batch_size, index+1*self.batch_size]
            i = indices[0]
            temp = []
            while i &lt; indices[1]:
                temp.append(i)
                i = i+1
    
            X, y = self.__data_generation(temp)
    
            return X, y
    
        def __data_generation(self, list_IDs_temp):
            X = np.empty(
                (self.batch_size, *(256, 256), 5))
            Y = np.empty(
                (self.batch_size, *(256, 256), 3))
    
            for i, ID in enumerate(list_IDs_temp):
                X[i, ] = np.load(self.stacks[ID])
                Y[i, ] = np.load(self.masks[ID])
    
            return X, Y
    

and a basic training setup:

    import tensorflow as tf
    
    import sys
    
    from tensorflow.keras.backend import pow
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add
    from tensorflow.keras.losses import binary_crossentropy
    
    
    from resunetBuild import *
    from generator import *
    
    TRAIN_LENGTH=5000
    EPOCHS=20
    BATCH_SIZE=64
    BUFFER_SIZE=100
    STEPS_PER_EPOCH=TRAIN_LENGTH // BATCH_SIZE
    
    stacks,masks=getFiles('/home/george/Documents/Github/larger-dataset-test/Data/np_tiles/stack', '/home/george/Documents/Github/larger-dataset-test/Data/np_tiles/mask')
    
    train_dataset = dataset_gen(stacks, masks, BATCH_SIZE)
    
    
    model = resuneta(256, 256, channels=5, outClasses=3)
    adam=tf.keras.optimizers.Adam(lr=0.05, epsilon=0.1)
    model.compile(optimizer=adam, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
    
    history=model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)

&amp;#x200B;

Is there a problem with my generator, or is this likely the result of the model that I'm training?",5,tensorflow,2020-10-13
ipjnhk,Custom loss function: Using network parameters directly in a loss function.,"I am trying to implement the learned regularisation term (5) from https://arxiv.org/pdf/1806.06438.pdf in a custom loss function in tensorflow 2.1. 

I was wondering how to recover weights of a network, w, in a custom loss function, and how to then use them in the way presented. Suppose I have array \mu in R^d and Sigma in R^dxd already. The learned reg term is given by:

(w - mu)^T Sigma^-1 (w-mu)

I can recover weights in a loss function by doing: 
``model.trainable_weights.``
However this returns a list. I feel like my problem is because my python ability is limited.

My questions are 

1. How do I use the list / convert it to be used in the term I need in the tensorflow backend? tf.convert_to_tensor(model.trainable_weights) doesn't work.

2. Will calling model.trainable_weights change dynamically in the loss function after each epoch?",11,tensorflow,2020-10-13
ipbse4,"ValueError: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=""NCHW""](add, BiasAdd/ReadVariableOp)' with input shapes: [?,1024], [1024].","So I'm running the program [https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/) on my system but facing some error although a few months ago, there was no error. Kindly help me resolve the issue.

    def train_model(weight = None, epochs = 10):
      # load dataset
      data = ld.prepare_dataset('train')
      train_features, train_descriptions = data[0]
      test_features, test_descriptions = data[1]
      print(len(train_features),len(train_descriptions))
      print(len(test_features), len(test_descriptions))
    
      # prepare tokenizer
      tokenizer = gen.create_tokenizer(train_descriptions)
      # save the tokenizer
      dump(tokenizer, open('C:\\Users\HareeM\Image_Captioning_master5\models\\tokenizer.pkl', 'wb'))
      # index_word dict
      index_word = {v: k for k, v in tokenizer.word_index.items()}
      # save dict
      dump(index_word, open('C:\\Users\HareeM\Image_Captioning_master5\models\index_word.pkl', 'wb'))
    
      vocab_size = len(tokenizer.word_index) + 1
      print('Vocabulary Size: %d' % vocab_size)
    
      # determine the maximum sequence length
      max_length = gen.max_length(train_descriptions)
      print('Description Length: %d' % max_length)
    
      # generate model
      model = gen.define_model(vocab_size, max_length)
    
      # Check if pre-trained weights to be used
      if weight != None:
        model.load_weights(weight)
    
      # define checkpoint callback
      filepath = 'C:\\Users\HareeM\Image_Captioning_master5\models\model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'
      checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,
                    save_best_only=True, mode='min')
    
      steps = len(train_descriptions)
      val_steps = len(test_descriptions)
      # create the data generator
      train_generator = gen.data_generator(train_descriptions, train_features, tokenizer, max_length)
      val_generator = gen.data_generator(test_descriptions, test_features, tokenizer, max_length)
    
      # fit model
      model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=steps, verbose=1, callbacks=[checkpoint], validation_data=val_generator, validation_steps=val_steps)
    
      try:
          model.save('C:\\Users\HareeM\Image_Captioning_master5\models\wholeModel.h5', overwrite=True)
          model.save_weights('C:\\Users\HareeM\Image_Captioning_master5\models\weights.h5',overwrite=True)
      except:
          print(""Error in saving model."")
      print(""Training complete...\n"")
    
    if __name__ == '__main__':
        train_model(epochs=10)
    

Model is:

    def define_model(vocab_size, max_length):
      # feature extractor (encoder)
      inputs1 = Input(shape=(4096,))
      fe1 = Dropout(0.5)(inputs1)
      fe2 = Dense(256, activation='relu')(fe1)
      # sequence model
      inputs2 = Input(shape=(max_length,))
      se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
      se2 = Dropout(0.5)(se1)
      se3 = LSTM(256)(se2)
      # decoder model
      decoder1 = add([fe2, se3])
      decoder2 = Dense(256, activation='relu')(decoder1)
      outputs = Dense(vocab_size, activation='softmax')(decoder2)
      # tie it together [image, seq] [word]
      model = Model(inputs=[inputs1, inputs2], outputs=outputs)
      model.compile(loss='categorical_crossentropy', optimizer='adam')
      # summarize model
      print(model.summary())
      plot_model(model, to_file='model.png', show_shapes=True)
      return model

Error:

    Traceback (most recent call last):
      File ""D:/Hareem/Image_Captioning_master5/CapGenerator/train_model.py"", line 66, in &lt;module&gt;
        train_model(epochs=10)
      File ""D:/Hareem/Image_Captioning_master5/CapGenerator/train_model.py"", line 38, in train_model
        model = gen.define_model(vocab_size, max_length)
      File ""D:\Hareem\Image_Captioning_master5\CapGenerator\generate_model.py"", line 126, in define_model
        se3 = LSTM(256)(se2)
    
    ValueError: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=""NCHW""](add, BiasAdd/ReadVariableOp)' with input shapes: [?,1024], [1024].",4,tensorflow,2020-10-13
iow8ik,why is my TF GAN not nearly as good as my PyTorch GAN?,"I   understand this will be a lot to ask of someone, but I've been at this   for over a month now and can't seem to improve my TF GAN so I need  some  help.

I'm just trying to  learn the  ropes. It's been a very useful experience, and I think my  code will be  helpful to many people. I started with Colab notebooks  from TF official  and a well known PyTorch tutorial and then commented  and improved the  code along the way. The TF GAN example uses MNIST at  28x28x1 for  example, and I increased the model to cater to higher res.  It's possible  that my architecture there is bad.

Long story short, at about 60 hours of training a 128x128x3 GAN in TF, epoch 17000\~ looks like this:

&amp;#x200B;

https://preview.redd.it/9qyl1uwn4yl51.png?width=1152&amp;format=png&amp;auto=webp&amp;s=b105333e65044a3b2f6b6e42231939f8381e0e41

Whereas  at a little less time than that, my PyTorch GAN at 512x512x3 looked  like this (obviously this is a single image as opposed to a 4x4 image  grid like above):

&amp;#x200B;

https://preview.redd.it/tgxlnyoo4yl51.jpg?width=512&amp;format=pjpg&amp;auto=webp&amp;s=7961d73ac9bb9e5dec7869db71ba1e95a903f1c0

The   pyTorch GAN is clearly better. I want to transition to TF, so please  no  ""just use PyTorch"" answers, unless there is a clear superiority to  the  PT library.

Here is my TF colab: [https://colab.research.google.com/drive/1A9Z5nBT3zZ03PtPSaHc97\_Cmry3bNwpH?usp=sharing](https://colab.research.google.com/drive/1A9Z5nBT3zZ03PtPSaHc97_Cmry3bNwpH?usp=sharing)

Note that the generator has three options, I am concerned with the 128x128 architecture at the moment.

Here is my Torch code:

[https://colab.research.google.com/drive/1\_pzhQrt9NdVQxp47xFqIU7kjXmh-OUqg?usp=sharing](https://colab.research.google.com/drive/1_pzhQrt9NdVQxp47xFqIU7kjXmh-OUqg?usp=sharing)

I used the same flowers photo dataset for both: [https://drive.google.com/file/d/16YGAAJOStFmzNn5fmM\_fDVoyntW0hq1L/view?usp=sharing](https://drive.google.com/file/d/16YGAAJOStFmzNn5fmM_fDVoyntW0hq1L/view?usp=sharing)",31,tensorflow,2020-10-13
iow3gd,Why tf.einsum is different than np.einsum?,"Can someone help me with this question?

I have two tensors, A and B:

A = tf.Variable(initial_value=tf.random_normal_initializer()(shape=(1, 64, 4, 1, 1)))
B = tf.Variable(initial_value=tf.random_normal_initializer()(shape=(32, 1, 4, 256, 256)))

I'm trying to perform this operation:

tf.einsum('abcde,abcde-&gt;abde', A, B) # in other words, ""K.sum(A * B, axis=2)""

But when I run the code, I get a error:

Expected dimension 1 at axis 0 of the input shaped [32,1,4,256,256] but got dimension 32 [Op:Einsum]

I don't understand what is happening, because the same code works using numpy:

A = np.ones((1, 64, 4, 1, 1))
B = np.ones((32, 1, 4, 256, 256))

np.einsum('abcde,abcde-&gt;abde', A, B) # works great

Why do the tensors need to have the same shape on tensorflow? I don't want them to have the same shape because it will consume a LOT of memory, the same code on numpy works perfectly.",2,tensorflow,2020-10-13
ioveog,Insert Layers in pre-trained Model,"Hey People,

I'm trying to insert some custom layer that shifts the tensors output by a layer in a certain way before it is input to the next layer into a pre-trained model. For example when I load a VGG16 with: 

from tensorflow.keras.applications.vgg16 import VGG16

base\_model = VGG16(weights='imagenet', include\_top=False)

Does anyone know how to insert a custom layer between block1 and block 2 for example? The only solution I currently see is that I extract the weights of each layer from the pre-trained model, design a new model with the same layers but my custom layer in between the blocks, and then load the extracted weights of the pre-trained model into the according layers of my new model.

Has anyone an easier approach?",2,tensorflow,2020-10-13
iosbe5,Easy way to give custom loss an optimizer?,"I want to use a custom loss but the caveat is that I need predictions on the entire dataset to the computer the loss. The problem is, the dataset is too big to be computed in a single batch. What I am going to do is compute predictions on large batches and then calculate the loss for an epoch (in this scenario, one epoch should be one optimization step). How can I supply my loss to an optimizer like Adam. What I am trying to do in pseudo code is something like

model.compile(my_custom_loss)
model.fit(x, y, batch_size=len(x))

Although I need to pre compute the predictions and loss because the dataset is too big to compute in a single batch.",1,tensorflow,2020-10-13
ionajb,Implementing Research Papers - for beginners,"So I have been looking up on some advice as to how to implement research papers. There are numerous guides out there for pytorch but I wanted to go with Tensorflow2 cz I'm familiar with the syntax. Can someone recommend some resources for getting started with implementing research papers as it looks like a mammoth task to me rn.
I tried implementing a GAN based style transfer paper and even though I feel like I've done things right im not getting the desired results. So I would like to learn this art of implementing papers properly before I go ahead with anything else. 
Would be really grateful if someone could link some really cool resources in the thread.

Thanks in advance❤️

(PS I have looked into paperswithcode.com but would like to implement papers that don't have any existing implementation)",8,tensorflow,2020-10-13
iom24j,Latest in drones! Efficient trajectory generation for chasing a dynamic target,,0,tensorflow,2020-10-13
iok3k2,Need some help,"Can you all lead me in the right direction. I need to use tensor flow for facial characteristic recognition. So I will be id’ing specific facial traits. 

Any help would be amazing and much appreciated. 

You are all amazing!",1,tensorflow,2020-10-13
ioihwe,Artificial Intelligence Academy (AIA),"Our mission is to Train and Certify the next generation of software developers and engineers worldwide in artificial intelligence and machine learning. Niches: Artificial Intelligence, Machine Learning, Data Science, Python, TensorFlow, PyTorch, Convolutional Neural Networks.. Top Partners: Google, Microsoft, IBM, DeepLearning.ai, MIT, Johns Hopkins, Stanford University, University of Michigan, UC Berkeley, Coursera, Pearson IT, EDx and Edureka.

Visit us today at:  https://aimlacademy.blogspot.com/    
  
Lawrence E. Wilson",2,tensorflow,2020-10-13
ioccd4,Trading environment for TF-Agent,,1,tensorflow,2020-10-13
ioaknb,I used tf.compat.v1.estimator.experimental.KMeans to implement KMeans on a dataset. How do I interpret the outputs?,"Basically I want to start making the change from sklearn to TensorFlow to do some more interesting stuff, but as far as I can tell I can't get a basic k means implementation in keras or TensorFlow in much the same way you can in sklearn.  I ended up on [this webpage](https://www.tensorflow.org/api_docs/python/tf/compat/v1/estimator/experimental/KMeans), which does k means, but I found it rather not user friendly.  When I ran the algorithm on a smallish dataset (\~130 instances of 121 attributes each), I got a lot of outputs like ""cluster center"", which I understand as just where the clusters are located, but then I also got outputs ""score"" (gave me a large integer value), ""delta"" (output a vector and occasionally the zero vector), and ""point"" (told me what points were in what cluster), which I don't understand.  

I was wondering if someone could explain what these outputs mean and how to interpret them.  My project that I'm using this for is to try to sort Shakespearian sonnets into similar categories.  I already did all the work like reading the text files, transforming each poem from text to numeric data and making all the input tensors into constant length of 121.  I'm more concerned about learning how to manage messy data than get groundbreaking results, but at the very least I would like to try to have something I can be proud to put on my GitHub.  Or if someone knows a better ML model to use than k means I would be glad to hear about that as well.  

As a side note I am also wondering if there is any way to create an embedding (I think this is the correct term) of a large input dataset (in my case 121 dimensional vectors), into much more manageable sizes (say 20 dimensions).  I know you can do it in keras sequential layers, but I was wondering how I could create this dimension squishing in an unsupervised network.",1,tensorflow,2020-10-13
io9ivn,I published a tutorial where I explain how the Short-Time Fourier Transform works,"The Short-Time Fourier Transform 🎧 🎧 is one of the most important tools an AI audio / music engineer has. It enables them to extract spectrograms, the main feature we feed to DL audio models. In my new video, I explain the theory behind the Short-Time Fourier Transform in a simple way.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=-Yxj3yfvY-4&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=15](https://www.youtube.com/watch?v=-Yxj3yfvY-4&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=15)",2,tensorflow,2020-10-13
io50vp,Guide to install Tensorflow 2.3.0 on Raspberry Pi 3/4 (Debian Buster),"[https://medium.com/@cawin.chan/installing-tensorflow-2-3-0-for-raspberry-pi3-4-debian-buster-11447cb31fc4](https://medium.com/@cawin.chan/installing-tensorflow-2-3-0-for-raspberry-pi3-4-debian-buster-11447cb31fc4)

It works surprisingly well! But you should aim for your model to be as small as possible to be around 1MB, you can look at Tensorflow post-quantization techniques which can reduce up to around x4 the size of your model with almost negligible accuracy loss (from my experience).

I achieved a prediction speed of around 1-2s for a 6MB h5 model, but this same h5 model converted to tf lite model now at 1MB would have a prediction speed of around 90ms.

Which really took me by surprise on how great of a performance improvement tf lite was able to churn and how much a rpi could handle a TF model.",21,tensorflow,2020-10-13
io16oc,How does a Self-taught machine learner learn advanced custom training and loss functions?,I am a self-taught machine learning person and I have a few personal projects that I have made like Image Captioning and CycleGAN but is there anyway to learn about these advanced TF functions.(I'm 14 btw so math isn't going to be my strong point),3,tensorflow,2020-10-13
io0rr6,"Introduction to TensorFlow for AI, Machine Learning, and Deep Learning",,1,tensorflow,2020-10-13
inug0e,First model ever went horrible. Any suggestions?,,3,tensorflow,2020-10-13
inrwdb,Generating text with GRU or LSTM,"I’m trying to create a network which can continue a sentence when given with a starting word. My dataset consists of ~21,000 samples with an average length of 70 characters each. I’ve seen great I’ve neural networks done with both keras GRU layers or LSTM layers.

I’ve seen that these layers are quite similar and was wondering which layer would be best to use for this application.

Thank you for any answers!",1,tensorflow,2020-10-13
inoq0t,TensorFlow Datasets: The Bad Parts,,18,tensorflow,2020-10-13
inko2r,Binary and Categorical classification in TensorFlow,,4,tensorflow,2020-10-13
inh4ho,Issue with Tensorflow installation,"This is the error message that I get when I try to install tensorflow using Anaconda. I am getting the same error when I tried installing tensorflow-gpu, how should I go about this issue?

(base) C:\\Users\\adwit&gt;conda install tensorflow

Collecting package metadata (current\_repodata.json): done

Solving environment: failed with initial frozen solve. Retrying with flexible solve.

Solving environment: failed with repodata from current\_repodata.json, will retry with next repodata source.

Collecting package metadata (repodata.json): done

Solving environment: failed with initial frozen solve. Retrying with flexible solve.

Solving environment: -

Found conflicts! Looking for incompatible packages.

This can take several minutes.  Press CTRL-C to abort.

failed

&amp;#x200B;

UnsatisfiableError: The following specifications were found

to be incompatible with the existing python installation in your environment:

&amp;#x200B;

Specifications:

&amp;#x200B;

  \- tensorflow -&gt; python\[version='3.5.\*|3.6.\*|3.7.\*'\]

&amp;#x200B;

Your python: python=3.8

&amp;#x200B;

If python is on the left-most side of the chain, that's the version you've asked for.

When python appears to the right, that indicates that the thing on the left is somehow

not available for the python version you are constrained to. Note that conda will not

change your python version to a different minor version unless you explicitly specify

that.

&amp;#x200B;

The following specifications were found to be incompatible with your system:

&amp;#x200B;

  \- feature:/win-64::\_\_cuda==11.0=0

  \- feature:|@/win-64::\_\_cuda==11.0=0

&amp;#x200B;

Your installed version is: 11.0",2,tensorflow,2020-10-13
indpdk,Anyone interested in mentoring?,"So im kinda new to tensorflow i have used it once in the past on a project with a dog food bowl cover to prevent my cat from eating my dogs food. However i want to better understand it and i do so much better when i have someone i can work with. It helps me grasp the concept better when someone can show and i can replicate or ask questions.

&amp;#x200B;

So i have a project now. My goal is as follows:

1. setup tensorflow to work on aws
2. setup an s3 bucket for tensorflow to pull images from
3. setup an s3 bucket to house the neural network
4. setup a raspberry pi to take photos and save to bucket in S3 from 2
5. possibly use the initial neural network here however setting up my own would probably help me understand tensorflow better.

What i would like tensorflow to do:

1. determine the type of plant in the picture
2. determine the growth stage of the plant
3. determine with the plant has produced produce or has flowered

ultimately i would like to grow various cycles with different nutrients and co2 levels to determine optimum growth scenarios for rare plants. So if it could keep track of plant to harvest times as well as size of produce that would be great and basically once trained be able to say optimal of suboptimal during additional growth cycles

&amp;#x200B;

Update: after a lot of trial and error I now have a script that uploads to an S3 bucket as well as a ec2 instance that is able to run tensorflow. I have been in the process of watching [this video](https://www.youtube.com/watch?v=tPYj3fFJGjk) and im currently 4 hours in. I have also found a couple of raw datasets (by this i mean literally folders with photos and metadata) and have started to king of work out how i would like my neural network to work. [see my thoughts on my nueral network setup here](https://mm.tt/1612582579?t=Ef8QU8hnFe)",4,tensorflow,2020-10-13
in0w5y,Best way to get lots of images for image classification,"Hello,

I was thinking of writing something that identified if a vessel was for bulk, containers, or cars but then it dawned on me that I would have to pull all these images from the web. Does anyone know of any good websites/sources that already do this? I would really hate to pull 500 images manually just for a fun weekend project.

I",3,tensorflow,2020-10-13
imzjj9,Add preprocessing layer to Model,"Hi I have a ResCNN Keras model that works with fbanks as inputs. I would like to include a preprocessing layer so the conversion from wav to fbank will be done inside the net. Righit now I'm trying like this:

    original_model = Model(inputs, x, name='ResCNN')  
    input = Input(shape=(SAMPLE_RATE,None))  
    preproc_layer = PreprocessingLayer(wav_tofb)(input)  
    output = original_model(preproc_layer)  
    model = Model(input, output) 

The proprocessing function is:

    def wav_tofb(input_filename): 
        audio = Audio.read(input_filename, SAMPLE_RATE) 
        energy = np.abs(audio)
        silence_threshold = np.percentile(energy, 95) 
        offsets = np.where(energy &gt; silence_threshold)[0] 
        audio_voice_only = audio[offsets[0]:offsets[-1]] 
        fb = fbank(audio_voice_only, SAMPLE_RATE) 
    return fb 

The problem is that when I try to do:

    model.predict('audio.wav') 

it doesn't work. How I can fix it? I think the problem is the input I give to the net. The original model have the following inputs:

    input (InputLayer)              [(None, 160, 64, 1)] 0",1,tensorflow,2020-10-13
imyvet,StyleGAN2 generates Ricardo Milos,,23,tensorflow,2020-10-13
imwn8j,tensorflow looking for old version of cuda,"I have a Window 10 machine with Cuda v11 and I have pip-installed tensorflow 2.3. 
But tensorflow is trying to load cudart64_101.dll, also some other dll from older versions.
What am I doing wrong ?",3,tensorflow,2020-10-13
imss1q,Saving unused variable in model created with the Keras functional API (TensorFlow 2.0),"I have some variables I would like to save in a Keras model so that when the model is saved to disk and later loaded, the variables are accessible from the model. However, the model itself doesn't use these variables: they are intended as hints to the user about what the model does and how to use it.

With the Keras functional API, I don't see an obvious way to do this. I could add a layer whose only function is to store these variables, but I think I would still need to connect it to the rest of the graph somehow so that it becomes part of the model. Is there a simple way to do this that I am missing?",3,tensorflow,2020-10-13
imoi8d,Confused about CSV formatting in data inputting?!,"I’m building a neural network that will predict the outcome between two sides based on statistics from previous fights. However I have a few questions on how I should format the data in the CSV file:

1. Should I put the name of the fighter / team of allocate each of them a number which would be the winning / losing end result given to us.

2. How should I format the data in the CSV file?

3. How should I separate the training data from the CSV file from the testing data in the CSV

Any suggested answers to these would be great!",1,tensorflow,2020-10-13
imn425,Using intermediate preprocessing layers in custom loss,"I created a preprocessing layer that just applies Sobel filter to the input and concatenates it as follows:  
class SobelPreprocessor(tf.keras.layers.Layer):

    class SobelPreprocessor(tf.keras.layers.Layer):
        def __init__( self, **kwargs):
            super(SobelPreprocessor, self).__init__(**kwargs)
    
        def call(self, inputs):
            sobel = tf.image.sobel_edges(inputs)
            sobel_y = sobel[...,0]
            sobel_x = sobel[...,1]
            return tf.concat([inputs, sobel_y, sobel_x], axis=-1)

I want to use those tensors (sobel\_y and sobel\_x) in a custom loss, but how can I retrieve them inside this function?  

    @tf.function
    def custom_mean_squared_error(y_true, y_pred):
        y_pred = tf.python.framework.ops.convert_to_tensor_v2(y_pred)
        y_true = tf.python.math_ops.cast(y_true, y_pred.dtype)
    
        #sobel_x and sobel_y needed here !!!
    
        return K.mean(tf.python.math_ops.squared_difference(y_pred, y_true), axis=-1)

There is something like ""sobel\_x = self.model.get\_layer('sobel\_layer')\[0\] "" ?",3,tensorflow,2020-10-13
imjjoq,How TF-Coder Works? (Explained),"How does TF-Coder synthesize the answers to TensorFlow questions in  StackOverflow at the ‘superhuman’ level? What are the technologies  behind it?

Check out my new blog post. 

[https://us.github.io/how-tf-coder-works](https://us.github.io/how-tf-coder-works)",9,tensorflow,2020-10-13
imfvuv,Lean how to Install TensorFlow Using Anaconda Navigator &amp; Prompt In 18 Minutes,,0,tensorflow,2020-10-13
im5ev8,Solution for long epochs,"Hi All,

When i was running quickstart MNIST model.fit() was showing an ETA of hours per epoch. Passing in verbosity=2 sped things up immensely (hours to seconds). 

Hopefully this helps someone who doesn't have time to dive down a rabbit hole of troubleshooting. I was using tf-nightly-gpu for cuda 11 and fully expected the troubleshooting. 

Tldr ""why isn't tensorflow using my GPU"" turned out to be caused by model fit default verbosity instead of GPU related",7,tensorflow,2020-10-13
ilxeoz,help getting gradients of a gradient computation,"I have a fairly complicated custom training loop (using TF2) and my loss function depends on the gradients of the output wrt the input. So in my loss function I use a gradient tape to get the gradients of the output of the network wrt the input passed in. However, in the overall training loop, I use a different gradient tape to get the gradients of the loss function with respect to the network weights so I can update the network. TF is telling me that the gradients of the loss wrt the network weights don't exist, although it is able to compute the gradients of the network output wrt the input within the loss function. How do I achieve/fix this?

&amp;#x200B;

here is the loss function, where I am able to get numerical gradient values of the output wrt the input

&amp;#x200B;

https://preview.redd.it/te0unjk3vyk51.png?width=720&amp;format=png&amp;auto=webp&amp;s=e9ea99aa512c790f075a9c3411b224d964249759

and here is where I update the network with the training loop

&amp;#x200B;

https://preview.redd.it/wq1kq6afvyk51.png?width=1256&amp;format=png&amp;auto=webp&amp;s=4b8e7c37651bf9d3beabb6aa3423847fb4eede20

I get warnings from the training loop saying that the gradients don't exist",1,tensorflow,2020-10-13
ilvfep,Sports betting data using AI to predict results.,"I’m using the tensor flow module in python to aid in my project set by my professor to create a program to predict soccer matches, the only problem is that I’m confused about how the data should be presented. 
Data types e.g. possession, av shots, av goals etc.
Any suggestions on how to layout this data?",1,tensorflow,2020-10-13
ilucmt,Hyperparameter tuning with Keras and logging for Tensorboard,"I'd like to find out how to use Keras tuning api together with HParams Dashboard for tensorboard.

I've found a nice example of using HParams Dashboard for tensorboard here:

[https://www.tensorflow.org/tensorboard/hyperparameter\_tuning\_with\_hparams](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams)

Unforunately the example doesn't use the keras tuner.

There are nice examples of using the Keras tuner here:

[https://github.com/keras-team/keras-tuner/blob/master/examples/helloworld.py](https://github.com/keras-team/keras-tuner/blob/master/examples/helloworld.py)

However there aren't any examples of it being used with tensorboard

Thanks in advance",5,tensorflow,2020-10-13
ilpu89,"John Snow Labs Spark-NLP 2.6.0: New multi-label classifier, BERT sentence embeddings, unsupervised keyword extractions, over 110 pretrained pipelines, models, Transformers, and more!",,16,tensorflow,2020-10-13
ilptk5,Is GradientDescentOptimizer's minimize function meant to be used in loop?,,1,tensorflow,2020-10-13
iloxjl,Difference between load and load_weights in keras,"It may be obvious for most of the people, but it wasn't for me, so I'm going to share what I have found.

When a model is saved through `model.save` it is possible to restore it in two ways: both recreating the model through `model = tf.keras.models.load_model(..)` or by creating the model manually and then loading the weights through `model.load_weights(..)`.

What I have found is that if you load a model through `load`, you will get a model identical to the one you have saved **included the optimizer state**, while if you load the model through `load_weights` the state of the optimizer will be discarded. Which means that if you are going to continue training a model which you have checkpointed through the model save callback, you will want to recreate the model using `model = tf.keras.models.load_model(..)`, or else your model will get worse during the first training epoch since your optimizer will have lost its internal state. On the other hand, if you plan to use a trained model in inference mode only, or you want to use it for transfer learning, `load_weights` is perfectly fine, since you won't need the state of the optimizer. 

While we are at it, I have found that the `experimental_steps_per_execution` option in compile, which is extremely useful while training models on TPU, is not being saved by model.save, so when you load the model using `model = tf.keras.models.load_model(..)` you need to set this property manually, which can easily done in this way:
```python
with strategy.scope():
    model = tf.keras.models.load_model('path/to/checkpoint.h5')
    model._configure_steps_per_execution(nsteps)
```

Where nsteps is the number of steps per execution. Ideally, you will want nsteps to be big enough so that the time required for on_batch_end operations is much smaller than the time required to train for that number of batches. This number can easily be found by looking at the timings included in the warning `Callbacks method on_train_batch_end is slow compared to the batch time` which is what tells you that you should use the `experimental_steps_per_execution` option. 

Have a nice day!",2,tensorflow,2020-10-13
ilijal,Recovering multi-person 3D poses from a single RGB image!,,12,tensorflow,2020-10-13
ildm1t,Tensorflow supports python 3.8.5?," Tensorflow says they support python 3.5-3.8 . Anybody know if it supports 3.8.5 ?

Just not sure if they mean strictly 3.8.0 or 3.8.x.",7,tensorflow,2020-10-13
il0yru,Confused about input shape for MobileNetV2 + LSTM,"I'm working on drowsiness detection using [this dataset. ](https://sites.google.com/view/utarldd/home)

The dataset has 3 classes of different awareness levels with 60 videos for each class and each video is about 10 mins long. 

Steps for preprocessing videos:

Converted to frames(a frame every 5 seconds).
Rotated them to ensure all the faces are vertical.
Extracted the faces from the frames. 
Combined all the frames in each class to a new folder. (example all frames of class 10 are combined together and each class is nearly perfectly balanced except 1 class which had 1% less frames than the others).
Split them into train(95%), validation(5%) and test(5%).

Now I am using transfer learning with MobilenetV2 for the spatial features then a LSTM layer for temporal features.

I resized the frames into (224,224,3) as MobilenetV2 requires them in this size and I load them in using tf.keras.preprocessing.image_dataset_from_directory in batches of size 32.

MobilenetV2 function:

```
def build_mobilenet(shape=INPUT_SHAPE, nbout=CLASSES):

    # INPUT_SHAPE = (224,224,3)

    # CLASSES = 3

    model = MobileNetV2(

        include_top=False,

        input_shape=shape,

        weights='imagenet')

    base_model.trainable = True

    output = GlobalMaxPool2D()

    return Sequential([model, output])

```

LSTM function:

```
def action_model(shape=INSHAPE, nbout=3):

    # INSHAPE = (5, 224, 224, 3)

    convnet = build_mobilenet(shape[1:])
    
    model = Sequential()

    model.add(TimeDistributed(convnet, input_shape=shape))

    model.add(LSTM(64))

    model.add(Dense(1024, activation='relu'))

    model.add(Dropout(.5))

    model.add(Dense(512, activation='relu'))

    model.add(Dropout(.5))

    model.add(Dense(128, activation='relu'))

    model.add(Dropout(.5))

    model.add(Dense(64, activation='relu'))

    model.add(Dense(nbout, activation='softmax'))

    return model
```

When I train the model I get the following error which I think means I'm trying to feed something like (32, 224, 224, 3) but my model needs (32, 5, 224, 224, 3). 

```
ValueError: Input 0 of layer sequential_16 is incompatible with the layer: expected ndim=5, found ndim=4. Full shape received: [None, 224, 224, 3]
```

I am not sure what to do to fix this.",4,tensorflow,2020-10-13
ikq115,Time Series Forecasting with change at certain point,"I have several records that all follow the same pattern. All of them change their behaviour from a variable point in time. This point is known in advance. What is the best way to pass on this fixed point next to the Time Series?

Thanks to everyone who helps me.",1,tensorflow,2020-10-13
ikiuc7,Extract weights of every hidden layers,"Hello community , as the title spoke of itself
I can see the distribution of the weights using tensor board but that is not what I’m looking for ,  , I want to extract weights  of every layer of my model and be able to see the tensor values of it .
Any solution ??
Thank you",1,tensorflow,2020-10-13
ikeqn9,Generating photo-realistic face images from hand-drawn sketches!,,6,tensorflow,2020-10-13
ikeavb,TfLite Android: Garbage values when running inference for multiple output model,"I have a model that predicts the age and gender of the input image of size 160X160. I am creating a byte buffer to input the image to the model and everything works just fine when using a model with only one output.

But when I am using the tflite.runForMultipleInputsOutputs(), I am getting garbage values which are of the form -&gt; \[\[F@e233 etc.

I have followed the documentation and the sample apps to the detail and have been stuck at this for almost 2 days. Please help. 

I am posting my code below for reference.

The model has 2 outputs: 

* age -&gt; float32 \[1, 2\]
* gender -&gt; float32 \[1,101\]

P.S - I am not doing anything with the output as of now. I just want to see the result of the model.

    String classifyImage(Bitmap bitmap){
            try{
                ByteBuffer byteBuffer = convertBitmaptoByteBuffer(bitmap);
    
                float[][] out_gender = new float[1][2];
                float[][] out_age = new float[1][101];
                Object[] input = {byteBuffer};
    
                Map&lt;Integer, Object&gt; outputs = new HashMap();
                outputs.put(0, out_age);
                outputs.put(1, out_gender);
    
                interpreter.runForMultipleInputsOutputs(input, outputs);
                
            }catch (Exception e){
                e.printStackTrace();
            }
            return """";
        }",1,tensorflow,2020-10-13
ikdsrw,Tensorflow Certificate Network dead?,"Just a quick question - is the certificate network dead as of early July?
Seems like the TF network hasn’t been updated for well over 2 months now and I want my name to be displayed!!! :D",5,tensorflow,2020-10-13
ik8pxk,ASL to English,"I'm an occupational therapy student and am wondering if anyone has tried to develop this demo into a real program:
https://github.com/shekit/alexa-sign-language-translator

It doesn't look like anything has happened for 2 years.

According to a person on deaf reddit, the current programs have a very limited vocab and require precise signing, but I thought you may have a handle on making the program work better and maybe we could collab with ASL signers to develop the machine learning further.

Also is there translation from Spanish to English in tensor flow?

Update: I've been talking to the creator and he confirmed that he trained and tested in the same environment.",3,tensorflow,2020-10-13
ik7gti,Calculating margins in tensorflow 2,"I'm trying to update some code from tensorflow 1 to tensorflow 2.

Could someone help me with an error I have with the following line of code?

    layer_activations = [end_points_collection[l] for l in loss_layers] 

I got it from:

[https://github.com/google-research/google-research/blob/master/demogen/margin\_utils.py](https://github.com/google-research/google-research/blob/master/demogen/margin_utils.py)

However, I'm getting the following traceback. In another post, I learned that the error comes from end\_point\_collection being empty. This was called from `model_config.get_modelfn()` which also calls `contrib_training.HParams()` from `tensorflow.contrib`. Alternatively, if anybody knows how to calculate layer margins with a keras or tensorflow 2, that would be greatly appreciated.

    Traceback (most recent call last):
    File ""C:\Users\user\.conda\envs\tf2\lib\site-packages\IPython\core\interactiveshell.py"", line 3417, in run_code
            exec(code_obj, self.user_global_ns, self.user_ns)
    File ""&lt;ipython-input-49-ccf9839481e1&gt;"", line 1, in &lt;module&gt;
            layer_activations = [end_points_collection[l] for l in loss_layers]
    File ""&lt;ipython-input-49-ccf9839481e1&gt;"", line 1, in &lt;listcomp&gt;
            layer_activations = [end_points_collection[l] for l in loss_layers]
    KeyError: 'inputs'",1,tensorflow,2020-10-13
ik4nom,AI and Deep Learning with TensorFlow,"AI and Deep Learning in TensorFlow with Python Certification Training master the concepts of SoftMax function, Autoencoder Neural Networks, Restricted Boltzmann Machine (RBM) and work with libraries like Keras &amp; TFLearn. Gain high-demand skills in Deep Learning and TensorFlow Concepts, Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Keras, TFlearn, Autoencoders, Restricted Boltz-mann Machine (RBM), Neural Networks &amp; Natural Language Processing (NLP), Python with TensorFlow Libraries, Text Analytics and Processing. 
Training modules include: 1) Introduction to Deep Learning, 2) Neural Networks with TensorFlow, 3) Deep Networks, 4) Convolutional Neural Networks (CNN), 5 )Recurrent Neural Networks (RNN), 6) Restricted Boltzmann Machine (RBM) and Autoencoders, 7) Keras API, 8) TFLearn API, and 9) In-Class Project.

Register today at: https://fxo.co/9PYj  

Much career success, Lawrence E. Wilson - Artificial Intelligence Academy (AIA)",0,tensorflow,2020-10-13
ik0100,Calculating loss from both good and bad examples,"Hi, I’m new to TensorFlow and machine learning, so please forgive my ignorance. I have a model that I’m trying to train to play a game by making moves based on the board state. The training data has examples of games lost and games won. It’s easy to find the loss for games won, because I know what the optimal move should’ve been. For games lost however, I only know what I shouldn’t have done. I also have a metric for how completely I won or lost that should probably factor in there somehow.

My question is: should I,
1) Only train on the winning data
2) Create a model that predicts the winning move and a model that predicts the losing move and compare them
3) Somehow find a way to feed all of it into the single network, a problem I’ve been struggling to figure out for a while

Any help would be greatly appreciated!",1,tensorflow,2020-10-13
ijzgct,I published a tutorial where I explain how to extract the Fourier Transform from audio with Python,"In my new video, I explain how to extract the Fourier Transform from an audio file with Python and Numpy. I also visualise and compare the magnitude spectra of the same note played on different musical instruments. 

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=R-5uxKTRjzM&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=14](https://www.youtube.com/watch?v=R-5uxKTRjzM&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=14)",28,tensorflow,2020-10-13
ijyi3d,[Tutorial] A Guide to TensorFlow Callback Functions,"TensorFlow callbacks have become a basic and essential part of training deep learning models. For instance, the EarlyStopping callback is commonly used to prevent overfitting; ModelCheckpoint is critical to prevent losing progress from interrupted training, or being able to return to a previous point in time; and TensorBoard is commonly used to visualize the progression of your model training.

This article covers the 10 main callback functions in TensorFlow, with runnable Python code included.

Tutorial link:  [https://blog.paperspace.com/tensorflow-callbacks/](https://blog.paperspace.com/tensorflow-callbacks/) 

Run the code for free: [https://ml-showcase.paperspace.com/projects/tensorflow-callback-functions](https://ml-showcase.paperspace.com/projects/tensorflow-callback-functions)",3,tensorflow,2020-10-13
ijy87z,Using Portuguese BERT with SparkNLP,"Hey, guys. SparkNLP has the option of using Embeddings, but only in English and Multi-language. I tried to train a NER model using multi_cased BERT, but when used in a pipeline, the resulting labels were all the same: ""in"". 
I was wondering if there's a way of downloading a tensorflow checkpoint (from neuralmind/portuguese-bert) and using it as an input for the training algorithm. Or even transform it to the format that works with SparkNLP. I don't know a lot about tensorflow and I'm struggling with that. 

Thanks!!",2,tensorflow,2020-10-13
iju7co,Seeking Tensorflow/Keras lead developer that specializes in object detection," Hello! I started a company (Naptic) about a year ago built around gun detection for ip cameras and recently received a 18m valuation. I need to hire an engineer to help deploy and develop our AI to act as head of AI development as well as a co-founder. This position would include a generous salary and a couple million dollars in company shares. If you or anyone you know would be a good fit for this position and are interested, please contact me at [jeffschulze@yahoo.com](mailto:jeffschulze@yahoo.com). Thanks for your help!",6,tensorflow,2020-10-13
ijqahx,Cross Validation in Object Detection,"Assuming a small dataset, in general machine learning problems, one should do cross validation.

But in the case of Object Detection I've been looking for some explanation on this topic but seem to find no concrete answer, and some say we should not do it.

Can someone elaborate on the matter? Should we do it or not?",4,tensorflow,2020-10-13
ijgcoe,Help for GSoC preparation.,"Hi, I'm a beginner in python and Tensorflow. I'm interested in AI and machine learning. I have done some introductory courses online on ML. I want to participate in GSoC 2021. I know the basics of python and github, but I'm unable to decide what to do next to get into open source development or how to contribute to the community. Can anyone guide me how to get started? Can you guide me to resources/tips to get better chances at cracking GSoC?

PS: I'm completely lost. I don't know what to do with python by knowing just the basics like if/else statements, loops etc. and also I'm not able to do much in ML other than basic classification, regression tasks. Any help is highly appreciated. Thanks in advance.",2,tensorflow,2020-10-13
ijczpa,My Tensorflow Tutorial Predicting Numbers,,5,tensorflow,2020-10-13
ijc7wr,Installing TensorFlow for AMD GPU in Windows,"Hi I just tried to install TensorFlow for the first time in my laptop but what I got was

https://preview.redd.it/zagtuz91z4k51.png?width=1470&amp;format=png&amp;auto=webp&amp;s=9b32848ce9406ff1f22efaaf58f6c411132c4df9

I am having **AMD Radeon (TM) R5 M330.** if I have an Nvidia GPU I could install CUDA but What could I do now for AMD GPU?",1,tensorflow,2020-10-13
ijblh0,[ELI5] Why is Tensorflow so difficult to install?,"I have some experience in Python and Python packages in general but I'm always amazed by the difficulties in installing Tensorflow. I couldn't find any explanations about the great number of encountered errors compared to other libraries.

PS: this is not a rage post, I'm genuinely curious.",15,tensorflow,2020-10-13
ij7b8j,Alfred Workflow to quickly jump to the TensorFlow official API docs https://github.com/lsgrep/mldocs,,44,tensorflow,2020-10-13
ij3du8,State of the art in lip-syncing a talking face video!,,2,tensorflow,2020-10-13
iix5cp,Storing a list of tensors(each of different shape),"Hi, I have a list of tensorflow tensors(some of them constants, and some tf.Variables), of different shapes. How to I store a list of these lists, to easily feed them back to SGD.minimize type calculations, while using multiprocessing(this last bit is a problem I haven't been able to solve yet)",7,tensorflow,2020-10-13
iirym9,Creating nice indoor maps from drawings,"Hi,

I'm quite fresh to machine learning and tensorflow but would love some idea of which direction to go on this one.

I have rough drawings and lidar scans of indoor maps, but I want to make basically ""nice"" looking maps from these scans and drawings.

My original idea was to use a CycleGAN and hand draw some of the maps myself as the truth. Then let it train itself on those.

What I don't know is if it would translate well to later use with brand new scans and drawings.

&amp;#x200B;

Any ideas or recommendations would be great. Thanks",1,tensorflow,2020-10-13
iihkht,How do you put a distribution over parameters?,"(TF 1.15, without keras)

My training data is a bunch of (x,y) pairs, and I'm trying to set up a regression. I'm learning a linear function yhat = Ax+B, with A and B as tf.Variables, and I'm optimizing loss=tf.norm(yhat-y) using AdamOptimizer.

This works, and I get a good value of A and B. But I think this system is too deterministic. For each x I want to predict a range of possible y's, with varying probabilities. So maybe it's better with a Bayesian approach e.g. A~Normal(a,b) and B~Normal(c,d)? But how do you set this up in tensorflow?",3,tensorflow,2020-10-13
iih9z6,"Google AI introduces TF-Coder, a program synthesis tool that helps you write TensorFlow code",,50,tensorflow,2020-10-13
iibrcd,StyleGAN2 generates Shrek,,29,tensorflow,2020-10-13
ii5gux,Linear learning rate warmup,"Hey, I have to implement a linear learning rate warmup which linearly increases the learning rate from 0 to 0.1 over k iterations for a neural network using TensorFLow 2 and Python 3.8. For example, k = 10000 training iterations/steps. Also, I am training a neural network model using tf.GradientTape. Any help for a tutorial or code demo is appreciated!

&amp;#x200B;

Thanks",1,tensorflow,2020-10-13
ii1ja3,TensorFlow for .NET Release Candidate,,10,tensorflow,2020-10-13
ihzlww,Create 3d photos from old photos as well!,,1,tensorflow,2020-10-13
ihu5gd,When to use generator TensorFlow in time series," I'm building a LSTM ANN and I'm not sure of using generator in my case. The net should have several hyperparameters it should iterate, e.g. epochs, activation functions..., however, if the generator is used in this case, each iteration is trained on different set, right? So more correct is using list or array instead? Thank you 

I mean, it shoul iterate like this:

 

    for epoch in EPOCHS do:
        for activation in ACTIVATION do:
            trainLSTM(epoch, activation)
            save best model",2,tensorflow,2020-10-13
ihpyvy,Has anyone attempted the tensorflow developer certificate exam ?,"If anyone has (or you know someone who has) the developer certificate , I've got a few questions :

1. How hard is the exam and what kinda questions are asked ? because the syllabus is from the specialization course on coursera which seems pretty basic 
2. Was 5 hours enough to solve 5 questions ?
3. How long were you preparing for it ? 
4. Has it impacted your career ?",12,tensorflow,2020-10-13
ihngx5,How should I get started with this technology?,I know python and I am decent at web dev and I wanted to start integrating this library into my own idea of a company. I have limited knowledge with TensorFlow so I wanted to reach out and see what resources were available out there for me to begin learning?,2,tensorflow,2020-10-13
ihlqp8,Autoencoder Input Output missmatch,"Hi! I have a question about an autoencoder model. How can i get the same output vector as the input vector? My input images are of shape (100, 510, 1). 

Here is my code.

    autoencoder = Sequential([
 # Encoder part
 Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(100, 510, 1)),
 MaxPooling2D((2, 2), padding='same'),
 Conv2D(8, (3, 3), activation='relu', padding='same'),
 MaxPooling2D((2, 2), padding='same'),
 Conv2D(8, (3, 3), activation='relu', padding='same'),
 MaxPooling2D((2, 2), padding='same'),
 # Latent representation shape (4, 4, 8)
    # Decoder part
 Conv2D(8, (3, 3), activation='relu', padding='same'),
 UpSampling2D((2, 2)),
 Conv2D(8, (3, 3), activation='relu', padding='same'),
 UpSampling2D((2, 2)),
 Conv2D(16, (3, 3), activation='relu', padding='same'),
 UpSampling2D((2, 2)),
 Conv2D(1, (3, 3), activation='sigmoid', padding='same')
], name='ConvAutoencoder')
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')

Summary:

Layer (type)                 Output Shape              Param #   

=================================================================

conv2d\_1 (Conv2D)            (None, 100, 510, 16)      160       

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

max\_pooling2d\_1 (MaxPooling2 (None, 50, 255, 16)       0         

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

conv2d\_2 (Conv2D)            (None, 50, 255, 8)        1160      

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

max\_pooling2d\_2 (MaxPooling2 (None, 25, 128, 8)        0         

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

conv2d\_3 (Conv2D)            (None, 25, 128, 8)        584       

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

max\_pooling2d\_3 (MaxPooling2 (None, 13, 64, 8)         0         

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

conv2d\_4 (Conv2D)            (None, 13, 64, 8)         584       

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

up\_sampling2d\_1 (UpSampling2 (None, 26, 128, 8)        0         

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

conv2d\_5 (Conv2D)            (None, 26, 128, 8)        584       

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

up\_sampling2d\_2 (UpSampling2 (None, 52, 256, 8)        0         

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

conv2d\_6 (Conv2D)            (None, 50, 254, 16)       1168      

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

up\_sampling2d\_3 (UpSampling2 (None, 100, 508, 16)      0         

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

conv2d\_7 (Conv2D)            (None, 100, 508, 1)       145       

=================================================================

I think the problem start at conv2d\_3 where we have to divide 25 by two.   
Is there a way to make this work with every image size?

Thanks in advance.",0,tensorflow,2020-10-13
ihjsyb,Learning Tensorflow 2.0,I am a beginner to tensor flow and wanted to learn how to make cool projects so I wanted to know what is the best way to learn Tensor flow. I already know how to program in python I just need to learn  Tensorflow and I would like to know what would be the best way to start my journey.,0,tensorflow,2020-10-13
ihj3mj,Structured Pruning in Tensorflow available?,Is there some way to do structure pruning in tensorflow. It seems there is just the method for unstrcutured pruning where the inference time actually stays the same?,1,tensorflow,2020-10-13
ihies9,I published a video where I explain the Discrete Fourier Transform in the context of audio data,"We live in a world engulfed with digital (audio) signals 🎧 🎧. To make sense of these signals, we can’t use the (Continuous) Fourier Transform. We should adapt this powerful tool to the digital domain. Hence, the Digital Fourier Transform (DFT). Discover the secrets of DFT in my new video!

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=ZUi\_jdOyxIQ&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=13](https://www.youtube.com/watch?v=ZUi_jdOyxIQ&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=13)",4,tensorflow,2020-10-13
ihdg7o,Loss:Nan when training and low acc,"Hello,

I recently did my first EDA and tensorflow model with a dataset. Not images, but just plain csv file. I used the coronavirus predictor dataset from kaggle if interested. Anyways I got the data in the way that I wanted it and fed it into the network. However when I hit train, my bar was showing loss:nan and my accuracy being the same each time: 0.25. I used three dense layers and split the dataset into test and train. I had 4 out put layers (none, low severity, medium severity, high severity) and I did loss as “sparse_categorical crossentropy) with an optimizer of “Adam”. Could anyone know what the issue is? Am I using the wrong loss function? I used relu as my activation function btw. Any help would be great. Thanks",6,tensorflow,2020-10-13
ih90vm,Generative Adversarial Network for approximating normal distribution,"I've been trying to make a GAN that can generate points that follow Brownian motion (normal distribution basically). However, I don't know why it fails to do so. I've seen GANs create realistic fake human faces so I figured this task would be trivial. Below is the code for the GAN. Any tips on how to improve the GAN or what i'm doing wrong?  Is there any conceptual reasons why what I am doing isn't working well? 

[https://github.com/bharddwaj/Summer2020/blob/master/approxBrownian.py](https://github.com/bharddwaj/Summer2020/blob/master/approxBrownian.py)",1,tensorflow,2020-10-13
ih7b0q,Odd behaviour after restarting training: loss increases,"Hello, I am training a model con google colab and after a while (1-2 hours) I get disconnected and I need to restart the runtime. I checkpoint my model every 5 epochs, and when I need to start training again I load the weights of the model using load\_weights and I restart the training from epoch N. What is strange is that usually when I restart the training the loss function is higher than it was before being stopped. For example, if I resume the training from epoch 30, I would expect that the loss for epoch 31 would be about the same, but it often tend to be higher.

I am training my model using Adam and a cyclical triangular learning rate, is it possible that when I restart the model the state of Adam or the learning rate scheduler is not properly loaded? If Adam had lost the momentum it would make sense that the loss function would suddenly increase and stabilize after a few epochs...

If the learning rate is dependent on the current step (which in turns depends on the epoch), much like an exponentially decaying LR, will the LR scheduler resume from the correct step after I start training my model again? 

Thank you for your help, I'm pretty confused...",5,tensorflow,2020-10-13
igxobo,"Learn more about the foundational concepts of TensorFlow, a software library used for numerical computation.",,0,tensorflow,2020-10-13
igxey2,TfLite,Is it possible to make a TFLite inference in C# or using C# TF lib?,2,tensorflow,2020-10-13
igx22q,Installing TF 1.13.1 on Ubuntu Server,"I'm trying to install the Solaris GIS Python package which has a requirement of TF 1.13.1, however, I cannot install this version. If I try

`pip3 install tensorflow==1.13.1`

I get the error

`ERROR: Could not find a version that satisfies the requirement tensorflow==1.13 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)`

I'm using Ubuntu Server 20.04.

Edit: How can I access and install the required version of Tensorflow?

Edit(2):  [https://github.com/CosmiQ/solaris#documentation](https://github.com/CosmiQ/solaris#documentation) ",2,tensorflow,2020-10-13
igktx5,"Public Transport ETA Prediction, what would be the best way?","Hi all, 

I am quite new to Tensorflow (well, beginner really), and working on a ETA Prediction. Currently the implementation I have is quite dumb (I have realtime data and routes, so I calculate an average), so I thought I would spice things up a little with Tensorflow maybe?

Anyone have any ideas as to where I would start with such data? I have Lat/Long, Time, Route (or point on route), so the idea is to normalize this data somehow such that I can then give it point A and get point B from real time data (GPS) and calculate ETA. Would something like this be possible with Tensorflow? What would be the best Algorithm to use (sorry if its not the right word, but is it RNN or something else?) and if anyone has anywhere to point me?

I am more fluent in javascript / nodejs so would I be able to implement something like this in nodejs?

Thanks in advance.",7,tensorflow,2020-10-13
ige5vn,Object Detection at the zoo,,3,tensorflow,2020-10-13
igcsun,Best Courses/ways to Learn Tensorflow,"I’m relatively new to Deep Learning, so I Started with Codecademy’s Machine Learning course you get the base concepts down, then I went through MIT’s Intro to Deep Learning class to go a little deeper on the mechanics. Now I want to really dive deep into learning Tensorflow and getting some practice building models. What are the best courses, materials, or places I should go to dive in?",5,tensorflow,2020-10-13
igbw6e,Creating my own keras Layer.,"I have an input of shape (16 million,) which needs to conserve it's shape through the network. My initial thought was to make a Dense layer, but that quickly ran out of memory (obvious in hindsight). However, I know that numbers far apart should have no impact on each other. So I want to have a layer somewhat like this matrix:

    |-                       -|
    |  a  b  0  0  0  0  ...  |
    |  c  d  e  0  0  0  ...  |
    |  0  f  g  h  0  0  ...  |
    |  0  0  i  j  k  0  ...  |
    |  0  0  0  l  m  n  ...  |
    |  :  :  :  :  :  :       |
    |-                       -|

Unfortunately, I can't afford to have all the 0's either, so this is merely what I want to replicate using only the non-zero, trainable values. I used the documentation to modify their 'SimpleDense', but I am unsure of what to replace the call method with:

    class PartialLayer(keras.layers.Layer):
        def __init__(self, units=32, window=3):
            if window % 2 != 1:
                raise ValueError(""Window should be odd, since it encompasses the current unit and the same amount of units either side."")
            super(PartialLayer, self).__init__()
            self.units = units
            self.window = window
        
        def build(self, input_shape):
            self.w = self.add_weight(shape=(window, self.units), initializer='random_normal', trainable=true)
            self.b = self.add_weight(shape=(self.units,), initializer='random_normal', trainable=true)
        
        def call(self, inputs):
            return

I cannot simply do a matmul like they do in the documentation due to the shape mismatch. I would do a loop through each of the inputs to calculate each ouput one by one, but that would be really slow. I am still very new to this, so please excuse me if I have missed something obvious. Thank you in advance for any help!",1,tensorflow,2020-10-13
igboh2,Where is TensorFlow's community?,"Hello, people say that TensorFlow has a bigger community rather than PyTorch. Can you tell me where is that community? I post questions here, most of the people never answer and ones who do(thank you so much), can't help me. I post questions on stack overflow and after 10 days somebody replies and most of the time they give incorrect answers (for the first time). so is there any place where I can get answers about TensorFlow quickly? (Pytorch has a website where that dude answers everything in less than a 6hours)",24,tensorflow,2020-10-13
igbcwi,Tensorflow not using the full extent of the GPU,"Hi!

Somewhat recently I got a new training server which is really fast, but I'm currently having trouble utilizing it's GPU and CPU to it's full potential when training my model.

I'm training an NLP classification model with a string as input and a category as target. When I set the batch size to a reasonally small number, like 16 or 32, only around 10% of each of the 2 GPUs as well as of the CPU are used. Only when I size the batches up to 4096, CPU gets close to a 100% load but GPUs still only hit 7-8%. Training is really fast then but extremely inefficient because such batch sizes are b\*\*\*s\*\*\*, so the model converges only very slowly.

I found a sweet spot around bs=256, where only around 20% CPU and 10% GPUs load is achieved and gradient descent is still somewhat efficient, which means I get the best results in terms of wall time.

The data pipeline is implemented with [tf.data](https://tf.data), reading the data from several CSVs in parallel from an SSD. I couldn't find any bottlenecks so far.

This is somewhat frustrating because I can only make use of a fraction of the full potential of my new machine. Any ideas on how to improve this?

I'm grateful for any help.

&amp;#x200B;

&amp;#x200B;

My specs:

\- AMD Ryzen Threadripper 3960X 24-Core Processor

\- 64 GB RAM

\- two NVIDIA GeForce RTX 2070 SUPER with 8192MiB each

\- Win 10 (unfortunately, the ASrock Creator TRX40 motherboard we bought is currently incompatible with Linux, wtf...)

\- TF 2.1.0 installed from binary (anaconda)

\- Python 3.7.7

\- CUDA Version 10.2.89

&amp;#x200B;

The relevant part of my code:

    class dataset_loader():
        
        def __init__(self, data_dir, csv_file, batch_size, cycle_length, tokenizer=None, n_threads=1, n_prefetch=1):
            
            self.batch_size = batch_size
            self.cycle_length = cycle_length
            self.n_threads = n_threads
            self.n_prefetch = n_prefetch
            
            self.output_size = 3943     ## !!!!!!!!!!! TODO nur für testzwecke bitte nicht hardcoden!!!!!!!!!!!
    
            if(tokenizer is None):
                self.tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)
                self.tokenizer.fit_on_texts(strat_search_words_with_beginnings)
            else:
                self.tokenizer = tokenizer
                
            char_dict = list(eval(self.tokenizer.get_config().get(""word_index"")).keys())[:-1]  # hier vorletztes weglassen, da ein Out-Of-Vocabulary slot bei StaticVocabularyTable zwingend angegeben werden muss
            char_index = list(eval(self.tokenizer.get_config().get(""word_index"")).values())[:-1]
            char_table_init = tf.lookup.KeyValueTensorInitializer(char_dict, char_index, value_dtype=tf.int64)
            self.char_table = tf.lookup.StaticVocabularyTable(char_table_init, 1)
            
            self.max_id = len(self.tokenizer.word_index)
            
            assert self.max_id == len(char_index)+1
    
            self._filepath_ = os.path.join(data_dir, csv_file)
    
        
        @tf.function
        def preprocessing(self, line):
            split_line = tf.strings.split(line, sep="";"")
            
            search_string = split_line[0]
            search_string = tf.strings.substr(search_string, 0, 50)      # zu lange strings abschneiden
            while tf.strings.length(search_string) &lt; 50:                 # zu kurze strings padden
                search_string = search_string + tf.constant("" "")
    
            chars = tf.strings.bytes_split(search_string)
            indices = self.char_table.lookup(chars)
            one_hot_string = tf.one_hot(indices, depth=self.max_id+1)
            
            icd_idx = tf.strings.to_number(split_line[1], out_type=tf.dtypes.int32)
            one_hot_icd = tf.one_hot(icd_idx, self.output_size)
            
            return one_hot_string, one_hot_icd
        
        def interleaving(self, filepath):
            return tf.data.TextLineDataset(filepath).skip(1)
        
        def get_dataset(self):
            dataset = tf.data.Dataset.list_files(self._filepath_)
            dataset = dataset.interleave(self.interleaving, cycle_length=self.cycle_length, num_parallel_calls=self.n_threads)
            dataset = dataset.map(self.preprocessing, num_parallel_calls=self.n_threads)
            return dataset.batch(self.batch_size).prefetch(self.n_prefetch)
        
        def get_tokenizer(self):
            return self.tokenizer
           
    
    
    import pickle
    f = open(os.path.join(data_dir, ""tokenizer.pkl""), ""rb"")
    tokenizer = pickle.load(f)
    
    train_data_loader = dataset_loader(data_dir,
                                 ""train_*.csv"",
                                 batch_size=batch_size,
                                 tokenizer=tokenizer,
                                 cycle_length=106,             #Anzahl der CSV-Dateien
                                 n_threads=tf.data.experimental.AUTOTUNE,
                                 n_prefetch=40)
    
    valid_data_loader = dataset_loader(data_dir,
                                ""valid_*.csv"",
                                batch_size=batch_size,
                                tokenizer=tokenizer,
                                cycle_length=13,
                                n_threads=tf.data.experimental.AUTOTUNE,
                                n_prefetch=40)
    
    
    mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
    
    with mirrored_strategy.scope():
        model = keras.models.Sequential([
        #    keras.layers.GRU(128, return_sequences=True, batch_input_shape=[batch_size, None, max_id+1]),
            keras.layers.GRU(128, return_sequences=True, input_shape=[ None, train_data_loader.max_id+1], use_bias=False),
            keras.layers.GRU(128, return_sequences=True, use_bias=False),
            keras.layers.GRU(128, use_bias=False),
            keras.layers.Flatten(),
            keras.layers.Dense(train_data_loader.output_size, activation=""softmax"")
        ])
        model.compile(loss=[focal_loss_umbertogriffo.categorical_focal_loss(alpha=.25, gamma=2)], optimizer=""adam"", metrics=['accuracy'])
    
    callbacks = list()
    callbacks.append(keras.callbacks.EarlyStopping(patience=2))
    callbacks.append(keras.callbacks.ModelCheckpoint(filepath = os.path.join(data_dir, ""checkpoints""), save_best_only=True))
    
    history = model.fit(train_data_loader.get_dataset(), validation_data=valid_data_loader.get_dataset(), epochs=25, callbacks = callbacks)",1,tensorflow,2020-10-13
ig6tiv,"Hey big guys, please help here","&amp;#x200B;

https://preview.redd.it/ece7ily4e3j51.png?width=3531&amp;format=png&amp;auto=webp&amp;s=a69802c8313cf7ae7a24c46259fb897a6d54cf7d

An app that can alert a homo sapiens he/she/others bend their neck too much, This is a sketch I submitted to the university but I don't know how to do this in TensorFlow.",1,tensorflow,2020-10-13
ig3593,TensorFlow Lite vs ML kit?,"So, I just came across a presentation of Google's ML Kit by Firebase. They claim that deployment of ML models is made really easy. I don't have much idea about ML Kit.
Is ML Kit a replacement to TensorFlow Lite? 
Or we'll have to deploy via TFLite anyways?

I am familiar with TFLite deployment on Apps, how could ML Kit help me? Should I even consider?

PS: I might be completely wrong on comparing these two. I'm just curious to know if this would ease deployment scene as compared to TFLite.",2,tensorflow,2020-10-13
ifrf31,Training multiple TensorFlow models,"I am quite new to Computer Vision. I am trying to make a decision in terms of architectural design for a real-time object detection project of mine. 

Currently, I have multiple custom TensorFlow models that detect a different object. I am now questioning if it makes sense to have a TensorFlow model per object or do is it better to re-train and introduce new classes every time I want to detect a new object. Both technically and practically, what would be the better approach? I essentially want to create an API, where I send an image to the backend wich runs the TF model(s) and returns the requested objects detected in the image.",1,tensorflow,2020-10-13
ifpv1u,We've launched a course on Deep Learning based Time-Series Forecasting with TensorFlow 2.0 and Python. It is an elaboration of the TensorFlow 2.0 documentation and we're constantly updating the course to cover more topics.,,33,tensorflow,2020-10-13
ifox13,Image Classification Made Easy in the Browser with TensorFlow.js,,1,tensorflow,2020-10-13
ifok7d,Difference in interleaving and shuffle,"Hello all,

I am new to Tensorflow and I am trying to understand all io,data processing part.

Anybody can explain, or direct me to understand difference between interleaving and shuffling of dataset ?

&amp;#x200B;

Thanks in advance, appreciate your response",1,tensorflow,2020-10-13
ifddc7,How to click inside the TensorFlox box?,"Hi all, I’m fairly new to programming and am learning Python to try to make a bot like this

https://medium.com/@youngdp/my-attempt-at-botting-in-runescape-63e9e61e47ed

However, I’m wondering once I’ve trained a neural net to detect things on screen, does anyone have any advice for how I could get Python to click inside the box?

Many thanks in advance :)

Baindraug",0,tensorflow,2020-10-13
ifcysv,Python Spark Certification Training using PySpark (CCA175),"Big Data Architects, Engineers and Developers - PySpark Certification Training will equip you to become a successful Spark Developer using Python and pass the Cloudera Hadoop and Spark Developer Certification Exam (CCA175). Gain in-depth knowledge of Apache Spark and the Spark Ecosystem including Spark RDD, Spark SQL, Spark MLlib and Spark Streaming and acquire comprehensive knowledge of Python Programming language, HDFS, Sqoop, Flume, Spark GraphX and Messaging System such as Kafka. Skill-based training modules cover: 1) Big Data Hadoop and Spark, 2) Python for Apache Spark, 3) Functions, OOPs, and Modules in Python, 4) Apache Spark Framework, 5) Spark RDDs, 6) DataFrames and Spark SQL, 7) Machine Learning using Spark MLlib, 8) Spark MLlib - Deep Dive, 9) Apache Kafka and Apache Flume, 10) Apache Spark Streaming - Processing Multiple Batches and Data Sources, 11) Implementing an End-to-End Project, and 12) Spark GraphX. 

Enroll today at: https://fxo.co/9YCB  

Much career success, Lawrence E. Wilson - Artificial Intelligence Academy (AIA)",1,tensorflow,2020-10-13
if90gu,New to tensorflow,"Hi! I’m a programmer taking his first steps in machine learning and I have a bunch of questions ! 
1) is there any workaround tensorflow 2.3 ? I’m trying to use the Keras models but 2.3 wont work and 2.0 is missing them ...

2) How can I get a number as an output ? All models I found are used to categorize, I am trying to output a number between -100 and 100... could a loss function be the solution? 

3) Is it possible to use .fit with one input array and one target at a time ? 


I’m trying to build a CNN with an array as an input and a number between -100 and 100 as output
Any code examples would help alot ! 
Thanks in advance ;)",2,tensorflow,2020-10-13
if202m,Is this graphics card suitable for training?,"Zotac GAMING GeForce GTX 1660 SUPER Twin Fan, ZT-T16620F-10L

[https://www.amazon.co.uk/dp/B07YVHCD8Q](https://www.amazon.co.uk/dp/B07YVHCD8Q)

It says Geoforce GTX 1060 but doesn't say Cuda anywhere. It's going into a B450m motherboard that has a 3200g chip.

I'm getting fed up with disconnects on Colabs and the 12 hour limit. I don't fancy Google's charges ($80 per month on their slowest machine), I'd much rather crunch my own numbers even if it's slower. I'll also have a large model coming up.

Any help would be grateful.",8,tensorflow,2020-10-13
if17h0,Do lower versions of Tensorflow work with higher versions of CUDA?,"I normally just go through here:

[https://www.tensorflow.org/install/source#tested\_build\_configurations](https://www.tensorflow.org/install/source#tested_build_configurations)

And check the necessary CUDA version I need for the respective tensorflow version.

But if i want to have TF 1 and TF 2 in different environments, will TF 1 work in GPU, considering I have a higher CUDA version? Or this is impossible?",1,tensorflow,2020-10-13
if0vrv,Question on keras layer,"I have a 9-dim input layer and the next kera layer is 

    tf.keras.layers.Dense(1)

Does that mean this layer only has one node or the output of this node is only one node? If the latter, how many nodes does the layer have?",1,tensorflow,2020-10-13
iex39j,Custom training loop results are different from fit().,"I found this issue when resolving a bug with my code. This is quite concerning. Anyone else facing the same issues?

&amp;#x200B;

[https://github.com/tensorflow/tensorflow/issues/37581](https://github.com/tensorflow/tensorflow/issues/37581)",1,tensorflow,2020-10-13
iew324,System built by USC researchers reconstructs a fully textured 3D human from each frame,,10,tensorflow,2020-10-13
iepmoe,Can you have tensorflow cut out certain parts of sentences or ignore parts of sentences along with predicting a users input?,"I am building an AI in python that acts kind of like google home or alexa, but i have run into a problem: I am going to train the model to detect the user command via feeding it forged command logs. Here is an example:

input: play drake gods plan

output: spotify\_play\_song(query=""drake gods plan"")

The problem here is having such data with the actual input data in the command logs worries me because i think including the input data like the song the user wants to play in the data may mess with the actual training data, So is there a way i can train it to ignore what it thinks is the input data so say for example that if it predicts the tag is spotify\_play\_song() the training stage may look like this:

input: play drake gods plan

predicted output: spotify\_play\_song(query=""play drake"")

output: spotify\_play\_song(query=""drake gods plan"")

or another option would be to just train it to predict the command and where the input is so the code can cut it out here's an example:

input: play drake gods plan

predicted output: spotify\_play\_song(query=""play drake"")

predicted cut: %play drake% gods plan

output: spotify\_play\_song(query=""drake gods plan"")

cut: play %drake gods plan%

Those were my two ideas im kinda new to tensorflow, I understand the concepts and how the epochs and training works but im new to creating and organizing data sets if there is a better way please let me know.

Thanks for reading :)",9,tensorflow,2020-10-13
iepjp4,How can I work with a data set that doesn't fit in memory?,"I have 16GB ram and 8GB swap. When I attempt to load in this data set, it is using most of the memory and is starting to use about half the swap. When I then use keras preprocessing to pad the sequences, it runs out of memory and the kernel crashes. I intend to use a sequential model, since I am new to this and keras makes my life easy. What I would like to do is have model.fit() run in batches where is loads some new data from disk and drops the old stuff in a random order. Looking at the documentation, I don't see a way to do that, bar doing the training one epoch at a time and manually changing the data provided. Is there a better way to do this?",5,tensorflow,2020-10-13
iepai7,Error creating dataset in Tensorflow 2,"I'm trying to create a dataset for cross validation using tensorflow 2 data api Dataset.from_generator. I create the generator and pass to from_generator function, but aparently tensorflow is looping past the end of my dataset. I tried to iterate through the generator and it works, i don't understand why tensorflow is running past the end of my dataset object. Here is the code:

    def cross_val_dataset():
        train = ShuffleDataWrapper('dataset.h5', 'train/data', label_encoder)
        targets = [t for _, _, _, t in train]   # t is the target variable

        skfold = sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=True)
        for train_indexes, val_indexes in skfold.split(np.zeros(len(targets)), targets):
            train_ds = tf.data.Dataset.from_generator(generator, args=[Subset(train, train_indexes)],
                                                                               output_shapes=((256,256,3), (72,), (24,), ()),
                                                                               output_types=OUTPUT_TYPES)

EDIT: Forgot the error message: ValueError: Can't convert non-rectangular Python sequence to Tensor.",1,tensorflow,2020-10-13
iecgq4,"ICYMI from Nvidia and UWaterloo researchers: Latest in synthesizing scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML",,6,tensorflow,2020-10-13
ieallh,IDE cannot find the reference Keras,"Hey guys, hope you're doing well. So I'm trying to use an Image generator and I inputted the following code to import it

from tensorflow.keras.preprocessing.images import ImageDataGenerator

and I encouter the following error

cannot find reference keras in \_\_init\_\_.py

I'm currently using PyCharm. I've tried using 

from tensorflow.python import keras

but it doesn't help - any ideas how to fix this? Thanks so much!",0,tensorflow,2020-10-13
ie763v,Feature Columns Implementation with Time Series,Is there an example I could read/follow in regards to using time series forecasting with feature columns or using time series forecasting just with categorical data? Most of the tutorials used numerical data so I was wondering if there were any resources that I could follow as I'm a little lost as to how to proceed.,1,tensorflow,2020-10-13
ie49a7,Tensorflow with CSVs?,"Hello, 

I am new to tensorflow as I just finished up learning and doing scikit learn projects. In scikit learn I did a lot with CSVs and excel sheets, but ultimately I did that just so I could get a solid foundation in machine learning before stepping up to tensorflow and deep learning. However before I get into using images, I wanted to start out doing a neural network in tensorflow with a CSV dataset from kaggle. My question is, can neural networks be fed with CSVs datasets with just numbers? I haven’t trained a tf model before but I wanted to use a basic dataset before moving onto higher level datasets. Any advice would be great, I know obviously people use tensorflow for deep learning but I thought I’d get my feet wet with a basic dataset.",0,tensorflow,2020-10-13
ie1p5d,Does model.save_weights(save_format='tf') save the Optimizer state?,"I am trying to implement a \`Keras\` based U-Net model in \`TF2.1\`. I have written a custom loss-function for this purpose. My aim was to save the model/its weights at the end of each \`epoch\`. For this, I am using [tf.keras.callbacks.ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint). 

&amp;#x200B;

`tf.keras.callbacks.ModelCheckpoint(`

`filepath, monitor='val_loss', verbose=0, save_best_only=False,`

`save_weights_only=False, mode='auto', save_freq='epoch', options=None, **kwargs`

`)`

&amp;#x200B;

In order to continue training at a later time, I need to save the `optimizer` state. The sure way to save the `optimizer` state is to pass `save_weights_only=False`, and use the `'tf'` or `'h5'` save\_format to save the entire model. I read [here](https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading), that Tensorflow `SavedModel` format is the recommended method to save models. But, for some reason, using that format takes a lot of time, whenever the model is being saved at the end of each (On the other hand, `save_format='h5'` saves the entire model pretty fast). Now, when I pass `save_weights_only=True` with `'tf'` format, it saves the weights as Checkpoints. I was wondering, if `save_weights_only=True, save_format='tf'` also saves the `optimizer` state? \[As far as I know, `save_weights_only=True, save_format='h5'` only saves the weights and not the optimizer. Also, after some trial and error, I could continue training from the saved Checkpoints, with a similar loss as with which the first round of training had completed. So, I think that the Checkpoints also contain the Optimizer state, but would love to hear other's opinions\]

&amp;#x200B;

I would be glad, if someone could clarify this, or point if I am wrong somewhere.",1,tensorflow,2020-10-13
idvfny,Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found,"As I import tf, I get this warning

I have completely followed tensorflow's ""using gpu"" guide, and I am sure that I have all the files and all directories added in path.

I use python 3.8.5 and last version of tensorflow available in pip.

pls help :\*(",6,tensorflow,2020-10-13
idslny,Difference MaxPool2D and nn.max_pool,"Hi,

I am trying to implement a CNN based on an older research paper and was planning to use a recent version of TF to do this.
All seems jolly so far, but I got stuck at a quite basic question around the max pooling layer. The original code uses tf.nn.max_pool and I am not sure how to add this to the layer model I am currently using. Are the two concepts (tf.keras.layers.MaxPool2D,tf.nn.max_pool ) interchangeable ?",3,tensorflow,2020-10-13
ids53k,"I’ve finished Andrew’s Deep Learning course 1&amp;2, and I want to use the code I learned in his assignment to solve this classification problem form UCI. But how do I even load my dataset?","UCI dataset

[https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work](https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work)

default ways to load dataset in assignment:

[https://jdwameieoblqbiugtcixrm.coursera-apps.org/edit/week7/tf\_utils.py](https://jdwameieoblqbiugtcixrm.coursera-apps.org/edit/week7/tf_utils.py) 

[https://jdwameieoblqbiugtcixrm.coursera-apps.org/tree/week7/datasets](https://jdwameieoblqbiugtcixrm.coursera-apps.org/tree/week7/datasets) 

code:

[https://jdwameieoblqbiugtcixrm.coursera-apps.org/notebooks/week7/TensorFlow\_Tutorial\_v3b-Copy1.ipynb](https://jdwameieoblqbiugtcixrm.coursera-apps.org/notebooks/week7/TensorFlow_Tutorial_v3b-Copy1.ipynb)",2,tensorflow,2020-10-13
idqzq6,Keras WGAN-GP example succeeds on slow local machine and fails on Google Cloud AI Platform. Not sure why.,"I've successfully run [this](https://keras.io/examples/generative/wgan_gp/) Keras WGAN example on my local machine, it took several days to fit but produced good results matching the example. I'm a beginner with not much time on my hands who wanted to speed things up and I was pointed to Google Cloud which I did not have much experience with. I quickly found my way to the AI Platform, created a new VM instance, launched jupyterlab and ran the exact same code. It was all going along very easily. This time it ran much faster but it completely failed. It consistently generates poor results. Here are the three images generated in Epoch 20.

&amp;#x200B;

https://preview.redd.it/0n3nimrylai51.png?width=28&amp;format=png&amp;auto=webp&amp;s=e320c744a46191134242edf19e66c2c421b06184

https://preview.redd.it/9edzxsrylai51.png?width=28&amp;format=png&amp;auto=webp&amp;s=9f8f51edc32f5108a50e036e1ef5ced087517241

https://preview.redd.it/dy8l1prylai51.png?width=28&amp;format=png&amp;auto=webp&amp;s=a61fb9b7136c9bc7f5ae291527013d49076714fa

I check the versions used in Google Cloud and they are:

* keras version 2.3.0-tf
* tensorflow version 2.2.0-dlenv
* python version  3.7.6

and on the local

* keras version 2.4.0
* tensorflow version 2.4.0-dev20200718
* python version  3.7.7

&amp;#x200B;

Here is an example of the fit output when it successfully runs on my local machine:

Epoch 1/20

118/118 \[==============================\] - 10078s 85s/step - d\_loss: -7.7434 - g\_loss: -17.1463

Epoch 2/20

118/118 \[==============================\] - 9820s 83s/step - d\_loss: -7.3016 - g\_loss: -6.4053

&amp;#x200B;

On Google Cloud

Epoch 1/20

118/118 \[==============================\] - 1266s 11s/step - d\_loss: -46.2237 - g\_loss: 36.3718

Epoch 2/20

118/118 \[==============================\] - 1260s 11s/step - d\_loss: -79.5681 - g\_loss: 105.9886

&amp;#x200B;

I noticed that the loss values are consistently highly different and recalled that in this example we override Model.train\_step so I think the problem is that the Google Cloud version is running an older version of TF/Keras.

Based on [this page](https://cloud.google.com/ai-platform/training/docs/runtime-version-list) the current (2.1) runtime version supports TensorFlow 2.1.0, but I'm seeing 2.2 on the default runtime.

How do I use TF/Keras vs 2.4 with Google Cloud AI platform?

Is the difference in version the reason why the example fails?

Thanks you in advance!",1,tensorflow,2020-10-13
idoz5f,Super-Human Performance in car racing using Deep Reinforcement Learning!,,3,tensorflow,2020-10-13
idhtit,Help me with fill this survey for my research,"Hi my fellow programmer,

I am programmer who work in the field of deep learning. Me and my colleagues usually make lectures and statistics about fields that we are interested in. That’s why we created a survey about the connection between deep learning and GPU utilization. If you have 3-4 minutes, please help us by answering our questions.

[https://forms.gle/2L6nUfxSN5KyVo786](https://forms.gle/2L6nUfxSN5KyVo786)

Thank you for your time and answers,

Richard",2,tensorflow,2020-10-13
idh1oz,Question: What computer could be used for TensorFlow certification exam,"Hello,

I am looking to take the TF certification, but a, a little confused with some of the requirements. I was reading that I will need NVIDIA gpu drivers ( [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu) ) and it mentioned something about needing a certain level of GPU. Would a computer like this be sufficient for taking the test and also learning AI?

Link to PC:  [https://www.bestbuy.com/site/ibuypower-gaming-desktop-intel-core-i7-9700f-16gb-memory-nvidia-geforce-gtx-1660-super-1tb-hdd-480gb-ssd-black/6400462.p?skuId=6400462&amp;ref=212&amp;loc=1&amp;ref=212&amp;loc=1&amp;gclid=Cj0KCQjwvvj5BRDkARIsAGD9vlJoLFBuKeOLx1YGdsTrEzLuelGx\_WCg9V8JnjfkNbS6bYqVr-TT01UaAmu6EALw\_wcB&amp;gclsrc=aw.ds](https://www.bestbuy.com/site/ibuypower-gaming-desktop-intel-core-i7-9700f-16gb-memory-nvidia-geforce-gtx-1660-super-1tb-hdd-480gb-ssd-black/6400462.p?skuId=6400462&amp;ref=212&amp;loc=1&amp;ref=212&amp;loc=1&amp;gclid=Cj0KCQjwvvj5BRDkARIsAGD9vlJoLFBuKeOLx1YGdsTrEzLuelGx_WCg9V8JnjfkNbS6bYqVr-TT01UaAmu6EALw_wcB&amp;gclsrc=aw.ds)",0,tensorflow,2020-10-13
idbq6o,tensorbook customer support sucks.,"I have a asus g75vx from 2013/14 and am now moving onto a new system jsut for ml/ai. I have narrowed down to tensorbook, system76, and asus zephyrus s. after researching had decided to go with tensorbook but the cust. support SUCKS. no one picks up their phones when i tried calling in, iv sent a few emails and have gotten 1 reply after 4 days, which only stated that they need payment upfront!? thats not even my question tensorbook support person. im dissapointed.",1,tensorflow,2020-10-13
id94gn,TensorFlow certification,Anyone interested to take the TensorFlow certification ?,6,tensorflow,2020-10-13
id7a0r,How can I set up a network that takes a single integer as input and gives a 2d array as output?,"I'm trying to make a network that takes one integer as an input and will output a 2d array.

So far I have this (below) but I get the error  `ValueError: Input 0 of layer conv1d is incompatible with the layer: expected ndim=3, found ndim=1. Full shape received: [None]` 

    i = Input(shape=x_train[0].shape)
    x = Conv1D(32, 3, strides=2, activation='relu')(i)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.2)(x)
    x = Dense(K, activation='softmax')(x)
    model = Model(i, x)

x\_train is simply a np.array = \[0,1,2\]. Initally I thought the issue might b fixed by changing the 3 in Conv1D to a 1 but this did not help. 

&amp;#x200B;

Does anyone know what my mistake is?",1,tensorflow,2020-10-13
id6ld3,From Facebook researchers: State of the art in 3D Hand and Body Motion Capture!,,2,tensorflow,2020-10-13
icmh64,"Object Tracking Using YOLOv4, DeepSORT and TensorFlow",,24,tensorflow,2020-10-13
iclxnv,Laptop requirements?,"Hi everyone!

I am willing to take 2 courses on tensorflow 2 by Imperial College London on Coursera. But my laptop doesn't have GPU and it is 4 gb RAM. Would I need a new laptop?

The courses are:
1. https://www.coursera.org/learn/getting-started-with-tensor-flow2
2. https://www.coursera.org/learn/customising-models-tensorflow2

Thank you.",5,tensorflow,2020-10-13
icgd8e,Capture 3D human motion from internet videos!,,1,tensorflow,2020-10-13
icdywx,Reinforcement learning with multiple actions - technical challenges,"I have some experience working with tensorflow dealing with CNN and LSTM. For my current project I am attempting to get a reinforcement learning ML application working. So far I have been successful at converting the previous environment code to a tf\_agents environment based on `tf_agents.environments.utils.validate_py_environment` not reporting any errors.

To build the AI I originally started researching deep Q networks. I have been working with several tutorials and I think I have a reasonable grasp on the code flow and what is happening. The issue I have run into is that `tf_agents.networks.q_network.QNetwork` will only work an action\_spec that has a single action. That is, this will work:

    action_spec = BoundedArraySpec(shape=(1,), dtype=np.float32, minimum=0, maximum=1, name='action')

while this will produce an error (note the change with shape):

    action_spec = BoundedArraySpec(shape=(2,), dtype=np.float32, minimum=0, maximum=1, name='action')

The error specifically is:

    ValueError: Network only supports action_specs with shape in [(), (1,)])

I believe I should be able to have multiple actions using an sac\_agent instead. I am working with this \[tutorial\]([https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/7\_SAC\_minitaur\_tutorial.ipynb#scrollTo=0pTbJ3PeyF-u](https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/7_SAC_minitaur_tutorial.ipynb#scrollTo=0pTbJ3PeyF-u)) but I am having some issue with their imports. My version of tf\_agents does not an ""experimental"" module as, for example, found in the line: \`\`\`from tf\_agents.experimental.train import actor\`\`\`

&amp;#x200B;

The current versions that I am working with are:

tf\_agents --&gt; 0.5.0

tensorflow --&gt; 2.2.0

&amp;#x200B;

I would greatly appreciate any help getting a DQN to allow multiple actions or with the ""experimental"" module import issue. Most of the tutorials I have found deal with the OpenAI Gym cartpole example and uses discrete action values, 0 or 1. I am attempting to work in a continuous space which I hope isn't an issue as I have specified in the action\_spec that dtype=np.float32. Even if you know of a decent tutorial that would be great. I have been researching this for a few days with little luck, I suspect because I don't know what terms to use.",5,tensorflow,2020-10-13
iccfg1,ISO: Tensor flow specialist to help build a trading algo.,"I have a strategy and it’s pretty profitable. I use traditional ML to enhance the strategy using libraries like scikit. I’m looking to take it to the next level, but tensor flow is a whole other beast. I’ve tried implementing a few things myself, but I’m quickly realizing I’m a bit over my head trying to accomplish this on my own at the moment. 

DM me if interested.",0,tensorflow,2020-10-13
ic10dq,How to export trained model from Python to C++?,"I have trained model on python I'd like to export it to C++, how do I convert it?  

Please help me...",2,tensorflow,2020-10-13
ic0jmr,How to substitute values in tensorflow array," 

I have an input\_array of dimension (70000, 1) type integer. E.g. first 10 values

    [[1] [9] [2] [10] [2] [5] [10] [3] [3] [10]]

I want to replace all the occurrences of 10 with 0 in the second axis. What is the best way to do this in tensorflow ? While doing

tf.map\_fn(fn=lambda t: tf.math.mod(t, 10), elems=input\_array) gives me

    NotFoundError: Could not find valid device for node. Node:{{node FloorMod}} All kernels registered for op FloorMod :   device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]   device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]   device='CPU'; T in [DT_DOUBLE]   device='CPU'; T in [DT_FLOAT]   device='CPU'; T in [DT_INT64]   device='CPU'; T in [DT_INT32] [Op:FloorMod]

Thanks!",2,tensorflow,2020-10-13
ic05yb,Now You Can Write One Code That Works On Both PyTorch And Tensorflow,[https://analyticsindiamag.com/eagerpy-pytorch-tensorflow-coding/](https://analyticsindiamag.com/eagerpy-pytorch-tensorflow-coding/),1,tensorflow,2020-10-13
ibzlzd,apply_gradients function is not updating weights of model in tensorflow," I have disabeled eager execution and hence using static graph. my model is not updating weights.it correctly computes gradient but optimizer is not updating weights of model. here is some code

`loss_value, grads = calculate_gradient(model, original_image, x, y)  optimizer.apply_gradients(zip(grads, model.trainable_variables)) optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)`

 it works in eager execution(i think because official tutorials are done that way), but now in static graph context",2,tensorflow,2020-10-13
ibwqvt,Working on a genetic algorithm library using TensorFlow and I want to hear your opinion,"Hi guys,  I am currently close to finishing a library I've been working on in my spare time that is supposed to provide a simple API for genetic algorithms that should be able to produce results like seen in [this](https://www.youtube.com/watch?v=qv6UVOQ0F44) video with **extreme ease.**

In the example below, you can see a basic use case of the library and the expected syntax:

    neat = Neat()
    neat.compile(inputs=4, hidden=1, outputs=4)
    history = neat.fit(func, population=100, generations=50)
    winner = neat.winner

The library is built with the user in mind and is extremely generic, you can customize almost every part of the evolution process. For example, defining your own crossover function (the function which defines how a child is generated) is as simple as passing `crossover=func` to the `.fit` function.

Another example is defining your own mutation function (the function which defines how a genome would mutate) by simply passing `mutation=func` to the `.fit` function.

The library also exposes a set of easy-to-use API that is supposed to make your process of customization extremely easy.

The reason I am creating this library is that I couldn't find any intuitive and easy-to-use libraries for genetic algorithms that are well documented and are working with TensorFlow, also The structure of the library is heavily inspired by the structure of the TensorFlow library.

My goal is to have this library community maintained, meaning that I hope that the community would decide this library is worth it and contribute things like more crossover functions, mutations, etc.. as it is built with this in mind.

I would really like to hear your opinion on this, what do you guys think? would you use it (or at least give it a try?)

Please provide honest feedback, worst case I would be the only one using this library and it's fine by me (although pretty sad) if it comes to that :)

Note: I am publishing this post in multiple subs in order to maximize the odds that someone would actually see it.",22,tensorflow,2020-10-13
ibujpy,Can Tensorflow run on an AMD GPU without Conda?,"Title, I'm really new to machine learning and I have a Radeon RX560 GPU. Using PyCharm, I ran the following code, just from a youtube tutorial.

    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Activation, Dense
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.metrics import categorical_crossentropy
    
    model = Sequential([
        Dense(units=16, input_shape=(1,), activation='relu'),
        Dense(units=32, activation='relu'),
        Dense(units=2, activation='softmax')
    ])

I know a good bit about Python, but I keep getting the following error.

    2020-08-18 00:44:32.860599: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
    2020-08-18 00:44:32.861357: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
    2020-08-18 00:45:04.933914: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
    2020-08-18 00:45:04.951858: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
    2020-08-18 00:45:04.963167: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-7Q6UUDU
    2020-08-18 00:45:04.964041: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-7Q6UUDU
    2020-08-18 00:45:05.360821: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2cce856d060 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
    2020-08-18 00:45:05.361396: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version

I have no idea what any of this means, but I looked it up and got the general idea that Tensorflow doesn't work on AMD GPUs. Is this true? What can I do to fix it? Any help is appreciated.",1,tensorflow,2020-10-13
ibtnhm,Automated capture of animal pose!,,3,tensorflow,2020-10-13
ibk9vw,What is the proper way to encode categorical labels?,"I have a dataset stored in a structured array inside a hdf5 file made with h5py. I'm currently encoding the label using this:

    vocab = ['class1', 'class2', 'class3""]
    indices = tf.constant([0,1,2], dtype=tf.int64))
    table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)
    table = tf.lookup.StaticVocabularyTable(table_init, 1)

I have a preprocess function that is called like dataset.map(preprocess) and does a table.lookup on the labels. It works, but i'm not happy with this because i can not inverse the transformation. What is an alternative, proper way to
do this? I don't want one hot encoding. I want something like LabelEncoder from sklearn but in tensorflow, in a way that i can inverse the transformation.",1,tensorflow,2020-10-13
ibf13a,"what is Tensor(""IteratorGetNext:1"", shape=(None, 3), dtype=float32)","hello, I have a simple code here. I was trying to make a custom loss but I got stuck. I have a function:

`def call(self, y_true, y_pred):`

`print(y_true)`

`print(y_pred)`

`return tf.math.reduce_mean(tf.square(y_true - y_pred))`

this function returns :

`Tensor(""IteratorGetNext:1"", shape=(None, 3), dtype=float32) Tensor(""functional_137/dense_137/Softmax:0"", shape=(None, 2), dtype=float32)`

followed by value error. i want to access y\_true tensor, it has 3 components and y\_pred tensor which has 2 component, so i need to make mse correctly. please help me to undestand what:

`Tensor(""IteratorGetNext:1"", shape=(None, 3), dtype=float32) Tensor(""functional_137/dense_137/Softmax:0"", shape=(None, 2), dtype=float32)`

these are and how to get infrmation from them.",1,tensorflow,2020-10-13
ibczyu,I published a video where I explain the complex (number) definition of the Fourier transform,"In my new tutorial, I explain how we can use complex numbers to define the Fourier transform in a compact and elegant way. I talk about the complex Fourier transform coefficients, and show how we can interpret the complex definition of the Fourier transform visually. I also introduce the inverse Fourier transform and provide a visual interpretation.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video!

[https://www.youtube.com/watch?v=KxRmbtJWUzI&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=12](https://www.youtube.com/watch?v=KxRmbtJWUzI&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=12)",14,tensorflow,2020-10-13
ib3t17,Applying color preservation to existing Tensorflow Code,"I found [this code](https://github.com/lengstrom/fast-style-transfer) online and I want to add color preservation functionality to it. This is the [paper](https://arxiv.org/abs/1606.05897), and from what I have gathered, I just need to apply the colors of the content image into onto the output image. I do not know how to write this code and would appreciate help. Thanks.",1,tensorflow,2020-10-13
ib3h1d,Applied Discord server for ML,"NNSG is a project based Discord server for building ML applications as a community.

https://discord.gg/pP8KTBR
~500 members

We're just getting started, and we've got 3 projects under our belt. Our neural networks have all been built from scratch so far and it's time to abstract our way out a little bit with the use of Tenserflow.

If you have a great project idea that can take advantage of Tenserflow please post it under #project-ideas. Then other members who are interested in helping out will tag you and hopefully you can get a project rolling.

See you there!",15,tensorflow,2020-10-13
iatdhz,Tensorboard with TPU model,"Hello, I am trying to train a model on Google colab using a TPU. I was wondering if it is possibile to somehow use tensorboard to keep track of the model. 
I have tried using the tensorboard callback for keras, adapting the example made for the case of gpu training, but it tells me that local filesystem is not supported, which means, if I'm not mistaken, that since I'm training the model with a tpu I cannot write the logs on the local disk. 
The problem is that I don't and can't create a gc bucket to upload the logs, in fact I'm using a public dataset which is already hosted on gcp.
Is there anything I can do?

Thank you very much!",5,tensorflow,2020-10-13
iartfa,Freezing TensorFlow2 layer,"I have a LeNet-300-100 dense neural network for MNIST dataset where I want to freeze the first two layers having 300 and 100 hidden neurons in the first two hidden layers. I just want to train the output layer. The code I have to do this is as follows:

        from tensorflow import keras
        inner_model = keras.Sequential(
            [
                keras.Input(shape=(1024,)),
                keras.layers.Dense(300, activation=""relu"", kernel_initializer = tf.initializers.GlorotNormal()),
                keras.layers.Dense(100, activation=""relu"", kernel_initializer = tf.initializers.GlorotNormal()),
            ]
        )
        model_mnist = keras.Sequential(
            [keras.Input(shape=(1024,)), inner_model, keras.layers.Dense(10, activation=""softmax""),]
        )
        
        # model_mnist.trainable = True  # Freeze the outer model
        # Freeze the inner model-
        inner_model.trainable = False
        
        
        # Sanity check-
        inner_model.trainable, model_mnist.trainable
        # (False, True)
        
        # Compile NN-
        model_mnist.compile(
            loss=tf.keras.losses.categorical_crossentropy,
            # optimizer='adam',
            optimizer=tf.keras.optimizers.Adam(lr = 0.0012),
            metrics=['accuracy'])

However, this code doesn't seem to be freezing the first two hidden layers and they are also learning. What am I doing wrong?

&amp;#x200B;

Thanks!",4,tensorflow,2020-10-13
iapf3b,Creating .tflite Object Detection on Android?,"Hi all, 

I've managed to train a couple of .tflite files thanks to a couple of different guides but the Android example uses a \[1,300, 300, 3\] shaped Tensor. I've created a model this shape but it's very poor at picking up the objects I want. 

The other model I made is a YoloV4 which is great for my detection but requires multiples of 32 for the Tensor shape. The Tensor shape is the image size and colour? Is this correct? So I created a \[1, 224, 224, 3\] keeping things small so it doesn't slow the detections down.

I'm now struggling creating the code on Android Studio. I actually thought this would be the easy bit as I understand Java far better than Python.  In the DetectorActivity I tried changing the Input Size of the image to 224, Quantised to false (or true) and changed other things like the txt.

I still cannot figure out how to implement it? Can anyone point me in the right direction?

Also I want to detect more than 10 instances of an Object. I think this is configured in my training but where?

Finally I don't want to flip the image during training (I'm training coins and you don't get a back to front coin) where can this be configured.

Thanks for any pointers.",1,tensorflow,2020-10-13
iamdmc,How to run tensorflow on quadro 6000.,Hi. Im dumb person dont know how to setup my GPU for tensorflow. I have quadro 6000 6gb cuda 2 which is older GPU. I did try to install tensorflow-gpu but gpu did not support. Can i use quadro 6000 for training. Plz share any resources or blog so i can learn and implement.,1,tensorflow,2020-10-13
iadbgl,"From Adobe, Stanford, &amp; UWashington: A new framework can predict a full head portrait for ages 0-70 from a single photo, modifying both texture and shape of the head",,13,tensorflow,2020-10-13
ia5aq6,Tensorflow writers/blogs,"Hi there,

I'm looking for some good blogs/writers who cover technical tensorflow topics and are worth following: I am interested in nitty gritty framework details not really machine learning.

Thanks",2,tensorflow,2020-10-13
ia24ri,How to get a tensorboard class to work with a simple DQN algorithm.,"For reinforcement learning I have read that tensorboard isn't ideal since it gives the input of per episode and/or step. Since in RL there are thousands of steps, it doesn't give us an overview of the content. I saw this modified tensorboard class that solves this problem: https://pythonprogramming.net/deep-q-learning-dqn-reinforcement-learning-python-tutorial/ 

the class:  

    class ModifiedTensorBoard(TensorBoard):
        # Overriding init to set initial step and writer (we want one log file for all .fit() calls)
        def __init__(self, name, **kwargs):
            super().__init__(**kwargs)
            self.step = 1
            self.writer = tf.summary.create_file_writer(self.log_dir)
            self._log_write_dir = os.path.join(self.log_dir, name)

        # Overriding this method to stop creating default log writer
        def set_model(self, model):
            pass

        # Overrided, saves logs with our step number
        # (otherwise every .fit() will start writing from 0th step)
        def on_epoch_end(self, epoch, logs=None):
            self.update_stats(**logs)

        # Overrided
        # We train for one batch only, no need to save anything at epoch end
        def on_batch_end(self, batch, logs=None):
            pass

        # Overrided, so won't close writer
        def on_train_end(self, _):
            pass

        def on_train_batch_end(self, batch, logs=None):
            pass

        # Custom method for saving own metrics
        # Creates writer, writes custom metrics and closes writer
        def update_stats(self, **stats):
            self._write_logs(stats, self.step)

        def _write_logs(self, logs, index):
            with self.writer.as_default():
                for name, value in logs.items():
                    tf.summary.scalar(name, value, step=index)
                    self.step += 1
                    self.writer.flush()


and I would like to make it work with this layer:

    n_actions = env.action_space.n
    input_dim = env.observation_space.n
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(20, input_dim = input_dim , activation = 'relu'))#32
    model.add(tf.keras.layers.Dense(10, activation = 'relu'))#10
    model.add(tf.keras.layers.Dense(n_actions, activation = 'linear'))
    model.compile(optimizer=tf.keras.optimizers.Adam(), loss = 'mse')

But I have yet to get it to work. Anyone who has worked with tensorboard before, do you know how to setup this up? Any insight is greatly appreciated.",1,tensorflow,2020-10-13
ia0xj8,how do you save epoch along with other model data in checkpoint?,"Here's my saving code:

    
checkpoint_dir = SAVED_MODEL
checkpoint_prefix = os.path.join(checkpoint_dir, ""tf-ckpt"")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

And loading:

    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

I'm wondering how I can add the epoch to what gets saved and loaded just to make continuing the learning process (on Colab) a bit smoother. I tried just adding epoch=epoch but it seems like the tf.train.checkpoint method expects specific params.

Lmk!",2,tensorflow,2020-10-13
i9tkdp,naive question: is tensorflow good for arbitrary graph-based computations ?,Im wondering about non-neural network implementations of Tensorflow. Is it good for making general types of compute graphs?,4,tensorflow,2020-10-13
i9so1r,Deep Compression (Han et. al.),"Hello, I was reading [Deep Compression (Han et al.)](https://arxiv.org/abs/1510.00149) research paper and came across Table-1 (mentioned on page 6) which is as follows:

&amp;#x200B;

[Table-1](https://preview.redd.it/0o9s07a7t0h51.png?width=702&amp;format=png&amp;auto=webp&amp;s=b4fedf1944d21bb608d60c38642d05b1d3cc09fb)

My question is: If I am using TensorFlow-2.0 and Python3 (in Linux OS), how do I find out the size of ""Parameters"" which is the 3 column of this table? Because I need to implement this pipeline to see the compression working. And as of now, I don't know how to see the size of the parameters for a given model.

Help?",2,tensorflow,2020-10-13
i9et6i,"Latest from Facebook researchers: Codec Avatars! A class of learned, photorealistic face models that accurately represent the geometry and texture of a person in 3D (i.e., for virtual reality), and are almost indistinguishable from video",,12,tensorflow,2020-10-13
i99p7g,Easy way to view the loss function overtime?,"I would like to see how the loss function from 

    model.compile(optimizer=tf.keras.optimizers.Adam(), loss = 'mse')

evolves throughout the game/learning process. Is there an easy way to do this?",4,tensorflow,2020-10-13
i971p5,Best resource ...,"... to get deeply into RNN &amp; LSTM. 

I mean like what they actually do. 

Thanks",0,tensorflow,2020-10-13
i96r90,Tensorflow project files/folder structure and OOP style programming,"Hey everyone I am new to tensorflow and machine and deep learning.I created my first tf project which is a simple CNN that classifies brain MRI's from people with alzheimer into alzheimer stages(83% accuracy).My question is how can I structure the files in the directory ""correctly""?Also how can I use OOP ( if possible) to brake down the script into smaller pieces because right now everything is in one script.(Data Loading structure,model architecture , training function,predicting function etc)

&amp;#x200B;

Thanks in advance",1,tensorflow,2020-10-13
i942ot,Where is the python interpreter executable located when installing tensorflow through anaconda under linux?,"I was following a tensorflow install tutorial for windows on how to use tensorflow with the pycharm ide because I couldn’t find any instructions on how to do it on linux. I got tensor to work in a virtual enviroment, but now I want to use it with pycharm. In the tutorial you needed to go to project settings and set an enterpreter by navigating to the anaconda install folder, picking your enviroment from the envs folder and then selecting the pythonw.exe from the tools folder. The problem is that anaconda on linux doesn’t have a tools folder with the enviroment executable, so I’m trying to figure out where it is.",5,tensorflow,2020-10-13
i90opa,I published a video where I explain complex numbers easily,"In my new video, I explain complex numbers in a simple way relying on visual interpretation. I talk about the Cartesian and polar representations of a complex number. I discuss the Euler formula, and the wonderful Euler identity. 

The video aims to provide an operational understanding of complex numbers necessary to define the Fourier transform in complex terms.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video!

[https://www.youtube.com/watch?v=DgF4m0AWCgA&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=11](https://www.youtube.com/watch?v=DgF4m0AWCgA&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=11)",1,tensorflow,2020-10-13
i8zaxr,New features coming to google colab (possibly)," Hello, today I was asked to fill a survey on new features that I may find interesting, and the list included background execution, more powerful GPUs (v100), 4x more RAM, persistent disk storage (much like google drive) and longer runtime (up to 48h).

If any of these are implemented it would be great! Especially background execution, which imo would be a killer feature!

What do you think? Did any of you receive this survey?",2,tensorflow,2020-10-13
i8v9nb,"From MIT, Google, UCSD researchers: Neural rendering--&gt;relight the scene photorealistically!",,11,tensorflow,2020-10-13
i8rkma,Multilevel Embedding Lookup,"Is there a good way to do a multilevel embedding lookup? What I mean is that if I have an embedding which is a `VOCAB_SIZE x EMBED_SIZE` tensor, embedding, and another lookup vector `NODE_COUNT x 1`, node_labels, and I want to do something equivalent to this `tf.gather(tf.gather(embedding, node_labels), node_index)`. If I try to do that directly I get this warning `UserWarning: Converting sparse IndexedSlices to a dense Tensor with 1512357312 elements. This may consume a large amount of memory.` and the application runs out of memory. I've also tried `tf.gather(embedding, tf.gather(node_labels, node_index))` which gives a similar warning and causes runaway ram usage on the CPU that eventually causes a crash.",3,tensorflow,2020-10-13
i8j5wa,Neural network model size,"Hey guys, I am using Python3 and Tensorflow 2. I would like to know how much GPU my model is using currently. How do I do that ?

I am using GNU/Linux OS.

Thanks !",1,tensorflow,2020-10-13
i8f29p,Is there a way to count people from a 720p timelapse video with tensorflow?,"Hello, I have only a little bit of knowledge of tensorflow. I am trying to find out if there is an ""easy way"" to count the number of participants from that source:

[https://www.youtube.com/watch?v=MhFYz4lpvIU](https://www.youtube.com/watch?v=MhFYz4lpvIU)

would it possible to use this source and get the real number of participants? If someone who has the knowledge, time and interest to let tensorflow count humans from this video I would really appreciate it. Thanks.  Or maybe someone has a library that can do this? Any tips welcome. I also add this here: 

[https://www.reddit.com/r/theydidthemath/comments/i8epat/request\_how\_many\_participants\_are\_attending\_this/](https://www.reddit.com/r/theydidthemath/comments/i8epat/request_how_many_participants_are_attending_this/)",10,tensorflow,2020-10-13
i8b1s9,Tensorflow 2 too slow for Deep RL,"I'm relatively new to Deep RL and I'm currently trying to create a Neural Network that plays TicTacToe. I've created an implementation in Numpy with my own functions that performs forward prop and backprop to train the network. Its performance is almost optimal but not at 100%, so I am trying to create the network on tensorflow to make training more efficient (by implementing optimizers like Adam or other techniques automatically done by TF).

I'm using a simple tf.keras.models.Sequential and I just call the function predict() whenever the agent has to perform an action (it basically evaluates afterstates) and the function train\_on\_batch() when an episode has finished, where I feed the network with the afterstates encountered and their respective Monte Carlo estimates.

It's exactly the same procedure I used with my custom numpy functions, but Tensorflow is about 100-1000 times slower, taking about a second to run a single episode. I thought train\_on\_batch() was faster than fit(), but both are equally slow. I have tried to only run train\_on\_batch every 100 episodes instead of every episode, but there are no significant improvements. That means that predict() is taking most of the time, and I can't get my head around why that is, since it simply has to multiply some matrices.

This is very frustrating since I want to obtain the benefits of TF, but it's just not worth it if I can perform 1000 times more iterations with a manual implementation. Thanks for the help!

&amp;#x200B;

Edit:

The problem has reduced into how to efficiently make a large number of predictions (say 100000) in a loop using tf.keras.model.Sequential(). It really comes down to performing forward prop on the neural net that many iterations. But I can't get tf to not run on eager mode, and that's what causes the slow execution. How can I do that?

[https://github.com/cpita/TicTacToe](https://github.com/cpita/TicTacToe)",0,tensorflow,2020-10-13
i87cun,Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image!,,8,tensorflow,2020-10-13
i7xg00,Tensorflow error when building a simple object classifier,"I am using tensorflow 2.3 to build a simple object classifier and I reached the step where I will start training the model but I am getting this error: 

“Faster_rcnn_inception_v2 is not supporter. See ‘model_builder.py’ for features extractors compatible with different versions of tensorflow “ 

Can someone please tell me how can I solve this?",1,tensorflow,2020-10-13
i7v2vz,tf.Variable vs np.array difference,I just saw that there is a new experimental tensorflow api with numpy. I was wondering if there was any benefit to using an numpy array and conducting operations with that as opposed to using a tf variable,7,tensorflow,2020-10-13
i7p6oz,Tensorflow Questions,"Hello,

I am new to this subreddit + Tensorflow and preparing for tensorflow certifications.

Wondering, why anyone haven't shared any questions that appeared during exam ?",0,tensorflow,2020-10-13
i7m16q,Latitude/Longitude Coordinates,"Is there a way to have tensorflow predict location/coordinates and if so, how/where can I learn more about this?",1,tensorflow,2020-10-13
i7k1w4,Denoise old music recordings with neural networks,,1,tensorflow,2020-10-13
i7h80p,tf.experimental.numpy: NumPy API on TensorFlow,,25,tensorflow,2020-10-13
i76dhv,I published a video where I explain the Fourier Transform easily,"In my new video, I explain how the Fourier Transform works. I avoid getting into the mathematical intricacies (for now!). Instead, I focus on the intuition using a visual approach. The Fourier Transform is a fundamental tool used in audio signal processing for extracting information from audio data, and transform a signal from the time to the frequency domain.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=XQ45IgG6rJ4&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=10](https://www.youtube.com/watch?v=XQ45IgG6rJ4&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=10)",9,tensorflow,2020-10-13
i71ixs,Adam optimizer - ValueError: tf.function-decorated function tried to create variables on non-first call,"I am using tensorflow 2.3

&amp;#x200B;

The code below 

&amp;#x200B;

`import  tensorflow as tf`



`y_N= tf.Variable([1., 2., 3.],name=""dd"")`



`@tf.function`

`def loss():`

`return -tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=tf.math.log(y_N), axis=0))`



`@tf.function`

`def run():`

`tf.keras.optimizers.Adam(0.5).minimize(loss, var_list=[y_N])`



`run()`

&amp;#x200B;

gives exception

&amp;#x200B;

`ValueError: tf.function-decorated function tried to create variables on non-first call.`

&amp;#x200B;

Problem looks like \`tf.keras.optimizers.Adam(0.5).minimize(loss, var\_list=\[y\_N\])\` creates new variable on &gt; first call, while using \`@tf.function\`. If must wrap it under \`@tf.function\`, how should I modify it? (in real case \`run()\` is a much bigger function)",1,tensorflow,2020-10-13
i70hpt,DeepR — Training TensorFlow Models for Production,,16,tensorflow,2020-10-13
i6ozcu,Train/Test Split on Custom Data,"Hi all,

I am trying to use TF to create a sequential model for UFC fight outcomes using [this](https://www.kaggle.com/mdabbert/ultimate-ufc-dataset) dataset in csv format.

I am trying to predict the winner/outcome, and have coded the values to 0's or 1's in place of ""Red"" or ""Blue"". I am hoping to use a train (80%) and test (20%) split.

My problem is that most examples I find online use the IRIS, Boston, MNIST, etc. datasets. I am curious what would be the best way to code the command for the train/test split for non-tf.dataset data.

I was told to try to use the ""from sklearn.model\_selection import train\_test\_split"" method but am confused how to take the data split from sklearn and use it within TF, if this is even possible.

If anyone can point me in the right direction of how to properly split data for training and testing models within TF, your help would be greatly appreciated!",1,tensorflow,2020-10-13
i6nd9x,How to reshape an image array properly with JavaScript/NodeJS for TensorFlowJS?,"Hi,

I have trained a CNN model which I would to run for inference.

In Python, I can use the following commands:

    	img = load_img(filename, target_size=(224, 224))
    	# convert to array
    	img = img_to_array(img)
    	# reshape into a single sample with 3 channels
    	img = img.reshape(1, 224, 224, 3)

In JavaScript with TensorFlowJS, I  currently use the following commands:

    const imageClassification = async path =&gt; {
      const image = readImage(path);
      const model = await tf.loadLayersModel('file://jsmodel/model.json');
      const predictions = await model.predict(image);
      console.log('Classification Results:', predictions);
    }

In Python, it works fine, but in JavaScript I get the following error:

    Error: Error when checking : expected conv2d_input to have 4 dimension(s), but got array with shape [244,244,3]

To combat this error in the JavaScript implementation, I think I need to use a reshape command, similarly to Python. However, I do not know the exact command. What JavaScript command can I use here?

Thank you very much for your help!",5,tensorflow,2020-10-13
i6m8pp,ValueError: No gradients provided for any variable: ['dd:0'].,"For the code

&amp;#x200B;

import  tensorflow as tf



y\_N= tf.Variable(\[1., 2., 3.\],name=""dd"")

cost = -tf.reduce\_mean(input\_tensor=tf.reduce\_sum(input\_tensor=tf.math.log(y\_N), axis=0))



loss=lambda:cost



train\_step = tf.keras.optimizers.Adam(0.5).minimize(loss, var\_list=\[y\_N\])

&amp;#x200B;

&amp;#x200B;

I got error

&amp;#x200B;

ValueError: No gradients provided for any variable: \['dd:0'\].

&amp;#x200B;

How to resolve it?",0,tensorflow,2020-10-13
i6c7gg,"Make easy edits to high-quality, diverse, and photorealistic images on real images and those generated by GANs!",,5,tensorflow,2020-10-13
i6b750,Combining Tensors from Tensorflow Dataset API,"Hi, I'm trying to learn the Tensorflow Dataset API (Tensorflow docs don't do a good job explaining it)

I'm working with the 'Titanic' dataset from their API and I don't know how to make the features tensors model friendly. here's the best I got, but it's for one tensor at a time how do I make it so it can handle all tensors in the features item?

    
    import tensorflow as tf
    import tensorflow_datasets as tfds
    from tensorflow.keras.optimizers import Adam
    
    data = tfds.load(""titanic"",split='train', as_supervised=True).map(lambda x,y: (x,y)).prefetch(1)
    
    for i in data.batch(1309):
        xx1 = i[0]['age']
        xx2 = i[0]['fare']
        yyy = tf.convert_to_tensor(tf.one_hot(i[1],2))
    
    model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(1),
    tf.keras.layers.Dense(13, activation='relu'),
    tf.keras.layers.Dense(2,activation='softmax')
    ])
    
    model.compile(optimizer=Adam(learning_rate=0.01),loss='categorical_crossentropy', metrics=['accuracy'])model.fit(xx1,yyy,epochs=30)
    
    

How do I concat the age and fare (and any of the other ones) tensors so that they're in one dataset?

I tried concat, and stack to no avail.",2,tensorflow,2020-10-13
i640fe,How to architect a web API that runs a TensorFlow model inference?,"Hi,

I am building a website which runs an inference on pre-trained machine learning models. I split the web development into frontend and backend, the machine learning inference will be on the backend / server side.

My question is now, which architecture would you suggest:

1. A Node.js backend server using TensorFlow.js. The question is, will the inference (a CNN) be much slower than with Python, or should I first analyze it before I continue this path, or do I not need to worry about it? Since for both, TensorFlow.js and the Python implementation, the core is actually written in C++, might there be only little difference in execution time?
2. A Node.js backend server and the TensorFlow runs in another language, for example Python (or something else?). Does the inference then run faster? Then I would need to brige Node.js with Python. What would you suggest for this? [This website](https://thecodinginterface.com/blog/bridging-nodejs-and-python-with-pynode/) suggests PyNode
3. Run the entire backend in Python. This is certainly possible, but then I would need to get familiar with Python async programming

What are your experiences?

Thank you very much!",10,tensorflow,2020-10-13
i5t55o,"Manipulate novel images in realistic ways, such as changing lighting effects and scene geometry!",,5,tensorflow,2020-10-13
i5lil9,What’s the equivalent of a multiclass version of IndependentBernouli layer in Tensorflow Probability?,"I’ve noticed there is a Multinomial distribution object but there is no layer for it as there is for Bernoulli 

https://www.tensorflow.org/probability/api_docs/python/tfp/layers/IndependentBernoulli",1,tensorflow,2020-10-13
i5jcd8,Debugging Lack of Parallelism,"I've been working on an application and after our most recent overhaul it seems the model lost almost all parallelism in training. CPU usage is very spiky across a few cores totaling less than 5% total utilization and the GPU shows 0% utilization. The model is an autoencoder built on top of a recursive neural network that's statically built into the model. The code is available here  [https://github.com/ruler501/CubeCobraRecommender/tree/parsed-cards](https://github.com/ruler501/CubeCobraRecommender/tree/parsed-cards) in src/ml/model.py.

How can I debug what is causing it to not be able to fully utilize the hardware? I've tried batch sizes from 8 to 1024 with no noticeable difference in utilization.",1,tensorflow,2020-10-13
i5grtd,Why am I getting “Attribute Error: module ‘tenser flow’ has no attribute flags”?,,0,tensorflow,2020-10-13
i5dz83,OmniNet: If Ben’s Omnitrix had a better Machine Learning/Artificial Intelligence system inbuilt?,,0,tensorflow,2020-10-13
i5dyt1,OmniNet: If Ben’s Omnitrix had a better Machine Learning/Artificial Intelligence system inbuilt?,,0,tensorflow,2020-10-13
i5d23v,How I can count simple objects from a still image file (.png or .jpg)? What libraries to use?,"I am wondering how I can count simple objects from a still image file (.png or .jpg)?   


Think of a Node app on the back-end with a React front-end with an input form.   


I want to let the user upload an image, and based on that image I will calculate the number of objects, based on the classes I have taught to the model.  


Now the question is, how would I implement this the best way preferably using JavaScript?  


For example, does anyone know if I could use [https://teachablemachine.withgoogle.com/](https://teachablemachine.withgoogle.com/)?",1,tensorflow,2020-10-13
i5cm7x,Date Extractor &amp; Transformer,"We built this free model using Tensorflow to help people make sense of dates in raw text. There are two models which perform the key steps of date handling: date extraction from surrounding text, and normalizing dates into a standard format. Given an input date and time, the date transformer model returns a normalized date in YYYY‐MM‐DD 00:00:00 format. It can also transform relative dates, for example ""3 years and 2 months ago"". The model supports dates in 13 European languages and can interpret any date between 1975 and 2050.

Please check it out and let us know what you think [https://blog.socialgist.com/socialgist-releases-date-interpretation-model-on-aws-sagemaker](https://blog.socialgist.com/socialgist-releases-date-interpretation-model-on-aws-sagemaker).",9,tensorflow,2020-10-13
i592vd,VGG16/19 Fine-tuning,"Hi! I have a question about fine tuning the VGG16 or VGG19 model. In this blog by keras they state that you need a trained classifier to do fine tuning.  [https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)

""note that it is necessary to start with a fully-trained classifier, including the top classifier, in order to successfully do fine-tuning ""

But in other resources I found they don't pre-train the top layers. Like here  [https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/](https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/)

or here

[https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)

So my question is ""Is it necessary to pre-train the top model for fine tuning?"".

Edit: Another question. What does the preprocess\_input function do from VGG16? I have tested it and my results show that when i preprocess the input for my fine tuned model I get an accuracy of 0.10 and without preprocessing i get around 0. 97.",9,tensorflow,2020-10-13
i58f1g,Reimplementing CenterNet and TTFNet in TensorFlow 2.2+,"Hi,

with my team at [Ximilar](https://www.ximilar.com), we are releasing our unofficial implementation of CenterNet and TTFNet in TensorFlow 2.2+.

You can find it on GitHub: [https://github.com/Ximilar-com/xcenternet](https://github.com/Ximilar-com/xcenternet)

We were missing this anchor-free model in TensorFlow 2 so we decided to reimplement it, there are several features:

* Easy to use and easy to extend with your own models, feature extraction heads, code, ...
* Train on your own dataset with coco format
* Utilizing train\_step and tf.data.Datasets
* TensorBoard results

This project is still in beta as some of the things like Deformable Convolutions are not yet merged in tf-addons. We have more plans with this and we hope to add more features in the future as we are using it in our own platform for a Custom Object Detection.

Maybe this small open-source library will help you with your own projects ...

https://preview.redd.it/u6qc056vpif51.png?width=1247&amp;format=png&amp;auto=webp&amp;s=c36df614b868fb71eb2d56d1f96303d280189a9a",2,tensorflow,2020-10-13
i55qmp,Tensorflow 2.0 train sequential model on a single GPU machine,"I want to train a sequential tensorflow (version 2.3.0) model on a single NVIDIA graphic card (RTX 2080 super). I am using the following code snippet to build and train the model. However, everytime I am running this code I do not see any GPU utilization. Any suggestion how to modify my code so I can run it on 1 GPU?

    strategy = tf.distribute.OneDeviceStrategy(device=""/GPU:0"") 
    
    with strategy.scope():     
    
        num_classes=len(pd.unique(cats.No))     
    
        model = BuildModel((image_height, image_width, 3), num_classes)     
    
        model.summary()     
        
        model=train_model(model,valid_generator,train_generator,EPOCHS,BATCH_SIZE)",1,tensorflow,2020-10-13
i53rhx,Please Help-- Beginner TensorFlow class/object error,"Hey Guys,

when i try to run this line of code in my machine learning program, I keep getting this error. This may be a python error or a TensorFlow error. If anyone could shed some light on this, it would be really appreciated. Thanks!! 

Matt

&amp;#x200B;

Line:

action\_q\_vals = [self.sess.run](https://self.sess.run)(self.q,feed\_dict={self.x: current\_state})

Error:

AttributeError: 'QLearningDecisionPolicy' object has no attribute 'sess'",1,tensorflow,2020-10-13
i50gld,Latest TensorFlow 2.3 wheels with CUDA 11 and Python3.8,"I built some new wheels for [Tensorflow 2.3](https://blog.tensorflow.org/2020/07/whats-new-in-tensorflow-2-3.html) with CUDA 11 and cuDNN 8 in case anyone finds them useful

[https://github.com/davidenunes/tensorflow-wheels](https://github.com/davidenunes/tensorflow-wheels)

btw, shameless plug of my [Github Sponsors profile](https://github.com/sponsors/davidenunes)  in case anyone finding these useful wants to contribute to my coffee  addiction 🙂☕ or support these builds and related projects.",1,tensorflow,2020-10-13
i4v1h2,In-depth tutorials on TensorFlow,"hello I am really a beginner in TensorFlow, and I have a hard time to understand how computational graph works, so I made many mistakes while writing code, so can you suggest online tutorials or any resources that you think would be enough to really understand TensorFlow and also would help me to write code fluently(like people write python code)",1,tensorflow,2020-10-13
i4r7lt,Counting Objects Using YOLOv4 Object Detection and TensorFlow,,29,tensorflow,2020-10-13
i4r4js,My 3 day tensorflow journey,I'm 16 and I really like machine learning and python so I started learning tensor flow using some course. I'm doing convulutional neural networks today and it is amazing when you fix some issue your code is having,4,tensorflow,2020-10-13
i4o3cz,Why are tensorflow .h5 model files of different size depending on which callback function stores it?,"Hi, 

if I want to train a tensorflow machine learning model and store the model after each training epoch on the hard drive, I can either use the following code (Python):

    checkpoint = ModelCheckpoint('model{epoch:08d}.h5', save_freq=1)
    history = model.fit(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=numberOfTrainingEpochs, verbose=0, callbacks=checkpoint)

Or, however, I can use a custom, potentially more complex logic which decides when to save the model:

    class CustomSaver(Callback):
    def on_epoch_end(self, epoch, logs={}):
        self.model.save_weights(""model_{}.h5"".format(epoch))
    
    saver = CustomSaver()
    history = model.fit(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=numberOfTrainingEpochs, verbose=0, callbacks=saver)

Both files create .h5 files with the ML model, however, the first one creates file sizes of ca. 100 MB, whereas the second one creates file sizes of ca. 50 MB. What is the difference between those files and what is the cause for it?

Fyi, my model is a relatively simple CNN and defined as follows:

    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224, 224, 3)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(1, activation='sigmoid'))
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

Thank you very much!",3,tensorflow,2020-10-13
i4mx5v,Classify Fashion Mnist with VGG16 Question,"Hi! I am trying different approaches to image classification. One of them is transfer learning. I found this tutorial from two years ago and followed it almost completely, with the exception on how I import the dataset. The rest (shape of the input data, model, optimizer, usw) is the same.However I get completely different results. The early stopper stops the training process after about 16 epochs and the accuracy is only 0.1009 and the val\_accuracy is only 0.0964.

My question is “Am I doing something wrong or why am I getting such bad results?”

I have read about Transfer learning and fine-tuning, but I don't realy understand how i would apply it to this problem.

I have included the training accuracy graph, training loss graph, some of the training output and the code that I added.

Thanks for any help!

Link:  [https://www.kaggle.com/anandad/classify-fashion-mnist-with-vgg16/notebook](https://www.kaggle.com/anandad/classify-fashion-mnist-with-vgg16/notebook)

https://preview.redd.it/w34ts4z4wbf51.png?width=960&amp;format=png&amp;auto=webp&amp;s=34e0bc5d2bd9f647fb6112da3653c425b5fdf864

https://preview.redd.it/zrjmd9x5wbf51.png?width=960&amp;format=png&amp;auto=webp&amp;s=446db6e426718b2cd7f51ccef2e18304e8623b33

https://preview.redd.it/30vro3f1ybf51.png?width=1025&amp;format=png&amp;auto=webp&amp;s=b5aacd168370d64806a0be88828647e0a0befe83

    from keras.preprocessing.image import img_to_array, array_to_img
    def change_shape(x):
        # Old shape (len, 28, 28)
        x = np.reshape(x, (len(x), 28, 28, 1))
        # New shape (len, 28, 28, 1)
        x_resized = x.copy()
        x_resized.resize((len(x), 28, 28, 3), refcheck=False)
        x = x_resized
        # New shape (len, 28, 28, 3)
        x = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48, 48))) for im in x])
        # New shape (len, 48, 48, 3)
        x = x / 255.
        x = x.astype('float32')
        return x
    
    (train_X, train_Y), (test_X, test_Y) = fashion_mnist.load_data()
    
    classes = np.unique(train_Y)
    num_classes = len(classes)
    
    train_X = change_shape(train_X)
    test_X = change_shape(test_X)
    
    train_Y_one_hot = to_categorical(train_Y)
    test_Y_one_hot = to_categorical(test_Y)
    

...The Rest is the same as in the tutorial

Edit: I found a solution. Its in the commends here are the results since i cant post images in commends.

https://preview.redd.it/6mhyihq96cf51.png?width=633&amp;format=png&amp;auto=webp&amp;s=12ab2976d50f06fe953e5d2e3b121610e39253fe

https://preview.redd.it/zfk2vcla6cf51.png?width=636&amp;format=png&amp;auto=webp&amp;s=f97d39700e8facd5c34584077466018b48a85e86",1,tensorflow,2020-10-13
i4iz35,How do I split my data into train/test/dev and set the label for each category using Tensorflow?,"Hi I have processed data in this format. There are 10 Folds each with 6 folders. In these folders there are labels of 0, 5 and 10 with their respective images. Does Tensorflow have any built in functionality to do this for me? 

\`\`\`

frames

├── Fold1\_part1

│   ├── 01

│   │   ├── 0

│   │   │   ├── 00001.jpg

│   │   │   ├── 00006.jpg

│   │   │   ├── 00011.jpg

│   │   │   ├── 00016.jpg

│   │   │   ├── 00021.jpg

\`\`\`",5,tensorflow,2020-10-13
i44552,"John Snow Labs Spark-NLP 2.5.5: 28 new Lemma and POS models in 14 languages, bug fixes, and lots of new notebooks!",,2,tensorflow,2020-10-13
i42955,"How does YOLO object detection work and How can you use it to make your own custom object detection Model? Also, see its simple implementation in OpenCV. You can also find a Keras Implementation of YOLO linked in the article.",,28,tensorflow,2020-10-13
i418o4,Loading Zipped models in TensorflowJS,"I have a pruned and quantized keras model that I have converted to a tfjs format. Currently this model is ~25MB.

Because it's pruned, the zipped weights are only about ~6MB

I intend to serve the weights in this zipped format. However tfjs io currently expects URL of the unzipped file( the .JSON file) . How do I make do by hosting the zipped file ?

I am not experienced with node or JS so I appreciate the help. Thanks in advance",2,tensorflow,2020-10-13
i40vu8,Using NumPy in Custom Layer,"I'm making a custom Keras layer which has no weights -- am I allowed to use numpy operations within the layer if there will be no weights / biases variables? If I use TensorFlow ops, how should I represent NumPy arrays (tf.Tensors, tf.Variables, or tf.constants)?",1,tensorflow,2020-10-13
i3zq1y,"I keep getting this value error: ValueError: Attempt to convert a value (&lt;TensorSliceDataset shapes: ((17,), ()), types: (tf.float64, tf.float64)&gt;) with an unsupported type (&lt;class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'&gt;) to a Tensor.","I'm trying to get into TensorFlow and I'm attempting to create my first model. Ive been stuck at this stage for a while now. 

    import tensorflow as tf
    import math
    import logging
    logger = tf.get_logger()
    logger.setLevel(logging.ERROR)
    import matplotlib as plt
    import pandas as pd
    import numpy as np
    
    # Yes - 1, No - 1, Data missing or NA - 97 98 99
    # Outpatient - 1, Inpatient - 2
    df_Original = pd.DataFrame(pd.read_csv('covid.csv'))
    df_Original.replace(97, np.nan, inplace = True)
    df_Original.replace(98, np.nan, inplace = True)
    df_Original.replace(99, np.nan, inplace = True)
    df_Original.pregnancy.replace(np.nan, 2, inplace = True)
    
    # since we are trying to predict itubation, we can drop some columns that are too strongly correlated to intubation
    
    df = df_Original.drop(['id', 'date_symptoms', 'date_died', 'pneumonia', 'icu', 'entry_date'], axis = 1,).dropna().reset_index(drop = True)
    df = df.apply(lambda x : x - 1)
    df.age = df.age.apply(lambda x : (x + 1) / 150)
    print(df.shape)
    
    
    
    Dataset = tf.data.Dataset.from_tensor_slices((df.values, df.pop('intubed').values))
    
    
    for element, labels in Dataset.take(5):
        print(element, labels)
    

Everything is smooth sailing until i try and use Dataset.take, and it gives me the error

    ValueError: Attempt to convert a value (&lt;TensorSliceDataset shapes: ((17,), ()), types: (tf.float64, tf.float64)&gt;) with an unsupported type (&lt;class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'&gt;) to a Tensor.

Im trying to learn the [tf.data](https://tf.data) pipeline and any help would be appreciated.

Im a little lost as for what the use of tf.split is for, and am in desperate need for example models to use that dont just use tf.load to import the dataset.

&amp;#x200B;

&amp;#x200B;

Thanks!",5,tensorflow,2020-10-13
i3rxd8,"Previously working code now errors: ""TypeError: An op outside of the function building code is being passed a ""Graph"" tensor""","Hi all, 

I've been trying to implement the code I saw in the book ""Deep Learning with Python"". Up until two days ago, the code ran fine. I tried to run the code today and experienced an error:  ""TypeError: An op outside of the function building code is being passed a ""Graph"" tensor""- despite not having changed any of the code. 

I'm wondering if the Tensorflow API has changed?

Any help in fixing this bug would be really appreciated, I've made the code available in a Google Collaborative Notebook which you can find here: [https://colab.research.google.com/drive/1ArmP3Nns2T\_i-Yt-yDXoudp6Lhnw-ghu?usp=sharing](https://colab.research.google.com/drive/1ArmP3Nns2T_i-Yt-yDXoudp6Lhnw-ghu?usp=sharing)

&amp;#x200B;

Thanks in advance.",1,tensorflow,2020-10-13
i3lz1m,"Can't ""tensor activate"" on Mac","I followed [Tensorflow Instructions](https://www.tensorflow.org/install/source#macos_1) yet I still cannot activate tensor. When I enter ""tensor"" into my terminal, I receive ""tensor: command not found"" which must mean that I didn't actually install it. Can someone please give me a rundown of what I should've done?",4,tensorflow,2020-10-13
i3j03j,How can we quantify the unquantifiable with AI?,"In my new video 🚀🚀, I share my strategy to create effective AI models which tackle non-objective problems (e.g., music genre classification, mood classification). I provide tips and best practices on how to define the problem, how to understand what’s realistic to achieve, and how to optimise the algorithms using an effective metric.

Here's the video:

[https://www.youtube.com/watch?v=\_IFH2TYesms&amp;list=PL-wATfeyAMNpd8nGIxJKLpTV5zpOWyLZC&amp;index=9](https://www.youtube.com/watch?v=_IFH2TYesms&amp;list=PL-wATfeyAMNpd8nGIxJKLpTV5zpOWyLZC&amp;index=9)",3,tensorflow,2020-10-13
i3ffue,TF Imaging and Labels format for Image Classification,"Hi,

I have a 6 class image classification problem. There's 6 folders of examples of each. I want to use them in Python for Mobilenet TFLite. What sort of format should they be in? Or structure the labels as? Can the labels be as straightforward as a CSV list of integers? Thanks for any help.",1,tensorflow,2020-10-13
i3e0j0,Converting TF 2 Object Detection Model to TensorRT,"Hey there,

 I am trying to convert the EfficientDet D1 640x640 of the new Tensorflow 2 Object Detection API to a TensorRT (TRT) Model to run on a Jetson AGX Board. 

This is the Code I am trying: 

    import tensorflow as tf
    import numpy as np
    from tensorflow.python.compiler.tensorrt import trt_convert as trt
    
    input_saved_model_dir = './efficientdet_d1_coco17_tpu-32/saved_model/'
    output_saved_model_dir = './models/tensorRT/'
    num_runs = 2
    
    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS
    conversion_params = conversion_params._replace(max_workspace_size_bytes=(1&lt;&lt;32))
    conversion_params = conversion_params._replace(precision_mode=""FP16"")
    
    converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir,conversion_params=conversion_params)
    converter.convert()
    
    def my_input_fn():
        for _ in range(num_runs):
            inp1 = np.random.normal(size=(1, 640, 640, 3)).astype(np.uint8)
            yield inp1
            
    converter.build(input_fn=my_input_fn)
    converter.save(output_saved_model_dir)

But for some strange reason I get the following Error: 

 (1) Invalid argument:  Input shapes do not match input partial shapes stored in graph, for TRTEngineOp\_5: \[\[640,640,3\]\] != \[\[1,?,?,3\]\] 

It says that the expected input shape is \[1,?,?,3\], but that it gets \[640,640,3\]. In contrast to that I am passing a numpy array with the shape of (1,640,640,3), which should be correct. 

Why is that strange Error happening and what can I do to solve it ? 

Thanks for your help!",9,tensorflow,2020-10-13
i3dg4f,anyone please help me now what to do ?,"python model\_main\_tf2.py 

2020-08-04 10:35:22.159780: I tensorflow/core/platform/cpu\_feature\_guard.cc:143\] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2

2020-08-04 10:35:22.276101: I tensorflow/core/platform/profile\_utils/cpu\_utils.cc:102\] CPU Frequency: 2526910000 Hz

2020-08-04 10:35:22.276610: I tensorflow/compiler/xla/service/service.cc:168\] XLA service 0x560a278f1330 initialized for platform Host (this does not guarantee that XLA will be used). Devices:

2020-08-04 10:35:22.276646: I tensorflow/compiler/xla/service/service.cc:176\]   StreamExecutor device (0): Host, Default Version

2020-08-04 10:35:22.291751: I tensorflow/core/common\_runtime/process\_util.cc:147\] Creating new thread pool with default inter op setting: 2. Tune using inter\_op\_parallelism\_threads for best performance.

WARNING:tensorflow:There are non-GPU devices in \`tf.distribute.Strategy\`, not using nccl allreduce.

W0804 10:35:22.299919 140692759455552 cross\_device\_ops.py:1175\] There are non-GPU devices in \`tf.distribute.Strategy\`, not using nccl allreduce.

INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)

I0804 10:35:22.300328 140692759455552 mirrored\_strategy.py:500\] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)

Traceback (most recent call last):

  File ""model\_main\_tf2.py"", line 113, in &lt;module&gt;

[tf.compat.v1.app.run](https://tf.compat.v1.app.run)()

  File ""/home/anandchauhan/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run

\_run(main=main, argv=argv, flags\_parser=\_parse\_flags\_tolerate\_undef)

  File ""/home/anandchauhan/miniconda3/envs/tf/lib/python3.8/site-packages/absl/app.py"", line 299, in run

\_run\_main(main, args)

  File ""/home/anandchauhan/miniconda3/envs/tf/lib/python3.8/site-packages/absl/app.py"", line 250, in \_run\_main

sys.exit(main(argv))

  File ""model\_main\_tf2.py"", line 104, in main

model\_lib\_v2.train\_loop(

  File ""/home/anandchauhan/miniconda3/envs/tf/lib/python3.8/site-packages/object\_detection/model\_lib\_v2.py"", line 461, in train\_loop

configs = get\_configs\_from\_pipeline\_file(

  File ""/home/anandchauhan/miniconda3/envs/tf/lib/python3.8/site-packages/object\_detection/utils/config\_util.py"", line 138, in get\_configs\_from\_pipeline\_file

proto\_str = [f.read](https://f.read)()

  File ""/home/anandchauhan/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/lib/io/file\_io.py"", line 116, in read

self.\_preread\_check()

  File ""/home/anandchauhan/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/lib/io/file\_io.py"", line 78, in \_preread\_check

self.\_read\_buf = \_pywrap\_file\_io.BufferedInputStream(

TypeError: \_\_init\_\_(): incompatible constructor arguments. The following argument types are supported:

1. tensorflow.python.\_pywrap\_file\_io.BufferedInputStream(arg0: str, arg1: int)

&amp;#x200B;

Invoked with: None, 524288

&amp;#x200B;

https://preview.redd.it/95vtaain7xe51.png?width=1366&amp;format=png&amp;auto=webp&amp;s=b94f1037a81dd1e09416974da143101028be9afe",3,tensorflow,2020-10-13
i3c1lr,"Infer spatial arrangements and shapes of humans and objects from a 2-D image: Latest from Facebook, Berkeley, Carnegie Mellon, and Argo researchers:",,1,tensorflow,2020-10-13
i3aafi,Importing Data directly from computer,"Hi, I’m having trouble with loading my data that’s a csv file directly on the computer. All the other online guides I see have a link that they use for the data. Any help is appreciated, thanks.",1,tensorflow,2020-10-13
i357w9,What exactly is MultiWorkerMirroredStrategy used for?,I looked at the Tensorflow and Keras documentation on it and it seems to be mostly geared towards multiple machines but I was curious if would be a good choice for 1 machine with multiple GPUs and CPU cores. Generally I just want to use the most efficient strategy for my 4x2080 GPU machine with a 20-core CPU and I am curious if I should stick with the MirroredStrategy or use another strategy?,1,tensorflow,2020-10-13
i33arn,Tensorflow model error,"I was following the tensorflow pandas tutorial when I got this [error](https://imgur.com/gallery/dCW4T7K). Any help is appreciated, thanks!",1,tensorflow,2020-10-13
i32fdg,Un-selfie your pictures: From Adobe and Berkeley researchers!,,1,tensorflow,2020-10-13
i2ytc9,Finally received my certificate - TensorFlow Developer Certificate,,27,tensorflow,2020-10-13
i2w7db,List of top online courses to Tensorflow for Beginners,"Found an amazing list of all the top-rated [Tensorflow courses](https://blog.coursesity.com/best-tensorflow-tutorials?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=redditPost&amp;utm_term=best-tensorflow) of all time. 

Do let me know if you find this useful so can update accordingly as per feedback and suggestions.",2,tensorflow,2020-10-13
i2icui,Our open-source TensorFlow JS project got featured on Product Hunt!,,14,tensorflow,2020-10-13
i27xij,Using learning rate schedule and learning rate warmup with TensorFlow2,"I have to use learning rate warmup where you start training a VGG-19 CNN for CIFAR-10 with warmup from a learning rate of 0.00001 to 0.1 over the first 10000 iterations (or, approximately 13 epochs) using learning rate warmup. And then for the remainder of training, you use the learning rate of 0.01 where a learning rate decay is used to reduce the learning rate by a factor of 10 at 80 and 120 epochs. The model has to be trained for a total of 144 epochs.

I am using Python 3 and TensorFlow2 where training dataset has 50000 examples and batch size = 64. The number of training iterations in one epoch = 50000/64 = 781 iterations (approx.).

How can I use both of the learning rate warmup and learning rate decay together in the code?

Currently, I am using a learning rate decay by:

    boundaries = [100000, 110000]
    values = [1.0, 0.5, 0.1]
    
    learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)
    print(""\nCurrent step value: {0}, LR: {1:.6f}\n"".format(optimizer.iterations.numpy(), optimizer.learning_rate(optimizer.iterations)))

However, I don't know how to use a learning rate warmup along with learning rate decay.

Help?",1,tensorflow,2020-10-13
i27fld,Does AWS Elastic Beanstalk support tensorflow with python 3.8 ?,"I have never used aws before and I have a django project I want to deploy. I am using Tensorflow (version  2.3) and python 3.8.2. I am also using tensorflow hub and not using any  databases. I was wondering if AWS elastic beanstalk is compatible with  tensorflow 2.3 and python 3.8?

Any insight would be greatly appreciated",1,tensorflow,2020-10-13
i26w92,"Let's say I have a background pic and then a pic with a person in it, how do I get the pixels that belong to the person? My attempt did not turned good enough. Thanks.",,12,tensorflow,2020-10-13
i215tw,"How to understand ""feature extraction"" weights in simple Tensorflow/Keras neural network?","**I'm trying to make the most basic of basic neural networks to get familiar with feature extraction in Tensorflow 2.x and, in particular, keras.**

Basically what I'm trying to do is the following with my simplified iris dataset (i.e. setosa or not)

Use the 4 features as input
Dense layer of 3
Sigmoid activation function
Dense layer of 2 (one for each class)
Softmax activation
Binary cross entropy / log-loss as my loss function

Here's how I build the neural network (Note, the performance is terrible but I'm just using this a toy example):


    # Load data
    from sklearn.datasets import load_iris
    import pandas as pd
    
    iris = load_iris()
    X, y = load_iris(return_X_y=True, as_frame=True)
    X = X.astype(""float32"")
    X.index = X.index.map(lambda i: ""iris_{}"".format(i))
    X.columns = X.columns.map(lambda j: j.split("" ("")[0].replace("" "",""_""))
    y.index = X.index
    y = y.map(lambda i:iris.target_names[i])
    y_simplified = y.map(lambda i: {True:1, False:0}[i == ""setosa""])
    Y = pd.get_dummies(y_simplified, columns=[""setosa"", ""not_setosa""])
    
    # Traing test split
    from sklearn.model_selection import train_test_split
    seed=0
    X_train,X_test, Y_train,Y_test= train_test_split(X,Y, test_size=0.3, random_state=seed)
    
    # Simple neural network
    import tensorflow as tf
    tf.random.set_seed(seed)
    
    
    # Input[4 features] -&gt; Dense layer of 3 neurons -&gt; Activation function -&gt; Dense layer of 2 (one per class) -&gt; Softmax
    inputs = tf.keras.Input(shape=(4))
    x = tf.keras.layers.Dense(3)(inputs)
    x = tf.keras.layers.Activation(tf.nn.sigmoid)(x)
    x = tf.keras.layers.Dense(2)(x)
    outputs = tf.keras.layers.Activation(tf.nn.softmax)(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=""simple_binary_iris"")
    model.compile(loss=""binary_crossentropy"", metrics=[ ""accuracy""] )
    model.summary()
    
    history = model.fit(X_train, Y_train, batch_size=64, epochs=10, validation_split=0.2)
    
Here's the summary: 

    Model: ""simple_binary_iris""
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_2 (InputLayer)         [(None, 4)]               0         
    _________________________________________________________________
    dense_4 (Dense)              (None, 3)                 15        
    _________________________________________________________________
    activation_2 (Activation)    (None, 3)                 0         
    _________________________________________________________________
    dense_5 (Dense)              (None, 2)                 8         
    _________________________________________________________________
    activation_3 (Activation)    (None, 2)                 0         
    =================================================================
    Total params: 23
    Trainable params: 23
    Non-trainable params: 0

Here's the feature extraction code from [keras FAQ](https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction):

    # https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction
    extractor = tf.keras.Model(
        inputs=model.inputs, 
        outputs=[layer.output for layer in model.layers],
    )
    features = extractor(X_train.values)
    list(map(lambda weights: weights.shape, features))
    # [TensorShape([105, 4]),
    #  TensorShape([105, 3]),
    #  TensorShape([105, 3]),
    #  TensorShape([105, 2]),
    #  TensorShape([105, 2])]

**What I understand is the following:**

1. `features[0]` are the input values of `X_train`
2. `features[1]` are the weights of the first dense layer
3. `features[2]` are the weights of the dense layer after the sigmoid activation function
4. `features[3]` are the weights of the second dense layer
5. `features[4]` are the weights of the second dense layer after the softmax activation function

**What I don't understand is the following:**

1. How do I know the value of each feature's contribution in the input layer (4 features) to the first dense layer weight? 

2. Shouldn't there be a matrix somewhere that is 4 x 3 = 12 weights showing the contribution of each of the 4 features to each of the 3 neurons in the first dense layer? 

Without this type of understanding, this algorithm is pretty much a black box.",1,tensorflow,2020-10-13
i1zxk4,i did't understand what i did wrong anyone please help me : ),,1,tensorflow,2020-10-13
i1vt19,Latest from Microsoft mixed reality &amp; AI lab researchers--- great applications for mixed reality: State of the art in 3D Model Fitting!,,9,tensorflow,2020-10-13
i1fpxh,"[0,1,2,3] from int64","Hi!

&amp;#x200B;

It seems like I've completely missed the train on TF and it's become this huge project that I can't get any oversight over and now I have this:

&amp;#x200B;

    model = tf.keras.Sequential([
        tf.keras.layers.Dense(1, activation='relu'),
    ])
    
    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),
                  metrics=['accuracy'])
    model.fit(training_dataset, epochs=15)

It started with a low loss and 1h 8 min to train (7.4 million rows) and now at 4.75 million and 28 min left, it shows me ""loss: nan"".

&amp;#x200B;

My input is as such:

&amp;#x200B;

x is int64  
y is either 0, 1, 2 or 3

&amp;#x200B;

Can anybody please guide me to the right path please? All of the tutorials I find online are for very complex stuff with multiple features whereas my input is just a single int64...

&amp;#x200B;

Thank you in advance! :)",1,tensorflow,2020-10-13
i1etni,VGG model loading weight ValueError TensorFlow2,"I have defined a VGG based CNN for CIFAR-10 using Python3 and TensorFlow2 as follows:

&amp;#x200B;

        def vgg19_nn():
            """"""
            Function to define the architecture of a convolutional neural network
            model based on VGG-19 architecture for CIFAR-10 dataset.
                
            Output: Returns designed and compiled convolutional neural network model
            """"""
        
            
            # l = tf.keras.layers
            
            model = Sequential()
            
            model.add(
            Conv2D(
                    filters = 64, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same', kernel_regularizer = regularizers.l2(weight_decay),
                    input_shape=(32, 32, 3)
                )
            )
        
            # model.add(BatchNormalization())
            model.add(Dropout(0.3))
            
            model.add(
            Conv2D(
                    filters = 64, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(
            MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
            Conv2D(
                    filters = 128, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 128, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(
            MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
            Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(
            MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(
            MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            
            model.add(
                # AveragePooling2D(
                MaxPooling2D(
                    pool_size=(2, 2), strides=(2, 2)
                )
            )
            
            model.add(Dropout(0.5))
            
            model.add(Flatten())
            
          
            model.add(
            Dense(
                    units = 512, activation='relu',
                    kernel_initializer = tf.initializers.he_normal(),
                    kernel_regularizer=regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.5))
            
            '''
            model.add(
            Dense(
                    units = 256, activation='relu',
                    kernel_initializer = tf.initializers.he_normal()
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(Dropout(0.4))
            '''
            
            model.add(
            Dense(
                    units = num_classes, activation='softmax'
                )
            )
            
            
            # Compile pruned CNN-
            model.compile(
                loss=tf.keras.losses.categorical_crossentropy,
                # optimizer='adam',
                optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),
                metrics=['accuracy']
            )
            
            return model

I am trying to do some pruning and therefore store the weights as a Python3 list-

&amp;#x200B;

        # Python3 list to hold layer-wise weights as
        # numpy arrays-
        vgg_19_np_wts = []
        
        for layer in winning_ticket_model.trainable_weights:
            vgg_19_np_wts.append(layer.numpy())
    
    
        len(vgg_19_np_wts)
        # 68
        
        # Initialize a model-
        model = vgg_nn()
        
        len(model.trainable_weights)
        # 68
        
        model.summary()
        '''
        Model: ""sequential_6""
        _________________________________________________________________
        Layer (type)                 Output Shape              Param #   
        =================================================================
        conv2d_96 (Conv2D)           (None, 32, 32, 64)        1792      
        _________________________________________________________________
        dropout_78 (Dropout)         (None, 32, 32, 64)        0         
        _________________________________________________________________
        conv2d_97 (Conv2D)           (None, 32, 32, 64)        36928     
        _________________________________________________________________
        batch_normalization_96 (Batc (None, 32, 32, 64)        256       
        _________________________________________________________________
        max_pooling2d_30 (MaxPooling (None, 16, 16, 64)        0         
        _________________________________________________________________
        conv2d_98 (Conv2D)           (None, 16, 16, 128)       73856     
        _________________________________________________________________
        batch_normalization_97 (Batc (None, 16, 16, 128)       512       
        _________________________________________________________________
        dropout_79 (Dropout)         (None, 16, 16, 128)       0         
        _________________________________________________________________
        conv2d_99 (Conv2D)           (None, 16, 16, 128)       147584    
        _________________________________________________________________
        batch_normalization_98 (Batc (None, 16, 16, 128)       512       
        _________________________________________________________________
        max_pooling2d_31 (MaxPooling (None, 8, 8, 128)         0         
        _________________________________________________________________
        conv2d_100 (Conv2D)          (None, 8, 8, 256)         295168    
        _________________________________________________________________
        batch_normalization_99 (Batc (None, 8, 8, 256)         1024      
        _________________________________________________________________
        dropout_80 (Dropout)         (None, 8, 8, 256)         0         
        _________________________________________________________________
        conv2d_101 (Conv2D)          (None, 8, 8, 256)         590080    
        _________________________________________________________________
        batch_normalization_100 (Bat (None, 8, 8, 256)         1024      
        _________________________________________________________________
        dropout_81 (Dropout)         (None, 8, 8, 256)         0         
        _________________________________________________________________
        conv2d_102 (Conv2D)          (None, 8, 8, 256)         590080    
        _________________________________________________________________
        batch_normalization_101 (Bat (None, 8, 8, 256)         1024      
        _________________________________________________________________
        dropout_82 (Dropout)         (None, 8, 8, 256)         0         
        _________________________________________________________________
        conv2d_103 (Conv2D)          (None, 8, 8, 256)         590080    
        _________________________________________________________________
        batch_normalization_102 (Bat (None, 8, 8, 256)         1024      
        _________________________________________________________________
        max_pooling2d_32 (MaxPooling (None, 4, 4, 256)         0         
        _________________________________________________________________
        conv2d_104 (Conv2D)          (None, 4, 4, 512)         1180160   
        _________________________________________________________________
        batch_normalization_103 (Bat (None, 4, 4, 512)         2048      
        _________________________________________________________________
        dropout_83 (Dropout)         (None, 4, 4, 512)         0         
        _________________________________________________________________
        conv2d_105 (Conv2D)          (None, 4, 4, 512)         2359808   
        _________________________________________________________________
        batch_normalization_104 (Bat (None, 4, 4, 512)         2048      
        _________________________________________________________________
        dropout_84 (Dropout)         (None, 4, 4, 512)         0         
        _________________________________________________________________
        conv2d_106 (Conv2D)          (None, 4, 4, 512)         2359808   
        _________________________________________________________________
        batch_normalization_105 (Bat (None, 4, 4, 512)         2048      
        _________________________________________________________________
        dropout_85 (Dropout)         (None, 4, 4, 512)         0         
        _________________________________________________________________
        conv2d_107 (Conv2D)          (None, 4, 4, 512)         2359808   
        _________________________________________________________________
        batch_normalization_106 (Bat (None, 4, 4, 512)         2048      
        _________________________________________________________________
        max_pooling2d_33 (MaxPooling (None, 2, 2, 512)         0         
        _________________________________________________________________
        conv2d_108 (Conv2D)          (None, 2, 2, 512)         2359808   
        _________________________________________________________________
        batch_normalization_107 (Bat (None, 2, 2, 512)         2048      
        _________________________________________________________________
        dropout_86 (Dropout)         (None, 2, 2, 512)         0         
        _________________________________________________________________
        conv2d_109 (Conv2D)          (None, 2, 2, 512)         2359808   
        _________________________________________________________________
        batch_normalization_108 (Bat (None, 2, 2, 512)         2048      
        _________________________________________________________________
        dropout_87 (Dropout)         (None, 2, 2, 512)         0         
        _________________________________________________________________
        conv2d_110 (Conv2D)          (None, 2, 2, 512)         2359808   
        _________________________________________________________________
        batch_normalization_109 (Bat (None, 2, 2, 512)         2048      
        _________________________________________________________________
        dropout_88 (Dropout)         (None, 2, 2, 512)         0         
        _________________________________________________________________
        conv2d_111 (Conv2D)          (None, 2, 2, 512)         2359808   
        _________________________________________________________________
        batch_normalization_110 (Bat (None, 2, 2, 512)         2048      
        _________________________________________________________________
        max_pooling2d_34 (MaxPooling (None, 1, 1, 512)         0         
        _________________________________________________________________
        dropout_89 (Dropout)         (None, 1, 1, 512)         0         
        _________________________________________________________________
        flatten_6 (Flatten)          (None, 512)               0         
        _________________________________________________________________
        dense_12 (Dense)             (None, 512)               262656    
        _________________________________________________________________
        batch_normalization_111 (Bat (None, 512)               2048      
        _________________________________________________________________
        dropout_90 (Dropout)         (None, 512)               0         
        _________________________________________________________________
        dense_13 (Dense)             (None, 10)                5130      
        =================================================================
        Total params: 20,315,978
        Trainable params: 20,304,074
        Non-trainable params: 11,904
        _________________________________________________________________
        '''

&amp;#x200B;

But, when I try to load the numpy array Python3 list as follows, it gives a ValueError-

&amp;#x200B;

        model.set_weights(vgg_19_np_wts)
    
    &gt; --------------------------------------------------------------------------- ValueError                                Traceback (most recent call
    &gt; last) &lt;ipython-input-75-576580841a1e&gt; in &lt;module&gt;
    &gt; ----&gt; 1 sample_model.set_weights(vgg_19_np_wts)
    &gt; 
    &gt; ~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py
    &gt; in set_weights(self, weights)    1522           'with a weight list of
    &gt; length %s, but the layer was '    1523           'expecting %s
    &gt; weights. Provided weights: %s...' %
    &gt; -&gt; 1524           (self.name, len(weights), expected_num_weights, str(weights)[:50]))    1525     1526     weight_index = 0
    &gt; 
    &gt; ValueError: You called `set_weights(weights)` on layer ""sequential_6""
    &gt; with a weight list of length 68, but the layer was expecting 100
    &gt; weights. Provided weights: [array([[[[ 0.54640555, -0.13893159,
    &gt; -0.21747045, ...

Help?

&amp;#x200B;

Guys, I fixed it.

&amp;#x200B;

The actual code to use is:

    winning_ticket_model.weights

instead of:

    winning_ticket_model.trainable_weights

&amp;#x200B;",0,tensorflow,2020-10-13
i1c86o,3D Sparse Matrix and Input Layer,"I would like to use a 3D sparse array as input for one of my network, scipy doesn't support it. I would therefore like to either supply coordinate and values to an input layer, or use sparse (  [https://sparse.pydata.org/](https://sparse.pydata.org/)  ). I struggle very much to find documentation to do a model pipeline with a sparse 3D input. Does someone have advice, snippet of code, or links ? (I checked the ""sparse"" section and the ""input layer"" of the tensorflow website, as well as the source code of the input layer, but it didn't help me very much)

Any help would be very much appreciated :)  
Thanks!",3,tensorflow,2020-10-13
i188kz,What's new in TensorFlow 2.3?,,9,tensorflow,2020-10-13
i171o2,TensorFlow Datasets: The Bad Parts,,2,tensorflow,2020-10-13
i15so7,Looking fot a magic pill. Reinforcement Learning.,,14,tensorflow,2020-10-13
i12wew,This Week in AI - Issue #28 | Rubik's Code,,3,tensorflow,2020-10-13
i0uvse,State of the art in instance segmentation: (Instance segmentation aims to classify each pixel in an image into an object category),,1,tensorflow,2020-10-13
i0ugtt,Tensorflow checkpoints,"Having trained a model on sagemaker and unzipped the model.tar.gz file, I see that there are 5 saved\_model.pb in export/Servo folder. Should I always be using the latest version and what does sagemaker do when it sees 5 different models?",1,tensorflow,2020-10-13
i0reqm,Need help caching TF hub model to make runtime faster,"I want to cache the Universal Sentence Encoder model so that it will run quicker but I honestly have no clue how. I have downloaded the tar.gz model file and one of the files in it is saved\_model.pb but I do not know how to use it.

Any advice would be really appreciated.",1,tensorflow,2020-10-13
i0r4c2,Where can I find an up-to-date tutorial on how to build TF Lite for Windows?,,2,tensorflow,2020-10-13
i0nrni,Extension to block NSFW images using TensorFlow JS,,20,tensorflow,2020-10-13
i0lgk6,Transfer learning/domain adaptation in TensorFlow,"I have a model that produces excellent results in an auxiliary domain for a binary output. However, I want to reuse the weights of this trained model for a related target domain where there is also an additional multi-class output along with the binary output.

1) Is this an example of transfer learning or domain adaptation?

2) How can I adapt the original weights for training in the target domain?  Since there's a mismatch between the layers and therefore the weights, I know this isn't a simple case of `save_weights` and `load_weights` right?",1,tensorflow,2020-10-13
i0l29r,I published a tutorial where I extract Root-Mean Square Energy and Zero-Crossing Rate from audio,"In my new video, you can learn how to extract Root-Mean Square Energy (RMSE) and Zero-Crossing Rate (ZCR) from audio data using the Python library librosa. I also show how RMSE and ZCR vary depending on music genre and type of audio source (i.e., voice vs noise).

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=EycaSbIRx-0&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=9](https://www.youtube.com/watch?v=EycaSbIRx-0&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=9)",16,tensorflow,2020-10-13
i0c72n,TensorFlow Certification Study Resources,,5,tensorflow,2020-10-13
i04xby,What does '???' mean in label maps?,"I'm using tflite for my android app, and I was wondering why some labels are separated by a '???'.",4,tensorflow,2020-10-13
hzw10b,How to use TensorFlow Object detection API to detect objects in live feed of webcam in real-time,,13,tensorflow,2020-10-13
hzv6x7,Saving the training of a model and loading it on start up of the app?,"Hi

So, I'm working on an image classification app, where you can upload a photo to the app for the CNN to classify.

I don't want the training to repeat every time I run the code, so, how can I save the training of the model and get it to load on the start up of the app.

Thanks",7,tensorflow,2020-10-13
hzu341,Latest from Carnegie Mellon and Facebook Researchers: 3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning,,2,tensorflow,2020-10-13
hznhyw,"Data fetch bottleneck at inference, but not during training for TPU","

This is what my inference setup looks like

    autotune = tf.data.experimental.AUTOTUNE
    
    with strategy.scope():
        model = LoadModel()
        raw_dataset = tf.data.TFRecordDataset(tfRecordAddress)
        train_dataset = raw_dataset.map(_parse_example, num_parallel_calls=autotune)
        train_dataset = train_dataset.padded_batch(batch_size, padding_values=(1, 1, b'-'), padded_shapes=(512, 512, 1))
        # train_dataset = train_dataset.repeat()
        train_dataset = train_dataset.prefetch(autotune)
        train_dataset = strategy.experimental_distribute_dataset(train_dataset)
    
    def per_core_inference_fn(inputIds,attnIds ):
        return model.inference((inputIds, attnIds))
    
    @tf.function
    def inference_fn(inputIds, attnIds):
        return strategy.run(per_core_inference_fn, args=(inputIds,attnIds))
    
    results = []
    for x in train_dataset:
        t0 = time.time()
        results.append(inference_fn(x[0], x[1]))
        t1 = time.time()
        print('time is :', t1-t0)

With huge batch_sizes, the inference is blazing fast, something like .0003 seconds. However, the fetching of the next batch takes a long time, for x in train_dataset:, like 60-80 seconds.

As far as I can tell, I am doing the inference correctly, but somehow the TPU's CPU is running into a huge bottleneck with the batch retrieval.

I did Not see this bottleneck during training. So it looks like model.fit is doing something I'm not.",1,tensorflow,2020-10-13
hzjmye,Good tutorials or information for learning tensorflow? (for object detection in ip camera images),"I currently have a custom ffmpeg service (on my raspberry pi) which streams an ip camera to youtube and a shared network drive.

I also have an ispy install on my home server (i'm going to switch to using zoneminder on my pi) which saves images from that ip camera whenever motion is detected, unfortunately lots of non-human, non-car things set it off. Such as cats, shadows of trees wavering in the wind. Where I just want it to save images of when a human or a car is in the image.

I've had the idea of instead tensorflow analysing those images motion detected by ispy/zoneminder to filter down to only relevant images that contain a human or a car.

So, say have zoneminder output motion detected images to a ram disk on my pi then have a tensorflow cronjob that checks that ram disk once a minute and outputs the images containing humans or cars to permanent storage.",2,tensorflow,2020-10-13
hzex4f,"How to Build Custom YOLOv4 Object Detector to Detect License Plates (TensorFlow, TensorFlow Lite and TensorRT)",,35,tensorflow,2020-10-13
hzb0b4,How and where can I download libtensorflow ?,"Hello,
With the release of tensorflow 2.3.0 I am looking for libtensorflow. The link for downloading it point to google cloud storage. It seems I can't download it without using gsutil from Google or going through google cloud web interface.

Please, is there any other method to download libtensorflow other than Google Cloud Storage and compiling from source ?

Thanks.",4,tensorflow,2020-10-13
hz3vl0,How do you import CSV data set?,"Hi guys, I am a beginner to tensorflow, I have some experience with machine learning since I used WEKA before but nothing with deep learning. On the tensorflow website, they have something about implementing CSV data but it looks way too complex for a beginner like me. Is there an easier way to do that?",2,tensorflow,2020-10-13
hz3ibi,TensorFlow 2.0 Tutorial For Beginners,,12,tensorflow,2020-10-13
hywcvr,I published a tutorial implementing the amplitude envelope feature for audio data from scratch,"In my new video, you can learn how to perform basic processing operations on audio with Librosa (e.g., load audio files, visualise waveforms). I also implement the amplitude envelope feature from scratch and show how it differs for music in different genres. 

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

https://www.youtube.com/watch?v=rlypsap6Wow&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=8",11,tensorflow,2020-10-13
hyrr8r,How do you implement a genetic algorithm with a deep neural network,"Hi guys, can someone please explain to me (or link to a tutorial) how do you implement a genetic algorithm with deep neural networks? 

When I try to think about it I come to the conclusion that the network is going to be changed every generation, but based on what? How do you decide when to add or remove new neurons or layers? How are you going to train a network like that?

Thank you.",3,tensorflow,2020-10-13
hyr34l,Help with multi-input layers,"I want to create an LSTM that takes in a list of inputs, each one to be passed through the layer normally, so I was thinking to either to do it by

1. Subclassing the tf.keras' Model class:

&amp;#8203;

    class CustomModel(tf.keras.Model):
    
        def __init__(self):
            super().__init__(**kwargs)
            hidden_lstm = LSTM(units=30)
            ...
    
        def call(self, inputs):
            for input in inputs:
                res = np.vstack(hidden_lstm(input))
            ...
            return output

2. Using the `Concatenate` layer with the Functional API:

    input1 = keras.layers.Input(shape=(200, 40))
    input2 = keras.layers.Input(shape=(200, 40))
    merged = keras.layers.Concatenate(axis=1)([input1, input2])

The problem I have with the first option is that I don't know if the gradients will backpropagate correctly to all input branches or not. With the second, I don't know which axis to concatenate, a single instance is of shape (no. of timesteps, dim. of features)

Does anyone have any insight?",10,tensorflow,2020-10-13
hyp2x4,CUDA and Tensorflow not compatible," I'm trying to run a GitHub code, using TensorFlow 1.x. I'm using colab for this. I'm encountering this kind of problem which I can't find a solution to. I'm using Tensorflow 1.15, my Cuda version installed is 10.1 and the Nvidia drive version in colab is NVIDIA-SMI 450.51.05 Driver Version: 418.67. When I tried to run another code above , cuda seems to be functional. I'm using the GPU mode on colab Can someone help me, please? Thanks. 

[Stackoverflow link](https://stackoverflow.com/questions/63088747/tensorflow-and-cuda-not-compatible)

&amp;#x200B;

https://preview.redd.it/fcbc4el2cdd51.jpg?width=1473&amp;format=pjpg&amp;auto=webp&amp;s=d5756b386dbf35ab2ebd019ddf360df79ab52088

https://preview.redd.it/sx7cfel2cdd51.jpg?width=1780&amp;format=pjpg&amp;auto=webp&amp;s=d1abf8f91da6b6c79d962f1a2c4eb82b8ee2f64c",0,tensorflow,2020-10-13
hyo4ww,Help with session and dataset,"Currently on tensorflow 1. I want to return a dataset from my custom train_input_fn using placeholders.

    def train_input_fn():
        # ... load real data into train_x and train_y
        placeholder_y = tf.placeholder(tf.int64, shape=([None]))
        placeholder_x = {}
        #... create placeholder shape and type

        dataset = tf.data.Dataset.from_tensor_slices((placeholder_x, placeholder_y)).map(parse_data)
        # parse_data function converts some rows into strings, i dont know if this is causing the problem, seems to 
         work fine with an iterator

        sess = tf.Session()
        sess.run(tf.global_variables_initializer())
        sess.run([], feed_dict={placeholder_x: train_x, placeholder_y:train_y})

        return dataset

error is: 
    ""You must feed a value for placeholder tensor 4 with dtype int64 and shape [?]""

When i print out the data using an iterator, the data seems to be loaded just fine. But only when i return it, it gives this error.",3,tensorflow,2020-10-13
hyf0t1,module 'tensorflow' has no attribute 'Session',"Please please please help me out here! I cannot resolve it! Nothing on stackoverflow or other websites helped.

I really hope I will get a reply here which helps.

so... my python version is Python 3.5.6 and Anaconda version is Conda 4.8.3. I am working with Jupyter Notebook. 

I am working in MacOS.

I have created an environment tfdeeplearning:

`conda create -n tfdeeplearning python=3.5`

which I have installed the following:

1. `conda install jupyter`
2. `conda install numpy`
3. `conda install pandas`
4. `conda install scikit-learn`
5. `conda install matplotlib`
6. `pip install --upgrade tensorflow` 

I am constantly getting this error: 

[Error Messages in Jupyter Notebook](https://preview.redd.it/866icechn9d51.png?width=1852&amp;format=png&amp;auto=webp&amp;s=7ecfff3dc9ba2a8107e3f0e23f430ad2e9607dd3)

I understand that the tf.Session() is not supported in tensorflow 2.2.0 which is the version I have after running the line pip install --upgrade tensorflow in the tfdeeplearning environment.

I have also tried doing:

`pip install tensorflow==1.4.0`

this broke the environment tfdeeplearning, and I had to reset the env. 

I have tried:

`pip uninstall tensorflow-gpu`

`pip install tensorflow-gpu`

which again, broke the env and I had to set it up once again. 

&amp;#x200B;

then I tried this in jupyter notebooks which is replacing `import tensorflow as tf`:

`import tensorflow.compat.v1 as tf`

`tf.disable_v2_behavior()`

which did not work either, because this time the error said AttributeError: module 'tensorflow' has no attribute 'compat'

So, I am out of options

&amp;#x200B;

P.S. Here is a list of all the dependencies I have in my virtual env called tfdeeplearning in conda if it helps at all:

`absl-py==0.9.0`

`appnope==0.1.0`

`astunparse==1.6.3`

`bleach==3.1.5`

`cachetools==4.1.1`

`certifi==2018.8.24`

`chardet==3.0.4`

`cycler==0.10.0`

`Cython==0.29.21`

`decorator==4.4.2`

`defusedxml==0.6.0`

`entrypoints==0.2.3`

`gast==0.3.3`

`google-auth==1.19.2`

`google-auth-oauthlib==0.4.1`

`google-pasta==0.2.0`

`grpcio==1.30.0`

`h5py==2.10.0`

`idna==2.10`

`importlib-metadata==1.7.0`

`ipykernel==4.10.0`

`ipython==5.8.0`

`ipython-genutils==0.2.0`

`ipywidgets==7.4.1`

`Jinja2==2.11.2`

`jsonschema==2.6.0`

`jupyter==1.0.0`

`jupyter-client==5.3.3`

`jupyter-console==5.2.0`

`jupyter-core==4.5.0`

`Keras-Preprocessing==1.1.2`

`kiwisolver==1.0.1`

`Markdown==3.2.2`

`MarkupSafe==1.0`

`matplotlib==3.0.0`

`mistune==0.8.3`

`mkl-fft==1.0.6`

`mkl-random==1.0.1`

`nbconvert==5.5.0`

`nbformat==5.0.7`

`notebook==5.6.0`

`numpy==1.18.5`

`oauthlib==3.1.0`

`opt-einsum==3.3.0`

`packaging==20.4`

`pandas==0.22.0`

`pandocfilters==1.4.2`

`pexpect==4.6.0`

`pickleshare==0.7.4`

`prometheus-client==0.8.0`

`prompt-toolkit==1.0.15`

`protobuf==3.12.2`

`ptyprocess==0.6.0`

`pyasn1==0.4.8`

`pyasn1-modules==0.2.8`

`Pygments==2.6.1`

`pyparsing==2.4.7`

`python-dateutil==2.8.1`

`pytz==2020.1`

`pyzmq==17.1.2`

`qtconsole==4.7.5`

`QtPy==1.9.0`

`requests==2.24.0`

`requests-oauthlib==1.3.0`

`rsa==4.6`

`scikit-learn==0.20.0`

`scipy==1.4.1`

`Send2Trash==1.5.0`

`simplegeneric==0.8.1`

`six==1.15.0`

`TBB==0.1`

`tensorboard==2.2.2`

`tensorboard-plugin-wit==1.7.0`

`tensorflow==2.2.0`

`tensorflow-estimator==2.2.0`

`termcolor==1.1.0`

`terminado==0.8.1`

`testpath==0.4.4`

`tornado==5.1.1`

`traitlets==4.3.2`

`urllib3==1.25.10`

`wcwidth==0.2.5`

`webencodings==0.5.1`

`Werkzeug==1.0.1`

`widgetsnbextension==3.4.1`

`wrapt==1.12.1`

`zipp==1.2.0`",1,tensorflow,2020-10-13
hycvhi,How does one download YouTube videos for a model?,"I’m trying to build a CNN to identify which video a specific frame belongs to, but in order to obtain training data I need to download a couple hundred videos off of YouTube. Is there an automated way for me to download all videos from select channels after a certain date? Thanks!",8,tensorflow,2020-10-13
hy9nd6,How to build an AI to read your handwriting with and without Tensorflow. (A comparison),"Hey guys/gals,

recently i have been working on two series. The first was building a digit analyzer from scratch:

 [https://www.youtube.com/watch?v=AsiGUfMSR5I&amp;list=PLB4RncStK2LU6gEzKvSKJcL2kLjHTMBTi&amp;index=2](https://www.youtube.com/watch?v=AsiGUfMSR5I&amp;list=PLB4RncStK2LU6gEzKvSKJcL2kLjHTMBTi&amp;index=2) 

Then i built the same system with Tensorflow, as you can see the latter is so much easier lol:

 [https://www.youtube.com/watch?v=CCygBhVNrTA](https://www.youtube.com/watch?v=CCygBhVNrTA) 

Take a peek and enjoy!",4,tensorflow,2020-10-13
hy9h5d,Various Tensorflow versions and their compatible Python versions?,"I am trying to get GPT-2 running and I'm finding that GPT-2 has very specific requirements for Tensorflow version(s), which in turn has very specific requirements for Python versions.  Is there a place that lists which versions are compatible with which Pythons?  Specifically Tensorflow 1.12.0, 1.13.0, 1.14.0, 1.15.0, 1.16.0, 2.0, 2.1.....  Thanks for your assistance!",1,tensorflow,2020-10-13
hy80o8,While training with tensorflow samples are very low . Is it normal or not?? Original code results have 17441 samples mine have 546.,,3,tensorflow,2020-10-13
hy6hx8,[AI application] Python implementation of Proximal Policy Optimization (PPO) algorithm for Super Mario Bros. 29/32 levels have been conquered,,2,tensorflow,2020-10-13
hy4drx,Browser-Based Augmented Reality Sudoku Solver using Tensorflow and Image...,,39,tensorflow,2020-10-13
hy37jh,Can someone please tell me how to create a custom object detection from scratch ...,I don't want any big dataset like coco...I just want to create a model from scratch and train it using my custom images...and I don't want to use object detection api..,6,tensorflow,2020-10-13
hxzf5d,"Object recognition with identical objects sometimes fails, why?"," I'm very new to TF and know pretty much noting when it comes to using it but I was able to adapt the existing example project to suit my needs. I'm trying to recognize on screen images like buttons, scrolls bars etc. All the images are identical, non clipped etc, but sometimes it still fails to recognize some of the elements. Here is an example (See attached image file) of the latest test where I'm training it to recognize ""continue"" buttons. Out of 5 continue buttons on the screen 4 are recognized correctly as buttons but the 5th is missed even though they are exactly identical. What exactly does that mean? 

&amp;#x200B;

This is what the output image looks like: [https://i.ibb.co/0tKp3gX/gif-frame-00.jpg](https://i.ibb.co/0tKp3gX/gif-frame-00.jpg)",1,tensorflow,2020-10-13
hxumai,Latest from Stanford and Adobe Researchers: Inferring 3D human motion from video sequences that takes initial 2D and 3D pose estimates as input.,,7,tensorflow,2020-10-13
hxou7d,Jupyter kernel resets every time I run this cells!,,6,tensorflow,2020-10-13
hxnzep,How to tell tensorflow that inputs are logically grouped with each other,For example I have a 5v5 game where each player is represented by 10 floats and I want to tell tensorflow that each 10 floats is related to a player and each 50 floats is related to a team. Is there some way to group values together within a single tensor or should I just let the AI figure it out?,1,tensorflow,2020-10-13
hxk85r,Tensorflow 2.0 Ressources,"Hey there!

I am searching for TF2 Ressources, but not about how to create Models with Keras but instead with ""real"" Tensorflow.

The background is that a few weeks ago the Tensorflow Object Detection API was released for TF2. (https://github.com/tensorflow/models/tree/master/research/object_detection). The pretrained models (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) can't be loaded with keras load_model, but instead with tf.saved_models.load(). This means the model can't be trained with Keras and basic Tensorflow needs to be used. That's why I am searching for Courses, Books and other Ressources to learn ""real"" Tensorflow. Do you know about any?",3,tensorflow,2020-10-13
hxk3t2,Low validation accuracy VGG-19 CIFAR-10 CNN,"Hey Guys, I am trying to train a VGG-19 CNN on CIFAR-10 dataset using data augmentation and batch normalization. The code can be found [VGG-19 CNN](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/VGG_19_CIFAR10.ipynb).

I took two approaches to training the model:

1. Using early stopping: loss = 2.2816 and accuracy = 47.1700%
2. Without early stopping: loss = 3.3211 and accuracy = 56.6800%

The loss and accuracy are on validation data. Also, when the model is trained without early stopping, it's trained for 145 epochs.

&amp;#x200B;

[Model Accuracy](https://preview.redd.it/bf24c8b209d51.png?width=610&amp;format=png&amp;auto=webp&amp;s=4814c9ff093676a0df6f60787c56aab3081d2966)

&amp;#x200B;

&amp;#x200B;

[Model Loss](https://preview.redd.it/mtwjwl0409d51.png?width=596&amp;format=png&amp;auto=webp&amp;s=e393a867b54e7baea5dff63c2e7171d04d0f32d9)

&amp;#x200B;

What's going wrong? Why am I not able to reach a higher validation accuracy and lower validation loss?

&amp;#x200B;

Thanks!",1,tensorflow,2020-10-13
hxjcrj,How long does it take to learn Tensorflow once you know basic DL theory?," 

I just finished the Deep Learning by [deeplearning.ai](https://deeplearning.ai/) course on Coursera. How long does it take to learn a respectable amount of Tensorflow now that I know the basics of DL? By a respectable amount, I mean enough to say that I know Tensorflow in my resume when applying for internships and such. Also what would be the best way to go about picking up the framework?

(Idk if this matters but I'm largely interested in Computer Vision and NLP.)",14,tensorflow,2020-10-13
hxiwzi,ERROR trying to install nvidia cuda toolkit on ubuntu 20.04 for tensorflow-gpu,"Here are the logs:

\# bash cuda\_10.1.243\_418.87.00\_linux.run

Installation failed. See log at /var/log/cuda-installer.log for details.

\# cat /var/log/cuda-installer.log

\[INFO\]:  Driver installation detected by command: apt list --installed | grep -e  nvidia-driver-\[0-9\]\[0-9\]\[0-9\] -e nvidia-\[0-9\]\[0-9\]\[0-9\]

\[INFO\]: Cleaning up window

\[INFO\]: Complete

\[INFO\]: Checking compiler version...

\[INFO\]: gcc location: /usr/bin/gcc

\[INFO\]: gcc version: gcc version 8.4.0 (Ubuntu 8.4.0-3ubuntu2)

\[INFO\]: Initializing menu

\[INFO\]: Setup complete

\[INFO\]: Components to install:

\[INFO\]: Driver

\[INFO\]: 418.87.00

\[INFO\]:  Executing NVIDIA-Linux-x86\_64-418.87.00.run --ui=none --no-questions  --accept-license --disable-nouveau --no-cc-version-check  --install-libglvnd --run-nvidia-xconfig  2&gt;&amp;1

\[INFO\]: Finished with code: 256

\[ERROR\]: Install of driver component failed.

\[ERROR\]: Install of 418.87.00 failed, quitting",1,tensorflow,2020-10-13
hxib6p,Tensorflow Course,"Hi, 

Do any of you know where i can get instructor let Tensorflow course to help me get certified...?",1,tensorflow,2020-10-13
hxhbfk,How do I send positive classifications from one CNN to another CNN in the same pipeline?,I have a dataset of human awareness levels and have converted them to frames and applied Haars Cascade to crop out the faces in the images. But im worried about the number of false positives in the images. I haven't checked exactly as there are 600000 frames so I doubt it's possible. Im planning on using transfer learning on the Inception model to detect if there are faces in these frames then send them over to my other model for classifying the different awareness levels. Any suggestions on how can I do this? Should I just identify the faces then save them in their respective folders then conduct classification on those images?,1,tensorflow,2020-10-13
hx8y6q,Accelerating TensorFlow Lite with XNNPACK Integration,,5,tensorflow,2020-10-13
hx74qf,How do you install tensorflow for anaconda,,0,tensorflow,2020-10-13
hx5oqs,Data Augmentation CIFAR-10 reduces CNN model accuracy,"I have a [Conv-6](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Data_Augmentation_Conv6_CIFAR10_Example.ipynb) CNN for CIFAR-10 where I am using Python3, TensorFlow2.0 based data augmentations as follows:

&amp;#x200B;

        # Example of using 'tf.keras.preprocessing.image import ImageDataGenerator class's - flow(x, y)':
        datagen = ImageDataGenerator(
            # featurewise_center=True,
            # featurewise_std_normalization=True,
            rotation_range = 90,
            width_shift_range = 0.1,
            height_shift_range = 0.1,
            horizontal_flip = True
        )
    
    

But using this data augmentation technique reduces the accuracy to 66.43%. As opposed to this, when I train the same model without any data augmentations, the accuracy is 79.22%.

&amp;#x200B;

What's going wrong?

&amp;#x200B;

Thanks",5,tensorflow,2020-10-13
hx3ebc,"I need Document Image with corner(x1,y1,x2,y2,x3,y3,x4,y4) points dataset. If anybody knows the link do share.",,1,tensorflow,2020-10-13
hx0cea,Using MobilenetV2 to train a mask detector and then use OpenCV to detect masks in live feed,,5,tensorflow,2020-10-13
hwzrjf,Right abstraction levels for multi-loss autoencoder,"Hi, I'm searching for the right abstractions to work with for my model, whose explanation doesn't quite fit in the title. Please bare (bear?) with me.

It is an autoencoder at heart, so it accepts one input and produces the same output. This could easily be achieved with sequential or functional models by passing in the same X and Y data. However, I'd like to enforce a constraint on a hidden layer, for which I've created another layer. The model shall jointly optimise for a ""good"" clustering result. The added layer contains trainable cluster centers. Its output is the cluster assignment for each sample fed to the autoencoder. But when training, the loss is calculated directly with the layer output, without any targets.

So the model has a single input, two outputs at inference time and one at training time but with two losses. As far as I've understood, `model.compile` only takes losses with a prediction and a target. But I think using Keras-level constructs is still appropriate.

Does this seem reasonable?
- Subclass `Model` and define layers in the constructor
- Override `call` and vary the number of outputs based on the `training` flag
- Override `compile` to generate the custom loss function but pass other parameters to `super`
- Override `fit` to handle the custom loss

The two first points seem solid, but I've not done anything even remotely related to the last two, so I'm quite unsure about them. Any pointers would be greatly appreciated! Cheers.

---
After digging around some more, I found out that `add_loss` is taken into account in `fit` out of the box. So now I simply return the decoder output when training, and add the clustering result when not. `compile` seems to work as if that output is the only one. But I'm not yet sure if it actually does the thing I expect.",1,tensorflow,2020-10-13
hwzcfs,Best way run mass inference on a lot of data?,"I've tried setups that use batching, but I find that putting the data in pandas in using `df['inference'] = df.map( function_with_model)` is faster. 

I don't know why this is, but I imagine there must be some faster way. Perhaps using tf.data/tf.records?",0,tensorflow,2020-10-13
hwytgd,CNN Receptive Field Computation Using Backprop with TensorFlow,"Consider an image classification problem that we typically solve using a Convolutional Neural Network (CNN).  


Most tutorials on image classification treat CNN as a black box.  


How do we know which part of the image the CNN saw to make the classification decision?  


To understand which part of the image our model is looking at, we need to calculate the receptive field by backpropagating the response at the output layer.  


We wrote a post about this a few months back and shared code in PyTorch.  


We got several requests for a Tensorflow version of the code. So, here it is  


[https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)

https://preview.redd.it/pa1l8kztyrc51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=688c393d463f551cefc888e6d907e0c8b02050cd",1,tensorflow,2020-10-13
hwxd6q,This Week in AI - Issue #27 | Rubik's Code,,1,tensorflow,2020-10-13
hwv13i,Error while running model,"I am getting this error 

tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor of shape [5,5,1632,1] and type float  

while running an efficientnet object detection model for  a video. How to solve this error ?",1,tensorflow,2020-10-13
hwtwru,"From ECCV2020: Reconstruct a morphable shape, texture, and viewpoint from an image collection without 3D ground truth *and* 2D keypoints, allowing us to explore new categories like shoes!",,4,tensorflow,2020-10-13
hwssgt,Is it still worth learning TF (not keras) for a junior data-sciencist?,"I have very solid knowledge of Keras, but I don't really know how to use Tensorflow far from ""from tf.keras.whatever import this"", and I would like to, but I don't know if I will take advantage of it and if it's better improve my keras knowledge or jump to Pytorch

Any advise?",1,tensorflow,2020-10-13
hwootp,TensroFlow2.0 CNN validation ValueError,"I have a Conv-6 CNN inspired from VGG-19 for CIFAR-10 dataset which I am using with Data Augmentation using tf.Datagen flow() method. The code is as follows-

&amp;#x200B;

&amp;#x200B;

        # Data preprocessing and cleaning:
        # input image dimensions
        img_rows, img_cols = 32, 32
        
        # Load CIFAR-10 dataset-
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
        
        print(""X_train.shape = {0}, y_train.shape = {1}"".format(X_train.shape, y_train.shape))
        print(""X_test.shape = {0}, y_test.shape = {1}"".format(X_test.shape, y_test.shape))
        # X_train.shape = (50000, 32, 32, 3), y_train.shape = (50000, 1)
        # X_test.shape = (10000, 32, 32, 3), y_test.shape = (10000, 1)
        
        if tf.keras.backend.image_data_format() == 'channels_first':
            X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)
            X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)
            input_shape = (3, img_rows, img_cols)
        else:
            X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)
            X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)
            input_shape = (img_rows, img_cols, 3)
        
        print(""\n'input_shape' which will be used = {0}\n"".format(input_shape))
        # 'input_shape' which will be used = (32, 32, 3)
        
        
        # Convert datasets to floating point types-
        X_train = X_train.astype('float32')
        X_test = X_test.astype('float32')
        
        # Normalize the training and testing datasets-
        X_train /= 255.0
        X_test /= 255.0
        
        # convert class vectors/target to binary class matrices or one-hot encoded values-
        y_train = tf.keras.utils.to_categorical(y_train, num_classes)
        y_test = tf.keras.utils.to_categorical(y_test, num_classes)
        
        print(""\nDimensions of training and testing sets are:"")
        print(""X_train.shape = {0}, y_train.shape = {1}"".format(X_train.shape, y_train.shape))
        print(""X_test.shape = {0}, y_test.shape = {1}"".format(X_test.shape, y_test.shape))
        # Dimensions of training and testing sets are:
        # X_train.shape = (50000, 32, 32, 3), y_train.shape = (50000, 10)
        # X_test.shape = (10000, 32, 32, 3), y_test.shape = (10000, 10)
        
        train_dataset_features = tf.data.Dataset.from_tensor_slices(X_train)
        train_dataset_labels = tf.data.Dataset.from_tensor_slices(y_train)
        test_dataset_features = tf.data.Dataset.from_tensor_slices(X_test)
        test_dataset_labels = tf.data.Dataset.from_tensor_slices(y_test)
        
        # Choose an optimizer and loss function for training-
        loss_fn = tf.keras.losses.CategoricalCrossentropy()
        optimizer = tf.keras.optimizers.Adam(lr = 0.0003)
        
        # Select metrics to measure the error &amp; accuracy of model.
        # These metrics accumulate the values over epochs and then
        # print the overall result-
        train_loss = tf.keras.metrics.Mean(name = 'train_loss')
        train_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'train_accuracy')
        
        test_loss = tf.keras.metrics.Mean(name = 'test_loss')
        test_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'test_accuracy')
        
        
        # Example of using 'tf.keras.preprocessing.image import ImageDataGenerator class's - flow(x, y)':
        datagen = ImageDataGenerator(
            # featurewise_center=True,
            # featurewise_std_normalization=True,
            rotation_range = 90,
            width_shift_range = 0.1,
            height_shift_range = 0.1,
            horizontal_flip = True
        )
        
        
        
        def conv6_cnn():
            """"""
            Function to define the architecture of a neural network model
            following Conv-6 architecture for CIFAR-10 dataset and using
            provided parameter which are used to prune the model.
            
            Conv-6 architecture-
            64, 64, pool  -- convolutional layers
            128, 128, pool -- convolutional layers
            256, 256, pool -- convolutional layers
            256, 256, 10  -- fully connected layers
            
            Output: Returns designed and compiled neural network model
            """"""
            
            l = tf.keras.layers
            
            model = Sequential()
            
            model.add(
                Conv2D(
                    filters = 64, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same',
                    input_shape=(32, 32, 3)
                )    
            )
                
            model.add(
                Conv2D(
                    filters = 64, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
            
            model.add(
                MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
                Conv2D(
                    filters = 128, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
        
            model.add(
                Conv2D(
                    filters = 128, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
        
            model.add(
                MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
        
            model.add(
                Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
        
            model.add(
                Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
        
            model.add(
                MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(Flatten())
            
            model.add(
                Dense(
                    units = 256, activation='relu',
                    kernel_initializer = tf.initializers.GlorotNormal()
                )
            )
            
            model.add(
                Dense(
                    units = 256, activation='relu',
                    kernel_initializer = tf.initializers.GlorotNormal()
                )
            )
            
            model.add(
                Dense(
                    units = 10, activation='softmax'
                )
            )
            
        
            # Compile pruned CNN-
            model.compile(
                loss=tf.keras.losses.categorical_crossentropy,
                # optimizer='adam',
                optimizer=tf.keras.optimizers.Adam(lr = 0.0003),
                metrics=['accuracy']
            )
            
            
            return model
        
        # Instantiate a new Conv-2 CNN model-
        orig_model = conv6_cnn()
        
        # Load weights from before having 92.55% sparsity-
        orig_model.load_weights(""Conv_6_CIFAR10_Magnitude_Based_Winning_Ticket_Distribution_92.55423622890814.h5"")
        
        # Create mask using winning ticket-
        
        # Use masks to preserve sparsity-
        # Instantiate a new neural network model for which, the mask is to be created,
        mask_model = conv6_cnn()
            
        # Load weights of PRUNED model-
        mask_model.set_weights(orig_model.get_weights())
            
        # For each layer, for each weight which is 0, leave it, as is.
        # And for weights which survive the pruning,reinitialize it to ONE (1)-
        for wts in mask_model.trainable_weights:
            wts.assign(tf.where(tf.equal(wts, 0.), 0., 1.))
        
        
        # User input parameters for Early Stopping in manual implementation-
        minimum_delta = 0.001
        patience = 3
        
        best_val_loss = 100
        loc_patience = 0
        
        # Initialize a new LeNet-300-100 model-
        winning_ticket_model = conv6_cnn()
        
        # Load weights of winning ticket-
        winning_ticket_model.set_weights(orig_model.get_weights())
        
        # Define 'train_one_step()' and 'test_step()' functions here-
        u/tf.function
        def train_one_step(model, mask_model, optimizer, x, y):
            '''
            Function to compute one step of gradient descent optimization
            '''
            with tf.GradientTape() as tape:
                # Make predictions using defined model-
                y_pred = model(x)
        
                # Compute loss-
                loss = loss_fn(y, y_pred)
                
            # Compute gradients wrt defined loss and weights and biases-
            grads = tape.gradient(loss, model.trainable_variables)
            
            # type(grads)
            # list
            
            # List to hold element-wise multiplication between-
            # computed gradient and masks-
            grad_mask_mul = []
            
            # Perform element-wise multiplication between computed gradients and masks-
            for grad_layer, mask in zip(grads, mask_model.trainable_weights):
                grad_mask_mul.append(tf.math.multiply(grad_layer, mask))
            
            # Apply computed gradients to model's weights and biases-
            optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))
        
            # Compute accuracy-
            train_loss(loss)
            train_accuracy(y, y_pred)
        
            return None
            
            
        u/tf.function
        def test_step(model, optimizer, data, labels):
            """"""
            Function to test model performance
            on testing dataset
            """"""
            
            predictions = model(data)
            t_loss = loss_fn(labels, predictions)
        
            test_loss(t_loss)
            test_accuracy(labels, predictions)
        
            return None
        
        
        curr_step = 0
            
        for x, y in datagen.flow(X_train, y_train, batch_size = batch_size, shuffle = True):
            train_one_step(winning_ticket_model, mask_model, optimizer, x, y)
            # print(""current step = "", curr_step)
            curr_step += 1
                
            if curr_step &gt;= X_train.shape[0] // batch_size:
                print(""\nTerminating training (datagen.flow())"")
                break
         
    

But the following code gives error:

&amp;#x200B;

        for x_t, y_t in test_dataset:
            test_step(winning_ticket_model, optimizer, x_t, y_t)
    

&amp;#x200B;

&gt;\&gt; ValueError                                Traceback (most recent call  
&gt;  
&gt;\&gt; last) &lt;ipython-input-77-c422951ac154&gt; in &lt;module&gt;  
&gt;  
&gt;\&gt;       1 for x\_t, y\_t in test\_dataset:  
&gt;  
&gt;\&gt; ----&gt; 2     test\_step(winning\_ticket\_model, optimizer, x\_t, y\_t)  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_\_call\_\_(self, \*args, \*\*kwds)  
&gt;  
&gt;\&gt;     578         xla\_context.Exit()  
&gt;  
&gt;\&gt;     579     else:  
&gt;  
&gt;\&gt; --&gt; 580       result = self.\_call(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     581   
&gt;  
&gt;\&gt;     582     if tracing\_count == self.\_get\_tracing\_count():  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_call(self, \*args, \*\*kwds)  
&gt;  
&gt;\&gt;     625       # This is the first call of \_\_call\_\_, so we have to initialize.  
&gt;  
&gt;\&gt;     626       initializers = \[\]  
&gt;  
&gt;\&gt; --&gt; 627       self.\_initialize(args, kwds, add\_initializers\_to=initializers)  
&gt;  
&gt;\&gt;     628     finally:  
&gt;  
&gt;\&gt;     629       # At this point we know that the initialization is complete (or less  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_initialize(self, args, kwds, add\_initializers\_to)  
&gt;  
&gt;\&gt;     504     self.\_concrete\_stateful\_fn = (  
&gt;  
&gt;\&gt;     505         self.\_stateful\_fn.\_get\_concrete\_function\_internal\_garbage\_collected(   
&gt;  
&gt;\&gt; # pylint: disable=protected-access  
&gt;  
&gt;\&gt; --&gt; 506             \*args, \*\*kwds))  
&gt;  
&gt;\&gt;     507   
&gt;  
&gt;\&gt;     508     def invalid\_creator\_scope(\*unused\_args, \*\*unused\_kwds):  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_get\_concrete\_function\_internal\_garbage\_collected(self, \*args,  
&gt;  
&gt;\&gt; \*\*kwargs)    2444       args, kwargs = None, None    2445     with self.\_lock:  
&gt;  
&gt;\&gt; -&gt; 2446       graph\_function, \_, \_ = self.\_maybe\_define\_function(args, kwargs)    2447     return graph\_function    2448   
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_maybe\_define\_function(self, args, kwargs)    2775     2776        
&gt;  
&gt;\&gt; self.\_function\_cache.missed.add(call\_context\_key)  
&gt;  
&gt;\&gt; -&gt; 2777       graph\_function = self.\_create\_graph\_function(args, kwargs)    2778       self.\_function\_cache.primary\[cache\_key\] =  
&gt;  
&gt;\&gt; graph\_function    2779       return graph\_function, args, kwargs  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_create\_graph\_function(self, args, kwargs,  
&gt;  
&gt;\&gt; override\_flat\_arg\_shapes)    2665             arg\_names=arg\_names,     
&gt;  
&gt;\&gt; 2666             override\_flat\_arg\_shapes=override\_flat\_arg\_shapes,  
&gt;  
&gt;\&gt; -&gt; 2667             capture\_by\_value=self.\_capture\_by\_value),    2668         self.\_function\_attributes,    2669         # Tell the ConcreteFunction  
&gt;  
&gt;\&gt; to clean up its graph once it goes out of  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/framework/func\_graph.py  
&gt;  
&gt;\&gt; in func\_graph\_from\_py\_func(name, python\_func, args, kwargs, signature,  
&gt;  
&gt;\&gt; func\_graph, autograph, autograph\_options, add\_control\_dependencies,  
&gt;  
&gt;\&gt; arg\_names, op\_return\_value, collections, capture\_by\_value,  
&gt;  
&gt;\&gt; override\_flat\_arg\_shapes)  
&gt;  
&gt;\&gt;     979         \_, original\_func = tf\_decorator.unwrap(python\_func)  
&gt;  
&gt;\&gt;     980   
&gt;  
&gt;\&gt; --&gt; 981       func\_outputs = python\_func(\*func\_args, \*\*func\_kwargs)  
&gt;  
&gt;\&gt;     982   
&gt;  
&gt;\&gt;     983       # invariant: \`func\_outputs\` contains only Tensors, CompositeTensors,  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in wrapped\_fn(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     439         # \_\_wrapped\_\_ allows AutoGraph to swap in a converted function. We give  
&gt;  
&gt;\&gt;     440         # the function a weak reference to itself to avoid a reference cycle.  
&gt;  
&gt;\&gt; --&gt; 441         return weak\_wrapped\_fn().\_\_wrapped\_\_(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     442     weak\_wrapped\_fn = weakref.ref(wrapped\_fn)  
&gt;  
&gt;\&gt;     443   
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/framework/func\_graph.py  
&gt;  
&gt;\&gt; in wrapper(\*args, \*\*kwargs)  
&gt;  
&gt;\&gt;     966           except Exception as e:  # pylint:disable=broad-except  
&gt;  
&gt;\&gt;     967             if hasattr(e, ""ag\_error\_metadata""):  
&gt;  
&gt;\&gt; --&gt; 968               raise e.ag\_error\_metadata.to\_exception(e)  
&gt;  
&gt;\&gt;     969             else:  
&gt;  
&gt;\&gt;     970               raise  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; ValueError: in user code:  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt;     &lt;ipython-input-61-9c297d161e54&gt;:45 test\_step  \*  
&gt;  
&gt;\&gt;         predictions = model(data)  
&gt;  
&gt;\&gt;     /home/majumdar/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base\_layer.py:886  
&gt;  
&gt;\&gt; \_\_call\_\_  \*\*  
&gt;  
&gt;\&gt;         [self.name](https://self.name))  
&gt;  
&gt;\&gt;     /home/majumdar/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input\_spec.py:180  
&gt;  
&gt;\&gt; assert\_input\_compatibility  
&gt;  
&gt;\&gt;         str(x.shape.as\_list()))  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt;     ValueError: Input 0 of layer sequential\_7 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: \[32, 32, 3\]  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; ​

&amp;#x200B;

&amp;#x200B;

What's the problem?

&amp;#x200B;

Thanks!",1,tensorflow,2020-10-13
hwonph,Predict the winner of a 5v5 game,"I have data from a 5v5 player game that looks like this:

`map1,ally1,ally2,ally3,ally4,ally5,enemy1,enemy2,enemy3,enemy4,enemy5,winner`

For example:

`Jaguar Falls,Androxus,Tiberius,Khan,Jenos,Seris,Inara,Viktor,Terminus,Io,Tyra,1`

Jaguar Falls is the map the match took place on. Androxus,Tiberius,Khan,ect. are all types of champions you can select in the game. The last column being 1 indicates that the allied team won the game.

I have about 750 games worth of relevant data and another 7250 games worth of less relevant data that would still be better than nothing if needed. I'm trying to predict which team will win before the game has started.

This is how I read in the data:

    field_names = [""map"", ""c1"", ""c2"", ""c3"", ""c4"", ""c5"", ""e1"", ""e2"", ""e3"", ""e4"", ""e5"", ""result""]
    df_train = pd.read_csv('input/match_results.csv', names=field_names, skiprows=1)
    for count in range(1, 6):
        str_count = str(count)
        df_train['c' + str_count] = pd.Categorical(df_train['c' + str_count])
        df_train['c' + str_count] = eval(""df_train.c"" + str_count + "".cat.codes"")
        df_train['e' + str_count] = pd.Categorical(df_train['e' + str_count])
        df_train['e' + str_count] = eval(""df_train.e"" + str_count + "".cat.codes"")
    
        df_train['map'] = pd.Categorical(df_train['map'])
        df_train['map'] = df_train.map.cat.codes
        df_train['result'] = pd.Categorical(df_train['result'])
        # df_train['result']=pd.Float64Index
        df_train['result'] = df_train.result.cat.codes
    target = df_train.pop('result')
    targets = np.array(list(x for x in target.values))
    dataset = tf.data.Dataset.from_tensor_slices((df_train.values, targets))
    train_data = dataset.shuffle(len(df_train)).batch(1)

When I try to train a model on this data it never really seems to make progress in predicting the winner column and output looks something like this:

    step: 7850, loss: nan, accuracy: 0.000000
    step: 7900, loss: nan, accuracy: 1.000000
    step: 7950, loss: nan, accuracy: 1.000000
    step: 8000, loss: nan, accuracy: 0.000000
    step: 8050, loss: nan, accuracy: 1.000000
    step: 8100, loss: nan, accuracy: 0.000000
    step: 8150, loss: nan, accuracy: 1.000000
    step: 8200, loss: nan, accuracy: 0.000000

Just a couple question so far:

1. How can my accuracy always be 0 or 1? Shouldn't it be some percentage or decimal between 0 and 1?
2. Is the way I'm reading in the data correct for at least a naive approach to this problem?

I'm also aware that I probably don't have enough data to train my model this way, so I was [reading about teaching the AI about team compositions](https://blog.insightdatascience.com/hero2vec-d42d6838c941). I even converted [this repository](https://github.com/ybw9000/hero2vec) to work with the game I have data for and now have a champion model file named model.p that based on the visualizations it generates seems to have learned the different roles that each champion fills on a team.

Now for more questions:

1. The person who wrote [this article](https://blog.insightdatascience.com/hero2vec-d42d6838c941) doesn't explain the whole process, so I'm not sure how to leverage the champion model.p file I have to improve my win rate prediction. Is there a way to tell tensorflow that c1,c2,c3,c4,c5 and e1,e2,e3,e4,e5 are all champions and give it this model to better understand how these champions relate? My understanding is I can use the model to encode similar champions in a similar way before passing them to tensorflow.",2,tensorflow,2020-10-13
hwobsu,Quantization aware training ...,,3,tensorflow,2020-10-13
hwn7kz,Nvidia Jetson and Coral.ai Edge TPU together,"I've seen plenty of Versus and Comparisons of both systems, vice versa for the INtel Neural Compute 2. But is there any way or possibility to combine a Jetson (such as the Xavier NX) and the Edge TPU or the NC2?",1,tensorflow,2020-10-13
hwjwt3,What combination of visual studio and CUDA can we use currently for tensorflow to run properly?,"I had installed CUDAv10. 2 and my tensorflow was working properly but as soon as I did a Windows update, it started showing the error that cannot find cudart64_100.dll file. The funny part is that I didn't have this file earlier. All my environments that were set up for using tensorflow gpu are all ruined now and I cannot run anything. I am having plans of reinstalling everything now including Anaconda, Cuda and Visual Studio. Someone please suggest compatible versions which have been tested to work together. Has anyone faced a similar issue with Windows version 2004?",1,tensorflow,2020-10-13
hwhbwr,I published a video explaining time-domain audio features for machine learning,"In my new video, I introduce fundamental time-domain audio features, such as Amplitude Envelope, Root-Mean-Squared Energy, and Zero Crossing Rate. I explain the intuition and the math behind these temporal acoustic features, and mention a few sample applications.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=SRrQ\_v-OOSg&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=7](https://www.youtube.com/watch?v=SRrQ_v-OOSg&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=7)",15,tensorflow,2020-10-13
hw3we5,GPU for cv and rf,"I want to upgrade gpu on my pc. Top level nvidia titan not an option, coz kaggling, learning and doing pet project is a hobby. 

I thought about nvidia 1080 or 20xx gpus. Found  out that these performance not so different as their cost. Do 20xxs really much better than 1080?

Or may be both gpus are not fitting for comfortably playing with tf at home?",1,tensorflow,2020-10-13
hw3jq8,"""Failed to get convolution algorithm. This is probably because cuDNN failed to initialize"" Error","Im trying to get my model training on GPU. Im using tensorflow 2.2.0 and python 2.8. I installed the proper CUDA and cudNN versions according to this: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source) (CUDA 10.1 and cuDNN 7.6). I limit the memory allocated on the card like so: 

    gpus = tensorflow.config.experimental.list_physical_devices('GPU')
    if gpus:
      # Restrict TensorFlow to only allocate 1GB of memory on the first GPU
      try:
        tensorflow.config.experimental.set_virtual_device_configuration(
            gpus[0],
            [tensorflow.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*2)])
        logical_gpus = tensorflow.config.experimental.list_logical_devices('GPU')
        print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")
      except RuntimeError as e:
        # Virtual devices must be set before GPUs have been initialized
        print(e)

My card is a GTX 1660 (6 GB of memory). My OS is pop\_OS.

When I try to train my model, I get this error:

    tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.

No fixes I can find online work.

Thank you.",1,tensorflow,2020-10-13
hvuwd6,Tensorflow GPU Setup,"https://preview.redd.it/4l16i7h47fc51.png?width=611&amp;format=png&amp;auto=webp&amp;s=4b542dd9ba98f6353799e8f5fc2eeeea07f3e972

Seen people facing issues on initial setup for Tensorflow GPU, sharing my personal repo which helped me initially to setup and work easier.

[Tensorflow -GPU Setup](https://github.com/rexdivakar/Deep-Learning-Setup)

Star the repo if u like it.",1,tensorflow,2020-10-13
hvunr8,"ValueError: Failed to find data adapter that can handle input: ,","This is a weird error...

&amp;#x200B;

https://preview.redd.it/bwb5jhda4fc51.png?width=1002&amp;format=png&amp;auto=webp&amp;s=b03df71635666922cf198c2f77d1e922c56798c3

&amp;#x200B;

https://preview.redd.it/1tq6oe0d4fc51.png?width=1415&amp;format=png&amp;auto=webp&amp;s=c3c3bac12c4aa1f1b0b0170ef585e17f7f057be1

https://preview.redd.it/rp0den1g4fc51.png?width=1377&amp;format=png&amp;auto=webp&amp;s=64eb3ed972fc557f554cf126eb5f960ebd058dc7",1,tensorflow,2020-10-13
hvt4u5,TensorFlow Object Detection API Officially Supports TF2: Google,,2,tensorflow,2020-10-13
hvqytu,TensorFlow 2.3.0-rc2 released with full keras preprocessing layers API,,23,tensorflow,2020-10-13
hvqtqk,Really need help installing tensorflow gpu on windows,"Hi there, I already formated my pc, followed every single online tutorial and couldn't make it work on my GTX 1660TI

Is there an script to install it or something? Kinda desperate rn",2,tensorflow,2020-10-13
hvqiwx,Large Scale COVID19 Contact Tracing using AI +Vision powered GeoLocalization — A.Eye-Vision,,5,tensorflow,2020-10-13
hvqb5a,Can you tell me what's wrong with the code??,"**here's the code:**

 import os

train\_cat\_dir = os.path.join('/Users/USER/Desktop/fox-or-cat/cat')  
train\_fox\_dir = os.path.join('C:/', 'Users', 'USER', 'Desktop', 'fox-or-cat/fox')

import tensorflow as tf  


model = tf.keras.models.Sequential(\[  
 \# This is the first convolution  
tf.keras.layers.Conv2D(16, (3,3), activation='relu', input\_shape=(300, 300, 3)),  
tf.keras.layers.MaxPooling2D(2, 2),  
 \# The second convolution  
tf.keras.layers.Conv2D(32, (3,3), activation='relu'),  
tf.keras.layers.MaxPooling2D(2,2),  
 \# The third convolution  
tf.keras.layers.Conv2D(64, (3,3), activation='relu'),  
tf.keras.layers.MaxPooling2D(2,2),  
 \# The fourth convolution  
tf.keras.layers.Conv2D(64, (3,3), activation='relu'),  
tf.keras.layers.MaxPooling2D(2,2),  
 \# The fifth convolution  
tf.keras.layers.Conv2D(64, (3,3), activation='relu'),  
tf.keras.layers.MaxPooling2D(2,2),  
 \# Flatten the results to feed into a DNN  
tf.keras.layers.Flatten(),  
 \# 512 neuron hidden layer  
tf.keras.layers.Dense(512, activation='relu'),  
 \# Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')  
tf.keras.layers.Dense(1, activation='sigmoid')  
\])  


model.summary()  
from tensorflow.keras.optimizers import RMSprop  
model.compile(loss='binary\_crossentropy',  
optimizer=RMSprop(lr=0.001),  
metrics=\['accuracy'\])

from tensorflow.keras.preprocessing.image import ImageDataGenerator  
train\_datagen = ImageDataGenerator(rescale=1/255)  
train\_generator = train\_datagen.flow\_from\_directory(  
 '/Users/USER/Desktop/fox-or-cat/',   
target\_size=(300, 300),  # All images will be resized to 150x150  
batch\_size=128,  
class\_mode='binary')  
 history = model.fit(  
train\_generator,  
steps\_per\_epoch=8,    
epochs=15,  
verbose=1)  


**And here are the errors:**

Found 1007 images belonging to 2 classes.

Traceback (most recent call last):

  File ""c:/Users/USER/Desktop/proj-test#1/proj-tst-1.py"", line 169, in &lt;module&gt;

verbose=1)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\[training.py](https://training.py)"", line 66, in \_method\_wrapper

return method(self, \*args, \*\*kwargs)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\[training.py](https://training.py)"", line 815, in fit

model=self)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data\_adapter.py"", line 1112, in \_\_init\_\_

model=model)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data\_adapter.py"", line 772, in \_\_init\_\_

peek, x = self.\_peek\_and\_restore(x)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data\_adapter.py"", line 830, in \_peek\_and\_restore

peek = next(x)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\keras\_preprocessing\\image\\[iterator.py](https://iterator.py)"", line 104, in \_\_next\_\_

return [self.next](https://self.next)(\*args, \*\*kwargs)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\keras\_preprocessing\\image\\[iterator.py](https://iterator.py)"", line 116, in next

return self.\_get\_batches\_of\_transformed\_samples(index\_array)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\keras\_preprocessing\\image\\[iterator.py](https://iterator.py)"", line 230, in \_get\_batches\_of\_transformed\_samples

interpolation=self.interpolation)

  File ""C:\\Users\\USER\\Anaconda2\\envs\\py3-TF2.0\\lib\\site-packages\\keras\_preprocessing\\image\\[utils.py](https://utils.py)"", line 108, in load\_img

raise ImportError('Could not import PIL.Image. '

ImportError: Could not import PIL.Image. The use of \`load\_img\` requires PIL.",1,tensorflow,2020-10-13
hvq71d,List of model deployment library model(Model Serving),,1,tensorflow,2020-10-13
hvo6cq,is it possible to transfer learn ai voice cloning to a different language ?,there are many neutral network in ai voice cloning is it possible to transfer learn this neutral network to a different language .,1,tensorflow,2020-10-13
hvkl04,Latest from Microsoft researchers: High-quality video inpainting!,,9,tensorflow,2020-10-13
hvd9yz,How can I define a layer using the Functional API that takes as input a tuple of instances?,"I'm trying to replicate a paper in which the input to the model is a tuple (or a list) of instances, and a batch would be a bunch of these tuples. I have used multi-input models in the past but each input would go to a different layer, however in this case the paper describes that all the inputs must go through the same LSTM layer. I have thought of passing them one by one but that would prevent the optimizer form propagating the loss back to all of the samples in the tuple.",2,tensorflow,2020-10-13
hvbzi7,warmstart of neural net model trained with parameter servers shows big drop in metrics,"Throughout my half decade working with neural network models, this question has been the most mysterious and at the same time highly practical. The basic setup is as follows: we train a neural network model with about 30 workers and 5 parameter servers. The input consists of about 100 numeric features and 10 sparse id features, each with 32 dimensional embeddings. The model architecture is DNN, and the task is binary classification in the context of search ranking. So this is fairly standard setup. It takes about 3m steps for the model to reach maximum auc on a held out test dataset (separated from training data by date). 

Now if we stop the training at that point and resume by means of warmstarting (there are two ways of doing that in Tensorflow: 1. simply restart the training job, 2. use the warm\_start\_from\_checkpoint RunConfig option), we observe precipitous drop in eval auc, on the order of 2-3 percentage points. Normal fluctuation of metrics between consecutive checkpoints is less than 0.5 percentage points. The training auc also sees similar amount of drop (we took out the window averaging of the training metrics which was misleading), suggesting something more fundamental than mere generalization problem. **Also this kind of drop is observed consistently.** The metric does recover a bit afterwards, but never fully recovers to the level of original training run (down by about 1 point).

We have tried various warmstart as well as the initial training settings, most notably the number of workers and parameter servers. If the initial training involves no parameter server or very few workers  (say 1-3) under parameter servers, then this drop in metric disappears or is hard to distinguish from normal fluctuation. However with 30 workers in initial training, nothing helps during warmstart. It is **also worth mentioning** that if the warmstart happens many steps before the maximum eval auc point (i.e., when the model ""converged""), say around 1m steps, then the drop is also much less pronounced, even with many workers.

Now I am a very careful ML person so I did stand-alone evaluation of the warmstart checkpoint and the metrics agree 100% with the ones reported on Tensorflow. So clearly nothing is wrong with the forward parameters of the model saved in the checkpoint. That leaves open whether the momentum and velocity parameters of the Adam optimizer are saved correctly. But inspecting the Tensorflow code, I see nothing wrong with the saving mechanism. The optimizer state parameters are treated the same way as forward pass parameters. We also did tf.Print on the optimizer parameters during warmstart and the printed values agree with the ones saved in the checkpoint (as reported by inspect\_checkpoint.py). 

We have also ruled out the effect of training data shuffling, as the same phenomenon occurs even when warmstarting using new training data.  

Another thing I have tried is to lower the learning rate by several orders of magnitude. But that merely delays the drop in terms of steps: eventually the same low metric is reached, even though at the very beginning sometimes I even see a slight uptick in auc, probably due to random luck. 

Same phenomenon is also observed for Adagrad, although we haven't tried SGD; the latter may behave differently since it's stateless. 

My current hypothesis is that the async nature parameter server based training manages to mitigate overfitting near eval ""convergence"", by letting the workers override one another, perhaps similar to the flooding technique mentioned in [this article](https://www.groundai.com/project/do-we-need-zero-training-loss-after-achieving-zero-training-error/). There are two ways to verify this hypothesis: (1). simulate single worker training when the 30-worker training reaches the point of ""convergence"". (2). Simulate parameter server multi-worker set up when a single worker (chief-only) training is near ""convergence"". (1) seems difficult to distinguish from single worker warmstart itself. (2) is actually an ideal experiment to run, but requires a lot of technical work, so I am not sure I am ready to put in that kind of energy and time.  

So this is a pretty complete sketch of the problem I am facing. I will be happy to fill in more details if they help. Also I am willing to post a $100 bounty for anyone who can convincingly and verifiably resolve the mystery: I can do the legwork as long as the idea seems promising.",1,tensorflow,2020-10-13
hvblf5,Help creating custom Object Detection in tensorflow,"Hi all, i am trying to create a custom object detection in tensorflow but when i execute the model\_main.py to start the training process i got stuck in this error:

*ImportError: cannot import name 'inception\_resnet\_v2' from 'nets'*

This are the environment specs:

\-Ubuntu 20.04

\-Tensorflow running on cpu (ryzen 2700) version 1.15

\-Python version 3.7

\-Using tensorflow models 1.13

\-verified tensorflow working with [https://github.com/Bengemon825/TF\_Object\_Detection2020/blob/master/scripts/updated\_old\_example.py](https://github.com/Bengemon825/TF_Object_Detection2020/blob/master/scripts/updated_old_example.py)

The error shows when I run *model\_main.py* and *train.py* in models/research/object\_detection directory.

If someone can help me I will be so pleased, as it is a high school work.

Thanks in advance.",2,tensorflow,2020-10-13
hva7rv,TF 2.0 XOR learning help!!!!!!,"import tensorflow as tf

import numpy as np

&amp;#x200B;

tf.enable\_eager\_execution()

tf.set\_random\_seed(777)

&amp;#x200B;

X = np.array(\[

\[0,0\],

\[0,1\],

\[1,0\],

\[1,1\]

\], dtype = np.float32

)

&amp;#x200B;

Y = np.array(\[

\[0\],

\[1\],

\[1\],

\[0\]\] ,dtype=np.float32

)

&amp;#x200B;

W1 = tf.Variable(tf.random\_normal(\[2,2\]))

b1 = tf.Variable(tf.random\_normal(\[1\]))

&amp;#x200B;

W2 = tf.Variable(tf.random\_normal(\[2,1\]))

b2 = tf.Variable(tf.random\_normal(\[1\]))

&amp;#x200B;

variables = \[W1,b1,W2,b2\]

&amp;#x200B;

lr = 0.1

&amp;#x200B;

for epoch in range(16000+1):

with tf.GradientTape() as tape:

g = tf.sigmoid(tf.matmul(X,W1)+b1)

hypothesis = tf.sigmoid(tf.matmul(g,W2)+b2)

predict = tf.cast(hypothesis&gt;0.5, dtype=tf.float32)

cost = -tf.reduce\_mean(Y \* tf.log(hypothesis) + (1 - Y) \* tf.log(1 - hypothesis))

optimizer = tf.train.GradientDescentOptimizer(lr)

&amp;#x200B;

grads = tape.gradient(cost, variables)

optimizer.apply\_gradients(grads\_and\_vars=zip(grads, variables))

if epoch%2000 == 0:

print(""epoch : %d   cost = %f  "" %(epoch, cost))

print(predict.numpy())

&amp;#x200B;

&amp;#x200B;

\----------------------------------------------

&amp;#x200B;

Why this code doesn't work?? I thought I did everything right.",1,tensorflow,2020-10-13
hv9bll,Any ideas for a Deep Learning Project?,"Im a 6th semester CS student and looking to do my FYP in Deep Learning with Computer Vision involved. I am really enthusiastic about:  
\- Space  
\- Microscopic Imaging   
\- Data Generation (i just wanna use GANs)  


Any Ideas would be appreciated.",12,tensorflow,2020-10-13
hv1yz1,Need to pass multiple images to a classifier. (Explained in detail below),"Here's what I'm doing -

- Trained an object detection model and passing the image to it.
- In the image, there are multiple objects detected. 
- Cropping ROI of object and passing to a Classifier.
- But the classifier takes a single object which makes the overall process slow.

 Instead of passing single image ROI to classifier , is it possible to pass all ROI's of previous image to classifier?

Give suggestions, even if I need to make or train something form scratch.",4,tensorflow,2020-10-13
hv0ann,match the profile pic of a user with their photo id,"I am building a angular 9 app - a JS framework, in which the user has to upload his photo while registration and then a photo id.

&amp;#x200B;

is there any JS AI or api to match the photo of the user with the photo available in his ID.",1,tensorflow,2020-10-13
huweov,[Question] How long did you wait for the TensorFlow certificate exam grades?,"Hi all, I took the certificate exam 2 or 3 days ago but still haven't received grade. Did you all get grades immediately after submitting the exam?",7,tensorflow,2020-10-13
huut5a,Complex meter reading with Tensorflow,"Hi guys.

Hope I'm not imposing here - I'm really new to machine learning but I have a strong background with software engineering, especially with JavaScript. This is why I want to use Tensorflow.

I want to build a solution that can take in images of analogue meters (digit based) and spit out a CSV containing the meter number (even just the last few digits) and the actual reading. I've tried passing some images through Google Cloud's Text Recognition demo and everything gets recognised well, I'm just not sure how to proceed.

&amp;#x200B;

https://preview.redd.it/0tgpnhbd43c51.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=f099312daff5b2b032bab384f7e712a760e1a0c6",2,tensorflow,2020-10-13
hur0vn,Can someone please explain this to me in brief? Need Help! (related to Real Time Object Detection &amp; Recognition),,0,tensorflow,2020-10-13
hun41d,[Question] I am running out of system memory during training of image classification problem.,"Sorry if I miss anything important to include, I am new to CS let alone anything to do with ML. I am curious if I just need more RAM because of my specs or if there is another issue that I'm not aware of. I'm using tensorflow 2.0/keras on a docker with multi-gpu support. I have 4xNvidia Geoforce RTX 2080 ti 11GB VRAM and 32GB of system RAM. I am using the inceptionResnetV2 model with about 1.2 million 120x120 images with mirrored strategy. The problem that I am running into is that my ""filling shuffle buffer"" step is using all my system memory. I have tried changing my batch sizes, lowering the image size, changing the strategy, reducing the number of GPUs being used and using smaller models. Any information is appreciated. 
PS. I am already looking into working on a VM that will negate this problem I just want to make sure there isn't something I am missing.",7,tensorflow,2020-10-13
humsmk,"John Snow Labs Spark-NLP 2.5.4: Supporting Apache Spark 2.3, 43 new models and 26 new languages, new RegexTokenizer, lots of new notebooks, and more!",,1,tensorflow,2020-10-13
hulwg1,Is it possbile to use Datasets with DataGenerators?,"I have a dataset and i am trying to create a datagenerator from it, but it seems like it is not working, because the data generator requires data and labels either seperate or from a directory.  


Is there still a way to do this? My dataset is quite big (large images) so i can't just load everything.",1,tensorflow,2020-10-13
huh07u,ECCV 2020: From Microsoft and UWashington researchers: Personalized Face Modeing!,,1,tensorflow,2020-10-13
hug5ia,"Back to Machine Learning Basics - Linear Regression with Python, SciKit Learn, TensorFlow and PyTorch | Rubik's Code",,3,tensorflow,2020-10-13
hucsh7,List of Audio and speech pre-trained model,"I found a list of pre-trained models for audio and speech. Instead of building a model from scratch, we can use a pre-trained model to build a computer vision application.

Github:- [https://github.com/balavenkatesh3322/audio-pretrained-model](https://github.com/balavenkatesh3322/audio-pretrained-model)",1,tensorflow,2020-10-13
hucljm,#ThankYouTensorFlow,"There has been a lot of tf.trolling in these past few years which didn't change much after the 2.0 release.

&amp;#x200B;

As a DeepLearning engineer I found TensorFlow to be a useful tool, a **big thanks** to all the developers and contributors who made tf possible.",34,tensorflow,2020-10-13
hu3ie1,Wide ResNet Implementation for CIFAR10,"Hi everyone! I'm looking for a Wide ResNet implementation in tensorflow (ideally tensorflow 2) that I can use to train a model for image classification on CIFAR10. If you have other model suggestions that perform close to state of the art, that would be great too! 

Thanks!",7,tensorflow,2020-10-13
htpi06,Please roast my image classifier code as it's really bad at predicting meals.,"I'm trying to classify foods with an Android app using a Tflite file I got from [here](https://tfhub.dev/google/lite-model/aiy/vision/classifier/food_V1/1).

My problem is my app is horrible at predicting foods. It's not the Tflite file that's the problem, if anything that file is excellent. It's my code that's the problem. For example, I would take a picture of a pizza and it would predict it's caramel with 8% confidence. So ya I need to make some improvements. This is the [code](https://drive.google.com/file/d/1_SDzkzG_uMFBNs0FQex56o05U8xOIWrR/view?usp=sharing). Please roast the shit out of it.",1,tensorflow,2020-10-13
htlrx4,how to use inputs with different sample counts with keras.Model?,,1,tensorflow,2020-10-13
htj22i,Just got this camera to use with Tensorflow!,,0,tensorflow,2020-10-13
hti58e,{HELP] AlreadyExistsError: Resource,"**Hello,**

**I am trying to train a  model like this.**

    sequence_input = Input(shape=(200,), dtype='int32')
    embedding_layer = Embedding(50000, 100, input_length=200)(sequence_input) 
    x = Bidirectional( LSTM( 64,recurrent_dropout=0.1) )(embedding_layer) 
    x = Dense(32)(x) x = Dropout(0.2)(x) y = Dense(5, activation=""sigmoid"")(x)
    model = Model(sequence_input, y)
    model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])
    model.fit(x_train, y_train, epochs = 4, batch_size=32, validation_data=(x_val,y_val))

**but I got this error  when I run** `model.fit`

    AlreadyExistsError:  Resource __per_step_0/gradient_tape/model/bidirectional/backward_lstm/while/model/bidirectional/backward_lstm/while_grad/body/_862/gradients/AddN_6/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE 	 [[{{node gradient_tape/model/bidirectional/backward_lstm/while/model/bidirectional/backward_lstm/while_grad/body/_862/gradients/AddN_6/tmp_var}}]] [Op:__inference_train_function_6589]  Function call stack: train_function

**I don't know why, but if I reduce the size of training set  the error disappear .**

**Thanks for your help**",0,tensorflow,2020-10-13
hthwz9,ROIs,"Hello, I have input ROIs(28x28x10) and I want to classify the labels(say Cardiac texture) based on CNN architecture which consists of 2 Con3D layers, 2FC layers, with a small kernel size(2x2x2), stride 2 with max pooling. Can you please give/write a code for me?",0,tensorflow,2020-10-13
hthvx2,TensorFlow Courses,,94,tensorflow,2020-10-13
ht0pdf,Rapid learning,"I want to learn and build a model for a particular object. I don’t have much time so I should learn fast.

•I learned a bit on coursera
•Finished DL course on Kaggle

I think I must learn data preparing, building optimum layer and printing out with mask. Which sources should I look?",2,tensorflow,2020-10-13
hszm2r,how to get high usage of GPU when I train the model. (Based on TensorFlow),"Hi, I'm a beginner, when I studied 3D-CNN, as for the problem, it was the usage of GPU. I found that it only used 1% of memory, and I had tried to improve the batch-size, but it seems not changed. Here are codes, I'm not good at it, any problems please point out, thank you.  


[https://github.com/Edenkut/3D-gan.git](https://github.com/Edenkut/3D-gan.git)

&amp;#x200B;",1,tensorflow,2020-10-13
hsz1k3,This Week in AI - Issue #26 | Rubik's Code,,5,tensorflow,2020-10-13
hsx8pe,Fashion Mnist,"I'm getting a value error trying to train my first neural network. I know this is a noob question. But I have less than a week of experience in machine learning.  I'm getting this error after running through all 5 of my epochs.

  **ValueError**: Data cardinality is ambiguous:   x sizes: 60000, 10000 Please provide data which shares the same first dimension. 

Below is the code(excluding import statements):

data = keras.datasets.fashion\_mnist

(train\_images, train\_labels), (test\_images, test\_labels) = data.load\_data()

class\_names = \['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',

'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\]

train\_images = train\_images/255.0

test\_images = train\_images/255.0

model = keras.Sequential(\[

keras.layers.Flatten(input\_shape = (28,28)),

keras.layers.Dense(128, activation=""relu""),

keras.layers.Dense(10, activation = ""softmax"")

\])

model.compile(optimizer=""adam"", loss=""sparse\_categorical\_crossentropy"", metrics=\[""accuracy""\])

[model.fit](https://model.fit)(train\_images, train\_labels, epochs=5)

test\_loss, test\_acc = model.evaluate(test\_images, test\_labels)

print(""Tested accuracy: "", test\_acc)",4,tensorflow,2020-10-13
hswm44,Low accuracy on Fashion MNIST dataset,"Hello, I'm new to tensorflow ( I started 3 days ago) and after installing it with cuda, today I made my first neural network.

I follow an example and made [this](https://pastebin.com/vR8Brk4r).

My problem is when I train my model, the max accuracy I get is 10% (0.1). I didn't normalize the dataset but, in the example, without normalization, he get 87% accuracy.

I thought maybe that 0.1 mean 100% but I'm not sure.

Can someone explain why is my max accuracy is 10% and how to correct it ?

Thank you",4,tensorflow,2020-10-13
hssumy,Object detection in tensorflow,Does someone know how to make object detection in tensorflow,2,tensorflow,2020-10-13
hsos1w,The latest in image-to-image translation!,,0,tensorflow,2020-10-13
hsnsgt,Trying to build Encoder-Decoder on TF,"I am currently trying to build a system similar to this paper: [https://arxiv.org/pdf/1712.03991.pdf](https://arxiv.org/pdf/1712.03991.pdf), but in TensorFlow instead of CUDA &amp; Theano. Most of the seq2seq models I have seen in Google tutorials are their approaches to build a neural machine translation, and I have tried but still can't load the data that I want into this system, since it's in different dimension. If anyone has experience with seq2seq model in TF, can I DM you for some questions?",1,tensorflow,2020-10-13
hsksr2,Does tensorflow placeholders work with multi-typed dataframe,"I am hitting an error with tensorflow's protobuf 2gb limit, so I was trying to use placeholders for my dataframe. Is it possible to have my dataframe with ints, floats, strings from different columns be represented in a placeholder and given to `from_tensor_slices` function?",8,tensorflow,2020-10-13
hsjfeo,Image Classification tutorial for newbies that don't use large datasets?,"Hi,

Apologies if this isn't the proper forum for the question. I'm new to TF and ML in general. I'm trying to find a tutorial for someone new that classifies images of small datasets leveraging augmentation. Basically, my first goal is to get this to identify if anyone in my house is in a picture.  
  
Right now, I have a really small datasets. 4 classes (ie members of household). Each class has 10 images in the train folder and 3 for the validation folder. My assumption is that I can use augmentation here. At least for the purposes of learning. I do understand anything professional grade is probably want 1000+ training images. Everything I find uses the msnist sets or whatever. I'm really trying to learn from the ground up because I feel like that will help me learn better.  
  
As an aside, I have no idea if I have this set up properly on win10 to leverage my gpu (rtx 2060super).",0,tensorflow,2020-10-13
hs9to2,Quantized TensorFlow2.0 model increases prediction time,"I am using Python 3.8 and TensorFlow 2.2 to train LeNet-300-100 Dense neural network on MNIST dataset using a sparse network with about 91.3% sparsity.

&amp;#x200B;

I am quantizing the neural network from FP32 down to FP16 using the following codes. Here 'winning\_ticket\_model' is a LeNet-300-100 model trained until convergence.

&amp;#x200B;

        # Initialize a new model-
        quantized_model = lenet_nn()
        
        # Load trained winning ticket weights-
        quantized_model.set_weights(winning_ticket_model.get_weights())
    
        # Python3 list to hold FP16 weights-
        fp16_wts = []
        
        # Iterate through each layer converting weights from FP32 to FP16-
        for layer in quantized_model.trainable_weights:
            # print(layer.shape)
            fp16_wts.append(np.float16(layer.numpy()))
        
        def lenet_nn_quantized():
            """"""
            Function to define the architecture of a neural network model
            following 300 100 Dense Fully-Connected architecture for MNIST
            dataset.
            Uses Quantization by: Floating-Point 16 bits instead of FP32!
            
            Output: Returns designed and compiled neural network model
            """"""
            
            model = Sequential()
            model.add(InputLayer(input_shape=(784, )))
            
            model.add(
                Dense(
                    units = 300, activation='relu',
                    kernel_initializer=tf.initializers.GlorotUniform(),
                    dtype = tf.float16
                )
            )
        
            # model.add(l.Dropout(0.2))
        
            model.add(
                Dense(
                    units = 100, activation='relu',
                    kernel_initializer=tf.initializers.GlorotUniform(),
                    dtype = tf.float16
                )
            )
                
            # model.add(l.Dropout(0.1))
        
            model.add(
                Dense(
                    units = num_classes, activation='softmax',
                    dtype = tf.float16
                )
            )
            
        
            # Compile pruned NN-
            model.compile(
                loss=tf.keras.losses.categorical_crossentropy,
                # optimizer='adam',
                optimizer=tf.keras.optimizers.Adam(lr = 0.0012),
                metrics=['accuracy'])
            
            return model
        
        # Initialize a new Quantized LeNet-300-100 model-
        quantized_model2 = lenet_nn_quantized()
        
        # Load FP16 weights-
        quantized_model2.set_weights(fp16_wts)
        
        # Sanity check-
        for layer in quantized_model2.trainable_weights:
            print(""layer.shape = {0}, dtype = {1}"".format(layer.shape, layer.numpy().dtype))
        '''
        layer.shape = (784, 300), dtype = float16
        layer.shape = (300,), dtype = float16
        layer.shape = (300, 100), dtype = float16
        layer.shape = (100,), dtype = float16
        layer.shape = (100, 10), dtype = float16
        layer.shape = (10,), dtype = float16
        '''
        start = time.time()
        
        # Make predictions using Original model-
        y_pred = winning_ticket_model.predict_classes(X_test)
        # y_pred = winning_ticket_model.predict(X_test)
        
        end = time.time()
        
        print(""\nTotal time taken = {0:.4f}"".format(end - start))
        # Total time taken = 0.4640
        
        start = time.time()
        
        # Make predictions using Quantized model-
        y_pred_quantized = quantized_model2.predict_classes(X_test)
        # y_pred_quantized = quantized_model2.predict(X_test)
        
        end = time.time()
        
        print(""\nTotal time taken = {0:.4f}"".format(end - start))
        # Total time taken = 33.7782
    
    

My question is: why does the Quantized version of trained model (FP16 bits) take 33.7782 as opposed to 0.4640 for Original trained model (FP32 bits)? According to my understanding, due to reduced precision, shouldn't the predictions happen faster or equal to original trained model (FP32 bits)?

&amp;#x200B;

Thanks!",5,tensorflow,2020-10-13
hs7yqp,Audio feature extraction pipeline,"In my new video, you can learn what are the necessary steps to extract acoustic features from audio signals, both in the time and frequency domains. I also explain key audio processing concepts like spectral leakage, windowing, frames and hop length. 

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=8A-W1xk7qs8&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=6](https://www.youtube.com/watch?v=8A-W1xk7qs8&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=6)",14,tensorflow,2020-10-13
hry679,Installing on Windows 10/Python 3.8,"Tensorflow now supports Python 3.8, so I installed Pyhton 3.8 64-bit (from python.org). I also have pip 20.1.1. However, when I try to install tensorflow I get the infamous ""No matching distribution found for tensorflow"" error. I get this from both ""pip install tensorflow"" and ""pip install &lt;&lt;url-to-wheel&gt;&gt;"" using the URL on the tensorflow site specifically for the Windows/Python 3.8 WHL.  Has anybody managed to get this to work?  What am I missing?",5,tensorflow,2020-10-13
hrrkiy,Coral Board or Google Vision Kit?,"Hi r/tensorflow!

I’ve been working on a video object recognition side project using the Google Vision Kit as I already had a Pi Zero. 

I was making some progress but having to go through transferring of files, compiling in a Linux environment for some steps, and completely failing to get bounding boxes to work due to some weird Mac related file naming issues was really killing my motivation. 

I’ve been looking at the Coral board and from what I can tell it seems to be able to manage anything I need to do with tensorflow models without needing to transfer files and compile in different environments. Is this true? 

Thanks in advance for any feedback!",1,tensorflow,2020-10-13
hrnojl,Image Classification vs Anomaly detection,"Hello! In my last post here I asked about a question for my Project. Turns out I had a bad understanding about the machine and I should not use video classification. Anyway to summarize it I have a machine where I get an image. I have to say from the image if the test was successful.

My first thoughts where that I should use image classification, but the test can fail in many different ways so getting a dataset of all different fail variants would be tough. 

Then I read about Anomaly detection. It seems like the right choice but the problem is that the result image of the test is mostly the same as the fail image, only slight changes can be seen. I can’t use the color scheme because it would be the same always.

So my question would be what can I use to differentiate the anomalies? 

And what algorithm should I use? Isolation Forest seems like a good idea?

From my understanding I should use Novelty detection, because I don’t have all the anomalies in the dataset. I am wrong? Or should I use Outlier detection?

And lastly should I even use Anomaly detection or just try to getter the dataset for image classification?

Thanks for any reply!",8,tensorflow,2020-10-13
hrayiv,Epidemic Exposure Notification with Smartwatch using Proximity Based Privacy Preserving Approach.,,1,tensorflow,2020-10-13
hr8c22,Help with Installation for use in Jupyter notebook using Keras,"Help with Installation steps to make tensorflow work with Keras in Jupyter notebook. I have tried everything online mentioned in stacked-overflow and elsewhere but I can’t figure out how to make it work. One last thing, the kernel dies while running tensorflow. So does anyone have a working solution ?",2,tensorflow,2020-10-13
hr6sfl,Huge flaw in Keras/Tensorflow resulting in compromised training? Using custom layers with the functional API results in missing weights in the `trainable_variables`.,"tl'dr: For anyone who has used the functional api with custom layers, it might be worth running


    for i, var in enumerate(model.trainable_weights):
        print(model.trainable_weights[i].name)
    

so see if all your weights are there. 

----

I am wondering if this is a huge bug/flaw in Keras; using custom layers with the functional API results in missing weights in the `trainable_variables`. Those weights are not in the 'non_trainable_variables` either. 

But if those weights aren't in `trainable_variables`they are essential frozen, since it is only those weights that receive gradient updates. 

The bug can be seen in this Colab gist 

https://colab.research.google.com/gist/Santosh-Gupta/766a27c1500a330cba6f479805dad27d/missingtrainablevarsinference.ipynb

This gist uses the transformers library to create the models so its easy to see the bug. For an in depth look, the colab gist below creates all the custom layers from scratch

 https://colab.research.google.com/gist/Santosh-Gupta/ec9f7c8a189a8d99e73d96fbe728aef8/model_weight_debug_scratch_public_inference.ipynb


As you can see in the notebooks, a workaround is to create models using keras subclassing to create the models instead; model subclassing results in all the weights appearing in `trainable_variables`. To be absolutely sure that the functional API and subclasses models are the same, I ran inference on them using the same input; the inference was exactly the same. 

However, I have models that I already trained with the functional API. I've been looking at this for about a month, as far as I can tell, any Keras model using custom sublayers and the functional API is prone to this. And I am wondering if any other the models I trained with custom layers and the functional API that may have compromised training

I put up a Github issue 24 days ago, but I can't tell if this is something being worked on. 

https://github.com/tensorflow/tensorflow/issues/40638

I have some deadlines coming up with the models I have trained, so I am intensely curious about the nature of this issue, so that I can report my results accurately and I am wondering if there are any places where I may be able to get info while in the Github issues queue.",4,tensorflow,2020-10-13
hr1vq0,"How to Implement YOLOv4 Object Detection with TensorFlow, TFLite, and TensorRT Models",,46,tensorflow,2020-10-13
hr18h3,A project I built using TensorFlow JS,,1,tensorflow,2020-10-13
hr0hj8,Has anyone done reinforcement learning with tensorflow.js or ml5?,,0,tensorflow,2020-10-13
hqz3gq,Introducing the `PathManager` module,"This is a little project I created while working with machine learning datasets,  I find it hideous to write or copy the path of my datasets folder again and again, and what if I need a subdirectory? or a sub-sub directory? it just gets messy and unreadable.

So in light of this, I decided to create [PathManager](https://github.com/tomergt45/PathManager), an efficient, easy, and convenient way to manage and access your local paths in python, it saves your paths in a local file which is accessible from all other projects, you can also save shortcuts to subdirectories with extreme ease!

Example of the syntax:

    from pathmanager import paths
    
    # Save the path to your datasets folder
    paths['datasets'] = 'desktop/projects/datasets'
    
    # Save a shortcut to a ""dogs"" subdirectory which is located inside an 'animals' subdirectory of the original path
    paths['datasets', 'dogs'] = 'animals/dogs'

and from an entirely different project:

    from pathmanager import paths
    
    print(paths['datasets'])
    # desktop/projects/datasets
    
    print(paths['datasets', 'dogs'])
    # desktop/projects/datasets/animals/dogs

This is just a simple example, but it shows how convenient using this package can be.

To install the package: `pip install path-manager`

To check out the GitHub page: [https://github.com/tomergt45/PathManager](https://github.com/tomergt45/PathManager)

I'll be really happy to hear some feedback from you guys! if you think I should add/change things feel free to comment below or issue a pull request on GitHub.",2,tensorflow,2020-10-13
hqo89m,Model Zoo: Deploy your TensorFlow model from Python with a single line of code.,,6,tensorflow,2020-10-13
hql2k8,Digit Detector for SVHN Data Set,"Been attempting to find a digit detector to use on this common dataset, but everything I've found has been for older versions of python, tf, keras, and/or opencv alike.

[https://github.com/penny4860/SVHN-deep-digit-detector](https://github.com/penny4860/SVHN-deep-digit-detector)

[https://github.com/penny4860/Yolo-digit-detector](https://github.com/penny4860/Yolo-digit-detector)

[https://github.com/penny4860/tf2-eager-yolo3](https://github.com/penny4860/tf2-eager-yolo3)

[https://github.com/penny4860/retinanet-digit-detector](https://github.com/penny4860/retinanet-digit-detector)

Does anyone know of anything else or tricks to get any of these libraries to work properly?",1,tensorflow,2020-10-13
hqd65b,Audio features for machine learning,"In my new video, you’ll learn to distinguish among different types of audio features which are instrumental to build intelligent audio applications. I introduce time domain, frequency domain, and time-frequency domain features. I explain how we can categorise audio features based on their level of abstraction, ML approach adopted and temporal scope.  

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=ZZ9u1vUtcIA&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=5](https://www.youtube.com/watch?v=ZZ9u1vUtcIA&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=5)",5,tensorflow,2020-10-13
hqd0qd,[Megathread] TensorFlow certification exam,"A while back I made a post requesting the admins of this sub to make a mega thread about the certification exam, but unfortunately, they didn't do it so I decided I would make one myself, you can find my original request [here](https://www.reddit.com/r/tensorflow/comments/hhl4co/suggestion_lets_make_a_mega_thread_about_the/).

# Can the exam be taken the second you pay?

Yes, after you paid for the exam and your ID has been verified you are eligible to start whenever you want, it can be at that moment, a week later, or even a month.

# Where do you take the exam?

The exam can be taken from your home computer and is administered via a PyCharm plugin, an explanation on how to set it up can be found [here](https://utility.trueability.com/google/tensor-flow/Instructions_for_taking_the_TensorFlow_Certificate_exam.pdf) (on page 4).

# What is the process of the exam?

The exam is divided into 5 questions, each is harder and worth more points than the previous one, each question requires you to build and train a different model. **After you finish each question you are required to submit your answer otherwise it wouldn't count!** you can find an explanation on how to do it [here](https://utility.trueability.com/google/tensor-flow/Instructions_for_taking_the_TensorFlow_Certificate_exam.pdf) (on page 15).

After submitting an answer you receive a score between 1-5, you can resubmit your answer but note that **ONLY YOUR LAST SUBMISSION IS TAKEN INTO ACCOUNT**.

# What if my computer is not strong enough?

If you can’t train models in a reasonable amount of time (I.e. hours to train a single model) then you should consider using google Colab for the training part, the exam only requires you to have a trained model, it doesn’t matter where it has been trained.

# How much time does it take to get your results?

Google's official response is ""up to 2 weeks"" but I personally received my results immediately after finishing the exam.

# Can I take the exam again if I fail?

Google's official statement is:

* If you don't pass on your first attempt, you must wait 14 days before purchasing and taking the exam again.
* If you don't pass on your second attempt, you can purchase and retake the exam after a two month waiting period.
* If after three attempts you still have not passed the exam, you must wait one year.

# How do I study for the exam?

1. [TensorFlow in Practice specialization](https://www.coursera.org/specializations/tensorflow-in-practice) \- Highly recommended, should teach you everything you need.

Got more questions? think I forgot something? comment below and I would add it to this thread!",64,tensorflow,2020-10-13
hqc06y,Quantization using Sparse Neural Networks,"I was reading and coding [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635) research paper where you get smaller sub-networks, where sparsity is maintained using masks (0 and 1). I have coded these experiments in [GitHub](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2) and was wondering whether it is possible to quantize the resulting sub-networks achieved using ""The Lottery Ticket Hypothesis""?

I am using TensorFlow 2.X and Python 3.8. [TF Quantization Aware Training](https://www.tensorflow.org/model_optimization/guide/quantization/training) doesn't talk about maintaining sparsity of the sub-network.

Help?!",4,tensorflow,2020-10-13
hqbdhq,Quantization Aware Training with tf.GradientTape gives Error in TensorFlow2.0,"I am using TensorFlow-2.2, tensorflow\_model\_optimization and Python 3.8. I am trying to quantize and train a LeNet-300-100 Dense neural network which contains sparsity of 91.3375%. This means that 91.3375% of the weights are zero. I was following the \[Quantization TF tutorial\]\[1\] and I wanted to train such a sparse network which has been quantized using \*tf.GradientTape\* rather than \*q\_aware\_model.fit()\*.

&amp;#x200B;

If you look into the \[example code\]\[2\], the relevant code snippets are:

&amp;#x200B;

&amp;#x200B;

    quantize_model = tfmot.quantization.keras.quantize_model
    
    # q_aware stands for for quantization aware.
    q_aware_model = quantize_model(model)
    
    # 'quantize_model' requires recompilation-
    q_aware_model.compile(
    
    optimizer = tf.keras.optimizers.Adam(lr = 0.0012),
    
    loss=tf.keras.losses.categorical_crossentropy,
    metrics=['accuracy']
    )
    
    # Define 'train_one_step()' and 'test_step()' functions here-
    @tf.function
    def train_one_step(model, mask_model, optimizer, x, y):
        '''
        Function to compute one step of gradient descent optimization
        '''
        with tf.GradientTape() as tape:
            # Make predictions using defined model-
            y_pred = model(x)
            
            # Compute loss-
            loss = loss_fn(y, y_pred)
    
        # Compute gradients wrt defined loss and weights and biases-
        grads = tape.gradient(loss, model.trainable_variables)
    
        # type(grads)
        # list
    
        # List to hold element-wise multiplication between-
        # computed gradient and masks-
        grad_mask_mul = []
    
        # Perform element-wise multiplication between computed gradients and masks-
        for grad_layer, mask in zip(grads, mask_model.trainable_weights):
            grad_mask_mul.append(tf.math.multiply(grad_layer, mask))
        
        # Apply computed gradients to model's weights and biases-
        optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))
    
        # Compute accuracy-
        train_loss(loss)
        train_accuracy(y, y_pred)
    
        return None
    
    
    @tf.function
    def test_step(model, optimizer, data, labels):
        """"""
        Function to test model performance
        on testing dataset
        """"""
        predictions = model(data)
        t_loss = loss_fn(labels, predictions)
        test_loss(t_loss)
        test_accuracy(labels, predictions)
        return None
    
    
    # Train model using 'GradientTape'-
    # Initialize parameters for Early Stopping manual implementation-
    # best_val_loss = 100
    # loc_patience = 0
    for epoch in range(num_epochs):
        if loc_patience &gt;= patience:
            print(""\n'EarlyStopping' called!\n"")
            break
    
        # Reset the metrics at the start of the next epoch
        train_loss.reset_states()
        train_accuracy.reset_states()
        test_loss.reset_states()
        test_accuracy.reset_states()
    
        for x, y in train_dataset:
            train_one_step(q_aware_model, mask_model, optimizer, x, y)
        
        for x_t, y_t in test_dataset:
            test_step(q_aware_model, optimizer, x_t, y_t)
    
        template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f},         Test Accuracy: {4:4f}'
    
        print(template.format(epoch + 1, train_loss.result(),train_accuracy.result()*100, test_loss.result(),test_accuracy.result()*100))
    
        # Count number of non-zero parameters in each layer and in total-
        # print(""layer-wise manner model, number of nonzero parameters in each layer are: \n"")
        model_sum_params = 0
        for layer in winning_ticket_model.trainable_weights:
            # print(tf.math.count_nonzero(layer, axis = None).numpy())
            model_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()
    
        print(""Total number of trainable parameters = {0}\n"".format(model_sum_params))
    
    
        # Code for manual Early Stopping:
        if np.abs(test_loss.result() &lt; best_val_loss) &gt;= minimum_delta:
            # update 'best_val_loss' variable to lowest loss encountered so far-
            best_val_loss = test_loss.result()
    
            # reset 'loc_patience' variable-
            loc_patience = 0
        else:  # there is no improvement in monitored metric 'val_loss'
            loc_patience += 1  # number of epochs without any improvement

Gives the following error:

&amp;#x200B;

&gt;\&gt; --------------------------------------------------------------------------- InvalidArgumentError                      Traceback (most recent call  
&gt;  
&gt;\&gt; last) &lt;ipython-input-47-bca851ce138d&gt; in &lt;module&gt;  
&gt;  
&gt;\&gt;      19  
&gt;  
&gt;\&gt;      20     for x, y in train\_dataset:  
&gt;  
&gt;\&gt; ---&gt; 21         train\_one\_step(q\_aware\_model, mask\_model, optimizer, x, y)  
&gt;  
&gt;\&gt;      22  
&gt;  
&gt;\&gt;      23  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_\_call\_\_(self, \*args, \*\*kwds)  
&gt;  
&gt;\&gt;     578         xla\_context.Exit()  
&gt;  
&gt;\&gt;     579     else:  
&gt;  
&gt;\&gt; --&gt; 580       result = self.\_call(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     581  
&gt;  
&gt;\&gt;     582     if tracing\_count == self.\_get\_tracing\_count():  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_call(self, \*args, \*\*kwds)  
&gt;  
&gt;\&gt;     642         # Lifting succeeded, so variables are initialized and we can run the  
&gt;  
&gt;\&gt;     643         # stateless function.  
&gt;  
&gt;\&gt; --&gt; 644         return self.\_stateless\_fn(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     645     else:  
&gt;  
&gt;\&gt;     646       canon\_args, canon\_kwds = \\  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_\_call\_\_(self, \*args, \*\*kwargs)    2418     with self.\_lock:  
&gt;  
&gt;\&gt; 2419       graph\_function, args, kwargs =  
&gt;  
&gt;\&gt; self.\_maybe\_define\_function(args, kwargs)  
&gt;  
&gt;\&gt; -&gt; 2420     return graph\_function.\_filtered\_call(args, kwargs)  # pylint: disable=protected-access    2421     2422   u/property  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_filtered\_call(self, args, kwargs)    1659       \`args\` and  
&gt;  
&gt;\&gt; \`kwargs\`.    1660     """"""  
&gt;  
&gt;\&gt; -&gt; 1661     return self.\_call\_flat(    1662         (t for t in nest.flatten((args, kwargs), expand\_composites=True)    1663  
&gt;  
&gt;\&gt; if isinstance(t, (ops.Tensor,  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_call\_flat(self, args, captured\_inputs, cancellation\_manager)  
&gt;  
&gt;\&gt; 1743         and executing\_eagerly):    1744       # No tape is  
&gt;  
&gt;\&gt; watching; skip to running the function.  
&gt;  
&gt;\&gt; -&gt; 1745       return self.\_build\_call\_outputs(self.\_inference\_function.call(    1746  
&gt;  
&gt;\&gt; ctx, args, cancellation\_manager=cancellation\_manager))    1747  
&gt;  
&gt;\&gt; forward\_backward = self.\_select\_forward\_and\_backward\_functions(  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in call(self, ctx, args, cancellation\_manager)  
&gt;  
&gt;\&gt;     591       with \_InterpolateFunctionError(self):  
&gt;  
&gt;\&gt;     592         if cancellation\_manager is None:  
&gt;  
&gt;\&gt; --&gt; 593           outputs = execute.execute(  
&gt;  
&gt;\&gt;     594               str([self.signature.name](https://self.signature.name)),  
&gt;  
&gt;\&gt;     595               num\_outputs=self.\_num\_outputs,  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py  
&gt;  
&gt;\&gt; in quick\_execute(op\_name, num\_outputs, inputs, attrs, ctx, name)  
&gt;  
&gt;\&gt;      57   try:  
&gt;  
&gt;\&gt;      58     ctx.ensure\_initialized()  
&gt;  
&gt;\&gt; ---&gt; 59     tensors = pywrap\_tfe.TFE\_Py\_Execute(ctx.\_handle, device\_name, op\_name,  
&gt;  
&gt;\&gt;      60                                         inputs, attrs, num\_outputs)  
&gt;  
&gt;\&gt;      61   except core.\_NotOkStatusException as e:  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; InvalidArgumentError:  var and grad do not have the same shape\[10\]  
&gt;  
&gt;\&gt; \[100,10\] 	 \[\[node Adam/Adam/update\_4/ResourceApplyAdam (defined at  
&gt;  
&gt;\&gt; &lt;ipython-input-37-9c297d161e54&gt;:29) \]\]  
&gt;  
&gt;\&gt; \[Op:\_\_inference\_train\_one\_step\_20360\]  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; Errors may have originated from an input operation. Input Source  
&gt;  
&gt;\&gt; operations connected to node Adam/Adam/update\_4/ResourceApplyAdam:  
&gt;  
&gt;\&gt; Mul\_4 (defined at &lt;ipython-input-37-9c297d161e54&gt;:26)  
&gt;  
&gt;\&gt; sequential/quant\_dense\_2/BiasAdd/ReadVariableOp/resource (defined at  
&gt;  
&gt;\&gt; /home/arjun/.local/lib/python3.8/site-packages/tensorflow\_model\_optimization/python/core/quantization/keras/quantize\_wrapper.py:162)  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; Function call stack: train\_one\_step

Is there a way to combine TF model Quantization along with tf.GradientTape?

&amp;#x200B;

Thanks!

&amp;#x200B;

&amp;#x200B;

\[1\]: [https://www.tensorflow.org/model\_optimization/guide/quantization/training\_example](https://www.tensorflow.org/model_optimization/guide/quantization/training_example)

\[2\]: [https://github.com/arjun-majumdar/Lottery\_Ticket\_Hypothesis-TensorFlow\_2/blob/master/Quantization\_LTH\_LeNet\_300\_100\_MNIST.ipynb](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Quantization_LTH_LeNet_300_100_MNIST.ipynb)",3,tensorflow,2020-10-13
hq91zb,How do I replace the tflite files in Tensorflow's image classification example for Android with my own tflite?,I cloned the [Tensorflow's Android image classification example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android) and I want to now use my own tflite that I have downloaded from [here](https://tfhub.dev/google/lite-model/aiy/vision/classifier/food_V1/1) to classify food. How do I replace the default tflite files with my own tflite? I only have one tflite whereas in the Android example there are a couple tflite files so I'm not sure on how I should go about this.,2,tensorflow,2020-10-13
hq0zbu,Upgrade windows? Or buy a new pc?,"I am having trouble installing tensorflow due to only having windows home therefore not being able to enable long paths, should i upgrade my current system to windows enterprise or should i buy a dedicated linux workstation? i have never done tensorflow/ML

(I have no experience in ML)",0,tensorflow,2020-10-13
hpzci1,Why is my Keras Model hdf5 failing to convert to a Tflite?," I ran this piece of code in Google Colab to convert my Keras hdf5 file to a Tflite file: 

    import tensorflow as tf
    keras_model = tf.keras.models.load_model(""/content/best_model_11class.hdf5"")
    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
    tfmodel = converter.convert()
    open ('model.tflite' , ""wb"") .write(tfmodel)

I keep getting this error when I run the code:

    ValueError                                Traceback (most recent call last)
    &lt;ipython-input-2-0804f3b57a48&gt; in &lt;module&gt;()
          2 keras_model = tf.keras.models.load_model(""/content/best_model_11class.hdf5"")
          3 converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
    ----&gt; 4 tfmodel = converter.convert()
          5 open ('model.tflite' , ""wb"") .write(tfmodel)
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
        481               ""None is only supported in the 1st dimension. Tensor '{0}' has ""
        482               ""invalid shape '{1}'."".format(
    --&gt; 483                   _get_tensor_name(tensor), shape_list))
        484         elif shape_list and shape_list[0] is None:
        485           # Set the batch size to 1 if undefined.
    
    ValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.

 Can someone tell me how do I fix this?",1,tensorflow,2020-10-13
hpxhog,I am trying to learn Tensorflow. I created a convolutional autoencoder sample project that tries to learn basic image emboss effect. But I'm not sure I did it in the right way. Your comments can help.,,2,tensorflow,2020-10-13
hpubez,YSK: You need to submit each question during the TensorFlow certification exam!,"I just finished my certification and during the exam, there was mentioned a ""test and submit"" button but I couldn't find it so I thought it referenced the ""end exam"" button, which was not the case!

after I finished all the questions I pressed ""end exam"" which means none of my questions were graded because of that! I currently try to contact the support team with the hope that they could help.

So, **just make sure you are submitting each question and getting a grade separately!**

I hate my life.

EDIT: After being asked I’ll clarify on how to find the button: close all tabs of opened scripts if you have any, open the assistant panel on the right and then open one of the scripts, it should show up now on the assistant tab with other text about the question.",35,tensorflow,2020-10-13
hpsqgq,Maintaining network sparsity using fit() method,"Hey guys, I have been reading research papers such as The Lottery Ticket Hypothesis (Frankle et. al) where you create a sparse sub-network. Say the model you get has 85% sparsity which means 85% of the parameters are 0!

Now if I want to retrain this model, I use tf.GradientTape() method where I use masks to do an element-wise multiplication between masks and parameters to control the number of non-zero parameters from growing during training.

Is there a way to do something similar using TF2.0's fit () method where you can provide the mask and it does the rest ?

Thanks!",1,tensorflow,2020-10-13
hpbg7e,Anyone know how to output contents of bounding box when using TF Object Detection API??,"I'm using the TF Object Detection API which returns my image with a bounding box around given objects.

How can I output just the contents of the box(es) to feed into a new layer of my pipeline??

Would really appreciate any help!",1,tensorflow,2020-10-13
hp9vhl,Help error 'tensorflow.contrib' not found in tensorflow V- 1.15.0 How to fix it.?,,1,tensorflow,2020-10-13
hp7bzp,How Do I get all trainable parameters as a one dimensional array?,"Lets say I have an arbitrarily complex model with CNN, RNN and fully connected layers. What Im looking for is a way to get all the trainable parameters in the network in a single one dimensional array. And also set a same size array as the parameters of the network. Is there any api support for that? if not then whats the cleanest way to do that?",1,tensorflow,2020-10-13
hp6hdh,Tensorflow 2 &amp; Object detection API,,35,tensorflow,2020-10-13
hp07rk,[QUESTION] Why can't I load my tflite?,"I'm using Google Colab to load a tflite file. I ran this line of code to prove that I have the tflite file saved on my Google drive:

    !ls ""/content/drive/My Drive/Colab Notebooks""

And the output I get is: Predictions.ipynb  model.tflite

So when I run this block of code I get an error on the load\_model line:

    import tensorflow as tf
    from tensorflow.keras.models import load_model, Model
    from tensorflow.python.keras import backend as K
    
    sess = tf.compat.v1.Session()
    K.set_session(sess)
    model = load_model('/content/drive/My Drive/Colab Notebooks/model.tflite')

The error is: 

    OSError: SavedModel file does not exist at: /content/drive/My Drive/Colab Notebooks/model.tflite/{saved_model.pbtxt|saved_model.pb}

I don't understand, I have the file correctly saved so why is it saying it doesn't exist?",1,tensorflow,2020-10-13
howkty,Latest from Purdue and Chicago researchers: Low-Power Object Counting!,,9,tensorflow,2020-10-13
houth0,"Help with compatible CUDA, cuDNN, and Tensorflow versions","Which versions CUDA, cuDNN, and Tensorflow are compatible? I found [this](https://www.tensorflow.org/install/source_windows) page with a chart for GPU but i do not know if that only applies if you build form source.

&amp;#x200B;

Edit: I figured out that when you install tensorflow with conda, it automatically installs the cuda and cudnn dependencies with the correct versions in the virtual environment. ",2,tensorflow,2020-10-13
hon1rk,[QUESTION] Output hidden state of LSTM RNN in tensorflow.js,I have posted my [question on stackoverflow](https://stackoverflow.com/questions/62832748/output-hidden-state-of-lstm-rnn-in-tensorflow-js) and don't want to spam the internet by repeating myself. Any answer (here or on stackoverflow) would be greatly appreciated.,1,tensorflow,2020-10-13
hoki3t,This Week in AI - Issue #25 | Rubik's Code,,2,tensorflow,2020-10-13
hois7k,What should be included in the training dataset?,"I'm pretty new to coding and TensorFlow so I've been doing a lot of learning as I go. I was wondering what I can do to improve my code/logic. Sorry if I'm rambling/unclear.

I'm programming a model to play a simple two-player strategy game (like tic-tac-toe, connect four etc.)

 It inputs the game state (a view of the board where each spot is either empty = 0, Player1 = 1, and Player2 = 2 ) and outputs an action. 

For the first generation, I ran a set of games of Player1 vs Player2 both taking random moves. I took all the games that Player1 won (about 50% of the games) and used each observation of the game state paired with whatever action Player1 took. That became my training data.

Then I created and trained the model from this data. I evaluated it by setting it as player1 and had it play against a player that takes random moves. It ended up winning about \~70% of the games which is pretty promising. 

Next, I wanted to create further generations of this model - so I ran a set of games of Player1 vs Player2 both utilizing this model. (When it was player2's turn I changed the observation of the game state to switch all the 1's and 2's) I took all the games that Player1 won and appended this new data to the previous training set.

Again, I created and trained a model from this combined dataset. I evaluated it by having it play against the previous model. It ended up winning literally 100% of the games which doesn't seem right to me. I checked what would happen if it played against a random move player and it won \~80% of the games.

Now here's where I'm unsure of what to do:

Is the training set bad?

Should the training set have included the latest model vs random moves? When I create more generations should it include the best model vs some of the previous generations of the model?

Should I not have combined the past training data with the new training data?",1,tensorflow,2020-10-13
hoa8nz,Latest from Adobe and UC Berkeley researchers: State of the art in deep image manipulation.,,13,tensorflow,2020-10-13
ho5bdk,Model not sensitive to input variations,"Hi, everyone.

I've a simple model that uses 2D convolutions (input data is treated as images) to predict continuous data (y is temperature data), given as follows:

https://preview.redd.it/fxycfvx8qu951.png?width=513&amp;format=png&amp;auto=webp&amp;s=d24160e419d7ca4fe0e860f950b6b25f833fb54e

The issue is that regardless of variability of both training and testing data, my model keeps predicting something like ""mean"" values of my predictions. Can somebody advise what I am doing wrong here please?

Thank you!

EDIT:

Problem that I am trying to solve:

Input data is image --&gt; convolutions to capture spatial information in the images --&gt; to predict temperature variations over time.

&amp;#x200B;

[Red: True, Blue: Predicted](https://preview.redd.it/jrjnd46a7w951.png?width=380&amp;format=png&amp;auto=webp&amp;s=ab30d839799e04e3afed339a65184bf340853886)

&amp;#x200B;

[loss](https://preview.redd.it/4blgaztv7w951.png?width=388&amp;format=png&amp;auto=webp&amp;s=c59cc73f0b6c73713dd8142b994ccfda84d69f99)",3,tensorflow,2020-10-13
ho1pt9,Image,"Hello, where can I find the patch wise segmentation code for segmentation?",0,tensorflow,2020-10-13
hnzyp2,Predicting new passwords based on the 'rock_you' Tensorflow Dataset,"Hi,I'm trying to create a program which loads the built-in tfds 'rock\_you' and predicts new passwords, but i'm getting this error:

`TypeError: Dimension value must be integer or None or have an __index__ method, got 'password'`

My code:

`import tensorflow as tf`

`import tensorflow_datasets as tfds`

`import keras`

`import os.path`

&amp;#x200B;

`model_path = ""passrockmodel.h5""`

&amp;#x200B;

&amp;#x200B;

`print('\nDownloading Train Dataset...\n')`

`train_dataset = tfds.load(name=""rock_you"", split=""train[:1%]"")`

`assert isinstance(train_dataset, tf.data.Dataset)`

&amp;#x200B;

`print('\nDownloading Test Dataset...\n')`

`test_dataset = tfds.load(""rock_you"", split='train[-25%:]')`

`assert isinstance(test_dataset, tf.data.Dataset)`

&amp;#x200B;

`model = tf.keras.Sequential([`

	`tf.keras.layers.Dense(128, activation='relu'),`

	`tf.keras.layers.Dense(128, activation='relu'),`

	`tf.keras.layers.Dense(1, activation='sigmoid')])`

&amp;#x200B;

`model.compile(`

	`loss='binary_crossentropy',`

	`optimizer='adam',`

	`metrics=['accuracy'])`

&amp;#x200B;

[`model.fit`](https://model.fit)`(train_dataset, epochs=20)`

&amp;#x200B;

[`model.save`](https://model.save)`(model_path)`

&amp;#x200B;

&amp;#x200B;

`test_loss, test_accuracy = model.evaluate(test_dataset)`

&amp;#x200B;",3,tensorflow,2020-10-13
hnzcp6,TensorFlow Model Server providing the model itself,"I am looking for a neat way to deploy trained TensorFlow models to production. Production in this case does not mean to have a way of requesting predictions over HTTPS, but to actually deploy the models to multiple devices.

Right now I am manually exporting my trained models as frozen graphs and copying them to the target devices. There, the models are loaded into C++ applications for inference. What I am looking for is a way to automate the deployment of the frozen models to my target devices. Something in the sense of *device requests a model from model server; model server determines it has a new, better model; new model is downloaded to target device*.

I have read a little about TensorFlow Serving, which seems to have desired features like keeping track of multiple models and serving the most recent or best one. However, I have the impression that TensorFlow Serving only provides a REST API for processing inference on the Server itself.

Perhaps someone can pinpoint to some tool or another idea of how to approach my problem! Thanks!",1,tensorflow,2020-10-13
hnsz2c,Question about large inputs,"For example, the tutorial with the Titanic survivors may be a million lines, but it's still one single data point per line. I am trying to input one massive CSV that outputs 10 probabilities, as opposed to 10 probabilities per line. I've got a 50MB csv with nearly a million lines and want it to output 10 probabilities (that are independent) for the whole csv. The issue is that a lot of the tutorials assume the data is one line each, and that the parameter being ""learned"" (like survival in the tutorial) is on each line. 

For background: I'm parsing CSGO demos and I'm trying to output probability that each player is cheating. The parser I wrote outputs a single csv for the whole demo, with every tick giving the location, viewangle, etc of all the players. 

Is this even possible? How would I do this with tensorflow either on colaboratory or locally? Or should I work on minimizing the input data to a single line or less? Perhaps I should, for each kill, calculate just the delta of the viewangle before and after each kill, then for each player generate a list of all of the kill deltas, and feed that in? cheaters would have much higher deltas since the view angle will change a lot. That ignores a lot of the other data that could be helpful though, like location, velocity, weapon, etc.",3,tensorflow,2020-10-13
hnrf86,How to use unit/neuron pruning in Tensorflow 2?,"Does Tensorflow only support weight pruning as seen here: [https://www.tensorflow.org/model\_optimization/guide/pruning/pruning\_with\_keras](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)

I'd like to do unit/neuron training. I'm open to writing the code myself, but would like to know if Tensorflow supports it.

Also posted on StackOverflow: [How to use unit/neuron pruning in Tensorflow 2?](https://stackoverflow.com/questions/62804758/how-to-use-unit-neuron-pruning-in-tensorflow-2)",1,tensorflow,2020-10-13
hnosz1,[My tutorial] TensorFlow 2.0 Dataset API - basics of the Dataset class,,17,tensorflow,2020-10-13
hnjia7,2 Errors when trying to run a CNN with TF/Keras,"I'm working on a CNN before the first epoch is completed I am getting the error messages:

""Function call stack: distributed\_function""

and

""Fused conv implementation does not support grouped convolutions for now.""

&amp;#x200B;

I am using slightly modified code that I used for another CNN that worked on the previous one so I'm a bit lost as to why this error is occurring now. TF version is 2.1.0 and Keras version is 2.2.4-tf

&amp;#x200B;

The images I am using are grey scale heat map images similar to [this](https://i.stack.imgur.com/DBmLF.png)

&amp;#x200B;

Code:

    TRAINING_DIR = '/Users/me/School/Research/mini'
    training_datagen = ImageDataGenerator(
        rescale = 1./255,
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest')
    
    train_generator = training_datagen.flow_from_directory(
        TRAINING_DIR,
        target_size=(640,480),
        class_mode='categorical'
    )
    
    model = tf.keras.models.Sequential([
        # Input shape is the desired size of the image 640x480 with 1 byte color
        # This is the first convolution
        tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(640, 480, 1)),
        tf.keras.layers.MaxPooling2D(2, 2), # factors to downscale by, (2,2) will halve
        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),   # 2nd convo layer
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),  # 3rd convo layer
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),  # 4th convo layer
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Flatten(),            # Flatten to DNN
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(512, activation='relu'),      # hidden layer 
        tf.keras.layers.Dense(3, activation='softmax')      # 3 class 
    ])
    
    model.summary()
    model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    history = model.fit(train_generator, epochs=15, verbose = 1)
    model.save(""rps.h5"")
    
    acc = history.history['accuracy']
    loss = history.history['loss']
    epochs = range(len(acc))",1,tensorflow,2020-10-13
hnibbk,Hard Problems: Eliminating Racism in Machine Learning,"Hey Everyone, 

We've been working away on a framework to eliminate or decrease racism in machine learning. Today we have a demo setup and first version release of the library. Would love if anyone has any vision problems classifying people to check out our library to balance out the dataset and keep it private. Any feedback on ways to improve are greatly appreciated. 

Demo Site: [https://privyfilter.herokuapp.com/](https://privyfilter.herokuapp.com/)

Repo: [https://github.com/Deamoner/privyfilter](https://github.com/Deamoner/privyfilter)

Architecture and Methodology Article: [https://medium.com/@mdavis\_71283/hard-problems-racial-bias-in-machine-learning-bf631c9d680c](https://medium.com/@mdavis_71283/hard-problems-racial-bias-in-machine-learning-bf631c9d680c)

&amp;#x200B;

Things to learn in this project: 

\- Creating a pipeline for new model creation - first create pipeline to generate dataset and then create model. 

&amp;#x200B;

Current Features:

\- Face Detection 

\- Demographic Information Extraction 

\- Synthetic Face Generation 

\- Face Swapping 

&amp;#x200B;

Future Features: 

\- Skin Detection 

\- Skin Hue Manipulation

\- Full Pipeline Process for the entire directory 

\- Multi-person photo support  

\- Remove Pipeline and Train pix2pix model 

&amp;#x200B;

This is an open source initiative, and open to any constructive feedback or even better actually getting your hands dirty by helping code. Looking for any feedback on ways we can improve it for any of your specific use cases.",0,tensorflow,2020-10-13
hni34w,Login with the Vulcan hand salute - TensorFlow on the Browser,,1,tensorflow,2020-10-13
hne5wh,"John Snow Labs Spark-NLP 2.5.3: Detect Fake news, emotions, spams, and more classification models, enhancements, and bug fixes",,10,tensorflow,2020-10-13
hn9noi,Latest from Adobe and UC Berkeley researchers: State of the art in deep image manipulation.,,1,tensorflow,2020-10-13
hn2247,How could I use the decoders to crop images according to their individual bounding boxes?,"Right now I am importing data from ```tensorflow_datasets``` as such;

```
(ds_train,ds_test),ds_info = tfds.load(
    name='caltech_birds2010',
    split=['train','test'],
    shuffle_files=False,
    as_supervised=True,
    with_info=True)
```

And preprocessing the images according to their individual bounding boxes, which I loaded in as a text file separately like a total plebeian (ds_info.features confuses me.)

I found a few things in the tensorflow_datasets documentation, decoders and the BBoxFeature class.

Supposedly, you could decode and crop the images at the same time using this:

```
@tfds.decode.make_decoder()
def decode_example(serialized_image, feature):
  crop_y, crop_x, crop_height, crop_width = 10, 10, 64, 64
  return tf.image.decode_and_crop_jpeg(
      serialized_image,
      [crop_y, crop_x, crop_height, crop_width],
      channels=feature.feature.shape[-1],
  )

ds = tfds.load('imagenet2012', split='train', decoders={
    # With video, decoders are applied to individual frames
    'image': decode_example(),
})
```

And this code is equivalent to the following:

```
def decode_example(serialized_image, feature):
  crop_y, crop_x, crop_height, crop_width = 10, 10, 64, 64
  return tf.image.decode_and_crop_jpeg(
      serialized_image,
      [crop_y, crop_x, crop_height, crop_width],
      channels=feature.shape[-1],
  )

ds, ds_info = tfds.load(
    'imagenet2012',
    split='train',
    with_info=True,
    decoders={
        'image': tfds.decode.SkipDecoding(),  # Skip frame decoding
    },
)
ds = ds.map(functools.partial(decode_example, feature=ds_info.features['image']))
```

**My question is,** how could one combine the bounding boxes from ```ds_info.features['bbox']``` with a custom decoder to generate a dataset of already cropped images?",1,tensorflow,2020-10-13
hmwpvj,"A few basic TF questions: float vs double, keras, others","Hi, I come from a PyTorch background but I need to catch up on some TF now. It's miles better than last time I used it (when it was still in 1.x).

I have a few questions I've run into, if anyone can help me out.

1) I often have variables of type `double` (i.e., float64) from various libraries like gym or numpy, but it seems like TF works with type `float` (i.e., float32) by default. It's not hard to convert between them (e.g., using `tf.constant(some_var, dtype=tf.float32)`), but I suspect I'm doing something wrong since it seems like I'd have to stick them all over the place. Is there some better practice for this?

2) What's the deal with how layers are set up? I was trying to just make a simple linear model to test something, and this seems like a pretty big difference from PyTorch. In PyTorch, a typical NN layer has the form `torch.nn.Linear(n_inputs, n_outputs)`, which make sense to me because it's really reflecting the weight matrix of dimensions `[n_outputs, n_inputs]`. On the other hand, it seems like in TF, I had to use:

```
layer1 = tf.keras.layers.Dense(n1, name=""layer1"")   
```

which I found confusing, because it only takes one size argument (`n1`), apparently the output? and it wasn't clear what size input it would take. AFAICT, it seems like it figures out the size from the first pass through, with a dummy variable, and then sticks with that?

```
dummy_input = tf.zeros((1, n2))
_ = layer1(dummy_input)
```
So now layer1 should take inputs of size `n2` and give inputs of size `n1`? If so, what's the logic here? I guess you're always supposed to use an Input layer in a real model? Still, seems really strange to me.

thanks in advance for any tips.",1,tensorflow,2020-10-13
hmwf7d,I’m making a tutorial on TensorFlow Dataset API and I need suggestions,"After looking online I found almost no youtube tutorials on the new TensorFlow 2.0 Dataset API so I decided I would make one, what topics/functions would you guys think I should cover?

My thoughts:
1. Loading data from tensor slices
2. Creating pipelines
3. Mapping, batching, shuffling
4. Creating data from generator
5. Difference between “Tensor” functions and “Array” functions",2,tensorflow,2020-10-13
hmui4t,I got TensorFlow certified and it was the best certification experience I've ever had,"I know we're very critical on certifications on Reddit and often for very good reason. A certification doesn't prove skill and is no substitute for experience, and an awful lot of existing certifications in the data science space are money grabs that lure in people with false promises of mountains of gold waiting to be data-mined.

I've done some proctored exams in 2015 and found them nerve wracking. I can't code or answer questions when there's someone watching me and quizzing me on ""WHATS THAT SOUND"" every five minutes. I recall finishing an exam I paid 300 dollars for and the amount of harassment made me sick to my stomach. I did not pass the exam, and I don't even think I finished on time.

I did the TensorFlow certification exam last week and my experience is so much different. The exam is coding-based, takes five hours, is administered via a PyCharm plugin and there's no one watching you. When I finished I got the results almost instantly. Only the certified directory and the accredible link take long. I basically spent five hours coding like I would normally do on my job.

I know this sounds like a sales pitch, but I hope we move towards this model of evaluation for more ""data science"" certifications. IMO any certification should tests the skills required to do the job, not the ability to select the right option from a multiple-choice list.

Too me this is what stood out about the entire process. 9/10 would do again.

I've written down more experiences in [this blog post](https://towardsdatascience.com/my-experience-with-the-tensorflow-developer-certification-exam-c75d2d1759de) for anyone interested in the exam itself, which I hope is allowed wrt the self promotion rules. It was removed on /r/datascience before for this reason.",64,tensorflow,2020-10-13
hmpic7,Creating an instance of ResNet50() uses high GPU Memory,"Creating a model using the below method results in 14GB of GPU usage  

`import tensorflow as tf` 

`from tensorflow.keras.applications import MobileNet, MobileNetV2, resnet` 

`base_model = resnet.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))`

&amp;#x200B;

[Result of nvidia-smi while running the above code](https://preview.redd.it/ch5uro3ftd951.png?width=718&amp;format=png&amp;auto=webp&amp;s=6aa247610091f175e25f29076888137e23e90f60)",1,tensorflow,2020-10-13
hmnfn4,Tensorflow serving is not installed,"I am getting below error while installing TensorFlow serving using this command **sudo apt-get remove tensorflow-model-server**

N: Ignoring file 'tensorflow-serving.lis' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension

E: Malformed entry 1 in list file /etc/apt/sources.list.d/tensorflow-serving.list (URI parse)

E: The list of sources could not be read.",6,tensorflow,2020-10-13
hmiqcu,Made a video on deploying tensorflow model and flask application. Looking forward to hear feedback.,“Build and Deploy Tensorflow Model and its integration with a Flask application from scratch” by Shravan C https://link.medium.com/zdfE4WUzU7,3,tensorflow,2020-10-13
hm5xi1,Understanding audio signals for machine learning,"In my new video 🔥🔥 you can learn about audio signals. I explain the difference between analog and digital signals, and how to convert an analog sound into a digital format that can then be processed for machine learning. I also delve deeper into Audio to Digital Conversion concepts such as  sampling, quantization, and aliasing.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=daB9naGBVv4&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=4](https://www.youtube.com/watch?v=daB9naGBVv4&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=4)",13,tensorflow,2020-10-13
hlz4jk,Is There Edgetpu Tflite Model Based On Open Images for Object Detection?,"Is there a tflite model based on open images dataset for object detection? I'd love to run it with coral edge on Raspberry pi.

I tried the one based on coco dataset, but it only has 80 classes.",1,tensorflow,2020-10-13
hlx7z3,"Hi, Is a Raspberry Pi 3 B (and 5mp CSI camera) fast/ powerful enough for facial recognition with tensorflow?",,7,tensorflow,2020-10-13
hludju,Best way to do instance segmentation on large images,"Hey, 
first of all sorry for my basic and bad English... 

I try to predict the mask of some specific brain regions in some whole slide images (wsi). For now I would like to train a u-net model to solve this task.
One problem is the wsi's are huge..(around 2gig) So far I understand I have to cut them into patches. 
But when I do this I don't know how to annotate the regions Im looking for to predict. 

Does any one has some experience in processing wsi data?",1,tensorflow,2020-10-13
hlmti2,[Question/Bug] Cannot use a constraint function on a sparse variable.,"Hi folks

so I have been trying to force a constraint on my embedding matrix which is unit\_norm across the each embedding components (each vector must have norm 1) using keras.constraints

tf says ""Cannot use a constraint function on a sparse variable.""

Let me show 2 examples where in the first one I couldn't and in the second I could and basically both are almost the same.

This is the first failing code:

[https://pastebin.com/NTNu1qpv](https://pastebin.com/NTNu1qpv)

and this is the working code:

[https://pastebin.com/rWH6MYSG](https://pastebin.com/rWH6MYSG)

both are solving different problems but still both should work

of course there's an issue on github for this bug and people are experiencing the same thing:

[https://github.com/tensorflow/tensorflow/issues/33755](https://github.com/tensorflow/tensorflow/issues/33755)

but I don't understand how i was able to get away in my second code

would appreciate any help",1,tensorflow,2020-10-13
hllley,"Im new to tf. Trying to make an image classification api. The tutorial im referring to is using imagenet's classify_image.py which is supposed to be in models/tutorials/image/imagenet/ in tf git repo, but the tutorial folder is not there on tf github repo.",Where do i find the classify_image.py file?,8,tensorflow,2020-10-13
hlht5k,"New to tf, am I trying to do something advanced? I can't figure out how to go about it.","I'm trying to build a model that takes a 2D list of varying number of columns and translates it into a 1D list.

I've created a sample data set, where the 3rd column is the ""output"". (it may be easier to think of it as (time,data,output)

    train = [[[1,2,3,4,5,6,7,8,9,10],[2,2,2,2,2,2,2,2,2,2],[0,1,0,1,0,1,0,1,0,1]],
            [[1,2,3,4,5],[5,5,5,5,5],[0,0,0,0,1]],
            [[1,2,3,4,5,6,7,8,9],[3,3,3,3,3,3,3,3,3],[0,0,1,0,0,1,0,0,1]],
             [[1,2,3],[2,2,2],[0,1,0]],
             [[1,2,3,4],[1,1,1,1],[1,1,1,1]]
            ]
    test = [[[1,2,3,4,5,6,7,8],[8,8,8,8,8,8,8,8],[0,0,0,0,0,0,0,1]],
            [[1,2,3,4,5,6,7,8,9,10,11],[4,4,4,4,4,4,4,4,4,4,4],[0,0,0,1,0,0,0,1,0,0,0]],
            [[1,2],[1,1],[1,1]]]

From the 20 or so hours of digging I've done, it seems like this *might* have something to do with ragged tensors, functional api, seq2seq, or some combination of the three. Or something else entirely.
Basically, I have time series data that varies in length, and I want to classify each time segment as either being true (1) or false (0).


Since I need my model to produce a list, and I'm not trying to put my data into a bucket, am I in way over my head?",7,tensorflow,2020-10-13
hl28da,Should I focus on Keras or go only with Tensrflow?,"Hello programmers!

This is my question: Should I focus on Keras or go only with Tensrflow? I am beginer with DL and Tensorflow in general. I have complited ""Intro to TensorFlow for Deep Learning"" from Udacity and completed some little model with Keras by my own. But, should I still focusing on Keras? In professional world, how is it going?

I would like to create a curriculum to enter to professional world of DL.

What are your sugestions?",9,tensorflow,2020-10-13
hkt2i6,[question] I understand the camera on top is known to work with Tensorflow... Would the bottom camera work as well? Both are meant to be CSI connected to a Raspberry Pi,,0,tensorflow,2020-10-13
hksvc2,How to convert PYMC3 code to TFP code? ( Therapeutic Touch Example),"Thanks in advance for reading my post.

This might be too specific for this community, but I'm going to ask anyways and let you guys tell me if I need to post this elsewhere :).

I am attempting to learn TensorFlow-Probability. My first attempt is to replicate the 'Therapeutic Touch' example from the *Doing Bayesian Analysis* by Kruschke. The PYMC3 code that I am referencing is here:

[https://nbviewer.jupyter.org/github/JWarmenhoven/DBDA-python/blob/master/Notebooks/Chapter%209.ipynb](https://nbviewer.jupyter.org/github/JWarmenhoven/DBDA-python/blob/master/Notebooks/Chapter%209.ipynb)

The data is very straightforward. There is a variable 'y' which is binomial (1 or 0) and a variable 's' which is a grouping variable ( 28 different groups ). The PYMC3 code looks like this:

&amp;#x200B;

&gt;practitioner\_idx = df.s.cat.codes.values  # an array containing integers 1-28  
&gt;  
&gt;practitioner\_codes = df.s.cat.categories.size  # a single integer 28  
&gt;  
&gt;with pm.Model() as hierarchical\_model:  
&gt;  
&gt;omega = pm.Beta('omega', 1., 1.)  
&gt;  
&gt;kappa\_minus2 = pm.Gamma('kappa\_minus2', 0.01, 0.01)   
&gt;  
&gt;kappa = pm.Deterministic('kappa', kappa\_minus2 + 2)  
&gt;  
&gt;theta = pm.Beta('theta', alpha=omega\*(kappa-2)+1, beta=(1-omega)\*(kappa-2)+1,shape=n\_practitioners)  
&gt;  
&gt;y = pm.Bernoulli('y', theta\[practitioner\_idx\], observed=df.y)

&amp;#x200B;

What is the best way to go about turning this into a TensorFlow-Probability code? I tried this: 

 

&gt;def group\_therapy(pract\_group):  
return tfd.JointDistributionSequential(\[  
&gt;  
&gt;tfd.Gamma(\[0.01, 0.01\], name=""Kappa""),  
        tfd.Beta(1., 1., name=""Omega""),  
lambda prior\_omega, prior\_kappa: tfd.Independent(  
            tfd.Beta(concentration1=prior\_kappa\[..., tf.newaxis\] \* tf.ones(\[n\_pract\])\[tf.newaxis,...\],  
                concentration0=prior\_omega\[..., tf.newaxis\] \* tf.ones(\[n\_pract\])\[tf.newaxis,...\],  
                name=""Theta""),  
            name=""treatment\_effect""),  
&gt;  
&gt;lambda prior\_theta: tfd.Bernoulli(  
            tf.gather(prior\_theta, pract\_group, axis=-1),  
            name = ""Root""  
&gt;  
&gt;)  
  \])

where

&gt;pract\_group = tf.convert\_to\_tensor(df\['group'\], dtype=tf.int32)  
resp\_y = tf.convert\_to\_tensor(df\['y'\], dtype=tf.float32)  
n\_pract = 28

But I currently get a type-error from Bernoulli telling me that I am missing a required positional argument 'rate'; which I find puzzling, as 'rate' is not named as an argument to Bernoulli(). 

My specific questions are: 

1. How to specify a 'plate' in Tensorflow-Probability? I am using a lambda function and tf.distribution.Independent() to model each group but I'm not sure if this is correct
2. Is it necessary to re-parameterize the inputs for the Beta() distribution as in the PYMC3 example? How do I add a single number to a distribution? ( emulating the kappa\_minus2 + 2 statement in the PYMC3 example ). 
3. Does anyone know of a github repository where the *Doing Bayesian Analysis* by Kruschke is implemented in TensorFlow-Probability? 

I appreciate any and all thoughts you have.",1,tensorflow,2020-10-13
hkrip4,"A browser extension that automatically finds code implementations for machine learning papers anywhere on the web (Google, Arxiv, Twitter, Scholar, and other sites)!",,8,tensorflow,2020-10-13
hkp32n,Some Cool Tensorflow Projects For Beginners,,29,tensorflow,2020-10-13
hkluv1,What do you guys think of this linear regression animated lecture by Ahmad Bazzi,,0,tensorflow,2020-10-13
hkkvmb,Face recognition (FaceNet) on Coral dev board using the Edge TPU,"I had some time to play around with TF and Google's Coral dev board. I figured face recognition would be an interesting idea for a first time hobby project.

I had thought I'd be able to find a working example of face recognition (not just detection) easily enough that I could just 'plug'n'play' but was a little surprised that I couldn't find one.

So I went about coding an example myself. Most of it was pretty straight forward enough, but I did run into some interesting hurdles. Getting a Keras FaceNet model quantized/compiled for use on the Edge TPU was one. being able to install some of the Python libs I wanted to use (like OpenCV and scikit) also took a little time.

After a lot of help from different blogs and resources (mentioned in the project readme), I was able to get a full demo project working (that includes using a Coral camera connected to the board) for both taking test images and for detecting/recognizing faces in a live video stream. It also streams the resulting video on a local Flask server running on the dev board.

Results are mixed. At 1280x720 face detection was pretty much fast enough for real-time (about 40ms) but recognition (including detection) is taking about 450ms (for a single face). Time to create an embedding (around 400ms) is the bottle-neck. May need to delve into that more.

Anyways, I thought I'd post it in hopes that might help others. The GitHub is located here: [https://github.com/terry-voth/facenet-on-coral-dev-board](https://github.com/terry-voth/facenet-on-coral-dev-board).",5,tensorflow,2020-10-13
hkk3d4,Attempting to build functionality using Socket.io. and Tensorflow JS and could use some direction,"Hey everybody,

I've been doing some research on websockets as a new JS developer and came  across [Socket.io](https://Socket.io) and it seems promising. I have two user models in my Postgresql database. One that I consider an admin and one is considered a user. I want to somehow get [Socket.io](https://Socket.io) to import webcam information into Tensorflow from the user and pass that information over the admin user on a dashboard that I built for them using react.

Is something like this even possible to do and if so do you have any direction for how to get started?

Ideally what I'd want is for Tensorflow to check for the user's attention (by checking if it's seeing if the users eyes or face are there or not, and for how long they are away) during their webchat calls through any platform, and for the admin to receive that information. I don't necessarily want a full video feed, just the events that are happening on camera.

Thanks!",3,tensorflow,2020-10-13
hkgn51,"Designing a network to process videos, pairs of frames at a time","My dataset consists of video frames, so I feed data in the format \[batchsize, depth, height, width, channel\]. I'm trying to design a network to find features from pairs of these frames and further use this vector of features later in the network. I am confused what would be the best way to implement this (diagram attached). A couple of ideas:

1. Design the input data in format \[batchsize, depth-1, 2 , height, width, channel\], have a single feature extractor and feed the pairs one after another while saving the outputs in a tf.Variable.
2. Read data as \[batchsize, depth, height, width, channel\] and use tf.while\_loop to feed pairs at a time and rest remains the same.
3. Something like a siamese network but I am not at all sure if it makes sense having that many copies of the feature extractor (depth = 100+).

Do any of these ideas seem plausible? Or could you suggest known models which work in this direction that I could familiarise myself with? 

[Proposed model](https://preview.redd.it/15zv9mo0am851.jpg?width=828&amp;format=pjpg&amp;auto=webp&amp;s=4bfda91b2e6c55acfa9cf550c50b9c4ca4bf6d53)",7,tensorflow,2020-10-13
hkfb5w,tf.py_function does not execute,"Hey i am trying to load some data via tf.data.Dataset. But i want to handle normal strings inside the process function and i came across a way to do it [here](https://stackoverflow.com/questions/56122670/how-to-get-string-value-out-of-tf-tensor-which-dtype-is-string). This does use the function ""tf.py\_function"", but i tried everything and i can't even seem to get it to execute that. It doesn't give me an error or anything. It just doesn't do anything.  


    dataset = tf.data.Dataset.list_files(path + ""/*.png"")
    labeled_ds = dataset.map(lambda x: tf.py_function(process_path, [x], [tf.string]))
    
    def process_path(file_path):
        print(""Hello i am here"")
        return file_path

Can anybody help me with that?",2,tensorflow,2020-10-13
hk4n8x,"Load local image into tensorflow, google colab","Hello! I am new to tensorflow and am going through the style transfer tutorial hosted here: [https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/style\_transfer.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb)

I want to load my own, local images for the content and style images but am having trouble.

I have tried:

content\_path = tf.keras.preprocessing.image.load\_img('/Users/USER/Desktop/nico.JPG')

to no avail.

Error: \[Errno 2\] No such file or directory: 'Users/USER/Desktop/nico.JPG'

Any suggestions?",5,tensorflow,2020-10-13
hk34bg,SIMPLE Object Detection retraining tutorial?,"Hi, is there a colab or tutorial for Mobilenet Object Detection retraining?

I was able to do the retraining using [Tensorflow Object Detection Github](https://github.com/tensorflow/models/tree/master/research/object_detection) a while back.

The setup is complex and time consuming. Is there a simpler solution to do the retraining for object detection?",1,tensorflow,2020-10-13
hjwyce,"Understanding the features of sound: Intensity, Loudness and Timbre","In my new video 🔥🔥 I continue discussing fundamental aspects of sound. I explain sound power, intensity, and loudness. I also delve into timbre, introducing key concepts like amplitude envelope, harmonic content, and amplitude/frequency modulation. 

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=Jkoysm1fHUw&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=3](https://www.youtube.com/watch?v=Jkoysm1fHUw&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=3)",8,tensorflow,2020-10-13
hjuhyc,How to use the TensorFlow dataset API,"I've been trying for several hours to complete this task with no success.

I have a very large dataset which is comprised of the following structure:

[My dataset, in the left, is my input, and in the right are my labels \(needed to be converted to categorical with \`tf.to\_categorical\`.](https://preview.redd.it/2kmo08eeef851.png?width=623&amp;format=png&amp;auto=webp&amp;s=b0c33fe096f5d13ccd3d3e046f0beab821ffb7a3)

I want to split this data into X and Y (and pass Y to `tf.to_categorical`) as in the picture using the `tf.data.Dataset` API, but unfortunately every attempt of me trying to use it has ended up with some kind of error.

How do I use `tf.data.Dataset` to:

1. Split each row to x and y.
2. Convert Y to categorical with `tf.to_categorical`.
3. Split the dataset into batches.

Thank you in advance.

EDIT: I got my answer [here](https://stackoverflow.com/questions/62694470/how-to-use-the-tensorflow-dataset-api-with-unknown-shapes-properly/62696738#62696738).",10,tensorflow,2020-10-13
hjuf3q,Move aside Keras Generator.. Its time for TF.DATA + Albumentations,,3,tensorflow,2020-10-13
hjom0k,ML/AI Code Implementation Finder (free browser extension),,4,tensorflow,2020-10-13
hje5lj,"Using Pyinstaller to create an Executable with ONNX, onnx2keras, and keras2onnx libraries"," 

Hi everyone,

I am currently having issues creating a pyinstaller executable that contains a script that converts an ONNX model to a keras.h5 format, trains the network, and converts the keras model back to ONNX format. The error is that pyinstaller cannot find the ONNX distribution for some reason despite feeding it into the .spec file. Just wondering if anyone has experience with this, thanks!",1,tensorflow,2020-10-13
hjcta2,Machine Learning with TensorFlow &amp; scikit-learn on Python Lecture 6 shows how to implement Multilabel and Multioutput classifications on scikit-learn. Do you think people still use Multilabel and Multioutput classifications ?,,12,tensorflow,2020-10-13
hj8y4z,Getting ‘MemoryError’ when trying to preprocess using ‘to_categorical’,"have a list with 2.5M values that I want to convert into categorical data using the keras.utils.to_categorical function but I am getting a MemoryError when trying to do so.

How would one do so without overloading the memory? batches? if so is there a proper ""tensorflowy"" way to do so?

Correction: I have a list of 2.5M values with 50K unique labels.

Update: tried to use TensorFlow dataset API and encountered a new problem [here](https://www.reddit.com/r/tensorflow/comments/hjuhyc/how_to_use_the_tensorflow_dataset_api/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf)",4,tensorflow,2020-10-13
hj4n3y,"Error when running on TPU, “NotImplementedError: TPUStrategy.run(fn, …) does not support pure eager execution…”","I am trying to train my model over a colab TPU, but I am running into 

&gt;NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled.

Even though I don't believe I have any Python functions in my code. 

Here is my best attempt to minimize the code in a colab notebook

https://colab.research.google.com/drive/11Yo1mdnKA3DqZCr_UpZI4umY8tpBpDzS?usp=sharing

And here is the code pasted below 


    ```
    %tensorflow_version 2.x
    !pip install transformers --q
    
    !gcloud auth login
    
    '''NEED TO RUN THIS CELL TWICE TO AVOID ERROR'''
    
    from google.colab import auth
    auth.authenticate_user()
    
    project_id = 'machinelearning-264918'
    !gcloud config set project {project_id}
    
    !pip install tfa-nightly
    import tensorflow_addons as tfa
    
    from transformers import TFBertModel, AutoModel, TFRobertaModel
    import tensorflow as tf
    
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.layers import (Dense,
                                         Dropout)
    import tensorflow_addons as tfa
    import numpy as np
    import os
    from copy import deepcopy 
    from time import time
    
    logger = tf.get_logger()
    logger.info(tf.__version__)
    
    autotune = tf.data.experimental.AUTOTUNE
    
    try:
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
        tf.config.experimental_connect_to_cluster(tpu)
        tf.tpu.experimental.initialize_tpu_system(tpu)
        strategy = tf.distribute.experimental.TPUStrategy(tpu)
        print('strategy.num_replicas_in_sync', strategy.num_replicas_in_sync)
        logger.info('Running with TPUStrategy on TPU {} with {} cores '
                    .format(tpu.cluster_spec().as_dict()['worker'],
                            strategy.num_replicas_in_sync))
        batch_size = 16 * strategy.num_replicas_in_sync
    except Exception:
        # raise ValueError
        strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')
        logger.warning('Failed initializing TPU! Running on GPU')
        batch_size = 16
    
    class Dora_A(tf.keras.Model):
        def __init__(self, **kwargs):
            super(Dora_A, self).__init__(**kwargs)
            self.bioRoberta = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)
    
        def call(self, inputIds):
            queryInputs, passageInputs = inputIds
    
            Q_outputs = self.bioRoberta(queryInputs)[0]
            P_outputs = self.bioRoberta(passageInputs)[0]
    
            dotProductMatrix = tf.linalg.matmul(Q_outputs, P_outputs, transpose_b=True, name='mm')
    
            return dotProductMatrix
    
    @tf.function
    def loss_fn(_, probs):
        '''
            1. Every sample is its own positive, and  the rest of the
                elements in the batch are its negative.
            2. Each TPU core gets 1/8 * global_batch_size elements, hence
                compute shape dynamically.
            3. Dataset produces dummy labels to make sure the loss_fn matches
                the loss signature of keras, actual labels are computed inside this
                function.
            4. Inputs are logits, for better numerical stability.
        '''
        bs = tf.shape(probs)[0]
        labels = tf.eye(bs, bs)
        return tf.losses.categorical_crossentropy(labels,
                                                  probs,
                                                  from_logits=True)
    
    CLS_inputID = tf.constant([0])
    SEP_inputID = tf.constant([2])
    
    def _parse_example(example_proto):
        features = {
            'bioRoberta_SentenceIndex': tf.io.VarLenFeature( dtype=tf.int64),
            'BioRoberta_IDs': tf.io.VarLenFeature( dtype=tf.int64),
        }
    
        parsed_example_dict = tf.io.parse_single_example(example_proto, features)
        bertIds = parsed_example_dict['BioRoberta_IDs']
        bertIds = tf.sparse.to_dense(bertIds)
        bertIds = tf.cast(bertIds, dtype=tf.int32)
    
        queryPiece = tf.slice(bertIds, [0], [510])
        restPassagePiece = tf.slice(bertIds, [0], [510])
        # add special tokens for proper input into the model 
        queryBertInput = tf.concat( [CLS_inputID, queryPiece, SEP_inputID], axis=0)
        paragraphBertInput = tf.concat( [CLS_inputID, restPassagePiece, SEP_inputID], axis=0)
    
        return queryBertInput, paragraphBertInput
    
    config_name = 'model_a'
    base_dir = 'gs://a-dora-semantic-scholar'
    model_dir = os.path.join(base_dir, config_name)
    tensorboard_dir = os.path.join(model_dir, 'logs_' + str(time()))
    tfrecords_pattern_train = os.path.join(base_dir, 'VersionA_00022*')
    tfrecords_pattern_val = os.path.join(base_dir, 'VersionA_00022*')
    
    
    if 'COLAB_TPU_ADDR' in os.environ:
        print('Setting tf.data objects')
        with strategy.scope():
            filenames = tf.io.gfile.glob(tfrecords_pattern_train)
            train_dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=autotune)
            train_dataset = train_dataset.map(
                                            _parse_example, num_parallel_calls=autotune)
            train_dataset = train_dataset.shuffle(130_000, seed=1000, reshuffle_each_iteration=True)
            train_dataset = train_dataset.padded_batch(batch_size, padding_values=(1, 1))
            train_dataset = train_dataset.prefetch(autotune)
            train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())
    
    with strategy.scope():
        model = Dora_A(dynamic=True)
        model.layers[0].trainable = False
        model.compile(loss=loss_fn,
                        optimizer=tfa.optimizers.AdamW(weight_decay=1e-5, 
                                                       learning_rate=1e-5, 
                                                       epsilon=1e-06))
    
    model.fit(train_dataset)
    ```

And here's a Google Drive link to the sample data file

https://drive.google.com/file/d/106gSmcClyshu98SDQ9VsUVOhd-LYamVq/view?usp=sharing

This is the full error output

    ```
    ---------------------------------------------------------------------------
    NotImplementedError                       Traceback (most recent call last)
    &lt;ipython-input-12-50bee5f74f82&gt; in &lt;module&gt;()
    ----&gt; 1 model.fit(train_dataset)
    
    4 frames
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
         64   def _method_wrapper(self, *args, **kwargs):
         65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
    ---&gt; 66       return method(self, *args, **kwargs)
         67 
         68     # Running inside `run_distribute_coordinator` already.
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
        846                 batch_size=batch_size):
        847               callbacks.on_train_batch_begin(step)
    --&gt; 848               tmp_logs = train_function(iterator)
        849               # Catch OutOfRangeError for Datasets of unknown size.
        850               # This blocks until the batch has finished executing.
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_function(iterator)
        570       data = next(iterator)
        571       outputs = self.distribute_strategy.run(
    --&gt; 572           self.train_step, args=(data,))
        573       outputs = reduce_per_replica(
        574           outputs, self.distribute_strategy, reduction='first')
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in run(self, fn, args, kwargs, options)
        166   def run(self, fn, args=(), kwargs=None, options=None):
        167     """"""See base class.""""""
    --&gt; 168     validate_run_function(fn)
        169 
        170     # Note: the target function is converted to graph even when in Eager mode,
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in validate_run_function(fn)
        104       and not (callable(fn) and isinstance(fn.__call__, def_function.Function)):
        105     raise NotImplementedError(
    --&gt; 106         ""TPUStrategy.run(fn, ...) does not support pure eager ""
        107         ""execution. please make sure the function passed into ""
        108         ""`strategy.run` is a `tf.function` or ""
    
    NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled.
    ```
    
Edit:

Following the comments, I decorated the call function, so that my model class looked like this

    ```
    class Dora_A(tf.keras.Model):
        def __init__(self, **kwargs):
            super(Dora_A, self).__init__(**kwargs)
            self.bioRoberta = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)
    
        @tf.function
        def call(self, inputIds):
            queryInputs, passageInputs = inputIds
    
            Q_outputs = self.bioRoberta(queryInputs)[0]
            P_outputs = self.bioRoberta(passageInputs)[0]
    
            dotProductMatrix = tf.linalg.matmul(Q_outputs, P_outputs, transpose_b=True, name='mm')
    
            return dotProductMatrix
    ```

But I got the same error message. 

I also tried decorating my tf.data parse function

    ```
    CLS_inputID = tf.constant([0])
    SEP_inputID = tf.constant([2])
    
    @tf.function
    def _parse_example(example_proto):
        features = {
            'bioRoberta_SentenceIndex': tf.io.VarLenFeature( dtype=tf.int64),
            'BioRoberta_IDs': tf.io.VarLenFeature( dtype=tf.int64),
        }
    
        parsed_example_dict = tf.io.parse_single_example(example_proto, features)
        bertIds = parsed_example_dict['BioRoberta_IDs']
        bertIds = tf.sparse.to_dense(bertIds)
        bertIds = tf.cast(bertIds, dtype=tf.int32)
    
        queryPiece = tf.slice(bertIds, [0], [510])
        restPassagePiece = tf.slice(bertIds, [0], [510])
        # add special tokens for proper input into the model 
        queryBertInput = tf.concat( [CLS_inputID, queryPiece, SEP_inputID], axis=0)
        paragraphBertInput = tf.concat( [CLS_inputID, restPassagePiece, SEP_inputID], axis=0)
    
        return queryBertInput, paragraphBertInput
    ```
    
But got the same result.",1,tensorflow,2020-10-13
hitoef,Is tf federated meant for production problems or only simulations as of now,"From what I can make out, I don't see how a datacenter would host data or a central server would consume it. All the examples on the tf federated site are with simulated data. Or am I missing something? Also, if anyone knows any other tools for cross silo federated learning that would be great.",2,tensorflow,2020-10-13
hijsr9,"Generate photo-realistic images given the input geometry and basic intrinsic properties, OR decompose real images back into their intrinsic components.",,6,tensorflow,2020-10-13
hii4lb,Top 15 AI Articles You Should Read This Month - June 2020,,20,tensorflow,2020-10-13
hihaah,PyTorch vs TensorFlow – Explained,,14,tensorflow,2020-10-13
hidft2,3D human pose reconstruction from a normal video,,2,tensorflow,2020-10-13
hi31ce,Looking for small projects to practice ML,"Hi guys, like in my previous posts, I’m about to take the TensorFlow certification exam and I was looking for some small projects to practice with before the exam.

Projects on the following topics:

1. Convolutional neural network (CNN) or image recognition.
2. Text classification (NLP).
3. Time sequences.

Does anyone have any ideas for easy but good for practice projects to do before the exam? Thank you.",3,tensorflow,2020-10-13
hi2yt8,A primer on sound and waveforms.,"In my new video, you can learn the physical properties of sound, how to interpret waveforms, and understand the concepts of frequency and pitch.

This video is part of the Audio Processing for Machine Learning series. This course aims to teach you how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

Here’s the video:

[https://www.youtube.com/watch?v=bnHHVo3j124&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=2](https://www.youtube.com/watch?v=bnHHVo3j124&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&amp;index=2)",4,tensorflow,2020-10-13
hhyxvg,Neural Network Optimal Control in Astrodynamics using TensorFlow,,19,tensorflow,2020-10-13
hhvuga,Top 3 Artificial Intelligence Research Papers – June 2020,,2,tensorflow,2020-10-13
hho8n6,Why are we allowed to pass in input_shape to tf.keras.layers.Dense if it is not one of the arguments listed?,"So... this is probably a stupid question. But I am not clear on why we are able to pass `input_shape` into `tf.keras.layers.Dense` when it's not listed as one of the arguments here:

[https://www.tensorflow.org/api\_docs/python/tf/keras/layers/Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)

For example, we can do this:

    model = tf.keras.models.Sequential([keras.layers.Dense(units=1, input_shape=[1])])

Does it have something to do with the `**kwargs` argument and if so, where can I find other arguments that I can pass into `Dense`?",2,tensorflow,2020-10-13
hhl4co,Suggestion: let’s make a mega thread about the TensorFlow Certification exam,"I’ve seen a lot of people asking questions about the exam, but not really receiving answers, so I suggest we should create a sticky mega thread post about the exam that should contain all the information you need, all the questions everyone has and all the answers that had been given.

Everyone who thinks this is a good idea comment below and upvote for visibility, let’s make sure the admins sees it.",37,tensorflow,2020-10-13
hhhl52,About to take the TensorFlow certification,"Hi guys, I am about to take the TensorFlow certification in the following days and I was wondering if some of you could provide some details on how the certification is handled, what are the questions like, how hard it was, etc

Please spare me your links for reports of other people, I wanna hear your personal experience with the certification.

and for those of you who have passed, how did you study for it? I personally just finished the Coursera [TensorFlow in Practice Specialization](https://www.coursera.org/specializations/tensorflow-in-practice) which I found to be interesting.

I am looking forward to hearing from you guys!


UPDATE: please check [this](https://www.reddit.com/r/tensorflow/comments/hhl4co/suggestion_lets_make_a_mega_thread_about_the/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=iossmf) suggestion I have made.",11,tensorflow,2020-10-13
hhc6hm,Google Adds New Privacy Testing Module In TensorFlow,,1,tensorflow,2020-10-13
hh8gbf,Classifications from a MobileNet model,"Hey all, just got started into ML &amp; Tensorflow. I know a little bit of Keras but not too much. I'm using this script to retrain my own image classification model based on MobileNet V2.

[https://github.com/tensorflow/hub/blob/master/examples/image\_retraining/retrain.py](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py)

Is it possible to also have access to the ImageNet predictions? For example, pencil, pen, chair &amp; all the basics. As well as still having my newly trained dataset show up in the predictions.  


At the moment I only see predictions from my newly trained dataset. All the ImageNet stuff doesn't show up.",6,tensorflow,2020-10-13
hh03g6,Class Weights for MultiLabel Output Fails on TF2 [HELP],"I'm trying to provide class\_weights for an imbalanced dataset for [model.fit](https://model.fit) in TF2. 

Based on many repos I have seen, they provide list of dicts to class\_weights parameter for [model.fit](https://model.fit) as shown below.

&amp;#x200B;

https://preview.redd.it/t869frjt8i751.png?width=488&amp;format=png&amp;auto=webp&amp;s=42afa2da7caf5a467b37e3cca49b92e08dfdf120

However, with TF2 this leads to the error in the data\_adapter saying list object has no keys.

&amp;#x200B;

https://preview.redd.it/epys0bnz8i751.png?width=1228&amp;format=png&amp;auto=webp&amp;s=f39cbb982b9a9b8a55f1e9ab59345cc4582cca43

So, I converted the above class\_weights to a dict of dicts.

&amp;#x200B;

https://preview.redd.it/sbvvmwhk9i751.png?width=548&amp;format=png&amp;auto=webp&amp;s=16f986289c41cf2f68496888e9cc2e93a825260d

This leads to Tensor conversion error.

&amp;#x200B;

https://preview.redd.it/4i6rvxun9i751.png?width=1224&amp;format=png&amp;auto=webp&amp;s=501f5dd9c71dd3af4c7ff955286133fb4a3f22d4

When I tried to convert to dict of dicts using a different keys:

&amp;#x200B;

https://preview.redd.it/l9f5tgp0ai751.png?width=600&amp;format=png&amp;auto=webp&amp;s=31de3661c85d862f02ee77cbe3e9da2fea1351cd

I get the following error:

&amp;#x200B;

https://preview.redd.it/go954ee7ai751.png?width=1208&amp;format=png&amp;auto=webp&amp;s=38fcd2ff583c8ba7fe3aa026702e8c1e52cf8723

So, it is expecting dict with keys 0 to TOTAL\_CLASSES in multilabel classification.

&amp;#x200B;

What is solution to passing CLASS\_WEIGHTS for multi-label classification problem?",4,tensorflow,2020-10-13
hgo8r5,Help starting with tensorflow,"I am a Product Designer with basic programming knowledge. I want to prototype something for which I need a basic number recognition to be done (point phone at a printed or handwritten number, it recognizes it and opens a respective file). I figured out that using tensorflow I can probably get the object recognition to work, but my question is, is there any easy ""visual programming"" way i can prototype and build this app? 

I have used Orange Data mining tool a bit and i was hoping something along that lines would be really helpful!",4,tensorflow,2020-10-13
hgnx1l,how to adjust dimensions to fix matrix size-incompatible error?,"Hey friends: ) This afternoon, I fit a model on a dataset successfully; however, when i try to run inference on a new example, i get the following error:

`Matrix size-incompatible: In[0]: [1,103], In[1]: [40,32]`

     `[[node functional_3/dense_2/Relu`

There are some similar questons on stack, so I definitely know I have to change the dimensions, but I'm not sure how! The inputs don't match up, but I'm not sure what they aught to be, nor how to change them to a proper dimension. any help is appreciated.

the whole program is embedded my stack post:

[https://stackoverflow.com/questions/62601287/keras-tensorflow-how-to-adjust-dimensions-to-fix-matrix-size-incompatible-er](https://stackoverflow.com/questions/62601287/keras-tensorflow-how-to-adjust-dimensions-to-fix-matrix-size-incompatible-er)

Any help is like so welcome : )",2,tensorflow,2020-10-13
hgau2r,This AI Recognises Moods in Songs and Explains How it Does it [Paper Analysis],"In my new video, I break down a research paper that introduces a new deep learning architecture 🤖🤖 to recognise moods in songs  🎧 🎧 and explain its predictions. 

The paper is called “Towards Explainable Music Emotion Recognition: The Route via Mid-Level Features” and was published at ISMIR in 2019.

This new type of video format (Papers’ Analysis) will enable you to stay on top of the literature in AI audio / music. It will also help beginners read and understand research papers.

I hope you enjoy it! 

Here’s the video:

https://www.youtube.com/watch?v=doUTqWUAuDw",29,tensorflow,2020-10-13
hg32uz,This Week in AI - Issue #23 | Rubik's Code,,4,tensorflow,2020-10-13
hg2xl1,Universal Text Encoder with a scaled databse,"Im looking at this [model](https://tfhub.dev/google/universal-sentence-encoder/4) and wondering how this would work on a scaled database. Considering the following use case:   


I want to compute the similarity of input\_text to the list of statements in my database currently.   


    database_statements_embedding = getCachedEmbedding() // returns precomputed embedding matrix of all statements in my databse 
    
    input_embedding = embed([input_text])
    
    corr = np.inner(input_embedding, database_statements_embedding)
    idx = corr.argmax()
    
    most_simillar_statement = getStatement(idx) // asumming embedding order is same as db order
    

My database is a list of statements that grows every-time a user inputs something, potentially 100,000+ statements. I compute and cache the embedding matrix every-time a statement is added.   


Assuming these conditions would this kind of algorithm for finding similarity work when scaled.   


Which would be a bigger bottle-next (consider DB with 100k statements)  


1.) Computing the correlation matrix   
2.) Recomputing and caching the total statement embedding.",1,tensorflow,2020-10-13
hg0f0s,Latest from Adobe and KAIST researchers: A novel video panoptic segmentation network - VPSNet.,,1,tensorflow,2020-10-13
hfwuyg,How can I load model weights in Tensorflow 2.0 for multiple GPU training?,"I am trying to load model weights in TensorFlow 2.0 for multiple GPU training. My model checkpoints are from training in Tensorflow 1.0.

&amp;#x200B;

This is how I used to load weights in TF1.

    var_list = tf.trainable_variables()
    for var in var_list:
      vname = ''.join(var.name.split('semantic/'))
      val = tf.contrib.framework.load_variable(checkpoint, vname)
      tf.assign(var, val)

&amp;#x200B;

I require a similar way to load weights in tf2. I would appreciate any help on this.

&amp;#x200B;

Thank you!",4,tensorflow,2020-10-13
hfjhy4,help with install of tensorflow 2,"hi, i have been having a lot of trouble getting python and tensorflow 2 installed. does anyone have a reproducable recipe? https://stackoverflow.com/questions/62376660/installing-tensorflow-2-gets-a-dll-failed-to-load-in-pywrap-tensorflow-py
thanks",0,tensorflow,2020-10-13
hfijfn,"""Object Detection"" pipeline for Bio-Potentials","Hello Redditors,

I'd like to share with you some interesting work on the application of deep learning to bio-potentials. Bio-potentials are micro-volts emitted through your skin whenever you move. This phenomenon can form the basis for the perfect human-machine interface and this work (5 years in the making) is something I would like to share within this community.

[https://medium.com/@leeor.langer/an-object-detection-pipeline-for-bio-potentials-on-the-apple-watch-46ea6aac2721](https://medium.com/@leeor.langer/an-object-detection-pipeline-for-bio-potentials-on-the-apple-watch-46ea6aac2721)

[https://medium.com/@leeor.langer/combating-false-positives-in-gesture-recognition-e727932b41b1](https://medium.com/@leeor.langer/combating-false-positives-in-gesture-recognition-e727932b41b1)

Would love to hear your thoughts, ideas and suggestions,

Leeor Langer, Wearable Devices CTO",13,tensorflow,2020-10-13
hfhwwm,How to reset RNN model states of a model being served on docker?,"I'm in the process of attempting to deploy an NLP RNN (akin to [https://www.tensorflow.org/tutorials/text/text\_generation](https://www.tensorflow.org/tutorials/text/text_generation)) through Tensorflow Serving. When testing the model locally, I'm able to reset the states of the model on each new batch, but I can't seem to find the solution to resetting the states of models being served through TF Serving.

When creating a local model stored in memory, the following simple line of code resets the states on each new batch/prediction:

    model.reset_states()

However, using Tensorflow Serving, I can't seem to find a solution to how this can be done through REST API or a call to the model.

I'm using Docker to host a container for two models; I've only used a simple implementation as I'm very new to Docker:

    docker run -p 8501:8501 -p 8500:8500    
    --mount type=bind,source='{PATH}\\Saved_Models\\Shakesbot',target=/models/shakesbot/1    
    --mount type=bind,source='{PATH}\\Saved_Models\\Tolstoybot',target=/models/tolstoybot/1    
    --mount type=bind,source='{PATH}\\model_config.config',target=/models/model_config.config    
    -t tensorflow/serving
    --model_config_file=/models/model_config.config


I've managed to write a piece of code which generates a body of text, on a character by character basis, through individual REST calls.


    def generate_text_JSON(start_seed,gen_size=500,temp=1):

        num_generate = gen_size
        input_eval = [char_to_ind[s] for s in start_seed]
        input_eval = tf.expand_dims(input_eval, 0)
        text_generated = []
        temperature = temp
        headers = {""content-type"": ""application/json""}
        #model.reset_states()

        for i in range(num_generate):

            data = json.dumps({""signature_name"": ""serving_default"", ""instances"": input_eval.numpy().tolist()})
            json_response = requests.post('http://localhost:8501/v1/models/shakesbot:predict',data=data, headers=headers)
            predictions = json.loads(json_response.text)
            predictions = tf.squeeze(predictions['predictions'],0)
            predictions = predictions/temperature
            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()
            input_eval = tf.expand_dims([predicted_id], 0)
            text_generated.append(ind_to_char[predicted_id])

    return(start_seed+"""".join(text_generated))



Now the issue here clearly is that once the model is served, it continues to retain model states regardless. Where the ""`#model.reset_states()`"" sits in the method above would be the ideal place to send a command to the served model, but I can't seem to find any information as to how this can be done, if possible.



Any assistance would be greatly appreciated!

Thanks",1,tensorflow,2020-10-13
hfgpj1,Sequence Prediction example,"Hi all, I'm trying to write a Discord bot that does sequence prediction on the game rock, paper, scissors. Basically I will train it using some previous data and then it will tell me what to choose to maximize my chances of winning based on previous moves.

Does anybody have some simple examples I can use to follow? Most of the ones I found are fairly complicated that involves doing image recognition and such. All I need is sequence prediction and a way to feed it training data (a sequence such as {paper,paper,paper},{paper,rock,scissors},{paper,paper,rock,rock}) and for it to continue learn from each interaction.

I found a really simple function from Wolfram Language and was wondering if there is something similar for TF, thanks a lot in advance for any feedback!

[https://www.wolfram.com/language/12/high-level-machine-learning/play-rock-paper-scissors-using-sequencepredict.html?product=language](https://www.wolfram.com/language/12/high-level-machine-learning/play-rock-paper-scissors-using-sequencepredict.html?product=language)",1,tensorflow,2020-10-13
hf4ba7,"YouTubeHey guys, just wanted to share a gradient descent ANIMATED lecture with some of my animations and some theoretical studies on how gradient descent works in 1D and 2D. Also, the video talks about batch gradient descent method and shows you how to get the job done on Python.",,12,tensorflow,2020-10-13
hf3bkl,Tensorflow 2.0: Pre-processing input data generated from ImageDataGenerator before putting to model.fit_generator,"I'm using `ImageDataGenerator` from `tf.keras` (Tensorflow 2.0) to generate data for my training. The raw data contains images and the corresponding segmentation labels (all in RGB format). The sample code with my intuitive approach as below:

    ....
    
        # Create two instances with the same arguments
        data_gen_args = dict(
            rotation_range=30,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.2,
            zoom_range=0.2,
            validation_split=0.2
        )
      
        image_datagen = ImageDataGenerator(**data_gen_args)
        label_datagen = ImageDataGenerator(**data_gen_args)
    
        # Provide the same seed and keyword arguments to the fit and flow methods
        seed = 2020
    
        image_train_generator = image_datagen.flow_from_directory(
            IMAGES_DIR,
            target_size=(IMAGE_SIZE, IMAGE_SIZE),
            class_mode=None,
            batch_size=BATCH_SIZE,
            seed=seed,
            subset=""training"")
    
        label_train_generator = label_datagen.flow_from_directory(
            LABELS_DIR,
            target_size=(IMAGE_SIZE, IMAGE_SIZE),
            class_mode=None,
            batch_size=BATCH_SIZE,
            seed=seed,
            subset=""training"")
    
        # https://github.com/keras-team/keras/issues/13123#issuecomment-529498919
        train_generator = (pair for pair in zip(image_train_generator, label_train_generator))
    
        image_val_generator = image_datagen.flow_from_directory(
            IMAGES_DIR,
            target_size=(IMAGE_SIZE, IMAGE_SIZE),
            class_mode=None,
            batch_size=BATCH_SIZE,
            seed=seed,
            subset=""validation"")
    
        label_val_generator = label_datagen.flow_from_directory(
            LABELS_DIR,
            target_size=(IMAGE_SIZE, IMAGE_SIZE),
            class_mode=None,
            batch_size=BATCH_SIZE,
            seed=seed,
            subset=""validation"")
    
        validation_generator = (pair for pair in zip(image_val_generator, label_val_generator))
    
        model.fit_generator(
            train_generator,
            steps_per_epoch=STEPS_PER_EPOCH,
            epochs=EPOCHS,
            callbacks=callbacks,
            validation_data=validation_generator,
            validation_steps=VADIDATION_STEP)
    

However, I need to pre-process images and labels first before putting it to `fit_generator()`: each image has to be standardized (subtract the `mean` and divided by the `std`), each label has to be converted to the class-wise channel of `numpy.ndarray` (specifically from the shape of `(512, 512, 3)` contains 8 color-labeled segmentation classes to the shape of `(512, 512, 8)` with each channel contains only one segmentation class).

I have tried:

* Pass functions to the `preprocessing_function` arguments in the `ImageDataGenerator.flow_from_directory()` → failed because this method required the output has to be the same shape with the input, which is not appropriate to my case.
* Use `list(map(processing_function, batch))` for each batch before putting to `model.fit`:

&amp;#8203;

    for e in range(EPOCHS):
            print('Epoch: ', e)
            batches = 0
            for x_batch, y_batch in train_generator:
                x_batch = np.array(list(map(preprocess_input_images, x_batch)))
                y_batch = np.array(list(map(preprocess_input_labels, y_batch)))
                model.fit(x_batch, y_batch)
                batches += 1
                if batches &gt;= STEPS_PER_EPOCH:
                    break

→ It runs but the results were rubbish (It may behave in the wrong way with my expectation).

* The other thing I'm going to try is making a custom `DataGenerator` or a custom `fit_generator`, inherited from `Keras` class but I'm not sure exactly which approach I should follow.

For now, I'm not quite familiar with `Keras`. So, my question is, with the new `Keras` under `Tensorflow 2.0`, what is the best way to deal with this issue?

You can post your answer [here](https://stackoverflow.com/questions/62554299/tensorflow-2-0-pre-processing-input-data-generated-from-imagedatagenerator-befo) so anyone gets troubles with this problem in the future can reach easily. Thank you in advance!",2,tensorflow,2020-10-13
hez91g,TensorFlow 2.0 Data Augmentation: tf.keras.preprocessing.image.ImageDataGenerator flow() method,"I am trying to perform data augmentation using TensorFlow 2.2.0 and Python 3.7 for LeNet-300-100 Dense neural network for MNIST dataset. The code I have is as follows:

&amp;#x200B;

    
        batch_size = 60
        num_classes = 10
        num_epochs = 100
        
        
        # Data preprocessing and cleadning:
        # input image dimensions
        img_rows, img_cols = 28, 28
        
        # Load MNIST dataset-
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
        
        
        if tf.keras.backend.image_data_format() == 'channels_first':
        	X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
        	X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)
        	input_shape = (1, img_rows, img_cols)
        else:
        	X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
        	X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
        	input_shape = (img_rows, img_cols, 1)
        
        print(""\n'input_shape' which will be used = {0}\n"".format(input_shape))
        # 'input_shape' which will be used = (28, 28, 1)
        
        
        # Convert datasets to floating point types-
        X_train = X_train.astype('float32')
        X_test = X_test.astype('float32')
        
        # Normalize the training and testing datasets-
        X_train /= 255.0
        X_test /= 255.0
        
        # convert class vectors/target to binary class matrices or one-hot encoded values-
        y_train = tf.keras.utils.to_categorical(y_train, num_classes)
        y_test = tf.keras.utils.to_categorical(y_test, num_classes)
        
        
        X_train.shape, y_train.shape
        # ((60000, 28, 28, 1), (60000, 10))
        
        X_test.shape, y_test.shape
        # ((10000, 28, 28, 1), (10000, 10))
        
        
        # Example of using 'tf.keras.preprocessing.image.ImageDataGenerator class's - flow(x, y)':
        
        datagen = ImageDataGenerator(
            # featurewise_center=True,
            # featurewise_std_normalization=True,
            rotation_range = 20,
            width_shift_range = 0.2,
            height_shift_range = 0.2,
            horizontal_flip = True
            )

&amp;#x200B;

Now, when I see the number of batches produced by 'datagen.flow()' with the code:

&amp;#x200B;

        # Sanity check-
        i = 0
        
        for x, y in datagen.flow(X_train, y_train, batch_size = batch_size, shuffle = True):
            # print(""\ntype(x) = {0}, type(y) = {1}"".format(type(x), type(y)))
            # print(""x.shape = {0}, y.shape = {1}\n"".format(x.shape, y.shape))
            print(i, end = ', ')
            i += 1
    

The value of \*i\* keeps increasing without terminating. Of course something is going wrong. According to what I know, the number of batches = number of training examples / batch size.

Therefore, in this example, the number of batches = 60000 / 60 = 1000.

&amp;#x200B;

Then why is it producing so many batches of augmented data? And how can I stop it? What's going wrong?

&amp;#x200B;

Thanks!",5,tensorflow,2020-10-13
hextnw,Make your first image classifier using tensorflow.js and node.js,,1,tensorflow,2020-10-13
heueu8,How I Passed the TensorFlow 2.0 Developer Certificate Exam - My path,"Please check out my post

[https://towardsdatascience.com/how-to-learn-data-science-my-path-ba7b9aa94f63?source=friends\_link&amp;sk=47d16e88a2bcb0635ab14792b0092fb6](https://towardsdatascience.com/how-to-learn-data-science-my-path-ba7b9aa94f63?source=friends_link&amp;sk=47d16e88a2bcb0635ab14792b0092fb6)

Please scroll down to the bottom and you can see the resources I used.

Certification is not a big deal. The objective is to learn TensorFlow 2.0

I just added it to the existing post which I published last year.

Hope it is helpful  👍",32,tensorflow,2020-10-13
hetni6,Computer vision application using the TensorFlow js model,"I have implemented a small computer vision application using the TensorFlow js model. 

Whenever I move my face while watching the video that video frames also will move as per my face movement.it's quiet very interesting.

The interesting part is you don't want to install anything just open the HTML file in the browser then you can play with it.

**Github link:-** https://github.com/balavenkatesh3322/tensorflowjs-demo",3,tensorflow,2020-10-13
heqll7,"Latest from Max Planck researchers: Estimate the clothing deformations with fine details from input body shape, body pose and garment style.",,1,tensorflow,2020-10-13
hemhde,Do you guys think this model design would suffice?,"I am building a Reddit post to upvotes predictor based on title, desc, and post age and I am wondering on how to design my model, what do you guys think about this design? I am passing multiple inputs along individual ""models"" and then concatenate the results into a dense layer which spits out a prediction, do you think it will suffice? 

[The design of my model, what do you guys think?](https://preview.redd.it/ygm85wuuzp651.png?width=1720&amp;format=png&amp;auto=webp&amp;s=b83a0d825420416ea88ed8a366a444f9ea9af962)",7,tensorflow,2020-10-13
helm5m,"What optimizer, loss function, and activation functions you use for decimal prediction","Not sure how to explain my question, but if I want my model to predict some uncapped number, what parameters should I use?

for context, I am building a Reddit upvotes predictor based on title, desc and post age and I am wondering how to design my model so that it spits out a prediction of upvotes.",1,tensorflow,2020-10-13
hegrx5,Image augmentation with tf.GradientTape,"Hey guys, I am looking for a tutorial where they combine Tensorflow 2.0 data augmentation with tf.GradientTape based custom training.

Any Help?

Thanks!",4,tensorflow,2020-10-13
hef8sy,Seq2Seq network check points are huge in size,"Hello, im in the process of replicating the works within Plan and Write (Yao et al 2019) which aims at generating text based stories from a user inputted title. with the exception of migrating from pytorch to TensorFlow 2.2, thus far i've made my own dataset of plot points from the ROCStories dataset using RAKE, extracting a single keyword per sentence as its plot. then following the TensorFlow addons seq2seq tutorial i managed to get promising results, however when saving check points after 100 epochs i have a 27GB folder of checkpoints, which i think is extreme considering the network simply predicts 5 words, pretty much everything is the same as the tutorial with the exception of changing the attention mechanism to the bahdanauAttention 

&amp;#x200B;

can check point sizes be reduced greatly ?",1,tensorflow,2020-10-13
hdy38h,TensorFlow 2 for all : The giant of Deep Learning,,1,tensorflow,2020-10-13
hdxwcr,Real-Time semantic segmentation in the browser using TensorFlow.js,,1,tensorflow,2020-10-13
hdv2ek,Is my GPU being underutilised during training?,,14,tensorflow,2020-10-13
hdubst,I'm Unable To Import TensorFlow," 

An error occurred while importing TensorFlow after a successful installation.  
please guide me further.

\&gt;&gt;Traceback (most recent call last):  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 18, in swig\_import\_helper  
fp, pathname, description = imp.find\_module('\_pywrap\_tensorflow', \[dirname(**file**)\])  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\imp.py"", line 296, in find\_module  
raise ImportError(\_ERR\_MSG.format(name), name=name)  
ImportError: No module named '\_pywrap\_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\_*init*\_.py"", line 66, in  
from tensorflow.python import pywrap\_tensorflow  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 28, in  
\_pywrap\_tensorflow = swig\_import\_helper()  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 20, in swig\_import\_helper  
import \_pywrap\_tensorflow  
ModuleNotFoundError: No module named '\_pywrap\_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):  
File """", line 1, in  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\_*init*\_.py"", line 24, in  
from tensorflow.python import \*  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\_*init*\_.py"", line 72, in  
raise ImportError(msg)  
ImportError: Traceback (most recent call last):  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 18, in swig\_import\_helper  
fp, pathname, description = imp.find\_module('\_pywrap\_tensorflow', \[dirname(**file**)\])  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\imp.py"", line 296, in find\_module  
raise ImportError(\_ERR\_MSG.format(name), name=name)  
ImportError: No module named '\_pywrap\_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\_*init*\_.py"", line 66, in  
from tensorflow.python import pywrap\_tensorflow  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 28, in  
\_pywrap\_tensorflow = swig\_import\_helper()  
File ""C:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap\_tensorflow.py"", line 20, in swig\_import\_helper  
import \_pywrap\_tensorflow  
ModuleNotFoundError: No module named '\_pywrap\_tensorflow'

Failed to load the native TensorFlow runtime.

See [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get\_started/os\_setup.md#import\_error](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error)",1,tensorflow,2020-10-13
hdqzfk,I launched a Slack community on AI audio/music.,"Interested in learning about AI audio / music? Join The Sound of AI Slack community for free! I’ve launched this community 🚀🚀 to create a 1-stop place to discuss all things AI audio / music and signal processing 🔊🤖.

In this Slack workspace, you can get advice, meet great people, and share knowledge.

Here’s an intro video to get an idea of my vision for the community:

[https://www.youtube.com/watch?v=8VoZylPibyg&amp;list=PL-wATfeyAMNpd8nGIxJKLpTV5zpOWyLZC&amp;index=7](https://www.youtube.com/watch?v=8VoZylPibyg&amp;list=PL-wATfeyAMNpd8nGIxJKLpTV5zpOWyLZC&amp;index=7)",1,tensorflow,2020-10-13
hdqtyf,Best hands on tutorial on tensorflow?,I would like to get started on tensorflow. but most of the tutorials on youtube start directly with pre-written code. Can someone suggest me hands on tutorial on tensorflow?,1,tensorflow,2020-10-13
hdoeaq,Ego graph for Tensorflow,,39,tensorflow,2020-10-13
hddl3r,How do you combine an image and meta data as input?,"Well my question is really about text, but image is easier to understand.

Let’s say I have some images and some meta data about the images and I want to train a model with the metadata and the image combined, how would I do it?

Would I combine the meta data and pixels as one array and pass that as input? Would my model learn properly this way?


Edit: Ok well it’s not really about images but about text, i thought it would be simpler but I guess I just over complicated it.

Anyway, I’m building a reddit post to upvotes predictor based on title, desc and post age and I’m just wondering how I would design that model.",2,tensorflow,2020-10-13
hdblfb,Windows vs Linux for TF,"What are the differences between Linux and Windows for Tensorflow? I use windows and it is fine. But somebody in my company told me that TensorFlow works much better with Linux. If this is true, why?",3,tensorflow,2020-10-13
hd81zq,Tflite model on classification,"Hello Everyone.
I was deploying my image classification on Android mobile.I converted my model to tflite and rechecked it by passing the input in it and Interpreter.invoke() way in python it works fine there .But when I integrate it with my android device it start predicting wrong classes and work completely different how it works in python .
I made float model my interpreter.run(mybytebuffer,result) is working fine and giving prediction in desired dimensions but the result which it's giving is wrong .Been trying to integrate from 3 days 
Seriously needs help 
Thanks in advance",5,tensorflow,2020-10-13
hd1ni8,Model created from other model layers do not contain all weights from layers. But model.summary() / plot_model shows those weights as part of graph,"I created a model which takes two layers from an existing model, and creates a model from those two layers. However, the resulting model does not contain all the weights/layers from those component layers. Here's the code I used to figure this out. 

(edit: Here's a colab notebook to tinker with the code directly https://colab.research.google.com/drive/1tbel6PueW3fgFsCd2u8V8eVwLfFk0SEi?usp=sharing )


    !pip install transformers --q
    %tensorflow_version 2.x
    
    from transformers import TFBertModel, AutoModel, TFRobertaModel, AutoTokenizer
    import tensorflow as tf
    import tensorflow_addons as tfa
    
    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
    
    from tensorflow import keras
    from tensorflow.keras import layers
    from copy import deepcopy
    
    logger = tf.get_logger()
    logger.info(tf.__version__)
    
    
    def get_mini_models():
        tempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)
    
        layer9 = deepcopy(tempModel.layers[0].encoder.layer[8])
        layer10 = deepcopy(tempModel.layers[0].encoder.layer[9])
    
        inputHiddenVals = tf.keras.Input(shape=[None, None], dtype=tf.float32, name='input_Q',
                                        batch_size=None) 
    
        hidden1 = layer9((inputHiddenVals, None, None))
        hidden2 = layer10((hidden1[0], None, None))
        modelNew = tf.keras.Model(inputs=inputHiddenVals, outputs=hidden2)
    
        del tempModel
    
        return modelNew
    
    @tf.function
    def loss_fn(_, probs):
        bs = tf.shape(probs)[0]
        labels = tf.eye(bs, bs)
        return tf.losses.categorical_crossentropy(labels,
                                                  probs,
                                                  from_logits=True)
    
    model = get_mini_models()
    model.compile(loss=loss_fn,
                    optimizer=tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-5, 
                                                    epsilon=1e-06))
    
    # Get model and layers directly to compare
    tempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)
    layer9 = deepcopy(tempModel.layers[0].encoder.layer[8])
    layer10 = deepcopy(tempModel.layers[0].encoder.layer[9])

When I print out the trainable weights, only the keys, query, and values are printed, but each layer also has some dense layers and layer_norm layers. Also, the keys, queries, and values from one layer are printed, but there are two. 

    # Only one layer, and that layer also has missing weights. 
    for i, var in enumerate(model.weights):
        print(model.weights[i].name)

&gt; tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/query/kernel:0
&gt; tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/query/bias:0 tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/key/kernel:0 tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/key/bias:0
&gt; tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/value/kernel:0
&gt; tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/value/bias:0

Here it is for a full single layer

    # Full weights for only one layer 
    for i, var in enumerate(layer9.weights):
        print(layer9.weights[i].name)

The output is 

&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/query/kernel:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/query/bias:0 tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/key/kernel:0 tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/key/bias:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/value/kernel:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/value/bias:0 tf_roberta_model_7/roberta/encoder/layer_._8/attention/output/dense/kernel:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/output/dense/bias:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/intermediate/dense/kernel:0 tf_roberta_model_7/roberta/encoder/layer_._8/intermediate/dense/bias:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/output/dense/kernel:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/output/dense/bias:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/output/LayerNorm/gamma:0
&gt; tf_roberta_model_7/roberta/encoder/layer_._8/output/LayerNorm/beta:0

But all the missing layers/ weights are represented in the model summary

    model.summary()

Output (EDIT: The output breaks Stackoverflow's character limit so I only pasted the partial output, but the full output can be seen in this colab notebook https://colab.research.google.com/drive/1n3_XNhdgH6Qo7GT-M570lIKWAoU3TML5?usp=sharing )


And those weights are definitely connected, and going through the forward pass. This can be seen if you execute

    tf.keras.utils.plot_model(
        model, to_file='model.png', show_shapes=False, show_layer_names=True,
        rankdir='TB', expand_nested=False, dpi=96
    )

The image is too large to display, but for convenience this colab notebook contains all the code that can be run. The output image will be at the bottom even without running anything

https://colab.research.google.com/drive/1tbel6PueW3fgFsCd2u8V8eVwLfFk0SEi?usp=sharing

Finally, I tested the output of the keras model, and running the layers directly, they are not the same.

# Test what correct output should be 

    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    inputt = tokenizer.encode('This is a sentence', return_tensors='tf')
    outt = tempModel(inputt)[0]
    hidden1 = layer9((outt, None, None))
    layer10((hidden1[0], None, None))

vs

    model(outt)",3,tensorflow,2020-10-13
hcrtys,Difference between Tensorflow and TensorflowLite,"How is tflite inference time way quicker compared to tf? And is there any impact on the accuracy on conversion to tflite? If not, why can't tflite models be used in production instead of tf irrespective of the device being edge or not?",3,tensorflow,2020-10-13
hciqii,"Cars from Radiator Springs, generated by StyleGAN2 Architecture in Tensorflow.",,8,tensorflow,2020-10-13
hc61ym,The difference between Keras and TF,"What exactly is the difference between Keras and Tensorflow? Are they the same? If I am starting off, should I fist watch a keras tutorial or TF ?",2,tensorflow,2020-10-13
hc326u,Multilabel Classification Confusion Matrix Plot,"For tasks involving Multilabel multiclass Classification, what is the best way to create a confusion matrix and plot it?

I'm currently using multilabel\_confusion\_matrix from sklearn, it yields a 3D matrix with 2\*2 matrix for each class (tn, fp, fn, tp). 

For just a multiclass Classification problem, the confusion matrix is more readable and easy to understand, as  seen below.

[FIG: 10 Versicolor samples are classified as Versicolor and 6 Versicolor samples are classified as virginica.](https://preview.redd.it/utzsixqz0w551.png?width=640&amp;format=png&amp;auto=webp&amp;s=30f62da08d0fc06b664935b4be8fedc650823720)

Is there a way to plot a similar confusion matrix for Multilabel Classification problem?",1,tensorflow,2020-10-13
hc2e4c,We're building a labeling platform for image segmentation. Looking for feedback!,,74,tensorflow,2020-10-13
hc20jb,Saving Trained model?,"Hello, I am a novice in tensorflow and I would like to ask about saving a trained model. 
I saved a trained CNN using the command

Model.save(‘example.h5’)

And I download it using 

files.download(‘example.h5’)

But after I load it to my backend, using
Tf.keras.models.load_model(‘example.h5’) 

the model becomes so inaccurate like it has never been trained before. Any solution? Thanks",1,tensorflow,2020-10-13
hbwknl,This Week in AI - Issue #22 | Rubik's Code,,1,tensorflow,2020-10-13
hbvjrr,Finding Neuron Activations in TensorFlow2.0,"0

I was reading [Training and Evaluation](https://www.tensorflow.org/guide/keras/train_and_evaluate) (TensorFlow 2.0 and Python3) for MNIST dataset with code as follows:

    inputs = keras.Input(shape=(784,), name=""digits"")
    x = layers.Dense(64, activation=""relu"", name=""dense_1"")(inputs)
    x = layers.Dense(64, activation=""relu"", name=""dense_2"")(x)
    outputs = layers.Dense(10, activation=""softmax"", name=""predictions"")(x)
    
    model = keras.Model(inputs=inputs, outputs=outputs)
    
    model.compile(
        optimizer=keras.optimizers.RMSprop(),  # Optimizer
        # Loss function to minimize
        loss=keras.losses.SparseCategoricalCrossentropy(),
        # List of metrics to monitor
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )
    
    print(""Fit model on training data"")
    history = model.fit(
        x_train,
        y_train,
        batch_size=64,
        epochs=2,
        # We pass some validation for
        # monitoring validation loss and metrics
        # at the end of each epoch
        validation_data=(x_val, y_val),
    )
    

In this code, if I want to see the activations for the neurons, say  either during each epoch and at the end of training, how can I get each  neuron's (in each layer) activations?

Thanks!",2,tensorflow,2020-10-13
hbu4g1,How to resolve dynamic library being unable to find CUDA .dll files,"I’ve been having issues getting tensorflow to install properly on my laptop. I’ve made sure I’m using 10.1 CUDA, cuDNN, updated drivers, checking install locations, modifying long names, modifying environmental variables, trying anaconda instead of pip and factory resetting my computer. 
Any help would be much appreciated.",6,tensorflow,2020-10-13
hbtphq,6 Easy Steps to Implement a Computer Vision Application Using Tensorflow.j,,0,tensorflow,2020-10-13
hbjicf,I started a video series that will dive into audio data for machine learning,"I’m starting a new series on Audio Processing for Machine Learning 🚀🚀 In this series, you'll learn how to process audio data 🎧 and extract relevant audio features for your machine learning applications 🤖🤖.

First, you'll get a solid theoretical understanding of key audio digital signal processing topics such as the Fourier Transform, Mel-Spectrograms, and sound waves. You'll also get your hands dirty by processing audio data with the industry-standard library for audio/music processing.

I hope you’ll join me in this exciting journey!

Here’s the course overview:

[https://www.youtube.com/watch?v=iCwMQJnKk2c](https://www.youtube.com/watch?v=iCwMQJnKk2c&amp;t=1s)",23,tensorflow,2020-10-13
hbe3cj,Does tensorflow lite only capable of machine learning through arduino nano 33 ble sense and not any other boards?,Isnt 33 ble sense the same as any other arduino with sensors attached and a bit of processing power?,1,tensorflow,2020-10-13
hbdo63,Defining which GPU should be used (inference),"I have two GPU's and I'm trying to adapt my code to use the second GPU instead of the first one.  


I can not find any information about how to to do this.  


Please help me. How do I do this?

Edit: I’m using TF 1.15.2",1,tensorflow,2020-10-13
hbatc7,Feeding Matrices to Neural Network in TensorFlow,Around 1000 matrices of 20x20 are generated in Matlab. Have to feed these matrices to train a neural network. Being a noobie in Tensorflow need some guidance in achieving so in Tensorflow,0,tensorflow,2020-10-13
hbagt8,Generating cooking recipes using TensorFlow and LSTM Recurrent Neural Network: A step-by-step guide,,21,tensorflow,2020-10-13
hb1eac,Help needed on Hlhow to train image classifier with multi label in tensor flow.,,0,tensorflow,2020-10-13
havszr,Should I Randomize (shuffle) my Images in the test/train folders?,"Hi all,

I've recently learned how to use the Tensorflow Object Detection API and was able to successfully classify pictures of tigers. I've mainly been using medium articles and sentdex's six-part tutorial series to learn all about the steps of loading in custom data. Since I feel a lot more comfortable using this API, I want to move on to the next step by classifying multiple images. I understand that all I have to do is adjust the label map and the .config file.  However, since I'm using 5 labels this time (i.e 5 sets of 100 images), is it necessary to shuffle the images inside the test/train folders? 

My understanding is that if I don't shuffle, the model will be 'memorizing' instead of 'learning'.  It will be able to train well on the first 100 images (essentially memorize the first label) , but will perform poorly on the next 100.  I haven't studied the theory enough, so I would appreciate it if someone could verify this for me. 

Thanks!",4,tensorflow,2020-10-13
haukf4,Ploting parallel coordinates plot with keras tuner in Tensorboard,"I have been working with keras tuner in Tensorflow 2.0, but I couldn't find a way to make parallel coordinates plots similar to the HPARAM Dashboard in Tensorboard. Is there any tutorial or example available for making those plots for keras tuner results?",1,tensorflow,2020-10-13
hau2gu,"Looking for help with activation, loss and optimizer funtions","I started a backend for a League of Legends related stats website. Nothing new, already done a fews time ([op.gg](https://op.gg), [champion.gg](https://champion.gg), etc) but a nice learning project. Now i wanted to relate each player with the respective role ingame. Sadly the data from the Riot-API is unreliable and so i wanted to try and do it with a ML approach. I already started with Tensorflow JS because the whole backend is made with JS in Node (Take a look at my Github if you want: [https://github.com/Knniff/ML-champion-roles](https://github.com/Knniff/ML-champion-roles)). My problem now relies with the activation, loss and optimizer functions and how many layers i would need. I would love if someone could help me directly but some links to general documentation would also be helpfull. As of now I looked at the tensorflow provided tutorials on their website and the math behind the functions but sadly that didnt help me. If you want to my data structure either look at the code([https://github.com/Knniff/ML-champion-roles/blob/master/model-creation.js](https://github.com/Knniff/ML-champion-roles/blob/master/model-creation.js)) or ask me in the comments.

Thanks in advance from a tensorflow novice.

Just ask me for further info if needed.",7,tensorflow,2020-10-13
haruwq,Looking for cool machine learning projects involving reddit,"Hoping to brainstorm some ideas to practice machine learning that involves reddit.

My thought was something like title to upvotes predictor or a model that learns the patterns in a successful Reddit post and tries to replicate it.",2,tensorflow,2020-10-13
hao97e,Any tensorflow js experts?,"Hello, I am trying to incorporate tensorflow into my react native mobile app project. Currently, I have tensorflow working, but I am using a model from this article. [https://medium.com/@namar/high-performance-image-classification-with-react-native-336db0a96cd](https://medium.com/@namar/high-performance-image-classification-with-react-native-336db0a96cd) A friend who I am working on this project sent me some files and told me to use tensorflow to incorporate it. He made his own model. He sent me two files: a .tflite file and a .json file. How am I suppose to use this and incorporate it into my react native app? Please DM me! Thank you!",4,tensorflow,2020-10-13
hao1ws,Video Classification Project,"Hello! I am really new to TensorFlow and Machine learning.

I have done a Project with TensorFlow and LSTM where I classify Sport Videos, it worked decently well. In the project I had lots of training data from Sports 1 Million (I did not use all of it of course) and lots of different classes. As I said above it worked decently well, but some of the sports the Neural Network mixed up often and it was really slow to analyse a video (probably due to old hardware), a 10 minute video took \~35 minutes to analyse.

Through some more research and reading papers I found out that LSTM is not the best option. I have heard from the relatively new Transformer neural network, from GRU and from NetVLAD. 

In my new Project I have a machine which test a chip, the result is a video where we can see if the test was successful or not. I want to classify this video with machine learning. I have lots of examples for how a video looks when the Test does not work or when it works. This time it must be very accurate and if possible quick and the videos are about 1-2 minutes long.

So, my question is what Neural Network would you advise me to use? LSTM, Transformer, GRU, NetVLAD or even something completely different?

Sorry for any spelling mistakes.",4,tensorflow,2020-10-13
ham7fv,Can I code in TF V2 and have it run along with V1 code?,"Hi, so I have been learning tensorflow 1, because I recently got an opportunity to work on a project. However the previous codes have been coded in TF V1, and although, I haven't had trouble learning V1 till now, I think V2 is obviously much better.

So, I want to know. Is there a way I can code entirely in V2, and have it run with previous codes in V1. How much time and effort will making the codes compatible take? First of all, is it even possible?

Since I'll be editing some of the previous V1 codes, is it simply better to continue learning TF 1? 

Please let me know. Thanks.

PS: Please note that the members of the group won't be upgrading the previous code to version 2. So its sort of reverse compatibility.",1,tensorflow,2020-10-13
hacck6,Linking MATLAB double to python matrix manipulation function,"Hi!

I am trying to link MATLAB double (matrix of 10000x100 size) to Python function that is supposed to take a function, and return a double. But I have encountered an error saying that Unable to resolve the name py.script. What script contains is a number of preprocessing functions and tensorflow loaded model that is supposed to receive a matrix from matlab and produce an y\_hat.

Does anyone know what could be possible issue?

EDIT:

So I kind of found what exactly was causing the error.

As soon as I enable importing **tensorflow** and/or **keras** libraries in .py script, I observe the same error, saying it is unable to resolve the line that calls the python function.

So i tried downgrading **tensorflow from 2.2.0 to 2.0.0**, and also h5py, and I observed no error. However, MATLAB kept crushing every time I wanted to execute the code.

Does anyone know how I can rectify the error or bypass around it?

I appreciate your feedback. Please let know if your require more specific info. Thanks!",0,tensorflow,2020-10-13
ha6g88,How to export the back-prop' graph in TF2,"Can any one help with how to export the back-prop' graph in TF?

[https://stackoverflow.com/questions/62399405/exporting-back-propagation-graph-in-tensor-flow-2](https://stackoverflow.com/questions/62399405/exporting-back-propagation-graph-in-tensor-flow-2)",1,tensorflow,2020-10-13
ha35ch,How I passed the Tensorflow Developer Certificate Exam,"If there is anyone planning to take the TF Developer Certificate exam, I've compiled all the resources and reviewed each one of them based on their usefulness and learning value. There is a bunch of things I did and I could capture all of those in [this story.](https://towardsdatascience.com/google-certified-tensorflow-developer-learning-plan-tips-faqs-my-journey-9f88016048e3)

I am open to any questions you might have regarding the exam.  
Cheers!",39,tensorflow,2020-10-13
h9zcif,LSTM,"I have a very little experience with Tensorflow's LSTM layers(I have used CNN fron past). But I have noticed it takes a little longer time than any other layers and even on GPU. 
I made a experiment scenario: 
I used python3.6( With tf-cpu==1.15) and python3.8(with tf-gpu==2.2 cuda10.1, gtx1050ti). The CPU version was faster by 2 times. I have never seen such big ratio, in comparison tith GPU. 
The batch_size did effect but as it did for cpu too(the ratio sinked down to 1.1-1.2). 
I feel very down today as I don't have indepth knowledge about rnn/lstm/Gru. Can somebody suggest me sources to read about same topic? 

I am no researcher but I would like to know why did the above scenraio happen...


Thank you .",2,tensorflow,2020-10-13
h9tvgm,Streaming Tensorflow Inference from Webcam,"I'm trying to implement one of those background blur filters you see on teleconference software like zoom these days.

I've downloaded a pre-trained Deeplabv3 and utilized the code in this post as a starting place [https://averdones.github.io/real-time-semantic-image-segmentation-with-deeplab-in-tensorflow/](https://averdones.github.io/real-time-semantic-image-segmentation-with-deeplab-in-tensorflow/)

I got it working, however it was taking \~500 ms loop to

1. Get the image from camera (\~47ms)
2. Resize image to appropriate resolution (\~20ms)
3. Perform inference (\~224ms)
4. Build Person mask (\~28ms)
5. Blur background and compose final image (\~159 ms)

This is obviously too slow for real-time use, so I though maybe I could at least squeeze the pre/post processing onto the GPU for a bit of gains, and that brings me to \~240 ms

Cythonizing the whole process didn't improve it beyond this

This is only about 4 FPS which is much too slow.

My next idea is possibly to use tfcompile: [https://www.tensorflow.org/xla/tfcompile](https://www.tensorflow.org/xla/tfcompile) but I'm skeptical about how much it can really help.

I feel like if there was a way to 'pipeline' this analysis where one frame begins processing before the previous finishes would help. In that way you could get a normal frame-rate with a bit of a delay. Another way of achieving this would be a thread which reads the camera, then sends them to a worker pool that processes the images, and then another thread which writes the images.

Anyway, any recommendations from folks would be appreciated!",2,tensorflow,2020-10-13
h9rk2o,Just followed style transfer tutorial switching the golden retriever with a picture of my dog 🥰 (predicted by vgg19 to be a bernesse boyer with probability .96! Amazing!),,26,tensorflow,2020-10-13
h9kgqy,Watch This Before Writing Neural Networks,,0,tensorflow,2020-10-13
h9hxk0,How to get tensorflow developers certificate??,,7,tensorflow,2020-10-13
h9e78h,Would anybody mind helping me implement beam search in this tensorflow example,"I am working on a fun project and suddenly realize one of the problems I need to solve needs ML, I have never used ML, I plan on learning it now having discovered how cool it is but I would like to solve this problem first without having to spend months learning and trying to understand the ins and outs of ML and Tensorflow, it is non-trivial for me, a novice but I assume it's pretty simple for you.

This seq2seq transformer code is perfect for my needs but I have no idea how to implement beam search(to list top N variants) in it:

[https://www.tensorflow.org/tutorials/text/transformer](https://www.tensorflow.org/tutorials/text/transformer)",4,tensorflow,2020-10-13
h9do3p,Save/Load Encode-Decode Model,"Hello,

I am still learning TensorFlow. 

Today I have created a encode decode network.

So, after fitting the model, I've saved it using `model.save``(""./my_model.h5"")`

Then I've loaded it using  `mymodel = tf.keras.models.load_model(""./my_model.h5"")`

My question now is, How can I access to the **encoder** and to the **decoder** separately from `mymodel` variable?

many thanks",2,tensorflow,2020-10-13
h9blzf,Infomercial on slouching detector made using tensorflow.js (teachable machines boilerplate),,20,tensorflow,2020-10-13
h8o4me,What the difference between using activations writhing a conv2D layer as opposed to after?,"Noob here, I was reading “Advanced Deep Learning with TensorFlow 2 and Keras” by Rowel Atienza and I’m up to a part where they start to use the functional api to explain resnet. My question is that the code in the book will be something like this

    ...
    x = Input(shape=input_shape)
    x = Conv2D(num_filters,
               kernel_size=kernel_size,
               strides=strides,
               padding='same',
               kernel_initializer='he_normal',
               kernel_regularizer=l2(1e-4))(x)
    x = Activation(""relu"")(x)
    x = Flatten()(x)
    ...

But the way i have been using the functional api would have been as follows

    ...
    x = Input(shape=input_shape)
    x = Conv2D(num_filters,
               kernel_size=kernel_size,
               strides=strides,
               padding='same',
               kernel_initializer='he_normal',
               kernel_regularizer=l2(1e-4),
               activation=""relu"")(x)
    x = Flatten()(x)
    ...

Is there any difference between these two? if so, can you explain how?",1,tensorflow,2020-10-13
h8n9lw,Issues while training FCN for Semantic Segmentation with Keras,"As the title says, I'm trying to implement a FCN from VGG16 for semantic segmentation of road images training with Kitti Dataset. However, when training, after a few epochs and with loss = 829.17, acc = 0.2911, mse  = 17368.0488, the next epoch it goes to loss = nan, acc = 1.0000  and mse = nan.
I'm using Categorical Crossentropy as cost function, and Adam with lr = 0.00002  as optimizer.
Batch size = 32
Steps per epoch = 6
Epochs = 75 (it goes nuts before it reaches 10 epochs)

What could be happening and how do I fix this?",5,tensorflow,2020-10-13
h8hb6v,TypeError: Unexpected keyword argument passed to optimizer: name,"When i am loading the model this error occurs and i don't know whether it is from mismatch version of tensorflow and keras or some other . My tensorflow version is 2.1.0 and keras version is 2.3.1 . Both are latest versions . Somebody please tell me the answer of it ?

import cv2  
import numpy as np  
import tensorflow as tf  
import keras  
from keras.models import load\_model  
from keras.utils import CustomObjectScope  


print(tf.\_\_version\_\_)  
print(keras.\_\_version\_\_)  
Categories = \[""Badshahi Masjid"", ""Minare Pakistan"", ""ShahiQila(Lahore Fort)""\]  
sift = cv2.xfeatures2d.SIFT\_create()  


def prepare(filepath):  
IMG\_SIZE = 124  
 img\_array = cv2.imread(filepath, cv2.IMREAD\_GRAYSCALE)  
new\_array = cv2.resize(img\_array, (IMG\_SIZE, IMG\_SIZE))  
keyImage, desImage = sift.detectAndCompute(new\_array, None)  
feat = np.sum(desImage, axis=0)  
 return feat  


from keras.initializers import glorot\_uniform  


with CustomObjectScope({'GlorotUniform': glorot\_uniform()}):  
model = load\_model(""mlp\_model.h5"")  


prediction = model.predict(\[prepare('E:\\Python Telusko\\OpenCv\\minare1.jpg')\])  
print(prediction)  
print(Categories\[int(prediction\[0\]\[0\])\])",3,tensorflow,2020-10-13
h8be9j,Calling saved tf.model from MATLAB,"Hi everyone!

I need to send a matrix from MATLAB into python script that is supposed to process that as the input and produce y\_hat. Subsequently, this y\_hat would be returning to MATLAB script to go through further processing. Unfortunately, i keep receiving error stating that .py was not able to resolve the function call. I also tested python function on its own, and observed no errors. 

Does anybody know what could be the issue in my case?",2,tensorflow,2020-10-13
h88ni3,Converting to .tflite - input and output tensors.,"Hi all, after much struggle I managed to train my own model thanks to Roboflow (great website). I was able to get a .pb file and test it against my own images. It's nowhere near perfect but it works.

Now I'm trying to convert the .pb to .tflite so I can use it on Android but I can't find my input and output tensors.

I ran a script that gave me my input tensor as ' image\_tensor' which I think is correct but my output tensor is given as 

`'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Switch',`

`'raw_detection_boxes',`

`'MultipleGridAnchorGenerator/assert_equal_1/Assert/Assert',`

`'detection_boxes',`

`'detection_scores',`

`'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/TopKV2',`

`'detection_multiclass_scores',`

`'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert',`

`'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Switch_1',`

`'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Assert/Assert',`

`'detection_classes',`

`'num_detections',`

`'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/TopKV2',`

`'Preprocessor/map/while/Switch_1',`

`'Preprocessor/map/while/Switch',`

`'raw_detection_scores'`

No matter what combination I try I can't get it to work?

Any help?",2,tensorflow,2020-10-13
h82t86,Question about loading npy file as weights in a model,Hello. I’m working with the PSPNet50 for scene segmentation. I downloaded and loaded a NPY file that contains the array meant to hold the weights for the pre trained model I’m working with. How would I go about getting those weights from the array into a model I can pass arrays through? I looked online but couldn’t find an answer(it’s likely I’m not looking correctly). Sorry if there’s a specific thread where I’m supposed to be posting questions like this.,1,tensorflow,2020-10-13
h82q9g,How to Distributed Prediction,"Hi. I want to run distributed prediction on my GPU cluster. I trained a CNN made with Keras using MirroredStrategy and saved it. I can load the model and use .predict() on it, but I was wondering if this automatically does distributed prediction using available GPUs. If not, how can I run distributed prediction to speed up inference and use more GPU memory? Thanks.",3,tensorflow,2020-10-13
h7zuyt,Lightweight OpenPose in TensorFlow 2,"Hi guys,

[Lightweight OpenPose](https://github.com/minhhoangbui/lightweight_openpose_tensorflow)

I've been trying painstakingly for the last 3 weeks to implement an available architecture in TensorFlow 2. The architecture is nothing fancy but I'm pretty satisfied of my implementation. I've done everything which I consider essential. Surely, it's far from perfect but I hope it could help you somehow.  


  


https://preview.redd.it/xcxlm7zcml451.jpg?width=700&amp;format=pjpg&amp;auto=webp&amp;s=3e4cdbe05841818a4b9ff70b05e1595cbdb8709c",1,tensorflow,2020-10-13
h7ymmj,Latest from Microsoft researchers: Recovering the 3D geometry of human head from a single portrait image,,0,tensorflow,2020-10-13
h7u5ka,Keras with C++ on Windows 10 64-bit,"Hi everyone,

New member of this subreddit, I have used Keras (Tensorflow) to run predictions on a set of images within a given folder using a pre-trained ONNX Model. On top of this, I ran predictions by using a polling watchdog python script to check for changes in the given folder, and run predictions to the images accordingly. Everything works, albeit a bit slower than ideal. 

Although it works fine, my supervisor told me to see if I could import the model to keras in C++ and then run predictions on all the files in a given folder (no polling watchdog). Has anyone here ran custom models in keras using c++ on windows 10 64 bit machines?",3,tensorflow,2020-10-13
h7s8eh,"Is it possible to overlay graphics, specifically a 3D model, on a hand using Tensorflow and Handpose?","Is it possible to overlay graphics, specifically a 3D model, on a hand using Tensorflow and Handpose?",1,tensorflow,2020-10-13
h7pycj,Developer Certificate,"Hey guys,

I am thinking about doint the Tensorflow Developer Certificate to round up my Resume. 
At the moment I am a student and I could not pay so much money for this.
I read that on coursera there is a recommended course which is 59$/Month for preparing for the Certificate. I also read that there is a Video series from Tensorflow on Youtube.

My Question is, if the Video series on Youtube is the same as the coursera course? I dont need the coursera certificste. I would only go through the topics and code the examples by myself. 

Thanks in advance",1,tensorflow,2020-10-13
h7pmwz,Question about training models and PC Hardware,"Hi, I'm very new tensorflow and am trying to teach myself how to train models.  My PC is about 7-8 years old, so as you can expect, it takes a long time to really do anything.  I've been toying with replacing the desktop with a laptop with a nvidia RTX 2060 and my wife would prefer a laptop to a desktop+monitor.  
  
  
So I guess my two questions are these.  Does anyone use a laptop for model training because I'd imagine using a GPU in a laptop for the ML would cause a ton of heat in a small package which wouldn't be good over the long term.  And then also, if I were to buy a RTX 2060 card and slap it in my existing desktop, would it still run slow due to my old CPU (amd fx 6350)?  
  
  
Thanks in advance!  And once I figure out the hardware situation, I fully expect to be back with actual TF related questions when I undoubtedly feel like an idiot with some of this!",0,tensorflow,2020-10-13
h7pezr,"John Snow Labs Spark-NLP 2.5.2: New Language Detection annotator, enhancements, and bug fixes",,2,tensorflow,2020-10-13
h7pcu7,Error while installing TensorFlow,"I got these errors when I ran pip install TensorFlow

ERROR: mysql-connector-python 8.0.19 has requirement protobuf==3.6.1, but you'll have protobuf 3.12.2 which is incompatible. 
ERROR: google-auth 1.16.1 has requirement setuptools&gt;=40.3.0, but you'll have setuptools 40.2.0 which is incompatible. 
ERROR: tensorboard 2.2.2 has requirement setuptools&gt;=41.0.0, but you'll have setuptools 40.2.0 which is incompatible.

I am an absolute beginner
Can somebody help me?",1,tensorflow,2020-10-13
h7ovoy,"New to tensorflow, I need your help!","Hello, I am a first time user of TensorFlow. I am trying to incorporate tensor flow into my react native app. For now, I followed this tutorial [https://heartbeat.fritz.ai/image-classification-on-react-native-with-tensorflow-js-and-mobilenet-48a39185717c](https://heartbeat.fritz.ai/image-classification-on-react-native-with-tensorflow-js-and-mobilenet-48a39185717c) and I was able to make the most basic form of tensor flow working. However, I am currently trying to use Google cloud services's auto ML vision feature and use that as a model and incorporate it into my app. How can i accomplish this? In short, how do I incorporate a model that is connected to Google Cloud Services' auto ML Vision feature into my react native app? If you are an expert, please comment or DM me! Thank you!",1,tensorflow,2020-10-13
h7ovbp,Limiting resource usage of Training,"Hello everyone,

so I have a question about limiting the resources a tensorflow model uses during training.

The reason is, that my notebook (with a nvidia 1060) is turning really hot, so that you can barely touch the side where the gpu sits. And I am having a bit of a fear that my notebook might take some damage from that.

I already saw that you can limit the memory usage, but I'm not sure whether that will actually lead to less heat or not.

&amp;#x200B;

So is it possible to trade resource usage vs longer training time ?

&amp;#x200B;

Thanks in advance,

Best regards",1,tensorflow,2020-10-13
h7oezt,Tensorflow pictures to predict the time of day,"Hello,

I am looking for some advice on TensorFlow. I have just followed a tutorial from the TinyML book where you use a sequential model to give the model x and it predicts Y on a sin.

I would now like to use images of a conservatory roof and the time they were taken to try to train a model to predict what time of day it is. as I thought this would be a good learning exercise. Here is the dataset on Kaggle.

https://www.kaggle.com/blimp10/conservatory-photos  

My questions are,

can I still use the sequential model? or should I use another model? 

Has anyone done something similar that you know I could look at?

Is this a correct application for TensorFlow/ml?",3,tensorflow,2020-10-13
h7l2a6,Tensorflow tutorial,"Hey everyone, 
I am new to tensorflow. I have read and understood the basic theory behind neural networks and how they work, now I need to learn how to implement those with tensorflow. 
Any recommendations/YouTube tutorials/articles to learn that?",2,tensorflow,2020-10-13
h7kx4q,"The accuracy is always exactly 0.20000, help?","code: [https://gist.github.com/ashtasht/5f7ae4bd7f918c2bb0d860907c3c3512](https://gist.github.com/ashtasht/5f7ae4bd7f918c2bb0d860907c3c3512)

Hi, I just tried to program something that will guess who wrote each text given. Right now it's a simple classifier LSTM with a neuron to each person it was trained on in the last layer (no triplet loss).

This is one of my first attempts to use Tensorflow, but it doesn't work:

When training it shows me a constant accuracy of 0.2000 (I trained it on 5 files) while training. Also, before I changed the code a bit, it did train correctly but the results were always the same or similar.

Do you know what have I done wrong or how can I optimize the model?

Thank you

btw the data was in a folder named ""data"", I just put a few Whatsapp chats histories in different files there.",3,tensorflow,2020-10-13
h7j9g4,Replicating Face Mask Detection,,96,tensorflow,2020-10-13
h7hw7u,This Week in AI - Issue #21 | Rubik's Code,,3,tensorflow,2020-10-13
h7ckkr,From SIGGRAPH 2020: Method reconstructs the geometry of complex 3D thin structures in high quality from a color video captured by a handheld camera,,5,tensorflow,2020-10-13
h79obq,How to use model loading for object detection code,"It looks to me like the Loader code in the object detection tutorial code here:

[https://github.com/tensorflow/models/blob/master/research/object\_detection/object\_detection\_tutorial.ipynb](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb)

is out of date. When I attempt to run something substantially similar I end up with a Python error of:

&gt;model = model.signatures\['serving\_default'\]  
&gt;  
&gt;AttributeError: signatures

As I'm new to the whole thing, I have no idea what I'm looking for or what the right approach is. The object\_detection code says its targeted at TFv1 and I have v1.14.0 installed.

Thanks!",1,tensorflow,2020-10-13
h16g04,Is this possible in tensorflow. Looking for help.,"Hi all, 

I am wondering if TensorFlow would be able to be used to determine the suitability of a job for a potential candidate. 

The thought process is that based on the static employee profile, you could feed create a model that would predict the suitability and chance of hiring for that potential candidate. 

For example, you would have attributes for the candidate such as salary requirement, years of exp, location, managerial experience, team size etc.

From this, you could then determine the suitability of the job on offer for that candidate.

Unlike a simple model, say using applicant details to determine if they are eligble for a loan. This has two moving parts - the candidates and the jobs. This is what has me a little confused.

Is this something that is currently possible in TensorFlow? What would the approach for something like this look like? Any and all comments welcome.

Thanks",5,tensorflow,2020-10-13
h13mm0,Getting input shape right,"Suppose I have 2 one dimensional array :

`X = [1, 2, 3, 4, 5]`

`y = 10*X`

`model = keras.Sequential([keras.layers.Dense(units = 1, input_shape = [1])])`

`model.compile(loss='mean_squared_error', optimizer = 'sgd')`

[`model.fit`](https://model.fit)`(xs,ys, epochs = 500)`

My query is when I set input shape = \[1\] doesn't it require at least one column? In my array there is no column so does the [model.fit](https://model.fit)() perform reshape(-1,1) when it detects that I'm passing a 1D array. Is the above code similar to the following:

`X = [1, 2, 3, 4, 5].reshape(-1,1)`

`y = 10*X`

`model = keras.Sequential([keras.layers.Dense(units = 1, input_shape = [1])])`

`model.compile(loss='mean_squared_error', optimizer = 'sgd')`

[`model.fit`](https://model.fit)`(xs,ys, epochs = 500)`

Just that the np.reshape(-1,1) is redundant",1,tensorflow,2020-10-13
h11a60,"TF 2.2 verbose fitting - is it discarding a ton of samples, or just displaying batch count now","Hello,

&amp;#x200B;

I just switched from TF 2.1 on windows, to TF 2.2 on linux to get around a problem with recurrent networks crashing while fitting.

&amp;#x200B;

As I'm sure many of you have seen, when verbose is turned on for fitting, you might see something like this when fitting begins:

&amp;#x200B;

\*Train on 30000 samples, validate on  5000 samples

Epoch 1/100

22390/30000 \[=================&gt;...........\] - information about speed and loss here\*

&amp;#x200B;

Now, in 2.2, training the same networks, with the same datasets, I don't see that ""Train on x samples, validate on y samples"" message at all, and the progress fraction next to the loading bar shows a much smaller number.

&amp;#x200B;

I'm assuming that before it showed (samples looked at so far / total samples) and now it's showing (\*batches\* looked at / total \*batches\*). But hey, maybe it's doing something else... like discarding most of the available samples, and I'd like to be sure it's not that. Does anyone know what's actually happening here?",1,tensorflow,2020-10-13
h0sl87,I'm planning to take the TensorFlow developer certificate exam and I have a few questions.,"Hey guys! I'm planning to take the TensorFlow developer certificate exam and I have a few questions.

1. Can I schedule the exam to be on a specific date?
2. Is the certificate worth it?
3. What's the difficulty level of the exam?
4. What's the TensorFlow certification network?
5. Does it help in getting better jobs?

If anyone here has taken up the exam, please let me know. I would love to talk more.",3,tensorflow,2020-10-13
h0l479,Any good guides for training your own model with images that you drew detection boxes on?,"I have tried Edje Electronics guide to fine tune my own model, but The guide is so outdated, it leads to hours of troublshooting errors.

Are there any good guides out there to train my own custom detection model?",8,tensorflow,2020-10-13
h0l304,is not available in checkpoint Error,"Does anyone know why I get this error when using: 

python [train.py](https://train.py) \--logtostderr --train\_dir=training/ --pipeline\_config\_path=downloadedmodel/pipeline.config

With: faster\_rcnn\_inception\_v2\_coco\_2018\_01\_28

**ERROR:**

W0610 17:40:49.988005 15800 variables\_helper.py:164\] Variable \[FirstStageFeatureExtractor/InceptionV2/Conv2d\_1a\_7x7/BatchNorm/beta\] is not available in checkpoint

Following this guide:

 [https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10)",1,tensorflow,2020-10-13
h0fm0m,What is the intended use of overwriting the train_step() method?,"I have tried different combinations, non of them are working. I have tf.keras.Model subclass, where my forward logic is defined in call() and the training logic is defined in train_step().

How should I call the fit() function, so that everything works as expected? Should I pass my generator directly to fit()? Or should I wrap it first with tf.data.Dataset()?

My train_step() is not following the conventional (inputs, ground_truths) signature by the way, I am passing tensors and the ground truth is produced inside the train_step() function, outside of the GradientTape's closure.

Thank you for any help in advance.

Edit: so after some experimentation, I think this question boils down to the following: does the 'data' parameter present in train_step() have a specific signature? Or can I pass a nested tensors of arbitrary configuartion?",2,tensorflow,2020-10-13
h011un,DeepFaceDrawing system allows users with little training in drawing to produce high-quality face images,,15,tensorflow,2020-10-13
gzye1l,Help in understanding steps_per_epoch in keras,"While following an online tutorial to understand how to code a CNN the person went to the keras site and copied and pasted this code from data preprocessing into their program

&gt;classifier.fit\_generator(training\_set,  
&gt;  
&gt;steps\_per\_epoch = 1000,  
&gt;  
&gt;epochs = 25  
&gt;  
&gt;validation\_data = test\_set,  
&gt;  
&gt;validation\_steps = 1000, verbose = 1)

Training and test set both have 32 batches, so the step\_per\_epoch should be (training\_set/batch\_size), but the didn't use that value instead they kept it at 1,000 and got very good results. I tried replicating the experiment with datasets I already had using my trainig\_sets = 224, batch=32, step\_epoch =7, and got poor results. I then set it to 1,000 and my accuracy was 98% and loss was 5%.  My model even performs well on data it has not been trained on.

I am having a hard time understanding what is happening, as no explanation was given on why they left it as it is.  

Am I misunderstanding and misusing the formula? 

Assuming I am not, what is happening to data given the additional steps I added?",4,tensorflow,2020-10-13
gzgha4,Why you can use classification metrics and loss on regression problem?,"Recently i'm looking for example on regression problem which use tf.keras and i found out few people use classification metrics (accuracy) and loss (binary_crossentropy) on regression problem.
An example : [https://www.kaggle.com/subhamoybhaduri/approaches-of-nlp-and-sentiment-classification](https://www.kaggle.com/subhamoybhaduri/approaches-of-nlp-and-sentiment-classification)

My questions are :
* Why it's possible?
* How it works on regression problem?",1,tensorflow,2020-10-13
gzg1yk,Please do check out my blogpost on an Intro to all things TF2,“Tensorflow 2.0 — from preprocessing to serving (Part 4)” by Tanmay Thakur https://link.medium.com/GHqlFMDua7,2,tensorflow,2020-10-13
gzex8d,I am getting multiple issues with my Keras binary classification model and I am not sure why ...,"I am getting many issues at the moment and I am not sure why or how to remedy them: 1) Extremely high loss at times at numbers ranging from 1-100 2) My accuracy plateaus at around 70% no matter what I try 3) It takes around 10 MINUTES PER EPOCH at the current parameter settings when I have a total of 400 images per class and only 2 classes

&amp;#x200B;

P.S. I am running this is on a CPU so that may be some explanation as to why it is running so slow, but I have 16 GB of ram and it appears at around 75% usage in my system monitor

&amp;#x200B;

My code is copied below - 

&amp;#x200B;

from keras.preprocessing.image import ImageDataGenerator, image 

from keras.models import Sequential

from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense 

from keras import backend as K

from matplotlib import pyplot as plt

import numpy as np 



img\_height, img\_width = 1000, 1000 



train\_data\_dir = ""data/train"" 

validation\_data\_dir = ""data/validation"" 

nb\_train\_samples = 400 

nb\_validation\_samples = 100 

epochs = 10 

batch\_size = 20 



if K.image\_data\_format() == ""channels\_first"": 

input\_shape = (3, img\_width, img\_height) 

else:

input\_shape = (img\_width, img\_height, 3)



train\_datagen = ImageDataGenerator( 

rescale = 1. / 255,

shear\_range = 0.2,

zoom\_range = 0.2,

horizontal\_flip = True)



test\_datagen = ImageDataGenerator(rescale = 1. / 255)



train\_generator = train\_datagen.flow\_from\_directory( 

train\_data\_dir,

target\_size = (img\_width, img\_height),

batch\_size = batch\_size,

class\_mode = ""binary"")



validation\_generator = test\_datagen.flow\_from\_directory(

train\_data\_dir,

target\_size = (img\_width, img\_height),

batch\_size = batch\_size,

class\_mode = ""binary"")



model = Sequential()



model.add(Conv2D(8, (7, 7), input\_shape = input\_shape, activation = ""relu"")) 

model.add(MaxPooling2D(pool\_size = (2, 2))) 



model.add(Conv2D(16, (5, 5), input\_shape = input\_shape, activation = ""relu""))

model.add(MaxPooling2D(pool\_size = (2, 2)))



model.add(Conv2D(32, (3, 3), input\_shape = input\_shape, activation = ""relu""))

model.add(MaxPooling2D(pool\_size = (2, 2)))



model.add(Flatten()) 

model.add(Dense(32, activation = ""relu"")) 

model.add(Dropout(0.2)) 

model.add(Dense(1, activation = ""sigmoid"")) 



model.summary()



model.compile(loss = ""binary\_crossentropy"", 

optimizer = ""rmsprop"",

metrics = \[""accuracy""\])



history = model.fit\_generator( 

train\_generator,

steps\_per\_epoch = nb\_train\_samples // batch\_size,

epochs = epochs,

validation\_data = validation\_generator,

validation\_steps = nb\_validation\_samples // batch\_size)",1,tensorflow,2020-10-13
gzeum5,How to output a differentiable binary layer?,"I'm developing a generator model to generate binary image which only has ""0"" or ""1"" as the value of each pixel, but only found using softmax to approximate it. I wonder if there is a layer which could directly produce binary output which is also differentiable.",6,tensorflow,2020-10-13
gzdjvr,How to develop a tf.data object from a generator that only outputs a list of arrays?,"I am trying to develop a tf.data object that yields a list of arrays, but I am getting a data mismatch error. Here's my attempt

    def labelGen():
        yield tf.constant([1, 0], dtype=tf.int64), tf.constant([1, 0], dtype=tf.int64), tf.constant([0, 1], dtype=tf.int64), tf.constant([0, 1], dtype=tf.int64)
    
    Labeldataset = tf.data.Dataset.from_generator(
         labelGen, (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64), ([], [], [], [], []) )
    
    list(Labeldataset.take(1))

And this is the error I get 

&gt;InvalidArgumentError: TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64), but the yielded element was (&lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])&gt;, &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])&gt;, &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])&gt;, &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])&gt;).
Traceback (most recent call last):",3,tensorflow,2020-10-13
gzd80i,"Tensorboard, plot curve on the command line","Hi, sometimes launching a TB process is not that easy especially when user-set port and IP are denied by the cloud machines. I found  gnuplot ([http://zenonharley.com/gnuplot/cli/2015/06/29/graphing-data-from-the-command-line.html](http://zenonharley.com/gnuplot/cli/2015/06/29/graphing-data-from-the-command-line.html)) can plot the curve on the command line, I just want to know if TB can plot curve on the command line?",1,tensorflow,2020-10-13
gz5qvs,Machine learning in Google Sheet with Tensorflow.js and Google Apps Script,,11,tensorflow,2020-10-13
gyztyh,Is tf.stop_gradient equivalent to freezing the layers behind the stopped gradient?,"Basically the title, when I say freezing the layers I mean not applying gradients to those layers by explicitly not passing them to the optimized train_var_list",3,tensorflow,2020-10-13
gyzf44,Does tf.stop_gradient save gpu memory?,"Hello, I'm working on a large network that I plan on training in a multistage manner. So after I train the first stage of my network I plan on freezing those layers and training the second stage with the first stage output as the second stage input. Would using tf.stop_gradient actually save any you memory as opposed to just applying gradients to the second stage layers only? I'm asking since maybe tensorflow might internally avoid storing intermediate outputs that may be necessary for gradient computation",1,tensorflow,2020-10-13
gyz7jr,5 YouTube Channels to Improve your AI Audio/Music Skills,"I published a video where I suggest 5 YouTube channels that you should check out to pick important skills to become an AI audio/music engineer.

Here's the video:

https://www.youtube.com/watch?v=D3j4mIqy50U&amp;list=PL-wATfeyAMNpd8nGIxJKLpTV5zpOWyLZC&amp;index=5

I hope you enjoy it!",10,tensorflow,2020-10-13
gytjth,Is there a way to set an upper-bound for predictions?,"Hey everyone, I am wondering if there is a way to always restrict the values of my model predictions to be below a value? Say if my input and output has the same dimension, and I know that my output has to be smaller or equal to the corresponding input. Is there a way in tensorflow to enforce this? 

Thank you all so much for helping!",1,tensorflow,2020-10-13
gyss58,Help with using TensorFlow on GPU in anaconda?,"What is the command to run something on the GPU. I know for Pytorch its .cuda(), is it the same for TensorFlow?",1,tensorflow,2020-10-13
gyrtqu,"Training on gcloud, missing Training checkpoints from DNN Estimator?","I've been doing a couple of DNN estimator iterations when all of a sudden one of my runs did not return back any training loss when viewing  tensorboard. The only change was a change in hyper-parameter: number of hidden units. **Does anyone know what might be the cause for training data loss to not show up in tensorboard, and if there is any way I can get the training data without re-running the job?** The job takes approximately 1 hour to re-run. 

Here's how I initiate my model: 

    ...
    
    run_config = tf.estimator.RunConfig(save_checkpoints_steps=1000,
                                        save_summary_steps=1000,
                                        tf_random_seed=42,
                                        )
    
    ...
    
    def build_estimator(feature_columns, model_dir, config):
    
        return tf.estimator.DNNClassifier(
            feature_columns=feature_columns,
            hidden_units=args.hidden_units,
            n_classes=len(target_list),
            optimizer=tf.compat.v1.train.AdamOptimizer(
                learning_rate=args.learning_rate,
            ),
            label_vocabulary=target_list,
            batch_norm=True,
            model_dir=model_dir,
            config=config)
    
    ...
    
    estimator = build_estimator(feature_columns=embedding_columns, model_dir=args.job_dir, config=run_config)
    
    ...

&amp;#x200B;

https://preview.redd.it/r0rvsqbh4m351.png?width=632&amp;format=png&amp;auto=webp&amp;s=c295e7419b69cac6a11401bdd38f3d00375816d3

https://preview.redd.it/1zaarocb4m351.png?width=910&amp;format=png&amp;auto=webp&amp;s=eb0304196ddcd58102235e6fb00e3e581e8d4eab",3,tensorflow,2020-10-13
gyigwo,How I passed the TensorFlow Developer Certification Exam,,28,tensorflow,2020-10-13
gyc9qy,Accuracy values when predicting (Tensorflow vs Tensorflow JS)," I built my model using transfer learning from VGG16, I have an accuracy near 90%.

I did convert it to TFJS, and when comparing results for pictures ... prediction values looks different.

Below my code and result with an example  :

In notebook (python)  :

&gt;xtensor = xtensor.astype('float32') / 255  
prob = model.predict(xtensor)  
print(prob)  
=&gt;  \[\[0.63048106\]\]

In browser (chrome) with TFJS with same approach

&gt;xtensor = xtensor.expandDims(0).div(255);  
var prob = model.predict(xtensor);  
console.log(prob.print());  
=&gt;\[\[0.6505585\],\]  (higher than value in python)

In browser (chrome) with TFJS : (normalization hint found here [https://stackoverflow.com/a/55285965/12828362](https://stackoverflow.com/a/55285965/12828362) )

&gt;xtensor = xtensor.expandDims(0).sub(127).div(127);  
var prob = model.predict(xtensor);  
console.log(prob.print());  
=&gt;\[\[0.5954741\],\] (lower than value in python)

is this normal ?  also I didn't apply any quantization when converting from tf to tfjs.",1,tensorflow,2020-10-13
gy9nc6,Getting tf.matmul(..) dimensions right,"While using the tf.matmul for tensors of rank 3 or more, I really get confused about the output dimensions. I am still a beginner and found that the way matmul works is different from the tensor multiplications we study in mathematics (correct me if I'm wrong here). 

There's this `Op: BatchMatMul` error that I keep getting. It turns out that while multiplying two rank 3 tensors, the first dimension size must be same since tf considers it the batch size.

Is there a comprehensive tutorial on how to get tf matmul dimensions right and how it really works? The documentation is really not that great. It's a one liner on the way this works. It would be great if someone can explain this or link a tutorial for the same. I couldn't find any tutorials that explain this in detail. Thanks.

Edit: I am using tf version 1.12 if that's relevant. I assume that matmul works the same way even in v2.0.",7,tensorflow,2020-10-13
gy6kvz,Tensorflow is not recognizing the GPU in my laptop,"Sorry if this question seems very basic. I am new to tensorflow and I couldn't manage to make tensorflow recognize gpu. I want to install tensorflow 2.2 ( the current stable version) on Ubuntu 16.04.

Here's how I installed and configured everything

1. I installed Nvidia drivers first. I have a RTX 2060 in my laptop and the latest driver that I found for this is nvidia-450. After installing I rebooted the device and here's the output of `nvidia-smi`

&amp;#8203;

    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  GeForce RTX 2060    Off  | 00000000:01:00.0 Off |                  N/A |
    | N/A   41C    P8     4W /  N/A |    206MiB /  5934MiB |      7%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+
                                                                                   
    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    |    0   N/A  N/A      1202      G   /usr/lib/xorg/Xorg                184MiB |
    |    0   N/A  N/A      2112      G   compiz                             12MiB |
    |    0   N/A  N/A      2742      G   /usr/lib/firefox/firefox            3MiB |
    |    0   N/A  N/A      3857      G   /usr/lib/firefox/firefox            3MiB |
    +-----------------------------------------------------------------------------+

2. Then I created a virtual environment and installed tensorflow-gpu with `pip install tensorflow-gpu`

3. For installing cuda I went to [Nvidia's official website](https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=deblocal) and downloaded cuda for followed the installation procedure for Ubuntu 16.04. Installer type: deb(local)

4. After installing cuda I downloaded CuDNN. At first I tried installing the developer library and it spit out a dependency error. So according to [this guide](https://towardsdatascience.com/how-to-install-tensorflow-gpu-on-ubuntu-18-04-1c1d2d6d6fd2), I tried installing libcupti-dev and added `export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH` to my bashrc. Although I could not find the file lib64 in the mentioned path. I tried giving installing cuDNN developer library again and still found some dependency error. So upon googling the error, I found out that I have to install the runtime library. So I tried installing that one and then installing developer library. This time I was successfully able to install the developer library. 

5. I performed the following two exports as it is mentioned in the post-installation actions in the official installation guide. 

    export PATH=/usr/local/cuda-111.0/bin${PATH:+:${PATH}}
    
    export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64\
                             ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}

6. Then to be on the safer side I did `pip install --upgrade tensorflow-gpu` once and tried running the following command in python

    tf.config.list_physical_devices('GPU')

The output for this line is as follows.

    &gt;&gt;&gt; tf.config.list_physical_devices('GPU')
    2020-06-07 13:20:51.814659: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
    2020-06-07 13:20:51.840069: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
    2020-06-07 13:20:51.840135: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: hari-Lenovo-Legion-Y740-17IRHg
    2020-06-07 13:20:51.840142: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: hari-Lenovo-Legion-Y740-17IRHg
    2020-06-07 13:20:51.840188: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.36.6
    2020-06-07 13:20:51.840226: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.36.6
    2020-06-07 13:20:51.840245: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.36.6
    []

&amp;#x200B;

Can someone please tell me if there anything wrong with my installation steps and what's the solution here?",4,tensorflow,2020-10-13
gy47lu,Is AlphaDropout that same as a standard dropout layer followed by a batch norm layer?, [https://www.tensorflow.org/api\_docs/python/tf/keras/layers/AlphaDropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AlphaDropout),1,tensorflow,2020-10-13
gy3pux,"Recapture your portrait photos with desired posture/view, figure, and clothing style!",,2,tensorflow,2020-10-13
gy19wg,Latest from Mitsubishi: State of the art in Joint Perception and Motion Prediction for Autonomous Driving!(Based on Bird's Eye View Maps),,2,tensorflow,2020-10-13
gy124p,Latest from Samsung researchers: State of the art in photo editing (Harmonization),,1,tensorflow,2020-10-13
gxxchw,How to use tensorflow's FFT?,"Hey everyone. I am having some trouble reconciling my FFT results from MATLAB and TF. The results are actually very different. Here is what I have done:

1). I would attach my data file here but didn't find a way to do so. Anyways, my data is stored in a .mat file, and the variable we will work with is called 'TD'. In MATLAB, I first subtract the mean of the data, and then perform fft:

    f_hat = TD-mean(TD);
    x = fft(f_hat);

2). In TF, I use

    tf.math.reduce_mean

to calculate the mean, and it only differs from MATLAB's mean on the order of 10\^-8. So in TF I have:

    mean_TD = tf.reduce_mean(TD)
    f_hat_int = TD - mean_TD
    f_hat_tf = tf.dtypes.cast(f_hat_int,tf.complex64)
    x_tf = tf.signal.fft(f_hat_tf)

So up until 'f\_hat' and 'f\_hat\_tf', the difference is very slight and is caused only by the difference in the mean. However, x and x\_tf are very different. I am wondering did I not use TF's FFT correctly?

&amp;#x200B;

Thanks! 

https://preview.redd.it/0e6o48jegd351.png?width=560&amp;format=png&amp;auto=webp&amp;s=c782992b54309d31492b6d9c6463079945d9b784",1,tensorflow,2020-10-13
gxow9t,Saliency detection using GCPAnet implementation in tensorflow 2.2. https://github.com/anish9/GCPAnet-tensorflow2.2,,36,tensorflow,2020-10-13
gxnfqo,YouTube,,0,tensorflow,2020-10-13
gxe7r0,How performant is my model?,"Hello,

I would to know your point of view about to accept or reject a model.

I am still learning about ML and in particular with Tensorflow.

During a Text Classification for Sentiment Analisys exercise, I get this values of accuracy and loss:

`125s 5ms/sample - loss: 0.1835 - accuracy: 0.9280 - val_loss: 0.2325 - val_accuracy: 0.9043`

According your experience, can I consider good or not this output?

Thanks",1,tensorflow,2020-10-13
gxdf9d,Tensorflow shape and size question,"Hello! I'm new to TensorFlow, and I'm trying to make a prediction on which of my robotics teams will win based on their team statistics. The data currently looks like this, and I have no idea how to feed this into a model. For some context, there are 4 teams on the field during a match (note 4 lists inside 1 large list, the match being the outside list) The first 2 lists represent the Blue team (there are 2 robotics teams on the blue team and 2 other robotics teams on the Red team)

&amp;#x200B;

x\_train = \[\[\[6, -11.7581, 10.0161, 37, 0, 5, 2\], \[9, 0.639439, 9.32356, 44, 0, 1, 3\], \[30, 12.3542, 10.4267, 115, 0, 3, 4\], \[18, 1.41827, 9.84946, 77, 0, 3, 2\]\], .... (more similar lists for other matches)    \]

y\_train = \[1, ...\]   &lt;---- 1 means the blue team (first 2 lists in x\_train) has won, a 0 means the red team has won

&amp;#x200B;

I HAVE converted x\_train into a Numpy array and normalized it. Now I have no idea if I should use a Conv1D, or a Conv2D layer and I have no idea what the kernel size should be or the input shape.

Here is my link to stack if you want more details, and the code:  [https://stackoverflow.com/questions/62224427/tensorflow-shape-and-size-for-winning-team-prediction](https://stackoverflow.com/questions/62224427/tensorflow-shape-and-size-for-winning-team-prediction)

Thanks in advance",4,tensorflow,2020-10-13
gx4xdy,This Week in AI - Issue #20 | Rubik's Code,,2,tensorflow,2020-10-13
gx4f5g,How to use an online model with Tensorflow.js and deploy with WarpJS,[https://medium.com/warpjs/how-to-run-tensorflow-js-on-a-serverless-platform-deploying-models-47c5d922c448](https://medium.com/warpjs/how-to-run-tensorflow-js-on-a-serverless-platform-deploying-models-47c5d922c448),7,tensorflow,2020-10-13
gx37z7,Hosting tensorflow model for free?,Hi! I am a student developing a CNN model for my school project and I was wondering what hosting platforms I could use for the model for inference for free beside heroku? Much thanks!,0,tensorflow,2020-10-13
gx062t,Understanding Seq-To-Seq with Embedding,"Hello, 

I am studying seq-to-seq NN and I am tring to elaborate [this](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) basic **char-to-char** approach to a machine translator in a **word-to-word** translator machine. 

If it is possible thanks your help, I would to understand if my approach is fine and where I am failing because during the training phase model has very high loss (&gt;10000).

This is my approach , I created two tokenizers one for the **Source** 

    source_tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    ...
    source_data = pad_sequences(source_sequences, padding = 'post', maxlen = MAX_PADDING)
    encoder_input_data = source_data
    

and one for **Target**

    target_tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    ...
    target_data = pad_sequences(target_sequences, padding = 'post', maxlen = MAX_PADDING)
    decoder_input_data = target_data
    

To preparate the training-set  I rolled the `decoder_input_data`  in order to create `decoder target data`:

    decoder_target_data =  np.array([ np.array(roll(d,-1,axis=0)) for d in decoder_input_data])

After,  I  created a **Encoder,** the differnce with the basic approach is that I added an  **Embedding layer** in the model:

    text_encoder_inputs = Input(shape=(MAX_PADDING,), name='text_encoder_inputs')
    
    text_encoder_embedding = Embedding(source_vocab_size, 100, input_length=MAX_PADDING, name='text_encoder_embedding')(text_encoder_inputs)
    
    text_encoder = LSTM(MAX_PADDING, return_state=True, name='text_encoder_lstm')
    
    text_encoder_outputs, text_state_h, text_state_c = text_encoder(text_encoder_embedding)
    
    encoder_states = [text_state_h, text_state_c]

and in a similar way I've creted a **Decoder**

    decoder_inputs = Input(shape=(MAX_PADDING,), name='decoder_inputs') 
    
    decoder_embedding = Embedding(target_vocab_size, EMBED_DIM,     input_length=MAX_PADDING,name='decoder_embedding')(decoder_inputs) 
    
    decoder_lstm = LSTM(HIDDEN_LSTM, return_sequences=True, return_state=True,name='decoder_lstm')
    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
    
    decoder_outputs = Flatten()(decoder_outputs)
    decoder_dense = Dense(MAX_PADDING, activation='softmax',name='decoder_dense')
    decoder_outputs = decoder_dense(decoder_outputs)

and at the end, I compiled and fitted the training set in this way:

    model = Model([text_encoder_inputs, decoder_inputs], decoder_outputs)
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
    
    model.summary()
    
    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
              batch_size=batch_size,
              epochs=epochs,
              validation_split=0.2)

This is the output of summary of compiled model.

&amp;#x200B;

&gt; Model: ""model\_2"" \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ Layer (type)                    Output Shape         Param #     Connected to                      ================================================================================================== text\_encoder\_inputs (InputLayer \[(None, 200)\]        0                                             \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ decoder\_inputs (InputLayer)     \[(None, 200)\]        0                                             \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ text\_encoder\_embedding (Embeddi (None, 200, 100)     1453200     text\_encoder\_inputs\[0\]\[0\]         \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ decoder\_embedding (Embedding)   (None, 200, 100)     3066100     decoder\_inputs\[0\]\[0\]              \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ text\_encoder\_lstm (LSTM)        \[(None, 128), (None, 117248      text\_encoder\_embedding\[0\]\[0\]      \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ decoder\_lstm (LSTM)             \[(None, 200, 128), ( 117248      decoder\_embedding\[0\]\[0\]                                                                            text\_encoder\_lstm\[0\]\[1\]                                                                            text\_encoder\_lstm\[0\]\[2\]           \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ flatten\_2 (Flatten)             (None, 25600)        0           decoder\_lstm\[0\]\[0\]                \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ decoder\_dense (Dense)           (None, 200)          5120200     flatten\_2\[0\]\[0\]                   ================================================================================================== Total params: 9,873,996 Trainable params: 9,873,996 Non-trainable params: 0 \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ 

Thanks for your help :)",2,tensorflow,2020-10-13
gwzcct,Creating an Adaptive Filter in Tensorflow,"Hi everyone!

I had a question for everyone in the r/tensorflow community. I am currently trying to build an adaptive filter inside of tensorflow of the shape:

conv(input, weights) = output

Because everything is 1-Dimensional, it makes sense to me that weights is just a tensor that is k long, and then I use an optimizer to find out what the value of my weights are (I already know what my output is *supposed* to be, the weights themselves, however, are unknown).

My question, as dumb as this sounds, is:

What should my object be that holds my weights? I am confused what kind of layer I should use for my tensor of weights that I use for the adaptive filter.

Is this even a smart thing to do? What kind of layer should I use that can be trained in this way?

Thanks!",0,tensorflow,2020-10-13
gwsbk7,Best pre-trained model for transfer learning,"Hi,

I am planning to implement an image classification model, using transfer learning.

I found this list, the only criteria that I am considering is the speed of inference [https://keras.io/api/applications/](https://keras.io/api/applications/)

&amp;#x200B;

Can you advise on which should I consider for my task ?

Thanks",1,tensorflow,2020-10-13
gwkxwd,Is there no easy way to use Local Response Normalization in a keras model?,"I'm following Aurelien Geron's O'reilly book: Hands-On Machine Learning with Scikit-Learn &amp; Tensorflow.

It's all for tensorflow v1, so it's a little dated but I don't have too much trouble using it on v2 since I have some experience already.

I'm on the chapter about CNN's, and as the author is going over popular CNN architectures like ResNet and AlexNet, he talks about *Local Response Normalization*, which is the idea that the if a neuron fires strongly in one feature map, it will diminish the activations of neurons in neighboring feature maps within a certain neighborhood. All the networks lately seem to be using this since it greatly increases the generalizability of the network since it finds a wide range of feature maps.

However, I am looking at the tf.keras documentation, and there is no Local Response Normalization layer nor is it a regularizer. I know that in the vanilla tf, there is a function **tf.nn.local_response_normalization()**, but if I am making a keras model, I can't just give it that function, can I? I assume that I would have to implement the layer myself, but I'm not too sure.

**Edit:**

Just tried just using the tf.nn.local_response_normalization() function in the functional API, but I get an error as expected.

    conv1 = tf.keras.layers.Conv2D(100, 3, input_shape=(None,None,3))
    local_response = tf.nn.local_response_normalization(conv1)
    dense = tf.keras.layers.Dense(10, activation='softmax')(local_response)
    
    model = tf.keras.models.Model(inputs=[conv1], outputs=[dense])

&gt; ValueError: Attempt to convert a value (&lt;tensorflow.python.keras.layers.convolutional.Conv2D object at 0x0000024C81F61C10&gt;) with an unsupported type (&lt;class 'tensorflow.python.keras.layers.convolutional.Conv2D'&gt;) to a Tensor.",3,tensorflow,2020-10-13
gwh6me,Generating Melodies with a Neural Network,"In my new tutorial, I explain how to convert a melody generated with a neural network from a custom music representation to MIDI using a music Python package called music21.

This video is part of the “Generating melodies with LSTM nets”. The series aims to teach you how to build an artificial neural network (RNN-LSTM) to generate effective melodies.

Here’s the video:

https://www.youtube.com/watch?v=ALAglxiHgXE&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=9",14,tensorflow,2020-10-13
gwgdal,How to convert tf-hub module to tf2-saved model?,"Hi,

I'm trying to convert [i3d Kinetics 400](https://tfhub.dev/deepmind/i3d-kinetics-400/1) pretrained Tensorflow hub module to Tensorflow 2 Saved Model using `tf.saved_model.save(hub.load(module_url), export_directory)` to use the saved tf 2 saved model with tensorflow serving.

Model is being exported along with variables, assets and pb file but I'm not able to use this model. Further, I tried to view the signature of loaded model using `model = tf.saved_model.load(export_directory); model.signatures` signatures of the loaded model is found to be empty.

Also I checked the `tensorflow_version` of hub module which is some tensorflow 1 version.

What is the correct way to convert tf-hub modules to tf2 saved model?


Thanks in advance.",1,tensorflow,2020-10-13
gw4gaq,XOR Problem with MLP,"Hi guys, I'm trying to reproduce this mlp in tf  (constraining it to have only one hidden layer with 2 units). However, like in playground tf, many times do not converge to global maxima. I think is due to weight initialization, tried xavier and he initializations but no success. The following piece of code is the model in tf keras.

    model = tf.keras.models.Sequential([
    Dense(units=2, activation='sigmoid', input_shape=(2,)),
    Dense(units=1, activation='sigmoid')
])

Any help, would be appreciated. Thanks.

https://preview.redd.it/otv9rmbrtr251.png?width=1375&amp;format=png&amp;auto=webp&amp;s=6757653b702f45a728dd032a89368fedbd6968e2",9,tensorflow,2020-10-13
gw0bdv,Pose Detection in Twilio Video with JavaScript and TensorFlow,,2,tensorflow,2020-10-13
gvvfd9,Keras Tuner on multiple GPUs,"Im trying to use keras tuner on an ec2 instance with 8 gpus.  I’m running tf 2.1 and cuda 10.1, and everything works if I don’t try to parallelize anything.  My model isn’t so big that I need data parallelism, so I’d like to test multiple hyperparameter sets simultaneously.  I’m starting by following the example here:

https://keras-team.github.io/keras-tuner/tutorials/distributed-tuning/

Can anyone explain how the two bash scripts in the example relate to one another?  I run the bash script for the chief service and I can see it listening on port 8000.  But keras tuner doesn’t seem to run.  So I run the worker bash script and  still nothing happens. Does anyone have a more complete example of parrallelizing keras tuner across gpus?",2,tensorflow,2020-10-13
gvsk6i,How to Plot Confusion Matrix in TensorFlow for 3+ Dimensions?,"How do you plot a confusion matrix in Tensorflow?

I have a y test and label predictions. I want to plot a confusion matrix in Tensorflow. Does anyone have an idea how?

There's the 2d array, but what about more dimensions?

Y Test:

    [array([[1., 0., 0., ..., 0., 0., 0.],
            [1., 0., 0., ..., 0., 0., 0.],
            [1., 0., 0., ..., 0., 0., 0.],
            ...,
            [0., 0., 1., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.]], dtype=float32),
     array([[1., 0., 0., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.],
            ...,
            [0., 0., 1., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.]], dtype=float32),
     array([[0., 0., 1., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.],
            [1., 0., 0., ..., 0., 0., 0.],
            ...,
            [0., 0., 1., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.],
            [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)]

Label Predictions:

    array([[[9.9434608e-01, 2.0394768e-03, 1.6671700e-03, ...,
             6.9635144e-07, 9.1386960e-07, 6.5815397e-07],
            [9.9324822e-01, 5.9449715e-03, 4.3048413e-04, ...,
             1.3199327e-07, 1.7925048e-07, 1.3644501e-07],
            [9.9196011e-01, 6.1822892e-03, 1.5873729e-03, ...,
             9.2281880e-08, 1.2384969e-07, 9.4705349e-08],
            ...,
            [1.4670823e-05, 2.2589884e-07, 9.9998403e-01, ...,
             4.4678453e-10, 6.2618072e-10, 4.1855044e-10],
            [4.3411623e-05, 6.2847118e-07, 9.9994016e-01, ...,
             6.6515904e-09, 8.9706900e-09, 6.7417658e-09],
            [3.3048415e-04, 4.0306286e-06, 9.9824750e-01, ...,
             5.2637608e-07, 6.5039575e-07, 5.6928235e-07]],
    
           [[9.9602157e-01, 3.0045363e-03, 1.9356112e-04, ...,
             2.7003202e-07, 3.6534422e-07, 2.7131804e-07],
            [5.3122878e-04, 6.1085485e-03, 9.9243528e-01, ...,
             3.4078897e-07, 4.8124537e-07, 3.2691227e-07],
            [7.4358368e-06, 5.1467991e-06, 9.9998081e-01, ...,
             2.9715559e-09, 4.1046011e-09, 2.6802129e-09],
            ...,
            [1.4670809e-05, 2.2589863e-07, 9.9998403e-01, ...,
             4.4678369e-10, 6.2618072e-10, 4.1854964e-10],
            [4.3411623e-05, 6.2847118e-07, 9.9994016e-01, ...,
             6.6515904e-09, 8.9706900e-09, 6.7417658e-09],
            [3.3048415e-04, 4.0306286e-06, 9.9824750e-01, ...,
             5.2637608e-07, 6.5039575e-07, 5.6928235e-07]],
    
           [[8.9576526e-05, 2.1821468e-05, 9.9962628e-01, ...,
             1.0252310e-07, 1.3143165e-07, 8.6875239e-08],
            [2.3756633e-05, 3.9258471e-06, 9.9995613e-01, ...,
             7.3692621e-09, 9.7641948e-09, 6.3885341e-09],
            [9.9942088e-01, 7.6706776e-05, 4.6123084e-04, ...,
             1.6810123e-08, 2.2758545e-08, 1.8000916e-08],
            ...,
            [1.4670809e-05, 2.2589863e-07, 9.9998403e-01, ...,
             4.4678369e-10, 6.2618072e-10, 4.1854964e-10],
            [4.3411623e-05, 6.2847118e-07, 9.9994016e-01, ...,
             6.6515904e-09, 8.9706900e-09, 6.7417658e-09],
            [3.3048415e-04, 4.0306286e-06, 9.9824750e-01, ...,
             5.2637608e-07, 6.5039575e-07, 5.6928235e-07]]], dtype=float32)",7,tensorflow,2020-10-13
gvi1kf,Latest from apple researchers: Deep learning approach for driving animated faces using both acoustic and visual information.,,5,tensorflow,2020-10-13
gves0t,Multi-branch network using tf.data.Dataset," I am trying to build a multi-stream network in TF 2.0 using pre-trained sub-models on different physical features of the face. I have 3 different models trained on LeftEye, RightEye and Mouth regions. I am now wanting to load all these models into one network and train it to produce 1 output- say Gender. I'm using tf.data.Dataset.from\_tensor\_slices since I have image names with paths and Gender labels in a csv file.

Here's some snippets of code I have so far:

    ...
    leftEye_train_data = tf.data.Dataset.from_tensor_slices((tf.constant(leftEyeFiles), tf.constant(labels))).map(_parse_fn).batch(BATCH_SIZE,drop_remainder=True)
    rightEye_train_data = tf.data.Dataset.from_tensor_slices((tf.constant(rightEyeFiles), tf.constant(labels))).map(_parse_fn).batch(BATCH_SIZE,drop_remainder=True)
    mouth_train_data = tf.data.Dataset.from_tensor_slices((tf.constant(mouthFiles), tf.constant(labels))).map(_parse_fn).batch(BATCH_SIZE,drop_remainder=True)
    
    valleftEye_train_data = tf.data.Dataset.from_tensor_slices((tf.constant(valleftEyeFiles), tf.constant(vallabels))).map(_parse_fn).batch(BATCH_SIZE,drop_remainder=True)
    valrightEye_train_data = tf.data.Dataset.from_tensor_slices((tf.constant(valrightEyeFiles), tf.constant(vallabels))).map(_parse_fn).batch(BATCH_SIZE,drop_remainder=True)
    valmouth_train_data = tf.data.Dataset.from_tensor_slices((tf.constant(valmouthFiles), tf.constant(vallabels))).map(_parse_fn).batch(BATCH_SIZE,drop_remainder=True)
    :
    :
    modelLeftEye=tf.keras.models.load_model('LeftEye')
    modelRightEye=tf.keras.models.load_model('RightEye')
    modelMouth=tf.keras.models.load_model('Mouth')
    
    :
    NUM_CLASSES=2
    X0=modelLeftEye.layers[-3].output
    ModelX0=tf.keras.models.Model(inputs=modelLeftEye.input, outputs=X0)
    
    X1=modelRightEye.layers[-3].output
    ModelX1=tf.keras.models.Model(inputs=modelRightEye.input,outputs=X1)
    
    X2=modelMouth.layers[-3].output
    ModelX2=tf.keras.models.Model(inputs=modelMouth.input,outputs=X2)
    
    combined=tf.keras.layers.Concatenate()([ModelX0.output, ModelX1.output, ModelX2.output])
    fc=Dense(16,activation=""relu"")(combined)
    fc=Dense(NUM_CLASSES,activation='softmax',name='prediction')(fc)
    
    ensembleModel=tf.keras.models.Model(inputs=[ModelX0.input,ModelX1.input,ModelX2.input],outputs=fc,name='ensemble_model')
    opt=tf.keras.optimizers.SGD()
    ensembleModel.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])
    
    dataset3 = tf.data.Dataset.zip((leftEye_train_data, rightEye_train_data,mouth_train_data))
    history = ensembleModel.fit(dataset3.repeat(),
                        epochs=num_epochs,
                        steps_per_epoch = steps_per_epoch,
                        validation_data=dataset3.repeat(),
                        validation_steps=val_steps,
                        validation_freq=1),
                        callbacks=[checkpoint_callback]))

 Obviously, the fit method throws the following error: 

&gt; ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model &gt;is not the size the model expected. Expected to see 3 array(s), for inputs \['leftEye\_0', 'rightEye\_0', &gt;'Mouth\_0'\] but instead got the following list of 2 arrays: \[, \] 

 What is the best way to prepare my input for the multi-stream network using tf.data.Dataset? 

P.S: I have posted the same question on stackoverflow as well, but haven't received any replies so far. 

Thanks!",4,tensorflow,2020-10-13
gv9g60,Data Generator Problem,"Dear all,

I am implementing my own data generator for an object detection CNN. I get the following error:

  (0) Invalid argument:  TypeError: \`generator\` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was \[array(\[\[112, 121, 148, ...,  68,  64,  68\],

where my generator is declared like following:

   dataset = tf.data.Dataset.from\_generator(  
generator,  
 args=(x\_train\[0:10, :, :, :\],  
boxes\_train\[0:10, :, :\], y\_train\[0:10, :\]),  
 output\_types=(tf.int32, tf.int32),  
 output\_shapes=(tf.TensorShape((None, 224, 224, 3)),  
tf.TensorShape((None, )))  


Could you tell me if you see something wrong? Thanks",1,tensorflow,2020-10-13
gv9492,Beginner looking to start out.,"Hi guys so I have a basic knowledge of python and am looking to get into tensorflow. I need some advice as to where to start.  

[https://www.youtube.com/watch?v=tPYj3fFJGjk](https://www.youtube.com/watch?v=tPYj3fFJGjk) 

I attempted to watch the youtube video above, while I could understand in the moment, I wasn't really remembering much or knowing what really to do at all. Hence, I would like to ask for any course reccomendations or useful ways to start learning.",1,tensorflow,2020-10-13
gv4kj7,Documentation of Tensorflow0.5.0,"Hi, I have ubuntu14.04 and have installed tensorflow for python2.7. It is necessary for me to use python2.7 so creating a virtual environment with python3 is not an option. I used this command for installing tensorflow.

`$ pip install` [`https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl`](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl)

Upgrading to Tensorflow1.x gives me an error ""invalid EFL header"" when I import tensorflow. So that's also not possible. So I am stuck with 0.5.0. But I am not able to find its documentation. Would be really great if someone would help me out.

Thanks.",1,tensorflow,2020-10-13
guyv61,Keras Tuner vs Hparams,What is the difference between the two hyperparameter training frameworks (1) Keras Tuner and (2) HParams? Which would you recommend?,4,tensorflow,2020-10-13
guukl7,ValueError: No gradients provided for any variable,"Would someone be so kind as to help me understand this error better? 

**The Code:**

def define\_model(vocab\_size, max\_length):  
 *# feature extractor model*  
 inputs1 = Input(shape=(2048,))  
fe1 = Dropout(0.5)(inputs1)  
fe2 = Dense(384, activation=**'relu'**)(fe1)  
 *# sequence model*  
 inputs2 = Input(shape=(max\_length,))  
se1 = Embedding(vocab\_size, 384, mask\_zero=True)(inputs2)  
se2 = Dropout(0.5)(se1)  
se3 = LSTM(384,recurrent\_initializer=**'glorot\_uniform'**)(se2)  
 *# decoder model*  
 decoder1 = add(\[fe2, se3\])  
decoder2 = Dense(384, activation=**'relu'**)(decoder1)  
 *# Attention Model*  
 attn\_out = Attention()((\[se3, decoder2\]))  
LSTMATTNOUT = Concatenate(axis=-1)(\[decoder2, attn\_out\])  
outputs = Dense(vocab\_size, activation=**'softmax'**)(LSTMATTNOUT)  
 *# tie it together*  
 model = Model(inputs=\[inputs1, inputs2\], outputs=outputs)  
model.compile(loss=**'categorical\_crossentropy'**, optimizer=**'adam'**, metrics=\[**'accuracy'**\])  
 *# summarize model*  
 print(model.summary())  
 *#plot\_model(model, to\_file='model.png', show\_shapes=True)*  
 return model

&amp;#x200B;

**The Error:**

 ValueError: No gradients provided for any variable: \['embedding/embeddings:0', 'dense/kernel:0', 'dense/bias:0', 'lstm/lstm\_cell/kernel:0', 'lstm/lstm\_cell/recurrent\_kernel:0', 'lstm/lstm\_cell/bias:0', 'dense\_1/kernel:0', 'dense\_1/bias:0', 'dense\_2/kernel:0', 'dense\_2/bias:0'\].

&amp;#x200B;

Please Helppp, thank you :( 

It wasn't occurring before when there was no Attention Layer in between ;-;",0,tensorflow,2020-10-13
gul4y8,Generating Melodies with a Neural Network,"In my new tutorial, I explain how to sample a melody from the output of a neural network trained to generate folk melodies.

This video is part of the “Generating melodies with LSTM nets”. The series aims to teach you how to build an artificial neural network (RNN-LSTM) to generate effective melodies.

Here’s the video:

https://www.youtube.com/watch?v=6YdQdf4eBD4&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=8",12,tensorflow,2020-10-13
guiup8,How to explain predictions of my text classification model to users?,"Hi,

I've just started working on my first deep-learning project. I've built a text classification model with Tensorflow/Keras that I want to deploy at a later stage. The model works and I can use it to make predictions. At some point, I want to ""explain"" my model's prediction to future users by showing words or combinations of words that were important in making the prediction. However, I have no idea where to get started with this. Any suggestions?

Thanks",5,tensorflow,2020-10-13
gug9w6,TensorFlow Cheat Sheet,,1,tensorflow,2020-10-13
gug4eg,Top 3 Artificial Intelligence Research Papers – May 2020,,1,tensorflow,2020-10-13
guezgw,"Segmenting and tracking multiple moving objects within a video fully automatically, without any manual initialization.",,3,tensorflow,2020-10-13
gu6lct,Does learning rate decay continue where it left after resuming the training ?,"If I used 

    lr=tf.keras.experimental.CosineDecay(3*10**(-4), 123428, alpha=1/100)
    opt = tfa.optimizers.Adam(learning_rate=lr,amsgrad=True)
    model.fit(dataset, epochs=1000,initial_epoch=0,steps_per_epoch = 250)

Then I stop the training when epoch reaches 300, and continue like this

    model.fit(dataset,epochs=1000,initial_epoch=300,steps_per_epoch=250)

Are the global steps inferred from the epoch and steps\_per\_epoch or does it reset to zero?  It really hard in tensorflow 2.1 to find the global step to check. I tried looking at the code but I don't understand it fully.",1,tensorflow,2020-10-13
gu5l36,Tensor flow lite | How to detect only objects in motion | In a limited frame,"Hi!

I am making a safety device that allows people to see if there are cars approaching from the lower part of a hill. It is currently the scene of multiple car accidents, and the county refuses to put a speed bump, so I am making an LED indicator that uses object detection to detect oncoming cars.

I am using the Coco SSD object detection model with tensorflow lite, on a raspberry pi, and I want to only detect cars in motion in a certain portion of a video frame

Is there a way to do that?

Here's the video with the cars in motion: [https://youtu.be/x4FrMaIjSQ0](https://youtu.be/x4FrMaIjSQ0)

Frame that I want to limit it to: [https://drive.google.com/file/d/1-1kHahLWVv5uOhgXrFtyepjzjg5DIxKt/view?usp=sharing](https://drive.google.com/file/d/1-1kHahLWVv5uOhgXrFtyepjzjg5DIxKt/view?usp=sharing)

The issue is the detection detecting cars driving away, and detecting cars that are parked.

Thank you in advance!

**EDIT**: Here's a link to my current detection (you can see the issues): [https://youtu.be/e2mzeyGxHjI](https://youtu.be/e2mzeyGxHjI)",3,tensorflow,2020-10-13
gu40s0,How to create multi-classification baselines when using sample_weights?,"I am using TF and want to create some baseline losses. For this i do

`from sklearn.metrics import log_loss`

`log_loss(test, np.full_like(test, np.mean(train, axis=0)))`

This works as it should. However the problem is when i start to use the sample\_weight argument.

Since ultimately i will want to supply weighting to my test set, i decide to just use sample\_weight instead of class\_weight.

For this i will balance my dataset by doing the following

`col_sums = np.sum(outputs, axis=0)`  
`class_weight = {0: 1 / col_sums[0],`  
 `1: 1 / col_sums[1],`  
 `2: 1 / col_sums[2]}`

This create class weights.

Then i make a sample\_weight array using the class weights.

`sample_weights = np.array([class_weights[np.argmax(pred)] for pred in outputs])`

Now one of the problems is that sklearn's log\_loss uses sample weights differently than TF's categorical\_crossentropy.

`import tensorflow as tf`  
`from sklearn.metrics import log_loss`  


`cce = tf.keras.losses.CategoricalCrossentropy()`  
`y_true = [[0, 1, 0], [0, 0, 1]]`  
`y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]`  
`sample_weight = [0.3, 0.7]`  
`cce = tf.keras.losses.CategoricalCrossentropy()`  


`# produces same output`  
`print(cce(y_true, y_pred).numpy())`  
`print(log_loss(y_true, y_pred))`  


`# produces different output`  
`print(cce(y_true, y_pred, sample_weight=tf.constant(sample_weight)).numpy())`  
`print(log_loss(y_true, y_pred, sample_weight=sample_weight))`

We can see that supplying the same data and the same sample\_weight produces different output. I have come to find that sklearn seems to scale the mean of the weights to 1, prior to weighting the loss.

Such that when i do

`sample_weight = np.array([0.3, 0.7])`  
`sample_weight /= np.mean(sample_weight)`  
`print(cce(y_true, y_pred, sample_weight=tf.constant(sample_weight)).numpy())`

Now TF's output matches sklearn's log\_loss.

But i have still having some trouble because my loss in TF and sklearn's baseline still do not match. I guess i could just use TF's loss but is there a reason am i not thinking of that can cause this discrepancy?",1,tensorflow,2020-10-13
gu0rrx,I made AI Profile Picture Maker app for AI people using Tensorflow.js,,40,tensorflow,2020-10-13
gtxhnn,Unable to verify / extract individual predictions after training a model,"Hi, 

I have a model that reports around 0.67 accuracy on a testing dataset. However i would like to extract the individual predictions per sample to see how it performed on each. I read that i should run the logits layer in the session and would get an array of predictions per sample. This does not seem to work, as it will just have a prediction for the first category for every sample in the verification set.

    
        # predictions
        logits = tf.layers.dense(flat_d, NUM_CLASSES, activation=tf.nn.relu)
    
        # Cost function and optimizer
        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))
        # cost = -tf.reduce_sum(labels*tf.log(logits + 1e-10))
        train_step = tf.train.AdamOptimizer(learning_rate_).minimize(cost)
    
        # Accuracy
        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')
    
    ....
    
        print(""Initializing the model"")
        sess = tf.Session()
        sess.run(tf.global_variables_initializer())
    
        for e in np.arange(num_epochs):
            bx,by = load_batch(trainX,trainY,batch_size_)
            sess.run(train_step,feed_dict={inputs:bx,labels:by,keep_prob:0.9,learning_rate_:learn_rate})
    
        acc = sess.run(accuracy,feed_dict={inputs:testX,labels:testY,keep_prob:1.0,learning_rate_:learn_rate})
        print(""----------\nAccuracy:%.3f""%(acc))
    
    
        # Extract individual precitions by class
        print(sess.run(logits,feed_dict={inputs:testX,labels:testY,keep_prob:1.0}))

Here is the summary of the output:

    Number of validation samples: 203
    
    By category:
     0: 34
     1: 138
     2: 31
    
    print(sess.run(accuracy,feed_dict={inputs:testX,labels:testY,keep_prob:1.0,learning_rate_:learn_rate}))
    Accuracy:0.680
    
    print(sess.run(logits,feed_dict={inputs:testX,labels:testY,keep_prob:1.0}))
    
    [[1.6453042  0.         0.        ]
     [1.454038   0.         0.        ]
     [1.6372575  0.         0.        ]
     [1.5505953  0.         0.        ]
     [1.6624011  0.         0.        ]
     [1.6376897  0.         0.        ]
     [1.5477558  0.         0.        ]
     [1.5303426  0.         0.        ]
     [1.3636262  0.         0.        ]
     [1.5397886  0.         0.        ]
     [1.8849531  0.         0.        ]
    .....
    
    etc.

Any pointers of what i'm doing wrong, or how i could extract the predictions for each sample instead of just the overall accuracy?

&amp;#x200B;

Thanks",1,tensorflow,2020-10-13
gtv311,3D Unet for tensorflow 2.0,"Hi, I'm looking for a 3D Unet model for mouse brain image segmentation. Most of the models that I've found on github are more than two years old and have lots of compatibility issues with tensorflow 2.0

Does somebody know a code that is compatible with tensorflow 2.0 ? I just need the model, not a whole training pipeline.",1,tensorflow,2020-10-13
gto9wq,"State of the art in instance segmentation: higher speed, more precise detection",,2,tensorflow,2020-10-13
gt7e7d,Best pre-trained model for detecting people?,"I am using the standard pre-trained COCO model for detecting people, and it does an ok job.. but 50% of the time it detects random objects as people too. Fence, archway, grill, all people. I'd set the threshold lower, but my grill and I would be competing for the best score.

Is there a pre-trained model focused just on people that might give better accuracy? Should I be attempting to train one myself? 

Thanks for the input.

Edit: My use case is evaluating a still image (jpg) when motion is detected from an IP camera on a Raspberry pi.",9,tensorflow,2020-10-13
gt6ses,Extracting editable 3D objects directly from a single photograph.,,1,tensorflow,2020-10-13
gt2vlv,how to build a custom tf model in consideration of converting it to tflite model? (noob),"how to build a model and expect it to work well in .tflite format, i mean are there any tips on how to expect an input and output?, I'm using tflite on Android. Thanks in advance!",2,tensorflow,2020-10-13
gso5e9,This Week in AI - Issue #19 | Rubik's Code,,9,tensorflow,2020-10-13
gsew8a,Latest from Facebook and CMU researchers: Navigating to the location indicated by a goal image in a novel previously unseen environment!,,4,tensorflow,2020-10-13
gsbekj,"Issue when connecting to Cloud TPU Pod, TF v2.1","I'm trying to use my (pre-emptible) Cloud TPU v3-256 on my Google Cloud Compute Engine VM with TensorFlow 2.1, but it doesn't seem to be working as the TPUClusterResolverthrows a Could not lookup TPU metadataerror.

Using individual (non-preemptible) TPUs works fine as long as I use the grpc://address rather than the TPU Name. However, neither individual TPUs nor my TPU Pod work when using the TPU Name, and throw this error.

Can someone help me fix this issue?

Code:

    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='my-tpu-name', zone='europe-west4-a', project='my-project') # The zone, project and TPU Name are correct

Output:

    ValueError: Could not lookup TPU metadata from name 'my-tpu-name'. Please double
    check the tpu argument in the TPUClusterResolver constructor.
    Exception: Failed to retrieve http://metadata.google.internal/computeMetadata/v1/
    instance/service-accounts/default/?recursive=True
    from the Google Compute Enginemetadata service. Response: {'metadata-flavor': 'Google', 
    'date': 'Thu, 28 May 2020 17:42:35 GMT', 'content-type': 'text/html; charset=UTF-8',
    'server': 'Metadata Server for VM', 'content-length': '1629', 'x-xss-protection': '0', 'x
    frame-options': 'SAMEORIGIN', 'status': '404'}",3,tensorflow,2020-10-13
gsaj2g,"Just started learning, what are some good resources?","My freshman year in college I became familiar with R and learned how to make basic models (i.e Linear Regression and Classification) and a few other things. For the final project, I learned how to make Neural Nets (using h2o.ai library) in R, and wanted to get a different perspective and learn TF. 

I'm about 1/2 through the 7-hour YouTube tutorial right now, and plan to do the tutorials laid out on the TF webstie after the video. It might be important to mention I don't have any python experience prior to this, but bought a few books to compensate which I plan to consult should I run into any issues. This sub seems pretty knowledgeable, and I was wondering if there were any important resources I'm missing.",1,tensorflow,2020-10-13
gsairh,Training a neural network for melody generation,"I published a tutorial where you can learn how to build and train a neural network for generating folk melodies. 

This video is part of the “Generating melodies with LSTM nets”. The series aims to teach you how to build an artificial neural network (RNN-LSTM) to generate effective melodies.

Here’s the video:

https://www.youtube.com/watch?v=H7H-VcE7tOI&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=7",0,tensorflow,2020-10-13
gs87dt,"Tensorflow serving issue: ""ValueError: Unknown layer: KerasLayer""","I am trying to serve a tensorflow model in the .h5 format, but I am experiencing this issue:

&amp;#x200B;

    File ""/app/.heroku/python/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 166, in class_and_config_for_serialized_keras_object
    
    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
    ValueError: Unknown layer: KerasLayer",1,tensorflow,2020-10-13
gs3yg1,"TFRecords is 'Ansi' Encoded, but I need it 'UTF-8' encoded","Hello everyone,  
I wrote here a question a while ago, but I made progress and this is why I now write a new question here.  


I still get

    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbd in position 8: invalid start byte

but I now know, that I possibly happends because the .tfrecords file seems to be ansi encoded.  
Can somebody explain to me, why to following code creates 'ansi' encoded files?  


    from __future__ import division
    from __future__ import print_function
    from __future__ import absolute_import
    
    import os
    import io
    import re
    
    import pandas as pd
    import tensorflow.compat.v1 as tf
    
    from PIL import Image
    from object_detection.utils import dataset_util
    from collections import namedtuple, OrderedDict
    
    
    flags = tf.app.flags
    flags.DEFINE_string('csv_input', '', 'Path to the CSV input')
    flags.DEFINE_string('output_path', '', 'Path to output TFRecord')
    flags.DEFINE_string('img_path', '', 'Path to images')
    FLAGS = flags.FLAGS
    
    
    # TO-DO replace this with label map
    def class_text_to_int(row_label):
        if row_label == 'car':
            return 1
        elif row_label == 'pedestrian':
            return 2
        elif row_label == 'bicycle':
            return 3
        else:
            None
    
    
    def split(df, group):
        data = namedtuple('data', ['filename', 'object'])
        gb = df.groupby(group)
        return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]
    
    
    def create_tf_example(group, path):
        with tf.gfile.GFile(os.path.join(path, '{}'.format(re.sub('\s+', '', group.filename))), 'rb') as fid:
            encoded_jpg = fid.read()
        encoded_jpg_io = io.BytesIO(encoded_jpg)
        image = Image.open(encoded_jpg_io)
        width, height = image.size
    
        filename = group.filename.encode('utf-8')
        image_format = b'jpg'
        # check if the image format is matching with your images.
        xmins = []
        xmaxs = []
        ymins = []
        ymaxs = []
        #classes_text = []
        classes = []
    
        for index, row in group.object.iterrows():
            xmins.append(row['xmin'] / width)
            xmaxs.append(row['xmax'] / width)
            ymins.append(row['ymin'] / height)
            ymaxs.append(row['ymax'] / height)
            #classes_text.append(row['label'].encode('utf8'))
            classes.append(class_text_to_int(row['label']))
    
        tf_example = tf.train.Example(features=tf.train.Features(feature={
            'image/height': dataset_util.int64_feature(height),
            'image/width': dataset_util.int64_feature(width),
            'image/filename': dataset_util.bytes_feature(filename),
            'image/source_id': dataset_util.bytes_feature(filename),
            'image/encoded': dataset_util.bytes_feature(encoded_jpg),
            'image/format': dataset_util.bytes_feature(image_format),
            'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
            'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
            'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
            'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs)
        }))
        return tf_example
    
    
    def main(_):
            writer = tf.io.TFRecordWriter(FLAGS.output_path)
            path = os.path.join(os.getcwd(), FLAGS.img_path)
            examples = pd.read_csv(FLAGS.csv_input, sep=',', engine='python')
            grouped = split(examples, 'filename')
            for group in grouped:
                tf_example = create_tf_example(group, path)
                writer.write(tf_example.SerializeToString())
    
            writer.close()
            output_path = os.path.join(os.getcwd(), FLAGS.output_path)
            print('Successfully created the TFRecords: {}'.format(output_path))
    
    
    if __name__ == '__main__':
        tf.app.run()",2,tensorflow,2020-10-13
gs0ik9,"After installing tensorflow and testing it it gave me a huge error, can someone please help me",,0,tensorflow,2020-10-13
grxt2e,From Adobe researchers: State of the art in High-Resolution Image Inpainting,,6,tensorflow,2020-10-13
grv5h6,Problem installing tensorflow on python 3.9.3,"python: 3.8.3
pip: 20.1.1

when I try to install with

pip install tensorflow

I get “Could not find a version that satisfies the requirement tensorflow (from versions: none) 

and “No matching distribution found for tensorflow”

Any help?

EDIT: found out I was using the 32 bit version of python - quite silly. Why do they still offer that, anyway? Thanks!",11,tensorflow,2020-10-13
grqtwy,Can anyone explain how tf.data.experimental.rejection_resample works ?,"I'm working on an extremely unbalanced dataset and was planning on using the experimental rejection resample tensorflow provides. I was under the impression that it works by dropping a fraction of the input dataset to balance the dataset. It looks like my resulting dataset after resampling is slightly bigger than my original dataset though, so really not sure what's going on.",1,tensorflow,2020-10-13
grioys,Loss and accuracy remains unchanged,"I was making a classifier with 8 classes. This is my model:

`model = Sequential() model.add(Conv2D(16, 3, padding=""same"", activation=""relu"", input_shape=(100,100,1))) model.add(MaxPooling2D()) model.add(Conv2D(32, 3, padding=""same"", activation=""relu"")) model.add(MaxPooling2D()) model.add(Conv2D(64, 3, padding='same', activation='relu')) model.add(MaxPooling2D()) model.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dense(len(classes))) model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])`

 `history = model.fit(x_train, y_train, validation_data = (x_test, y_test),epochs = 20,batch_size= 64 )`

The loss and accuracy of this model remains constant. I don't know why. Can anyone tell what to do?",0,tensorflow,2020-10-13
griipw,"Video: Building a basic image classifier with Node.js, React, and Tensorflow",,15,tensorflow,2020-10-13
grhebg,Plot customer metrics in tensorboard,"How can I plot customer metrics like accuracy Vs epochs in scalar section of tensorboard.

If I want to display pdf of accuracy or tpr Vs fpr something like that in tensorboard ( tensforflow &gt; 2.1) ?

Please sharing any reference or links",1,tensorflow,2020-10-13
grc50r,Multi-image classifier model strategy questions,"I am going to attempt to build a Multi-image classification model for classifying US coins photos using Tensorflow 2.0. Several others have done this from a simplistic standpoint given there are only \~60 or so categories of US coins (Barber quarters vs. Washington quarters etc..). I have a “generic” CNN working now to do this and it is about 75% accurate in most cases if the validation image is well taken. I am continuing work on the model and adding to the image dataset to make it better.  However, this simple solution has made me (**non-data scientist**) wonder about a better approach and also how far this can be taken...  I currently have about 10k images and I'm using InceptionV3 minus a few layers. 

**Question 1:**

In regards to accuracy, I am wondering if I started with a bad strategy of 100 or less categories.  Here is the issue: There are \~1000 permutations of US coins (for example one year a Bust Half Dollar will have big letters and then another year small letters)--and the changes can be from small to dramatic across each broad coin category. There are also several mints (5 common ones, but not every coin was minted at each mint). ...and we cannot forget that the year changes. Oh yea, then there is also the complexity of the front and back of the coin. 

so, the classification problem seems to get hard quick once you consider all the possible categories there are... 

am I overthinking this? Can the “generic” CNN just be made better to get the accuracy higher--if so is that a better strategy then trying to make more categories?

How would you go about solving such a problem?

**Question 2:**

Grading coins is about analyzing the coin for wear (from none, to a lot). It is done on the 0-70 [Sheldon](https://en.wikipedia.org/wiki/Sheldon_coin_grading_scale) scale which about 28 of the grades are generally used in the marketplace.

What strategy would you use to categorize coins into their specific grade? How would someone estimate how many images would be required in the training/testing set per grade per coin? Does solving this problem change your answer to Question 1?

Any guidance before I jump into the rabbit hole would be helpful?

Thanks in advance.",2,tensorflow,2020-10-13
grc048,Abnormal execution order in tf.function while looping through Dataset,"Given the following code snippet. 
If executed in TF 2.0, the result is `1 2 3 3`.
If executed in TF 2.1, the result is `1 2 3 0`.
Why there are differences between two versions and why the second output is valid?
```
import tensorflow as tf

a=tf.Variable(0)

@tf.function
def f():
    dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
    for i in dataset:
        a.assign(i)
        tf.print(a)
    tf.print(a)

f()
```",2,tensorflow,2020-10-13
gram75,Most computational efficient way for list of random numbers in Tensroflow given a list of maxiumum values like in `np.random.randint`,"For `np.random.randint`, you can input a list of maximum values, and get a list of random ints from 0 to those maximum values. 

    np.random.randint([1, 10, 100, 1000] )
    
    &gt;array([  0,   7,  31, 348])

Tensorflow `tf.random.uniform` doesn't allow lists for `maxval`, so you need to either create a statement for each, or run a loop. I was wondering if there was more elegant way to get these random numbers.",1,tensorflow,2020-10-13
gr933f,"Deep Fashion3D, the largest collection to date of 3D garment models",,6,tensorflow,2020-10-13
gr3tks,"Anyone interested in using NLP for making search engines for Covid-19 papers? We have a casual group working on this, looking for others who have an interest in IR / scientific texts. We working with a ton of data, so focused on working with TPUs.",,9,tensorflow,2020-10-13
gr0n6i,John Snow Labs Spark-NLP 2.5.1: Adding support for 6 new BioBERT and ClinicalBERT models,,2,tensorflow,2020-10-13
gqu4wi,MedicalAI Tutorial: Image Classification in 5 Lines of Code [Based on Tensorflow 2.0],,6,tensorflow,2020-10-13
gqrgem,MNIST GPU memory error with RTX5000,"Hi, this may be a noob question that has been answered somewhere else, but I could not find the answer. So I am trying the  Basic classification: Classify images of clothing tutorial on TensorFlow ( [https://www.tensorflow.org/tutorials/keras/classification](https://www.tensorflow.org/tutorials/keras/classification) ) using my laptop with an RTX5000 graphic card, which comes with 16GB memory.  I always get this "" **InternalError**:  Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 128), m=32, n=128, k=784 	 \[\[node sequential/dense/MatMul (defined at &lt;ipython-input-7-f5657308f9a1&gt;:4) \]\] \[Op:\_\_inference\_train\_function\_519\]  Function call stack: train\_function "" everytime I tried to execute [model.fit](https://model.fit/) command. I noticed the GPU memory usage instantly jumped to 15.7/16GB when I started fitting, and the error popped up before the first pouch finished. The error goes away if I force to use CPU, the memory usage is less than 1GB if I switched to CPU based on task manager.   
What could be wrong, and how to fix this?  
I am using TensorFlow 2.2.0.  
Many thanks.",1,tensorflow,2020-10-13
gqqbyl,why is my validation accuracy so high?,"I'm training a neural net on some sign language images I found online. I'm using the built-in validation_split. For some reason, my validation accuracy is consistently higher than my training accuracy. On one test I even got a 1.0 on accuracy, with a 0.9699 as my training accuracy. For reference, my test accuracy was 0.8063.

I understand that testing accuracy is expected to be lower. I though validation accuracy would be similar.

How is this possible? Is this normal? Or am I doing something wrong?",2,tensorflow,2020-10-13
gqa73l,How to generate (music) sequences from a time series for training an RNN-LSTM net that generates melodies,"In this tutorial, you can learn how to generate sequences for training an RNN-LSTM. This video is part of the “Generating melodies with LSTM nets”. The series aims to teach you how to build an artificial neural network (RNN-LSTM) to generate effective melodies.

Here’s the video:

https://www.youtube.com/watch?v=prnwPdvSyBY&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=6",10,tensorflow,2020-10-13
gq7iwl,I'm simulating Gradient Descent Using Tensorflow. Someone help me with optimization,"`import numpy as np`

`import tensorflow as tf`

`X = tf.cast(tf.constant([1,2,3,4,5]),tf.float32)`

`y = tf.cast(tf.constant([10,20,30,40,50]),tf.float32)`

&amp;#x200B;

`m = tf.constant(0,dtype=""float32"")`

`c = tf.constant(0,dtype=""float32"")`

`a = 0.1`

&amp;#x200B;

`def dm(X,y,y_hat):`

`dm = tf.constant(0, dtype = ""float32"")`

`with tf.Session() as sess:`

`for i in range(5):`

`dm += (y_hat[i] - y[i])*X[i]`

`return dm/5`     



&amp;#x200B;

`def grad_desc(X,y,m,c,a):`

`with tf.Session() as sess:`

`for i in range(1000):`

`y_hat = tf.multiply(m,X)+c`

`c = c - tf.reduce_sum(y_hat - y)*(a/5)`

`m = m - tf.cast(a*dm(X,y,y_hat),tf.float32)`

`print(`[`sess.run`](https://sess.run)`([m,c]))`

`grad_desc(X,y,m,c,a)`

&amp;#x200B;

Do I have to explicitly take care of dtype every time? I got many errors before so I tried to mention dtype while declaring a constant.

Also, is this valid? : `dm += (y_hat[i] - y[i])*X[i]`",2,tensorflow,2020-10-13
gq6sxm,Create Deepfakes in 5 Minutes with First Order Model Method,,0,tensorflow,2020-10-13
gq3sw7,Tensorboard projector takes too long to import,"Hi there!

I love tensorflow's embedding projector, nevertheless it takes too long to import.

The line

`from tensorflow.contrib.tensorboard.plugins import projector`

takes about 9 to 12 seconds. Anyone else with this problem? What can I do to import the projector faster?",1,tensorflow,2020-10-13
gpirkf,TFRecord and TfExample?,"There's so much less info about these. Someone's has link or something that explains it clearly? Also, from where do i learn TF? Tbh the Google documentation sucks",8,tensorflow,2020-10-13
gpfx23,Failed to Convert a NumPy array to a Tensor,"I researched this problem, but when I found the answer, I didn't quite understand it. I am still fairly new to Tensorflow. The answer was to use \`np.asarray\`. But the I didn't know what and where to use it on.

`import tensorflow as tf`  
`import pandas as pd`  
`import numpy as np`  
`train_data_path = ""C:/Users/User/Desktop/Machine_Learning/Neural_Network/BTCUSD.csv""`  
`TRAIN_DATA = pd.read_csv(train_data_path)`  
`train_target = TRAIN_DATA.pop(""Close"")`  
`eval_data_path = ""C:/Users/User/Desktop/Machine_Learning/Neural_Network/BTCUSD_eval.csv""`  
`EVAL_DATA = pd.read_csv(eval_data_path)`  
`eval_target = EVAL_DATA.pop(""Close"")`  


`model = tf.keras.models.Sequential()`  
`model.add(tf.keras.layers.Flatten(input_shape=(1973, 8)))`  
`model.add(tf.keras.layers.Lambda(`  
 `lambda x: tf.expand_dims(model.output, axis=-1)))`  
`model.add(tf.keras.layers.LSTM(128, activation=""tanh""))`  
`model.add(tf.keras.layers.Dense(1))`  
`model.compile(`  
 `optimizer=tf.keras.optimizers.Adam(),`  
 `loss=tf.keras.losses.mean_absolute_error,`  
 `metrics=['Accuracy']`  
`)`  
`train = model.fit(`  
`TRAIN_DATA, train_target,`  
 `batch_size=32,`  
 `epochs=10`  
`)`

&amp;#x200B;

And here are my errors:

`Traceback (most recent call last):                                                                                                                                                                                                             File ""neural_network.py"", line 31, in &lt;module&gt;                                                                                                                                                                                                 epochs=10                                                                                                                                                                                                                                  File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\keras\engine\`[`training.py`](https://training.py)`"", line 66, in _method_wrapper                                                                                                                       return method(self, *args, **kwargs)                                                                                                                                                                                                       File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\keras\engine\`[`training.py`](https://training.py)`"", line 815, in fit                                                                                                                                  model=self)                                                                                                                                                                                                                                File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 1112, in __init__                                                                                                                        model=model)                                                                                                                                                                                                                               File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 364, in __init__                                                                                                                         dataset = self.slice_inputs(indices_dataset, inputs)                                                                                                                                                                                       File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 390, in slice_inputs                                                                                                                     dataset_ops.DatasetV2.from_tensors(inputs).repeat()                                                                                                                                                                                        File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 562, in from_tensors                                                                                                                          return TensorDataset(tensors)                                                                                                                                                                                                              File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2839, in __init__                                                                                                                             element = structure.normalize_element(element)                                                                                                                                                                                             File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\data\util\`[`structure.py`](https://structure.py)`"", line 98, in normalize_element                                                                                                                       ops.convert_to_tensor(t, name=""component_%d"" % i))                                                                                                                                                                                         File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\framework\`[`ops.py`](https://ops.py)`"", line 1341, in convert_to_tensor                                                                                                                           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)                                                                                                                                                                        File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 321, in _constant_tensor_conversion_function                                                                                                 return constant(v, dtype=dtype, name=name)                                                                                                                                                                                                 File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 262, in constant                                                                                                                             allow_broadcast=True)                                                                                                                                                                                                                      File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 270, in _constant_impl                                                                                                                       t = convert_to_eager_tensor(value, ctx, dtype)                                                                                                                                                                                             File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 96, in convert_to_eager_tensor                                                                                                               return ops.EagerTensor(value, ctx.device_name, dtype)                                                                                                                                                                                    ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).` ",3,tensorflow,2020-10-13
gpfld1,$15 gift card for 30 mins to better understand data science bottlenecks!,"Hi folks! 

We're a small team in the San Francisco Bay Area looking to better understand the bottlenecks Data Scientists face in their day to day lives while building ML models.

We only need 30 minutes of your time, and in exchange we are willing to offer you a \*$15 Gift card for Starbucks or Amazon! (your choice)\*

Please fill out this super-short form to help coordinate!

[https://forms.gle/L96iEorkr8ttT6bg7](https://forms.gle/L96iEorkr8ttT6bg7)

Thank you very much!",0,tensorflow,2020-10-13
gp95gt,Text recognition,"Hi

I'm new to tensorflow and played around with the hand written numbers MNIST set.  
I'd like to do my own project that recognises text instead of numbers but can't find a good tutorial. Is it the same principle as numbers but instead of 10 layers at the end I have to use 26? Or include upper and lowercase and special characters?

If so I'd have to first crop the words into each character, right? Or is there a way to recognise entire sentences?

I'd like to train three different fonts, so no handwriting, and don't care about upper or lower case.

Later I'd like to use the trained model on photographs. A printed article for example. Does the model work if I align the image, do I have to retrain for a little bit or train it from the start with the new data?

Where do I start? The [Keras example](https://keras.io/examples/image_ocr/) is overwhelming.",1,tensorflow,2020-10-13
gp40nf,Running prediction saved_model locally is extremely slow,"After much hair-pulling we have managed to get saved models (output from ML) running locally on instances (n1-standard-4) via the C-API.

Querying the model for a prediction is absurdly slow, so slow that we reduced it down to a model that takes one float input. It adds about 40ms to the latency of each request, which is unacceptable. These models aren't that complicated. I should add that adding GPUs to these instances is not an option.

I'm not tuning the runtime config in any way, and I can see Tensorflow creating its own thread pool but even these threads hardly go over 40% CPU usage. This would suggest to me that there's a quite a bit of blocking going on internally in the model execution.

Any ideas? I can't believe it's so slow.


Edit: Here's a dump of the graph's operations:
https://pastebin.com/XBQvQvRb",6,tensorflow,2020-10-13
gozfzo,How to change momentum during training in TensorFlow 2.2," I want the momentum to cycle from 0.95 to 0.85. I have seen some Keras implementation where you can K.set(model.opt.momentum, value) in the callback. However, in TensorFlow there is no momentum attribute in SGD optimizer. I'm using TensorFlow 2.2

Will this code below work? How can I know if it work?

    class momsec(tf.keras.callbacks.Callback):
    
    
        def __init__(self, initial_mom,maximal_mom,step_size):
            super().__init__()
    
            self.initial_mom = initial_mom
            self.maximal_mom = maximal_mom
            self.step_size=step_size
            self.step=0
    
    
        def __call__(self):
            with tf.name_scope(""CyclicalMom""):
    
    
                initial_mom = tf.convert_to_tensor(
                self.initial_mom, name=""initial_mom"")
                dtype = initial_mom.dtype
                maximal_mom = tf.cast(self.maximal_mom, dtype)
                step_size = tf.cast(self.step_size, dtype)
                cycle = tf.floor(1 + self.step / (2 * step_size))
    
                x = tf.abs(self.step / step_size - 2 * cycle + 1)
    
                mom=initial_mom + (
                        maximal_mom - initial_mom
                    ) * tf.maximum(tf.cast(0, dtype), (1 - x)) 
                self.step+=1
                return mom
    
        def get_config(self):
            return {
                ""initial_mom"": self.initial_mom,
                ""maximal_mom"": self.maximal_mom,
                ""step_size"": self.step_size,
    
            }    
    mom=momsec(0.95,0.85,600)
    opt = tensorflow.keras.optimizers.SGD(learning_rate=lr,momentum=mom,nesterov=True)",4,tensorflow,2020-10-13
goxnli,Can a Model maintain the order of prediction in which the features were given to it.,"Hi Everyone,

Can a keras model maintain the order of predictions in loss function based on the order of the batch of input features given to it while training.? if not how can i make it remember the order because i need this order in my custom loss function currently its failing which i think is because its not following the order.  
e.g 

features x,y,z &amp; ground truth of x=0, y=1,z=2 so in predictions i want the same order p\_x,p\_y,p\_z.",1,tensorflow,2020-10-13
goxmhn,Improving semantic segmentation for urban-scene images,,2,tensorflow,2020-10-13
gop9v0,"No module named Tensorflow_datasets , although tf version &gt; 2.0 ?","&amp;#x200B;

[Hi,](https://preview.redd.it/gv35o9x03d051.jpg?width=1423&amp;format=pjpg&amp;auto=webp&amp;s=87c04a625598c0399bd5db788bb4725e77296d26)

As  mentionned in the title, I get an error message saying there's no module named tensorflow\_datasets hen I try to import it from tensorflow (if I understand correctly). 

I also read that as long as tf version is &gt; 2.0, I don't need to install TensorFlow datasets library because it's enabled by default. ( [https://intelligence-artificielle.agency/tensorflow-dataset/](https://intelligence-artificielle.agency/tensorflow-dataset/) )

&amp;#x200B;

I'm using Python 3. Thank you if you can help me to solve the issue here.",1,tensorflow,2020-10-13
gomt3k,TfLiteGpuDelegate error,"Hi everyone, I know this is probably not the right place to post this. 

&amp;#x200B;

I am trying to run an android project running a tflite model. I am getting the following error:  
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: New object definition is not supported.  
TfLiteGpuDelegate Prepare: delegate is not initialized  


&amp;#x200B;

Now, Can anyone let me know what the issue is. I am really not even able to understand the error, there 's not much I  found on googling.",1,tensorflow,2020-10-13
gomiqc,"[TF2] Newbie trying to learn object tracking on a custom dataset, hoping to get pointed in the right direciton","I'm very new to python (but not coding, I'm a professional web developer) and tensorflow, and I've been looking for a good tutorial to walk through to try to write an object tracker for a custom data set (namely something that will track sport fencers in video - I've used a couple tutorials with various COCO models, but they don't track the fencers so well, i think because of the masks and full body white suits).

I keep finding various tutorials, but I'll get halfway through, and one of the steps will just flat-out not run, followed by me doing a to of googling to find that the versions are now out of date so the tutorial doesn't work, or that it doesn't work on my machine (windows 10 with nvdia gforce GPU), or something.

I've started and gotten hung up on about 4-5 tutorials now, and I was hoping that maybe someone on here could point me in the right direciton. Some questions:

What version of tensorflow should I work with? 1 or 2? 

I'm currently trying to use Tensorflow 2, but a lot of tutorials say that they don't work on tensorflow2. Is it worth rolling back and trying to do those tutorials? Ultimately I want to run an object tracker on a bunch of video clips on a server that doesn't have a GPU on it (doesn't need to be real time, I'm happy to let it run over many days), and I heard TF1 doesn't do CPU rendering though. 

Also what version of CUDA should I be running? I got TF2 running with 10.1, and I get the sense that it doesn't quite run with 10.2

Should I be trying to produce a custom model using transfer learning?

Are there any tutorials or good starting points that anyone can point me to that will get me going?",3,tensorflow,2020-10-13
gojuc3,State-of-the-art transfer learning ...,,14,tensorflow,2020-10-13
gocgp1,"ValueError: A target array with shape (32, 19) was passed for an output of shape (None, 43) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.","I tried running my script with just 3 classes for training and tests, it ran with no errors, but after I tried adding all of my classes with 43 classes on training and 19 classes for testing, it shows the error   
I'm still new to this, what am I missing on the script? here is my   


\`\`\`  
**from** \_\_future\_\_ **import** absolute\_import, division, print\_function, unicode\_literals  
**import** matplotlib.pylab **as** plt  
**import** tensorflow **as** tf  
**import** tensorflow\_hub **as** hub  
**from** tensorflow.keras **import** layers  
classifier\_url **=**""https://tfhub.dev/google/tf2-preview/mobilenet\_v2/classification/2""   
IMAGE\_SHAPE **=** (224, 224)  
classifier **=** tf.keras.Sequential(\[  
hub.KerasLayer(classifier\_url, **input\_shape=**IMAGE\_SHAPE**+**(3,))  
\])  
**import** numpy **as** np  
**import** PIL.Image **as** Image  


imagenet\_labels **=** np.array(open('./sample\_dataset/labels.txt').read().splitlines())  
train\_path **=** 'sample\_dataset/Training'  
test\_path **=** 'sample\_dataset/Test'  
export\_path **=** 'sample\_dataset/tensorflow/mobilenet'  
model\_analytics\_path **=** 'sample\_dataset/tensorflow/mobilenet/mobilenet\_info'  
image\_generator **=** tf.keras.preprocessing.image.ImageDataGenerator(**rescale=**1**/**255)  
training\_set **=** image\_generator.flow\_from\_directory(train\_path, **target\_size=**IMAGE\_SHAPE)  
test\_set **=** test\_datagen.flow\_from\_directory(test\_path, **target\_size=**IMAGE\_SHAPE)  


feature\_extractor\_url **=** ""https://tfhub.dev/google/tf2-preview/mobilenet\_v2/feature\_vector/2""   
feature\_extractor\_layer **=** hub.KerasLayer(feature\_extractor\_url,  
 **input\_shape=**(224,224,3))  
feature\_extractor\_layer.trainable **=** False  


model **=** tf.keras.Sequential(\[  
  feature\_extractor\_layer,  
  layers.Dense(training\_set.num\_classes)  
\])  
model.summary()  
model.compile(  
 **optimizer=**tf.keras.optimizers.Adam(),  
 **loss=**tf.keras.losses.CategoricalCrossentropy(**from\_logits=**True),  
 **metrics=**\['acc'\])  
class CollectBatchStats(*tf*.*keras*.*callbacks*.*Callback*):  
 **def** \_\_init\_\_(*self*):  
 *self*.batch\_losses **=** \[\]  
 *self*.batch\_acc **=** \[\]  
 **def** on\_train\_batch\_end(*self*, **batch**, **logs=**None):  
 *self*.batch\_losses.append(logs\['loss'\])  
 *self*.batch\_acc.append(logs\['acc'\])  
 *self*.model.reset\_metrics()  
steps\_per\_epoch **=** np.ceil(training\_set.samples**/**training\_set.batch\_size)  
batch\_stats\_callback **=** CollectBatchStats()  
r **=** model.fit\_generator(  
  training\_set,   
 **validation\_data=**test\_set,  
 **epochs=**1,  
 **steps\_per\_epoch=**steps\_per\_epoch,  
 **validation\_steps=**len(test\_set),  
 **callbacks** **=** \[batch\_stats\_callback\]  
)  
print(r.history.keys())  
class\_names **=** sorted(training\_set.class\_indices.items(), **key=**lambda **pair**:pair\[1\])  
class\_names **=** np.array(\[key.title() **for** key, value **in** class\_names\])  
class\_names  
plt.figure(1)  
*# summarize history for accuracy*  
plt.subplot(211)  
plt.plot(r.history\['acc'\])  
plt.plot(r.history\['val\_acc'\])  
plt.title('Model Accuracy')  
plt.ylabel('Accuracy')  
plt.xlabel('Epoch')  
plt.legend(\['Training', 'Validation'\], **loc=**'lower right')  
*# summarize history for loss*  
plt.subplot(212)  
plt.plot(r.history\['loss'\])  
plt.plot(r.history\['val\_loss'\])  
plt.title('Model Loss')  
plt.ylabel('Loss')  
plt.xlabel('Epoch')  
plt.legend(\['Training', 'Validation'\], **loc=**'upper right')  
plt.tight\_layout()  
plt.savefig(model\_analytics\_path)  
**import** time  
t **=** time.time()  
model.save(export\_path, **save\_format=**'tf')  
export\_path  
reloaded **=** tf.keras.models.load\_model(export\_path)

\`\`\`",0,tensorflow,2020-10-13
go16tj,How do I use TensorFlow's LSTM model to predict multiple features?,"I've been going through the TensorFlow tutorial on [Time series forecasting using LSTM](https://www.tensorflow.org/tutorials/structured_data/time_series).  It shows great examples of predicting one feature based off of only that feature's history, and predicting one feature based off of multiple features.  However, it doesn't show how to predict multiple features based off of multiple features.

I'm assuming there is a parameter for ""output\_shape"" and some way to feed in another dimension for this, but I must be missing it completely.

My model is fairly simple. I've got a time series that consists only of 2 features, and I want to predict both of these features based off of only their history (they have a fairly predictable pattern).  For what it's worth, I also want to predict a series of outputs (like the final example in the link above).",5,tensorflow,2020-10-13
gnzmyv,Getting loss: nan in tensorflow while using custom loss function,"I am getting loss: nan in tensorflow while using custom loss

loss function code:

`def compute_cost(A,Y):    cost = tf.math.divide(tf.math.multiply(-1.0, tf.reduce_sum(tf.math.multiply(Y, tf.math.log(A)) + tf.math.multiply( tf.math.subtract(1.0, Y), tf.math.log(tf.math.subtract(1.0, A))))), tf.cast(tf.size(Y), tf.float32)) return cost` 

&amp;#x200B;

https://preview.redd.it/bxw2y09a65051.png?width=492&amp;format=png&amp;auto=webp&amp;s=e9b0bbabae467090ef14d4d731f3da9a08ce109f

`model = tf.keras.Sequential([ tf.keras.layers.Dense(32, activation = 'relu', input_shape= (784,)), tf.keras.layers.Dense(32, activation = 'relu'), tf.keras.layers.Dense(10, activation = 'softmax')]) model.compile( loss = compute_cost, optimizer = 'adam', metrics = ['accuracy'] )`",1,tensorflow,2020-10-13
gnwz7k,Common sub-networks,"Guys, I am pruning a neural network (CNN and Dense) and for different sparsity levels, I have different sub-networks. Say for sparsity levels of 20%, 40%, 60% and 80%, I have 4 different sub-networks.

Now, I want to evaluate the similar connections between them. Any idea how to visualize this or compute this?

I am using Python 3.7 and TensorFlow 2.0.

Thanks!",4,tensorflow,2020-10-13
gntr5d,Cooking recipes generator (RNN + TensorFlow),,2,tensorflow,2020-10-13
gnqjba,Catboost Tutorial on Google Colaboratory with free GPU,,8,tensorflow,2020-10-13
gnpojn,How to invoke a Hub model using the Java API?,"The following python code invokes the universal sentence encoder using \[""hello"", ""world""\] and returns an array of floats denoting their encoded representation.

    import tensorflow as tf
    import tensorflow_hub as hub
    
    module = hub.KerasLayer(""https://tfhub.dev/google/universal-sentence-encoder/4"")
    model = tf.keras.Sequential(module)
    print(""model: "", model([""hello"", ""world""]))

This code works but I'd now like to do the same thing using the Java API. I've successfully loaded the module, but I am unable to pass inputs into the model and extract the output. Here is what I've got so far:

    import org.tensorflow.Graph;
    import org.tensorflow.SavedModelBundle;
    import org.tensorflow.Session;
    import org.tensorflow.Tensor;
    import org.tensorflow.Tensors;
    import org.tensorflow.framework.ConfigProto;
    import org.tensorflow.framework.GPUOptions;
    import org.tensorflow.framework.GraphDef;
    import org.tensorflow.framework.MetaGraphDef;
    import org.tensorflow.framework.NodeDef;
    import org.tensorflow.util.SaverDef;
    
    import java.io.IOException;
    import java.nio.charset.StandardCharsets;
    import java.nio.file.Paths;
    import java.util.ArrayList;
    import java.util.List;
    
    public final class NaiveBayesClassifier
    {
        public static void main(String[] args)
        {
            new NaiveBayesClassifier().run();
        }
    
        protected SavedModelBundle loadModule(Path source, String... tags) throws IOException
        {
            return SavedModelBundle.load(source.toAbsolutePath().normalize().toString(), tags);
        }
    
        public void run()
        {
            try (SavedModelBundle module = loadModule(Paths.get(""universal-sentence-encoder""), ""serve""))
            {
                Graph graph = module.graph();
                try (Session session = new Session(graph, ConfigProto.newBuilder().
                    setGpuOptions(GPUOptions.newBuilder().setAllowGrowth(true)).
                    setAllowSoftPlacement(true).
                    build().toByteArray()))
                {
                    Tensor&lt;String&gt; input = Tensors.create(new byte[][]
                        {
                            ""hello"".getBytes(StandardCharsets.UTF_8),
                            ""world"".getBytes(StandardCharsets.UTF_8)
                        });
                    List&lt;Tensor&lt;?&gt;&gt; result = session.runner().feed(""serving_default_inputs"", input).
                        addTarget(""???"").run();
                }
            }
            catch (IOException e)
            {
                e.printStackTrace();
            }
        }
    }

I used [https://stackoverflow.com/a/51952478/14731](https://stackoverflow.com/a/51952478/14731) to scan the model for possible input/output nodes. I believe the input node is ""serving\_default\_inputs"" but I can't figure out the output node. More importantly, I don't have to specify any of these values when invoking the code in python through Keras so is there a way to do the same using the Java API?",1,tensorflow,2020-10-13
gnnzbf,BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation: Paper and Code,,2,tensorflow,2020-10-13
gnnu7v,Semantic Segmentation from Image Labels,,3,tensorflow,2020-10-13
gnlzmq,Error while trying to load pre-trained model on Kaggle,"Hi guys,

Today I am working on a notebook on Kaggle. I want to load pre-trained models from tf.keras.applications for some transfer learning task but fail to do so.  
It keeps outputting this error:

 ""Exception: URL fetch failure on [https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception\_v3\_weights\_tf\_dim\_ordering\_tf\_kernels\_notop.h5:](https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5:) None -- \[Errno -3\] Temporary failure in name resolution""

Could anyone please help me with this problem?

Thank you very much!",1,tensorflow,2020-10-13
gnjrcw,Anyone knows how to use weights trained with tf1 in tf2,"Hi everyone,

&amp;#x200B;

I currently have pretrained weights stored in .data-00000-of-00001 and .index file, and I have already converted the tf1 file into tf2. However, I cannot seem to load the weights into the network written in tf2, as it says RuntimeError: Key \_CHECKPOINTABLE\_OBJECT\_GRAPH not found in checkpoint. is anyone able to help?",8,tensorflow,2020-10-13
gnjgfr,Install Tensorflow 2.2 with NVIDIA GPU enabled on Ubuntu 20.04,"Sources: [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)

[https://youtu.be/BQ6bxQ-7h8Y](https://youtu.be/BQ6bxQ-7h8Y)

&amp;#x200B;

**Step 1: Set up CUDA:**

Be sure to download the appropriate version of CUDA from their website ([https://developer.nvidia.com/cuda-toolkit-archive](https://developer.nvidia.com/cuda-toolkit-archive)).  For this tutorial, we are using CUDA 10.1.

In terminal:

CC=$(which gcc-8)

CXX=$(which g++-8)

sudo apt-get update

sudo sh cuda\_10.1.105\_418.39\_linux.run –override

&amp;#x200B;

**Step 2: Modify .bashrc**

You will also need to append the following to .bashrc:

export LD\_LIBRARY\_PATH=$LD\_LIBRARY\_PATH:/usr/local/cuda/extras/CUPTI/lib64

export PATH=$PATH:/usr/local/cuda-10.1/bin

export LD\_LIBRARY\_PATH=$LD\_LIBRARY\_PATH:/usr/local/cuda-10.1/lib64

&amp;#x200B;

**Step 3: Set up cuDNN**

You will also need cuDNN, which you can get from here: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn) .

In terminal:

sudo cp cuda/include/cudnn.h /usr/local/cuda/include

sudo cp cuda/lib64/libcudnn\* /usr/local/cuda/lib64

sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn\*

&amp;#x200B;

**Step 4: Install Tensorflow**

After running 'source \~/.bashrc' or resetting your session:

sudo apt install python3-pip

pip3 install tensorflow

&amp;#x200B;

**Step 5: Test your installation**

To test your installation, run the following and verify there aren't any warnings or errors:

python3 -c ""import tensorflow as tf;print(tf.reduce\_sum(tf.random.normal(\[1000, 1000\])))""",3,tensorflow,2020-10-13
gncn3y,An example of Tensorflow running at edge: interested?,,4,tensorflow,2020-10-13
gn4abx,(Request) Tensorflow v2 / ML resources request for a beginner (Python),,10,tensorflow,2020-10-13
gn2uru,Would an external GPU help run python code faster?,,2,tensorflow,2020-10-13
gn2igf,Twitter NER Datasets,,1,tensorflow,2020-10-13
gn20nx,tf.keras.utils.Sequence vs tf.data,"What are the advantages of using tf.keras.utils.Sequence as opposed to using [tf.data](https://tf.data) anf vice-versa? I know both ""act"" like generators reading in data from disk every batch, and I know that in the case of multiprocessing, tf.keras.utils.Sequence guarantees that each sample from the training set will be used by the network only once each epoch. Is there something else?",1,tensorflow,2020-10-13
gn1efy,Does anyone know if being able to use tensor locally requires some specific hardware on your computer?,,1,tensorflow,2020-10-13
gmvujq,Hyper-Scale Machine Learning with MinIO and TensorFlow,"[Hyper-Scale Machine Learning with MinIO and TensorFlow](https://blog.min.io/hyper-scale-machine-learning-with-minio-and-tensorflow/)

This is a [blog post](https://blog.min.io/hyper-scale-machine-learning-with-minio-and-tensorflow/) about how to leverage MinIO Object Storage to build scalable Machine Learning and AI Pipelines that train and access all the data they need straight from MinIO. This has huge implications, for starters you don't need expensive NVMe memory locally to have a high speed access to your own data, if your training hardware has 25Gbps or 100Gbps ethernet then you could be accessing your datasets at much higher speed than a local NVMe could offer also you could be accessing much larger datasets and avoid having to copy the datasets to your training hardware, potentially opening the gates to distributed training.",1,tensorflow,2020-10-13
gmvpku,TensorFlow I/O now supports AVIF input,,1,tensorflow,2020-10-13
gmvnt1,Expand Keras tokenizer,"Hi I am beginner in ML and TensorFlow. I have already practiced on some of data-sets to build NN classifier with help of Keras tokenizer to tokenize sentences.

I wonder what is good practice to expand the tokenizer. Let’s say I have built a model for some classification task. Some time later when I would like to add more training examples for the model to fit I have to use the same dictionary of tokens. What If my new data-set which I want to fit the model (In additional to the old one) contains new words? I would like to avoid OOV tokens in training data-Set.

Using fit_on_words on initialized tokenizer just overwrites the old dictionary which for obvious reasons is not acceptable (tokens order may vary).",1,tensorflow,2020-10-13
gmux1x,10 Minute Tensorflow Setup for Jetson Nano Hardware,,10,tensorflow,2020-10-13
gmo1q6,"Shaastra - IIT Madras, Spotlight Stay@Home Lecture Series"," 

https://preview.redd.it/ex9s900qypz41.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=12368d07b01133ab6c3232aa4881e5d7add4b1a3

Shaastra Spotlight Stay@Home series brings to you a fascinating fireside chat with one of the pioneers in the field of Machine Learning - Mr. Rajat Monga!

Mr. Rajat Monga is the Co-Founder and former leader of TensorFlow, an open-source machine-learning library, and the center of Google’s efforts at scaling up deep learning. He is one of the founding members of the Google Brain team &amp; is known for his ground-breaking work in Artificial Intelligence. A veteran developer, Rajat has worked at eBay, Infosys, Attributor, DistBelief, and a number of startups.

Catch his enthralling talk live only at Shaastra IITM’s Youtube channel:-

Link:[ https://youtu.be/qkWPPxOQkb0](https://youtu.be/qkWPPxOQkb0)

Date: 20th May 2020

Time: 11:30 AM IST

\#BeintheSpotlight",12,tensorflow,2020-10-13
gmnihe,"Fireside chat with Tensorflow Co-founder Rajat Monga, Shaastra IIT-Madras","After that inspiring inaugural lecture, the Shaastra Spotlight Stay@Home series brings to you a fascinating fireside chat with one of the pioneers in the field of Machine Learning - Mr. Rajat Monga! 

Mr. Rajat Monga is the Co-Founder and former leader of TensorFlow, an open source machine-learning library and the center of Google’s efforts at scaling up deep learning. He is one of the founding members of the Google Brain team &amp; is known for his ground-breaking work in Artificial Intelligence. A veteran developer, Rajat has worked at eBay, Infosys, Attributor, DistBelief and a number of startups.

Catch his enthralling talk live only at Shaastra IITM’s Youtube channel:-

Link: https://youtu.be/qkWPPxOQkb0
Date: 20th May 2020
Time: 11:30 AM IST

#BeintheSpotlight",4,tensorflow,2020-10-13
gmfzxf,What is the difference between a Tensor and a DataSet in TensorFlow 2.0? (noob question),"I've been having trouble understanding the difference between a Tensor and a DataSet. From the definitions I managed to find in the documentation:

[Tensors:](https://www.tensorflow.org/guide/tensor)

&gt;Tensors are multi-dimensional arrays with a uniform type

[Datasets:](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)

&gt;Represents a potentially large set of elements.

Is a DataSet a special type of big tensor?

My understanding is that Tensors are basically pandas DataFrames, they are rectangular structures with a uniform type along a ""column"" (or tensors need to be entirely uniform?). If that's the case then why do DataSets exist? Do they have additional functionalities that Tensors do not?

&amp;#x200B;

I'm kinda lost in the documentation of different tensor types and with the transition to tensorflow 2.0 I'm never sure if the information I'm finding online is deprecated. Any help would be appreciated.

&amp;#x200B;",2,tensorflow,2020-10-13
gmdybo,Does anyone know of any tutorials of a voice recognition model that can differentiate between voices?,"I want a model that can detect who's speaking _not what they're saying_. If you don't know of a tutorial, do you know how one would structure a model to do so?",4,tensorflow,2020-10-13
gmbp2r,Separate a target speaker's speech from a mixture of two speakers,,3,tensorflow,2020-10-13
gm6vgd,"Tensorboard: ""For faster results, the data will be sampled down to 10,000 points."" How can I avoid this?","I'm running tensorboard locally. I generated a lot of sentence embeddings (92K) for a list of 92K sentences. Now I want to visualize them with T-SNE. I got it working great so far on Tensorboard except for that message. If I understand correctly it's not showing all the sentences / data points, but only 10K. How can I get rid of this limit so that it shows all 92K?

  
Or did I misunderstand what this means?   


I saw this: [https://stackoverflow.com/questions/43702546/tensorboard-doesnt-show-all-data-points](https://stackoverflow.com/questions/43702546/tensorboard-doesnt-show-all-data-points)  


But trying both suggestions:  
 

\--sames\_per\_plugin scalars=0

and:

\--sames\_per\_plugin sentence\_embeddings=0  


And changing ""scalar\_metadata.PLUGIN\_NAME: 1000"" to ""scalar\_metadata.PLUGIN\_NAME: 92324""  


Didn't seem to do anything. The message ""For faster results, the data will be sampled down to 10,000 points.""  still shows up. Perhaps that message shows up regardless?",4,tensorflow,2020-10-13
gm6ost,Project Management Tool Research,"Hi TF Devs!

I’m a computer science student from Berlin and part of a research project where we want to understand how project management tools are used in software-development teams.  
That is why I am asking you: What are your opinions on the tools you use? Help me with my research by filling out the survey below!  
Thank you!

[Click here to start survey](https://pmtoolstudy.typeform.com/to/sdzA6D)",0,tensorflow,2020-10-13
gm0afh,TensorFlow Lite: Accelerate your Android and iOS App with AI,,5,tensorflow,2020-10-13
glyvrp,Optimal way to precompute augmentation,"What are the best practices when using precomputed augmentations. I have an application where I'd like to change the label values based off an augmentation technique. To avoid heavy computation during training, I'd like to precompute it and use it. I know I won't be able to get as many changes as say if I do random scaling between some range. A specific questions I have:

 What's the best way to avoid training with more than one manipulation of a single input in one epoch?",1,tensorflow,2020-10-13
glygzz,Preprocessing music for melody generation: Collapsing the dataset in a single file,"I published a new tutorial in my “Generating melodies with LSTM nets” series. This time you can learn how to collate all the songs encoded as a time series in a single file. Also, create a mapping to encode the symbolic notation with integers, in order to get the data ready to be ingested by the neural network.

The series “Generating melodies with LSTM nets”, aims to teach you how to build an artificial neural network (RNN-LSTM) to generate effective melodies.

Here’s the video:

https://www.youtube.com/watch?v=ZChzgFE9ye0&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=5",6,tensorflow,2020-10-13
glxo0d,Best certifications and/or resources to learn TensorFlow?,I can't say I'm new to ML/DL.I have precisely used Keras to train models but I wish to learn core TF concepts and master openCV module. How do I go about it? Where do I begin? How is the deeplearning.ai coursera course for TensorFlow?,8,tensorflow,2020-10-13
glx1ip,Receptive Field Arithmetic for Convolutional Neural Networks,,1,tensorflow,2020-10-13
glrs1z,Tensorflow GUI,Is a GUI possible for Tensorflow?,0,tensorflow,2020-10-13
glpg9m,New to tensorflow dont understand what the heck is going on.,"So i am just a freshman high schooler in his room bored i recently picked up python and learned syntax, pygame, tkinter, scrapy, beautifulsoup , and a few more pretty quickly and im planning on becoming fluent in opencv. But it seems that this knowlege can only really bring me so far as there is this looming shadow cast over me called; machine learning. Now i have messed around in tensorflow a bit trying to use google guides and it just really seems that they try to explain what they are doing without going into much deatil and it sounds like gibberish. Now i am very passionate about learning how to use tensorflow but i dont understand what most of the functions or things even do, and i really dont want to end up compiling code taken from stack overflow or the coding gods of reddit. Is there any really easy to digest guides on how to use and understand tensorflow? So much would be appreciated",5,tensorflow,2020-10-13
glmkwl,Newbie in Deep Learning having some troubles implementing Data Augmentation to an AlexNet CNN. Can anyone give some help? I will very appreciate it.,"(Sorry about the chewed english, It's note my first language, hehe)

So, I'm very newbie in Neural Network development and I'm having some troubles to implement **Data Augmentation** into my CNN. I'm using an AlexNet model to train some data to identify between flowers. 

Firts, I try running my model without Data Augmentation. And get an accuracy in training data of 0.97 and in test data an accuracy of 0.66 so I tryed to use D.A. to get better results. I understand the idea of D.A., that expand the set of data (images in my case) by making adjust to the original data like shear, zoom, flips, etc. 

My running code is like this:

`start_time = time.time()`  
`history = oxflower17_model.fit(train_x, train_y, batch_size=64, epochs=50, verbose=1, validation_split=0.2, shuffle=True)`  
`end_time = time.time()`  
`print(""Time for training: {:10.4f}s"".format(end_time - start_time)`

Firts of all, it give me the amount od data I have: 1088 images for  Training Data and 272 for Test Data. Then, the training by epoch begins.  

Since I have very few images, I decided to implement D.A. but I haven't been able to stick it (so to speak) to my Network training.

My code for Data Augmentations is like this:

`from keras.preprocessing.image import ImageDataGenerator`  
`train_datagen = ImageDataGenerator(`  
`shear_range = 0.2,`  
`zoom_range = 0.2,`  
`horizontal_flip = True`  
`)`

Next, I compute quantities required for featurewise normalization.

`train_datagen.fit(train_x)`  
`train_generator = train_datagen.flow(`  
`train_x,`  
`train_y,`  
`batch_size = 64`  
`)`

`get_steps_augment = 64`   
`print (""train_x shape: "" + str(train_x.shape[0]))`  
`steps = int(train_x.shape[0]/get_steps_augment)`  
`print(""Augmentation steps = {}"".format(steps))`

Finally, for running the model:

I did a first tests using my original running code that I showed up at the begining. But Train data and test data kept 1088 and 272 images respectively, making me see that the increase in images by D.A. did not apply. So I decide to change my code:

`start_time = time.time()`  
`history = oxflower17_model.fit(train_generator, epochs=50, verbose=1, validation_split=0.2, shuffle=True)`  
`end_time = time.time()`  
`print(""Time for training: {:10.4f}s"".format(end_time - start_time)`

But it shows  ValueError: If your data is in the form of a Python generator, you cannot use \`validation\_split\`.

Well... thats all as far. Here you can se my entire code whit all the packages that I importa, graphics, normalization, network architecture, etc. In case it is of more help \^\^

[This one is my CNN whitout Data Augmentation](https://www.kaggle.com/arcano97/alexnetflowersda)

[And this is my attempt of apply Data Augmentation](https://www.kaggle.com/arcano97/alexnet-dataaugflwrs?scriptVersionId=34198025)

Thanks for the help!",4,tensorflow,2020-10-13
gliu1j,Tensorflow backend file location,Can anyone tell me where tensorflow_backend.py file is located. It is downloaded in a virtual environment in ubuntu,3,tensorflow,2020-10-13
gkpdll,Is Deeplearn.ai's tensorflow specialization a good resource to learn tensorflow and tensorflow 2.0 functionalities?,I sign thinking about doing this specialization but I am worried that this course is a bit outdated for the new version of tensorflow,9,tensorflow,2020-10-13
gkma3g,As new as can be - not even sure where to begin - quick question,"Hello all!

I have never done ANY AI/ML anything, ever.  I'm hoping to 'train a model' (is that the right terminology?) based on some historical data to predict patterns, and I can't even figure out what kind of model or what-have-you I should be looking at.

It's people moving through rooms of a house.  At 4-5 AM on most Mondays - Fridays for example, someone will move from the top floor bedroom to the back door (picked up on various motion sensors along the way) to have a smoke in the morning.  On a Friday or Saturday night, around 2-4 AM, someone will come down from upstairs and go to the kitchen for a snack, etc.  So the data will go -

4 AM Monday - Motion sensor 1, 2, 3, 5, 6

3 AM Friday - Motion sensor 1, 2, 3 &lt;pause&gt; 2, 1

etc.

I see ""linear models"" and ""time-series forecasting"" and all sorts of other things, and of all these huge sets of documentation and tutorials, I'm not even sure which one I should be reading!

If someone could please point me in the right direction to get started, I'll RTFM and all that, and my next questions will be from a place of much, MUCH better understanding!  Thanks!!

Edit: Sorry, I forgot the most important part - I then want to 'predict' that when someone triggers motion sensor 1 at 3 AM the following Friday, they're probably going to the kitchen, and then turn on the lights the whole way there (or just a bit before they hit the motion sensors for the lights along the way).  Primarily just as an exercise to learn how to do this stuff ... but also to get them to go ""Wait!  How did the light turn on before I even got to the motion sensor?  It's like it knew where I was going to go!""",3,tensorflow,2020-10-13
gk9bmy,Image Segmentation,"Hey Guys, I have a question: I have a dataset having many images. Where each image has labels to explain it's components. An example of such an image is attached.

&amp;#x200B;

https://preview.redd.it/kbxniap1nxy41.jpg?width=1006&amp;format=pjpg&amp;auto=webp&amp;s=12c82a3adb93b13718d225b6f912d89eb3ec9e13

I know about Conv nets for image classification but have little idea of how to approach this problem. Should  I be looking into image segmentation? Can you please provide tutorials where I can read sample code and then experiment? I am using Python 3.7 and TensorFlow 2.0.

Thanks!",2,tensorflow,2020-10-13
gk756u,This Week in AI - Issue #18 | Rubik's Code,,5,tensorflow,2020-10-13
gk66hu,How do I override ```tf.keras.Model.make_train_function()```?,"I am working on a GAN to convert text to images. I'm trying to replicate the efforts of [this paper](https://arxiv.org/abs/1912.10479). I decided to leverage the `tf.keras.Model` subclassing technique to code this. I had read about the ability to override some methods such as `tf.keras.Model.train_step()` as well as `tf.keras.Model.make_train_function()`. I wrote my code following a pattern similar to [this notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/dcgan_overriding_train_step.ipynb#scrollTo=wdGWfkUopTOU) by F. Chollet. However, when trying to override the method, I am unable to pass in custom datasets. I need to be able to pass in 3 different batched `tf.data.Dataset` objects. Is there any way I can override the `tf.keras.Model.make_train_function()` to feed into my dataset and train it? I have created [this notebook](https://colab.research.google.com/drive/1qIHH7opDRPYmyeKYReVmgpTN14r_g_1V?usp=sharing) which shows the issues at hand.

I'm a little new to the subclassing interface and I have used it very sparingly. Any pointers on how I would be able to rectify my code are also welcome. Thanks!

&amp;#x200B;

Update: So I decided to avoid overriding the method. However, this comes at the cost of not being able to log my metrics on to Tensorboard the way I expected to. I'll try out a couple of things and update here. If anyone knows the right way to override the method though, it would be cool to learn.",1,tensorflow,2020-10-13
gk3bh2,Embedding the structural properties of nodes: Riding the GraphWave,,2,tensorflow,2020-10-13
gk0lvn,"LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery",,9,tensorflow,2020-10-13
gk0ju0,is there any roadmap for custom object detection for tensorflow 2.0 and above?,"the current object detection API can only be used until 1.15

Has anyone found a workaround.

currently I am using detecto framework as part of pytorch which is really simple and easy to use.

I use tensorflow for everything else except object detection. Would be nice if their object detection API works for custom datasets in 2.0

Is there any plan or workaround?",5,tensorflow,2020-10-13
gjwwme,Tensor is unhashable if Tensor equality is enabled,"I'm getting an error that I have no idea how to fix it, please help me! 

Traceback below and I'm running Tensorflow 2.1.0 with Python 3.7 and Keras 2.3.1

Traceback (most recent call last):

  File ""C:/Users/1234567/Desktop/YOLOv3-custom-training/image\_detect.py"", line 186, in &lt;module&gt;

r\_image, ObjectsList = yolo.detect\_img(frame)

  File ""C:/Users/1234567/Desktop/YOLOv3-custom-training/image\_detect.py"", line 169, in detect\_img

r\_image, ObjectsList = self.detect\_image(original\_image\_color)

  File ""C:/Users/1234567/Desktop/YOLOv3-custom-training/image\_detect.py"", line 114, in detect\_image

K.learning\_phase(): 0

  File ""C:\\Users\\1234567\\.conda\\envs\\tensorflowEnvironment\\lib\\site-packages\\tensorflow\_core\\python\\framework\\[ops.py](https://ops.py)"", line 705, in \_\_hash\_\_

raise TypeError(""Tensor is unhashable if Tensor equality is enabled. ""

TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental\_ref() as the key.",5,tensorflow,2020-10-13
gjwufv,Tensorflow Runtime Error,"Does anyone know how to fix this Error? :

Failed to load the native TensorFlow runtime.

&amp;#x200B;

Failed to load the native TensorFlow runtime.

&amp;#x200B;

See [https://www.tensorflow.org/install/errors](https://www.tensorflow.org/install/errors)

&amp;#x200B;

for some common reasons and solutions.  Include the entire stack trace

above this error message when asking for help.",2,tensorflow,2020-10-13
gjscod,Loading image data in TensorFlow 2.0,"Hey Guys, I am using Python 3.8 and TF2.0. I have a problem where I have about 650 different sub directories under a parent directory called 'train'. Now all of these 650 sub directories contain some input images which I would like to use for training.

Therefore, the number of class labels = 650. How can I create a data generator for this situation using TF2 ?

The tutorials I read are about loading images for 2 to 3 classes only.",2,tensorflow,2020-10-13
gjqc5h,Is there a direct substitute for tf$audio$audio_spectrogram for Tensorflow version 2.0 in R?,"I'm working through [this example][1] and it looks like tf$audio$audio_spectrogram is no longer available for Tensorflow 2.0. Is there an equivalent function I could use? I'm looking through the TF directory under [signal][2] and not seeing a 'spectrogram' function. I tried using tf$compat$v1$audio$audio_spectrogram but that didn't work. I used tf$signal$stft in a different example but not sure how to directly sub in here. I'm fairly new to Python and Tf and using TF in R. Thank you

edit: One of the problems is that stft has a line/scatter plot output when the spectrogram before was (I think) an image. And in the example, the brightness etc is adjusted, and not sure how you could do that with stft..


  [1]: https://www.kaggle.com/thegrigorian/freesound-code-with-r/data
  [2]: https://www.tensorflow.org/api_docs/python/tf/signal",1,tensorflow,2020-10-13
gjnl9v,Converting TensorFlow older to newer version.,"Hello everyone,

I am working on a project from a git hub repo which is written in older version of TF which 0.12. I have good understanding of Keras but, I don't understand this version of TF which I would like to change to Keras or newer version of TF say 2.0. Could anyone suggest me the best way to do the conversion. I am ok with any means of help, even may be a paid help from a suggested source. I am asking this because, I would like rather learn the newer version and work on improvising the current project rather trying to learn the older version and learn the equivalent newer to improvise the project.  
All possible suggestion is appreciated.  Thanks.",2,tensorflow,2020-10-13
gjn4m8,Preprocessing music for melody generation: Encoding songs as time series,"In my new tutorial you can learn how to encode songs as time series. We’ll use this music representation for feeding the song dataset to a neural network for melody generation.

This tutorial is part of the series “Generating melodies with LSTM nets”, a course that’ll teach you how to build an artificial neural network (RNN-LSTM) to generate effective melodies.

Here’s the video:

https://www.youtube.com/watch?v=QlvrfQYA-WE&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=4",7,tensorflow,2020-10-13
gjlbn0,Has anyone successfully run a saved_model with the C-API?,"I'm trying to do the above but running in to problems with the TF_SessionRun function. It appears that some operations are being given the squared number of expected inputs. The model runs fine with the saved_model_cli script with the same input values, so I'm obviously forgetting a call somewhere, but I don't know where. I don't change the graph, just run it to get a prediction.

There's an SO question here: https://stackoverflow.com/questions/61787472/reshape-input-layer-requested-shape-size-always-input-shape-size-squared

Any help appreciated.

TF_SessionRun status: 3:Input to reshape is a tensor with 2 values, but the requested shape has 4
	 [[{{node input_layer/source_indicator/Reshape}}]]

TF_SessionRun status: 3:Input to reshape is a tensor with 28 values, but the requested shape has 784
	 [[{{node input_layer/country_indicator/Reshape}}]]

TF_SessionRun status: 3:Input to reshape is a tensor with 2 values, but the requested shape has 4
	 [[{{node input_layer/multiplier_indicator/Reshape}}]]

TF_SessionRun status: 3:Input to reshape is a tensor with 6 values, but the requested shape has 36
	 [[{{node input_layer/id_indicator/Reshape}}]]

TF_SessionRun status: 3:Input to reshape is a tensor with 2 values, but the requested shape has 4
	 [[{{node input_layer/source_indicator/Reshape}}]]",3,tensorflow,2020-10-13
gjjopw,New to TF and looking for simple Tutorials for Object Detection,"Most of the tutorials on YouTube teach you how to train your own Models or even use a picture with the webcam. (The webcam thing don't work for me because i'm unable to install cv2, an old version of tf or get some errors with protoc)

I'm just looking for some simple tutorials where they show you how to make your own Object Detection using a simple image as Input and get a image as output or just some keywords of the Objects.  
I don't want to train my own model, I want to use some pre-trained models from the zoo or the Object Detection API.

How did you started with TensorFlow and can you recommend some videos or instructions?",1,tensorflow,2020-10-13
gjcwsa,State of the art in lane detection!,,11,tensorflow,2020-10-13
gjcapc,Keras.fit ops?,"I find myself needing to train a model step by step through a C API, for a reinforcement learning application. The model is complex and so I've built it using Keras's functional API. But I can't seem to find any ops or functions to call from the C API to accomplish training. What am I missing? Does Keras provide any low-level training operations?",1,tensorflow,2020-10-13
gj9u5e,How to run Tensorflow SavedModel 2.x in Tensorflow 1.x,"Hi,

I was using this Tensorflow model:  [https://tfhub.dev/google/universal-sentence-encoder/4](https://tfhub.dev/google/universal-sentence-encoder/4) 

I downloaded the model, and it works correctly in Tensorflow 2.2:

`import tensorflow as tf`

`import matplotlib.pyplot as plt`

`import numpy as np`

`import os`

`import pandas as pd`

`import re`

`import seaborn as sns`

`model_dir = ""C:/Users/admin/Downloads/4""`

`model = tf.saved_model.load(model_dir)`

`print (""module loaded"")`

`def embed(input):`

`return model(input)`

&amp;#x200B;

`word = ""Elephant""`

`sentence = ""I am a sentence for which I would like to get its embedding.""`

`paragraph = (`

`""Universal Sentence Encoder embeddings also support short paragraphs. ""`

`""There is no hard limit on how long the paragraph is. Roughly, the longer ""`

`""the more 'diluted' the embedding will be."")`

`messages = [word, sentence, paragraph]`

&amp;#x200B;

`message_embeddings = embed(messages)`

&amp;#x200B;

`for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):`

`print(""Message: {}"".format(messages[i]))`

`print(""Embedding size: {}"".format(len(message_embedding)))`

`message_embedding_snippet = "", "".join(`

`(str(x) for x in message_embedding[:3]))`

`print(""Embedding: [{}, ...]\n"".format(message_embedding_snippet))`

&amp;#x200B;

It worked correctly and printed output. I plan to serve this model on a C# client, however the C# port of Tensorflow is only up to version 1.15, so I was trying to make the model work in Tensorflow 1.15:

`import tensorflow.compat.v1 as tf`

&amp;#x200B;

`with tf.Session() as sess:`

`tf.saved_model.loader.load(sess, ['serve'], model_dir)`

`print(""Model restored."")`

&amp;#x200B;

`word = ""Elephant""`

`sentence = ""I am a sentence for which I would like to get its embedding.""`

`paragraph = (`

`""Universal Sentence Encoder embeddings also support short paragraphs. ""`

`""There is no hard limit on how long the paragraph is. Roughly, the longer ""`

`""the more 'diluted' the embedding will be."")`

`messages = [word, sentence, paragraph]`

&amp;#x200B;

`# ???`

What should I fill in to make this model run?

Thank you for your help",3,tensorflow,2020-10-13
gj9d03,Reverse Image search on local computer hard drive,"I have a bunch of poor quality photos that I extracted from a PDF.  Somebody I know has the good quality photo's somewhere on her  computer(Mac), but it's my understanding that it will be difficult to  find them.

I would like to

* loop through each poor quality photo
* perform a reverse image search using each poor quality photo as the  query image and using this persons computer as the database to search  for the higher quality images
* and create a copy of each high quality image in one destination folder.

**Example pseudocode**

    for each image in poorQualityImages:
        search ./macComputer for a higherQualityImage of image
        copy higherQualityImage to ./higherQualityImages 

I need to perform this action once. I am looking for a **tool, github repo or library** which can perform this functionality more so than a deep understanding of content based image retrieval.  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

There's [another reddit post](https://www.reddit.com/r/DataHoarder/comments/asr5iz/reverse_image_search_for_local_files/)  where someone was trying to do something similar

[imgdupes](https://github.com/knjcode/imgdupes)  is a program which seems like it almost achieves this, but I do not  want to delete the duplicates, I want to copy the highest quality  duplicate to a destination folder",1,tensorflow,2020-10-13
givcmj,Conv-2 Lottery Ticket Hypothesis numpy pruning,"Hey Guys, I coded [Conv-2](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Conv_2_CIFAR10_Magnitude_based_Pruning_Gaussian_Glorot_initializations.ipynb) [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635) in TensorFlow 2.0 in conjunction with numpy absolute magnitude based pruning technique .

Let me know what you all think.

Stay safe!",4,tensorflow,2020-10-13
gisydb,Function/Layer to Compute the average of the Input Vectors,"From Neural Models for Sequence Chunking:

https://preview.redd.it/h66c4ud6wgy41.jpg?width=616&amp;format=pjpg&amp;auto=webp&amp;s=47fef9a14ca3f74f98e7c3778cf232b9194da031

My question what is the Average function here?

Is it GlobalAveragePooling1D, AveragePooling1D, or something else?",2,tensorflow,2020-10-13
giseb5,How can I use TensorBoard's Histogram Dashboard to diagnose a model?,"I am using TensorFlow to create a candidate generation model following youtube's paper: [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf)

In a nutshell my model it is the following:

* Multi-class classification dense neural network with embeddings
* Vocab dim roughly 50k
* Embedding layer of dim 256
* Dense hidden layers 2048, 1024, 512, 256
* Output dim = vocab dim

I've trained my model for 20 thousand steps @ 256 examples per step and these are the resulting histograms of my model:

&amp;#x200B;

https://preview.redd.it/egtmyazvsgy41.png?width=2162&amp;format=png&amp;auto=webp&amp;s=56f329cb749f6a56ee8e0fac974a9ed2393a26d1

I have no idea what these histograms are supposed to look like. My initial thoughts were that hiddenlayer\_0, which i am assuming is the distribution of the embedding weights, should be changing throughout the training process, but the distribution looks fairly constant (or maybe its just the scale?).

**Does anyone know what these histograms are supposed to look like and how can I use these histograms to diagnose my model?**

Thanks for reading my post! Greatly appreciate it. Let me know if I can clarify anything.",3,tensorflow,2020-10-13
gilpnd,What's the use for converter.build() in TensorRT?,,3,tensorflow,2020-10-13
gighao,Implementation of Slimmable network using TF2.0 in static mode.,"Hi, guys. :) I implement the Slimmable network via TF2.0 on static mode.

The Slimmable network is composed of neural layers that can be run with arbitrary depth without any performance drop. Therefore, we can search for the optimal sub-network at once, which is much efficient than the NAS.

In order to train the Slimmable network, each layer's depth is arbitrary set, so the author's implemented it via Pytorch, which is efficient in the dynamic graph. However, I found that it can be implemented in static mode, and it is much faster than the eager execution.

I implement the Slimmable ResNet, MobileNet-v2, and WResNet, and their performance and trained params will be upload soon. :)

Below is my Github repository, and I will be appreciated to your visit and star. :)  
[https://github.com/sseung0703/Autoslim\_TF2](https://github.com/sseung0703/Autoslim_TF2)

&amp;#x200B;

[The concept of the Slimmable network. image from the author's repository.  https:\/\/github.com\/JiahuiYu\/slimmable\_networks ](https://preview.redd.it/ymel6mkthdy41.png?width=1181&amp;format=png&amp;auto=webp&amp;s=fe3ccd3df2aa2da2d0cd4da346203bb688465126)",5,tensorflow,2020-10-13
gi2s1r,ICYMI: Novel approach to generating high-resolution images,,2,tensorflow,2020-10-13
ghz9g6,ICYMI: Real-world Masked Face Recognition Dataset (RMFRD) is currently the world's largest real-world masked face dataset,,17,tensorflow,2020-10-13
ghw72g,Latest from MIT researchers: A new methodology for lidar super-resolution with ground vehicles,,4,tensorflow,2020-10-13
ghq8ft,Preprocessing music for melody generation: Transposing/filtering songs,"I published a tutorial where you can learn how to preprocess a song dataset, transposing the songs to Cmaj/Amin and filtering them based on note values. You’ll also get familiar with music21, a Python package that makes symbolic music manipulation quick and intuitive.

This tutorial is part of the series “Generating melodies with LSTM nets”, a course that’ll teach you how to build an artificial neural network (RNN-LSTM) to generate effective melodies.

Here’s the video:

https://www.youtube.com/watch?v=coEgwnMBuo0&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=3",8,tensorflow,2020-10-13
ghproa,Conceptional question about tf.data: Isn't dataset.batch() supposed to work similar to a Keras Generator and not load a whole dataset into memory at once?,"Hi,


I'm trying to train a model on a somewhat big amount of data (big at least for my machine, ~17gb).


When I throw the numpy array with this data directly into model.fit(...), it crashes with


    2020-05-07 12:46:43.785479: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16823566556 exceeds 10% of system memory.


Alright, so the data seems to be too big for the memory in some way (although it could be loaded into RAM?!), so I wrote a short data generator to feed batches into model.fit(...). This works well, but is incredibly slow, because for some reason I can't use multithreading for this due to a bug in keras.utils.Sequence (this is a whole other issue, I put it on github already, so I won't go into the details here...)


So in order to avoid the keras.sequence I set up a pipeline with tf.data. I wrote a class around it in order to provide batches:


    class dataset_loader(...):
        
        def __init__(...):
            
            X = np.load(# numpy file on disk)
            target = np.load(# numpy file on disk)
    
    		## Lot's of preprocessing
    
            self._dataset_ = tf.data.Dataset.from_tensor_slices((X, target))
            del X
            del target
        
        def preprocessing(self, line, y):
    		## Some more on-the-fly preprocessing
        
        def get_dataset(self):
            dataset = self._dataset_.map(self.preprocessing, num_parallel_calls=self.n_threads)
            return dataset.batch(self.batch_size).prefetch(self.n_prefetch)
    


Then I call model.fit:


    train_data_loader = dataset_loader(...)
    valid_data_loader = dataset_loader(...)
    
    history = model.fit(train_data_loader.get_dataset(), validation_data=valid_data_loader.get_dataset(), epochs=25, callbacks = callbacks)#, max_queue_size=5000, workers=2, use_multiprocessing=True))



...but I get the same memory error again!


    Allocation of 16823566556 exceeds 10% of system memory.

Isn't tf.data supposed to work similar to Keras Generators, meaning they only load one (or a few) batches into memory? Or did I get this wrong? 


What can I do to get this working?",2,tensorflow,2020-10-13
ghpbf6,Creating a Confusion Matrix,"Hi,

I've used transfer learning to make my own object detection model.

I'd like to display it's effectiveness through a confusion matrix and didn't spot this feature in tensorboard.

I've done a bit of Googling for a tutorial to manually create one but I'm not too sure what to follow. (outdated code, etc.)

I'm fairly certain tf has one built in, but as a newbie it'd be helpful if there was a step-by-step tutorial for this.

Anyone know of any existing material or of an actual solution/cheat sheet?

&amp;#x200B;

Many thanks",3,tensorflow,2020-10-13
ghmxpv,Average of the Input Vectors,"I know I have posted this question before, but I was not very well and deleted it. So I will ask again, but this time, my mind is clear.

In the paper ""[Neural Models for Sequence Chunking](https://arxiv.org/pdf/1701.04027.pdf)"", there's a formula 2. It follows:

Chj = Average (h\[i\], h\[i+1\], ... h\[i+l-1\])

I first thought that it was Average Layer, but it said:

""Average(·) computes the average of the input vectors.""

So it's actually tf.math.reduce\_mean, not keras.layers.Average? Or is it really Average Layer? Need to clear this out.",1,tensorflow,2020-10-13
ghjz6v,"Spark NLP 2.5.0: ALBERT &amp; XLNet transformers, state-of-the-art spell checker, multi-class sentiment detector, 80+ new models &amp; pipelines in 14 new languages &amp; more",,5,tensorflow,2020-10-13
ghikzt,Python 3.9 - The Shape of Things to Come,,2,tensorflow,2020-10-13
ghd73f,How to batch inference?,"Is it possible to batch images (2?) for inference? If yes, are there any benefits, and are there any in built functions I can use? 
Note: TF version: 1.14.0 (can’t use 2.0)",1,tensorflow,2020-10-13
ghcwcj,"User error or bug? ""ValueError: No gradients provided for any variable"" on single unit network","The following code gives `ValueError: No gradients provided for any variable: ['y_pred/kernel:0', 'y_pred/bias:0']`:

```
import tensorflow as tffrom tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import MeanSquaredError
x = Input(shape=(1,), name='x')
y = Input(shape=(1,), name='y')
y_pred = Dense(1, name='y_pred')(x)

model = Model(inputs=[x, y], outputs=[y_pred])
mse = MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)
loss = lambda: mse(y_pred, y)

optimizer = tf.keras.optimizers.Adam()
train_op = optimizer.minimize(loss, model.trainable_variables, name=""train"")
```

You can also see this hosted [here](https://www.kaggle.com/joshhansen/adam-minimize-no-gradients?scriptVersionId=33730888). It did the same thing when I wrote the loss function myself instead of using the provided MSE. What am I missing?",4,tensorflow,2020-10-13
gh9etl,TensorFlow 2.2 + Ubuntu 20.04 + Nvidia GPUs,,24,tensorflow,2020-10-13
gh3fu6,Average Layer on Bi-LSTM Outputs?,"The paper I'm using said to use the Average layer after Bi-LSTM.

How do you do that? The average layer needs two lists(?).

https://preview.redd.it/5bkjl3vlgyx41.jpg?width=609&amp;format=pjpg&amp;auto=webp&amp;s=4f6463e8c2a7f864eb971679abbd6032793402fe

    # Input
    inputs = Input(shape=(max_length,), name=""Input"")
    
    # Embedding
    embed = Embedding(input_dim=n_words+1,
                      output_dim=embedding_size,
                      input_length=max_length,
                      name=""Embedding"")(inputs)
    
    # Bi-LSTM
    encoder = Bidirectional(LSTM(units=hidden_state_encoder_size,
                                 return_sequences=True,
                                 dropout=dropout_rate,
                                 name=""LSTM""),
                            name=""Bi-LSTM"")
    
    encoder_outputs = encoder(embed)
    
    # # Average
    # average = Average()([])
    
    # Outputs
    outputs = Dense(n_tags,
                    activation=""softmax"",
                    name=""Output"")(encoder_outputs)",6,tensorflow,2020-10-13
ggy5f2,Help Installing Tensorflow,"I've followed all the instructions on the documentation but when I try to install Tensorflow via : pip install --upgrade tensorflow-gpu==1.14 

I get this error:

ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.14 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)

ERROR: No matching distribution found for tensorflow-gpu==1.14

Ive installed CUDA toolkit, cuDNN, Setup the 4 environment variables and updated my graphics driver, the only thing I havent done was installing anaconda which is optional. Any ideas on where I went wrong? Ty in advance.

EDIT: solved, turns out I was using the wrong python version.",0,tensorflow,2020-10-13
ggwcfe,Model.fit_generator(),"Is depreciated, but fitting with a generator using model.fit() crashes my kernel. What gives?",1,tensorflow,2020-10-13
ggtc1o,New Tensorflow Runtime (using MLIR Compiler)!,"There's a brand new runtime for tensorflow: [https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html](https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html)

Here's a link to the github: [https://github.com/tensorflow/runtime](https://github.com/tensorflow/runtime)",7,tensorflow,2020-10-13
ggo5t8,"Error when checking target: expected Output to have 2 dimensions, but got array with shape (631, 80, 2641)","I'm making a Text Chunking program using Bi-LSTM. The inputs are sequences of words and the outputs are ""B-NP"", ""I-NP"", and ""O"".

The problem I have is the output. The expected output is 2 dimensions, but 3 dimensions.

The X Train shape is (631, 80) while the Y Train shape is (631, 80, 2641).

I get the problem, but I don't know how to solve it. I don't have experience using Tensorflow and Keras and this is my first time, so I was hoping you could help me.

    # Token Indexing
    word2idx = {""PAD"": 0, ""UNK"": 1}
    word2idx.update({w: i+2 for i, w in enumerate(vocab_words) if w in vocab_words})
    tag2idx = {t: i for i, t in enumerate(vocab_tags)}
    idx2tag = {v: k for k, v in iteritems(tag2idx)}
    
    # Padding
    X = [[word2idx[w[0]] for w in s] for s in quran_sentences]
    X = pad_sequences(maxlen=max_length, sequences=X, padding=""post"",value=word2idx[""PAD""])
    
    y = [[tag2idx[w[1]] for w in s] for s in quran_sentences]
    y = pad_sequences(maxlen=max_length, sequences=y, padding=""post"", value=tag2idx[""O""])
    y = [to_categorical(i, num_classes=n_tags) for i in y]
    
    # Split Data
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, shuffle=False)
    
    # Parameters
    hidden_state_encoder_size = 100
    
    hidden_state_decoder_size = 200
    
    batch_size = 64
    
    training_epoch = 200
    
    embedding_size = 80
    
    # Input
    inputs = Input(shape=(max_length,), name=""Input"")
    
    # Embedding
    embed = Embedding(input_dim=n_words+2, output_dim=embedding_size, input_length=max_length, name=""Embedding"")(inputs)
    
    # Bi-LSTM
    encoder = Bidirectional(LSTM(units=hidden_state_encoder_size, return_state=True, name=""LSTM""), name=""Bi-LSTM"")
    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(embed)
    
    # Concatenated
    state_h = Concatenate(name=""Concat_1"")([forward_h, backward_h])
    state_c = Concatenate(name=""Concat_2"")([forward_c, backward_c])
    
    # Average
    average = Average()([state_h, state_c])
    
    # Outputs
    outputs = Dense(num_decoder_tokens, activation=""softmax"", name=""Output"")(average)",1,tensorflow,2020-10-13
ggn2zm,[Question] How do I convert code written in TensorFlow to PyTorch?," Hi

I want to convert a Restricted boltzman machine model (code in  python) from TensorFlow to PyTorch. Is there any tool or software that  can be helpful?",4,tensorflow,2020-10-13
gggki1,Setting up Tensorflow on Ubuntu 20.04,"Apologies in advance if these are dumb questions, I just want to make sure I've got this right. I'm trying to set up Tensorflow on my home computer. I'm currently dual booting Ubuntu with Windows and can't find many guides on getting Tensorflow (GPU) set up on 20.04. For anyone who already has it set up, can I safely follow the guides related to 18.04 (CUDA and CUDNN too) or is there a specific way to set it up for 20.04?",4,tensorflow,2020-10-13
gg8ris,Help with getting tensorflow running,"I installed tensorflow 1.15 (CPU only) because I'm on a pretty old PC with an Intel Core 2 Duo and an Nvidia GTS450. But I keep getting the following error when I try to import tensorflow:

    Traceback (most recent call last):
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in &lt;module&gt;
        from tensorflow.python.pywrap_tensorflow_internal import *
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
        _pywrap_tensorflow_internal = swig_import_helper()
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
        _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
        return load_dynamic(name, filename, file)
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
        return _load(spec)
    ImportError: DLL load failed with error code -1073741795
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""&lt;pyshell#0&gt;"", line 1, in &lt;module&gt;
        import tensorflow
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 99, in &lt;module&gt;
        from tensorflow_core import *
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\__init__.py"", line 28, in &lt;module&gt;
        from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
        module = self._load()
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
        module = _importlib.import_module(self.__name__)
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in &lt;module&gt;
        from tensorflow.python import pywrap_tensorflow
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in &lt;module&gt;
        raise ImportError(msg)
    ImportError: Traceback (most recent call last):
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in &lt;module&gt;
        from tensorflow.python.pywrap_tensorflow_internal import *
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
        _pywrap_tensorflow_internal = swig_import_helper()
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
        _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
        return load_dynamic(name, filename, file)
      File ""C:\Users\Amogh\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
        return _load(spec)
    ImportError: DLL load failed with error code -1073741795
    
    
    Failed to load the native TensorFlow runtime.

I cannot figure out how to fix this. Can I not run tensorflow?",1,tensorflow,2020-10-13
gg8qqt,From CVPR '20: High-Fidelity 3D Face Reconstruction,,8,tensorflow,2020-10-13
gg3wnt,Bring Old Photos Back to Life,,15,tensorflow,2020-10-13
gg1kh3,Local Text to Speech Synthesis,"I am looking to render text to speech locally on my machine with quality on par with at lest Amazon Polly, if not closer to wavenet or voysis. I find it hard to believe cloud computing options offer 2080 ti levels of hardware to all their consumer/personal level subscribers, so I believe I have more computing at my personal disposal than I can get without subscribing to commercial or enterprise versions of the online product (Likely out of my price range, but maybe not).   


I have an RTX 2080 ti that is overclocked, and frankly under-utilized. I enjoy gaming on my free-time but am far too busy with study and school to indulge that hobby.  I could however use some help with my studying by making it available in audio files. That said, I know most voice to text programs utilize tensorflow and some variation of tensorcores and the turing architecture (I should say, I 'think' I recall hearing as much at least). Is there any prototype software or consumer level software I can buy that will utilize my GPU to it's full capability to render decent text to speech?   


To be clear I'm not looking for 'live' generation, the GPU can take its time, but I am looking for the ability to render a large say 20 page document from text to speech.   


If this doesn't exist in a version I can run locally using my GPU, I welcome suggestions of more user friendly software that I could use in the cloud or online. I have gotten Amazon Polly figured out, but find it clunky, and difficult because it requires work through the API, switching between multiple screens, copying and simplifying the format, saving the link to the output folder, re-entering the information for the location it's to be sent to each time, waiting for it to load in, going to that output folder, and then downloading the final version. This is all grand for programmers and people who enjoy tinkering with software and have a disconnect with decent user experience, but frankly most of these steps are annoying, pointless, redundant, and just add a ton of extra overhead time to my use of the application (Almost rendering it pointless since that time might be better spent just studying in the first place).   


I use Amazon Polly as an example because it has the 'easiest' and 'most intuitive' of the user interfaces and API's which given how convoluted its use is says a lot... But again, this isn't finalized software designed for consumer use, so I get that it's not 'supposed' to be streamlined and easy, and hence my question, has anyone actually used this API to develop an easy end-user solution with similar quality capable of large throughput?",4,tensorflow,2020-10-13
gfpztg,Help with facenet,I'm trying to create a face recognition system using facenet and tensorflow. Can someone help me with resources on where to begin and stuff?,1,tensorflow,2020-10-13
gfotdo,This Week in AI - Issue #17 | Rubik's Code,,10,tensorflow,2020-10-13
gfeusq,Custom implementation of operation optimised using SIMD instructions,"Hi everyone,  
I came across a paper, where they have mentioned they wrote a custom implementation of the operation resize\_bilinear, optimised using SIMD(Single Instruction multiple dataset)  


Now, I know what SIMD is, but do not have the first clue how to optimize a function using SIMD instruction set.  
I found that if we build tensorflow by source, it allows for SIMP operations, But how do we write a custom implementation of a function optimised by SIMD.  


Any help would mean a lot.",1,tensorflow,2020-10-13
gfe6md,Tensorflow lite on raspberry pi model questions,"So far I have been doing some Ann and rnn tutorials in python on desktop, but I really want to switch to using the C++ api  

I'm seeing there is a tensorflow lite library which to my understanding is used to run already trained models, but not to train them

If so, this is what I'm looking for. 


So my questions are: 
I would like to find some tutorials on using the library in my C++ code, can you point me in the right direction? Preferably something in object detection realm with OpenCV. 

If I should need to build a model, can I do it using python on PC and copy it to the PI to use the C++ API?",3,tensorflow,2020-10-13
gfc4tx,"Need assistance with dense layer issue: ""Error in py_call_impl(callable, dots$args, dots$keywords) : ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.""","Hi, I am following along [this](https://github.com/skeydan/audio_classification/blob/master/audio_classification_tf.R) example and at the last step (layer_dense(units = 128, activation = 'relu'))  I am receiving this error:

""Error in py_call_impl(callable, dots$args, dots$keywords) :
  ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.""


From what I've read it has to do with the dense layer and that there needs to be some kind of adjustment to the input shape? There is a ""flatten layer"" line just before this that when removed allows it to work, but then gets messed up in the next step later on when fitting the model. Also, I'm working in R, not Python. Thank you",6,tensorflow,2020-10-13
gf90qs,Error when checking model target,"I'm making sequence chunking using model I (Bi-LSTM) by my prof.

I finished making the model, but the training went error. The numpy arrays are not the model expected.

The input data is a sequence of words.

The output data are B-NP, I-NP, and O.

    # Parameters
    hidden_state_encoder_size = 100
    batch_size = 64
    training_epoch = 200
    embedding_size = 80
    
    # Input
    inputs = Input(shape=(max_length,), name=""Input"")
    
    #Embedding
    embed = Embedding(num_encoder_tokens, embedding_size, name=""Embedding"")(inputs)
    
    # Bi-LSTM
    encoder = Bidirectional(LSTM(hidden_state_encoder_size, return_state=True, name=""LSTM""), name=""Bi-LSTM"") 
    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(embed)
    
    # Concatenated
    state_h = Concatenate(name=""Concat_1"")([forward_h, backward_h]) 
    state_c = Concatenate(name=""Concat_2"")([forward_c, backward_c])
    
    # Average
    average = Average()([state_h, state_c])
    
    # Outputs
    outputs = Dense(num_decoder_tokens, activation=""softmax"", name=""Output"")(average)
    
    # Build Model
    model = Model(inputs, outputs, name=""Seq2Seq Chunking"")
    
    # Compile &amp; run training
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    model.fit(X_tr, y_tr,
              batch_size=batch_size, epochs=training_epoch,           validation_split=0.1)

&amp;#x200B;

&gt;ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 631 arrays: \[array(\[\[0., 1., 0., ..., 0., 0., 0.\],",4,tensorflow,2020-10-13
gf8r69,Music representation for melody generation with deep learning,"In this tutorial, you’ll learn how to encode melodies effectively to train a neural network with the aim to generate music. In the process, you’ll also learn fundamental music theory concepts (e.g., key, time signature) that are important to understand the melody generation problem better.

This video is part of the series “Generating melodies with LSTM nets”, a course that’ll teach you how to build RNN-LSTMs to generate effective melodies using TensorFlow.

Here’s the video:

[https://www.youtube.com/watch?v=LFDovU96EdY&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=2](https://www.youtube.com/watch?v=LFDovU96EdY&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=2)",1,tensorflow,2020-10-13
gf36eg,TF 2.x on Ubuntu 18.04 with RTX 20xx,"Hi, 

I am trying to build tensorflow 2.0 or 2.1 with cuda 10.1 cuDNN 7.6 bazel 0.26.1/0.29.1/1.1.0 but always get “Build did not complete successfully” The gcc version is 7.5.0 but can’t seem to downgrade to 7.3.1 as it shows in the test build page.

Has anyone had any luck with building ?",5,tensorflow,2020-10-13
geydlf,"Operands could not be broadcast together with shapes (200,) (100,)","I'm using the Average layer, but something is wrong with the Average layer. It mentions the shapes are not similar.

    inputs = Input(shape=(max_length,), name=""Input"")
    
    embed = Embedding(num_encoder_tokens, embedding_size, name=""Embedding"")(inputs)
    
    bi_lstm = Bidirectional(LSTM(hidden_state_encoder_size, return_state=True), name=""BI-LSTM"")(embed)
    
    average = Average()(bi_lstm)
    
    outputs = Dense(num_decoder_tokens, activation=""softmax"", name=""Softmax"")(average)

Any idea how to solve this?",2,tensorflow,2020-10-13
gep8vd,how to shuffle the data for model.fit with custom data generator?,"&amp;#x200B;

https://preview.redd.it/cui1jxhpo6x41.png?width=2383&amp;format=png&amp;auto=webp&amp;s=9ba5f3b85ec15137b823c4c9359201922d5c9f7f

&amp;#x200B;

So trainfiles is a list that contains the files' directory and name e.g. \['../train/1.npy' ,  '../train/2.npy'\] 

and then I create a dataset as shown in the middle of the code

then I apply it to model fit function 

&amp;#x200B;

just adding shuffle=True in the argument for model fit function doesn't do anything it seems. 

How do I go about shuffling the trainfiles correctly? i.e. when does the trainGenerator function gets executed? i.e. does it gets executed 516 times  when generating 1 batch?

&amp;#x200B;

can I just add random.shuffle(trainfiles) right above the for-loop in trainGenerator?",1,tensorflow,2020-10-13
gekotd,Can I use an arbitrary algorithm as a loss function for a network?,"I has been trying to understand this machine learning problem for many days now and it really confuses me, I need some help.

I am trying to train a neural network whose input is an image, and which generates another image as output (it is not a very large image, it is 8x8 pixels). And I have an arbitrary *fancy\_algorithm()* ""black box"" function that receives the input and prediction of the network (the two images) and outputs a float number that tells how good the output of the network was (calculates a loss). My problem is that I want to train THIS neural network but using the loss generated by the black box algorithm. This problem is confusing me, I researched a lot and I didn't find much about it, it seems like reinforcement learning, but at the same time I'm not sure because it’s not like an agent, but it has some kind of reinforcement at the same time.

In case you need more details to help me just ask. Thanks in advance.",15,tensorflow,2020-10-13
geihle,"Freeze only some rows of a Layer matrix during training, TF 2.1","Hi, I know it is possible to train only some of the layers in a network. 

Is it possible to train, for example, only some rows of the weights matrix of a Dense layer, keeping the rest of the matrix fixed?",2,tensorflow,2020-10-13
geeyum,Train a model to generate meaningful flavor combinations,"Hi everyone, it's my first post here, so please excuse any dumb thing I might write below. But I have an idea and no clue where to start so I would appreciate some help.

I have an android app that help people who vape to create recipes for DIY eliquids. I got the crazy idea of introducing a feature for generating recipes automatically for providing suggestions to users. I briefly looked at tensorflow in the past but gave up since I didn't have a reason for studying it deeply, now it seems I have one. So I'm at the point where I kindof know there should be a way, but no idea how to approach this practically.

The problem is the following: given a set of existing recipes created by people, which contains a set of flavors and percentages for each flavor, is there a way to feed this data into some kind of model and get meaningful predictions for new recipes?

A recipe can be summarized as

\- Flavor\_1 -&gt; 5%

\- Flavor\_2 -&gt; 2.5%

\- Flavor\_3 -&gt; .5%

\- ...

Is this even possible?

Is Tensorflow the right choice?

What kind of model works best for this use case?

How many existing recipes would I need for training?

Thanks a lot for the help.",4,tensorflow,2020-10-13
ge91lx,[R] Announcing the release of StellarGraph version 1.0 open-source Python Machine Learning Library for graphs,,3,tensorflow,2020-10-13
ge6lrf,Saving and reading tfrecord from tfds datasets,"Hello,

I'm trying to work on the NSynth dataset. I was planning to filter this dataset to use only the data that I need. And eventually, generate chord audio data from single notes. I thought manipulating data would be as easy as pandas with tfds. I was mistaken BIG TIME.

After adding a few filter predicates, training time increases incredibly. Probably because the data can't be filtered in batches, otherwise lots of other problems arise. So I thought, I could filter the data and save it to a new file and use that one for my models.

    datasets = tfds.load(""nsynth"", data_dir=""data"")
    train_dataset, test_dataset, valid_dataset = datasets[""train""], datasets[""test""], datasets[""valid""]
    train_dataset = train_dataset.filter(lambda x: x['instrument']['family'] == 3)
    tf_writer = tf.data.experimental.TFRecordWriter(""filtered/onlyGuitar.tfrecord"")
    tf_writer.write(train_dataset)

But apparently life is not that beautiful. And I'm getting the following error;

    
AttributeError: 'dict' object has no attribute 'is_compatible_with'
    

I'm not sure what's the problem here. But I am guessing it's related to serializing the data. I would have expected that the Dataset class would have some kind of self serializing mechanism. I could maybe get a hint from the below link. But I feel like I'm really lost and maybe I should just try to stay away from tfds altogether.  
 [https://github.com/tensorflow/datasets/blob/master/tensorflow\_datasets/audio/nsynth.py](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/audio/nsynth.py)",0,tensorflow,2020-10-13
ge2alc,🤖 Interactive Machine Learning Experiments using TensorFlow and TensorFlow.js,,17,tensorflow,2020-10-13
ge1j2e,Is any good book about Tensor Flow?,"I'm tired to read many articles about Keras and Tensor Flow. So much information and examples, but I want something more cohesive. In addition, I just like good comprehensive technical literature. Can anybody advice me with a book?",1,tensorflow,2020-10-13
ge0xm3,Tensorflow_dataset doesn't support as_supervised,"Hi everyone.

I've been trying to use the diabetic\_retinopathy\_detection dataset of tfds and specifically the btgraham-300 configuration.

I want to use the dataset for classification and as such loading the dataset with `as_supervised=True` makes a lot of sense. However this particular dataset doesn't support `as_supervised=True`. I quickly went through the documentation and found that `as_supervised` returns a tuple instead of a dictionary. I thought I'd create the tuple myself using the `.map` method but whenever I do that my model doesn't improve/train/etc. I even tried using the mnist dataset with `as_supervised=False` and custom made tuples and again, the model did not improve, suggesting that something is wrong with the input data.

Now my questions are:

Is there a way to enable `as_supervised=True` in a dataset that doesn't support it?

Am I creating the tuples the wrong way, or does `as_supervised` do anything else?

If I use \`as\_supervised=False\` instead, how should I feed my data into the model?

&amp;#x200B;

Thanks in advance!",2,tensorflow,2020-10-13
gdsodo,Fake quantization for quantization-aware training,"Hi,  
I am trying to convert a model to tflite. On converting the model normally,  the output varied a lot. I mailed the authors and he suggested me to use Fake quantization for quantization-aware training.  


He directed me to a [research paper](https://arxiv.org/abs/1712.05877).  


I have not worked with something like this before, Can anyone please guide me on how I should proceed to convert the model into tflite using Fake quantization.",2,tensorflow,2020-10-13
gdsl6y,Language Translation System,,5,tensorflow,2020-10-13
gdnwjv,installing TensorFlow,"I'm trying to install TensorFlow for hours and id love your help because I just can't do it...  
I have both python 3.7 and 3.6  

At first, I got this error:  

ERROR: Could not find a version that satisfies the requirement TensorFlow (from versions: none)  
ERROR: No matching distribution found for TensorFlow 
 
Then I went to pycharm and define the interpreter to be the 3.6 one and installed TensorFlow from pycharm but after like 15 minutes it crashed saying

FileNotFoundError: \[WinError 2\] The system cannot find the file specified:   
regarding the file: \\\\lib\\\\site-packages\\\\setuptools-39.1.0-py3.6.egg'.  

I then opened the cmd in the folder of the pycharm environment (with python 3.6) and ran pip install TensorFlow and this time it stated a bunch of requirements that are satisfied and in the end:  

You are using pip version 10.0.1, however, version 20.1 is available.  
You should consider upgrading via the 'python -m pip install --upgrade pip' command.  

So I ran it but when I did it said that pip is already up to date in lib\\site-packages (20.1).  

When I run pip install TensorFlow from there it raises this error again:  

ERROR: Could not find a version that satisfies the requirement TensorFlow (from versions: none)  
ERROR: No matching distribution found for TensorFlow
  
Any advice? Anything that would help me solve this?",3,tensorflow,2020-10-13
gdnf27,Problem on Accuracy values,"Hello,

I have a multilabel text classification model that maps embedding vectors with a hot-vector.

    model.compile(loss = 'binary_crossentropy',
                 optimizer='adam',
                 metrics = ['accuracy'])

After the `mode.fit\`\`(...)`, the model has this output:

`loss: 0.1234 - accuracy: 0.9529 - val_loss: 0.1149 - val_accuracy: 0.9512`

But when I compute the `accuracy_score` with `sklearn.metrics` the accuracy is **0.45**

    x_train, x_val, y_train, y_val = train_test_split(x_seq, labels, test_size=0.25, random_state=1)
    ...
    y_pred = model.predict(x_val, verbose = 1)
    y_predicted = tf.argmax(y_pred, axis=1)
    y_val_class = tf.argmax(y_val, axis=1)
    accuracy_score(y_predicted, y_val_class) # 0.45",4,tensorflow,2020-10-13
gdkx1y,Parallel line detention,"Does anyone know of an existing model or an approach to generate a model that would detect parallel lines in an image/video? 

I am tracking an object successfully but need to know if the object is either below, between or above a set of parallel lines in the background.",1,tensorflow,2020-10-13
gdfb3n,Free Online Talk | Reinforcement Learning Explained: Overview and Applications,"Speaker is the author of [Reinforcement Learning for Cyber-Physical Systems: with Cybersecurity Case Studies](https://www.amazon.com/Reinforcement-Learning-Cyber-Physical-Systems-Cybersecurity/dp/1138543535)

RSVP [https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd](https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd)

Outline:

\- Introduction to reinforcement learning and its framework  
\- RL solutions: model-based methods  
\- RL solutions: model-free methods  
\- Deep reinforcement learning  
\- Real-world applications: Alpha Go, Self-driving cars, Robotics, finance, etc.",1,tensorflow,2020-10-13
gde5hv,"OK, hello there, I'm new to ML and have a doubt","Hello and nice to me you all

I'm very new to ML and I'm starting my journey in this field of programming and development, so I was looking in where to start and pretty much universally the first step is to install TensorFlow, I have a GPU with CUDA, an EVGA 1070 FTW to be more precise, so I want to install TensorFlow on my GPU, but since this is my gaming PC and either I'm a moron, probable, or really bad at googling, but I want to know if installing TensorFlow in my GPU would affect my capacity on gaming, I don't intend to do both things at the same time for obvious reasons, but it would bring a lot of peace of mind knowing that I can install Tensorflow on GPU and it still operating normally otherwise.

Since now, thanks

Any beginner hints are also welcome...",1,tensorflow,2020-10-13
gde49v,.h5 file in android,Hi guys. I'm new to CNN. I'm trying to build an app that recognizes what plant using captured camera image. I have pretrained .h5 and what do I do with this in android? Is there a tutorial that helps? Thank you.,4,tensorflow,2020-10-13
gde3cm,[Project] TF2.x callback for GPU stats monitoring,,3,tensorflow,2020-10-13
gdcuou,How can I save two tensors in an array to a file?,,2,tensorflow,2020-10-13
gdccmq,Custom tf.keras model with 2 input tensors without functional API (TF 2.1),"Hello everyone

&amp;#x200B;

I am trying to build a custom model (including custom layers). I managed to build the whole model and it works and trains just fine. However now I wanted to add a second set of Inputs.

&amp;#x200B;

I tried:

\`\`\`

InputA = np.array(dataA)  #shape (events, 500, 8) which will be Tensor of shape (None, 500, 8)

InputB = np.array(dataB)  #shape (events, 2) which will be Tensor of shape (None, 2)

[model.fit](https://model.fit)(\[InputA, InputB\], targets, batch\_size = 70)

\`\`\`

&amp;#x200B;

Which returns the error:

'   ValueError: Tried to convert 'input' to a tensor and failed. Error: Shapes must be equal rank, but are 3 and 2

From merging shape 0 with other shapes. for 'ABCNet/AggregationGAP/GAPBlock/ExpandDims/packed' (op: 'Pack') with input shapes: \[?,500,8\], \[?,2\].'

&amp;#x200B;

It looks like it tries to create a Tensor out of both inputs, can I somehow keep it as 2 seperate inputs without the functional API?

&amp;#x200B;

I am looking forward for any sort of idea or input.",3,tensorflow,2020-10-13
gdcapo,Error while importing keras,"ERROR:root:Internal Python error in the inspect module.

&amp;#x200B;

I am having this error while trying to import keras. Keras is using tensorflow backend. I have installed ternsorflow version 2.1. And python version 3.7. Kindly help me solve this issue. Thanks.",5,tensorflow,2020-10-13
gdai6y,Automatic music generation with neural networks,"I’m releasing a new AI music series on my YouTube channel. It teaches you how to generate melodies with neural networks (specifically, Long Short-Term Memory networks). In the process, you'll also be learning about TensorFlow/Keras, time-series data, and symbolic music representations.

I’m sharing the first video of the course, which provides an overview of the series and introduces some fundamental concepts.

Here's the video:[ https://www.youtube.com/watch?v=FLr0r-QhqH0&amp;feature=youtu.be](https://www.youtube.com/watch?v=FLr0r-QhqH0&amp;feature=youtu.be)

Enjoy!",13,tensorflow,2020-10-13
gd6rh0,Top 3 Artificial Intelligence Research Papers – April 2020,,11,tensorflow,2020-10-13
gd699y,Help with Decoder Model for Sequence Chunking,"I need help with my Decoder Model. I'm using model II from Neural Models for Sequence Chunking ( [https://arxiv.org/pdf/1701.04027.pdf](https://arxiv.org/pdf/1701.04027.pdf) ).

The paper mentions using CNNMax Layer, Word Embeddings, and Average activation. But I can't find anything about CNNMax Layer in Keras, only pooling, it doesn't mention what context word embedding I should use, and no Dense average function in Keras.

So I'm stuck in the Decoder.

This is my Decoder code (and Encoder, just in case). There is some code from other seq2seq. I thought I can use some of it as a reference.

Encoder

    # Define an input sequence and process it.
    encoder_inputs = Input(shape=(None,), name=""Input_1"")
    X =  Embedding(num_encoder_tokens, embedding_size, name=""Embedding_1"")(encoder_inputs)
    encoder = Bidirectional(LSTM(hidden_state_encoder_size, return_state=True, name=""LSTM_1""), name=""Bi-LSTM"")
    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(X)
    
    # Concatenated
    state_h = Concatenate(name=""Concat_1"")([forward_h, backward_h])
    state_c = Concatenate(name=""Concat_2"")([forward_c, backward_c])
    
    # Discard `encoder_outputs` and only keep the states.
    encoder_states = [state_h, state_c]

Decoder

    # Set up the decoder, using `encoder_states` as initial state.
    decoder_inputs = Input(shape=(None,), name=""Input_2"")
    x = Embedding(num_decoder_tokens, embedding_size, name=""Embedding_2"")(decoder_inputs)
    
    # CNNMax Layer
    Cx = 
    
    # Word Embeddings
    Ch = Embedding(num_decoder_tokens, embedding_size, name=""Embedding_2"")(decoder_inputs) #Is this right or wrong?
    
    # Average
    Cw = Dense(state_h, activation=) 
    
    x = LSTM(hidden_state_decoder_size, return_sequences=True, name=""LSTM_2"") (x, initial_state=encoder_states)

    decoder_outputs = Dense(num_decoder_tokens, activation='softmax', name=""Outputs"")(x)",1,tensorflow,2020-10-13
gd2kgh,Is it possible to fix this error,"Im new to Tensorflow right now. i got this error when trying to load a model.

Here is the error Im getting:

I tensorflow/core/platform/cpu\_feature\_guard.cc:142\] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

Here is my code:

    import tensorflow as tf
    import matplotlib.pyplot as plt
    mnist = tf.keras.datasets.mnist  # 28x28 images of hand-written digits
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    # normalize
    x_train = tf.keras.utils.normalize(x_train, axis=1) x_test = tf.keras.utils.normalize(x_test, axis=1)
    new_model = tf.keras.models.load_model('epic_num_reader.model')
    predictions = new_model.predict([x_test])

I am using pyCharm. I there a different version of tensorflow I should import?",3,tensorflow,2020-10-13
gcw07p,easiest way to access variable inside graph?,"Hey,

im looking for a way to reuse the the output values of my net in the next ""loop"". Im predicting a 257 long vector from a 257x16 image and always want to append the output vector predictions to something like a variable to create another 257x16 image which i then want to concatenate to the next input data like a second channel of an image.

Any ideas how to access or store the output prediction values at training for reusing? Would be highly appreciated.",1,tensorflow,2020-10-13
gcsifs,I'm looking for more information on the Tensorflow Developer Certificate,"I'd like more info on the \[Tensorflow Developer Certificate\]([https://www.tensorflow.org/certificate](https://www.tensorflow.org/certificate)). I've consulted all the resources they give, but it doesn't give much details. I bought books, practiced a lot, got my organization to pay for it, but have no idea what to expect. Has anyone here done it, and have comments/details?

PS. I'm not interested in doing the Tensorflow Specialization on Coursera.",6,tensorflow,2020-10-13
gcqu85,Our experience with using TensorFlow Lite on Android,,18,tensorflow,2020-10-13
gcp4u8,TensorFlow.js: Machine Learning in JavaScript by Jason Mayes (Google); Please RSVP here,,2,tensorflow,2020-10-13
gcd2pu,[Help] How to improve accuracy for recommendations autoencoder,"Hi everyone. I'm fairly new to machine learning, and am trying to make a product recommendations system with an autoencoder, where the layers are basically `Input -&gt; Dense -&gt; Dense -&gt; Dropout -&gt; Dense -&gt; Output(Dense)`.

My data matrix has product ids as columns, user ids as rows, and each cell has a value [0, 1] representing the user rating for that product.

The result has low loss (0.001 and 0.003) and high precision (around 0.999) but a overall low recall (between 0.2 and 0.44).

I think recomendations' systems benefit from having a higher precision than recall, but I'm afraid even if it is so, the recall is too low right now.

Is there any way to increase the recall, even if the precision lowers slightly (without lowering the threshold for the Precision and Recall metrics)?",5,tensorflow,2020-10-13
gcaxlx,"Preferred way to access/modify/copy subset of model variables, TF 2.1","What is the preferred way to access and perform operations on a subset of the model variables?

Example of operation I need to do:

* Take from the first `Dense` layer of `n` units all the weights that are connected to the first input unit of the input layer and copy them into another network where the first hidden layer has `m` &lt; `n` units (and therefore discard the exceeding ones)

More generally, I need to have full control of model weights. 

What is the correct way to do this?",1,tensorflow,2020-10-13
gc6abl,Tensorflow 1.15 with modern CUDA driver (10.2),"Hi to all. I want to use [mrcnn](https://github.com/matterport/Mask_RCNN) detection library. The problem is, that is compatible only with Tensorflow 1.XX. All forks, that promise Tensorflow 2.X support, don't work well. I have computer with RTX 2070. If I install tf 1.15 from pip CUDA version 9 will be supported. There is no such driver for RTX 2070. My idea was to build 1.15 from source with CUDA 10.2 driver support, but it failed two times. So I see two possible ways of solution:

1. To find a built tensorflow 1 (I hope the version &gt;1.15 will work) with at least CUDA 10.0 driver support. OS: Ubuntu 18.04
2. To find a mrcnn, which works fine with tensorflow &gt;=2.0I will appreciate any advice!",13,tensorflow,2020-10-13
gc44o0,Get Only Encoder Hidden States &amp; Cell States,"I was following an algorithm from this [site](https://github.com/devm2024/nmt_keras/blob/master/base.ipynb), but the Encoder uses Bidirectional LSTM.

But when I tried to build it, it said, too many values to unpack. There are five outputs, but what I want are state\_h and state\_c.

Does anyone know how to handle this?

My code was like this:

    # Define an input sequence and process it.
    encoder_inputs = Input(shape=(None,))
    X = Embedding(num_encoder_tokens, embedding_size)(encoder_inputs) 
    _, state_h, state_c = Bidirectional(LSTM(latent_dim, return_state=True))(X)
    
    # We discard encoder_outputs and only keep the states.
    encoder_states = [state_h, state_c]

&amp;#x200B;",2,tensorflow,2020-10-13
gc0c9o,'list' object has no attribute 'get_config',"I'm currently making a Keras Seq2Seq Word-level model. I use this ( [https://github.com/devm2024/nmt\_keras/blob/master/base.ipynb](https://github.com/devm2024/nmt_keras/blob/master/base.ipynb) ) as a reference (Something's wrong with the link button, so I post the link).

Anyway, when I tried to build the model on Google CoLab, this happens:

https://preview.redd.it/bnmoo2vl8aw41.jpg?width=999&amp;format=pjpg&amp;auto=webp&amp;s=769519f8a02e2792421ae109f30b6c1221bad74f

Why? Is there something wrong?

&amp;#x200B;",1,tensorflow,2020-10-13
gbxqzj,Deep Dream Frame Generated Video | Machine Learning,,2,tensorflow,2020-10-13
gbmvjm,`tf.keras` yielding lower accuracy than `keras`,"Hello, I'm coming back to TensorFlow after a while and I'm running again some example tutorials. I have been trying to use the Keras CNN Mnist example and I get conflicting results if I use the `keras` package or `tf.keras`. 

I have tried the example both on my machine and on google colab and when I train  the model using keras I get the expected 99% accuracy, while if I use `tf.keras` I get a much lower accuracy.

Here is the code:

```python
'''Trains a simple convnet on the MNIST dataset.
Gets to 99.25% test accuracy after 12 epochs
(there is still a lot of margin for parameter tuning).
16 seconds per epoch on a GRID K520 GPU.
'''

from __future__ import print_function
import tensorflow.keras as keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K

# import keras as keras
# from keras.datasets import mnist
# from keras.models import Sequential
# from keras.layers import Dense, Dropout, Flatten
# from keras.layers import Conv2D, MaxPooling2D
# from keras import backend as K

batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```

I am puzzled... Is it a known bug? Did I do something wrong?
Thank you for your help

Edit: apparently Adadelta has a different default learning rate in tf if compared to tensorflow, explicitly stating the learning rate solves the problem...",17,tensorflow,2020-10-13
gbefpy,This Week in AI - Issue #16 | Rubik's Code,,5,tensorflow,2020-10-13
gb43wg,Tensorflow 2 and CUDA 10.2,"I'm trying to install Tensorflow 2 on Ubuntu 16.04.

I had cuda 10.1 on the system and tensorflow would not find it.

I ran the script to install cuda 10.1 on the tensorflow page.

[https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)

It says it's installing 10.1 and 418.xx and upon restart is always 440 and 10.2.

I restart the computer and now it's on CUDA 10.2 on nvidia-smi.

&amp;#x200B;

Is there a build for tensorflow using CUDA 10.2?

&amp;#x200B;

I've tried installing using downloads from nVidia or from using the scripts and using apt and they all never work. PyTorch can find the GPU and work but Tensorflow doesn't seem to.

Even instructions a month before seems to be out of date. Any insights into how to easily install tensorflow gpu on ubuntu 16.04?

&amp;#x200B;

Basically, I'm checking `print(tf.config.list_physical_devices('gpu'))` and it always shows me the empty list.

&amp;#x200B;

Problem was:

\- Should have capitalized GPU `print(tf.config.list_physical_devices('GPU'))`

I was able to lower the version of the driver on ubuntu from the ""Additional Drivers"" section. But, even with 440, it seems to work.

Update: Still didn't work while I was training a model. It is not able to find certain libraries. I used Windows and it worked without any problems. Something is out of date with package management in the directions given by tensorflow page. It seems to update libraries to 440 version of nVidia driver and not 435.",8,tensorflow,2020-10-13
gax2j9,An online AI helps to diagnose COVID-19,,3,tensorflow,2020-10-13
garey4,Dodge the Wicked Witch of ML Pipelines with TensorFlow Extended (TFX),,5,tensorflow,2020-10-13
gahrx4,Practical neural architecture search,"Have you guys found an efficient way to search for the best NN architecture? I basically define a ""NNarch"" function and modify it manually (see below), which I'm sure is very inefficient. The literature has fancy ideas like Bayesian optimization or Evolutionary methods, but do any of them work for you in practice?

    
    layer_size = [128, 120,115,110,100]
    
    def NNarch(x):
      x = tf.reshape(x, (1, layer_size[0]) )
      layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),
                                     biases['encoder_b1']))
      layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),
                                     biases['encoder_b2']))
      layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['encoder_h3']),
                                     biases['encoder_b3']))
      out_layer = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights['encoder_h4']),
                                       biases['encoder_b4']))
      return out_layer
    
    #somewhere in the graph definition..
    weights = {
      'encoder_h1': tf.Variable(tf.random_normal([layer_size[0], layer_size[1]], dtype = tf.float32)),
      'encoder_h2': tf.Variable(tf.random_normal([layer_size[1], layer_size[2]], dtype = tf.float32)),
      'encoder_h3': tf.Variable(tf.random_normal([layer_size[2], layer_size[3]], dtype = tf.float32)),
      'encoder_h4': tf.Variable(tf.random_normal([layer_size[3], layer_size[4]], dtype = tf.float32)),
    }
    biases = {
      'encoder_b1': tf.Variable(tf.random_normal([layer_size[1]], dtype = tf.float32)),
      'encoder_b2': tf.Variable(tf.random_normal([layer_size[2]], dtype = tf.float32)),
      'encoder_b3': tf.Variable(tf.random_normal([layer_size[3]], dtype = tf.float32)),
      'encoder_b4': tf.Variable(tf.random_normal([layer_size[4]], dtype = tf.float32)),
    }",2,tensorflow,2020-10-13
gaf2qz,How can I use TFP to fit some data?,"Hello, newbie here. If I have some data that looks like a combination of two distributions, what steps do I need in order to fit the data and does that functionality exist on Tensorflow or Tensorflow probability?  And then reuse that model elsewhere?",5,tensorflow,2020-10-13
ga8oxf,AttributeError: module 'tensorflow' has no attribute 'gfile',i am using tensor flow version 2.0.0 and i know in tensor flow 2.0 the gfile package has been moved into [t](https://tf.io)f.io.gfile.GFile i have also tried using this but it gives the same error,2,tensorflow,2020-10-13
ga3nqj,Weird phenomenon with the Dataset API,"I am developing a training pipeline, for which the tf nodes look like as follows:

    index = tf.data.Dataset.from_tensor_slices(self.indices)
    if self.shuffle:
        index = index.shuffle(buffer_size=len(self.indices)
    images = index.map(self.make_image)
    coordinates = index.map(self.get_coordinates)
    ground_truth = coordinates.map(self.make_ground_truth)
    images = images.padded_batch(...)
    ground_truth = ground_truth.batch(...)
    return tf.data.Dataset.zip((images, ground_truth))

If executing the above code with `shuffle == False`, everything works fine. If shuffle is set to `True`, it seems the images and ground truths are somehow shuffled differently.

Is this an intended behaviour? How could this be easily solved?

Edit: I am using TensorFlow 2.0, but 2.1 also produces this behaviour

Edit2: Further investigation revealed further weirdness. So it seems, it is not specific to the Dataset.shuffle() method, its root cause is that there is branching in the chain of transformations. Correct me if I'm wrong, but somehow the branching causes the dataset to re-sample the same index as many times as many branches originate from the given node. If there is no shuffling, the re-sampling works as expected, but shuffling causes different indices to be fed to different branches.

I also switched to Dataset.from_generator() and I shuffle the indices in the generator (in NumPy) and it still produces the same bug.

Do you guys think this is a bug in TF? should I file an issue about this?

Or is my approach completely wrong? How could this situation be handled differently?",1,tensorflow,2020-10-13
g9iwub,Should the lambda for L1 norm regularizer inversely be proportional to the number of trainable weights?,"Say I want to implement Conv2D in keras and for each Conv2D layer, if I apply 20 filters of \[2,3\] filter on an input with depth of 10, then there will be 20\*(2\*3\*10+1) = 1220 trainable weights. 

&amp;#x200B;

the value of L1 norm would proportionally increase the more trainable weights there are. Similarly for L2 norm. 

&amp;#x200B;

So shouldn't the lambda, as in kernel\_regularizer=l1(lambda), be inversely be proportional to the number of trainable weights? 

&amp;#x200B;

intuitively for me, if lambda of 0.1 worked for 10,000 weights, then applying the same or bigger lambda for 1 million weights doesn't make sense to me.",6,tensorflow,2020-10-13
g98d4y,Is there a way to add gaussian noise to the weights?,"I want to add gaussian noise to the weights (preferably per epoch), NOT on the input features themselves. 

&amp;#x200B;

Is there a way to implement that? in keras?",1,tensorflow,2020-10-13
g93mn9,Why is the depth = # of filters for Conv2D in keras?,"For SeparableConv2D, if there are 20 filters, each is \[3,3\] then it convolves a \[3,3\] kernel, followed by a depthwise convolution such that there will be 20 feature maps (i.e. if there's only 1 filter of \[3,3\] then the resulting block with have a depth of 1). Therefore, the depth of the output block = # of filters, regardless of the depth of the input block. 

&amp;#x200B;

But for Conv2D, I don't get why depth = # of filters in this case. If the input block is, say, (40,40,10), with channel\_last format, then if I were to apply one \[2,2\] kernel, with no padding, then shouldn't the resulting block have the dimensions (39,39,10)? i.e. if I were to do five \[2,2\] kernels, then shouldn't the resulting block be (39,39,50)? but when I actually implement Conv2D, the resulting dimension is (39,39,5) 

&amp;#x200B;

why is the # of filters = depth for Conv2D?",1,tensorflow,2020-10-13
g8zpxq,Play snake by moving your head: face mesh detection in the browser with TensorFlow.js,,33,tensorflow,2020-10-13
g8tgw3,Increase in number of filters in Conv2D layer worsens training/prediction performance,"Hi, I have a bunch of 2D images, and I am performing convolution on them. I am experimenting with with the number of filters in the convolutional layer, and I have found that the training worsens when increase the filters. For example, my setup looks likes this:

&amp;#x200B;

1. Conv2D(**\*\*8\*\***, (3,3), input\_shape = (100, 100, 1)),

2. Flatten()

3. Dense(Output)

&amp;#x200B;

And for this, let's say training RMSE is 0.245. But, if I change Conv2D(**\*\*16\*\***, (3,3), input\_shape = (100, 100, 1)), the training RMSE gets worse, and gets to 0.534, for example. Please note that I am keeping same random\_seed why running my experiment. 

&amp;#x200B;

Can somebody explain why this could be happening? I kinda thought that if I increase the number of channels, the model will have less struggle to learn the data, and thus predict better too.

&amp;#x200B;

Thank you!",2,tensorflow,2020-10-13
g8remb,"I have a freshly installed windows 10 with fresh anaconda install , which version of tensorflow should i install for GPU",,1,tensorflow,2020-10-13
g8nuw3,Join Our Official Tensorflow Discord Server,"[Join Here](https://discord.gg/2uGZ3Sv) 

Engage with the community and have healthy discussions",4,tensorflow,2020-10-13
g8mzb8,"Is TensorFlow suitable for large scale recommender systems, like on YouTube?","If so, how difficult would it be to create?

Thanks &lt;3",11,tensorflow,2020-10-13
g8cgps,Introduction to Swift for Tensorflow live webinar,,9,tensorflow,2020-10-13
g8b6lr,Why do i get cuDNN initialization error only for model.predict? while it's fine otherwise?,"I'm  using a Paperspace's Gradient service which is an ""upgrade"" version of  Google Colab and it is considered ""machine learning in a box"" where it  comes with pre-installed tensorflow 2.0 with gpu support

I could perfectly train via keras' model.compile then [model.fit](https://model.fit/)

and it does recognize the GPU as well and there is a performance improvement due to the use of the GPU.

But, after saving the model in a h5 format and I load the weights and try to predict with it, it fails, saying

""Failed to get convolution algorithm. This is probably because cuDNN failed to initialize""

But  this cannot be true as I've trained the convolution algorithms to get  those h5 files and the exact same code runs fine on my computer either.",3,tensorflow,2020-10-13
g80kfv,Squeeze and expand_dims bugged in TF 2.1,"I am currently doing a lot of work with point cloud classification and tried to make some custom keras models and layers.

However the squeeze and expand_dims function seem to be bugged.
Basically if I create and compile my model this works:

Model(data)

However if I try to use the built in predict (Model.predict(data)) or fit functions for keras models it fails. (the error says sth about the dimensions not being known).

I think this has to do with the fact that expand dims creates ragged tensor dimensions.

Has anyone else experienced this and/or even found a workaround?

SOLUTION:

Always stating axis solves the problem.

Example: tf squeeze(data, axis=-2)

Additional hint... Always use tf.shape(tensor) to specify shapes as it is generally more stable than tensor.get_shape()",3,tensorflow,2020-10-13
g7ktw8,Github NER Project using Sequence-to-Sequence,"Does anyone know any GitHub NER project using Seq2Seq model?

Since NER and chunking are similar, I want to use it as a reference.",7,tensorflow,2020-10-13
g792km,Loss doesn't decrease proportionally between normalized and non-normalized data,"I've built an RNN that predicts the output at a later time period from one of its  predictors (e.g. takes account balance as a predictor, but predicts  account balance at a later date).  As such, the labels need to be  normalized along with the rest of the data set.

I  find that as I'm evaluating the predictive error using MAE, some models  which improve on MAE when it is calculated using normalized columns, do not show the improvement when the predicted vs actual are de-normalized and then compared. Why should both error calculations not increase and decrease  proportionally? And which gives a better indication of predictive fit?  Thanks a lot for your help!",6,tensorflow,2020-10-13
g74ycw,This Week in AI - Issue #15 | Rubik's Code,,4,tensorflow,2020-10-13
g6y8ef,"What is this "" CUDA_ERROR_INVALID_VALUE: invalid argument"" error or warning?","When I run this simple script
```python
import os
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

import tensorflow as tf

print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))

config=tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True)
config.gpu_options.allow_growth = True
sess = tf.compat.v1.Session(config=config)
```

I get this strange error where GPU is detected but error is generated(see 3rd last line `CUDA_ERROR_INVALID_VALUE: invalid argument`)
```bash
2020-04-23 18:02:12.062095: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-04-23 18:02:12.076166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:b3:00.0
2020-04-23 18:02:12.076490: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-04-23 18:02:12.077444: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-04-23 18:02:12.078345: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-04-23 18:02:12.078625: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-04-23 18:02:12.079732: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-04-23 18:02:12.080611: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-04-23 18:02:12.083012: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-04-23 18:02:12.083780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
Num GPUs Available:  1
2020-04-23 18:02:12.094039: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-04-23 18:02:12.098687: W tensorflow/compiler/xla/service/platform_util.cc:256] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE: invalid argument
2020-04-23 18:02:12.098783: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
Aborted (core dumped)
```

Any ideas, how to resolve this?",1,tensorflow,2020-10-13
g6p5g6,Deploying TensorFlow model on Amazon AWS,"Deploying TensorFlow models on the cloud can be a hassle. In my new tutorial, you’ll learn how to spawn an AWS EC2 instance and deploy the speech recognition system I built in previous videos on the cloud.

This video is the last installment of the ""Deep Learning (Audio) Application: From Design to Deployment"" series. In this series, you’ll learn how to build a simple speech recognition system and deploy it on AWS, using Flask and Docker.

Here’s the video:

https://www.youtube.com/watch?v=ceNWWxjtG3U&amp;list=PL-wATfeyAMNpCRQkKgtOZU\_ykXc63oyzp&amp;index=8

Enjoy!",16,tensorflow,2020-10-13
g6oc3r,Does Tensorflow create virtual repetitions of data with tf.repeat?,"I'm wondering if `tf.repeat` duplicates the input tensor in memory or stores metadata about repetitions.

To be more precise, I have an `N x D1` tensor and another tensor of length `D2`. I want to concatenate the second tensor onto each of the `N` vectors in the first tensor, resulting in an `N x (D1 + D2)` output tensor. Will this output tensor have `O(N*(D1 + D2))` memory complexity or `O(N*D1 + D2)`?",2,tensorflow,2020-10-13
g6o43g,Tensor flow epoch problem,"Hi, im trying to learn tensorflow on python.
I have installed cuda, tf.
I write same code with tutorial

Epoch 5/5 60000/60000 [==============================] - 3s 54us/sample - loss: 0.0750 - accuracy: 0.9761 

This is tutorials message

But when im running this code, terminal writes 1875 instead of 60000. Am i using all data? İf not why i am using only 1875 data? How can i fix it?

My gpu 1660ti and tf uses 5gb ram from gpu while running code.",1,tensorflow,2020-10-13
g6nrz2,Combining NN and XGB,"Hi I'm new to using tf v2. Previously been using keras with tf as a backend.

I have a fairly complex (Binary)classification problem I'd like to make predictions on and I get good results with a NN (AUC\~0.94) and with XGB (AUC\~0.94) but I know the two predictions aren't perfectly correlated, and from experimentation i know that if I stack a LogisticRgression on top, using the predicted probabilities.

As the rest of my framework in in python and uses sklearn, i'd like to fit the tf model into the sklearn stacked classifier. That doesn't work and is a known bug. 

Next i made custom wrappers for the keras classifier, but that still doesn't work. Now I'm making a custom class to create the stacked model. This seams awfully contrived and complicated for what must be the most common way to improve your model performance, throw the two best different model architectures at a classification problem and use an ensemble. So i'm wondering how others have tackeled this. Are tf BoostedTrees any good compared to xgb, can a NN and a boosted tree classifier be easily combined / stacked in tf?

Before changing a lot of code over to tf I'd like to hear other peoples mileage..",2,tensorflow,2020-10-13
g6jukk,Making custom subclasses of LinearOperator,"Hi all, 

I've been working on a physics peoject which uses SGD to find global minima of a potential function(a landscape, essentially). So far, the limited code works okay, but i want to extend and systematize it. I have a bunch of complex variables(matrices) which i'd like to define as classes, or rather, as subclasses of the tf.linalg.LinearOperator, so that I inherit the methods(adjoint, inverse, is\_diagonal, etc. ) while also retaining the high-performance aspects of the library. 

Problem is, I am not that well versed in writing subclasses, and can't make sense of the docs of the other subclasses. Could someone point me to a minimal implementation of it? Any kind of advice would be appreciated. Also let me know if it is pointless, and i can achieve all that by just simply defining a matrix object in tf.",2,tensorflow,2020-10-13
g6jaff,How to run autographs and tf.function on TPU.? Bcz as far as I know TPU only supports keras fit method.,I,1,tensorflow,2020-10-13
g68mwt,How to break a loop inside a custom made Tensorflow model (using Keras)," I'm  building a machine learning application.I try to optimize the  computitional power by breaking a for-loop when iterating over facts  inside the call function, but it returns an error saying:

&gt;OperatorNotAllowedInGraphError:  using a \`tf.Tensor\` as a Python \`bool\` is not allowed: AutoGraph did  not convert this function. Try decorating it directly with [u/tf](https://www.reddit.com/u/tf/).function.  
(I've tried adding a tf function the call method

Anyone know how to break a for-loop inside the call function in a custom tensorflow model?",4,tensorflow,2020-10-13
g68ben,Running TensorFlow Lite Object Detection Models in Python,,6,tensorflow,2020-10-13
g5y1o5,Implementation of EvoNorm S0 and B0 on TensorFlow 2.0,"Hey everyone!

I spent some time implementing Evonorm layers from [Evolving Normalization-Activation Layers](https://arxiv.org/pdf/2004.02967.pdf).

The code implementing the layers is [available here](https://github.com/sicara/tf2-evonorm), along the training script.

I have been running experiments against CIFAR 10 &amp; 100 with a ResNet18 architecture, comparing BatchNorm + ReLU vs EvoNormS0 or EvoNormB0. EvoNormB0 shows consistent performance gains over BN+ReLU for this task. Here are the TensorBoard links :

\- [Single training of each model against CIFAR 10 &amp; 100](https://tensorboard.dev/experiment/QmwLVEBvSd2k9pN1AjTZsg/#scalars)

\- [Multiple trainings of each model against CIFAR10](https://tensorboard.dev/experiment/dfV7xdAqTl29iBZYjYlnLg/#scalars) :

Feedback welcome !",5,tensorflow,2020-10-13
g5vc14,Using TF to help music producers,,18,tensorflow,2020-10-13
g5v0ji,Texts to Numbers: All Words or just Words in Train_Sentences?,"In Keras, you have to represent the text into numbers. So... do I have to use words from both train\_sentences and test\_sentences or just use words from train\_sentences?  If only train\_sentences, what about unknown words in test\_sentences?

Some of the tutorials say that you just use train\_sentences, but why? Won't unknown words from test\_sentences affect the system?",1,tensorflow,2020-10-13
g5mben,I'm trying to install tensor flow and set up an environment for it using the command - conda create -n tensor python=3.6 - whenever i run this i get a huge error code which ends in ImportError: DLL load failed: %1 is not a valid Win32 application.,Any advice?,6,tensorflow,2020-10-13
g5hxcx,TypeError when trying to create model?,"Hi, 

I'm in the process of trying to generate some class activation maps for a Mask-RCNN model I have created based on [this Gihutb project](https://github.com/matterport/Mask_RCNN/). I am attempting to implement [Score-CAM](https://arxiv.org/abs/1910.01279), and to do this I need to extract the activations from the final convolutional layer. 

I have loaded my Mask-RCNN model in fine, and then converted this to a Keras model using the Github project's keras_model attribute. The summary of my model can be found [here](https://pastebin.com/AEgBhGm3).

I then take the last convolutional layer of the model, and get it's output, before building a second model which takes the imput of the previous model, and outputs the last convolutional layer when predicting on the image. I seem to be running into a TypeError when building the new model however, and I am unsure why. Thee code and stack trace of the error can be seen [here](https://pastebin.com/huXvA3r1).

Any help would be greatly appreciated.",4,tensorflow,2020-10-13
g50p72,"I need help with a seq2seq example. I'm getting ""Exception has occurred: InvalidArgumentError"" because of the line ""targets: target_batch[:, 1:]})""","Hi, I've been struggling for a few days with an error while trying to replicate this example of seq2seq: [https://github.com/sachinruk/deepschool.io/blob/master/DL-Keras\_Tensorflow/Lesson%2019%20-%20Seq2Seq%20-%20Date%20translator%20-%20Solutions.ipynb](https://github.com/sachinruk/deepschool.io/blob/master/DL-Keras_Tensorflow/Lesson%2019%20-%20Seq2Seq%20-%20Date%20translator%20-%20Solutions.ipynb) . I just can't find a way to solve it (also left an issue, but seems like the owner doesn't read it very often). I'm using vscode and it occurs because of this line:

    targets: target_batch[:, 1:]})

But it stops when I remove the next part:

    for epoch_i in range(epochs):
          start_time = time.time()
          for batch_i, (source_batch, target_batch) in enumerate(batch_data(X_train, y_train, batch_size)):
              _, batch_loss, batch_logits = sess.run([optimizer, loss, logits],              feed_dict = {inputs: source_batch,
                   outputs: target_batch[:, :-1],
                   targets: target_batch[:, 1:]})
          accuracy = np.mean(batch_logits.argmax(axis=-1) == target_batch[:,1:])
          print('Epoch {:3} Loss: {:&gt;6.3f} Accuracy: {:&gt;6.4f} Epoch duration: {:&gt;6.3f}s'.format(epoch_i, batch_loss,
                                                                             accuracy, time.time() - start_time)) 

But, of course, without that, I'm not training the model. So, can you please give me an advice to solve it, please?The error that I get is the following:

&gt;**Exception has occurred: InvalidArgumentError**  
&gt;  
&gt;Cannot assign a device for operation 'enc\_embedding/read': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'Colocation Debug Info:Colocation group had the following types and devices:VariableV2: GPU CPUIdentity: CPUConst: GPU CPUGatherV2: GPU CPUStridedSlice: GPU CPUUnique: GPU CPUShape: GPU CPUCast: GPU CPUUnsortedSegmentSum: GPU CPUSparseApplyRMSProp: CPUColocation members and user-requested devices:enc\_embedding (VariableV2)enc\_embedding/read (Identity)embedding\_lookup/axis (Const)embedding\_lookup (GatherV2)optimization/gradients/embedding\_lookup\_grad/Shape (Const)optimization/gradients/embedding\_lookup\_grad/ToInt32 (Cast)enc\_embedding/RMSProp (VariableV2)enc\_embedding/RMSProp\_1 (VariableV2)optimization/RMSProp/update\_enc\_embedding/Unique (Unique)optimization/RMSProp/update\_enc\_embedding/Shape (Shape)optimization/RMSProp/update\_enc\_embedding/strided\_slice/stack (Const)optimization/RMSProp/update\_enc\_embedding/strided\_slice/stack\_1 (Const)optimization/RMSProp/update\_enc\_embedding/strided\_slice/stack\_2 (Const)optimization/RMSProp/update\_enc\_embedding/strided\_slice (StridedSlice)optimization/RMSProp/update\_enc\_embedding/UnsortedSegmentSum (UnsortedSegmentSum)optimization/RMSProp/update\_enc\_embedding/SparseApplyRMSProp (SparseApplyRMSProp)\[\[{{node enc\_embedding/read}} = Identity[T=DT\_FLOAT, \_class=\[""loc:@enc\_embedding""\]](https://github.com/sachinruk/deepschool.io/issues/enc_embedding)\]\]Caused by op 'enc\_embedding/read', defined at:File ""C:\\Users\\jorge\\Anaconda3\\lib\\runpy.py"", line 193, in \_run\_module\_as\_main""**main**"", mod\_spec)File ""C:\\Users\\jorge\\Anaconda3\\lib\\runpy.py"", line 85, in *run\_codeexec(code, run\_globals)File ""c:\\Users\\jorge.vscode\\extensions\\ms-python.python-2020.3.71659\\pythonFiles\\lib\\python\\debugpy\\no\_wheels\\debugpy\_main*.py"", line 45, incli.main()File ""c:\\Users\\jorge.vscode\\extensions\\ms-python.python-2020.3.71659\\pythonFiles\\lib\\python\\debugpy\\no\_wheels\\debugpy/..\\debugpy\\server\\cli.py"", line 429, in mainrun()File ""c:\\Users\\jorge.vscode\\extensions\\ms-python.python-2020.3.71659\\pythonFiles\\lib\\python\\debugpy\\no\_wheels\\debugpy/..\\debugpy\\server\\cli.py"", line 266, in run\_filerunpy.run\_path(options.target, run\_name=compat.force\_str(""**main**""))File ""C:\\Users\\jorge\\Anaconda3\\lib\\runpy.py"", line 263, in run\_pathpkg\_name=pkg\_name, script\_name=fname)File ""C:\\Users\\jorge\\Anaconda3\\lib\\runpy.py"", line 96, in \_run\_module\_codemod\_name, mod\_spec, pkg\_name, script\_name)File ""C:\\Users\\jorge\\Anaconda3\\lib\\runpy.py"", line 85, in \_run\_codeexec(code, run\_globals)File ""c:\\Users\\jorge\\Documents\\U\\2020 1er semestre\\Skrit\\Práctica.py"", line 111, ininput\_embedding = tf.Variable(tf.random\_uniform((len(char2numX), embed\_size), -1.0, 1.0), name='enc\_embedding')File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py"", line 145, in **call**return cls.\_variable\_call(\*args, \*\*kwargs)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py"", line 141, in \_variable\_callaggregation=aggregation)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py"", line 120, inprevious\_getter = lambda \*\*kwargs: default\_variable\_creator(None, \*\*kwargs)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable\_scope.py"", line 2441, in default\_variable\_creatorexpected\_shape=expected\_shape, import\_scope=import\_scope)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py"", line 147, in **call**return super(VariableMetaclass, cls).**call**(\*args, \*\*kwargs)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py"", line 1104, in **init**constraint=constraint)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py"", line 1266, in \_init\_from\_argsself.\_snapshot = array\_ops.identity(self.\_variable, name=""read"")File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array\_ops.py"", line 81, in identityreturn gen\_array\_ops.identity(input, name=name)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen\_array\_ops.py"", line 3994, in identity""Identity"", input=input, name=name)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op\_def\_library.py"", line 787, in \_apply\_op\_helperop\_def=op\_def)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py"", line 488, in new\_funcreturn func(\*args, \*\*kwargs)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py"", line 3272, in create\_opop\_def=op\_def)File ""C:\\Users\\jorge\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py"", line 1768, in **init**self.\_traceback = tf\_stack.extract\_stack()InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'enc\_embedding/read': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'Colocation Debug Info:Colocation group had the following types and devices:VariableV2: GPU CPUIdentity: CPUConst: GPU CPUGatherV2: GPU CPUStridedSlice: GPU CPUUnique: GPU CPUShape: GPU CPUCast: GPU CPUUnsortedSegmentSum: GPU CPUSparseApplyRMSProp: CPUColocation members and user-requested devices:enc\_embedding (VariableV2)enc\_embedding/read (Identity)embedding\_lookup/axis (Const)embedding\_lookup (GatherV2)optimization/gradients/embedding\_lookup\_grad/Shape (Const)optimization/gradients/embedding\_lookup\_grad/ToInt32 (Cast)enc\_embedding/RMSProp (VariableV2)enc\_embedding/RMSProp\_1 (VariableV2)optimization/RMSProp/update\_enc\_embedding/Unique (Unique)optimization/RMSProp/update\_enc\_embedding/Shape (Shape)optimization/RMSProp/update\_enc\_embedding/strided\_slice/stack (Const)optimization/RMSProp/update\_enc\_embedding/strided\_slice/stack\_1 (Const)optimization/RMSProp/update\_enc\_embedding/strided\_slice/stack\_2 (Const)optimization/RMSProp/update\_enc\_embedding/strided\_slice (StridedSlice)optimization/RMSProp/update\_enc\_embedding/UnsortedSegmentSum (UnsortedSegmentSum)optimization/RMSProp/update\_enc\_embedding/SparseApplyRMSProp (SparseApplyRMSProp)\[\[{{node enc\_embedding/read}} = Identity[T=DT\_FLOAT, \_class=\[""loc:@enc\_embedding""\]](https://github.com/sachinruk/deepschool.io/issues/enc_embedding)\]\]File ""C:\\Users\\jorge\\Documents\\U\\2020 1er semestre\\Skrit\\Práctica.py"", line 148, intargets: target\_batch\[:, 1:\]})",1,tensorflow,2020-10-13
