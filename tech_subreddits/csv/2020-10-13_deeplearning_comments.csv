comment_id,post_id,comment,upvotes
g8ovrde,jacdk8,Have you considered using some cloud GPUs instead of running locally? It will likely be a lot cheaper,3.0
g8ou424,jabsdc,"Umm as far as I know, you will be creating a vocabulary / using embeddings for your words. Won’t that be enough?

What is your use case?",1.0
g8ozv5n,jabsdc,"Thanks for replying. With the sentences, I have allied metadata which essentially is the context in which sentence was said... This is ordinal data and I want to include this context. So this metadata has to be one-hot-encoded and I want extract the features or ""include"" this context in the already encoded sentence",1.0
g8op1jk,jaaza1,Difficulties... like what? There’s a thousand things under “coding the model” you could be talking about here. There’s literally nothing we can help you with based on what you’ve told us so far.,3.0
g8otayr,jaaza1,Like deciding on what to do next and how to do next.,-1.0
g8ou2uu,jaaza1,"Do you not see a problem here with how you're asking this question?

There's literally thousands of potential decisions you could make. We cannot read your mind. We have no idea what types of decisions you're struggling with.

Regardless, your best bet here is to take - and really internalize - several in-depth deep learning courses.",4.0
g8opr4o,ja5sct,"Contrary to popular opinions, the DL book by Goodfellow is Not a good text for anything. You'd be better served taking a Mooc. 

On the theory side of things, I understand your frustration (I feel the same) but there is no other way other than to study those topics and practice them. If you want to be more rigorous in your approach, then look up the bachelors Applied math and stats program for Caltech and study the courses which you haven't before.",5.0
g8p4s0r,ja5sct,Yes it is a horrible book for a beginner. The book glosses over so much math it's impossible to learn from the book if you are a beginner.,1.0
g8nv7cx,ja5sct,"I'd start with ***Deep Learning*** by Ian Goodfellow, Yoshua Bengio and Aaron Courville.",10.0
g8ovw2u,ja5sct,Online version free btw,2.0
g8nztvj,ja5sct,it more about non convex optimization,4.0
g8oqh45,ja5sct,"https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html

This is a good introductory book on what you are looking for.
It would be a much easier read if you know probability well and understand at least some real analysis",2.0
g8pkqoo,ja5sct,This is another good one in a similar vein https://cs.nyu.edu/~mohri/mlbook/,1.0
g8o3q67,ja5sct,"Are you only interested in theory or do you have a practical goal in mind.

If the latter, as an engineer myself I started by using Deep Learning on Kaggle (https://www.kaggle.com/learn), then implementing it and reading papers at the same time.",4.0
g8ppblb,ja5sct,Following,1.0
g8nzczd,ja5sct,Commenting to follow this thread.,1.0
g8o7cq2,ja5sct,Me too.,1.0
g8oj66f,ja5sct,Follow this thread,0.0
g8okilp,ja5sct,Commenting to follow thread,0.0
g8pg2mp,ja5sct,Following,0.0
g8nbu94,ja1e1e,"This is from David Silver’s reinforcement learning course, no? If so I’ll try to find some notes that I should have saved that might help.",5.0
g8nhu47,ja1e1e,"He was able to explain how to verify a state value using bellman's equation but nothing on how to calculate each individual state... I'm genuinely confused. If you could, I would be absolutely grateful if you could share the notes with me.",4.0
g8nqcwq,ja1e1e,"For particularly small problems (like the graph shown above), the standard approach is through value iteration.

1. Initialize the states with the immediate rewards of that state. I believe in this example, everything starts off at 0 (but other problems there are desirable states to be in).

2. Update each state values based on the next best state (i.e., the action that has the best expected reward based on the state you land, make sure to include rewards/costs of taking that action). This step could also be according to some policy in which case you'll be updating expected transition rewards according to that policy (which could be deterministic simple sum of action reward + implicit resulting state rewards, or probabilistic where you use expected value of policy). Generally, each action has an associated probability of transitioning to some other set of states (non-deterministic transitions).

3. Repeat until convergence of values (updates no longer change state values).

Hope this is helpful. If you have a discount factor, it's important to include in the calculations of current state valuation = reward(state) + y(value of next state under our policy)",6.0
g8nqo22,ja1e1e,Exactly. Since you have the MDP fully defined you can simply use dynamic programming to calculate all the values.,2.0
g8nysln,ja1e1e,"This was well explained, thank you for taking the time to write this. I'm still a little fuzzy on the calculations though. So if I start at the node containing -1.3 (pretending that all nodes are empty). To calculate in order to get -1.3, would It be -2(reward state) + 1(gamma)? Where does the policy go into the calculation? I apologise I'm a little slow lol",1.0
g8oa88s,ja1e1e,This isn't Deep Learning,1.0
g8oarze,ja1e1e,Lol then what is it then?,0.0
g8okiic,ja1e1e,"Reinforcement Learning is a machine learning paradigm. You don’t need to use DL to solve these problems, in fact, in this case you have to use dynamic programming",1.0
g8n99fk,ja0ka0,The best bet would be to learn how to code before posting questions on Reddit about hypothetical things that will never happen in the future.,-6.0
g8mm9e9,j9r82n,"marked the video for later watch.

BTW is this is the paper from neurIPS 2020 which is receiving a lot of attention which seems to outperform CNNs?",2.0
g8mx3sb,j9r82n,"Yep, that's the one",3.0
g8niloj,j9r82n,"Thanks for marking it. This paper is currently under review at ICLR 2021. And yes, it's receiving a lot of attention because it achieves better results compared to CNN-based models when trained on a larger dataset.",1.0
g8l2aoh,j9pvnr,I released this blog post on end to end image classification last week. Let me know what you think about it :),1.0
g8kd128,j9jvgl," i assume u have done something related to nlp before

1. build a small Q&amp;A bot. a lot of people have done this before. resources are literally everywhere. 
2. kg-bert. if you are looking a bigger ( aka. cool) project , do some research on this one. [https://arxiv.org/abs/1909.03193](https://arxiv.org/abs/1909.03193), this is the paper's link. it is kinda cutting edge work, you gonna love it i guess.",2.0
g8kdmen,j9jvgl,"Yeah i have already made an Chatbot using RASA framework , it is a open source framework to develop textual and voice over chatbots

You can have a look at it . i have written a blog on that too :
https://medium.com/data-science-community-srm/developing-chatbots-with-rasa-intuition-to-implementation-39c1dd34274c",1.0
g8kt0bb,j9jvgl,then u should take a look at kg-bert. interesting research. still BerT but use triples as input.,1.0
g8k5e11,j9iwqz,It couldn't be one to one since you could invent any number of loss functions that could be used with a softmax final layer.,3.0
g8k8wvr,j9iwqz,No. All that matters is whats the input of the loss function. Whatever requires a probability needs a softmax (sigmoid if binary). The loss could be something completely irrelevant to cross entropy and would still require the last layer to be a softmax,3.0
g8k2puj,j9iwqz,"From my perspective the only thing that matters is, Sigmoid=bce, Softmax=cce/sparse cce, Linear=mse (and other mse variations). Other combos don't work theoretically and not practical imo. Feel free to correct me if anyone feels I'm wrong. Happy learn a thing or two.",5.0
g8lslrm,j9iwqz,Could you speak to why bce always goes with sigmoid? I would have guessed bce would work with any activation with the same output range.,1.0
g8lwdvj,j9iwqz,"Sigmoid will give you output between 0 and 1. Also if you notice the output dimensions for Sigmoid layer would be 1 (because it is binary). Hence, bce would be the only option here. Can you tell me other activation functions that gives you a single output between 0 and 1, other than Sigmoid?",2.0
g8m82et,j9iwqz,"I suppose you're right. I can brainstorm other mathmatical functions in that range [step function, saturated linear, (tanh(x)+1)/2] but I don't know any practical reason to use them for activations.",2.0
g8kdgj3,j9iwqz,"There is no such 'correspondence' since we need a loss function to account for proper handling of log loss and activation function for updates of our weights and biases while backpropagation. Every Activation function and loss functions are different for different purposes and sometimes, you will find they may work entirely in a different way! You have to explore them while working on a MODEL.

  
It all depends upon what sort of operation you are trying to achieve   
Like if you are performing a multi-class classification, then you have to use categorical cross-entropy as a loss function and softmax as an activation function. But we can also use different loss functions for Multi-class classification while keeping the activation softmax function as it is. So I don't think there is any such connection or link between them!",2.0
g8k7ip8,j9iwqz,How can I choose a loss function? What features/properties should I be looking for..?,0.0
g8k7xvu,j9iwqz,"There should not be any confusion choosing loss function. Because, most of the times you implement a network that's already been published on arxiv. So, they'll provide the best possible loss function for the given network.

If you are experimenting on a new deep learning technique, then you should check with different types of loss functions as well as optimization functions.",0.0
g8ky2n4,j9ivsw,Someone want to give a tldr (tldw?) for those of us who can't play audio right now?,5.0
g8kylsi,j9ivsw,"Sure, [here's the one](https://twitter.com/bhutanisanyam1/status/1309582455842639873?s=20) I wrote right after the interview",3.0
g8l20u5,j9ivsw,"Thanks!

TLDR: 3070 for students, 3080 for professionals. 3090 only if you are doing high resolution machine vision or pretraining NLP, where the extra RAM is useful.

Crazy detailed article: https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/

Interesting point: Nvidia made things a little awkward by dropping 1gb of vram from the 2080 ti to the 3080. Some existing pretrained models will be too big now. May require framework tricks to free up the extra vram.",2.0
g8l5l1i,j9ivsw,"&gt;Crazy detailed article

Thanks for the link, Tim Dettmers is the author of the article :) (The guest of this interview)

We're basically discussing the article + a Few FAQs here\^",1.0
g8kteho,j9ivsw,"Nice work. I’ve read Tims blog a few times and it’s great to have him talk through it. Thanks for this, keep up the good work.",2.0
g8kymin,j9ivsw,Thanks for watching and for the kind words! :),2.0
g8kuok0,j9ivsw,"Great job bhai..!! After reading his blog, it good to directly listen from him. You were mentioning about a community here in india, what community were you talking about?",2.0
g8kyndj,j9ivsw,"Thanks for watching :) 

I was referring to the ML Community in India, generally speaking!",2.0
g8kz0tw,j9ivsw,Oh. Thank you..!!,2.0
g8k8p48,j9hbho,"No.

It should be easy to see that it doesnt help by training with and without",1.0
g8j09r8,j9bi9v,Do u mean upsampling2d??,2.0
g8j2uxo,j9bi9v,"I want to do something like [this](https://www.researchgate.net/figure/An-illustration-of-two-types-of-unpooling-operations-a-simply-replacing-each-entry-of_fig9_323470988) , I can't do that with Upsampling2D",1.0
g8hy40r,j95938,"Some thoughts to try, in order from most likely to least likely:

Many of the base models in Keras come with their own data preprocessing function. Replace your data preprocessing layers with that one function call to guarantee correctness. https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception/preprocess_input

Is there a reason you're sampling a mid-level of the base model (add_5) and not its last layer? I've never seen that pattern before. (You've set include_top=false so the last layer should be safe to use).

I would leave out that global average pooling 2d while debugging. Then once your model is training nicely you can try it and see what happens to accuracy.

If you unfreeze the base model, does it make training progress?",1.0
g8k1acy,j95938,"Thanks for response.

I have tried other blocks upto 11 but didn't go till last. Since the domain of the data is different so I thought initial blocks will provide basic elementary image related feature. Going deeper in the architecture might result is abstractions particular to the domain (eg. imagenet has object images which is different from brain electrophysiological data). However, will try and visualize what is the last block's output just to unturn every stone.

Are you suggesting straight flattening the feature output from the basemodel instead of global avarage pooling. If yes I did it with no avail.

The suggestion mentioned in the first point is I think incorporated as the normalization layer is added which is the preprocessing step for Xception model.

Actually, I did try unfreezing during the first round of training but due to less data model didn't gone well. 

However, I would revisit all the points just to crosscheck. 

Thanks.",1.0
g8kdlq2,j95938,"Oh. Your images are brain scans? Then the imagenet pretrained weights might not be very useful. To get good performance, I bet you would need to train without freezing and provide more training data.

The strategy I use is to freeze the base model and train (to get ok weights in the new layer). Then reduce the learning rate 10x, unfreeze the base model, and train some more.
https://keras.io/guides/transfer_learning/


With the preprocessing, I would still try replacing this:

norm_layer = keras.layers.experimental.preprocessing.Normalization()

...

norm_layer.set_weights([mean, var])


With this:

x = tf.keras.applications.xception.preprocess_input(x)

So you can be 100% sure it's preprocessing correctly.",1.0
g8hj5f3,j92tlz,I wanted to know more about using multiple PCIE risers in a 4x GPU system that I am going to build for deep learning. I was searching for some information regarding this for a very long time but couldn't find much out there. Your post helped me a lot. Thanks a ton!,2.0
g8hkxrq,j92tlz,Glad it helped! Deep learning is different from rendering with r/rendertoken but some of it should work the same.,2.0
g8hkysu,j92tlz,"Here's a sneak peek of /r/RenderToken using the [top posts](https://np.reddit.com/r/RenderToken/top/?sort=top&amp;t=year) of the year!

\#1: [AMA with Jules Urbach, CEO of OTOY/RNDR - ask questions for the AMA at 2pm PDT on 6/23!](https://np.reddit.com/r/RenderToken/comments/hcztqb/ama_with_jules_urbach_ceo_of_otoyrndr_ask/)  
\#2: [deadmau5 &amp; The Neptunes - Pomegranate (Official Music Video) - You guessed it, all done in Octane and partially rendered on the RNDR Network](https://www.youtube.com/watch?v=aAvyS6HSa0A) | [6 comments](https://np.reddit.com/r/RenderToken/comments/hk3wsf/deadmau5_the_neptunes_pomegranate_official_music/)  
\#3: [$RNDR Wins!! Lowest price for GPUs in this render farm test.](https://twitter.com/MacDaffy/status/1299345559044722688?s=20) | [16 comments](https://np.reddit.com/r/RenderToken/comments/ii7kub/rndr_wins_lowest_price_for_gpus_in_this_render/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/fpi5i6/blacklist_vii/)",2.0
g8hl0pd,j92tlz,"2pm PDT happens when this comment is 6 hours and 55 minutes old.

You can find the live countdown here: https://countle.com/LHDgcLKln

---

I'm a bot, if you want to send feedback, please comment below or send a PM.",1.0
g8hnaxy,j92tlz,it just happened. bots are taking over the conversation.,1.0
g8go87l,j927ey,"You either scrape it or find an existing dataset. There is also the option of buying datasets, but if you're new it might be expensive for you.

In your case, https://www.kaggle.com/techsash/waste-classification-data is a good start.",4.0
g8h1389,j927ey,"Thanks

but where can I buy image datasets?",2.0
g8h2db3,j927ey,You could sign up on Microsoft for Bing APIs (paying some amount monthly) and use the key to create your own custom datasets.,2.0
g8h2wjt,j927ey,A free flickr downloader would probably be a better option,3.0
g8h6p6c,j927ey,"Unsplash released last month for high quality images.

https://unsplash.com/data",3.0
g8h8jdb,j927ey,"Ohhh, that's awesome. Gotta love Unsplash.",2.0
g8h9boa,j927ey,"That's great, indeed.",2.0
g8hfrrr,j927ey,**thank you**,1.0
g8gb352,j8zbbu,"Attention with ensembles. I don't think it will work they way you imagine it, as it will take too much memory to place in RAM and input sizes aren't the same. Also, there is absolutely no reason to do something like that, at least not in that way.

Routing has its benefits when you're routing for one task, because then you might end up developing multiple efficient subnetworks that end up making your model better than one without routing.",2.0
g8gdjdp,j8zbbu,"Its a difficult problem.. what you could do is, to have many ML/DL services and instead of have them all in one place, to implement a “smart routing main node” which automatically identifies the type of the input data and calls the corresponding ML/DL service.. at least it was a “near optimal” solution for me. Of course this solution will have some errors..",1.0
g8fcis1,j8x0rp,"Have a look at the fast.ai course, it’s free, amazingly good and very top down/hands on...",3.0
g8ge0oh,j8x0rp,As you are a youngling try to focus on statistics and probability rather than Deep learning frameworks or even deep learning. Start with maybe Khan Academy's college/AP statistics and probability.,1.0
g8i9igd,j8x0rp,Stanfords Computer Vision Course should be fun! http://cs231n.stanford.edu,1.0
g8jb549,j8x0rp,"Hi, I am currently actually doing the same thing! 

I am taking Udacity’s deep learning course and I am writing articles on medium about the concepts I find difficult. This really helps solidify the information! Additionally if something peaks my interest (like the basic idea of perceptrons) I try to build them myself and write articles about them.

I recommend maybe retaking the course, and maybe writing some articles on the concepts that are difficult. Since you have taken it once you should be able to skim through a lot of information. 

If you want to start building check out PyTorch 60 mintue Blitz : https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html

Maybe try the same approach a.k.a write about the things you find difficult!

If you have anymore questions feel free to message me.",1.0
g8koder,j8x0rp,"Hello! My name is Alejandro and I am 18 years old. I started in Machine Learning when I was 15 years old. My recommendation for everyone is to start with courses like Linear Algebra. Statics and probabilities. You need to understand concepts. Only that! You can follow me on all social networks as ""@lexandermorales"". 5 months ago I posted my video on: ""How to get started in AI if I am a teenager"".

[https://www.youtube.com/channel/UCAHSbPZyWCuXKyoM8rnnNYA?view\_as=subscriber](https://www.youtube.com/channel/UCAHSbPZyWCuXKyoM8rnnNYA?view_as=subscriber)",1.0
g8eo1qy,j8vc46,[deleted],1.0
g8eohr5,j8vc46,[deleted],1.0
g8eok90,j8vc46,Is ram important for sequence data in general.,1.0
g8ee8rf,j8tb9y,Use something premade like https://neuro-ai.co.uk you never have to touch cloud infrastructure it’s made for you. You’re charged only for the compute time on GPUs,1.0
g8fd0yl,j8tb9y,This is one of the reasons I’m building [seeme.ai](https://seeme.ai).,1.0
g8fbeoj,j8s3dn,"Some of these have pretrained ones and a GitHub to run them

https://paperswithcode.com/task/image-super-resolution


ESRGAN has one that I have used many times before.",1.0
g8gab51,j8lk2h,This is a great piece of work and very good engineering.,1.0
g8h9v1y,j8lk2h,"Thanks, I really appreciate your comment!

Hope you got some value from it.",2.0
g8hsjpv,j8lk2h,Curious if multiprocessing made any sense to get around the GIL? Maybe to complicated in the code flow?,1.0
g8ldbhq,j8lk2h,That’s a lot of FPS. Are you sure?,1.0
g8cqg5t,j8leeg,"Since deep learning of structured time series isn't my forte I won't try to answer, but I do have one or two best practices that should apply -  

1. No matter how you store the data, you probably want a way to store a local cache so you do not have to wait for the data when debugging or when you spin an instance. 

2. Don't wait for later, implement pipeline testing. I suggest GreatExpectations.

Hope this helps!",3.0
g8cu5uq,j8leeg,"thanks, I'll see what I can do.",1.0
g8avmg8,j8gb26,"neat, but what's this got to do with deep learning?",23.0
g8aw5w5,j8gb26,and the code isn't in the comments...,3.0
g8bqb4v,j8gb26,This has nothing at all to do with deep learning and should be removed from this sub.,9.0
g8brima,j8gb26,okay cool but why in this sub?,5.0
g8azc6f,j8gb26,"Well done!  
Always glad to see people using C#.

Recently just built a small camera app for computer vision on UWP in C#, hoping to add some prediction models to it later this year. Big fan of the language,",0.0
g8bpl8t,j8gb26,It'd be actually interesting to see deep learning application for building cartoon stories start to end by itself.,0.0
g8ac9yn,j8a0gy,"I think making your own projects is one of the things you could try. It's incredibly helpful, because in the course there's a lot of code that is just given to you, and while building something from scratch, you have to go through the entire process by yourself. I found it to help my understanding of both the models and the process. You could hop onto Kaggle and try participating in one of the competitions there, or just pick up one of their datasets and build your own thing with it.  

Personally, I've found the courses to be a great starting point. But it was only after finishing one or two pet projects that I gained some confidence in the knowledge. The projects don't have to be anything super fancy or reality shattering, it could just be something simple. The aim is to just get started. And once you do, you'll discover a lot of things that are done practically that the course didn't teach you (although for a good reason). This helped me when I finished the specialization, I hope it helps you too :)",3.0
g8axh3n,j8a0gy,Thanks a lot,1.0
g8altwo,j8a0gy,"
Hi There.  Glad to hear you finished the first step of learning deep learning. That's very clever of you. Now, I bet you have a basic understanding of deep learning. For instance,  the concept of gradient descent,gradient vanish... What is object detection, why LSTM have a better performance than RNN etc. Things like what I just mentioned is particularly essential for building up a much more complicated architectures like transformer and reading state of the art papers published recently. If you have no idea what I just talked about, please watch that specialization again. A robust basis of deep learning is the key to make groundbreaking works in your research journey.

  Now,let focus on what you need to do in the next step. Learning a deep learning framework like pytorch. There is a saying that practice makes perfect. What you just learned is just a bunch of theoretical knowledge. It is dramatically different in practice. A lot theories may not work as you read in the books.Also deep learning is still quite a empirical process, so it's important to gain some practical tuning hyper parameters experiences which you will never get from Ng's lectures. So learn a framework then apply it to your practice.
  By the way, when it comes to learning a framework, here are a series of questions and problems you are gonna need search the answer for yourself.
 1.which framework you wanna use? There is pytorch for research and TensorFlow for application in real world. It Its on what you wanna do with the framework. I am currently using pytorch.
2. Find some learning materials of learning framework.
3. Find some actual projects to practice.
For these two problems, I suggest you to look up in github.

 I still got a lot to share with you, but....

You have a nice day!

I am not a native speaker and i am currently practice my English writing skill. So I looking forward to any of your advice. Thanks a million : )",2.0
g8as5bd,j8a0gy,"Really appreciated that, thanks. I was about to start learning pytorch. Do you recommend a path for that?",1.0
g8axzeb,j8a0gy,"Read papers

You probably want to get a list of important architectures up until now and study those papers first, I don't know if that course goes that in depth (doubt it does).

Once you find something that you like or you get an idea for something, try implementing it. For an example, as I was reading up on RNNs I stumbled upon Differentiable Neural Computers. The concept of a memory networks interested me, so I kind of experiment with certain ideas relating to them ever since. Eventually by reading papers you're going to find something that sparks joy in you, but to know whether it is sane and worth you probably need to know if a better solution exists, and that requires reading a lot of papers.",2.0
g89s3i0,j8a0gy,"Have you finished the entire deep learning specialisation? Or just course 1(Neural Networks and Deep Learning) of the the 5 part specialisation? 

I am about the finish the specialisation and I was wondering about the same. I was thinking of taking up the tensor flow specialisation and simultaneously looking for an internship. 

I also looked into the deep learning nano degree on Udacity as a worst case substitute for an internship. But it’s quite expensive.

Some of my friends also suggested that I take up mini projects on the internet. Am probably going to consider that too.",1.0
g89sbyb,j8a0gy,I have finished one course which is the first couse of specialisation I guess. I didnt know it has five part. Nice to here that,1.0
g89sout,j8a0gy,It’s called Deep Learning Specialisation on Coursera. Have a look at it.,2.0
g89t1fz,j8a0gy,"Thanks, btw I don't know about Udacity but Coursera has financial aid opportunity. Take a look at that",2.0
g898erp,j87swm,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/XimingFeng/ddpg-stock/blob/master/DDPGTest2.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/XimingFeng/ddpg-stock/master?filepath=DDPGTest2.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1.0
g8971cc,j87evq,"Having worked with both, there isn't much of a difference if you're doing gpu training. The CPU is just there to preprocess and move data around at that point. I like my 3900X because it got me more cores per dollar and I do end up doing some work that is cpu intensive.",6.0
g89j1t5,j87evq,"Yes, CUDA works fine on AMD CPUs. It only cares about the GPU, not the CPU",6.0
g897js5,j87evq,I have R7 2700X and GTX 1660Ti. CUDA works fine.,4.0
g89r2af,j87evq,"You might be a little confused. CUDA is about GPU computation, so the CPU doesn't matter with what you're talking about. If you want to use CUDA then you need an Nvidia GPU though, so AMD CPU + Nvidia GPU (as you say, and as I have) is a good way to go.",3.0
g8atn3y,j87evq,"Always NVIDIA, but consider using something like [this](https://docs.neuro-ai.co.uk) instead if you really need the speed",2.0
g8a4nzb,j87evq,Thank you for your answers.  What should I buy between AMD 3900X or Intel i9 10900 (with a 2070 super GPU)?,1.0
g8a7mfs,j87evq,"Intel is historically known for single thread performance, while amd for multi core. As training models doesn't really require higher single thread performance, amd is the way to go. Personally, I would suggest you to wait for a while (if possible) and get your hands on the new zen 3 amd chips and/or the rtx 3000 series, as they are only slightly mor expensive than their predecessors, but provide a significant boost in performance.",2.0
g87ymwd,j7v57i,"Oof, these comments are a mess.  I'm not sure if the OP got the answer they were looking for, but just for reference:  


&gt;I want to precise that in my task ,images don't have classes ,they are just a bunch of images that cannot be classified . I can simply say that I have one class.

Although cross-entropy is often used for classification, that's not really how it's being used in this case.  You're not determining the class of each image, but the pixel value of each pixel in each image.  Pixel values should range between 0 and 1 for gray-scale images, so using cross-entropy makes sense as you're essentially trying to find the value in this range to which the pixel should be assigned.  There's plenty more to say about this, but the OP is misunderstanding the role of cross-entropy as a loss in this instance.

And you *can* use MSE, it just has different properties which may or may not be more suitable for this task.  In general, cross-entropy is better for pixel prediction for a number of reasons (other comments discuss this), but I've had success with both with regards to VAEs.",5.0
g880air,j7v57i,"Very helpful Nater ,thanks . What if I use sigmoid activation in my last layer , I would be able to get ride off the sigmoid_cross_entropy_with_logits ,isn't it ?",2.0
g8825mn,j7v57i,"Basically. [sigmoid\_cross\_entropy\_with\_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits) takes logits as input, applies a sigmoid function, then takes the cross entropy.  If you're already applying sigmoid in your final layer, do not use this.  Instead, use [binary\_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy) (with `from_logits` set to `False`).",2.0
g88fpsn,j7v57i,"Perfect , thanks a lot",2.0
g883utu,j7v57i,"Looking through the code from the TF website, they actually said explicitly that each pixel is modelled by a Bernoulli dist and that they did binarize the images. I guess that’s why they used binary cross entropy.",2.0
g884oqw,j7v57i,"Yeah, I haven't looked at the specific code, but that shouldn't change the use of binary cross-entropy.  Whether the images are binary, gray-scale, or RGB; pixel values should always have a value in `[0, 1]`, so the use of binary cross-entropy is always appropriate.",1.0
g87f1pu,j7v57i,"I don't understand why you are trying to avoid sigmoid + cross entropy at the output layer, but if you want to use mse you can do relu + mse. You would need to clip your results while doing inference in that case.",2.0
g87g4u3,j7v57i,What do you mean by clipping the results ?,1.0
g87g8bi,j7v57i,"If you use relu your output is unbounded, but the pixel values for an image is bounded. So you have to clip them (setting anything above 1 equal to 1, or however you are normalizing your data), if you want to use relu at the output layer.",2.0
g87gb7h,j7v57i,"Yes you’re right ,I can do a RGB mapping in that case",1.0
g898218,j7v57i,"From my exp, relu works awfully if you put it in the end like you said, so I’m guessing you’re not saying this from exp.",0.0
g89a2ta,j7v57i,"It really depends on the application. You shouldn't assume someone is talking bullshit just because your ""experience"" tells you otherwise.",1.0
g8b4au6,j7v57i,"Yeah, well, at least in this case the use case is quite clear.",0.0
g877mzd,j7v57i,"they just did that since they treated the images as binary I think, but if you just want a generic reconstruction loss MSE is fine, i've done it, it works (combined with KL divergence on latent distribution)",2.0
g87drq1,j7v57i,"No, they don't treat images as binary. They use sigmoid activation function at output (mapping to the range [0, 1]) and so they can use crossentropy. If you don't map your values to that range you can't use crossentropy.",3.0
g87dxxg,j7v57i,"By using cross entropy as the loss they're assuming the labels are binary. Yes, the output is continuous, but the target is to be binary",0.0
g87e175,j7v57i,"No they aren't. Do you know how crossentropy works? It's not just for classification, it can also be used for regression, like in VAEs or GANs. [Cross entropy for dummies](https://towardsdatascience.com/cross-entropy-for-dummies-5189303c7735)",1.0
g87e44v,j7v57i,"Lol, yes I do. What are you even talking about man? Binary cross entropy for an output that has a sigmoid activation is very natural.",1.0
g87e7zl,j7v57i,You claimed that images and labels are binary which is simply wrong. I think you don't know what you are talking about.,2.0
g87ed5x,j7v57i,"For the examples that person was mentioning it was grey scale images with range bounded between 0 and 1, so yes binary. You dont seem to make much sense. And if the labels were continuous between 0 and 1, then I mis-spoke.",-4.0
g87ef75,j7v57i,Haha bro you don't even know what binary means and you are trying to argue. Please stop embarassing yourself,3.0
g87eh97,j7v57i,"Dude you're really heated, try not to let it get to your head.",0.0
g87ek1g,j7v57i,"No I'm not, I'm just trying to provide correct information unlike you. It's very clear that you don't even know what binary means and you are still pushing it.",1.0
_,j7v57i,,
g87dc05,j7v57i,"Yes , I just wanted some opinions , which I thank you for ! When I try to use  the sigmoid cross entropy I ended up with an extremely high loss . Thank you again !",1.0
g898gwk,j7v57i,I’d recommend you look at some other people’s successful code or guides.,1.0
g898bq0,j7v57i,You should probably use Adam optimizer and LR on the order of like 0.001. You basically can’t train VAE with SGD/Nesterov/etc nonadaptive.,0.0
g89ad2i,j7v57i,"Wow I don't understand how certain you can talk like that, without even knowing the problem. SGD+momentum beats Adam in numerous occasions, quite likely including this problem.",2.0
g8b6hk5,j7v57i,Do you have any exp in training VAE? I’m quite certain of what the answer is,0.0
g8b6td9,j7v57i,I implemented a VAE with custom latent loss using only NumPy for my senior year project. So I know what I'm talking about. And you definitely sound like a pretentious ahole.,1.0
g8denwp,j7v57i,"Fair enough in that case. But non-adaptive optimization algorithms don’t work well on generative networks, nor VAE, nor GANs. Which is why it failed for thread author. I don’t remember well, I think they might have worked for non-variational plain autoencoder, but not for VAE. In some special circumstances, for example if your network is super simple, might work. Maybe that was the case with your numpy net. With better performing deeper network with residuals, it most likely won’t.",1.0
g8762bp,j7v57i,"I remember I tried both on fashion mnist dataset and had different results. As far as I remember, it was also weird to me that sigmoid loss worked better. I recommend you try both and look directly at results. If anybody would comment on why sigmoid makes more sense theoretically I would love to hear that.",1.0
g876mly,j7v57i,"I think the trick is with distribution tails.. With MSE your distance is always the same, not weighted, where with sigmoid cross if you predict class that should be 1 with something like -5 prior to activation, the loss will be huge. Its like stricter loss function.",0.0
g8789zz,j7tuft,"Weights are learned and changed automatically by algorithms like sgd, adam. Suppose u know how to change weights, how would u change it? There are so many parameters and infinite number of values to choose from. Maybe the learning has stagnated when accuracy isn't increasing.",3.0
g88tcrl,j7tuft,there will be no use of randomly changing weights during training as ultimately the trainer might converge to same point and get stuck as in your case. What you can try is cyclic or dynamic learning rate which varies from lower to upper bound and help you to overcome local minima or saddle point.,1.0
g86xj9t,j7tijb,Look at the documentation of the learning rate scheduler.,1.0
g86yocq,j7tijb,Already did. Unable to get useful insight.,-1.0
g875t55,j7tijb,"Well mate, we're not your personal coders. What exactly are you struggling with? There's quite a lot of info on doing this kind of thing that you can google.",1.0
g87vb62,j7tijb,Thanks for such a nice help,0.0
g87vfk9,j7tijb,"It's exactly as much help as your low-effort post here deserves. Ffs. Learn to help yourself, and then ask people when you have specific issues, *along with* a description of what you've tried so far.",1.0
g881rj7,j7tijb,So nice of you,0.0
g883ix6,j7tijb,"Pay me, and I'll be nicer.",1.0
g86z6n2,j7tijb,Look at your optimizer’s attributes. I think you can access (and change) the learning rate directly,1.0
g88zz3t,j7tijb,"Have a look at LambdaLR, https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.LambdaLR",1.0
g86qmnp,j7nwje,Have yo tried GradCam?,1.0
g86x1c1,j7nwje,"Just some ideas, as - afaik - this has only been implemented for classification so far. For semantic segmentation, I think you would end up with one saliency map per pixel if you start from the segmentation map. For one saliency map per segment, you would have to start from one of the inner layers, however, you don’t know if and where a segment as a whole is encoded. I also remember a paper from Google (Been Kim, i think, CVPR 2019) where she said that network saliency maps generated by GradCAM are not really trustworthy. Maybe you could look into LIME, which does not work with gradients but masks parts of the input and then looks at the effects at the output, which could effect multiple pixels at once in the segmentation map. For object detection, I think you would have to work with the layer before the bounding box regression.",1.0
g86xkcb,j7nwje,"Some of the existing methods are implemented in torchray, https://facebookresearch.github.io/TorchRay/",1.0
g86ulf6,j7nm83,No,1.0
g86uktp,j7nkby,No,1.0
g85tsdh,j7nkby,No. It’s not.,1.0
g86t6m1,j7mng0,This whole post barely provides more information than the headline,1.0
g86wybj,j7k3dm,"I'd call Attention an *architecture* because there many practical decisions when deciding to use attention. The *technique* of the attention function isn't going to be very useful just by itself. All the videos that I've seen that cover attention refer to it as the 'attention architecture'.


Some examples of decisions I've seen are: wether you need input embeddings; single or multi-headed attention; how many layers of attention; will you add a fully connected layer as output? The answer to these questions affect your design, hence architecture.

However, I'm not an expert and only added my two cents since nine hours have passed without a single reply.",2.0
g84wpox,j7inbs,Take a class on linear algebra,1.0
g85xn74,j7ig7y,"I know unrealistic right? However, I find their process of reviewing is slightly unfair. Why? 

First, the conference allows the authors to submit a paper that was already on arxiv or even allow them to post their paper during the reviewing process.  So, the latter isn't ""blind"" anymore. 

Let me give you an example. There are a lot of papers that have been uploaded to arxiv and are cited more than 50 times but never got accepted or submitted to conferences. Now, let us say you are an assigned reviewer, and was given a submission. First thing you could do is google the paper's name and hop! There you find the authors' name and their affiliations and that paper has already more than 120 citations. Let alone if these authors are from well known company/universities. What would you do? There would be a bias in reviewing the paper and will easily get accepted. I am not generalizing but this happens.   


Another thing is that, what is unfair in most conferences is that big tech companies are getting more involved in DL conferences. I think it is ok. But again it is unfair for purely university research work where the research students do not have the luxury of owning 10 GPUs, 10 TPUs ...  A research paper could take a year or two to be finalized in such condition. However, for big companies their research is done in 4 months due to a lot of collaborators, and of course abundant computational resources.    


I would like for big conferences like Neurips, CVPR, ICML, etc... to consider making two rounds of submissions: (1) For research that is purely done through university research (2) For research work that is done through big tech companies.

Anyway, I apologize if I seem to rant but I needed to get this out of me.   


Thanks for the post!",20.0
g86lgal,j7ig7y,"I definitely can see the necessity of two different selection processes for papers by industry and papers by univ. 

But is the bias in the case of already published and cited papers really bad ? I mean, doesn't a lot of citations and a brand name (big tech company) most probably will be equal to good quality research?",1.0
g8737bu,j7ig7y,"Not only that, even my shit tier Uni can afford to buy a lot of GPUs for research or at least rent some.

The point of research on unis is to make efficient, good stuff. Not inefficient, state of the art stuff. I have not seen A SINGLE researcher on my uni that is impressed or advocates for using large networks that need a lot of GPU hours to train. But that doesn't mean than 10 GPUs/TPUs is out of reach.",1.0
g86i36z,j7ig7y,I'll read half tonight before going to bed and the second half will make great bathroom reading tomorrow.,2.0
g84su8v,j7hxb4,"What you are asking doesn't make any sense. You are comparing oranges (editors) with shoes (resources).

Here is the thing which will connect both, enjoy:

[https://github.com/abhishekkrthakur/colabcode](https://github.com/abhishekkrthakur/colabcode)",2.0
g84vjxc,j7hxb4,"How does doing a deep learning project in python vscode differ from a google colab oratory notebook, that’s my question. I’m a undergrad whose still new to this stuff.",0.0
g877v0t,j7hxb4,VSCode will use your local computer while colab uses Google computers. Training deep learning is best done using GPUs. Other things like evaluation could be handled locally. You should learn to use both VSCode and Colab.,2.0
g87doa8,j7hxb4,"Even with colab GPUs things can become quite expensive, and the notebooks are not always the best for responsiveness. I locally host a Jupyter Notebook and then use [https://docs.neuro-ai.co.uk](https://docs.neuro-ai.co.uk), it just replaces the training function and deploys on a gpu with some nice graphs.",2.0
g84o9gr,j7hjff, Are they the same as you read.csv()? I'm still confused,1.0
g85cj4h,j7geyh,"Its a couple years old but you can start with this one https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.23730

In case you've never worked with it, I've never gotten good results classifying eeg that wasn't from a published dataset such as the one used in the linked to paper.",1.0
g84g6b8,j7f1c9,"Some potential reasons (not sure if these are right in your case): you aren't initialising the weights well (e.g. try xavier), or you aren't normalising the input data properly",3.0
g84glw2,j7f1c9,"Sure I will take a look at Xavier. I am using the below transformation to normalize the inputs:

`transform_train = transforms.Compose([#transforms.ToPILImage(),`  
`transforms.RandomCrop(32, padding=4),`  
`transforms.RandomHorizontalFlip(),`  
`transforms.RandomRotation(15),`  
`transforms.ToTensor(),`  
`transforms.Normalize(mean, std)])`",1.0
g84mn9m,j7f1c9,another thought: you seem to be doing data augmentation aswell at the same time. generally its a good idea to turn complications like data augmentation and any regularisation off at first just to make sure there aren't any bugs and the network is able to learn something.   after that if the network shows it can learn then you can gradually reintroduce those things,2.0
g84q2o9,j7f1c9,"Yea I will do those things. Also, I have applied the xavier weight initialization as below: 

`def init_weights(m):`  
 `if type(m) == nn.Linear:`  
`torch.nn.init.xavier_uniform(m.weight)`  
`m.bias.data.fill_(0.01)`

`model.apply(init_weights)`

But it doesn't change the accuracy for the validation set. Is it preferable to apply it for the Conv. layers as well?",1.0
g84pfeg,j7f1c9,"Well, batchnorm used for a reason.",2.0
g84qjtu,j7f1c9,"Yea, with Batch Normalization I am getting some decent accuracy. But I want to figure out why the model is getting stuck at 1% when I don't use the Batch Normalization layers.",1.0
g85lzi1,j7f1c9,"I cannot give you a theoretical explanation.

There were a lot of studies about why batchnorm so cool.

In general its fine then your model crush without normalization.

If you want to make it run without batchnorm (for whatever reason), you should try advanced weight initializings, dropout, activation functions, residual connections etc.

But in general big models do not work without normalization.

And sometimes big model do not work with certain types of normalization lol.",1.0
g85pnog,j7f1c9,"Yea got it. I am actually pretty new to Deep Learning, so was checking out how different architectures and functions work. I will be trying them. Thank You.",1.0
g84cnjl,j7f1c9,Why don't you include your code if you want to get an answer? Do you think we are psychics or something so we can tell exactly whats wrong?,3.0
g84d4sr,j7f1c9,Sorry about that. You can look at the code here: [https://colab.research.google.com/drive/1MJ5sBuUeirh1XQTshZi1amw\_j5cZ0syV?usp=sharing](https://colab.research.google.com/drive/1MJ5sBuUeirh1XQTshZi1amw_j5cZ0syV?usp=sharing),2.0
g84diw5,j7f1c9,"No need to apologize, just a suggestion for your future posts.  Hope someone has an answer for this",1.0
g846wsk,j7drox,"You can compare your solution against baselines, such as some naive algorithms or more simple features.",9.0
g84m14y,j7drox,Face the same problem and this is my approach. It's especially valid in deep learning solutions when simpler models or classical ml can be a close rival.,1.0
g84dxr8,j7drox,"If you're the first one to solve a problem then that's your contribution. In general, papers have to have a new/better solution to an existing problem or create a new problem and offer a solution. If yours is the latter then I don't see a need to compare to anything else since that's where your novelty is.",6.0
g84fww6,j7drox,Thanks for your guide!,1.0
g85ltbz,j7drox,I would put my efforts on explaining the problem you tried to solve and why you decided to focus on it. That's what I did when on my paper when I encountered a similar situation https://link.springer.com/chapter/10.1007/978-3-030-13342-9_15,2.0
g83ejhd,j7a1rx,"Does it really have to be a laptop? A workstation would be better for deep learning.

I always refer to Tim's article on this:

https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/

Also, he helps you pick the rest of your system setup based on your goals with Deep Learning.",10.0
g83eph9,j7a1rx,"I am currently a student and I stay in a hostel. So a laptop would be easy to carry and move around. Besides, I would also use the laptop for other work like presentations in college, etc.",2.0
g83ewqc,j7a1rx,"There are also cloud services (some are free), where you can train neural nets with code running in the cloud so you wouldn't be bottlenecked by your system specs.

First have a look at Google Colab, maybe also Kaggle's notebooks. There are videos and I believe tutorials online on how to leverage those shared GPUs and train NN's for free online.",8.0
g83gicl,j7a1rx,"In my experience, a 4GB graphics card is practically useless for Computer Vision using DL. A minimum of 8GB is good. I’d suggest to use Google Colab as it provides 12GB graphics card (sometimes 16GB) for free. That’s good for learning purpose and you can train good big models as well.",5.0
g83quwt,j7a1rx,Public clouds and a shitty Chromebook,3.0
g83f773,j7a1rx,"I recently bought a desktop mainly for personal projects in AI and Machine Learning. I still do most of my deep learning on colab/kaggle notebooks. The reality is - no amount of retail GPUs for students can match the ones available for free on cloud services.

Hell I’d even suggest against buying the 30 series if you wanna do deep learning...

But if youre looking to buy a laptop with a good GPU, you open yourself to faster preprocessing, loading, and maybe even gaming.

Dm if you wanna know more i guess, i built my own pc so i might be able to point u in the right direction

Cheers",3.0
g84nk47,j7a1rx,"4GB cards are practically useless for DL (your OS alone may occupy 1GB VRAM), rather spend the money on the fastest CPU (intel i7 H-series 6 or 8 core) you can get and 32GB of RAM which would allow you to at least debug on the Laptop. Also you don't want an HDD in a laptop when 1TB M.2 SSDs are ~100$",3.0
g83wjz1,j7a1rx,Use colab/kaggle notebooks. I have a 2080Super except I still use colab to train my models most of the times because the free GPUs they provide are far superior to my 2080Super.,1.0
g86xp4a,j7a1rx,"Same!! When did you buy the card? I bought it recently and i think i could’ve waited for the 3080. I regret the decision but then it would’ve been a little more expensive since GPUs cost higher in India (imported, luxury items apparently)

Do you think I’d be good for another 4 years with this card?",2.0
g87vx3q,j7a1rx,I bought it this July and regret it too since the 3070 came out a month later which has better performance for cheaper than the 2080S. I think I’ll wait another 4 years cuz currently it does it’s job well lol,2.0
g83f8vn,j7a1rx,"Checkout Tensorbook, maybe?
[tensorbook](https://lambdalabs.com/deep-learning/laptops/tensorbook)",1.0
g83d1zl,j7941f,Build a model that can analyze thousands of reddit posts and then suggest a random thesis idea.,10.0
g83dg1u,j7941f,Automatic hentai decensoring,6.0
g8k9kpp,j7941f,"&gt; Automatic hentai decensoring
can you plz elaborate a little bit please",1.0
g839fwc,j7941f,"What thesis? Bachelor, Master, PhD?",3.0
g83hnkt,j7941f,sir its master thesis,1.0
g83o720,j7941f,"I'm still learning the basics, so I'm unsure whether it makes sense or not to use deep-learning nor whether it would make a good thesis, but I have a problem that needs solving :). Maybe it will interest you.

For a [personal open-source project of mine](https://openpaper.work/), I have to deal with document scanners. The problem I have is that scanners provide an image, but are unable to tell which part of the image is paper and which is not. I would like to crop the image automatically according to the borders of the page (*not* the text borders ; this problem is already solved and it is not what I'm looking for ; I want to keep the page layout for possible re-printing).

Most scanning tools solves this problem by asking the user ""what is the size of the paper you put in the scanner ? A4, Letter, ... ?"". This is remarkably annoying to me and probably to others. Most of the time, it's A4/Letter, and sometimes .. I just have no idea what it is and I don't care. So I don't want to ask this question to my users. Right now I use a calibration mechanism + quick edition of the page, which is not that much better.

At first sight, it looks like a simple problem that could be solved with usual computer vision algorithms. [I've tried and mostly failed](https://gitlab.gnome.org/World/OpenPaperwork/libpillowfight#scan-border). 

It seems to me that most of my problems stem from the diversity of available scanners: For a same piece of paper, there isn't 2 scanners that will give you the same result (luminosity, contrast, dust on the glass, weird background, etc). There is also a bunch of possible scan resolutions (75 dpi, 300dpi, etc) and modes (Color, B&amp;W, etc).

Or maybe I'm just not good enough with CV algorithms ...

Examples:
* [Epson XP-425](https://openpaper.work/scannerdb/report/271/scanned.png): Lot of dust
* [Fujitsu fi-5110Cdj](https://openpaper.work/scannerdb/report/269/scanned.png): Returns a strangely long image with an annoying ""2 steps"" background.
* [Brother MFC-7360N](https://openpaper.work/scannerdb/report/267/scanned.png): This j***-a** cleans up the image so much that the page borders are hard to find even for an human.
* [Brother MFC-7360N again](https://openpaper.work/scannerdb/report/268/scanned.png)
* [Brother DS-620](https://openpaper.work/scannerdb/report/311/scanned.png): Black background instead of the usual white

Anyway, I was thinking of trying to solve this problem with deep-learning, but right now I'm short on free time ... :/

More examples can be found in [OpenPaper.work scanner database](https://openpaper.work/en/scanner_db/). Unfortunately, I doubt there are enough good samples at the moment to use deep-learning. So I'm currently thinking of improving the website/database and the [tool used to collect the scans](https://openpaper.work/en/scanner_db/#contribute) and then trying another round of crowd-sourcing :)",3.0
g83djkm,j7941f,"Because your question is broad, I'll suggest a broadly applicable topic: 

Interpretability and the methods thereof of Deep Learning models in Natural Language Processing/Language Generation/Image Detection/Object Detection/Image Generation. 

Any broad application context suits just fine.",2.0
g83hr9a,j7941f,"ok sorry for my broad question, i want to do thesis on object detection using convolution neural network (deep learning), can u please give me your precious suggestions",1.0
g83vd72,j7941f,"Well there is my suggestion already, just read the ""title"" and pick Object Detection from the options.",1.0
g8k9ova,j7941f,ok thanks,1.0
g839a57,j7941f,Wot m8,1.0
g83hntf,j7941f,what?,1.0
g83bhfq,j7941f,RemindMe! 24 hours,1.0
g83bi97,j7941f,"I will be messaging you in 1 day on [**2020-10-09 08:43:53 UTC**](http://www.wolframalpha.com/input/?i=2020-10-09%2008:43:53%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/deeplearning/comments/j7941f/thesis_suggestion_in_deep_learning/g83bhfq/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdeeplearning%2Fcomments%2Fj7941f%2Fthesis_suggestion_in_deep_learning%2Fg83bhfq%2F%5D%0A%0ARemindMe%21%202020-10-09%2008%3A43%3A53%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20j7941f)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
g84b29i,j7941f,"Doesn’t your advisor pretty much tell you what to do?

Take the journal articles you published and staple them together.",1.0
g8k9rgh,j7941f,you comment is not helpful,0.0
g8k9wig,j7941f,It was an option for me to take my journal articles that were published and present them as my thesis.  So I didnt have to write a thesis.,1.0
g8k9z6z,j7941f,ohhhh great,0.0
g8346q0,j77790,"Knowing math and python alone is not enough. This day to be a practitioner, you need more than that, and the DL industry moving toward a traditional software engineering.  Practical skill and knowledge in the DL framework are a must.",3.0
g832q5v,j77790,enter at the shallow end,2.0
g83dalp,j77790,I started with Zen and the Art of Motorcycle Maintenance. It’s a basic book on the metaphysics of quality. That’s some deep learning.,1.0
g81yrxy,j6z32g,How exactly did you add this LSTM? What is your base architecture? Adding random components can hurt performance just like it can improve performance.,5.0
g836ob2,j6z32g,"Base architecture in deeplabv3+, this works well on the image level problem. I added a convolutional lstm at a few different positions: after the encoder, after the decoder, etc...",1.0
g837g3e,j6z32g,"Seems to me like you need to train it a lot and the only place where you'd want to put the LSTM is after the backbone. Of course, you want to batch norm or layer norm after that output, you want to make sure the LSTM is big enough and that it has 2+ layers.

As I said you'll probably need to train it for way longer and you'll potentially need more data.",1.0
g83bxig,j6z32g,"By 2+ layers, you mean stacked lstms?
Any reason why training it end to end should decrease performance? 
After the backbone you mean encoder. If so, there was actually a [paper](https://arxiv.org/pdf/1905.01058.pdf) showing it was better after the decoder. Im taking those results with a pinch of salt, but I don't think it's obvious to do either.",1.0
g83d4jz,j6z32g,"&gt; By 2+ layers, you mean stacked lstms?

Yes, not sequentially.

&gt; Any reason why training it end to end should decrease performance? 

Well if you think of layers as messengers, every layer delivers messages in a different way. We use convolutions for images because they optimally deliver messages from an image to some representation. If the LSTM delivers messages suboptimally for a given task then it will surely screw up the performance. If it is large enough then it probably won't but since you need it to be overparametrized, you have to give it enough data so it learns the noise from it as well, and this usually means that you have to augment your data further or get a larger dataset.

&gt; If so, there was actually a paper showing it was better after the decoder. Im taking those results with a pinch of salt, but I don't think it's obvious to do either.

Think of it like this - why do you need an LSTM? Your LSTM basically only converts a sequence to a single element. It's sort of a summarizer. So where you place it depends on what each component does. In this case, I'd never put it after the decoder because we don't trust the rest of the network more than the backbone. The backbone is usually very well trained and if issues arise then you know it's the LSTM. If you put the LSTM after the decoder, then you don't know if it's the LSTM or the decoder. Not only that, the decoder output probably has more entropy than the backbone output, making training that LSTM harder. Generally though, even though it's logical to me that putting it after the decoder would give you better performance, as the decoder output is less generalized when compared to the backbone output, theoretically speaking it is suboptimal because you're not generalizing well, you're just using tricks to overfit on the output. You want your LSTM to give you a summary of what's happening based on the thing you've seen, not a summary of what's happened based on the things you've concluded. Sometimes the conclusion will give you better performance because they contain more information than the original information. But if it's that severe, you'd probably consider looking at a better backbone because it means that the rest of your network is likely hallucinating some of the input, which could be hard to fix once it starts hallucinating incorrectly. If your LSTM overfits on the backbone output, then you just have to increase the size and train it longer to counter double descent, or cut the number of parameters/layers so you don't overfit.

**tl;dr** putting the LSTM after the backbone should generalize better and be easier to debug",1.0
g82bc22,j6z32g,LSTMs typically need tons of training data to perform well. May be your dataset is just too small,2.0
g81zqbs,j6z32g,What’s mloU?,1.0
g81zy6b,j6z32g,Maybe IoU?,1.0
g836pbx,j6z32g,Mean intersection over union of the ground truth masks and the predicted masks.,1.0
g80srk5,j6tdlw,Nice! Do you include results from arxiv-sanity.com?,2.0
g815pkj,j6tdlw,&gt; arxiv-sanity.com thanks I had forgotten about this site :) No I don't I monitor various forums and social media channels for papers that are being talked about and who's sharing and talking about a paper helps with calculating a score for the paper.  https://42papers.com/about,2.0
g8274im,j6tdlw,"Signing up is the best way to stay updated I have an interesting roadmap with features like audio channels for collaborative paper reading or tools for authors to give talks on their papers, code sharing and a bunch more.",2.0
g8362dx,j6tdlw,What a wonderful idea! Will share this on other platforms too :),2.0
g83zxqp,j6tdlw,Thanks I appreciate your comment. It's been a labour of love,2.0
g8bds2y,j6tdlw,Great resource. Added to [https://github.com/BAILOOL/DoYouEvenLearn](https://github.com/BAILOOL/DoYouEvenLearn). Thank you for sharing!,2.0
g7zpzf1,j6lkrd,link to further info?,1.0
g81rhay,j6lkrd,GitHub link is in the post,1.0
g81dzdo,j6lkrd,[Link](https://github.com/NVlabs/imaginaire) to the repo,1.0
g7z6fxy,j6j9zc,"At our company, we are planning to open source CRAL. (cral.segmind.com). DM me if you are interested.",2.0
g7z6gso,j6j9zc,"**I found links in your comment that were not hyperlinked:**

* [cral.segmind.com](https://cral.segmind.com)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g7z6fxy&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",2.0
g7zt4fm,j6j9zc,I sent you a DM. Thank you.,1.0
g7yciqa,j6g2cj,What’s the resolution? The complexity of convolution scales quadratically here.,5.0
g7yal9m,j6g2cj,"If not doing it already,  try to use GPU.
CNNs use to require heavy training time.",6.0
g7yhr1g,j6g2cj,"If you're writing in Python, run it on Google CoLab for the free GPU.",4.0
g7yjpu6,j6g2cj,Yeah I’m in colab wasnt using a GPU,1.0
g7zdtkm,j6g2cj,I need resources on learning to use GPU for training the models. Any resource would be appreciated.,1.0
g80jvwn,j6g2cj,"If you have a GPU, a deep learning library that can use GPU (ie Tensorflow GPU distribution), and you have installed all of the required graphics drivers and dependencies (ie CUDA, cudnn, etc), then the training function of a neural network using that DL library will automatically use the GPU.

If you're trying to use the GPU for some other task not provided by the library, that's more intricate.",2.0
g7zbzka,j6g2cj,Checking timestamps in the pipeline helps. I had a problem where getting the next batch from generator took longer than the forward and backward pass.,2.0
g7zlz9t,j6g2cj,"Check out [https://neuro-ai.co.uk](https://neuro-ai.co.uk) free £100 credits on signup, and works with Colab",2.0
g7yeifr,j6g2cj,"* Make sure your batch sizes are as large as possible, given your GPU memory.
* Are you using data augmentation? If so, consider saving out pre-augmented training data, trading storage space for reducing the bottleneck that CPU-based augmentation creates.",1.0
g7ylfl5,j6g2cj,How long is it taking?,1.0
g7yn2nw,j6g2cj,I’m on epoch 40/80 and it’s been 5 hours....,1.0
g7ynlqe,j6g2cj,"Any reason you chose to do 80 epochs? Are you sure your model is still improving significantly after 40 epochs? You may want to consider some form of early stopping. This won't help with the time per epoch, of course, but it will reduce the total amount of time you spend training.",4.0
g7z4yk0,j6g2cj,I’ll try this,1.0
g7x2w0g,j68320,/r/learnpython,3.0
g7xpwp2,j68320,"Coding is really not very hard. Most of the best practioners taught themselves, and learned any theory later to hone their skills or just out of interest. I have yet to meet a great coder who thinks their education path was key, most curriculums are way behind the curve.

Just do it. You don't need to be a great coder to do interesting things in DL. You just have to keep at it.",3.0
g7x3qof,j68320,"Could  try [fastai](https://course.fast.ai),  a free course that comes with a free book, no sign up, or STEM PhD  required.",2.0
g7yopem,j68320,"You can start off with Deep Learning Specialization. A lot of people who are now working in this domain have used this as their starting point. You get a good fundamental intuition out of this course. And as per python, it is not very hard. You can try YouTubing it. There are several free 6+ hour contents on it. I would really recommend that you finish some of these free python courses from YouTube and go ahead with andrew’s specialization on coursera. Once you are done with the specialisation, you will be equipped with a good grasp of fundementals, which is feel is very necessary for anyone who’s trying to get into this field.",1.0
g7ww78a,j67r95,"Not overfitting.

To me it seems like the validation set is much easier that the training set. Could be that they come from very different sources, could be some kind of imbalance. Not necessarily a problem if the validation set is truly representative of the use case.",35.0
g7xj2pr,j67r95,"Many loss functions (unspecified here?) are a function of the quality of the solution \_and\_ a function of the prevalence of, say, the positive class. As u/trexdoor calls out, if the validation set differs in distribution from the training set, then the distribution of loss will likely differ as well. Binary crossentropy loss definitely has this property.

If you think the training and validation data should come from the same distribution, and that training is representative of the validation problem, then you have a bug somewhere.

All that being said, it's not strictly required to train on data from the same distribution as the validation or test sets. You can train on random noise if it gets you desirable performance on the test set and ""in the wild.""

If you're ok with the mismatched distributions, you can do a few things to validate your model is ok:

* train a simpler model that must have worse validation error, and confirm that it does
* train a null model whose loss should converge to the expected loss for that validation data set (eg cross\_entropy\_loss = - (p \* numpy.log(p) + (1.0 - p) \* numpy.log(1.0 - p)) where p is the prevalence of the positive class for a binary classification problem iirc)
* watch the shape over the course of training (pointed out by other comments)... if validation loss starts to rise you are definitely overfitting
* probe the model with more difficult metrics... for example, compute a precision-recall curve on the validation set or a test set; compare models by the full shape of the curve, rather than single point summaries; watch for weird kinks in the curve that are likely exploitation of some weak feature of the test set that won't generalize
* try some qualitative tests IRL... does the model make predictions you are proud of? \[harder than you think\]. Any head scratchers?",7.0
g7xn7vr,j67r95,Also it may happen because things like dropout turned off on validation.,5.0
g7y8ifg,j67r95,Or a regularization penalty during training that’s off for validation,3.0
g7z1xpv,j67r95,"https://github.com/wirelesshydra/Text-Generator/blob/main/LSTM_1.ipynb

This is the GitHub link it will give you a better idea. The training and validation are from the same source i have split the data into training - 67% and validation - 33%",1.0
g7zbxu6,j67r95,"In cell 12, before splitting the data into train and val, shuffle it (i.e. shuffle variables X and y, and make sure their mappings are preserved as well). This will ensure the train and val are coming from the same distribution.

EDIT: In fact, looking at a sample of the text it looks like it starts at “Chapter 1” and is coming from some book. You can imagine that the last third of chapter 1 could contain easier sequences than the first 2/3’s (e.g. more simple word usage due to resolution of plot or something). This will result in a shift of distribution from training to validation.",2.0
g7z1ygs,j67r95,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/wirelesshydra/Text-Generator/blob/main/LSTM_1.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/wirelesshydra/Text-Generator/main?filepath=LSTM_1.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1.0
g7wwwmb,j67r95,"You're not accidentally training on the validation set, are you?",19.0
g7wovmt,j67r95,"It doesn't mean overfitting, no. I'd be overfitting if the validation curve went upwards at some point. I'd continue training until you validation curve flattens - this means it can't be improved anymore and stop before it shows any upwards trajectory.

Edit: I just noticed that the loss of your validation loss is lower than the training set - did you not switch them accidentally? If not then something doesn't seem right",8.0
g7wnzzj,j67r95,My guess is you have leakage between your test and training sets. So it's simultaneously overfitting to both.,11.0
g87uviz,j67r95,"He doesn't have a test set, only a validation set.",1.0
g7wo0zx,j67r95,Are you sure you have the right legends on the graph?,3.0
g7wo556,j67r95,Yeah I'm sure about it.,1.0
g7woo6d,j67r95,"Are you sure the validation set and the training set are completely separate? It shouldn't really be possible to have consistently lower validation loss than training loss (if they are normalized, in absolute loss units you could though).",7.0
g7xqc87,j67r95,"Yeah usually it's the exact opposite.. loss on validation slightly higher. If it were reversed then yes this would be evidence of over fitting. In it's current state this just doesn't make much sense, and indicates the validation and training data must be fairly different distributions (with the validation data being a small subset or something)",1.0
g7xd1xx,j67r95,"Are you calculating mean instead of sum?
Are the batch size same for bith train and val set?

Just some basic sanity check",3.0
g7x7eiv,j67r95,"For these kinds of questions, OP should really describe how they have set up the training and validation data samples and how they were divided. This is key because the validation data should ideally be representative of the distribution of data that the network is trying to generalize for, but is distinct from the training data itself. People often seem to mess this part up and, for instance, only select a subset that is representative of a small range of examples or sometimes allow the network to see the validation data during the training phase.",2.0
g7z2zlx,j67r95,"https://github.com/wirelesshydra/Text-Generator/blob/main/LSTM_1.ipynb

This is the GitHub link i have commented the code for better understanding please go through it and correct me if possible.",1.0
g8053i4,j67r95,"Yeah, you're passing the entire labeled data to model.fit(), which contains both your training and validation data. It should be model.fit(X\_train, y\_train,  batch\_size=128,epochs=100,validation\_data=(X\_test,y\_test)). Your model is training on all the data, including your validation data. So, if it is overfitting, you wouldn't know because you're validating against data it has already fitted.

Also, the best practice is to have three entirely separate sets of labeled data: training, validation, and testing. You fit the model on the training data, use the validation data to monitor for overfitting, and finally measure the accuracy of the model against the testing data as the ultimate test to see if your model is generalizing. Testing and validation are best separate because by using the parameters that best fit the validation data we are biasing the model. We don't know how much that biasing affects the model's ability to generalize until we test it against data it has never seen before.",2.0
g7z3097,j67r95,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/wirelesshydra/Text-Generator/blob/main/LSTM_1.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/wirelesshydra/Text-Generator/main?filepath=LSTM_1.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1.0
g7x853m,j67r95,This seems like underfitting to me. Train it for more epochs. Overfitting would happen when the training curve continues to decrease but the validation curve starts increasing.,2.0
g7y22ds,j67r95,"Not to be a stickler, but underfitting would mean you stopped training prior to covergence, overfitting is when you train too long that your covergence point has already happened and your actually memoizing things.

In some conditions overfitting isn't all that bad as long as the real loss is ok.",1.0
g7xfjmh,j67r95,Do u have dropout layers in your architecture?? Because i have seen similar loss patterns when adding dropout layers.,2.0
g7xhn95,j67r95,Looks good to me.. try upping your validation set size,2.0
g7y0mt8,j67r95,"Seems to me like a data leak, where the validation set is on average easier than the training set.",2.0
g7x91gv,j67r95,Are you using keras? Because I remember something similar happening to me. It's the way loss is calculated that makes it more while training and less while evaluating on the validation set.,3.0
g7x9xdi,j67r95,"Agreed, I've seen this behavior before in keras.",1.0
g7yzn4f,j67r95,Yeah im using keras.,1.0
g7wzmx3,j67r95,"Not overfitting, but convergence is suboptimal. I would first use a very small amount of data and try to get the model to overfit (test on the same data as well) as quickly as possible by tweaking the hyperparams.",1.0
g7xe0dn,j67r95,There's not enough information to say for sure.  What do your data setup functions look like?  Are you scaling the whole dataset before splitting into train and test?,1.0
g7xhwbf,j67r95,How have you chosen the validation set? It should be a random selection.,1.0
g7xj64c,j67r95,Could you describe it in a detailed manner ?,1.0
g7xpc39,j67r95,"As others have noted before, if you are using dropout layers in your architecture, that could explain this behavior.

And here is a short explanation why:
Dropout ist a regularization that for each train step randomly ""deactivates"" neurons, i.e. sets their activations to zero with the probability you provide during configuration. The reason to use this kind of regularization lies in preventing that only a subset of neurons in a layer learn relevant information and the subsequent layer just relies on these more effective neurons to give the correct output.

As dropout is only active during training, during evaluation all neurons are available, which in the ideal case means that more useful neurons contribute to the output.
This in turn, can lead to the evaluation problem being easier than the training problem which manifest as the evaluation loss being better than the training loss.",1.0
g7zda6k,j67r95,"In case of overfitting, the training loss decreases and the validation loss keeps increasing. So no.",1.0
g7zeja4,j67r95,"First and most obvious thing to check (as other comments have said) would be to check if the test and validation sets are coming from different distributions. I’d probably reshuffle the data and rerun the training just to be extra sure.

Second thing is look for is if you overfit your hyper parameter tuning to your validation set. The way I’d recommend to check this would be instead of splitting your data 67/33, do 60/20/20 (or anything in that ballpark) and have a train set, dev set, and test set. Then build your model to perform well on your train and dev set, and reserve your test set for situations like this so you can see if that’s truly how it performs on now data or if your hyper parameters are just optimized to do well on your validation set.

Hope this helps!",1.0
g7wo0zq,j67r95,"Is this your first time in the field of DL?   


If it were overfitting, validation loss would be going up instead.",-6.0
g7x05iv,j66m30,Does this depend on the random seed? Did you tweak other hyperparameters? How does normal sgd fare? - these are some of the questions I would ask myself in similar situations.,1.0
g7x19wc,j66m30,"Adam if you're just deciding between the two.

Also try SGD with added momentum if you have the option.",1.0
g7xb8y0,j66m30,You can also try out Adagrad,1.0
g7w7zup,j65h41,Here is the link to the original paper: [https://ondrejtexler.github.io/patch-based\_training/](https://ondrejtexler.github.io/patch-based_training/),1.0
g7veceo,j5va2k,"I think the most direct route with a BS is to go for a SWE role that touches on deploying ML models. For example, Google doesn't have ML engineers, they have SWEs for that job.",3.0
g7us0d6,j5va2k,What kinds of jobs are you going for? Do your projects include an application of computer vision or a demonstrable knowledge of robotics?,2.0
g7utixi,j5va2k,"I usually apply to ML jobs that have a deep learning focus, but there are very few where I live so I suspect it is very competitive. I have one project that is an image classification of foliar diseases, not sure if that counts as computer vision though. I'm really interested in content generation with GANs right now and thinking of project ideas to bring to life rather than just slapping a model on a kaggle data set, as I'm taking the [deeplearning.ai](https://deeplearning.ai) course in them now.",1.0
g7vzcue,j5va2k,Yes of course. Look for applied research roles and come with a solid resume of project work to show you have real skills and not just academic knowledge. There are dozens of shops in each state that need people who have a basic understanding of models to implement SOTA for their projects but dont necessarily want to fork out the money for a PhD.,2.0
g7wexk6,j5va2k,Are there remote jobs? Do you know where the postings are?,1.0
g7ts170,j5n787,I feel personally attacked by this post,16.0
g7vddcb,j5n787,Are you a GAN?,7.0
g7u83zi,j5n787,This cuts deep,7.0
g7vtjqw,j5n787,* this sucks,-1.0
g7vgbpe,j5n787,r/croppingishard,4.0
g7vpxky,j5n787,"not only gans my friend, even NLP is plagued by this bullshit",4.0
g7u9ou5,j5n787,"Two possible reasons:
1) Difficulties in optimization: Sometimes, you cannot obtain the similar performance with authors
2) Naturally, people select the best images for the paper",8.0
g7wo7d7,j5n787,[deleted],3.0
g8b02lg,j5n787,"This is what turned me off peer review, my brother (whom is doing his Phd in geomatic prediction models) and I believe that a paper is now only as good as its reproducibility.",1.0
g7t7go4,j5n787,"TecoGAN, anyone?",2.0
g7v9pbo,j5n787,What about it (besides being pain in the ass to train)?,2.0
g7vbx44,j5n787,I used the same images that were shown in the readme and it did not produce anything close to the output. It should have performed excellently given that the images were either presumably included in the sample set or at least a high performing test set (and thus I shouldn't have to perform any additional training). It seemed like false advertisement.,2.0
g7u55lh,j5n787,I wish I didn't know what you were talking about.,2.0
g7v757h,j5n787,This is so true. I worked on the celebrity face generation using GANs and my output looks nowhere near as good as the paper (or other video demos on YouTube).,2.0
g7vbehn,j5n787,😂😂,2.0
g7uxnhr,j5n787,I'm sorry but I have to downvote you cause that crop gave me cancer.,-2.0
g7wq8ie,j5lwwr,"Try Faceswap

https://youtu.be/1leXXHfaOFw",1.0
g7tm6d6,j5imvv,Why didn't they include the GPU times?,0.0
g7rpwz0,j5deli,"CNN is position relational without the need of feature engineering. Or more accurately put in KNN you look for more of the same thing and how it relates to what is around it for a radius, in a CNN you just look for what is in a radius and update a weight.",4.0
g7ruuve,j5deli,"Neural Networks will always outperform heuristic approaches to machine learning due to their having fewer inductive biases, and a vastly increased ability to fit any arbitrary function.

CNNs are NNs which have a specific bias introduced which makes it easy for them to learn spacial patterns. Despite this bias, they are still less biased than older methods, and generally have many more fittable parameters.

For interests sake, the level and type of bias, the amount of data required and the final performance are very related in ML.
Generally, having more inductive biases (eg manual feature detectors) will reduce the amount of data required for training as these biases give the model a 'warm start' but eventually reduce the performance. Every helping hand you give your model eventually becomes a limitation of the model.

Consider a chess engine, in the past we gave them lists of rules and ways to decide which moves were best. This helped them perform well under limited resources. But when these resources became more plentiful, the better solution was to give the model nothing, and have it learn everything on its own. This in essence is why CNNs will always outperform older methods. They are given less, and so can learn their own rules in a more nuanced way than we could ever teach them explicitly.",5.0
g7t4kl1,j5deli,"It's a great comment about hard-coded domain knowledge bias (shift invariance and local information gathering) and amount of training data required. I think it explains pretty much everything.

Today I found a youtube video ([https://www.youtube.com/watch?v=TrdevFK\_am4&amp;list=LLYMrhENc4uSwsDCsqkgBGzw&amp;index=1](https://www.youtube.com/watch?v=TrdevFK_am4&amp;list=LLYMrhENc4uSwsDCsqkgBGzw&amp;index=1)) that talks about transformers applications in computer vision. Considering transformers to be somewhat of a generalization of a feed forward NN (with input-dependent weights) it describes the impact of having domain bias on model's performance. (starts at 21:00) 

P.S. There are cases of combining CNNs and k-NN. For example, standard approach to face recognition task is training a neural net to create an embedding and then running k-NN over that embedded space. So every algorithm has its niche.",1.0
g7tj36w,j5deli,"Ooh thank you. That sounds interesting. I'm currently doing my thesis in relation to Transformers in NLP, so I'll definitely give it a watch.",1.0
g7rq34o,j5deli,"I'm just a student and I'm not sure if I'm right. But I think K nearest neighbours is the simplest classification algorithm because you're just finding the closest neighbour of a point, whereas a CNN is a neural network which can detect features from images like edges and more complex ones like shapes.",1.0
g7s2600,j5deli,"
KNNs take more time to predict than CNNs 
It has to run through all of the dataset and manually search for differences and similarities, takes O(n) time to predict for a dataset with n images.
KNNs don't generalise as well as CNNs
KNNs localise features and only remember features at a particular spot, while CNNs can generalize it to any part of an image.",1.0
g7rhdpu,j5dahl,"np.random.randint(0, high = 50)",4.0
g7rom3k,j5dahl,"I want to say /sarcasm here, but this is exactly the method automl uses to determine feasibility. 

My general rule of thumb is take your output dimension and square it",3.0
g7sipwn,j5dahl,"This falls under ""hyperparameter tuning"", which means empirically by maximizing performance on a held out validation set",3.0
g7rpmk8,j5adzo,"I own the jetson nano. It was a fun toy, but I find it's value to be in limited edge applications. 

Basically, do you have an interesting idea to port to an iot device? If not no.",2.0
g7pygun,j55g1l,"A few years ago I started to focus on computer vision and collect some material. The link below is most complete for me. (Obviously, also on the internet there are a lot of useful material)A few years ago I started to focus on computer vision and collect some material. The links below bring together a lot of material (Obviously,  on other internet resources there is a lot of useful material too).  


[https://github.com/llSourcell/Learn\_Computer\_Vision](https://github.com/llSourcell/Learn_Computer_Vision)  
[https://github.com/toniesteves/DeepLearning.ai-Summary](https://github.com/toniesteves/DeepLearning.ai-Summary)  


I hope it helps.",1.0
g7pznqq,j55g1l,"Thank you
I'll look into it",1.0
g7q0lhm,j549ut,Looks fake to me,18.0
g7py8h9,j549ut,Can you share some details !!! I am a rookie.,2.0
g7q1ho0,j549ut,Where is the AI in this,4.0
g7qciqy,j549ut,Learning algorithm needed to capture card value and suit.,6.0
g7qcngt,j549ut,Ok that's fair,1.0
g7q02p4,j549ut,Wrong sub that is probably theory,0.0
g7qb6ju,j549ut,So useless. It's just a vision algorithm with an odds calculator,0.0
g7pj6yk,j53s7v,The best route is to use NGC docker containers.,3.0
g7pl1ch,j53s7v,"Thanks, I'll give that a try",1.0
g7pk7z7,j53s7v,NGC docker images works the best especially for new hardware and/or CUDA version. Otherwise images on DockerHub work as well.,1.0
g7pl27t,j53s7v,Ok thank you,0.0
g7q1loc,j53s7v,"I just install all versions of cuda, cudnn, nccl, tensorrt and symlink as needed (symlinks are in PATH)...

 \# ls -ltra

    TensorRT-6.0.1.5
    TensorRT-7.0.0.11
    nccl_2.5.6-2+cuda10.2_x86_64
    nccl_2.6.4-1+cuda10.1_x86_64
    cudnn-10.2-linux-x64-v7.6.5.32
    cudnn-10.1-linux-x64-v7.6.4.38
    cuda-11.0
    cuda-10.1
    tensorrt -&gt; /opt/TensorRT-6.0.1.5/
    cudnn -&gt; /opt/cudnn-10.2-linux-x64-v7.6.5.32
    nccl -&gt; /opt/nccl_2.5.6-2+cuda10.2_x86_64/
    cuda -&gt; /usr/local/cuda-10.2",1.0
g7poagk,j52awj,"I'm sorry to say that we don't understand adversarial attacks that much to enable us to synthesize an attack based on a known weakness.   


Also, currently, you need access to the model in order to create the attack - so you have to luck out with the used model  


Hope that does not kill your creativity.  


PS: Because most image classifiers/detectors are convolutional in the first layers, it probably won't matter where to put them. Size is a crucial factor though.",2.0
g7pr58d,j52awj,sad. I was thinking of this problem similary to a region proposal problem with alrwady annotated bounding box datasets( stickers) . But if thats not possible yet then meh:(( . Do you know any paper that explains the limits of adversarial attacks a little bit deeper perhaps? thanks for the answer tho,1.0
g7roxm3,j4ykk7,Check with telnet to see if the port is open.What WSGI server are you using? Try changing the timeout there,2.0
g7s9jo7,j4ykk7,"Hi thanks so much for replying!

I solved the error a few hours ago. The issue was that AWS by default assigns a t2.micro instance, which has a really small CPU. When looking through the logs I realized it was failing because of memory allocation issues, changing the instance to t2.small worked :)",2.0
g7o185g,j4xnxj,"If you have any questions shoot I'll try to answer all of them!

I personally have a high-end (RTX 2080) laptop and I use Azure ML and Spell as I have 150$ of free, recurring credit on Azure as a Microsoft employee and I got some 300$ of free credit from Spell for doing a tutorial for them.  

I started with a cheap laptop and Google Colab as most of us do.",1.0
g7ooyk3,j4wad3,Can you share your learning curve?,1.0
g7ri1b1,j4wad3,"Sure, I'm planning to release a blog post for the same",1.0
g7ofn04,j4w2p1,"Shameless plug here but also very relevant: join the [r/GeometricDeepLearning](https://www.reddit.com/r/GeometricDeepLearning/) subreddit for posts in graph representation learning, 3D learning, etc.

If you are comfortable with conventional deep learning 3D deep learning should be relatively simple to pick up. Survey papers are always a great place to scope out a topic you're keen on diving into:

[Deep Learning for 3D Point Clouds: A Survey](https://arxiv.org/abs/1912.12033)

[A survey on Deep Learning Advances on Different 3D Data Representations](https://arxiv.org/pdf/1808.01462.pdf)",2.0
g7oo3zr,j4w2p1,"I'd recommend you implement a simple DeepSDF model. Take a look on original paper and write your own version. It only uses MLP and skip connections, so it's not hard to do, and does not involve graph theory. Don't push it too hard and do not train it on large families - it takes too much resources. It also includes some hidden details: how to visualize the result? How does ""Marching cubes"" work? What are the assumptions on the dataset?

Once you've done that, you can understand better the topic of neural rendering, novel view synthesis.

Also, look at the tutorials and workshops from conferences. Good news is they are mostly online on YouTube! For example CVPR 2020:
https://sites.google.com/view/geometry-learning-foundation/schedule?authuser=0",2.0
g7oq749,j4w2p1,"* [PointNet](https://stanford.edu/~rqi/frustum-pointnets/)
* [3D Bounding Box Estimation for Autonomous Driving](https://github.com/lzccccc/3d-bounding-box-estimation-for-autonomous-driving)
* [Multiple View Geometry in Computer Vision](https://www.youtube.com/playlist?list=PLyH-5mHPFffFvCCZcbdWXAb_cTy4ZG3Dj)",2.0
g7odju8,j4v9rh,"Recent iPads and iPhones contain extremely powerful neural engines that are capable of running inference faster than many PCs. For a demo see iDetection, which runs YOLOv5 at 30 FPS on recent iOS devices.

[https://apps.apple.com/us/app/idetection/id1452689527](https://apps.apple.com/us/app/idetection/id1452689527)",1.0
g7n2zxx,j4ulw3,"I have the same issue.Currently i am doing some tutorials on YouTube for Pytorch.
Here,you can refer this,
1)[Youtube Tutorial  1](https://www.youtube.com/playlist?list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz)
2)[YouTube Tutorial 2](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4)

Also there is entire deep learning course by Alfredo Canziani which is implemented in Pytorch,so you can also check that out.
[Alfredo Canziani Pytorch] (https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq)",10.0
g7n460n,j4ulw3,"You can checkout the official PyTorch tutorials I believe it's the best resource to learn Pytorch. Link to the tutorials:-

https://pytorch.org/tutorials/",8.0
g7o7oe4,j4ulw3,"Yes exactly, people are making so much money just by reading official documentations on their youtube channels",4.0
g7nf1vn,j4ulw3,"A friend of mine suggested https://www.fast.ai/ for PyTorch, I haven't checked it out myself so can't vouch for it anything.",8.0
g7nhzd9,j4ulw3,Checkout Udacity course of intro to deep learning using pytorch. It's great.,7.0
g7nhkuu,j4ulw3,"You should try fast.ai I don't think there is a better course for deep learning with pytorch. 

In part 1 of the course you will be taught how to do vision, nlp, collaborative filtering, time series etc using fastai library. Fastai is an open source library built upon pytorch.

In part 2 of the course they teach you how to build important parts of the library using pytorch.

All the models and optimisations are SOTA at the time the lecture was taught. A very practical approach to deep learning. Please do try it out.",5.0
g7nhm0j,j4ulw3,"**I found links in your comment that were not hyperlinked:**

* [fast.ai](https://fast.ai)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g7nhkuu&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",2.0
g7o1zn1,j4ulw3,"There is a free ebook version of ""deep learning with pytorch"" (manning pub) available through their official website:

https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf",6.0
g7o6452,j4ulw3,"This is the best one I found, from the creator of Pytorch it'self (it's free)

[https://www.udacity.com/course/deep-learning-pytorch--ud188](https://www.udacity.com/course/deep-learning-pytorch--ud188)",5.0
g7ro331,j4ulw3,I finished the Deep Learning Nanodegree related to this intro course during the COVID-19 promotion and I was very satisfied with it. [Here](https://github.com/udacity/deep-learning-v2-pytorch) are the materials for the lectures and projects.,2.0
g7nemmx,j4ulw3,"I enjoyed this course:

 [https://www.udemy.com/course/actor-critic-methods-from-paper-to-code-with-pytorch](https://www.udemy.com/course/actor-critic-methods-from-paper-to-code-with-pytorch)

The knowledge of the algorithms is also directly applicable to real world problems I would argue. At least it helped me to understand actor critics and how use pytorch.",4.0
g7ng9h2,j4ulw3,"https://www.youtube.com/c/deeplizard

I strongly recommend their pytorch series, worth taking a look at.",3.0
g7nmjqy,j4ulw3,I swear i suffer from the same problem!! Do let me know if you find any good lead,1.0
g7v8xlh,j4ulw3,"I preferred Deep Learning for coders with fastai &amp; pyTorch awesome book, try that",1.0
g7ktam5,j4kuun,"In my experience, I have found openvino to be very annoying wrt installations and the like.",2.0
g7j3hwh,j4gwnf,"


I personally have had challenge understanding how to converge a GAN properly for a long time. There are some things that are not commonly stated: things that have a wide breadth of orientation ( like how one places their hand on something ) will not generally perform well, faces are easy because we as humans do not see flaws as well as we think, 

Things that I personally value: Wasserstein Loss, Augmentation, and batch sizes that represent a good scope.

I recommend downloading a bunch of images and also the github project for StyleGAN2-Tensorflow-2. 0.

Start there",3.0
g7j3pde,j4gwnf,Thanks mate ill start there .,1.0
g7k73fo,j4f7da,"Looks pretty good, however as a person who edited a lot of underwater pics, I must say that the same result can be achieved by simply doing an auto white balance in photoshop or gimp. It doesn't work that well on darker images, but then it's mostly the problem of the photo taker. 

Which brings me to this: just use filters. They are cheap, if you can afford a camera with underwater housing, you certainly can get a set of filters from ebay. They fix these problems and preserve the purity of the original. Professionals certainly use filters and proper lighting already, and if you are an amateur facing this problem - USE FILTERS.

Also, does this actually use deep learning? It shouldn't require any machine learning, as there is a precise formula for colour change.",8.0
g7ka0zm,j4f7da,"""We showed the model a bunch of before and after pictures and trained it to use the white balance slider.""",5.0
g7myfum,j4f7da,"Well, did you actually watch the video? Or look at the paper?",1.0
g7npicm,j4f7da,"Yep, and I still don't understand where deep learning comes into play. They say they use exact colour shift depending on distance, and then ""we use machine learning"" without any further explanation, so it sounds like they wouldn't need anything more than a regression. I don't want to bash the research, I just don't see how it's related to deep learning...",2.0
g7nyx2f,j4f7da,"The point kind of was that when you look at the actual paper, you'll see you are indeed correct, and no deep learning was used, nor did they claim that (though they do suggest that using it to calculate certain parameters might end up working better than their estimation method). It's very interesting stuff regardless though! And definitely more than just a simple colour shift using a filter on Photoshop.",1.0
g7rhglz,j4f7da,"Ah, well, I'm just confused why it's in deeplearning subreddit then. There isn't even machine learning as far as I can tell (unless you count regression). I don't discount the value of this software, it's just not AI-related at all.",1.0
g7rop99,j4f7da,"Yeah I agree with that. I think at this point some people see ""computer vision"" and assume it must be deep learning...",2.0
g7igpey,j4f7da,The paper covered here: [https://openaccess.thecvf.com/content\_CVPR\_2019/papers/Akkaynak\_Sea-Thru\_A\_Method\_for\_Removing\_Water\_From\_Underwater\_Images\_CVPR\_2019\_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2019/papers/Akkaynak_Sea-Thru_A_Method_for_Removing_Water_From_Underwater_Images_CVPR_2019_paper.pdf),4.0
g7nqsys,j4f7da,But then it is not underwater anymore,2.0
g7ntxf2,j4f7da,Indeed!,1.0
g7thb1k,j4f7da,"Added the channel to the list: [https://github.com/BAILOOL/DoYouEvenLearn](https://github.com/BAILOOL/DoYouEvenLearn)

Keep up the good work.",2.0
g7tyz9b,j4f7da,"Thank you very much my friend, I will do my best!",2.0
g7jf7ty,j4f7da,"While it seems an interesting method with good results, there's essentially no justification for why one would need/want this aside from ""potential discoveries"". I wonder if the method is applicable to other domains?",0.0
g7kpn22,j4f7da,https://robonation.org/programs/robosub/,1.0
g7ih429,j4ei3j,"There's nothing special about this, these are what are called residual blocks. You take the output of the first subnetwork and incorporate it in the second subnetwork input further down. Now, because the output of the first network is a fixed size tensor, you probably don't want to use it as an input to the LSTM (or you should use it as the initial hidden state, doubt this would be stable, though). Also, you probably want to use the logits of your first subnetwork rather than probabilities. What I would do is illustrated by this:

Input -&gt; Subnet 1 -&gt; Logits 1 -&gt; Probs 1 (this is after softmax or sigmoid)

Input -&gt; LSTM 2 -&gt; Logits 2

Then what you'd do for your second subnet is this, the ""|"" operator being concatenation:

Logits 2 | Logits 1 -&gt; The rest of Subnet 2 (dense layers? convolutions?) -&gt; Final logits

It's fairly common to do this kind of stuff but not exactly for the reason you'd use it (ensemble). Keep in mind that by doing this the outcome of your second subnetwork will train your first subnetwork because its result depends on the first subnetwork. If you don't want that (it's not exactly stable), you should probably make sure that it doesn't propagate to the first subnetwork by detaching the Result 1 tensor, or freezing a part of the graph, or using 2 optimizers that optimize different groups of parameters (ex. one that optimizes the first subnetwork, other that optimizes the second). Two optimizers is probably how I'd do it because Adam will have an easier time learning its parameters separately, and those two networks might be vastly different at the distribution level.

The reason why you wouldn't want to use the output of your first subnetwork as an input of your 2nd LSTM is because it's likely not going to belong to the same distribution as your input or Result 2 and you will increase your entropy, leading to information loss and poor performance. You could of course try to counter this by trying to learn how to interpolate that output to match the distribution of the Input or Logits 2, however that's what ""The rest of Subnet 2"" will do anyways in a more elegant fashion, and you're likely to use LayerNorm or BatchNorm after concatenation to improve flow if that alone doesn't do a good job.",4.0
g7inyk4,j4ei3j,"Thank you, that's a very thorough explanation, do you suggest any books / resources to look at for neural network architectures / design?",1.0
g7iomjr,j4ei3j,"Don't think there's anything better than Goodfellow's 2016 deep learning book. After that it all comes down to reading the most important and latest papers

EDIT: If you really put your back into it, perhaps you can catch up from 2016 to present in a year, so don't be discouraged if you feel like a new better thing comes out before you master the old one, it will happen all the time.",1.0
g7iddrg,j4ei3j,"You can train them separately and manually handle the logic of communication between them. If you want to combine them in one model and train it using backprop, you can use a concat layer to connect their outputs. Not sure if it will work for LSTMs but worth trying.",3.0
g7io2ec,j4ei3j,"Yeah, I'm aware of the concatenation layer but as you said, I'm not sure as well whether this would work",2.0
g7ipus9,j4ei3j,Not sure. Give it a shot. Please update me once you get it to work.,1.0
g7iv8tn,j4ei3j,"Sure, I will",1.0
g7hvygp,j42da6,"Well GANs aren't really interpretable so you're probably not going to find out why it is or isn't working that simply. There was one method that used autoencoders to recover images it interpolated the result from, so if something was incorrectly generated you could see what picture the generator thought it had to use as an example, but that was done on autoencoders which are an order of magnitude simpler than GANs. I'm not that familiar with the StyleGAN architecture, but even if the generator is some kind of autoencoder, you could only probe the generator and find out why it's performing poorly. Chances are it's not going to produce very interpretable results because autoencoders in GANs do not memorize the images as well as ordinary variational autoencoders do. If you're interested in doing it regardless, [here's the paper](https://arxiv.org/abs/1810.10333)

If you have a better discriminator, then sure, you have to gimp it somehow. I wouldn't touch learning rates separately as they don't actually make the component more or less capable, just learn more slowly, but if your discriminator has an easier loss hyperplane to traverse, then all you're going to do by slowing it down is disable your generator from learning as quickly. Whether this is slow enough to make them learn equally as fast I do not know, however from experience the difference in the generator and discriminator can be a very nasty function, after all you're trying to find Nash equilibrium which is a difficult problem, so mere hyperparameters or schedulers aren't a sure way to fix this. I would rather lower the learning rate of both components, perhaps introduce warmup, rather than just gimp one of them. After all, the generator relies on the discriminator.

Recent work on GANs show that the best methods for balancing two adversaries is indeed changing learning rates to something lower, loss averaging or advanced batch normalization techniques. However, if I remember correctly, StyleGANv2 has a bug in the learning rate for the discriminator, so before doing any of this make sure that you're using an implementation that doesn't have a bugged discriminator LR, because the bug is that the discriminator uses only the generator LR, it can't be set separately.",1.0
g7hxeqa,j42da6,"&gt;Well GANs aren't really interpretable so you're probably not going to find out why it is or isn't working that simply. There was one method that used autoencoders to recover images it interpolated the result from, so if something was incorrectly generated you could see what picture the generator thought it had tonuse as an example, but that was done on denoising autoencoders which are an order of magnitude simpler than GANs.  
&gt;  
&gt;If you have a better discriminator, then sure, you have to gimp it somehow. I wouldn't touch learning rates as they don't actually make the component more capable, just learn more slowly, but if your discriminator has an easier loss hyperplane to traverse, then all you're going to do by slowing it down is disable your generator from learning as quickly. Whether this is slow enough to make them learn equally as fast I do not know, however from experience the difference in the generator and discriminator can be a very nasty function, after all you're trying to find Nash equilibrium which is a difficult problem, so mere hyperparameters or schedulers aren't a sure way to fix this. I would rather lower the learning rate of both components, perhaps introduce warmup, rather than just gimp one of them. After all, the generator relies on the discriminator.  
&gt;  
&gt;Recent work on GANs show that the best methods for balancing two adversaries is indeed changing learning rates to something lower, loss averaging or advanced batch normalization techniques. However, if I remember correctly, StyleGANv2 has a bug in the learning rate for the discriminator, so before doing any of this make sure that you're using an implementation that doesn't have a bugged discriminator LR, because the bug is that the discriminator uses only the generator LR, it can't be set separately.

Thank you very much for your advice! Finally, I made the decision to reduce in both of the LR, what I was thinking but is to let the generator learn a little more than the discriminator for a few times to try to bring it into ""harmony"". I don't remember exactly where I found it, but they based this analysis on the loss, for example:

If score/fake - score/real &gt; 2: The policy is to train only the generator to reduce this gap. Or put the discriminator in warmup mode.

Regarding the bug you mention, I was also seeing it and I think I solved it in the following way (in case someone finds it useful):

* The problem is mainly found in the training / training\_loop.py file where the same LR is erroneously sent to the discriminator. Specifically in the feed\_dict line: [https://github.com/NVlabs/stylegan2/blob/master/training/training\_loop.py#L279](https://github.com/NVlabs/stylegan2/blob/master/training/training_loop.py#L279).
* In my case I have simply created two different dictionaries, one for the generator and the other for the discriminator:

&amp;#8203;

    feed_dict_G = {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_size_in: sched.minibatch_size, minibatch_gpu_in: sched.minibatch_gpu}
    
    feed_dict_D = {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_size_in: sched.minibatch_size, minibatch_gpu_in: sched.minibatch_gpu}

* Finally I have put it in each corresponding tflib.run:

&amp;#8203;

    # Fast path without gradient accumulation.
    if len(rounds) == 1:
        tflib.run([G_train_op, data_fetch_op], feed_dict_G)
        if run_G_reg:
            tflib.run(G_reg_op, feed_dict_G)
        tflib.run([D_train_op, Gs_update_op], feed_dict_D)
        if run_D_reg:
            tflib.run(D_reg_op, feed_dict_D)
    
    # Slow path with gradient accumulation.
    else:
        for _round in rounds:
            tflib.run(G_train_op, feed_dict_G)
        if run_G_reg:
            for _round in rounds:
                tflib.run(G_reg_op, feed_dict_G)
        tflib.run(Gs_update_op, feed_dict_G)
        for _round in rounds:
            tflib.run(data_fetch_op, feed_dict_G)
            tflib.run(D_train_op, feed_dict_D)
        if run_D_reg:
            for _round in rounds:
                tflib.run(D_reg_op, feed_dict_D)",1.0
g7hz46e,j42da6,"Yeah, that's the fix probably. I mean, usually usually you don't want to desync the components, so it might be a design decision, not simply a bug. But regardless, if you need to fix imbalance it might be worth to look at. It's just that you have to be careful when changing the learning rate of modern networks, they're much more sensitive to those changes (as in they break apart easily) and usually don't get you that much positive change. Meanwhile, screwing up the learning rate when using Adam even by a small margin is the difference between a SOTA and a mediocre model. Generally what we do in practice is use other methods because it's hard to judge what changing the LR will do before the training is done (or diverges). But even then it's lot of testing because in practice a lot of things you don't know and there's no single paper that will solve your problem, even though it might tackle something similar to what you have.",1.0
g7eeb0m,j3tlsz,"I'd suggest using manjaro. You can install everything with 1 line from pacman. Don't use conda.
Also, pytorch is certainly better",11.0
g7elcjr,j3tlsz,"+1 for manjaro. I'm very happy with it, especially every time that a colleague comes asking how to solve x problem with ubuntu and nvidia/cuda",2.0
g7h1d1w,j3tlsz,What's wrong with conda?,2.0
g7hxzga,j3tlsz,"Id also like to know.

Been using conda for over a year now with zero issues.  Which is shocking to be honest.  There are always issues.",1.0
g7eh3g0,j3tlsz,What line exactly?,2.0
g7ehckc,j3tlsz,"pacman -S python-pytorch-cuda python-tensorflow-cuda

It'll install all dependencies including cuda on its own.",6.0
g7f5vbk,j3tlsz,"Are you aware that `pacman` is Arch and not Manjaro-specific ?

It's like if you advised someone to use PopOS to be able to use `.deb` packages. Doesn't make sense.

It's a logical nonsense to recommend a subset of the set that has property A in order to use property A.",0.0
g7gx49y,j3tlsz,"Even though both use pacman as their package manager they pull from different repos. Many of the manjaro repos are held back and may lead to incompatibilities when compared to vanilla arch. In the larger picture this can cause breakages with the rolling release paradigm.

On my dev machine (arch btw) I use miniconda to install the packages in conda environments as I believe the pacman installs them in the system Python env.",1.0
g7gxgko,j3tlsz,"Of course, but these packages u/vajra_ was talking about are the one that Manjaro pulls from upstream : https://www.archlinux.org/packages/?sort=&amp;q=cuda",1.0
g7gxobg,j3tlsz,👍,1.0
g7f5xlz,j3tlsz,"Do you know Manjaro is Arch based?

Edit. A google search can save you from being embarassed by your stupid comments.",-4.0
g7fb719,j3tlsz,[deleted],1.0
g7fbjdo,j3tlsz,"I did pay attention. He acted like a jerk first. I added the edit after his 'Dear God' comment. Ignorance is acceptable, arrogant ignorance - not so much.

Edit. Also his comment doesn't make sense at all. He/she did not make an attempt to understand my point at all

Edit 2. Please stop catering to stupid comments. This sub has enough of them already.",-2.0
g7f64z6,j3tlsz,"Oh god, you didn't understand my point at all...

How is it even possible to answer this with the point flying so high over your head baffles me lol.",-1.0
g7f6hvq,j3tlsz,"Stop being adamantly stupid. Pacman is a package manager for Manjaro. 
I know that this sub is full of some really stubborn stupidity. Just don't press on it.",-2.0
g7f6uue,j3tlsz,"Dear lord.

You ""work"" in deep learning and are unable to understand this basic logical reasoning which I will give again as a weak hope that maybe it enters your brain:

It's a logical nonsense to recommend a subset of the set that has property A in order to use property A.",0.0
g7f8h4f,j3tlsz,Bro... just stop humiliating yourself. Keep your stupidity to yourself.,-3.0
g7f8rql,j3tlsz,"*Go to u/adept-cheesecake-812 profile*

*11 karma*

*last comment a month ago*

*same country*

*same subreddits*

Hi u/vajra_ !",-2.0
g7f9fkx,j3tlsz,"Stop being stupid, creepy and an asshole.",1.0
g7e9mk4,j3tlsz,Conda environments handle these pretty well actually.,13.0
g7ea6ph,j3tlsz,Also with regards to CUDA versions and the different drivers etc.? I'm aware that i can isolate tensorflow/keras versions in different environments.,2.0
g7ec950,j3tlsz,"If you make a conda installation, it will also install the necessary cuda version for you. I don't know how it does that but I am using different tensorflow versions on different environments on the same server and they do not conflict although they require different versions of cuda.",3.0
g7el7r7,j3tlsz,"Wait, what!",2.0
g7ed2cm,j3tlsz,what about tensorrt? I struggled with tensorrt versions several days and gave up.,1.0
g7fcgxa,j3tlsz,I didn't use tensorRT so I don't know that one.,1.0
g7gs8g9,j3tlsz,"I second this. conda (I prefer miniconda over anaconda) is very good at this. Didn't have a single issue after switching to it over 4 years ago. For packages that aren't registered with conda, just install pip within conda (`conda update --all`) and use this new pip (`miniconda/bin/pip`) to add packages to conda.",2.0
g7ek8iw,j3tlsz,"No, but it may be a good reason to try docker",8.0
g7ev9c3,j3tlsz,You need to install CUDA on your computer. PyTorch comes with CUDNN in conda- you need to specify distro in addition to version when installing like `conda install pytorch=1.01=cudnn10`. I find this to be very simple and every version of pytorch supports multiple cudnn. All you need to know is which versions of cudnn work with the CUDA you installed on your computer.,3.0
g7f7lan,j3tlsz,I've just spent the week getting my head around docker in pycharm for this reason.  It works well but there was a steep learning curve.,3.0
g7hrufk,j3tlsz,Steep as in fast or slow? Because it’s usually misused,2.0
g7hwzdq,j3tlsz,"As in, trying to get a pytorch app to open an opencv gui window in pycharm has been an interesting few days! There has been more to learn than if I was picking up conda from scratch. Pulling a pre-built docker is easy enough, but then it needs access to x, gpus, file systems etc. Then I needed to learn about dockerfiles.  None of it is that hard, just new for me. Now I think docker is great and well worth learning. It’s the way forwards for me.",2.0
g7gj4f0,j3tlsz,Go with docker,2.0
g7gqc0s,j3tlsz,This is exactly why I made the switch,1.0
g7scr26,j3tlsz,I am only using pytorch and never experienced any compatibility issues. I use pytorch in docker.,1.0
g7e7t29,j3q6j6,"I'm just following the post , I have same doubt.",1.0
g7b9xoa,j3cgun,Generative,13.0
g7cbb00,j3cgun,Lol,1.0
g7d3jlf,j3cgun,You are still bounded by the distribution of the dataset that the GAN was trained with.,5.0
g7cf0m4,j3cgun,Generative*,2.0
g7d0bp2,j3cgun,"So cool!

Where should I start if I want to primarily do things with GAN? I have no coding background whatsoever but I just graduated college with a social + math degree. I am very good at music production and sound design in Ableton, I have many creative ideas.

I'm eager to learn but I want to start somewhere that can get the rolling in this field of AI. I was going to start with Python but I don't know if that is the best one.",1.0
g7dn5rs,j3cgun,You'll have to start with the very basics. For coding just learn basics like control flow loops and data types. Advance data structures aren't needed as libraries handle everything. Use python for this. Learn numpy basics and start Andrew Ng course on Coursera. Once you learn a basic neural network and CNN you can start learning about GANs,1.0
g795zqd,j2zzs8,[https://colab.research.google.com/github/deepmind/sonnet/blob/v2/examples/mlp\_on\_mnist.ipynb](https://colab.research.google.com/github/deepmind/sonnet/blob/v2/examples/mlp_on_mnist.ipynb),1.0
g797hr2,j2zzs8,"I have seen the source code and from what I see and from the dev summit of sonnet 2. Deepmind was using torch then they created sonnet for both deepmind and google brain. Which is probably why there a lot of resemblance between torch and the sonnet library. There are shared vocabulary in keras and sonnet like Sequential, etc. But they also do differ with what they call there base classes: Module(torch like, sonnet) and Model(keras). Then there is the common layers like Linear vs Dense and so on. I saw the github code at google brain and they use sonnet. I would like to know what things I could do in sonnet that not with keras. 

PS i was very curious when they said thay it was very close to the math which to me means that custom operations/modularized operations might be better done in sonnet. 

To sum up, i would like to what I could gain specifically with using sonnet as the one level up API from tf 2.",1.0
g77jgfc,j2rde3,Fastest in the world? What are your references? Also low FLOPs doesn't translate to faster inference necessarily.,7.0
g76nqxn,j2nda3,"Disclosure: I work at Microsoft and co-lead the ONNX operator SIG. 

A lot of team inside Microsoft deploy their model using ONNX on ONNX Runtime.",12.0
g76slfr,j2nda3,I see example online from most frameworks except pytorch. Is pytorch used in any capacity inside Microsoft?,0.0
g76z29k,j2nda3,"Example about what? Convert to ONNX, PyTorch ONNX export is in PyTorch:  


[https://pytorch.org/docs/stable/onnx.html](https://pytorch.org/docs/stable/onnx.html)

[https://pytorch.org/tutorials/advanced/super\_resolution\_with\_onnxruntime.html](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)

&amp;#x200B;

And yes, PyTorch is used a lot also.",4.0
g776c5c,j2nda3,Yes we use ONNX on NVIDIA Jetson devices in production. It's an absolute bitch to compile but it works once you get there.,6.0
g77izia,j2nda3,Interested to know what you use Jetson devices for in production? All the material I’ve seen are makers or people learning.,1.0
g7773gm,j2nda3,"Yes, I routinely use ONNX to convert PyTorch models to compiled TensorRT engines.  (https://github.com/onnx/onnx-tensorrt)",5.0
g76mlhj,j2nda3,Yes... You might run into some issues if using with tensorRT though,4.0
g77c2r3,j2nda3,"We're experimenting with it as optimizations are being rolled out. It can run the things its written for but our stance is that it's not worth running for models that weren't specifically optimized with it as you can usually get better performance and throughput by simple pruning or distillation.

If you don't have the time or knowledge to fine tune it like that and just want a relatively plug and play solution, then yeah, it's good.",2.0
g77fp6p,j2nda3,"We do, but only as a intermediate format before TensorRT.",2.0
g76mwnm,j2nda3,"I can drop my two cents based on second hand experience. I took data mining in my college last semester and our professor, who works in the industry full time, put a lot of emphasis on using ONNX for deploying models. Our project had to also be submitted in onnx format. So based on this experience, I will say that it is heavily used in industry.",2.0
g78pu0a,j2nda3,I used ONNX runtime to deploy an image segmentation model developed in PyTorch to AWS Lambda. I found it easier to get it to fit within the layer size constraints and also slightly more performant than using PyTorch.,1.0
g76jh0p,j2mxsz,Not sure I understand the question. Why do you need more than a couple phones?,1.0
g77bl5j,j2mxsz,"I'm going to have multiple phones in different locations, each running different TF models.",1.0
g77fuu5,j2mxsz,Does it have to be a phone or would something like jetson nano work better?,1.0
g78gqh8,j2mxsz,"&gt;jetson nano

I like the idea of a jetson nano or similar to run models on the edge, but by the time I get all the accessories needed the price is close to phone.  Would GPU powered phone be able to perform on par with a Jetson?",1.0
g7h8hry,j2mm8q,"I don't think without finding similar cursive datasets, you will be able to achieve a lot. I used OCR for my job a lot. Even typed texts are hard to read sometimes. It is a hard thing to do.

[https://research.aimultiple.com/handwriting-recognition/](https://research.aimultiple.com/handwriting-recognition/)

See if above link helps. If you can find a good dataset similar to yours, blob detection on binarized image is one of the steps that might have been good to create a dataset for text detection. Tesseract is the most advanced OCR tool you can find if you want to develop, esp with Python, however even that feels thousand years old, despite it having its NN model that you can train(that's a pain in the ass, but there is a way) .",2.0
g7ibrew,j2mm8q,"Thanks a lot man!!
Yeah, training set is the hard one. I do have some already transcribed texts that could help to do that but, just confess, it's quite small. 

A friend also recommended tesseract. I will take a look over this weekend. Again, thanks a lot!",1.0
g75rgxi,j2jtzm,"YOLOv4 paper: [https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934)

YOLOv4 code: [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)

A complete read of YOLOv4:  [https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe](https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe)",0.0
g74c6ei,j28o0h,"As SouthernLightsBBQ mentioned, the ""final"" accuracy should really be your ""final"" model tested on a dataset that it has not seen. If, for whatever reason, you're interested in the training or validation accuracy of your training, then you just need the last value in the \`history.history\['accuracy'\]\` list. Since the training history is there just to give you an idea of how the model improved with each epoch, averaging over epochs, as you are suggesting, does not give you a meaningful result",4.0
g74ezmm,j28o0h,"This is actually a real problem that i'm working on, i have everything ready, my training, testing and independent set, my model is already tuned, i only need a way to obtain the accuracy of my training, testing and independent set (i need the training accuracy too). I know how to do everything else, but only with this final part i have problems. i feel really dumb for that.

I was reading and i think you can see what is the accuracy of your validation set with model.evaluate(X\_test, y\_test) \[using Keras\], but i don't know if i can use that value obtained to say something like ""after training my model, I got an accuracy of 98% on my dataset for validation""",1.0
g74s21p,j28o0h,"For your _test_ set, you can certainly use `model.evaluate(x_test,y_test)`. For your _validation_ set, look into the different fields of the `history.history` dictionary - one of them will be your validation accuracy (probably named `val_accuracy`). And, yes, if the last value of the `history.history['val_accuracy']` list is 0.98, then you can say that your model achieved 98% accuracy on your _validation_ set",2.0
g74uhie,j28o0h,Thank you so much! this really help me,1.0
g73x3wq,j28o0h,"In other words, I have seen that in papers when they train a model they give the final accuracy they obtained. They take the precision obtained in the last epoch? or do they average all the epochs together?

is there something like this?  ""**history.history\['total\_accuracy'\]** ""

&amp;#x200B;

i know it's a really dumb question but i'm just starting with Machine Learning",1.0
g741iq6,j28o0h,"When you talk about papers mentioning accuracy this should usually not refer to train accuracy. Normally your datasets will be trained on training data (yielding a train accuracy on this data), evaluated on a validation set (yielding a validation accuracy) and finally the network is tested on a separate test-set. The accuracy on this test-set is usually what papers refer to. 
So instead of extracting your accuracy from the training history you should ideally use your network to run inference on a separate test set. The accuracy of the results from such a run would be what you are looking for.",7.0
g74fjhr,j28o0h,"Yeah, That's exactly what I am looking for. In fact I already have a set of training, testing and an independent dataset. What I want to know is how to get that value of ""final accuracy"". Because I already trained my model with 50 epochs and got the individual accuracies of each epoch from my training and validation set (in fact I even plotted them). 

Should i use something like 

""loss, accuracy = model.evaluate(X\_test, y\_test)"" 

and the same for the training dataset? Because i also need the accuracy of my model when i was training it.",2.0
g74wmi6,j28o0h,`model.evaluate` might be the function you're looking for,1.0
g743yla,j27697,Img segmentation and classification,3.0
g74jbio,j27697,New drug design,2.0
g75vryi,j27697,How exactly? I mean what would be the work flow or model designs in this case?,2.0
g75y8ao,j27697,"Generative models are usually used for novel drug design.

This is the one example of the workflow consisting of four steps.
1) Pretraining step: By training a model with druglike compounds, the model is trained ti generate druglike compounds.
2) Generation step: By exploring a pretrained model using optimization or reinforcement learning, the model generates the compounds for the specific purpose, e.g., inhibiting the target of interest or having better properties.
3) Filtering step: filtering or prioritizing the generated compounds with medicinal chemistry knowledge or other models
4) Validation step: synthesizing the compounds and validating them whether they were made for intended purpose.

If you wanna know in detail, please refer to the paper below which is one of the most famous paper using deep learning for new drug design.

https://www.nature.com/articles/s41587-019-0224-x",3.0
g75z8gb,j27697,"Cool. Thanks, will also read the paper.",2.0
g73j3nq,j25i16,Well i made one to scrap my search history few days ago. One very useful thing that i found was that you should use user agent of some very old browser so the page won't have java script which would make the task easy. You can simply google facebook scraper and you would find enough helpful material.,3.0
g7aw492,j25i16,Thanks a lot ☺️,1.0
g73a0xh,j257mz,Yolo is great for object detection. You can train tiny yolo,3.0
g74lvoo,j257mz,"alright, ive read the theoretical part!! will try to work on the implementation",1.0
g74m547,j257mz,"if i were to also classify, id need to pair it up with a CNN as well right ?",1.0
g74uruc,j257mz,what do you need to detect in a video because they have pre trained weights for some classes,2.0
g74uy8z,j257mz,"i need to detect standard things like people, cars etc",1.0
g74v1bl,j257mz,they have pre trained weights for that,2.0
g74vdbg,j257mz,"alright, thank you!!!! ill first get to work on the YOLO algorithm and then proceed with the classification ! Thank you for your help!!!",1.0
g74vhti,j257mz,"read this

[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)",2.0
g74vuof,j257mz,thank you!!,1.0
g73vipu,j257mz,"Try tensorfow hub and use one of the pretrained object detection models, once you're comfortable and have enough time on your hands start training it with your own inputs and then slowly get around to building your own model",3.0
g74m1j0,j257mz,"ill go check it out as soon as i can, thank you so much !!!!",1.0
g73v268,j257mz,Watch Mark Jay’s Object Detection videos on YouTube,2.0
g74ly4t,j257mz,will check it out! thank youu!!,1.0
g71yyat,j1y86h,"Hi,

1. Switching between version with anaconda is possible in both Ubuntu and Arch due to the inclusion of binaries with Conda installs
2. I've tried setting up an Arch box. Although exciting that you build everything from scratch, it quickly becomes very frustrating when something does not work. When building binaries does not work as expected, since people do not expect XYZ bleeding edge version of the compilers you're running. When something as simple as installing a proprietary .deb/.rpm can become a big headache, if the developer did not do their job properly
3. My takeaways, do not go into Arch unless you are ready to face severe struggle, for which little of the StackOverflow and forums have solutions for, and, you've tried PopOS and it did not scratch your no-buntu back.

tl;dr -&gt; Most likely you will enjoy PopOS more as your main DL Box rather than going through Arch-hell.",15.0
g720mwk,j1y86h,"I agree, this has nothing to do with ML/DL/Anaconda. Besides you can also easily switch between python/tf versions using virtualenvs.

Kudos to the OP for trying out a new distro. Note that Ubuntu is very bloated, and simply switching to Kubuntu or Lubuntu also decreases memory usage significantly.",4.0
g73m50n,j1y86h,"I agree, this doesn’t really sound like a good suggestion for a typical ML practitioner. Ubuntu works, and that’s all you need to do almost anything ML related. 

However, Arch has a great (if occasionally spiteful towards low effort noobs) community and the best, most comprehensive wiki I’ve found. With a certain degree of Linuxy competence it’s actually one of the easiest distros to use.",3.0
g720d4n,j1y86h,"I use arch btw, deep learning edition.

(I use arch btw too)",10.0
g73m5s1,j1y86h,I too use arch btw,1.0
g738go0,j1y86h,[deleted],6.0
g73n3ys,j1y86h,"Yep, its possible to spend days caught up in Linuxland achieving nothing that you couldn’t with an Ubuntu usb stick. It’s not something to do on employer time imo. However, it *does* help you understand a lot more about how Linux works, which can be very useful if you end up with a wonky system somehow, or need to do something other than DL.",1.0
g73v9wo,j1y86h,"I agree with you but I'm amazed to see you have never had any problem with Anaconda, especially its own upgrade or installation of conda-forge packages.",1.0
g72ucsi,j1y86h,"I have ubuntu at home and debian at work... Although I don't need to change between tf versions I think that having different virtual environment simplifies the process a lot (I already have two, one for R and one for python) so I don't really get how the process is simplified in Arch, same for the cuda, cudnn... I have to agree with you removing gnome. Gnome on systems with high memory usage is a pain. I've experienced local freezing because one of the users was using gnome.",2.0
g74jf4d,j1y86h,"Isn't this what venv are for? I'm not really sure there's much difference other than the headaches associated with setup in arch box.

I'd rather concern myself with DL problems than I would with arch box setup problems",2.0
g733dz7,j1y86h,could you have two separate distro servers and pass parameters for the functions that require more intensive processing to the arch server? or would their be too much of a latency challenge (lag) over the lan for it not to be worth it?,1.0
g73wjli,j1y86h,"Although the effort is good and useful if you do purpose build and would like to get the most  computing power/$ out of cloud, there is a serious flaw in this approach.

You know as data scientists, economy is very important. But what's more important is the time to market. If you can use latest PyTorch or use GPT-3 to build something useful, it is worth spending extra money on best hardware and newest software. Since Ubuntu is so widely accepted and used as blueprint images for several cloud vendors, all major ML libraries would publish on latest versions of Ubuntu first and bugs would be discovered and fixed first as well and training new team member on Ubuntu is also easier.

So unless the application is for industry use cases that need both scale while having strict limitations, I would say always go with generic distro.",1.0
g72mh3g,j1wboy,[deleted],2.0
g7442kx,j1wboy,"I think his question wasn’t so much about why overfitting happens, but more:

Why doesn’t overfitting happen as much in the case of gradually introducing data while its much more prevalent in the case of having all the data from the beginning?

Which I think is much harder to answer (as this is not going to be the case for every network, type of data and data distribution)",1.0
g71ltkc,j1vzny,"If you're using different types of data like image, text, numbers etc, you can check the keras API functionality that takes multiple inputs. You'll find it in the keras docs",2.0
g71lxxp,j1vzny,Thank you for your reply. They are all numerical values. Problem is part of the data is timeseries(logged at every epoch) and part of the data is single values.,1.0
g71mtnm,j1vzny,Use an RNN for the time series data and an ANN for the other parts of the data and then use multiple inputs function of the keras API to combine the both.,1.0
g71njel,j1vzny,I will try that. Thank you,1.0
g6zk2ug,j1jb04,"IEEE-VIT brings to you OpenCon: A Virtual Tech Conference with Speakers from well established companies like Google, Microsoft, Uber, Spotify who are experienced in various domains ranging from UI/UX and Web development to AI/ML and Big Data Analysis.

These industry experts are going to be talking about their journey in their respective domains and will also be interacting with the attendees, answering their queries and providing them with the right guidance to kickstart their journey in the world of tech.

.

Register for OpenCon at [opencon.ieeevit.org](https://opencon.ieeevit.org)",1.0
g6ziodb,j1hq38,"~~No, it does not remove ""any"" object. It removes objects the underlying image segmentation model was trained to detect. This is far less impressive than it seems - mostly just clickbait at this point.~~

Edit: while this was posted with a super clickbaity title (ugh), I made the mistake of assuming this was just Mask R-CNN + OpenCV, and not something different - and legitimately impressive.",16.0
g702uop,j1hq38,"I think this is impressive. It also needs to correctly infer the background. Question is, is it one-shot or zero-shot i.e. does it need to be trained on the video it's removing the segmentation from, or can it be applied to any video it's not been previously trained on?",7.0
g70in0n,j1hq38,"I'm changing my position, haha. I'm much more impressed seeing the actual source, instead of the clickbait title posted here. I (wrongly) assumed this was just Mask R-CNN plus some OpenCV tricks (which I do not find all that impressive). This is definitely something different, so I retract my point about this being clickbait and just relying on a pretrained segmentation model.

Actual source:
http://chengao.vision/FGVC/",6.0
g717okm,j1hq38,source link is not working!,0.0
g71t2p5,j1hq38,Works completely fine for me. Not sure what to tell you.,1.0
g71ujf4,j1hq38,ohh yeah its working now for me. 😅nevermind,1.0
g6zivqp,j1hq38,If you speak spanish there is a good video explaining this (the entire channel is very interesting),3.0
g702law,j1hq38,"Interesting, wonder if it could be trained to segment moving objects in general and recover the background behind those, instead of segmenting specific classes.",2.0
g71fhbk,j1hq38,How does this work ?,1.0
g71h3vf,j1hq38,That's really impressive if it is correctly predicting the background on new videos.,1.0
g6z5dv0,j1h1v8,By educating people. I do not see any way Deep Learning can help.,2.0
g6yts01,j1ff1z,"For context, I am using the Yolo framework, and I was able to generate the intermediate output of each layer. As instructed by the author of the repo, [https://github.com/AlexeyAB/darknet/issues/571#issuecomment-378433259](https://github.com/AlexeyAB/darknet/issues/571#issuecomment-378433259)",1.0
g6zc08l,j1ff1z,"Usually feature maps are h x w x depth, where depth is a large number. Those feature maps are representing h x w x depth as h x w x 3 x depth/3. So color is just a dimension that can displayed",1.0
g6ymg60,j1bjye,I think the UTD is a good option for MS in CS,2.0
g6vezq3,j0rxcc,"This may not be helpful, but I have had a very hard time getting convergence with RNN based networks in a gan. I tend to find that they misrepresent ( something like overtrain ) too much.

I would very much like to see others using an lstm based GAN appropriately but I fear it is not well suited for.

I would think a regressive approach may be better.",1.0
g6vgd2m,j0rxcc,I wonder if how well a 1D conv generator in a GAN set up would work,1.0
g6vi3rl,j0rxcc,"Well 1d conv may be great but you are losing any associative context of time disparity.

I would be more compelled with multiple discriminators for each time step ( something like N 2d unets to 1 (x)d-convolution generator)",1.0
g6xsor2,j0rxcc,Check out Neural ODEs,1.0
g71dx6c,j0rxcc,Not uncommon research topic in my field (healthcare). Heres an example https://arxiv.org/abs/1706.02633,1.0
g6w53od,j0r6fg,Have you tried adding more layers or using pretrained models and fine tuning?,2.0
g6wqm6y,j0r6fg,"Hey. Thanks for your reply. I always have this idea that if I add more layers it might reduce accuracy(I’m still new to this). But I shall give that a try. Do you know exactly how adding more layers might increase accuracy?
That’s cool. I haven’t looked at pretrained models as of yet. Might have a look on those soon. Thanks for your help. Appreciate it :)",1.0
g6uhicu,j0q7qs,"really cool video;

LIKED and SUBSCRIBED; if you can please give me some feedback on my video [https://www.youtube.com/watch?v=3NVL4vkty-I](https://www.youtube.com/watch?v=3NVL4vkty-I)",2.0
g6ujj9e,j0q7qs,Good one bro. Subscribed..!,1.0
g6uk9cb,j0q7qs,Thanks a lot bro and again amazing video !!,1.0
g6ujldm,j0q7qs,Thank you for the award ❤😭,1.0
g6pcicc,j0akdm,"Here is the link to my GitHub: [https://github.com/gordicaleksa](https://github.com/gordicaleksa)

Which cool projects did you implement yourself so far?

Building stuff yourself beats reading blogs, papers, and watching videos any time!",5.0
g6sa0va,j0akdm,Hey great projects that you have implemented. Currently I am working on emotional recognition for a social robot. Would love to know your thoughts if you had done emotion recognition through facial expression. I have used the FER2013 dataset and gained an test accuracy of around 65% so far. Still learning and figuring on improving my model itself.,3.0
g6sx5o0,j0akdm,"Awesome! First I'd strictly call it facial expression recognition and not emotion recognition because e.g. smiling expression can be a consequence of:
1) true happiness
2) social pressure
3) I hate you 😂
4) ""fill in the blank""

The relationship between expression and emotion is super complicated with many latent variables included like culture, age, etc.

I did mentor a similar project on ML summer camp where I'm a lecturer. There are basically 2 approaches you could take:
1) directly doing the classification from the raw image
2) regressing landmarks first and then doing classification - you get the advantage of lower dimensionality data. But FER 2013 has 48x48 images, so it shouldn't be super tough to go with 1) also

Which approach did you take? The workflow is basically the same as for other tasks go and figure out what SOTA approaches are, try to implement those and maybe further refine them if you have some additional ideas hahah.",2.0
g6uhedi,j0akdm,"Hey. Thats awesome! Thanks for the reply. I mainly did through supervising learning using CNN with 4 hidden layers. Initially had an accuracy around 60%. Bought it to 65% after adding dropouts and data augmentation. Currently looking at adding batch normalization as it seems to be improving the test accuracy. However, I face two main issues currently:
1. As for I heard the FER2013 dataset at most gives an accuracy of around 70% using traditional CNN route.
2.GPU is a huge issue. Currently use kaggle notebook with an awesome GPU they provide. But, you easily hit there 37 hour usage limit as u experiment more. Google collab GPU's are really slow compared to kaggle.
Do you have any suggestions to further improve my model.
My two ideas for improving as of now is:
1. I am looking at CK+ dataset and thinking of combining both FER and CK+.
2.Remove emotions which are quite confusing for machine to learn such as disgust
I am still new to deep learning, but its really been a fruitful experience to have came this much. Would love to know any technical suggestions from your side as well like ensemble and transfer learning perhaps?",2.0
g6ulvwi,j0akdm,"That sounds like an awesome learning experience!

Not sure about what the accuracy plateau is for FER 2013 to be honest, I'd need to start working on it to get some additional context.

What type of GPUs does Kaggle offer? I usually use either my own machine (RTX 2080) or some cloud solution (I have 150$/month for free on Azure ML as a Microsoft employee and also some free credit on Spell MLOps platform).

Ensemble is always promising it just depends what your time limits/constraints are during the inference.

I wouldn't remove those if they exist in the test set!

Adding additional data like CK+ may help. Again unfortunately I don't have enough context to help you without digging deeper into it myself.",2.0
g6uoq44,j0akdm,"Wow that’s really cool that u work in Microsoft. Kaggle uses Tesla P100 gpu which in my experience is training much faster than Google colabs gpu. Not sure why though.

Thanks for the valuable feedback will look further on it. Really appreciate your help. Cheers :)",2.0
g6ur5ag,j0akdm,"Thanks! Microsoft is getting back it's reputation in the AI world. 4 years ago everybody wanted to work for Google now things are changing!

Oh nice I didn't know Kaggle offered P100s for free! I'll take a look, thanks! (I assume it's for some specific Kaggle tasks only I may be wrong)

You're welcome, keep crushing it!",2.0
g6pd0l9,j0ahcc,"This is a question which belies inadequate research and ignorance. DL is a tool and CV is a field. You say that you want to build 'autonomous vehicles'. Okay. Which part of it? If you're not sure yet, then first read as much as you can on all parts of building an autonomous system. Maybe, that will clarify your objective for you.

Also, 'good' data scientists do need dev skills as well.",10.0
g6ro5ym,j0ahcc,"DL is a tool as well as a field. It is a part of ML that incorporates DL within itself.

DL and CV are separate.",0.0
g6pee6x,j0ahcc,I don't know really. Like something that has a connection with CV. I like programming and CV.,-2.0
g6pemsa,j0ahcc,Ok. You need to read and research more. Yourself.,4.0
g6rr72x,j0ahcc,"It is understandable that you are seeking answers to all your doubts, but you have to realize that there is no single correct answer. Worse, people will only be able to tell you the part that they have explored in their careers. For example, I have only explored CV, and I was never into NLP. But I know that NLP is a much more emerging and challenging field with more papers and more space for innovation and career building opportunities and higher paying jobs as of now. However, since I have no knowhow of NLP, I won't be able to persuade you sufficiently. Worse I might actually push you to pursue CV, which could be the wrong answer for you.

Hence, my suggestion is to ""KNOW THYSELF"" first. You have to know what your passions and interests are. And then see which top class university and professor within your reach (and means) can help you fulfill your passion. Also, people these days weigh in too much on what's trending. This makes them take bad career decisions and leaves them in a dissatisfying job for years. Don't fall into that trap. Warren Buffet once said, ""Take advice from everybody but then make your own decisions."" Which means that you should not decide on what other people has decided for you. Decide for yourself.

Also, as other people in this thread has rightly mentioned, research is the key. Do more and more research. And write things down along the way. Don't fool yourself by thinking that you can remember all the stuffs. If you scan the field and research hard enough, you can assimilate a lot of knowhow within days.

Coming to your data scientist vs developer thing, I can tell answer this question for you (somehow). The simple answer is, that there is no guarantee that you will land up in a job that suits you the best on your first try. If you find after joining a certain company in a certain position, that the work is not suitable for you, you must quit immediately and look for alternatives. After a few tries you will most likely end up in a job that fulfills your purpose. Saying no to a lot of things, open the doors to new things which you never knew that they existed. It increases the likelihood of finding that one job which is absolutely right for you.",2.0
g6pchao,j0ahcc,"Data science, statistics, computer science may all touch on these, I’d check the course descriptions for different programs/degrees, and research specific prereqs for deep learning (math, etc) and look for those as well.",1.0
g6pd1ek,j0ahcc,"Can I give you a syllabus, to check and tell me what do you think?",0.0
g6pygee,j0ahcc,"This might sound rude to you, yet hopefully will do you some good:

1. Find out yourself. Read the syllabus and figure out (with the help of your friend, the internet) what the descriptions mean and whether it relates to your interest 
2. Understand what it is you want to do first (easier to say what you don’t want to do, but it doesn’t provide clarity for you) - which is related to 1 
3. This is your life, shape it into something you want to and can be proud of. Don’t put any weight into some random dudes opinion (including mine)

Your Post and your comments feel very much like “I wanna have a cool job doing cool things earning shit to a of money without doing anything for it”. This isn’t going to happen, mate.",5.0
g6pz2q2,j0ahcc,Yep I understand. You are right,1.0
g6pwabp,j0ahcc,"Depends upon your university what department do they offer. Based on the experience of my university, I could suggest taking something like autonomous systems in electrical Engineering which includes control and robotics and minor in Computer Science. Computer Vision is a broad field and DL is just a SOTA tool.",1.0
g6q1pqf,j0ahcc,"For 99% of universities it's going to be CS. I'm also assuming this is for undergrad, even for grad depts it'd probably still be CS. As other people mentioned it's going to take some more research on your part to figure out what exactly you're wanting.",1.0
g6nzgy2,j04lgp,Paper covered: [https://arxiv.org/pdf/2009.07047.pdf](https://arxiv.org/pdf/2009.07047.pdf),0.0
g6o8tki,j03oz4,it was a problem with the 2080ti it will be the same for the 3090 it seems.,1.0
g6pzmf7,j03oz4,I mean you could do [This which is almost as good](https://www.pugetsystems.com/labs/hpc/NVLINK-on-RTX-2080-TensorFlow-and-Peer-to-Peer-Performance-with-Linux-1262/) in linux,1.0
g6n3cl9,j01y64,"From sampling theory:

Every sample of an audio is a value (which is the quantized version of the voltage level of the microphone that recorded it).

Every sample of an image, is a pixel (which is the quantized version of the voltage level of the photoresistor of the sensor that recorded it).

Every sample of a video is an image (which is many pictures taken in successive time instances giving us the illusion of flow).",28.0
g6o4721,j01y64,"You can make an image of any sound by mapping “fourier transforms” of the samples in the y axis and time in the x axis, search for audio spectrum and FFT

EDIT: Sorry, audio spectrum wasn´t the term I wanted to write, I meant spectogram",12.0
g6p0c6q,j01y64,MFCC,3.0
g6p24sk,j01y64,"Some networks I have seen cut the sample rate down to 22050 Hz or so, and create a frame size of 512-4098 samples or so. They then shift the frame window over the audio file by a certain number of samples per shift, and feed the values into the network.

You'll have to convert the audio files to 32-bit float waveforms using a tool like Audacity, as most tend to use 16-bit signed integers as sample data types.",1.0
g6rig97,j01y64,Bits.,1.0
g6pc94m,j01y64,"So first off mp4 is compressed, your first step will be to convert it into WAV or some other representation that is literally a one-dimensional array of bytes. Then you can do your science.

This field is called Music Information Retrieval (MIR) - I don’t know what you’re trying to do with your RNNs but there’s a vast amount of research being done specifically on audio.

To answer your question, sound itself is variations in air pressure. A speaker is a device that converts variations in voltage to variations in air pressure. A DAC (digital analog converter) is a device that converts bytes into voltage. So essentially each value in your WAV represents an air pressure at a particular time.",1.0
g6np9z8,iztuo1,"So the terminology in graph neural networks is still a bit all over the place, but I'll try answer based on what I know so far.

Unless you're referring to some specific method, a graph embedding is just generally a set of feature vectors for the graph nodes. This is independent of how these embedding are generated.

Graph embeddings can be node feature aware, or topology aware, or both. Graph convolutional NNs are used to generate feature and topology aware graph embeddings.

There's are a lot of options here as GNNs (GCNs) are extremely general. For example you can use a relational GNN (rGNN/rGCN) to generate graph embeddings which are also edge feature aware (if you have edge features).

As for GNNs, during my literature review, I decided to ignore all spectral approaches. These were the attempts to generalize the CNN to graphs. When you get hit with hectic math involving Laplacian regularisation and stuff like that - that's spectral. Despite its vastly increased conceptual complexity, they don't seem to be any better than the much more intuitive versions which use the message passing pattern.

Have a look at the Graph Attention Neural Network (GAT) it's intuitive and powerful. Also its essentially a Transformer model sans the residual connections and layer norms. On that point, sparse attention transformer models are in fact GNNs themselves, capable of generating feature and topology aware graph embeddings.",3.0
g6p08zt,iztuo1,"The problem with graph embeddings is that if your network changes (for ex: You have new nodes/edges), your only way to calculate a value for them is to rerun your whole analysis in the Network. However, with GCNs, you have learned weights that transfer well to unseen data, so calculating new nodes/edges values is not a problem.

Another point that I find very interesting is that with  Graph Attention Neural Network (GAT), it is possible to have some interpretability of the model using the attention weights.

Stanford has a whole course on graphs. Only 3 classes talk about NN specifically. The professor addresses the differences along the course:  [https://www.youtube.com/watch?v=4PTOhI8IWTo&amp;list=PL-Y8zK4dwCrQyASidb2mjj\_itW2-YYx6-&amp;index=7](https://www.youtube.com/watch?v=4PTOhI8IWTo&amp;list=PL-Y8zK4dwCrQyASidb2mjj_itW2-YYx6-&amp;index=7)",0.0
g6lo7u7,iztuo1,RemindMe! 1 day,-1.0
g6lpn7m,iztuo1,"I will be messaging you in 1 day on [**2020-09-27 02:24:12 UTC**](http://www.wolframalpha.com/input/?i=2020-09-27%2002:24:12%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/deeplearning/comments/iztuo1/whats_your_opinion_regarding_graphnode_embeddings/g6lo7u7/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdeeplearning%2Fcomments%2Fiztuo1%2Fwhats_your_opinion_regarding_graphnode_embeddings%2Fg6lo7u7%2F%5D%0A%0ARemindMe%21%202020-09-27%2002%3A24%3A12%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20iztuo1)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
g6mz987,iztuo1,RemindMe! 5 days,-1.0
g6m9xxf,iztuo1,RemindMe! 10 days,-2.0
g6lbjhe,izq97n,"I am more or less shocked by the premise, in the US they want you to do your ""computer job"" with your ""computer tools""? Total WTF LOL. I do expect my plumber to have his own tools, but mainly because a whole load of tools and spare parts make no sense in most homes or even businesses. But a company that has/needs ML which is run on special hardware, can skip the hardware by hiring a contractor? Nothing I ever saw in Europe (certain corporate policies like not allowing contractors to use the staff canteen notwithstanding). This almost sounds like you're also competing ""on price"" with the next contractor who might invest in 10x the performance in his rig and demand no more $$$ than you. Don't know, interesting, mind expanding :) I don't think the novelty shock will allow me to offer the right advice, but as things stand now I would either go for 2 RTX 3080/3090s in a PC, or would demand/negotiate the employer standardises on something and offer it even to you. I've done a lot of laptop number crunching and it is generally not worth it. I've also done some thinking about what to offer in a coworking space of mine, and concluded a couple of eGPUs, Thunderbolt boxes with heavy duty GPUs inside, is a vital offering. So you could have that ""at work"", and plug in your otherwise modest, thin and light laptop to monitor, keyboard and GPU.",2.0
g6lp022,izq97n,"I will admit the idea of a corporation employing contractors to save money by not paying benefits etc is a little off putting, but I have to say I'm just so stoked to have my foot in the door in an awesome field. Also, California has some new laws that make it pretty hard on companies who want to take advantage of the ""contractor"" tax title. I'm honestly not too worried about it. I had a good interview process and really liked the guys I'll be working with. Is it ideal to buy my own stuff when I'm basically broke and just starting out? Not really, but it's a short phase in my life and will pay dividends later.

Thanks for the advice, btw. Have a great weekend!",1.0
g6lt4qg,izq97n,"what I was hinting at, penny-pinching employer aside, the ""balance of power"" is open to negotiation (I get it you might not want to negotiate any more right now) and it almost feels like some clueless accountant specified your Terms and Conditions. We've seen pretty much everything in contracts,  from full time staff being charged for the company t-shirt to random external workers being given expensive equipment for free, but common sense and/or law should dictate MLops is too integrated in the Enterprise to be treated as a visit from pest control ""come with your own rat poison"". Good luck in this first ML enterprising enterprise of yours!",1.0
g6kexek,izq97n,"Highest reliability is probably going with something like a HP, Dell, Lenovo with 24hr support and 3-5 year warranty, but thats not cheap. I run 3x Lenovo P720 and have 24hr support getting replacement gpus as loaners before you ship them back your own card.  These are the specs of my personal ML machines

dual xeon 4114 (20 cores, 40 threads, 96 pcie lanes) 

ram- 96gb to 192gb 

ssd - 1td nvme to 4tb raid-0 nvme 

gpu- 4x rtx 5000 and 2x rtx titan 

psu- 1000w 92% plat

&amp;#x200B;

If I were buying new desktop, then I would aim for 7 gpu slot SP3(epyc) board with 24Core epyc and enough ram for you use.  Get a 1200W psu for expandability to 2-3 gpus and see if you need 7 cards.  Getting 3080's would be the goal but you are going to have to wait to november for stock.

If you want to go laptop, then I like the lenovo P53 with rtx5000 or P15 with rtx5000.  You can get a lenovo p53 on heavy discount right now. 

For a ""value"" MLE build, 1900x, x399 taichi/ creation/ auorus , 4x 1080ti and 1600w psu all in one case. Probably get it all in 3k usd.",1.0
g6kiwv3,izq97n,Thank you. I will research those options.,1.0
g6kigzh,izq97n,"What is your budget? And is there any reason to restrict yourself to a laptop? Desktop is far better for performance, but if you need a laptop you just do...

 And I think you will get better advice on r/buildapc frankly about bang for buck. You need as many cores and gpu and mem as your budget will allow. Whereby you probably don't need as much as you think you do, the real bottleneck is the wetware, so get a couple of decent monitors.",1.0
g6kjrp3,izq97n,"Fwiw I have a 24 core threadripper 3960, 128 GB ram, 4 TB of NVRM and SSD mixed and a 2070 super that I probably will replace with something newer like a 3090.

But I could do all my work just as well with 12 cores, 64 GB, 2TB SSD and a 2060 Super. I think you could build that new for around 2400 USD.",2.0
g6kr514,izq97n,"I'd argue for getting 128gb ram. Many, many datasets I work with for customers are several tens of gb, and will comfortably fit in 128gb, but require that you pay attention when you only have 64gb ram. Most datasets I've encountered that won't fit in 128gb also wouldn't fit in 256gb. So I'd say 128gb is the sweet spot.",4.0
g6l4mmb,izq97n,"Yeah, I second the amount of ram for professional use.  

I have a client where the standard data set is 622gb, and other clients with a lot of datasets from 30gb to 150gb.  Being able to load everything is ram is really useful.  Otherwise I also use 4x 1tb nvme drive in raid 0 for cache.",3.0
g6kp2uu,izq97n,"Thanks, I'll definitely take a look at building my own. Those sound like good options.",1.0
g6ks0an,izq97n,"Don't skimp on the mobo, that is a pain to replace.

And the PSU. Those tend to fail to my experience.",2.0
g6kj2os,izq97n,"Not firmly established because I should be able to deduct the cost from my taxes. However I'd like to keep it within reason. A number I had in my head was 1500, but that's totally arbitrary.",1.0
g6kks4s,izq97n,Well definitely post this on buildapc and mention your software... like TF or R whatever. People there love delving into that and figuring out the tradeoffs. Also what market you are in. And check out pcpartpicker. Great way to compare alternative  builds.,1.0
g6nhhx7,izq97n,"Kind of out of topic but where did you spend your 300usd GCP, I mean what did you learn in the meantime? That would guide new learning people to get a job.",1.0
g6p0npa,izq97n,"Partially on online classes like Udacity's PyTorch course and partially on personal projects designed to bolster my resume. 

It took about 12 months from finishing my master's to getting hired. I spent every day teaching myselfs DL/ML, and I truly believe I got very lucky to land this job. I honestly think another year could have easily gone by if I hadn't gotten very lucky. Commit to the grind. 

Also get help on your resume. I thought mine was great, but it sucked. Like totally sucked.",2.0
g6kp3s9,izq97n,Build your own rig. Look at r/buildapc to get started.,1.0
g6jw3mz,izney5,I forgot to add that it is free,1.0
g6k0uti,izhxmn,check here: https://paperswithcode.com/task/image-captioning,3.0
g6jty9b,izhxmn,i am also interested in it.,2.0
g6kel7y,izhxmn,Sadly Tensorflow has only the implementation of show attend tell in their official doc,1.0
g6ivn62,izfqkk,"It really depends.

How different are the classes? How large are your images when being passed into the network? What kind of augmentations are possible for your specific data?",2.0
g6j21eo,izfqkk,Brain topography is the data and 11 emotions are classes. It seems from the literature that classes must have a significant overlap. Images are with resolution150x150x3. I can't perform any data augmentation as I cannot mess with the brain data.,1.0
g6jcyih,izfqkk,"Is the data set balanced? I would try using a few standard models first and see what you get, you can always train the weights from scratch as well instead of using pre-trained weights.",1.0
g6yraio,izfqkk,I have edited the question with the desired information. It might interest you.,1.0
g6jfxuw,izfqkk,That’s definitely a different use case to what I am familiar with. Honestly I would just give it a try with one of the state-of-the-art classifiers and see what comes from it. Something like [Efficientnet](https://keras.io/api/applications/efficientnet/) could quickly show you if your dataset is large enough.,1.0
g6yrb71,izfqkk,I have edited the question with the desired information. It might interest you.,1.0
g6z0hpw,izfqkk,"What loss function are you using? In the code at least I can not find your loss function. As you said in the edit, you reduced your 11 classes to two classes. So did you switch it to Binary_Crossentropy?",1.0
g70tpiv,izfqkk,I am using crossentropy as a loss functions. I think categorical crossentropy is the more general case which works in all the conditions even when we have only two classes.,1.0
g6l04jn,izfqkk,"10k samples? Either your problem is pretty simple or you will need to use transfer learning to get good results. 

Try out a small CNN with few layers, it's better than nothing...",1.0
g6yrc03,izfqkk,I have edited the question with the desired information. It might interest you.,1.0
g6im99u,ize0ak,"Most of the designing just come from building on top of previously established designs and just iterating through a lot of modifications until it works well.

If you want to start building start looking through the references of the papers for the cutting edge models. See what has been done before and use the higher level explanations as heuristics for combining and mixing different pieces together. Then keep testing and iterating until you have something that is better than previously established baselines.

A lot of the ideas for architecture  design is mostly just experimenting and trying out lots of designs. There's a lot of feedback loops between designing using heuristics, experimenting, and coming up with theory.",3.0
g6imyq1,ize0ak,Wonderful explanation.,2.0
g6io29g,ize0ak,"thanks, that's very informative, I'm just not really sure how to apply this on trading because as far as I know there are no cutting edge models that predict price movements but I'll see what I can find",1.0
g6ioka6,ize0ak,"You can start looking at posts and papers that use keywords like ""deep learning for algorithmic trading"" and search through their references. Trading is a bit tricky because cutting edge models are mostly proprietary, but the principles and designs are most likely built off of what is publicly available. My best guess is they do probably spend some time fine tuning the model, but a lot of what makes their models good is probably the types of data they curate and use.",1.0
g6ipjf9,ize0ak,"I totally agree, the kind of data they have is key however I'll figure something out with intraday data and a combination of alternative data",1.0
g6j3e0f,ize0ak,Take a full course on DL. It’s pretty much the only way you’re going to get the level of understanding you’re looking for here.,0.0
g6j5iwr,ize0ak,"In fact I already completed Andrew NG's specialisation as well as ml course on coursera, read several books, implemented yolo from scratch in tensorflow and I can't say I can design a neural network architecture but I know how things work that's all",1.0
g6j9h23,ize0ak,"I get that you've taken ML courses, but have you taken a DL-specific course, e.g. fastai? If not, I recommend that as the next step. Also, check out the book Deep Learning by Goodfellow, Bengio, &amp; Courville.",1.0
g6j9pdh,ize0ak,"Yes, i did complete Andrew NG's DL specialisation and sure i'll check",1.0
g6jwb8f,iz6k2e,"Do you have something to explicitly tell your computer to utilise all of your cores? 

If not you might want to look at implementing Dask.",1.0
g6hj4uq,iz09q0,"Thanks for sharing, I enjoyed that :)",4.0
g6exkro,iyujyh,I didn't think I would say that but it actually looks good!,2.0
g6fdj5q,iyujyh,"It looks like face recognition + something special there.. But what...

The fact that there is smooth face and hair colour transition.. Suggest it might be actually gan based and not just manual rule base",2.0
g6gny7b,iyujyh,"my mind is going to explode with this work, daaaaam good, Imagine to do an anime with this type of thing, I don't like particularly to see anime but for persons who likes anime their pepe is so hard like a leg of a table",1.0
g6ewh2x,iyub9f,"if you have one domain, but two tasks, i’d say you will be able to use elements of multi-task learning here.",3.0
g6ewrpg,iyub9f,"What you're looking for are so called ""Continual Learning"" methods, but I would strongly recommend against them unless retraining is really prohibitive. Continual learning mostly revolves around avoiding an issue called catastrophic forgetting, basically overfitting to the new data and forgetting some of the old data. You end up losing significant performance on the old data and achieving sub-par results on the new data. Whereas if you just retrained from scratch you'd get good results on both.",5.0
g6evkha,iyub9f,"Quite simply, just adjust the number of output neurons in the final Softmax layer. 

In your case, instead of having Softmax outputting two neurons, you adjust to the number of classes you want to model in the dataset.

Freezing the weights is useful if you want to build architecture on top of an existing ""backbone"" feature extractor, ie. Inception V3 or ResNet152 etc. 

This would ensure that the gradients and their updates would not be computed for such layers.",2.0
g6evs62,iyub9f,But wouldn't this require the whole model to be retrained? It shouldn't have to train on detecting the previous types of defects again (i.e. dents and scratches) but only the new type of defect,1.0
g6ewd7o,iyub9f,"Nope. Not at all. 

Given the assumption of common properties that you want to leverage with transfer learning, you have a number of choices.

1. Keep updating all weights and finetune on new examples
2. Freeze backbone architecture and compute gradients on top X layers in the architecture
3. Start from scratch by random initialisation of all weights and re-use backbone architecture (not recommended)

In all cases however, you need to adjust the output layer to conform with your ""new"" number of classes.",6.0
g6ewsjp,iyub9f,"So would you agree that instead of 1 output node with a sigmoid activation function, It'd be better to swap this layer to 3 output nodes with softmax (no defect, dent, scratch) and then add a new node for the new defect (in this case a change in color)?",1.0
g6exysy,iyub9f,"No, I would simply ~add~ modify the number of nodes corresponding to the actual classes you model. 

Softmax(No_defect_blue,no_defect_red, ... scratch_red)

If you add another node like you propose, you just get the probability of change in color.

EDIT: modify instead of add, for clarity.",2.0
g6f00jx,iyub9f,"I think I did not phrase the changing of color correctly, I'll try to be more clear. In this case, we e.g. add a black dot to the product to simulate ""any new type of defect"", we want the model to learn from a few new images with a black dot on the product, that this should also be classified as a defect, just like dents and scratches are types of a defect :)",1.0
g6ffqba,iyub9f,"Ahh I see. You want to expand the dataset the model is trained upon. 

Then I would go with proposition 2 above. 

Not knowing the depth post your InceptionV3 - If it is narrow (few parameters) freeze less layers, if it is wide, freeze more parameters. 

So just to sum it up, my recommendation would be:

- Keep your output layer as is
- Freeze low level layers, train a number of epochs with the expanded dataset
- Track your performance with datasets with and without the new anomaly, ensure you have checkpoints to backtrack.",2.0
g6f7r28,iyub9f,"I suggest you look up Subclass Distillation. This might help you gradually expand your domain without difficult training. In fact we found out that if you have a multilabel problem (which you're likely to have), in order to transfer this knowledge well without doing surgery on a model you will need a Subclass distillation-esque technique because softmax and sparesemax don't work, and BCE and soft margin loss is REALLY trash at distilling knowledge. You will need to simply distill the knowledge you have from your existing model, and then finetune the new one as you wish. Later you will be able to use more knowledge from your subclasses which you can distill into a new model and then finetune with the expanded domain. If your models are well initialized and not overfitted, then you will see benefits from the implicit subclasses, they're like a probe inserted into your model.

Otherwise just make your model an ensemble. It's idiot-proof and heavily backed by ML theory, but will have redundancy. In general continual learning is still in its infancy and what you're trying to tackle is something not even MNCs are that successful at tackling.",2.0
g6g3xg2,iyub9f,Train a binary classifier that handles the new class (is new class or not) if it is not then run it through your already trained net. Boom.,2.0
g6q7ic9,iyub9f,"Add a new column to the final linear layer. Let's say the current size of the final linear layer is MxN. Make it Mx(N+1). While training, freeze all conv layers. Only train the linear layer. After a certain numbers of epochs when the loss has stopped decreasing, save checkpoint, unfreeze the conv layers and resume training the saved checkpoint at a very low learning rate (1/100 of the initial learning rate).",1.0
g6ek79l,iyscql,Have you checked this out?: https://github.com/hassony2/torch_videovision,2.0
g6curfi,iyics7,"Here you go: https://roboflow.com

You are able to upload images, label them, augment data and generate .tfrecord files.",0.0
g6cm4zz,iyfop0,"Your demo is telling me that ""realDonaldTrump"" has no toxic tweets. I don't even have to test any other handles to tell that it's not working.",9.0
g6cmcj2,iyfop0,Hahah try fetching more tweets (on top selector),3.0
g6d38vm,iyfop0,Lmaooooo,0.0
g6du8bz,iyfop0,It's a nice project 🙂,2.0
g6dug77,iyfop0,Thank you very much!,1.0
g6ddbw6,iyfop0,This literally crashed my Reddit app,1.0
g6ddjj0,iyfop0,"😂 There are some warnings, in the page advicing you don't use your phone to perform it, because it takes so much processing",1.0
g6d44ox,iyfop0,"""Black people in America have the same right to be safe as all people, though this is hard to conceive when we think of the abhorrent killings of Breonna, Ahmaud Arbery, George Floyd, Dijon Kizzee and too many more""
This was detected as Toxic by @amnesty 
Was it a correct detection?",0.0
g6cy1dp,iyfop0,Why am I not getting any results with @AOC?,-1.0
g6c1wr1,iyae7q,You are using image augmentation on your training data. This makes the training task harder than the validation task.,4.0
g6cbnaz,iyae7q,So shall i remove image augmentation from train_data or add it in valid_data?,1.0
g6dk5lm,iyae7q,No that how it supposed to work . Do you know what  image augmentation  does?,1.0
g6epbg4,iyae7q,"of course! data augmentation it's a way to prevent overfitiing.

It takes images and does some edit like rotation or zoom or flip ..etc to make train data big as much as possible",1.0
g6ewmuu,iyae7q,Exactly the validation data is less complex than the training data. You neuranet  night have figure how to detect a simple image of  husky (validation)for example but stragle   in recognizing a croped rotated zoomed image of a husky.,3.0
g6f5xe9,iyae7q,"I'll try to increase the number of the validation set, and I'll remove the Augmentation from the training! 

and I'll see!",1.0
g6h57ep,iyae7q,"What, why?

That will make your model worse.",2.0
g6hb51b,iyae7q,"Then, what's the better solution to improve my model ?",1.0
g6hgwmi,iyae7q,"Keep your augmentations enabled.

And don't think too much about the validation. Human validation is more important anyway - you should judge yourself how your model looks.",2.0
g6bag7r,iy81uz,"I can't wait until we see it used in the next cop show ...

""Zoom-in, and again, yes, left, zoom in, the number plate, can you zoom in on that? Perfect, put out a BOLO for that plate.

Hold on, our target has a mole on their left cheek, I can see it in the reflection off the hubcap of that passing vehicle.""",6.0
g6b2542,iy81uz,"Project's website: [http://pulse.cs.duke.edu/](http://pulse.cs.duke.edu/)  


Try it now yourself with their demo on Google colab (upload an HD picture of a face, it will downsample it for you!): [https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true](https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true)  
 

*(note that it is impossible to reconstruct the exact same picture, but the results are quite impressively close!)*",1.0
g6aq6nk,iy4yw3,"It guarantees to return a valid probability distribution, something a simple regression does not. It is simple to implement (mind the numerical issues though), fast, and produces good results (which is always the most important aspect in DL).

On the more negative side, it tends to produce almost one-hot vectors, meaning that it is difficult to use the returned probabilities as some sort of confidence. It can also create strong gradients, since the preceding layer must output very large values in order for the softmax to converge towards 1. This is sometimes mitigated by setting the targets not to 1, but 0.9 (and distribute the remaining 0.1 over the other classes).",20.0
g6c0pn8,iy4yw3,"I hadn't heard that trick about changing the label, does that seem to improve ease of training primarily (or also resulting accuracy?).",3.0
g6du9oo,iy4yw3,It's called Label Smoothing. [Here](https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06) is an introduction and some papers as follow-up.,2.0
g6dwcve,iy4yw3,Thanks alot,2.0
g6aq4ah,iy4yw3,You can refer to chapter 6.2.2. (specifically 6.2.2.3.) from the [Deep Learning book](https://www.deeplearningbook.org) by Goodfellow et al.,13.0
g6dofik,iy4yw3,lol a bit of roundabout answer but still appreciate it,1.0
g6b9r1u,iy4yw3,"To add an obvious one to what others have said: it's differentiable :) But also has a nice and simple derivative.

I mean... what is the alternative?

You might be interested in knowing about sparsemax (https://arxiv.org/abs/1602.02068) and mellowmax (https://arxiv.org/abs/1612.05628) though the latter is mostly motivated by its use in RL.",5.0
g6b6m4s,iy4yw3,"It's the best we currently have for many problems. No, we have not proven that it is the globally optimal solution. You (or anybody!) are free to propose something better and prove that it works with experimentation. Often, good things come from questioning a field's defacto practice.",2.0
g6au4p6,iy4yw3,It is related to log-linear models. Softmax is the link function for multinomial regression in generalized linear model. And in many cases we are just dealing with multiclass classification in DL. So people use softmax as default.,3.0
g6bjedk,iy4yw3,"I think this is the most important aspect of the issue. There are infinite ways to normalize the output to obtain a valid probability distribution, but the connection with the generalized linear model gives softmax a nice theoretical background.",2.0
g6c1on9,iy4yw3,"This is the way I understood its significance too.  

The explanation that helped me get an intuitive understanding of link functions and using sigmoid to obtain valid probability distribution for binary classification was Josh Starmer's Statquest on [Logistic Regression](https://www.youtube.com/watch?v=vN5cNN2-HWE&amp;list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe&amp;index=2) explaining how it is used in Logistic regression for Binary Classification. 

I'm not too sure but the relationship between softmax and calculating probabilities in multiclass classification should extend naturally from this, right?",1.0
g6bpriy,iy4yw3,"I abhor arguments that amount to ""it's simple"" or ""it's well-behaved"" or anything other than ""it's correct."" There are so many dumb heuristics in this field because of that reasoning.

That said, softmax is well-motivated if you're familiar with physics (statistical mechanics). It's equivalent to something called the partition function, which is used to compute the probability of finding a (large) system in a state given the energy associated with that state. Here's my intuition:

Consider some problem involving a PDF.  It probably involves many features, which specify the kinds of things involved in the problem. These things can be associated with a large number of abstract facets: for NLP, for example, the words are the features, but this corresponds to many potential high-level concepts (like topic, sentiment, tone), there could be sarcasm or the expression could be idiomatic and thus non-literal. If we quantify these facets in hidden vectors, we basically have a large number of interacting hidden features that we must model statistically. If this were a physics problem, the correct thing to do is to take the system defined by the features, and write down the energies of that system for each possible state it could be in, then compute the partition function.  I don't think this should be different for non-physics problems, so the problem reduces to having a network compute energies given a defined system, and then computing the partition function (softmax) over those energies for whatever downstream task you want.",2.0
g6atnte,iy4yw3,"You can use cross-entropy loss with softmax activation. Softmax activation ensures that your outputs are always between 0 and 1. -log(1) = 0 implies that if you correctly classified, your loss will be zero. -log(p) gets arbitrarily large as p goes to 0. If your network output allows values outside the range of 0 and 1, the cross-entropy loss won't work.",1.0
g69lbll,ixygnc,os.getcwd()+'/'+folder\_path,1.0
g69ll9m,ixygnc,I am sorry can plz elaborate it more like where to write this code?????,1.0
g69p0rr,ixygnc,Python is saying that your file doesn't exist. So check the file path. Usually if the file does exist in the directory but python says it doesn't it's usually because you are running the python file from a different directory then you thought you were. You could just add the full directory instead of the relative directory to the file.,3.0
g69p7i2,ixygnc,Ok sir!,1.0
g6ah5bl,ixygnc,"It literally says FileNotFoundError.

Are you formating your path correctly? Is it an absolute path? Do you need elevated permissions to access it?",1.0
g6ahtoe,ixygnc,"You messed up slashes in folder names, try printing out folder name and path name. From cursory glance, it looks like your cat and dog folders endup without /",1.0
g6aimtb,ixygnc,"Print fpath first and show us!!

The very least debugging u have to do when asking us

🤞",1.0
g6au1o2,ixygnc,"You were right the file was not there where I was thinking it is!!!!
I got the correct file path by checking it one by one and I found it in some another directory!!",2.0
g6au31b,ixygnc,Great! And u learned by that,1.0
g6au8od,ixygnc,Yus.......Thanks a lot by giving me a hint!!!!,1.0
g6910c8,ixv4f2,"I would like to do a speaker verification model, what hardware would I need to have an acceptable model?

Apply contrast learning on spectrograms

Will I be able to do it on a gtx1060, or will I need a better graphics card?",1.0
g6a38p7,ixo019,For text classification you'd just have a linear layer at the very end of your network that outputs (features\_out) the number of classes you want to predict. You'd train this with crossentropyloss (or NLLLoss but you'd have to apply softmax yourself). Best bet is stick with crossentropyloss.,1.0
g6aqoiy,ixkkn3,"I’m not quite sure what you’re specifically asking. Do you mean different techniques used in CNN? Or CNN as a type of technique in learning algorithms?

What you just mentioned (convolution, relu, pooling, fc) are different types of layers and activation function you can use as a building block of a CNN.

From my take, the ones you mentioned are not really precise to describe as the “techniques” of a CNN.",1.0
g6ee3t9,ixkkn3,yes i want to know different techniques used in CNN,1.0
g6iexn4,ixkkn3,can you please tell?,1.0
g674xam,ixj12c,"Okay. So all of this is basically different ways to achieve the same result. Let's say you want to build a cat/dog deep learning classifier. For this you would want to build a CNN. You would make API calls and get the job done. Let's take keras as an example. Now currently keras has been integrated into TF completely. Keras is simply a higher level wrapper for TF. You can make API calls using keras to describe a simple CNN layer for you. But maybe you are not happy with it. Or you want to create your own layer, maybe add a more complex CNN structure to it. Then you should definitely look into tensorflow. For Keras/TF, it's basically various levels of abstraction in the framework. [This](https://www.youtube.com/watch?v=5ECD8J3dvDQ&amp;ab_channel=TensorFlow) video does a great job of explaining various abstraction levels in TF2.0

I would suggest, if you are just starting out, to start with Keras. Easy to use, loads of documentation and starter projects on github. And you can then move to TF or Pytorch. Basically to write low level ML/DL code, you use TF and knowledge of linear algebra to define your loss functions/ CNN layers etc etc. Stick to TF or Pytorch. I think a lot of companies use them.",5.0
g6799n3,ixj12c,"If you starting fresh, forget about google cloud ML or amazom sagemaker or recognition. First understand the basics like tensors, linear algebra, computation graphs of neural networks, forward propagation, backpropagation, losses and metrics. Then try building beginner friendly models, Like MNIST classification and try to understand the whole process of building a model,i.e data gathering, data cleaning, preprocessing, feature engineering, model building, training, evaluating model's performance and deploying the model. 
For this I would recommend you Tf2 which obviously inherits keras api or you can try keras also. But if you know python very well, like classes, decoraters, dunder methods etc go ahead with pytorch.
Now what's the difference between tf keras and pytorch.
I am talking about tf2 here which includes keras so I am gonna treat both of them as single unit. 
So tf/keras is very easy to learn and use. These frameworks are old hence you will get more resources for them easily. They are easy to learn. They have well maintained and industry supported frameworks. Productionizing your model using tf/keras is also easy.
Pytorch is new framework comparing to tf and keras.
But using pytorch will make you feel that you are actually programming a neural network. Using Pytorch is very to building a neural network from scratch. Trust me. I have been doing deep learning for almost 2 years and I started with tf but I didn't get the programmers feeling. I switched to pytorch because it was getting popular and lots of research papers were implemented in pytorch. But it is so systematic and well structured framework. 
Pytorch lacks in model deployment services, this is why pytorch is used more in research and academics field, while tf/keras is used more in industries. That doesn't mean that they aren't used in each others field. 
Pytorch gives you many functionalities like autograd nn module etc that will give power to your hand.

In simple words using tf/keras is like riding a taxi and using pytorch is like driving your own buggati😎

Or tf/keras :: Windows
&amp; Pytorch :: Linux

After getting your foundation strong, you can move to cloud services which you have mentioned as they offer very useful tools and services for your ML project like GPUs, model monitoring, CI/CD, hyperparameter tuning, model deployment on server, and stuffs like that.

Hope this clears your confusion!
Happy learning!",2.0
g63ocd3,iww40v,How deliciously biological,3.0
g66moat,iww40v,What do you mean? How is it more biological than any existing algorithm (I'm new in this field)?,1.0
g6o7uyv,iww40v,"It just seems to draw a parallel to the brains synapses which grow and prune.

Typically NNs are fixed in topology, and change only in weights. But the brain changes in topology and weight",1.0
g637w2o,iwv0f8,"I will wait for October, maybe they release the 3080 20GB.",6.0
g63k4fu,iwv0f8,"I’m doing the same, although I think it will be more November at the earliest, but maybe even next year. Just hoping.",4.0
g63wtd0,iwv0f8,Why not 3090? It has more CUDA cores and tensor cores. Then why are you focusing only on the vRAM?,0.0
g64mli9,iwv0f8,Because the 3090 costs double?,3.0
g64z1h4,iwv0f8,"2 3080s are almost double as fast as a single 3090. Also, VRAM is very important nowadays and is likely to be exponentially more important in the future. Tensor cores might make your GPU run faster, but lack of VRAM will make your GPU obsolete.",2.0
g62te0s,iwv0f8,"Definetely 3090, you get 24 GB of memory and don't have to deal with multi-GPU configurations. Due to doubling the number of cores that perform fp32 operations (aka cuda cores) the ampere cards are quite good in computation tasks (the 3080 doubles the performance of the 2080 in blender benchmarks).

Regarding tensor cores, I believe Ampere tensor cores can be used in fp32. There is no need in using fp16 to benefit from them like in Turing cards.",7.0
g62w62k,iwv0f8,"You should probably wait and see the 3080 Super benchmarks. With 20 GB of RAM, if it's like 1000$ or less it's better to get 2 of those instead of a single 3090 or Titan. Unless you want the NVLink. The 3090 is not that much greater than a 3080 silicon-wise.",3.0
g63i9v7,iwv0f8,Why 2? 3080 doesn't support SLI and doubt the 3080 Super will either.,1.0
g64x8np,iwv0f8,"You don't use SLI for machine learning, maybe NVLink, but it's generally not that great either.

2 3080 Supers will work almost twice as fast as a single 3090, will have almost twice as much memory (40 GB vs 24 GB), and is more likely to have good blower style 2 slots than a 3090. Also, who's to say the 3080 Super won't have NVLink? Not that you'd use it in most cases - even at work we don't use it.

Overall my recommendation is to get more GPUs rather than more memory, because 2 GPUs train faster than 1 can :) The only problem with this setup is PCIE bandwidth, with 3+ GPUs you'll probably need a Threadripper, but if you have 3 or more 3080 Supers that shouldn't really be a problem.

My optimal setup for a personal DL server would be this:

- Ryzen 9 3900 (or upcoming 5900)
- 2x 3080 Super
- 1x 3080 (for inference or a slave card for exotic architectures)
- 64 GB of RAM (128 GB if you don't want to stream load datasets)
- 1600W PSU with at least 80+ Gold
- 1 TB Samsung Evo 970 (upcoming PCIE4 variant won't have PCIE lane space on a Ryzen!)
- 4 TB WD NAS (long time archiving)

This would be adequate for someone who's looking to do mild research and Kaggle. Also, you'll likely need a riser for the 3080 to avoid space and temperature issues.",2.0
g65oyou,iwv0f8,Is there a release date for 3080 super?,1.0
g676v0b,iwv0f8,"Nope but it is assumed they will be announced after Big Navi reveal to crush them completely, so I guess expect it late November, and general availability early January (unless retailers can combat bots and scalpers this time)",1.0
g678auk,iwv0f8,"People keep saying that about scalpers but it doesn't make sense. There is a general lack of supply, even if there were 0 bots there would still be a shortage.",1.0
g678ibp,iwv0f8,"According to manufacturers, the amount of supply was ordinary. There would be a shortage but not this severe. It was not possible for as many aftermarket cards to be produced because the blueprints arrived late. Even if you gave nvidia a week or two to make more, the hype was just too much to handle.

These cards, on the other hand, won't be as hyped probably. But they'll be great value and probably in great demand because they're so adequate for AI startups.",1.0
g678qk6,iwv0f8,Completely agree. My point is the scalpers have been a scape goat. The hype was so great that even if scalpers didn't exist we'd still have a massive shortage,1.0
g67p9os,iwv0f8,Can you elaborate on the PCIE lane issue? When I go to build my new PC I'm leaning towards basically exactly what you outlined except only 1 or 2 GPUs not planning on 3+ most likely and was thinking if 2TB SSD instead of NAS but maybe I should consider that. Also undecided on whether I'll wait for 3080 super or not,1.0
g69mq2i,iwv0f8,"If you don't have enough PCIE lanes, your GPUs will suffocate...

Ryzen CPUs have 20 PCIE lanes. Usually 8 is enough to fully utilize a single GPU (as shown by benchmarks). If you get 3 GPUs, your system will NOT be able to utilize them fully if they're powerful GPUs, even more so if you stream a lot of data from the SSD. I don't know what happens with PCIE gen 4, it might require less lanes per GPU if it's faster. But notice how in my recommendation I have organized it so there's never more than 2 GPUs hitting full load for a long period of time. Even if you do inference on the 3080 asynchronously, inference doesn't actually require that much bandwidth, so it's not likely to clog up the lanes. Even if it does, it's not during the whole training.

Remember, NVMEs take up 2-4 PCIE lanes. A GPU can take up 4-8 lanes (depending on how many are left and how powerful the GPU is, ex. a 2060 would probably only need 4 lanes, but an RTX Titan and the Quadro cards might need even more than 8!). You do not want to buy a powerful card which your system might not be able to utilize fully because of bandwidth issues.

Not to mention clogging up PCIE lanes can lead to soft lockups on both the CPU and GPUs which will freeze your system and potentially require a reboot. I've seen problems on a Threadripper 2950 system when 4x 2080Tis are fully loaded: that thing has 64 lanes and we can only use around 40-44 with our setup!

You shouldn't get an SSD for long time storage as they aren't reliable. With an HDD you only have to worry about strong magnetic fields. An SSD isn't cost effective, will thermal throttle when moving big blocks of data and isn't beneficial outside of saving data (as for reading an HDD is enough). If you want 3+ GPUs for training (no slaves) my recommendation is to go for a Threadripper, which has more PCIE lanes. A 2000 Threadripper should be enough if you're on a budget, remember, you're not looking for performance as much as PCIE lanes per dollar. But most of the cost when buying a Threadripper is the motherboard. You should get a good one, not for overclocking, but a stable one. In my experience this is not a given! This is all of course assuming you don't plan on buying multiple 3070 or 2060, in which case - don't. Those amounts of VRAM are already obsolete, even the 10 GB in a 3080 is sketchy.",1.0
g69vch1,iwv0f8,"I'm probably going to wait for a 3080 SUPER or whatever and get 1-2,  AMD 5900, and look to spend &lt;$4k full setup. Not planning on any crazy long term storage. Doing both deep learning and gaming but am not doing much of either right now.",1.0
g6j5p4l,iwv0f8,"An Asus x570 ACE would provide 3 slots @ X8, gen 4. It will be more than sufficient for 3 cards.",1.0
g6jn2uz,iwv0f8,"Even though your slots allow for x8, doesn't mean the CPU will be able to handle 3x GPUs consuming 8 lanes of PCIE each. It will likely do 8/4/4 and give up to 4 lanes to your SSD, or 8/8/4 if you don't run an NVME drive. There is 0 chance you will get 3 GPUs all running at full x8 capacity because your CPU doesn't have that many lanes no matter what you do. Your only saving grace is if PCIE 4 lanes are fast enough so your GPU can't take up all 8 lanes. If they're double as fast and there is no overhead, that is a possibility, but still, you won't run at 8 lanes, it's just that the 4 lanes might run as fast as 8 lanes gen 3. I have yet to see a benchmark that demonstrates the maximum lane consumption of the Ampere cards.

For reference, a 2080Ti takes 8 lanes. So a 3080 would probably take up to 10 lanes, gen 3 (3080 bandwidth / 2080Ti bandwidth * 8). Which means that if PCIE 4 is double as fast, you will probably be on the edge, barely able to run 3 GPUs fully loaded and an SSD. For reasons related to soft lockups, I do not recommend this. DL is much more different than gaming, where a fully loaded card might still not fully consume bandwidth. That's why I recommend one gets a weaker 3rd card, it won't have as much bandwidth and so you are less likely to run into problems. Even a 3070 as the 3rd card would probably save you a lot of headaches.",1.0
g6of2yv,iwv0f8,"I didn't make myself clear. Let me address your considerations one by one.

1. &lt;&lt; Even though your slots allow for x8, doesn't mean the CPU will be able to handle 3x GPUs consuming 8 lanes of PCIE each. &gt;&gt;That board has been engineered precisely to allow for 3x8 no matter what. The first two slots will always go at 8x, gen4. Such lanes are provided by the CPU.The third slot will also go always at 8x, gen4. These lanes are provided by the x570 chipset.
2. &lt;&lt;There is 0 chance you will get 3 GPUs all running at full x8 capacity because your CPU doesn't have that many lanes no matter what you do. &gt;&gt;  
On the contrary, there is 100% chance. See above.
3. &lt;&lt; For reference, a 2080Ti takes 8 lanes. &gt;&gt;  
Where did you take such reference? How many lanes a gou would ""take"" is a function of the number of gpus installed.
4. &lt;&lt; So a 3080 would probably take up to 10 lanes, gen 3 (3080 bandwidth / 2080Ti bandwidth \* 8) &gt;&gt;

To learn a bit about PCIe lanes &amp; GPUs, start here:  [https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/#Question\_Answers\_Misconceptions](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/#Question_Answers_Misconceptions)",1.0
g6uiz1k,iwv0f8,"&gt; That board has been engineered precisely to allow for 3x8 no matter what. The first two slots will always go at 8x, gen4. Such lanes are provided by the CPU.The third slot will also go always at 8x, gen4. These lanes are provided by the x570 chipset.

Buddy. If a Ryzen CPU has 20 lanes, which is a hardware limitation, you WILL NOT be able to get higher bandwidth than 20 * max speed of a lane. This is not on a motherboard level - the motherboard could provide an interface that allows for 1000 lanes and it still wouldn't matter because the silicon itself is limited to 20. Like it's so sad how much misinformation you're spreading because you don't understand Zen 2...

&gt; On the contrary, there is 100% chance. See above.

No, there is not. AM4 supports at most 24 lanes. 4 lanes are reserved for chipset communication. 20 are left for NVME and the GPU(s). Ryzen 1000 and 2000 theoretically supported 32 lanes but could only use 24, again, due to socket limitations.

&gt; Where did you take such reference? How many lanes a gou would ""take"" is a function of the number of gpus installed.

People did benchmarks and ran the GPU in x16, x8 and x4 mode. x16 and x8 were virtually no difference, x4 was bottlenecked. This was PCIE gen 3, however, so the numbers are halved for PCIe gen 4.

&gt; To learn a bit about PCIe lanes &amp; GPUs, start here: https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/#Question_Answers_Misconceptions

You do realize this was written before the series came out and with disregard for specs, right?

To conclude, you do realize that what I'm saying is not only theoretical, but empirical too? You do realize that I've had experience with Ryzens, Threadrippers, i9s and Xeons, right? I experience issues because of this things. We not only see, but feel the bottleneck - you might not feel a difference of 10 FPS when gaming, but we sure as hell feel the difference when our model has to be trained for an additional day. Please, read up on the Ryzen architecture. Read how PCI-E works. Don't spread misconceptions because you fell for a marketing trick of 3 slots running a x8 when it is not physically possible! They can run 3 slots at x8 on gen 3 speeds, but that is just 3 slots at x4 on gen 4 speeds, which is obviously: not x8.

I talked about how on PCIE4 you could theoretically run 3 3080 Tis if the motherboard would allow for lane splitting outside of power of 2. In practice this doesn't work (or at least I've never seen it) - they will either run at x4 (and be bottlenecked at full load), or at x8 (where they waste around 3 lanes). But they WILL NOT run all at x8 because

- you likely have an NVME that takes 2-4 lanes
- Ryzen 3000 doesn't even allow for 24 PCI-E lanes because all you have are 20 lanes which the AM4 socket can handle",1.0
g6xpudl,iwv0f8,"Ok, as you say.",1.0
g7r3gn6,iwv0f8,With a pcie riser will 1x work or does it need to be a full lane?,1.0
g7rkjzf,iwv0f8,"Unsure what you're talking about. If you're talking about a 1x slot - then no, you can't use a GPU with that because it doesn't fit. If you're talking about 1x mode, it's going to be severely bottlenecked. Risers don't dictate what speed you're going as long as they have an equal number of lanes as a slot or more.",1.0
g7t3jdb,iwv0f8,You can actually buy adapter risers that support 16x on the gpu and 1x size on the mobo.,1.0
g63kiky,iwv0f8,I was wondering about this as well. How to use 2 3080s if they don’t support SLI?,1.0
g63p86q,iwv0f8,"Because 3090 costs about twice as much as 3080, so the price between 3090 and two 3080S shouldn't be that different. And if you put more than two air-cooled 3080 in a case you are likely to get heating problems. So two 3080S look like a perfect setup for a DL server.",1.0
g637ci0,iwv0f8,"I don't think the Supers are going to come out anytime soon. Probably they would, by the end of next year.",0.0
g637897,iwv0f8,4x 3090 with ZeRO optimizer = Life,4.0
g633iiy,iwv0f8,"Will have to wait for 3090 benchmarks to know for sure.  It seems like 3090 is way underperforming its price. Only looks to be 5-20% faster than 3080% for 2x price.  

If your models fit in the 10g or maybe the 20g 3080, that will be the best price to performance.  10G vs 24G is a big deal, really depends if you can fit your models in 10G",1.0
g637twa,iwv0f8,"That 5-20% is only for gaming. DL models that can harness the full 24G mem of the 3090 will see additional speed gains with increased batch sizes. Tim Dettmers says the speed up on doubling the batch size is 13.5%. So the expected speedup of a 24G card over 10G card would be (1.135)\^(0.5\*24/10) = 1.16 or 16% just from increasing the batch size alone. If we multiply a 20% speed gains due to the higher Tensor core count, we get 1.16\*1.2 = 1.4 or a 40% improvement over 3080.",3.0
g63x8t3,iwv0f8,What about Titan RTX? Will it be a good choice over 3090?,1.0
g64h7s4,iwv0f8,We will have to wait for benchmarks to be sure. Most users of the Titan RTX and of the V100 say that the 3090 will outperfrom the V100 easily. I too think the same. But only time will tell the truth.,1.0
g64i81j,iwv0f8,V100! That's too costly!,1.0
g6454yz,iwv0f8,I am hoping this is the case.  So far Nvidia has been a bit optimistic with their claims.,1.0
g64ipl0,iwv0f8,"In the A100 benchmarks, Nvidia has (optimistically) taken into account the max batch size that you can fit into its 40GB memory. To be frank, I would do the same if my model can scale accordingly, or if I am not aiming for SOTA. After all maxing out the batch size is something I do regularly at the prototyping phase (which is 90% of my research time). At this stage, I need the results to come in fast. It is only before a paper submission that I reduce the batch size or do a grid search over some of the common smaller batch sizes.",1.0
g64oif9,iwv0f8,"I think I might be ordering a 3090 on launch and testing out the performance uplift compared to my rtx 5000s. The RNN/ LSTMs performance will probably be worth upgrading.  I am especially interested in limited the power levels to about 290W - 300W and see if there is a greater performance delta between the 3080 vs 3090.  If the performance is reasonable at 300W, I might be building a 2-4x 3090 machine.

Are you waiting for benchmarks?",1.0
g65jpwa,iwv0f8,"I hope you will be able to nab one at launch day (really looking forward to some DL specific benchmarks). Death to the scalpers :D

I'm looking to build a 4x 3090 machine myself too. I am gonna wait out a bit though, probably till the end of this year, mostly to see how all other components of the build play out. So far I have shortlisted the 3970x on a TRX40 Aorus Master with 128 gigs of RAM and 2x 1300W PSUs on separate sockets. But that can change due to any of the following:

1. Zen3 launch: New Threadrippers? Price drops on old (Zen2) threadrippers.
2. Current TRX40 are only 48 lanes wide making for restricted 16/8/16/8 PCIe config on all mobos, even on the top of line Asus Zeniths. Would love to see an equivalent of the X299 SAGE that has true quad x16 support. It is unlikely though, since AMD has confirmed that Zen3 threadrippers would not change the socket type.
3. New motherboards with 3 slot spacing between the PCIe slots: All current TRX40 quad slot boards are 2 slot wide between the PCIe slots. This means that if I install a 3090 on Slot 0, it would cover up Slot 1 from being accessible. Even risers won't be able to get in underneath the card because they don't sit flush with the PCIe slot. The only solution would be to have 3 risers off the first 3 slots and the fourth GPU can take the fourth slot (Slot 3) directly on the mobo. Or we can have four risers on all four slots. But it would be nice if we can have Slot 0 and Slot 3 populated right on the board and take risers out of Slot 1 and Slot 2. This makes later installing an NVLINK easier.
4. Benchmarks: I care more about thermals and (especially) noise. Some 3080 cards like the Zotac Trinity are notorious for being nerfed at BIOS level simply because they are the lowest in Zotac's lineup. The TUF 3080 has made a positive mark in this regards, but it is about 2-3% slower than the X Trio. Some people say that 2-3% doesn't matter, but I think it does. Every 2% saves 30 mins by the end of a 24h run. With a slight overclock, a 4% OC can save an hour a day, which is equivalent to saving about a day a month or half a month in a year. Such stable overclocks require better binned chips which you can only expect from the top of the line models and not the MSRP ones.

Let me know your thoughts and suggestions. What system config would you go for, if you build a 4x 3090 yourself?",1.0
g66sjj2,iwv0f8,"I am thinking of going epycs or dual xeons for 128 lanes or 96 lanes.  Kinda given up on the idea of running gpus in case and on motherboard, so I will be using a mining case/ chasis to house 2x 1200w PSU and using extenders/ risers for cards. This is be easier to set up for air cooling.  Having 4x 300W+ gpu is like turning on my electric kettle(1400w) all day.  I cant even imagine water cooling it.  Cooling will be a bit easier on air and in an open air chasis.  

The problem with threadrippers are that there just isnt quite enough pcie lanes and there are no 1900x cheapo TRs anymore.  Although 8x pcie 4 ~ 16pcie 3, running 8x on TRX40 is not really a problem.  I am looking at getting dual epycs or dual xeon, and you can get 8x pcie16 motherboards with dual cpu, since I am running risers, it will be trivial to change a 4x gpu system to 8x gpu.   A bonus of running epycs and xeons is that there are a lot of data-center leftovers right now.  xeon platinum and ddr4 16gb ecc rams are quite cheap right now, so I might be heading in that direction for a ""value"" 96 pcie lanes, tho its will be pcie 3.0.

I am having the exact same concerns with current configurations of motherboard/ cpu to running 3 slot gpus.  There is a gigabyte 3090 blower coming out, but I really doubt it will cool 350W in a dual slot blower design.  If I am buying new, my specs are probably the same/ close  as yours, SP3 ROMED8-2T +AMD EPYC Rome 7232P+ 256gb to 512gb ram + space for 7x gpus",1.0
g67eowy,iwv0f8,"Same here. I am also going for a mining rig and full on air cooling. Open air cooling performs almost as good as liquid cooling.

Epyc setups are indeed much better for future expansion.

Thanks for your input.",1.0
g62vney,iwslx6,"Read Goodfellow's book and start reading papers. You need a good theoretical foundation to know what will work and a good imagination to get an idea of what might work.

Generally all architectures are improvements over their counterparts that didn't do the job well. MLPs are an improvement from a single layer perceptron with a shiny new nonlinear activation. CNNs are MLPs that can detect patterns locally. Resnets are very deep CNNs that use residual connections to fix the vanishing gradient problem. RNNs are MLPs that can process sequences well. Transformers are MLPs that can process sequences without recurrence in a GNN-like way. GNNs are MLPs that can work with graphs. All of these are improvements over something that didn't work well at the time.",7.0
g63dfh0,iwslx6,Thank You...I will start with the book and when I feel confident enough try to improve some old architecture which hopefully improve my insight,2.0
g62uqmh,iwslx6,"I've struggled with this myself. Implementing a known architecture? Easy. Understanding what of that architecture works well for it? Also easy. Understanding all the bits and pieces and their purpose? Also not too hard. Building my own model architecture and improving on what is currently available is extremely difficult. People get paid a lot money and earn a lot of prestiges for doing so. I try to deeply understand why one type of model woks the way it does, then think how to improve it, and run as many experiments as I can to see what works. Then in the end the baseline model still arguably outperforms mine. I have no idea honestly",4.0
g62v3v3,iwslx6,"I have designed my own before and here's how you can too;

1) pick an existing architecture that is generally used to solve the problem.

2) LEARN EVERYTHING ABOUT IT IN AS MUCH DETAIL AS POSSIBLE.

3) try to do it on paper (understanding the math) to see how the input transforms into the output step by step. You can use small dummy values and variables.

4) meditate over the process and see how you can improve it.

5) talk to experts. Doesn't have to be ML people, I've found people good at data structures and algorithm can provide good insights to ML problems.

6) keep thinking about it while doing other menial tasks.

7) hope lol

Note: when making new architecture I'd recommend using pytorch over tf and keras. Keras works best for basic models but if you wanna create something brand new it's a pain.",6.0
g63ddk3,iwslx6,"As stated in other comments, read papers to know which methods work and which do not.

But to be fair I think building a deep learning architure is a lot of wild guesses based on intuition (therefore not necessarily rational).

A teacher of mine once told me that Google deep learning engineers often test a lot of ""random"" architectures at the same time and write a paper about the one that performed the best. Given the fact that they practically don't have any limit in computer power, they have the luxury to do such a thing when smaller labs cannot. Is it true or just a bit of jealousy I couldn't tell, but in my experience DL architectures are often, indeed, a random guess that worked on the test set.",2.0
g62nwui,iwslx6,fastai,-1.0
g62taue,iwslx6,Im not looking for modules or APIs to make my own models..Im looking to learning how to plan and go about designing an architecture...,2.0
g63h6yk,iwslx6,Terrible APIs,1.0
g60xz9t,iwkrfm,"You would need a really big computer to do that by backpropagation, remember that in bp you  must instantiate the whole model on parallel instances and make a layerwise weight correction. Maybe for probabilistic models or by using other training algorithm that allows to reduce the dimensionality of the network there couls be benefits.",3.0
g62mye4,iwkrfm,"Not backprop,  but RBM-style models can benefit a lot from specific types of computers called quantum annealers as mdoels are well aligned to quantum process",3.0
g618fek,iwkrfm,MIT &amp; IBM did [a study](https://www.nature.com/articles/s41586-019-0980-2) where it was shown that supervised learning performs better than on Classical Computing.,2.0
g60ta2v,iwkrfm,"No one knows the answer yet.

Or maybe everybody knows, I just haven't looked into it deep enough.",3.0
g61i4o8,iwkrfm,"To add, it would probably be better. It'w hypothesized that in order for the network to work well, it has to have good data flow. Among other things, what's needed for good data flow is reversibility. Given that quantum operations are reversible because they do not destroy information, a quantum layer would have such a property. But there are many other problems before we get there from what I understand.",1.0
g61iz1d,iwk50x,"This is pretty neat! But I must ask, why does this have to be done with a neural net? Aren’t there algorithms that can do this much faster?",3.0
g61k7g0,iwk50x,"Yes it can be done procedurally, but that is normally based in perlin noise and similar technics which ends up looking randomly generated and not realistic. A terrain model created by an artist generally looks better and less random. A neural network has the potential of learning from that and generate terrains of similar quality.",4.0
g61xbpz,iwk50x,Ah I see. Thanks for the explanation! :) they do look great indeed!,1.0
g61eks8,iwk50x,What does epochs mean here? Is the model supposed to simulate geologi movements and erosion?,3.0
g61gz9j,iwk50x,"As u/nmkd already said, epochs represent the times the entire training dataset was processed.

The model is supposed to generate new terrain 3d models, because the models used for training already have the results of erosion and sedimentation modeled, it is supposed to learn that too.",2.0
g61evlp,iwk50x,"Epoch is an ML term.

One epoch = The training dataset was processed once.",1.0
g60m6dx,iwk50x,ML Terraform is a Neural Network Terrain Generator trained on handmade 3D models. More details at: [https://apseren.com/mlterraform/](https://apseren.com/mlterraform/),1.0
g648p7z,iwk50x,"This is really cool. I have two questions:

What does the ""DC"" in DCGAN stand for? (Forgive my ignorance!)

What do you see as some of the potential applications of artificially generated terrain? Are there any specific corners of industry who have signalled a genuine interest in this innovation?",1.0
g64kvy0,iwk50x,"DC stands for Deep Convolutional.

In the field of game development in particular, terrain generators are quite used and the ones that do it procedurally and completely without the artist intervention (no stamps) produce very random looking results. Some indie developers in particular would prefer to focus on the programming part and use a more automated tool to generate the scenarios instead of having to learn and get proficient in terrain 3d modeling.",1.0
g64rd4m,iwk50x,"That's interesting, thanks for the response. 

I wonder as well if you might find some interest in the geoscience community. In the past I have studied flood inundation modelling.  One of the aspects of that and also other earth sciences (such as landslide modelling) which garners interest is the fabrication of large synthetic datasets. These allow you to understand what permutations of features have more proclivity towards flooding and similar hazardous processes. This might be an area you would find interest from users.",1.0
g6532yc,iwk50x,"Thank you for the suggestion, seems an interesting idea. I think the difficulty of that would be to get good datasets of floods and landslides. Do you know any?",1.0
g68yfrs,iwk50x,"If you go on Google Scholar and search for ""Global/Continental Flooding/Landslide datasets"", I am almost certain you will find some papers of interest. There might not be open source datasets but you will find the academics who are the gatekeepers of that data and they're normally quite open to collaboration.",2.0
g68ydlj,iwk50x,"If you go on Google Scholar and search for ""Global/Continental Flooding/Landslide datasets"", I am almost certain you will find some papers of interest. There might not be open source datasets but you will find the academics who are the gatekeepers of that data and they're normally quite open to collaboration.",1.0
g6arszs,iwk50x,"Cool works!! 3D GANs seems still in its infancy and your works look promising. Definitely would follow your works more.

May I ask a bit about the model? What kind of 3D model inputs/outputs you used (like a pointcloud or mesh models)? And do you require any special NN architecture for it? 

I’ve had random ideas to use arch. like PointNet which geared for 3D pointcloud for generative purpose, though haven’t actually put the time to experiment with it haha.",1.0
g6c2ekc,iwk50x,"Thank you. The model is a regular DCGAN with some modifications, I have experimented with other architectures but had worse results, complexity generally is not desirable if you do not have the infrastructure to train a large model. The dataset was converted to a point cloud with color information per point.",1.0
g6dtsse,iwk50x,"Ah I see, thank you. Keep up the good work!",1.0
g62c1y3,iwfcm1,Watching Obama turn into Trump was the stuff of nightmares,1.0
g63wyqh,iwfcm1,Sorry 🙂,2.0
g64kqb4,iwfcm1,Code?,1.0
g5zmis7,iwczxf,"Resnet + squeeze and excitation, efficienet is good on paper but slow when trained",1.0
g5zo7u7,iwczxf,What are squeeze and excitation? Are these augmentation techniques? Any papers I could read?,3.0
g605wez,iwczxf,"Its not an augmentation technique. It just a normal CNN with extra layer which happen to weigh each conv layer. It supposedly captures the global dependencies. Although I've personally trained it, and it could't outperform ResNet for what I was using it for.",2.0
g63hu1a,iwczxf,"Man resnet with se outperforms vanilla resnter, they show the results in the paper. It is an attention technique",1.0
g5zr3a3,iwczxf,does Resnet perform better than vgg? particularly vgg 19?,3.0
g609pe4,iwczxf,"Absolutely. [https://www.researchgate.net/profile/Frank\_Koss/publication/320084139/figure/fig2/AS:543716588744704@1506643544031/Comparison-of-popular-CNN-architectures-The-vertical-axis-shows-top-1-accuracy-on.png](https://www.researchgate.net/profile/Frank_Koss/publication/320084139/figure/fig2/AS:543716588744704@1506643544031/Comparison-of-popular-CNN-architectures-The-vertical-axis-shows-top-1-accuracy-on.png)

This picture only shows performance on imagenet but the results will likely be similar for your application and the efficiennet paper has a similar graph.  Models without residual connections usually get worse once you start adding a large amount of layers, usually \~16 depending on what layer types you're using because of the vanishing gradient problem.  You'll often see VGG16 perform better than VGG19 on several datasets.  With resnets you'll effectively sidestep that problem and you can always go deeper.",1.0
g61l9br,iwczxf,"For SOTA check out PapersWithCode, they're great for beginners and will usually lead you to the implementations of the networks you're looking for.

As far as I know, resnets and retinanets are the kings of anything face related.",1.0
g5yuezb,iwcp5m," Project page (paper &amp; code coming soon, as per the authors): [https://research.cs.cornell.edu/crowdplenoptic/](https://research.cs.cornell.edu/crowdplenoptic/)",1.0
g5yqek1,iwbxqe,"Depends on what language you’re using, but there’s plenty of guides on running a python app in Flask, and then making an API that you can query for predictions.",2.0
g5yqhra,iwbxqe,Yeah I use python,1.0
g5zne6d,iwbxqe,Streamlit,2.0
g5ydtb3,iw9y4o,"I'm also i want to build an object detection model using YOLO.
But I think if you build it from scratch it will be very difficult (its model simple like any CNN model).
I'm a beginner in this field and i'm trying to read as much as possible of articles to understand what really Yolo is.
If we have any simple idea we can work together.
Thank you",1.0
g5zhfkx,iw9y4o,"Not exactly object detection, but segmentation made easy and explained line by line here: [https://www.youtube.com/playlist?list=PLZsOBAyNTZwYuFfht61R0b-N1TNIX5\_Vy](https://www.youtube.com/playlist?list=PLZsOBAyNTZwYuFfht61R0b-N1TNIX5_Vy)

Tensorflow object detection API is easier for object detection because of scripts: [https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/)",1.0
g5xwmth,iw7uuy,"*Beep boop*

I am a bot that sniffs out spammers, and this smells like spam.

At least 100.0% out of the 9 submissions from /u/samboylansajous appear to be for courses, coupons, and things like affiliate marketing links.

Don't let spam take over Reddit! Throw it out!

*Bee bop*",19.0
g5y6ekm,iw7uuy,Haven't checked the actual content yet but very well done! Most deep learning courses guide the student to training the model but not how to deploy the model to a real use case. Your niche on bringing the AI to Flutter App is cool!,3.0
g5xsaov,iw7uuy,Thanks a lot for make it free!,2.0
g5yfae9,iw7uuy,"i dont know flutter, do i need to learn it first before this course?",1.0
g5ys30p,iw7uuy,"Knowing some flutter would be helpful! But if you knew OOP, you would be fine!",1.0
g61b2ra,iw7uuy,Thank you !,1.0
g8oa9wt,iw7uuy,"Thanks so much for sharing the course! Is the coupon code still active, or did I miss out :(",1.0
g5y5bi3,iw7uuy,"I was trying to start flutter deep learning, it will be a good start I guess. Those NNs are good choice.",-4.0
g5zb9g2,iw7uuy,why the hell I got downvoted.,1.0
g5utn84,ivzbug,You have posted this every day for the past three days...,3.0
g5t5lvy,ivp8y0,What classifies someone as a beginner vs intermediate?,5.0
g5t7tiu,ivp8y0,"Not sure how to answer that one in an exact manner but let me try. 

If you know Python and how to search for stuff on your own (Proficient at Google lets call it that way 😂) these projects are considered to be beginner level (especially MNIST classification). There are many step by step tutorials out there just google it. 

The idea here was to give you some rough guidelines but not to explain exactly how to do it.

MNIST classification using off-the-shelf models can be done im 50 lines of code if you search it up. Like query ""MNIST classification in PyTorch.""

Intermediate is somebody who has done all of these. 😅

Hope that helps!",3.0
g5tjfz0,ivp8y0,Please tell me it wasn't too hard. If you have any feedback for me it's super appreciated!,1.0
g5urkue,ivp8y0,I still need to go through it in more depth although I was just asking out of curiosity! I have been a full-time python SWE with ML for 3 years but haven't touched neural networks yet really at all so I am planning to start a side project soon for fun.,3.0
g5utj1s,ivp8y0,"Awesome glad to hear that! Now I get your question, unfortunately the word beginner is overloaded (in the C++ sense of the word 😅).

These are not ""beginner"" in the sense you're new to programming altogether, more of a beginner in deep learning and you want to gain some confidence before you start tackling some harder problems. So they could still be of use to you!",3.0
g5uuxoq,ivp8y0,Perfect! Appreciate the responses,2.0
g5uv4rg,ivp8y0,You're welcome!,1.0
g5srff4,ivp8y0,\*still NOT an ex-Google engineer. If you have any questions I'll be monitoring the post!,3.0
g5ssqmp,ivp8y0,"soon to be ex google engineer😂! Have you been using PyTorch from the very beginning, or you migrated from Tensorflow? And what do you think about Tensorflow 2?",3.0
g5st28p,ivp8y0,"Hahahaha If I say that then I'll soon become an ex-Microsoft engineer and not by my will! 😂

Noup, I was using Keras and TF in the beginning. I was less knowledgeable back then so it does not count as much. 😅

All of my more serious ML projects I've done in PyTorch. But it's super easy to switch especially if you know ML.

Here is what I think about TF vs PyTorch:
https://youtu.be/NuJB-RjhMH4",3.0
g5sirzr,ivn79x,"It is hard to answer without having more details. Let’s go step by step.
Do you need AWS? Depends if you have a decent server on your own you don’t need it. A machine with a couple of GPUs with 8+ GB each should be enough for your problem.
You have to use a pretrained model, it would be stupid to start from scratch. No matter the task, you can always import weights from another model.
Definitely use an existing python library, I suggest pytorch. Forget tensorflow and Keras, unless you need to deploy to mobile. Nevertheless, everything you need was already implemented, it’s just a matter of putting the code together and write a data loader specific for your data.",1.0
g5t8uto,ivn79x,"This seems one sided. 

While pretraining is often valuable, it may not be in some contexts ( imagenet was trained on things at a local scale, medical content / 3d content / content from aerial / microscopic content plays with different rules ). You should explore with and without. If you have 1 mil images, and your time is more valuable than figuring out how to pretrain, just let it run longer ( no harm ).

Keras/tf2 is super easy to start with, and honestly has magnitudes of more examples than pytorch. I learned on keras to start, and i still feel pure tf is too much for me and pytorch looks like a globby mess ( IMHO ). I like keras in the way that it forces you to "" think of your layers "", and then it becomes a model.

You don't need multiple GPUs or a cloud account some where. If you have an nvidia card you are already in great shape, if not it's still ok to run on your desktop ( it will just take 10* longer ). But check out Google or Kaggle they both offer free use of their hardware on a juniper like interface. 1mil images wouldn't exhaust your training time per week.

I really recommend visiting keras-io or keras github pages. There are basically methods to do image classification prebaked for you ( some using pretraining too ). Copy that code and your images over to colab and bingobango.",4.0
g670um8,ivn79x,"How much of an advantage, in terms of time, does using a pre-trained model have over creating a model from scratch? 

Thanks for the insight on Keras and tf. So the issue we're facing in hiring rn is that the companies we are trying to partner with to create this solution, are quoting exorbitant rates and time periods. My question here is, with my understanding of IR and ML, this sort of a solution, should not take too much time correct and the cost of developing this shouldn't be very high either right? Or is my understanding on this wrong?",1.0
g678c79,ivn79x,"Well I personally advocate in house solutions always. The second you are willing to sub, then you are talking about overhead with such a solution. 

The sub is probably going to offer pretraining and that gives hefty price tag for validation of claims. It also reduces time of training down significantly. I would be less interested in how long it would take and more interested in what the classification / loss rate is. This is probably where you ought to be interested.

If you are thinking of this as a bid perspective: ask for a ratio of validation accuracy / time to completion and then what is that worth to you.

Is finishing the project tomorrow, next week, one year worth X,Y,Z dollars.. extrapolate.

Is finishing within 50%,75%,90%,or 99.99999% feasible for a solution?",1.0
g67azwr,ivn79x,Thank you!,1.0
g5sldre,ivn79x,"Would using a pre-trained model essentially mean that I would have to use transfer learning and customise it for my need?

I do require it to be deployed on a mobile, so would you suggest Keras/Tensorflow? Any reasons why to choose one over the other?

All in all, is it relatively safe to conclude that developing such a model should not take very long?

Edit - Should I consider making use of OpenCV as well?",1.0
g5soms8,ivn79x,"1. Yes, use transfer learning to avoid training from scratch and save time. It has been proven that on the long run it does not improve performances when your training set is big enough, but it can lower the training time from a week to a day.
2. I am not an expert on mobile development. What I know is that since tensorflow is developed by google, they provide several API for mobile development since they are in that business. Pytorch is developed by facebook which doesn't care about mobile for now. If you develop an app using Flutter, which is also developed by google, it has a native integration with tensorflow. Forget Keras, that's just a toy. 
3. Yes, it's quite standard nowadays and honestly, you could do it yourself without any programming experience by using tools like AWS CustomLabels or Google cloud AutoML(?). However, these tools are relatively new and expensive. I suggest to ask a freelancer who can guide you through the process and suggest you a solution in case the model does not reach the required performances.",1.0
g5rxqmz,ivjuv5,"If you have control of the annotation task, then annotate both the bikes and their riders. Then you can detect both. If not, then it sort of depends on the goal of your model. Do you really need to know where all exposed people are for safety reasons? Does your model not drive a vehicle or something that needs the context and information provided by knowing someone is riding a bike? Then just treat them as the same class as person.",1.0
g5s3aew,ivjuv5,it's an analytics model. No being used for driving. But the goal is to get better accuracy for all 3 classes.,1.0
g5qucd6,iv7xct,I'm so excited to use this once I get a new GPU,1.0
g5puo4r,iv3rnz,"Yes, GPT-3 is a better chatbot. It is trained on a much deeper corpus and, in effect, uses longer Markov chains, and has more capacity to refer back to previous keywords. But since the underlying logic has not changed, GPT-3 has all of the same problems as the thousands of predecessor chatbots.

All chatbots are impressive for the first (x) seconds or (y) sentences, at which point the narrative loses coherence and you see that there's no logic or direction, it's just babbling. GPT-3 increases (x) and (y) over GPT-2, which also increased (x) and (y) over earlier iterations - but the mechanics and results have not changed.

But there is one part of GPT-3 that *is* technically impressive and an advance in the field: its ability to model language at a basic level.

Past chatbots had serious issues producing valid sentences, because concatenating sentences based on overlapping fragments is a clever parlor trick but inadequate for real communication. If you staple together parts of the sentence: ""the dog had a bone in its mouth"" and ""a bone in my foot is broken,"" you get ""the dog had a bone in my foot is broken,"" which doesn't scan. GPT-3 produces surprisingly few of those gobbledygook sentences, which raises the prospect that deep learning may be capable of modeling the mechanics of writing.

That is an interesting result, and one that we've seen repeatedly in the story of deep learning: In tasks such as computer vision, speech recognition, and walking models for bipedal robots, the best deep learning models qualitatively outperform the best hand-coded algorithms.

So, yes, GPT-3 does not move us closer to artificial general intelligence. Unscripted machine-learning conversational models still cannot conduct meaningful discussions, even about simple topics like the weather. But GPT-3 raises a new prospect that *if* we can find ways of modeling the *underlying semantics* of intelligent conversation, then we may be able to use algorithms like GPT-3 to generate expressions in the recipient's language.

Also, dividing up the problem that way provides a promising advance for universal translation.",12.0
g5oze26,iv3rnz,Of course we have all seen plenty of human writing that didn’t evidence thinking either.,25.0
g5p2p9p,iv3rnz,deep,6.0
g5ps3iu,iv3rnz,Variational,7.0
g5r2qyv,iv3rnz,Auto,5.0
g5r38i1,iv3rnz,Bahn,8.0
g5py6o0,iv3rnz,"I am not saying they are wrong, I am obviously not even remotely knowledgable in any way to even have an opinion. But whenever someone comes out to shoot down the idea that something not human could have a characteristic that we humans hold, it just feels full of hubris and overly myopic. I think we hold ourself too seriously (as a species) and we are not as expeceptional as we think we are.",3.0
g5pzlu8,iv3rnz,"Almost every AI scientist worth his salt has held this opinion for a long time. GPT-3 works by making relational connections - not causal connections. If you see the word “hello” frequently appearing before the word “there”, then that’s what you’ll choose. It’s not a matter of deeper thinking if you are choosing algorithmically.",1.0
g5q0ibn,iv3rnz,"&gt; Almost every AI scientist worth his salt has held this opinion for a long time

The opinion of the article or the opinion I voiced?

Again not an expert, but don't we do that already? Unconsciously make decisions based on something we learned? (\*cough\* algorithmically \*cough\*).",3.0
g5s6b3w,iv3rnz,"Yeah, but you don't compute gradients and matrix inversions when deciding whether to shit your pants or hold it in.",1.0
g5ro0zk,iv3rnz,"Humans can write like humans,   
but don't mistake that for thinking.",3.0
g5qb7jb,iv3rnz,"It cannot ""write like a human"". Don't kid yourself.",1.0
g5rcwhu,iv3rnz,"Yes. Put another way: 

It's great at \*mimicking an understanding\* of the universe though language. 

But that does not mean it \*actually understands\* the universe and can express it through language. 

It's a fancy look up table.",1.0
g5s9vuf,iv3rnz,Biological brains are a fancy lookup table.,1.0
g65xoem,iv3rnz,Then why can’t lookup tables do thinking ?,1.0
g5r3z2k,iv3rnz,man the state of nlp really is depressing and my boys ain't afraid to say it,0.0
g5ouxz5,iv3jc4,Lol good luck getting any RTX cards in 2020 let alone 5 lol.,3.0
g5ovott,iv3jc4,"It's not time sensitive, it can be done in the next 4 months. Could you answer by other questions? Thanks",1.0
g5q0p8i,iv3jc4,"This is a barely thought out and very broad question. You need to do some research about computer system working, build, components and their compatibility. Decide on the rest of your build and then, come back and ask about GPGPUs (if you still have a question).

If you don't want to research yourself, go to r/buildmeapc or contact a company which will build them for you.",2.0
g5pg89b,iv3jc4,Just wait for 3080s to get more in stock.,1.0
g5qe5w6,iv3jc4,"This guide can help with some basic orientation in multi GPU rigs
https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/",1.0
g5qt2l3,iv3jc4,"hmm do it ML style.  SGD.  

buy a 3080 and a 3090.  do some runs. buy the next set of cards that minimizes your loss(time).  Then buy some more until your loss is a local min",1.0
g6wo2mq,iv3jc4,This will be one of the most expensive training.,2.0
g6y67j0,iv3jc4,Good thing I didnt suggest a adaboost procedure.  Think about all the extra cards.,1.0
g6312c7,iv3jc4,Yes great advice,1.0
g5y5uk5,iv3jc4,"Really depends on your budget, the types of models you want to train, if you’re building separate machines for production and deployment, etc.

In general though, more vram is better. 3090 has a lot more Vram than the 3080 (24 GB vs 10 GB). There are rumors of a 3080 variant with 20 GB coming soonish though. 

If your particular workload doesn’t need as much VRAM, then the twice as many 3080s would be better. If you need the extra VRAM, then go 3090s.

If you’re trying to build a cluster of GPUs (like a data center) then you can’t use consumer grade GPUs like the 30series anyway.

First I’d start with your CPU and motherboard, that’d dictate how many PCI-E lanes you have, slots, and potential cooling issues.",1.0
g6311dr,iv3jc4,Thank you very much!,2.0
g5qlp53,iv34y2,Can you do object detection on the same dataset ? Thanks,1.0
g5qlx1y,iv34y2,"In what context? I could probably do it on something that contains objects. Are you looking at distinguishing different parts of the lungs? If labels are provided, it should be doable, I am guessing.",1.0
g5qmcsa,iv34y2,Like maybe a fracture in bone annotations provided ofcourse. But how many images do you think it takes to train a model? I have trained 6k images and my model doesnt detect anything out of dataset. Sometimes it doesnt recognize a fracture in test dataset too. So I was wondering which model you would use to detect a shape in xray images. Since all xray are black and white.,2.0
g5sa5g5,iv34y2,"That's a great idea to detect fractures. 6k images with data augmentation should be enough. Did you use the ImageDataGen object to augment the images? Also what were the issue? Underfit, overfit?",1.0
g5sad3t,iv34y2,"I used augmented data, inspite of that the accuracy is terrible. Like in one photo (ik1) the fracture is on thumb finger. But when you flip the image the fracture is on middle finger(with less confidence) . I'm not sure what I should do there",1.0
g5sc4bw,iv34y2,"How many layers? And did you experiment with dropout, batch normalization, weight decay, early stopping and other optimizers and activation functions? Sometimes what I do is see what weights have been assigned and do the math to see what features have been detected.",1.0
g5ozjki,iv2daq,"Awesome tool, highly recommended!",1.0
g63khqe,iv2daq,"up to 100 images on the desktop version, plus the other posts you've made have obvious fake accounts commenting... ""Super !!!!"", ""Fastest"", ""Awesome tool, highly recommended!"" all by new accounts with no posts. Forgive me for being skeptical but this just seems like spam, how is this tool useful and a better alternative to free/open source tools when the desktop version is limited to 100 images.",1.0
g5y708u,iuzayw,"Drivers are typically the same across board manufacturers. The only software that varies between boards is the bios for the board, and that doesn’t matter for the OS. You’ll be fine using the proprietary Nvidia drivers on Linux",2.0
g5oouoi,iuqfna,"Certain libraries might prefer Intel's MKL (which is worthless on AMD chips) over ATLAS or OpenBLAS which is a minor note. Obviously obtaining a three-generation old CPU in decent condition might not be easy either. You won't have access to PCI-E 4.0, if you care about that.

It seems a poor choice to hunt down such an old chip (on a dead-end motherboard, since the only upgrade path is other 1000 or 2000 series threadrippers) for an otherwise very high-spec machine. Why not go 3900XT on the consumer platform? or keep to 2 3080s and 128GB RAM for now and go current-gen threadripper? upgrade the ram/GPU when you need it",1.0
g5meadi,iuqfna,[deleted],0.0
g5mk21f,iuqfna,"Actually you can use tensorflow and pytorch on AMD gpus on linux using Rocm, excluding Navi cards at the moment, on Windows you can use tensorflow-directml to run on AMDs cards as well. There's a news project that aims to run MLops on top of Vulkan, if that gets adoption these frameworks could run on any CPU or GPU and any system.",1.0
g5noy0y,iuqfna,Sadly this takes time. You are probably talking of another 2-3 years until we see some sort of adoption.,1.0
g5nrlh4,iuqfna,"I have to agree, CUDA is already pretty old. If Pytorch team put some effort into using AMD cards this could quickly gain adoption, also AMD seems to lack interest in selling Deep Learning cards to the mainstream, Rocm is a great start although it doesn't work on Windows, so is almost the same as Directml that only works on Windows, but Vulkan works on both you can check their repo in the link below.

[https://github.com/EthicalML/vulkan-kompute](https://github.com/EthicalML/vulkan-kompute)",1.0
g5n86iq,iuqfna,Its a CPU \^\^. I meant for preprocessing etc in general for a workstation.,1.0
g5oo76l,iuqfna,Good thing the 1920X isn't a GPU then?,1.0
g5lzc8d,iuog81,"I think you’d be better off picking up linear algebra, statistics, probability and computer programming.",3.0
g5lzldw,iuog81,"Yes but as the main intention is to create a artificial human brain, so don't we need to study it a little bit?",2.0
g5m5uui,iuog81,Do you want to study gastroenterology before allowing yourself to cook breakfast?,4.0
g5m7hit,iuog81,not the whole of it but i'd surely want my dietician to know the digestive system and it's disorders !,1.0
g5m5jen,iuog81,"Certainly and there are some parallels and in some AI takes inspiration from the brain.  Even hardware developers are looking to CPUs that operate more like a brain.  But neurology as a starting point, I’d say no.  Those other disciplines I mentioned are a better foundation I’d say unequivocally.",3.0
g5lmsnm,iumvgf,"Hi all,

Following the amazing turn in of redditors for previous lecture, we are planning another free zoom lecture for the reddit community.

In this next lecture we will talk about a new a new research on semantically understanding visual scenes, in part based on the CVPR 2020 paper - Hierarchical Conditional Relation Networks (HCRN) for Video Question Answering. The lecture is titled: ***Visual Question Answering Based on Image and Video***. 

The speaker is the researcher and the paper's author.

&amp;#x200B;

**Lecture abstract:**

Deep learning has recently achieved remarkable successes and become a de facto approach to many computer vision problems. Its superb performance is, however, limited to tasks mostly requiring visual perception. It is still very challenging to solve tasks requiring new knowledge acquired through multi-step inference.

In this talk, I present our research on learning to reason visually by asking machines to respond to a natural question based on knowledge presented in a visual scene, either from a static image or a

dynamic scene from a video. This visual question answering task is multi-disciplinary by nature, which constitutes the high-level understanding of both vision and language, hence, considered to be a good proxy for visual reasoning.

git: [https://github.com/thaolmk54/hcrn-videoqa](https://github.com/thaolmk54/hcrn-videoqa)

arxiv: [https://arxiv.org/abs/2002.10698](https://arxiv.org/abs/2002.10698)

&amp;#x200B;

**Presenter BIO:**

Thao Minh Le is currently a second-year PhD student at Applied Artificial Intelligence Institute, Deakin University. He works on how machines learn and reason about the world from what they see. His interests are in deep learning and its applications to computer vision and bio-medicine.

Going back in time, he obtained a Bachelor of Engineering from Hanoi University of Science and Technology in 2014 and a Master of Engineering from Tokyo Institute of Technology under the Japanese Government MEXT Scholarship Program in 2018.

Thao's git: [https://github.com/thaolmk54](https://github.com/thaolmk54)

&amp;#x200B;

**Link to event (October 7th):**

[https://www.reddit.com/r/2D3DAI/comments/imw1ed/visual\_question\_answering\_based\_on\_image\_and\_video/](https://www.reddit.com/r/2D3DAI/comments/imw1ed/visual_question_answering_based_on_image_and_video/)

&amp;#x200B;

(The lecture will be recorded and uploaded to Youtube. All previous lectures and recordings can be found in our sub-reddit: /r/2D3DAI)",1.0
g5k1ess,iubv8x,Sadly the website was so obnoxious with popups and newsletters on mobile I couldn’t read the article :(,1.0
g5jth4k,iu9fwy,Use arch based system. Ubuntu has gone to shit recently - especially their package management.,1.0
g5kijn2,iu9fwy,"lol don't go with arch you'll want to kill yourself with the cuda mismatches 

New cuda version ? congrats , rapids and co do not work anymore",0.0
g5kv1mq,iu9fwy,"I'm already with it. It's pretty easy to set up the right cuda version though. 🤷‍♂️
And the package management is awesome.
If you use something like Manjaro, then you don't need to do set up anything at all.",1.0
g5llcds,iu9fwy,“TensorFlow Deep learning Setup using GPU” by rexdivakar https://link.medium.com/TFceZAZgR9,1.0
g5lp2vh,iu9fwy,I tried doing `sudo apt install cuda-10-1` and it says `unmet dependencies: cuda 10-1 depends: cuda-runtime-10-1 but it is not going to be installed depends: cuda-demo-suite-10-1 but it is not going to be installed`,1.0
g5k2g2f,iu9fwy,Which python version does ubuntu 16 have?,0.0
g5k7qpc,iu84zu,"Honestly, my understanding is that only TensorFlow and PyTorch are now used in the industry. By learning how to work with these two only, you should be covered!",1.0
g5kc8xr,iu84zu,"I have tried pytorch and Tensorflow. I would recommend pytorch as is more friendly to python programmers, and on the other hand I kind of found Tensorflow on a rather stiff side. By default tf takes all the GPU memory on initialisation of a layer function and won't let any other tf script(even if it only contains ""import Tensorflow""), maybe it could be changed, but I have never tried it. And yes building a final pytorch model is kind of hectic as compared to keras(a framework on top of Tensorflow). So for short or on the go projects I use keras, or just to build a dirty implementation (as Andrew Ng said). 

But believe me if you know the basics and if you have determined what you want from the framework it really doesn't matter which framework you use.

But in the end I would like to say pytorch just works the right with its very flexible nature and can be easily used for production.",1.0
g60ns8o,iu84zu,How do you serve pytorch models in production? And building a final pytorch model is hectic?,1.0
g60ot57,iu84zu,"I can't say hectic, more like elaborative and efficient. Well the weights can be saved into h5 files, can be used in javaBased programs. But I have heard that Tensorflow is better in terms of production (maybe because of Tensorflowjs...)",1.0
g5kg83f,iu84zu,"Haven't tried MXNet, in fact, this is the first time I've heard of it. I guess it's irrelevant for academia and the industry. I started with Keras before it was in TF, and had a taste of pure TF. I work mainly in PyTorch now.

At this point, the only weakness of PyTorch is that you can't really use it for deployment as well as you can with Tensorflow (because of Tensorflow Serving). But you need to know PyTorch because nowadays everything is written in PyTorch.. Tensorflow as Tensorflow barely exists because it's a heavily autistic construct, neither adequate for Python nor for research, it's mostly Keras nowadays that's being used. I would not bother with Tensorflow or Keras until you master PyTorch - it has way too many gotchas, while PyTorch provides better high abstraction with practically no gotchas. The only time I mess around with it nowadays is when we complete research in PyTorch and have to deploy it for a release - we then either transfer the weights we have to TF or train it etc. But that is pain. Also, I was reluctant to use PyTorch after using Keras for 1.5 years. After trying PyTorch once I never looked back, if that means something to you in terms of my bias. The main reason being I didn't have memory leaks in PyTorch where in Keras it was a daily sight with ResNet models.

Also, the benefit of Tensorflow is that it works (almost) flawlessly with Rocm (pre-Navi AMD cards), while PyTorch is a bother.

**tl;dr** learn both, use PyTorch.",1.0
g60nusw,iu84zu,"When you say everything is written in pytorch nowadays, can you give me some examples?",1.0
g61hj8n,iu84zu,"New papers with implementations are almost exclusively in PyTorch, very rarely in TF/Keras.

For NLP just look at huggingface transformers - although generally providing both TF and PT implementations, the PyTorch ones are available earlier and are more stable.

For a general view, go to PapersWithCode and look at what each paper is implemented in. If you go to trending, 1 paper is TF and the rest is PyTorch before you have to load more. If you go to newest, most papers are PyTorch or custom implementations (some ML stuff). I think the last relevant paper I read that had a TensorFlow implementation before a PyTorch one was BERT, and even then the TF implementation was so broken it was basically unusable before the PyTorch implementation. I remember seeing half the latency on a PyTorch BERT implementation as opposed to Kamalkraj's graph mode TF2 BERT implementation lol

Overall it's kind of hard to show you all these examples because papers with implementations are somewhat rare and I don't save or memorize the papers I've read, but browse away on PapersWithCode, or check out GitHub repos, it's easy to see what framework is in the majority.",1.0
g5j6gzj,iu84zu,*frameworks. There are others as well.,-7.0
g5i45te,iu0yz5,"&gt;My main academic interest is in deep learning  
&gt;  
&gt; I'm looking to get a new MacBook for University

You ought to pick up a decent Thinkpad and install Ubuntu on it.  Not saying a Mac won't work for you, but you'll be starting in the wrong direction.",7.0
g5k4uaw,iu0yz5,"I wouldn't do this. Don't use Linux as your daily driver unless you want to spend time fighting your operating system. The world expects you to be using either Mac or Windows - there will be documentation for Mac/Windows, the software that your professors pick/write will probably be tested on Mac/Windows. Go with one of them. I suggest Mac. 

No matter what laptop you get, it probably won't have a GPU large enough to actually do real DL on, so the OS is irrelevant (you will be using Linux on GPU machines, but they will probably be remote).",0.0
g5ki2z3,iu0yz5,"I disagree. Deep learning (and development in general) is often done in Linux. Even in remote settings, the OS is more than likely going to be Linux (usually Ubuntu). By making it your personal OS, you're (a) eliminating any software incompatibilities (a huge issue if you use Windows) and (b) getting practice with Linux (which, if you look around at job postings, you'll see ""Linux experience"" is pretty important). 

Presumably the OP is going into CS. At my university (and it would seem many others), most of the CS classes will assume you're using Linux. Folks who use Windows or Mac would either have to translate instructions to their OS or (usually) use a virtual machine running Linux. I agree with your advice that you don't want to be fighting against the software/docs professors give you, but in CS, that software is more than likely expected to be used on Linux. 

&gt;No matter what laptop you get, it probably won't have a GPU large enough to actually do real DL on, so the OS is irrelevant (you will be using Linux on GPU machines, but they will probably be remote).

I agree, but having familiarity with Linux is the important bit. Even in remote settings, you need to know how to navigate the OS, and if you've been using a Mac (or God forbid a Windows), then you're going to have to also learn Linux to work with these machines. Nobody is running remote Macs, and Windows isn't suitable for development.

If the OP really wants the option, they can dual boot. But I've been using Ubuntu as my main machine since my junior year of college, and aside from playing games or wanting to use Photoshop (things I've never needed for school), it gets the job done, and well. My Linux experience makes me a much stronger developer, especially in remote environments, compared to those who exclusively use Macs and definitely those who exclusively use Windows.",1.0
g5i5c0u,iu0yz5,"Any sort of legit DL will not run well on a laptop. I would recommend looking into Google Collab as its free for most reasonable tasks, runs on their servers, and provides you with a UI in chrome/safari, this way you could run models using an Apple Watch if you really wanted to.   For running stuff like Jupyter notebooks and blender, an air *could* get you through, but it’s likely to be painful. 4 port pro is your best option if you’ve got the $$",2.0
g5khuua,iu0yz5,"I've worked on all possible OS combos possible for DL during my uni days. Here's my 2 cents

1. Pick an os you're super comfortable with. Learning a new OS always has a hit on your productivity. If you've used OS-X all your life 100% get a reasonably priced Mac. Don't even think of anything else.
2. If you're free to explore, within the same price point as MBP you can get a ThinkPad or a Dell XPS, both equally portable, and the XPS also being MUCH MORE powerful (i7 + 1650Ti)
3. Ideally you shouldn't be doing any training on your laptop. In my workflow I get the model ready for training (dry runs + checking for numerical stability on dummy data). And then run it on GCP/AWS/University Clusters.
4. People hating on &gt;insert os name here&lt; are just projecting thier own biases (funny given this is a DL forum) so take that with a grain (or a heaping tablespoon) of salt.

good luck for your uni!",1.0
g5kkldf,iu0yz5,"As someone has previously said, don't get a Macbook, at least not until you've settled down. When in uni, you need freedom, something Macs generally don't give to you.

My recommendation is to get a Thinkpad or a Carbon (Thinkpad is a de-facto engineering standard, Carbon is a sleek laptop). If you're thinking about a Dell as a Macbook replacement - don't, their customer service and quality control is not at an acceptable level (first hand experience). Also, from my experience, get something without a dedicated GPU, and with a good screen, battery life and good thermal performance. I have an XPS with a 1050 and even when undervolted it still shuts my PC down when it overheats. If you can't fully utilize your GPU, what's the point even? Dedicated GPUs in sleek laptops are a tiny bit more powerful than AMD's APUs.

Now to address your concerns:

1. no laptop will be adequate for deep learning, at least this decade's deep learning - you do that on a remote device or a desktop/workstation
2. no, but you can do some small stuff at least - Macbooks WILL NOT handle the load or heat of deeplearning even 5 minutes, while some laptops might
3. uhh not really, deep learning is about the most intense it gets in uni, there might be some doctorate classes that you'd probably do on a supercomputer if you're lucky, but I guess you might've picked up the pattern by now - you're not doing it on a laptop
4. then get a Ryzen laptop (potentially with a dedicated GPU) - it won't be anywhere near as good as a desktop, but might work given adequate cooling

Keep in mind that if you really really really wanted to do DL on a laptop, it's probably more cost efficient to get a CPU only laptop with a TB3 port and hook up an external GPU to it - that way you get max performance at low heat and a good price. The price premium you pay for an eGPU case is about the same as the premium you pay for a laptop GPU, minus the overheating and increased power draw. The only problem is, however, that your limited to pretty much Intel CPUs, which are trash currently. I think there are one or two Ryzen laptops with Thunderbolt 3, but don't remember if they're even that good.",1.0
g5onucj,iu0yz5,"I have just finished a masters in Data Science and will offer my 2 cents. I had a MacBook Pro which was the ideal choice. As you write, a lot of models needs to be run on a cloud resource. However, most models you will run will most likely be small and can quickly be run on the local machine. You will test a zillion different models, and the quickest is just to run it locally. At least that’s what I did. Of course the bigger models need to be run on the cloud.

Avoid Windows as the pleague. The poor students that used Windows (or rather started out with Windows) had a horrible time get everything up and running. So by the time I was done with my homework, they were still struggeling with getting the environment up and running.

Linux is a viable option. However, is harder to work with than a mac. But if you are comfortable with Linux, just go for that. If not, go for a Mac.

But again, avoid Windows regardless of how powerful computer you can get for a lot less. Just about nothing in data science will be easy on a Windows computer",1.0
g5suyvk,iu0yz5,any recommendations on specs. what were your specs. im thinking i7 1TB 32gb memory,1.0
g5syqs6,iu0yz5,That looks like a good choice. Memory is usually a lot more important than processor. I made it thru my masters with a computer with lower specs than what you are considering,1.0
g5j22tw,iu0yz5,"Okay soo a bit to unpack here:

1. A macbook pros are very powerful, but they aren't used for training deep learning models since they don't have nvidia gpus. The GPUs shipped in the pros don't have apis for the deep learning frameworks (tensorflow, pytorch, etc) to leverage, so you're training with CPU-only. The cpus in modern macbook pros are very powerful.
2. Yes, students often train models on their home machines. It's especially nice for learning and debugging. But, that being said, if it's possible to do the debugging and learning on a cpu and switch to a cloud gpu for your ""big data"" jobs.
3. I'm not sure where you are going to university, but no, most computer science courses won't require more computing power than an air. I wouldn't recommend an air, though, for different reasons (see below).
4. Yes

So, based on my experience, I'd recommend you get the higher end macbook and assume you'll be doing your large deep learning jobs on a cloud gpu. My rationale is:

* University is typically 4 years. You'll start to notice the less power of the macbook airs in 1-2 years. Get a pro with a lot of RAM (32G at least) and you'll have a machine that will get you through your 4-5 years at university.
* Most university computer science homeworks don't require a lot of cpu/etc, but as a technologist, you'll find yourself frustrated by being limited. Also, if you're like me, undoubtably you'll write something at some point that isn't very efficient, so the extra power is nice :D
* If you're concerned about portability, the modern 13 inch macbook pro is only a pound or two heavier than the air. It's still very portable. I just got one for my spouse (who used to have an air) and they didn't mind the weight for the added power.
* I find the mac user experience easier than maintaining a linux install on a laptop just for deep learning training. This is my opinion, I certainly know people who disagree, though. A thinkpad with a GPU is likely going to save you some money over a mac, but I'd caution you to be sure you're comfortable maintaining a linux laptop before counting on it for university. I love linux for my servers/etc, but not so much for my get-work-done machine.

Ultimately your education will likely involve only a small number of resource-intensive deep learning projects and, while it would be nice to have the capability to train locally, you'll likely value the other perks of a higher powered machine for your non-deep learning projects (or even your non computer science courses) more often.Source: I'm a university adjust professor who teaches computer vision and does deep learning for his day job. I use a 2019 16"" macbook pro.",1.0
g5huv3f,iu0yz5,"If you have a good budget, I would get a MacBook Pro, but maybe not highly upgraded. Even though DL models are trained on GPUs and in cloud, you often need to debug your code and model locally, and a slow computer will really bottleneck you here. So while the best of best is not needed, I wouldn’t skimp too much on processing power.",-1.0
g5hv4ns,iu0yz5,"Thanks for the reply. With regards to how much upgrades, what do you think of i5 vs i7 and storage?",1.0
g5hx36h,iu0yz5,This is for undergrad? i5 and 256 or 512gb would be enough imo,2.0
g5iatpg,iu0yz5,"Note that it is also dependent on the model of i5 or i7(not all are equal), and a NVIDIA GPU would also be beneficial, you don't have to go hard with a RTX 3090 but anything relatively modern with decent libraries will do.

For OS me personally I prefer to use Linux Ubuntu dual boot machine, anything UNIX based is the way to go.",2.0
g5j0bf3,iu0yz5,"Laptops have a really low lifespan if frequently and constantly used for DL training. People use PCs with multiple GPUs for serious DL stuff.

And, if you want to buy a high-end laptop, go with ThinkPad (safe option) or Asus (their products are really good rn).

Your uni will likely have resources for doing DL.",0.0
g5jrfh9,iu0yz5,"You might want to check OS support for the tools/drivers you want to use, because macOS isn't generally super well supported in comparison to Ubuntu/Linux in general. Though if you're wanting to train any models larger than toy ones, then you'll be wanting to use a PC/cloud service with a GPU anyway.",0.0
g5gjq09,itt5al,"Paper: https://arxiv.org/pdf/2004.00452.pdf

Demo: https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt

GitHub Code: https://github.com/facebookresearch/pifuhd",1.0
g5hyt4c,itt5al,Thanks for uploading this,1.0
g5g6t82,itoiev,Like github?,2.0
g60ti5z,itoiev,Check out Omdena,1.0
g5fwfnb,itlejn,"The bias term is redundant because batch normalization effectively removes its effects as you pass from one convolutional layer to the next.

Think about it. The bias term just creates an additive change in the outputs from the previous convolutional layer. Batch normalization centers and normalizes this output before it is used as input for the next convolutional layer. The centering effectively removes the bias term, so it's redundant to have this extra variable during training.",3.0
g5he6on,itlejn,"Thanks for your responses! What if there is no batchnorm in any layer, yet we set the BIAS = FALSE, how will that affect the network and what will that mean for its performance?",1.0
g5g2mfo,itlejn,"Its not actually about DCGAN as such... Bias in any layer followed by batchnormalization does nothing... Since the mean of the bias across a batch is that bias itself... So that would be subtracted automatically when subtracting the mean ...

Keep in mind though... if you're doing Batch Normalization after the activation layer... Then bias is not wasted... As before.",2.0
g5heae7,itlejn,"Thank you! What if there is no batchnorm in any layer, yet we set the BIAS = FALSE, how will that affect the network and what will that mean for its performance?",1.0
g5hge1j,itlejn,"In that case... the bias wouldn't be wasted... so in such cases when there's no batchnorm you should indeed use bias parameters for better accuracy.

I think you still didn't clearly understood my previous explanation... otherwise you could have answered this que by yourself.

Watch the andrew ng courses in improving deep learning models... there he cleary explained what happens to the bias terms when you us batchnorm.",2.0
g5em0xe,ithesd,"Only if you've seriously overfitted your data. Otherwise all they'd probably be able to extract is features from the training data.

If you want to protect your model, encrypt it and only provide access to it by a public interface pointing to a virtualized code segment, which then interfaces with the model. Virtualized software protection is a pain in the ass to reverse engineer.",4.0
g5emlyz,ithesd,"Generally no. It’s sort of like trying to reconstruct a face from a shadow - models are lossy compressed representations of the data. 

IMO you need to provide incentives if you want people to complete 15 minute surveys.",3.0
g5eo6md,ithesd,"No.
Yes.",1.0
g5g09k5,ithesd,"Kind of...

I would recommend you to read this paper: https://arxiv.org/abs/1709.07886

There the authors try to get information out of a trained ML model perceived as ""black box"" where they only have access to inference. Then the could get information out and regenerate important features of the training data set by querying the blackbox with different combinations. It's not perfect but at least for the computer vision use-case they present, it yields pretty impressive results. They also proposed some white box approaches where they have more information about the type of model used.

Just see the Figure 3 on page 10.",1.0
g5gkkgr,ithesd,"Yes and Yes 

Not only can you just replicate the model by just imitating it but you can also extract secrets. You can even get GPT to spit out passwords and usernames if you want",1.0
g5ed1ji,itgjpz,"Pretty pointless. I guess you used Video Enhance AI?

It really loses detail if you upscal more than 2x. 8K is just overkill here.",1.0
g5eepzz,itgjpz,is there anything better than AI Video Enhance?,1.0
g5efhde,itgjpz,TecoGAN or EDVR,1.0
g5enkf4,itgjpz,are these programs free? where can i download them?,1.0
g5esuhr,itgjpz,GitHub.,1.0
g5e8luo,itezec,"If the target is one label per sample, the output comes from the output at the final timestep. Otherwise for outputs per timestep, you need a total loss against the targets (the number of targets need to match the output sequence length). For simplicity and stability, many training models use the input context to predict the final output only. If you don’t you may consider using curriculum learning or teacher forcing to help the model learn later timestep predictions when the earlier prediction sequence is incorrect.",1.0
g5fl12a,itezec,Thanks!,1.0
g5e8ec3,itcf71,People still use it?,1.0
g5fh0vg,itaprw,"Hi :) I'm happy you found your niche of interest.

Actually, both robotics and ML can overlapp more than what you think. Of course you would need to learn some background in ML, but you can use it in robotics.

Particularly, you might want to read about Reinforcement Learning, which allows machines to actually learn from mistakes (neural networks are good fit here). Furtthermore, you can read about evolutionary robotics, it's a very interesting branch of ML applied to robotics.

I believe that Pieter Abbeel from University of California, Berkeley, is one of the leaders of ML + Robotics. You can find good resources in his website.

Take care!",1.0
g5cj4ii,it7i4h,"This is a talk from GOTO Chicago 2020 by Davis Sawyer, co-founder and chief product officer at AI software startup Deeplite. You can find the full talk abstract pasted below, give it a read before jumping into the talk:

The emergence of deep neural networks (DNNs) in recent years has enabled ground-breaking abilities and applications for modern intelligent systems. State-of-the-art DNNs have been found to achieve high accuracy on tasks in computer vision and natural language processing, even outperforming humans on object recognition tasks. Concurrently, the increasing complexity and sophistication of DNNs is predicated on significant power consumption, model size and computing resources. For example, since 2012, the training complexity of AI models has increased by 350,000x. These factors have been found to limit deep learning’s performance in real-time applications, in large-scale systems, and on low-power devices.

Furthermore, many low-end and cost-effective devices do not have the resources to execute DNN inference, causing users to sacrifice privacy and offload processing to the cloud. Application developers, software engineers and algorithm architects must now create intelligent solution that deal with strict latency constraints, such as in smart city, mobility and healthcare applications which often require that inference be performed in a matter of milliseconds, often with limited hardware.

To do so, we will take a look at promising new ways of using AI to help human experts design highly compact, high-performance Deep Neural Networks on cloud and edge devices.",1.0
g5co97a,it69fl,"Closest I think of is Disentangled VAE, maybe can apply to other networks",1.0
g5fied0,it69fl,"Thanks, I gonna read some papers on Disentangled VAE.",1.0
g5ck8wu,it4ao2,"Hey mate - I wouldn't say those architectures are so much tweaked for specific datasets as much as they are purposefully designed to be both efficient and able to propagate gradients into deeper layers (residual connections, batch norm, etc).

So in my opinion, it is probably best to stick with a well known and researched architecture and just fine-tune it to your specific dataset (which is what you've been doing).

If you want further optimisations and have a relatively simple dataset you can try to remove deeper layer groups from your favourite architecture, or use the fundamental block of that architecture to build a new stubbier model - but you'd have to be careful not to blow out your final fully connected layers if you try this.",3.0
g5cl3li,it4ao2,What do you mean by blowing out your final fc layer?,1.0
g5clc8c,it4ao2,"""blowing out"" the number of learnable parameters in fc layers :)",3.0
g5cpbbo,it4ao2,"You're right, I shouldn't have said it's ""tweaked"". I meant, because the ImageNet has 1000 classes whereas I need 2-10 classes, I  thought of maybe removing the deepest layers before the classifier head, like you've suggested. I might try additional optimisations sometimes in my own time, I'll stick to the known models for now. Thanks for the advice :)",1.0
g5jhq0p,it4ao2,"You should read this article:  [https://keras.io/guides/transfer\_learning](https://keras.io/guides/transfer_learning)

You can set the pre-trained models to trainable after you've trained the last 2 layers and eek some performance out of the model. It explains why this works better in the article. You can even just set some of the deeper layers to trainable if you want to see what happens, though the article doesn't cover that, I imagine it's just manually setting layers to trainable. Works decently. Removing some of the layers is not something I've seen to be honest, though I'm an amateur.",2.0
g5k9u6m,it4ao2,"I've been doing that, actually. Tried just the top, setting the deepest layers to be trainable and setting the whole model to be trainable.  I wondered if other approaches could get me to higher accuracies because the objects in the dataset aren't so similar to ImageNet. I haven't tried training from scratch because I doubt that around 100k images would be enough.   
As for changing the architecture, in the paper [Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf) the authors use the model as a feature extractor for object detection and state they reduce the channel counts in the deepest layers because the COCO dataset has 90 classes. So maybe I'll try that some day.",1.0
g5dbvsq,isxemm,"Cuda and cudnn versions mismatch, kindly install tha appropriate version and then run it.",1.0
g5dhycb,isxemm,"I just installed the cudnn for Cuda 10.0, copied the files from it to the Cuda-10.0 folder, and yet I still get the same errors as before",1.0
g5ac6lc,isu4mj,What do you mean by two networks separately? You do something like this with the merge function and set it to  perform a merge by summation.,1.0
g5adrma,isu4mj,Thanks for the reply.  I am confused about how I should insert this variable u into the structure.,1.0
g5adw7x,isu4mj,Is it a single scalar number?,1.0
g5aext2,isu4mj,"It is a scalar but is a changing variable.  Below is the code that I used to do concatenation between a(x)+b(x) . I am just confused how to insert u into the structure. 

&amp;#x200B;

**def** build\_model():  


   A = Input(shape=\[3\])  
B = Input(shape=\[3\])  
a1 = Dense(32, activation=**'relu'**)(A)  
b1 = Dense(32, activation=**'relu'**)(B)  
c = concatenate(\[a1, b1\])  
O = Dense(1, activation=**'linear'**)(c)  
model = Model(inputs=\[A, B\], outputs=O)  
model.compile()  


**return** model",1.0
g5bu6sx,isu4mj,"There you're concatenating, not adding, a1 and b1. You add two tensors by using this:
https://keras.io/api/layers/merging_layers/add/

If u is a learnable variable then the only way I found how to do that is with this:
keras_var = tf.keras.backend.variable(np_init_value)

See here:
https://www.tensorflow.org/api_docs/python/tf/keras/backend/variable",1.0
g5abj3a,istrr5,"If you look closer all those posts link to badly edited YouTube videos (video taken from paper then shitty music added) or they link to blog posts (mostly on medium.com) targeting beginner and amateur audience, and filled with ads.

In other words, they are spam. You can also check the posters' history if you still have doubts.",3.0
g59k9tc,isqjgd,Even after reading this article I am still torn about whether to get 3080 or splash out for the 3090. My thinking is that I could sell 3080 if I ever feel like I need more VRAM but if this is going to happen then I may just as well get the 3090 at the start and not worry about it.,9.0
g59oaa9,isqjgd,"The 3080 is fantastic for convnets (as long as you're not working with massive training data like that found in radiology). A lot of academics use the RTX 2080 TI for state-of-the-art research. As a disclaimer, I'll add that we are currently doing some research with convnets on video that requires 48 GB of VRAM.  


If you're doing NLP, I'd recommend the RTX 3090 though.",5.0
g59oqr6,isqjgd,[deleted],2.0
g59q73b,isqjgd,So if you buy say 3080 for £800 then sell it for £500 and but 3080Ti for £1000 you will have spent a total of £1300 at which point you may have just as well bought the 3090 - this is my thinking as I don't have a card to wait with until this happens... umm,2.0
g59pkbp,isqjgd,"Yeah, I haven't heard any substantiated rumors of this yet. There's definitely a big gap in VRAM between the RTX 3080 and RTX 3090 and a Super or Ti card could fill.",1.0
g59fm9z,isqjgd,"# Summary

**Blower GPU versions are stuck in R &amp; D with thermal issues**

* Lambda is working closely with OEMs, but RTX 3090 and 3080 blowers may not be possible.
* RTX 3070s blowers will likely launch in 1-3 months.

**4x GPUs workstations:**

* *4x RTX 3090/3080* is not practical. Typical home/office circuits will be overloaded. Moreover, there aren't any PSUs that can support this power draw at standard home/office voltages.
* *4x RTX 3070* may require the currently unavailable blower edition.

**3x GPU workstations:**

* *3x RTX 3090:* Will require liquid cooling or PCIe extenders to fit/achieve proper thermals.
* *3x RTX 3080:* Will require liquid cooling or PCIe extenders to achieve proper thermals. A two-slot blower edition may enable a 3x air-cooled workstation without extenders, but it's currently stuck in R &amp; D with overheating problems.
* *3x RTX 3070s:* Will likely work out of the gate, even without blowers -- but leave a PCIe slot empty between cards.

**1x/2x workstations**

* Cooling should be relatively straightforward if you leave proper space between GPUs.

**RTX 30XX performance vs. the RTX 2080 Ti:**

* *RTX 3090 (24 GB):* 1.57x faster for convnets and 1.5x faster for transformers.\*
* *RTX 3080 (10 GB):* 1.4x faster for convnets and 1.2x faster for transformers.\*
* *RTX 3070 (8 GB):* 1.1x faster for convnets and 0.8 faster for transformers.\*

\* See Tim Dettmer's post for [details](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/#GPU_Deep_Learning_Performance).",7.0
g59r3a0,isqjgd,I plan to get two 3090 for deep learning and gaming. I wish I had you guide earlier so I could have reduced the time I took to research the subject.,2.0
g59y3my,isqjgd,Would paying for google cloud or something be cheaper at this point,4.0
g5a0cuc,isqjgd,"For my personal use, yes definitely. Now the gaming part (4k120+fps) alone would justify that for me.",2.0
g6wnnw1,isqjgd,Wow!   You are so lucky that you get 2 of them!  Have you try memory pooling with it?  The only thing preventing to buy 3090 is if it supports memory pooling for ML workloads.,1.0
g5ansod,isqjgd,Is there a guide on considerations for choosing the CPU? I was thinking about about getting a Intel i9 -10900K for my 2x3090 setup. But I started to think I should go with a threadripper. Can anyone guide me on this?,1.0
g5aoj3x,isqjgd,"Threadripper is more PCI lanes, that's good for multi-GPU setups with PCI-E SSDs. Otherwise just get a Ryzen. Intel is only good for gaming.",3.0
g5auj88,isqjgd,"Dont use 10900K or any K cpu.  There arent enough pcie lanes and they are pcie 3.0 .  Use at least X series cpu or xeon or dual xeon for 44 or 48/96 pcie lanes with a matching motherboard.  

Or better yet, use threadripper 3rd gen for 64pcie 4.0 lanes.",2.0
g5av31w,isqjgd,Thanks for the reply! Do you think a 3950x would suffice or should I go with 3960x?,2.0
g5aw8w2,isqjgd,"3900x or 3950x both have the same amount of pcie 4.0 lanes, as follows: 16 lanes to gpu, 4 to m.2 drives and 4 to chipset -&gt; 20 shared lanes via chipset.  
so if you are going 3950x, probably just get the 3900x unless you need the cores and spend the extra on ram.  If you are running 2 cards then the current x570 motherboards will run them at 8x pcie 4.0 which is like 16x pcie 3.0, and thats fast enough for now. 


The threadripper has the power consumption problem of using 250w or so, difficult to put more than 2 cards.

I would buy 3900x and a x570 board with the 2 3090, knowing that there is probably a bit of performance left by the pcie lane limit.

If there are those low-end threadrippers like 1950x or 1920x would be the best choice, but the current landscape of cpu-pcie lanes is kinda restrictive.",3.0
g5ax2wo,isqjgd,"Really appreciate your detailed reply and explanation, many thanks!",1.0
g5bk783,isqjgd,"No problem.  I was researching all these specs trying to put together machines at my firm.  

Not sure if it is still true. But in the 2000 series cards when you run more than 1 multiple of 2 cards, the interconnect speed is closer to 4x pcie 3.0.  Hence the scaling factor for parallel gpus are not linear.",1.0
g5bfp9c,isqjgd,"I am considering water cooling two FE 3090s for my workstation, however I have never done any water cooling for pcs so I am kinda hesitant.

  
Is it common to watercool a DL workstation that runs for days or weeks non stop?

I am mainly concerned with a potential leakage even after leak testing for 24 hours or so.",1.0
g5bkmbk,isqjgd,"Why do you want to water-cool them? I generally recommend against water-cooling unless you have a good reason. With good airflow and spacing between air-cooled RTX 3090s, they shouldn't throttle.",1.0
g5bms0n,isqjgd,"&gt;Why do you want to water-cool them?

I was just afraid that 3090s will overheat more easily than 2080 Tis hence they will throttle.  


But since you said that they shouldn't throttle with good airflow and spacing between them, I might actually consider air cooling first and see if they throttle or not.  


If I were to go with air cooling, what case do you recommend for two FE 3090s?",1.0
g5o3t93,isqjgd,"With a 2x RTX 3090 workstation, I'd stick with air-cooling. Water-cooling is more expensive and more prone to failure.

* This motherboard looks like it will provide pretty good clearance between GPUs: [http://www.asrock.com/mb/AMD/X570%20Taichi/](http://www.asrock.com/mb/AMD/X570%20Taichi/)
* This case: [https://www.newegg.com/black-corsair-carbide-series-atx-cube-case/p/N82E16811139022](https://www.newegg.com/black-corsair-carbide-series-atx-cube-case/p/N82E16811139022)

Don't take my word though, as I haven't tested these :)",1.0
g5rzz26,isqjgd,Cool. Thanks for the recommendations.,1.0
g6ictbp,isqjgd,Would you recommend an air cooler (e.g. noctua nh-u14s tr4-sp3) or an aio for a 2x RTX 3090 workstation?,1.0
g5a6dot,isqjgd,"There are people out there (self styled researchers) trying to build 96GB VRAM DL stations for personal use - which is pretty stupid. 

The only use case for any thing above 20GB of VRAM in research is for a lab setup with multiple people sharing the resources.",-2.0
g5aogri,isqjgd,Uhhhhhh have you ever tried to finetune big Transformer models?,2.0
g5axa84,isqjgd,"I have 2 papers on GANs and Transformers, so yeah.",0.0
g5bxy4n,isqjgd,"So you must know that in order to finetune them quickly you need loads of VRAM and a bunch of GPUs, right? There are also Resnet based conv nets for which 11 GB is the bare minimum and 24+ GB is recommended (ex. that 3x ResNet-152 thing, forgot what it's called). There are people saying that pretty much only 24 GB VRAM cards are future proof for 3-7 years.

Not talking about simple BERT (that's still finetunable with a 2060). I'm talking about current SOTA models and future SOTA.",1.0
g5clddd,isqjgd,Anything around 20+GB VRAM is good enough for vast majority of the cases in research and someone who actually does research can actually make all of the models you mentioned within the range pretty easily. Try using sparse computations and lower precision in your models - you'll find it good enough.,0.0
g5ese0w,isqjgd,"I actually work in this field, quantization is absolutely broken on PyTorch BERT models (can't be trained at all, only good for inference speed), TF implementations suck massive peepee, and some institutions ban TensorRT because of its scummy license, so you're left with ONNX.

Sparse models are so badly implemented and so poorly documented that even if you did somehow get a team of experts to do things right it would still probably be bad because there is no good reference: it's still better to just buy more hardware - a one time cost that is lower than paying for extra time while everyone is running full speed ahead in research.

Overall IMO messing with low memory is worth only if you are broke or mess around with AI as a hobby, in the industry and academia you just have no time, other people will get financing and contracts because they get shit done, and you're left only with your efficient implementation no one cares about. An RTX3090 is basically an intern monthly pay where I live, but if it allows you to train things two times faster, it will be worth it after the next product release, which could be as soon as 2 weeks.

Implementing a sparse model that is stable and has all those benefits is something you'd have to do for every new model. Such a thing in an NLP setting where it is not financially viable to train a model from scratch is both a bad business and academic decision, you're just wasting resources. If ResNets were bigger and harder to train, CV would feel it too.",1.0
g59a7bc,islf3b,links to the extension??,1.0
g59ey0t,islf3b,Description for video,1.0
g59ino4,islf3b,"Aha my bad didn't open it in YT.
here is the link if anyone needs
https://github.com/deepklarity/jupyter-text2code",2.0
g58sq8c,isleqt,The models are likely pretrained and then with the 3 weeks data are likely fine-tuned/transfer learned to represent the user specific case. This is common in a lot of ML use cases.,1.0
g5f3ss4,isleqt,I see! What sort of model could power this usage prediction? Or are they likely taking it purely statistically based on the peaks of the graph?,1.0
g58bhxo,isje34,Is this open source  ?,1.0
g57visy,isi3w8,"Within each sum.. all the elements are getting multiplied. Moreover, d\_s3/d\_sk is itself a result of multiplication (chain rule on previous gradients)",3.0
g58bpja,isi3w8,"So does that mean that the update of the weight from s1 to s2 is calculated by (I will leave out the d's):

E3/W = E3/y3 * y3/s3 * s3/s2 * s2/W

or is it with the sum like this:

E3/W =  (E3/y3 * y3/s3 * s3/s2 * s2/W) +(E3/y3 * y3/s3 * s3/W)",1.0
g591vaa,isi3w8,"RNN is similar to the game telephone. Imagine that you are playing a game with `x` people, you pass one word to the first person, and each person has to say all the previous words and add an additional word to the network. 

The process isn't difficult for a network with few people, but it gets more difficult as you add more people. Over time the network will forget the earlier words in the network and all the newer words will be remembered.  

Similarly, RNN work in a similar way, information passed from the earlier node start slowing vanishing, and the information from the immediate network has more weight on the output.",1.0
g599co7,isi3w8,"The Vanishing Gradient is a problem all neural networks that update their weights by backpropagation can encounter. This isn't an RNN thing but happens to be a bigger problem for RNNs.

https://stats.stackexchange.com/questions/262750/why-is-it-hard-to-train-deep-neural-networks/369353#369353",1.0
g5ab8i7,isi3w8,"Each term is made of multiplications. The further back in time you go, the more multiplications and hence the worse the vanishing or exploding gradients will be.",1.0
g57l6ox,isfxci,"Is this a ""feature"" of every CNN network you've trained, or just the most recent one?",1.0
g57l6wb,isfxci,Batchnorm will behave differently when your batch has a different size in training.,1.0
g57lf19,isfxci,"My whole data set has 1000 images. 
To debug, I just took a batch of 100 images and I am only training that batch. So essentially, it's just batch gradient decent for only that batch of 100. That trains well, it overfits. So now I can worry about regularising it. 
But if my hypothesis completely over fit that batch and predicts with 100% accuracy cumulatively, then it should also predict with 100% accuracy for a subset in that batch. It predicts wrong. How is that even possible?",1.0
g57n7qg,isfxci,"What I said was, if you have BN in your model, it will behave differently when you have a large batch (100 images) vs when you have a small batch (5 images), even when the latter is a subset of the former. BN in training mode is not independent of the batch size. Everything else (conv, relu, etc.) should be.",2.0
g57mo0o,isfxci,"You are using online augmentation?

If it's not the case then you have a bug.",1.0
g57p8ej,isfxci,"Seems like it. I am not using online augmentation. 
Heck, I don't know what is online augmentation.xD
I am trying to code CNN only using numpy",1.0
g57pp1i,isfxci,"As the other commenter said, batchnorm could also be the cause. If you have only a subset of the training data for testing then running the normalization on them will most probably result in different data inputs, so in this case you are not testing the nn with the same samples.",1.0
g57pxxx,isfxci,"So what you are trying to say is:
1. I train my model with 100 examples. 
2. After I train, if I test my nn with, say a batch of 20 images, would probably do bad because the batch size is different thus, my normalisation is different thus, the weights will compute differently as it was trained for a batch of 100?",1.0
g57q88e,isfxci,Exactly.,1.0
g57qguc,isfxci,Is there a solution to this? What if I just want to classify one image than a batch ( same size I used for training?),1.0
g57qntl,isfxci,"I am not familiar with your system, but I would

1. Try to turn batchnorm off. You may not need it anyway.

2. Calculate batchnorm parameters for the whole training data, then save and use these parameters to apply normalization for the test dataset. I have no idea if it is supported though.",1.0
g57lze3,isfxci,Just a recent one as in?,1.0
g57qpl4,isfxci,"Yes, I will try that. Thank you. Ill let you know if it worked. :)",1.0
g56phvc,is9vc3,3070 hands down,3.0
g5acol4,is9vc3,It is a compelling choice.,1.0
g5ao82u,is9vc3,"You can always upgrade your motherboard down the line, with 3070 you'd be future proofing",1.0
g578frl,is9vc3,Getting a RTX 3070 is highly recommended as it has high volumes of processing power compared to its predecessor.,2.0
g5acm5n,is9vc3,"Definitely a popular choice, it seems.",1.0
g57rkgm,is9vc3,"mostly the limiting factor for deep learning is VRAM. the 3070 has 8gb of VRAM, just like most of it's predecessors. however, the computation power of the 3070 is (expected to be, wait for benchmarks) vastly better compared to its predecessor the 2070S or 2070. 

depending on how prices are going to develop after the nvidia launch a new or used 2070/2070S would be an option if you prefer to spend less money. the 2080ti with 11gb VRAM might be a good pick as well once prices are going down.",2.0
g5acjdl,is9vc3,"This aligns with what I've observed, anecdotally, VRAM usage climbs quickly (I start encountering memory errors for slightly larger architectures), but the GPU usage doesn't actually climb all that much. Was also considering the 2070S but it physically is a bit larger than my current case can support.

Will wait and see what the 3070 reviews are like.",1.0
g57lgru,is9vc3,"If you do not intend to upgrade your mobo within the next two years save your money. By then a new GPU will be all the rage and you will consider upgrading from your 3070 anyway. If you can, take someone's lunch money and buy a used 2060 super from some smuck that decided to waste all their $$ on a 3070 with the launch for their 1080p gaming setup.

If you have concrete plans to upgrade your mobo in the next 12 months then i would consider the 3070 strongly especially if you feel like, barring any insane performance upgrades, you will not upgrade your mobo, ram, cpu or gpu again afterwards for at least another 2 years.",0.0
g5ac5nd,is9vc3,"Thanks for the input!

Hadn't considered the perspective of 'in two years, might want to upgrade again'.

I don't intend to upgrade much of my hardware other than the GPU which seems to be the primary bottleneck I am encountering now, so the 2060S seems to align best; but the 3070 is tempting and might lure me into upgrading my mobo...",1.0
g6at2a0,is9vc3,"Given the abysmal launch of the RTX3k series, I would strongly suggest settling for a 2060S or even a 2070S over the 3k series counterpart. In any case I find it odd that you are comparing a 2060S to a 3070, the 2070S would be a much more apt comparison.",1.0
g6bactb,is9vc3,"The 2060S for just addressing the immediate short term challenges of GPU memory at minimum cost. I definitely am leaning towards the 3070, though. With the rumours of a 3070 Ti/S at 16GB, I might wait a bit longer to see if that materializes.",1.0
g55tt9r,is6kxz,"Hey everyone, we recently open sourced [Onepanel](https://github.com/onepanelio/core), our computer vision platform with fully integrated components for model building, semi-automated labeling, parallelized data processing and model training pipelines.

Under the hood, we integrate our own and other best of breed open source components to provide a seamless user experience and abstract away infrastructure complexities that come with running parallelized data processing and training pipelines on different cloud providers.

Our near future goals are to add serverless APIs for inference and VNC enabled workspaces so teams can also run simulation environments inside of Onepanel.

We would love to hear your feedback! And of course we welcome and encourage any contributions.

GitHub: [https://github.com/onepanelio/core](https://github.com/onepanelio/core)  
Docs: [https://docs.onepanel.ai/](https://docs.onepanel.ai/)",1.0
g56ud29,is5zer,"&gt; My first question is: how is that possible that the network does not react much to the input data variability?

If your training data didn't have examples of noise and zeroed out input then you should not have any expectations regarding the network's performance on these data.",1.0
g573wwx,is5zer,Is it possible that it’s predicting the mean of the time series?,1.0
g578d6j,is5zer,"I don't think so. Because when I had input, a matrix of zeros, the network did predict something but that was not the mean values.",1.0
g549z86,irznyg,"It's beginner-friendly in the sense that all of the major design decisions are explicitly communicated through comments, and I've tried to make it as clean and readable as possible.

Code is here: [https://github.com/gordicaleksa/pytorch-gans](https://github.com/gordicaleksa/pytorch-gans)",8.0
g559nx6,irznyg,"Bro, this is a goldmine for a beginner like me. Thank you!",5.0
g55k2av,irznyg,Woohoo! Glad you find it useful! Keep crushing it!,2.0
g55gvvj,irznyg,Thank you so much,2.0
g55k3i5,irznyg,You're welcome!,1.0
g55x2ii,irznyg,"I wish there was an easier way to play with a GAN. I have almost no experience with them, but I'm sitting on a dataset that would be perfect for one.",2.0
g560ybk,irznyg,"What does it mean easier? Like you just point to your dataset (say you input some path) and the GAN gets trained? Or you need more comments, better code readability?

If I can somehow help I'll try. I'm about to add some jupyter notebook in there I got a feedback that that may help.",1.0
g56xdio,irznyg,"Perhaps something that's entirely UI based, streamlined, even if the results are less than they could be.",2.0
g57cnlp,irznyg,"You might want to check out RunwayML. It's affordable, (if you let it run locally) and made with artists in mind, not programmers. I got it running last night, a clip costs about 20cent, for 15 dollars you get to train machines for a month.",3.0
g57kwit,irznyg,Thanks for sharing! Haven't heard of RunwayML until now.,2.0
g57kveg,irznyg,"Got it, my next projects will be more application-like, this one was more geared towards people who want to learn ML - so by definition you have to be exposed to implementation details.

Follow me on GitHub for those future projects:
https://github.com/gordicaleksa

I'll also be covering everything I create on GitHub on my YT channel:
https://www.youtube.com/c/TheAIEpiphany",1.0
g572yoa,irznyg,I swear I saw Michael Jackson and Prince Andrew at one point.,2.0
g57lc21,irznyg,"Latent space is a vicious beast you may see things! 😅

Like CelebA, the dataset this GAN was trained on probably has those 2. So some interpolations between them are possible - hence the resemblance.",1.0
g531lnl,irup88,You know it’s good because of the emoji,2.0
g53ms1h,irup88,Ik it's good so I have shared,0.0
g51ubjr,irs8a8,"take an instance of y=f(x) ,  f(x) = e\^x/1+e\^x . once u put the input through activition function (f(x) in this case) u get a value that is not linear (eg. f(x) = 2\*x + 1)

&amp;#x200B;

what this allows to do is it enables the neurons and neural nets in general to learn a function that is non-linear such as XOR or curves which are the underlying processes in most of real-life applications",6.0
g51x6hu,irs8a8,nice explanation thanks,3.0
g526cfl,irs8a8,"If there is no activation function in your neural network, it would act as linear regression. This no good because we shouldn't depend on only a single formula.

To solve this we need many different formulas for different scenarios. For example for case A we can use formula ""a1\*w1 + a2\*w2"", but for case B it should be like ""a1\*w2 + a2\*w2 + a4\*w4"". So we need something that activates and deactivates neurons at some point. 

Let's say how many different systems can 1 neuron can learn if we use ReLU? It is 2; values are 0 and x. So let's think about how many different combinations we can create with 10 neurons?   
Roughly speaking it would be 10 factorial. What if we keep 5 neurons active out of 10 neurons all the time? It would be roughly C(10,5)=252 different systems in a single neural network.",5.0
g52powk,irs8a8,"The answers here are good, just to give another, straight forward answer

Take the definition of linear vs nonlinear functions;. If a NN, did not have activations, no matter how many layers, it would just be a series of matrix operations, multiplication and addition, or linear operations.  As said in another answer, it collapses down to linear regression.",5.0
g542bav,irs8a8,"I'd like to point out that although you are technically correct, the gradient decent properties between the two are different.  In my experience multi-layered linear NNs perform better than a single layer.",1.0
g57s47o,irs8a8,agree,1.0
g57wvru,irs8a8,everyone thanks,1.0
g52hfk9,irrwcd,"I would suggest you to read the notes from [CS231](https://cs231n.github.io/convolutional-networks/#:~:text=You%20can%20convince%20yourself%20that,would%20get%20a%203x3%20output.). I copied the answer to your question:

""We can compute the spatial size of the output volume as a function of the input volume size (W), the receptive field size of the Conv Layer neurons (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. You can convince yourself that the correct formula for calculating how many neurons “fit” is given by (W−F+2P)/S+1. For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output.""",1.0
g57wxbn,irrwcd,thanks dear akukaja,1.0
g57esvf,irr85q,you can use label img tool for annotation,2.0
g51lecv,irr85q,"Is this an open dataset? For labeling, you can use labeling.",1.0
g525qs3,irr85q,"No I hav images, which I collected from hospitals, now I want to annotate them...",1.0
g57s5ih,irr85q,"What kind of segmentation mode are you looking for? bounding boxes, polygons, per pixel?

i found h[ttps://www.scalabel.ai/](https://www.scalabel.ai/) to be pretty usable, cvat [https://github.com/openvinotoolkit/cvat](https://github.com/openvinotoolkit/cvat) is another good opensource choice.",1.0
g58buv5,irr85q,Bounding box,1.0
g58ee6w,irr85q,in that case look for semi-automatic annotation tools. cvat (and others) offer support for object detectors to increase your annotation speed. it allows you to train a simple detector with few labels and use its results to faster annotate novel images.,1.0
g58eluf,irr85q,"Thanku so much, very appritiatable",1.0
g58b4qw,irr85q,"CVAT? 

Or alternatively image.ly",1.0
g58b5pm,irr85q,"**I found links in your comment that were not hyperlinked:**

* [image.ly](https://image.ly)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g58b4qw&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1.0
g58bwbq,irr85q,Wow amazing,1.0
g4zv2re,irk5mo,This is really a great way to capture semantics!,3.0
g4yng2n,irh1lk,I guess Hinton is,4.0
g51qs2r,irh1lk,"Came here to say this.

I believe he is working on it in Google Brain. I believe he has team for it.",3.0
g4xwtgu,irbp5k,I requested API access and never heard back from them. :(,2.0
g4xx70g,irbp5k,"Try sending back an email to them personally! I don't remember it by heart, but I showed the email I personally contacted in the video, towards the end! :)",2.0
g4xxa8t,irbp5k,"Don't forget to tell why you want to use it and convince them to take you, as I say in the video. It's super important, they only want the best for the beta and the most ""potential money makers""!",1.0
g4x9g5q,irbp5k,"***GPT-3 Paper,*** [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)  
[***GPT-3 video explanation***,](https://www.youtube.com/watch?v=gDDnTZchKec) [https://www.youtube.com/watch?v=gDDnTZchKec](https://www.youtube.com/watch?v=gDDnTZchKec)  
**OpenAI’s API request,** [https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8sx3ELsv0PEhHphhNz30FttVUNkYwTlNPMVI1V0lXNjExMlExUlc4SE5YSS4u](https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8s0v5E5gdyQhOuZCXNuMR8i1UQjFWVTVUVEpGNkg3U1FNRDVVRFg3U0w4Vi4u)",1.0
g4wiffv,ir200f,"Please don't buy yet, wait for the new GPUs. Also consider a 2nd hand GPU. if you can get one. 

&amp;#x200B;

If you haven't read this post, I'm assuming based on a few Qs, please check this out: [https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)",4.0
g4vo81c,ir200f,Oh yeah: Budget --&gt; $500-700,1.0
g4w3u7e,ir200f,"With that budget, you can safely go for 3080 (699$). However 3070 (499$) should be able to do just fine.",1.0
g4vpk65,ir200f,3080/70 and you're straight,1.0
g4x6w2i,ir200f,"If you don't already know this:

Google Colab and Kaggle offer cloud-gpus for free.",1.0
g4y1u2k,ir200f,Yeah but you can’t play Minecraft RTX on the cloud,3.0
g4uyduw,iqy33c,".... for what, exactly? Fine tuning YOLO or ResNets? Something much bigger?

Can’t answer this without more details.",2.0
g4uywlj,iqy33c,"training a resnet34 on say, 20-50 thousand images?",1.0
g4uzf74,iqy33c,You might be memory constrained a bit but if you use a small batch size (like 4) it might work. Will be on the slow side to train though.,4.0
g4wueat,iqy33c,"I don't see any reason to buy one today, since there are better GPUs for the same price, but if you already have it, it's a decent GPU.",2.0
g4wxh8y,iqy33c,"IMO I wouldn't buy a GPU with less than 10-11 GB of memory nowadays, I have a 1060 and it's super shit to even train a RetinaNet, not only because of memory but because of processing speed, too. Colab is better than that.",2.0
g4xbpao,iqy33c,"A 1050ti has served me well for 2 years so far
Yolo and working with a bunch of lidar data",1.0
g4vj1qb,iqy33c,Its a pretty decent GPU,1.0
g4vsjzd,iqsul7,This is way more interesting than GPT-3.,1.0
g4x4crz,iqsul7,Thanks for sharing!,1.0
g4t49e1,iqn485,"What does ""crop it from the edges"" mean? Please provide an example.",3.0
g4t6xiy,iqn485,Lets say I have an image of apple(or any other fruit) with white background (or any color background ). I want to remove the background and only want image of the fruit.,0.0
g4tfmku,iqn485,"You can go with PIL crop feature but if you insist on using opencv then load the img as img array(cv2 imread())  and then use numpy slicing on the img array ,it will get cropped.",2.0
g4t9sau,iqn485,"you can use object segmentation techniques, check here some state-of-the-art implementations: https://paperswithcode.com/task/semantic-segmentation/latest",2.0
g4wr1wl,iqn485,You can use template matching technique followed by warpPerspective.,2.0
g4ujp36,iqmfuo,NER is a sequence to sequence task. The input is a sequence of tokens and the output is a sequence of tags (e.g. see [IOB notation](https://en.m.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))).,1.0
g4tdq91,iqkzuj,Trying getting tiny-yolo or yolo-lite on tf-lite,1.0
g4te8m7,iqkzuj,Is this your first work. Any luck sofar.,1.0
g4tg7sd,iqkzuj,Got it working on a pi 4. Decent results.,2.0
g4sr3pf,iqk97j,Im curious what your question has to do with deep learning?,1.0
g4sr6re,iqk97j,Well this is the output of [https://github.com/mkocabas/VIBE](https://github.com/mkocabas/VIBE),1.0
g4se7yk,iqioqt,[Training Large Neural Networks with Constant Memory using a New Execution Algorithm](https://arxiv.org/abs/2002.05645),2.0
g4tavto,iqioqt,Does this also help with huge input data? (I did not yet read what deep speed is doing),1.0
g4sid34,iq939r,A100.  and can i get fried with that.,2.0
g4sjeul,iq939r,"I can't explain the huge difference between A100 and 3090 presented in the article since from a technical specification standpoint (I have no access to either of the cards) they are very similar. The 3090 even has significantly more CUDA cores. The A100 has slightly more Tensor cores. Not sure if this is actually referring to a single standalone A100 or a DGX? 

It bothers me that as basis a PCIe 3.0 system is used. The new 3000 lineup is PCIe 4.0. That makes quite a difference and can't be ignored. 

I dislike the created bar plots. They show normalized values on a 2080 ti but are not centered around 1 but still start by 0. That makes the presented information much less clear. 

Based on this I don't trust the presented information to give an accurate representation for the 3000 lineup.",1.0
g4swhlk,iq939r,Aren’t the consumer cards throttled for DL?,1.0
g4swsfh,iq939r,"Not that I'm aware. Nvidia officially just doesn't ""allow"" to use them for that but they are not enforcing this. I think that was just a move to bring big companies to buy the 10x more expensive Quadro variants instead.",1.0
g4t27dd,iq939r,"You're wrong. Not only are Ampere Gaming tensors 2x slower physically, but they are limited to half rate in software, making them 4x slower",1.0
g4ul013,iq939r,[deleted],1.0
g4vaq7w,iq939r,Yes,1.0
g4x8y40,iq939r,[deleted],1.0
g4xa39o,iq939r,Well that's nvidia for you :),1.0
g4t2630,iq939r,"Yes, the tensors cores are limited to half rate.",1.0
g4t353a,iq939r,"But could they run faster if ""unlocked"". So are they intentionally held back or are they simply targeted differently since the cards are first and foremost consumer cards?",1.0
g4t3bxg,iq939r,"Both, half the performance is just because Nvidia allocated the silicon elsewhere, this is fine.

The other half is PURELY a software lock and is just nvidia saying ""fuck you pay us and buy a titan / quadro """,1.0
g4t4k0m,iq939r,"Interesting. I did not know that. I wonder if this has a logical reason like ""we reserve this other part of the cards performance for gamer features like raytracing, upscaling, ..."" which do not apply (I assume) for a Quadro or A100.

I mean: If the hardware is actually capable of more... why lock it down instead of simply building cheaper less powerful cards. Because it is cheaper to produce more of the same sku of some components and then to lock them down on cheaper devices?",1.0
g4t522k,iq939r,"&gt; ""we reserve this other part of the cards performance for gamer features like raytracing, upscaling, ..."" 

It's half this reason. Tensor cores are smaller (1/2) on GA102 vs GA100 because the silicon saved is used for RT cores. So yes here the hardware is just not there

See [here](https://www.techpowerup.com/review/nvidia-geforce-ampere-architecture-board-design-gaming-tech-software/images/tensor-core-2.jpg)

But there is *another* limitation and it purely in drivers: the drivers artifically limit FP16 Mul + FP32 Acc (ie the operation used in training) to half rate. 

The reasons being that nvidia wants you to force you to buy Quadro/Titans but still uses the GA102 dies for those. Because it's a lot more economical.

Economies of scale makes it that up to a point it's actually cheaper to build a big die and then artifically nerf it for segmentation",2.0
g6awlhl,iq939r,"Proof, please",1.0
g6awr3a,iq939r,"https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf

Check table 1, line "" Peak FP16 Tensor TFLOPS with FP32 Accumulate""

The speed of FP32 Acc is artificially limited to half rate because that's the operation that is ACTUALLY USEFUL for training. It's been the case for all geforce cards compared to Tesla cards.",2.0
g6crilr,iq939r,"Seems like you have a point. https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/ V100, Titan RTX and A100 aren’t neutered like that. Both have same peak tensorcore fp16 and fp32 accumulate.",1.0
g6d4zc7,iq939r,"Meanwhile A100 is just wildly different and only bears the same name in resemblance to consumer Ampere cards. I’d say for some networks that are heavy in compute 3090 is going to even be slower than Titan RTX or Titan V. To make the best out of consumer cards we’ll have to minimize compute as much as possible and use something more memory-bound, like efficientnet.",1.0
g4t2584,iq939r,"There is nothing assumed about the A100 performance, it's out and Tim probably even has access to many.

No offense but you are pretty condescending in your analysis considering how wrong you are",1.0
g4t2k8l,iq939r,"You are right, thank you for pointing it out. I could have been more neutral in my disagreement with the presented information. I will edit my post.",1.0
g4t2uwf,iq939r,"No problem :) I didn't mean to be offensive.

At least now you know that nvidia is being an asshole and nerfing cards haha",1.0
g4z3x7d,iq939r,"&gt;I can't explain the huge difference between A100 and 3090 presented in the article since from a technical specification standpoint (I have no access to either of the cards) they are very similar.

Can you summarize how you got to the conclusion that they are very similar? When I look at memory bandwidth, number and architecture of tensor cores, and l2 cache they don't seem similar at all to me.",1.0
g65mbqb,iq939r,"3090 is reportedly 20% faster than 3080 on CUDA workloads.

[https://videocardz.com/newz/nvidia-geforce-rtx-3090-shines-in-blender-benchmarks](https://videocardz.com/newz/nvidia-geforce-rtx-3090-shines-in-blender-benchmarks)",1.0
g4nklp4,iq10sj,"Hi all,

Following the amazing turn in of redditors for previous lectures (more than 1000 total people registered - not bad), we are planning another free zoom lecture for the reddit community.

In this next lecture we will talk about a new GAN method for image style transfer, the lecture is titled: ***Council-GAN - Breaking the Cycle***. The speaker is the researcher and the paper's author.

&amp;#x200B;

**Lecture abstract:**

This paper proposes a novel approach to performing image-to-image translation between unpaired domains. Rather than relying on a cycle constraint, our method takes advantage of collaboration between various GANs. This results in a multi modal method, in which multiple optional and diverse images are produced for a given image. Our model addresses some of the shortcomings of classical GANs: (1) It is able to remove large objects, such as glasses. (2) Since it does not need to support the cycle constraint, no irrelevant traces of the input are left on the generated image. (3) It manages to translate between domains that require large shape modifications. Our results are shown to outperform those generated by state-of-the-art methods for several challenging applications on commonly-used datasets, both qualitatively and quantitatively.

git: [https://github.com/Onr/Council-GAN](https://github.com/Onr/Council-GAN)

arxiv: [https://arxiv.org/abs/1911.10538](https://arxiv.org/abs/1911.10538)

&amp;#x200B;

**Presenter BIO:**

Paper's author Ori Nizan is a PhD student at the Department of Electrical Engineering, Technion, CGM Lab, suprevised by Professor Ayellet Tal. His main field of study is Image domain transfer.

Website: [https://onr.github.io/](https://onr.github.io/)

&amp;#x200B;

**Link to event (September 24th):**

[https://www.reddit.com/r/2D3DAI/comments/i0japo/councilgan\_breaking\_the\_cycle\_cvpr\_2020\_lecture/](https://www.reddit.com/r/2D3DAI/comments/i0japo/councilgan_breaking_the_cycle_cvpr_2020_lecture/)

&amp;#x200B;

Lecture will be recorded and uploaded to Youtube, all previous lectures and their recordings are in /r/2D3DAI",8.0
g4nqo0q,iq01sn,"I have never worked on this kind of problems (deploying on edge devices) but I think a common way to face a problem like that with Tensorflow is

1) Training your model as usual

2) Quantize your model with Tensorflow-lite (more info in [https://www.tensorflow.org/lite/guide](https://www.tensorflow.org/lite/guide))

Something similar can be done with Pytorch (Pytorch Mobile?) also but I am not that experienced with Pytorch.

Have in mind that the trade-off is accuracy.",2.0
g4momj9,ipxh5e,Or would it be better to go for a AWS VM?,2.0
g4tsyt8,ipxh5e,"Buy hardware if you plan to train images or videos in future(long run)

if this project is one time thing use VM",1.0
g58e44q,ipxh5e,"In that case we'll buy hardware.  
Is there anything you can recommend?

As stated before I was thinking about getting a Workstation with a RTX3090. I have no clue what kind of RAM and CPU I should go with though.  
Could someone give me a hint in that direction?",1.0
g5hqc6o,ipxh5e,"I must apologize, i cant really help you with that, the little advice that i shared with you was given to me by someone else when i was in similar situation such as yours.

I chose neither of those options instead i went with google colab (free) It has timer that destroys the instance in 12 hrs. But i managed to automate it with Selenium.

 I hear good things about rtx3090 but cant help you more than that.",1.0
g4mn7j0,ipvgbu,"I have working with deep learning for five years now during my Ph.D. and I can tell you that you are absolutely right: even though a lot of research has been done applying deep learning to medical problems ( especially from DeepMind, Apple, and many others) I think that this is just the tip of the iceberg.
As a health scientist, you are the one who should tell us (computer scientists) where this possibilities are XD Start looking for the most common, time consuming problem regarding data analysis (images or time series) that is needed for a good diagnosis.
For example, DL or simple machine learning also, is perfect to find irregular heartbeat, however the rules in medicine are so strict that it is very hard to obtain data and implement a real prototype.",1.0
g5h9o9g,ipv0kf,Nice work :),1.0
g63ix0q,ipv0kf,Hey Thanks!,1.0
g4q78n2,ipnoh4,"I mean, ""suggests"" is an odd word choice, because obviously they don't actually understand language.  The title should be more like ""provides objective evidence that""",1.0
g4jl8zy,ipfe2l,"I thought someone shouted ""HTML is a programming language """,67.0
g4jpem9,ipfe2l,YOU TAKE THAT BACK.,26.0
g4jy5r1,ipfe2l,"Not all programming languages are Turing complete!

That makes a somewhat clunky battle cry, though...",6.0
g4kes6p,ipfe2l,"Nobody will never shout that, who tf thinks it.",8.0
g4kky38,ipfe2l,Lrn2English,6.0
g4jptd5,ipfe2l,CS folks really don't want to learn stats...,25.0
g4jy8tf,ipfe2l,They really don't want to admit that that's what they've been learning...,21.0
g4ljt6a,ipfe2l,Deep learning is much more than stats,2.0
g4lqu7w,ipfe2l,"Nope it is still statistics and maths with some touch of emperical science (tuning).

The core idea are still the same as they were in 1999.",2.0
g4jy3bf,ipfe2l,"Well, its APPLIED statistics.😂",19.0
g4mmg5k,ipfe2l,Applied your mom.,5.0
g4ni5jl,ipfe2l,got em,1.0
g4jybbj,ipfe2l,just turn this pile of linear algebra until it looks right,10.0
g4klwqz,ipfe2l,When they realize logistic regression is also a neural network.,6.0
g4jvkzx,ipfe2l,"Now I'm curious. What was this video of, IRL?",5.0
g4jwbfz,ipfe2l,"[Football riot in Groningen, NL](https://www.rtvnoord.nl/nieuws/739942/Voetbalrellen-in-binnenstad-Groningen)",8.0
g4l2442,ipfe2l,Funny... I'd thought it was backyard furniture riot outside an IKEA,3.0
g4kuw1j,ipfe2l,Hopfield neural nets have energy functional equivalent to that of quantum ising spin-glass systems motherfuckers,4.0
g4jkucc,ipfe2l,Thanks for the giggle ahaha,7.0
g4lkdjn,ipfe2l,Planet of the apes...,3.0
g4miewj,ipfe2l,"I just heard ""JavaScript is a fine language"", I don't know where all that anger could've como from?",3.0
g4jlkkr,ipfe2l,"It is funny and of course true, but really machine learning is a confluence of statistics, computer science, hardware design, and a whole ecosystem.",7.0
g4jue4y,ipfe2l,Fight me you knave,10.0
g4k1u6o,ipfe2l,"I don't agree with the statistics part, I think statistics is much lower level, looking at a single group of data, but I think in general deep learning is more meta, like aiming to generalise across multiple dataset, so you can't really look at the numbers within the data and theorise about the data like you do in statistics. I think?

Edit actually yeah I agree with you.",1.0
g4lqgy9,ipfe2l,"Oh, I thought I heard, AI is just Machine Learning with bigger machines",1.0
g4niqja,ipfe2l,"Not all AI learns from data. There are many different types of AI, many of which do not use machine learning at all. Look up symbolic AI and GOFAI.",1.0
g4m77hw,ipfe2l,PHP is the best language! /s,1.0
g4nawun,ipfe2l,"The movements of the crowd look fake (but maybe that's because I associate this type of movement with simulations), and the graphics look real. Legitimate question, is this video real or simulated?",1.0
g4p8eh8,ipfe2l,I think they shouted Bielefeld in Groningen 😁,1.0
g4wfal0,ipfe2l,Funniest moment was at :30 where a guy gets hit with a chair on his head from his own side 😂 (person at the right with a cap on),1.0
g5to7lf,ipfe2l,Always has been.,1.0
g4kx3bk,ipfe2l,"Well yes but no, the academic discipline is an offshoot of Computer Science the the philosophy of the two fields and what the departments’ focus on is completely different. Machine learning is purely focused on predictive capabilities of an algorithm. Statistics cares about data the different distributions of data and relationships between the sample data and population and the relationship between variables. Trying to apply or describe machine learning methods of doing models to my econometrics/stats professor confused him.",0.0
g4jkxw0,ipf8aq,Implement a paper which is written in Tensorflow into PyTorch or vice versa. You will learn a lot along the way,1.0
g4pa003,ipf8aq,I was thinking more on the lines of  building an application. Could you suggest a few?,1.0
g4xh9o2,ipf8aq,"A tool I recommend is Streamlit. It lets you easily convert a PyTorch model to a web application. So you could for example build a simple image classifier, and then showcase it using Streamlit.",1.0
g4xr6p0,ipf8aq,"I recently came across Streamlit a couple of weeks ago. I had to present something in my class and everybody loved it. It was easy to build it too. 

Maybe I am unable to express what I really want to do properly. My question is something along these lines - 

I've seen someone do a project where they identified graffiti. A drone dedicated exclusively for monitoring walls and other surfaces would send the captured pictures to a server, where those pictures would be classified as graffiti/non-graffiti. That location would then be flagged if it turned out to be graffiti.

I want to do something interesting like that. I apologize for not being able to convey it better.",1.0
g4jcre1,ipcngp,"I really like these style of videos, it's a nice addition two something like ""two minuute papers"" by going into much more detail of how the system actually works:)",2.0
g4j3z7l,ipbugk,"Even if it works, your 2070 super needs to wait for the 1050 ti for gradient synchronization. I think you should stick with your 2070 super.",3.0
g4pgvd0,ip2ji6,"I'm not sure to understand your question but as I started reading about neural networks on 3D volumes and here is what I have understood so far:

* The format of your 3D model's matter:

The mains format to store a 3D models are: voxels ( 3D pixels), point clouds or mesh (made of vertex, edges and faces). Those representations will heavily influence your neural networks models as voxels are in a Euclidian space (all neighbours pixels are always at the same distance) and meshes are in a non-Euclidian space (neighbours points/vertex are not always at the same distance). 

* In a Euclidian space:

With a Euclidian space, you can apply any CNN or RNN by just modifying the dimension of the input. The drawback will be that lot of part of your 3D models don't have useful information, like void area, or the inside of the part.

* In a non-Euclidian space:

With a non-Euclidian space, you have to redefine your convolution and other layers. I'm still dinging that part but I found some interesting papers as:

* [MeshCNN: A Network with an Edge](https://arxiv.org/abs/1809.05910). Nice paper (doing **mesh segmentation** ) extracting invariants features form the mesh to feed convolution layers.
* [Geometric deep learning on graphs and manifolds using mixture model CNNs.](https://arxiv.org/abs/1611.08402)

When you have the correct type of layers/features extractors, you can do what you want with them, so you can apply CNN or RNN or apply your neural networks on a part of a 3D object.",1.0
g72b3vd,ip2ji6,Thank you that was very helpful!! Especially the graph based approach! So both CNNs and RNNs can be applied! I think it's euclidean/voxel based in this scenario. The 3D volumes are stored as 16 bit grayscale TIF stacks of the same dimensions and they are composed of brain neuroimagery.,2.0
g4je6zn,ip1yby,Anything guys? :(,1.0
g4h4q0h,ip09ls,"YOLO doesn't use anchors. YOLO v2 does.
You can read about the effect in the YOLO v2 Paper https://arxiv.org/pdf/1612.08242",6.0
g4heaam,ip09ls,"How many is multiple? 

It is easier to loop through 9 differently predefined anchors and make the network learn confidence and offset rather than finding some functions that finds an arbitrary amount of proposals. 

It is in general hard for a model to predict something of an unknown size. 

That’s why object detection algorithms have a specific number of proposals that are all being evaluated in some form. There is no „predict unknown amount of boxes“. How do you want to define that with neurons ?",5.0
g4j5bd5,ip09ls,"Quite recently anchor less models like CenterNet and CornerNet are outperforming anchor based two stage detectors. If defining anchors make the detection models learn better, why anchor less one stage detectors are more accurate nowadays?",1.0
g4ibh04,ip09ls,"Predicting the boxes makes the problem easier. The original authors tried not using the anchor boxes and the results were worse. Having better ""prior"" works out to be much better. Having good priors is a pretty common practise in object detection models, a lot of them use it.",3.0
g4ic3ky,ip09ls,"anchor boxes are introduced so that the objects detection became a regression and classification problem.

classification: is there an object close to a certain anchor box

Regression: if answer is yes for the first question, how to regress the pre-defined anchor box to the actual bounding box of the object (deviation and size change)",2.0
g4j0o0n,ip09ls,"Can you please tell how the anchors are used in loss function, let say in YOLOv2 and faster RCNN?",1.0
g4jpqbl,ip09ls,"I’m more familiar with faster RCNN. 

In faster RCNN, we assume there are N different anchor boxes with different size and ratio. In the feature map, each grid point can be mapped back to the original image right? On each grid point, the algorithm assumes there could be an object of size close to the three anchor boxes. Then, classification is done on the feature map to see if there is an object or not (of three sizes). This part of loss is binary cross entropy. If the classification result says there is an object of size X, then the regression will happen to regress the location of the center of the anchor box to the actual center of the object, and to regress the diameter of the anchor box to the actual object size. This part of regression loss could be L1.

Thus, the loss is a combo of L1 and binary cross entropy in faster RCNN.",1.0
g4ijyr3,iozr85,Haha! What's with the hashtags?,2.0
g4gvus8,iozlqu,Looks like a typical loss curve. What were you expecting?,11.0
g4gw4jx,iozlqu,Looks fine to me,6.0
g4gw57p,iozlqu,"Just looking at your # of iterations, I dunno your data set size, but it seems low. Typically my curves only become consistent into the tens of thousands (comparable to the data set size)",1.0
g4ird5s,iozlqu,"spikes in the loss curve can indicate a high learning rate, try a lower learning rate maybe with a decay",1.0
g4ixlqf,iozlqu,"See you are using mini-batch gradient decent those bumps were supposed to be there. If you were using batch gradient decent ( batch-size = #samples) then the curve would have been different, but it is not computationally possible with most of the cases.

**So the loss graph seems right to me**",1.0
g4hmf0q,ioxkih,same,1.0
g4if6ab,ioxkih,Mount your google drive or Upload it to your colab (on the sidebar) and use /content/02.png as path. Basically just check if you can see your file in the file explorer on the sidebar and use that path,1.0
g4j7cax,ioxkih,"I had the same issue, but my gdrive was mounted. What is the colab?",1.0
g4j7pm0,ioxkih,"Colab is the site where your code is running (colab.research.google.com). It gives you free compute resources for several hours, including ~16gb gpu",1.0
g4j7q0p,ioxkih,"**I found links in your comment that were not hyperlinked:**

* [colab.research.google.com](https://colab.research.google.com)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g4j7pm0&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",2.0
g4j7szr,ioxkih,"Also, you need to make sure that it is mounted as ""gdrive"". For me, it mounts as ""drive"". For you, the name might be something different, so your overall path to image is different",1.0
g4gimsy,iowfgp,"I finished my MS with just about the cheapest laptop I could buy. Not like a chrome book, but a step or two above that. 

Sign up for both Azure and AWS free tier and bounce back and forth as necessary. Every once in a while you may stumble on some free credit promotion as well, which helps. Also, if you happen to have an employer paid  MSDN account that comes with like $50/month in Azure credits. 

Familiarize yourself with IaC so you can quickly set up and take down environments. It’s a pain transferring data in and out sometimes, but storage is generally very cheap so it’s not a problem to let it sit for a while. 

I’m not sure how much a laptop that could train a serious DL model would cost, but I’m sure that all things considered I spent less than that over those 2 years.",7.0
g4hu04w,iowfgp,Can you expand on what an IaC means?,1.0
g4jjloy,iowfgp,"Infrastructure as Code is a way to acquire and set up cloud resources through a YAML or json file. This makes setting up environments pretty easy.

Check our AWS CloudFormation to see how it all comes together.",1.0
g4gn5nv,iowfgp,"&gt;planning to buy a laptop(PC is out of option)

Why is it not an option?  A good approach is to buy a PC that acts as a server and to use a cheap laptop or even a Chromebook to access it.  It's not necessarily the cheapest or easiest option, but I'd definitely suggest it over investing a bunch into a ""deep learning laptop.""  


&gt;I have heard nvidia drivers have poor support on linux.

Not just false, but also confusing.  Most deep learning is done in Linux, and most deep learning is done by leveraging CUDA to speed up training.  CUDA, of course, is created and maintained by Nvidia.  Maybe you mean something else, but a typical deep learning setup will more than likely involve Nvidia GPUs ran on a Linux machine.  


&gt;If that is the case, I would be better off without a GPU.

Yeah, a deep learning laptop is not a good investment, so I agree with this sentiment.",9.0
g4gthvh,iowfgp,"This is an excellent set of advice! I was going to write exactly the same arguments while going through OP's post. I would just  add the following:

&gt;I also don't know how economically feasible it is to use cloud for training.

[Cloud is expensive, damn expensive](https://medium.com/the-mission/why-building-your-own-deep-learning-computer-is-10x-cheaper-than-aws-b1c91b55ce8c). 

Your home-built machine can be on par, or even faster than the cloud GPUs (the I/O kills the speed). I used to run codes on Google Colab, and then moved to Google Cloud K80 (got some free credits), and also used AWS as well as Paperspace. None matched the performance of my home-built RTX 2080 TI with a \~1TB NVME.",3.0
g4gkube,iowfgp,[vast.ai](https://vast.ai) has crazy value (4x1080tis for $0.614/hr) EDIT: added specs,5.0
g4pf22p,iowfgp,You are forced to pay without pausing though if you don't want to lose your data.,1.0
g4j2j99,iowfgp,"Nvidia is fantastic on Linux. Used it to train multiple models on Ubuntu without a problem. In fact, I actually feel that Windows and Macs are far worse with deep learning, due to software and hardware limitations respectively. I currently have the MSI GE 75 with an RTX 2060 and its more than sufficient for deep learning jobs. I am running POP OS on it (derivative of Ubuntu 20.04)",2.0
g4gjjhd,iowfgp,You’re better off investing in a Cloud Platform with GPU access—you’re just practically not going to get fast results on a Laptop GPU.,2.0
g4gnqdt,iowfgp,"I bought a laptop with an Nvidia GPU for the exact same purpose but I quickly realised that I tended to use the GPU less and less as time progressed. I have been using pop os and they have pretty good support for Nvidia cards and I haven't had any problems but for me, that hasn't mattered much because I haven't used my GPU in about 6 months. The fact is for any serious project, a laptop GPU just won't be fast enough/ have enough memory. I had a cluster at my university which I've been using so be sure to check if your university has any such services but if not, I think cloud services might be a nice option too.

And although I do really like my laptop, I don't think I got much use out of my GPU. The only things I used it for during the course of my degree were running toy examples and just working with small datasets to learn. Other than that, I've used the university cluster for pretty much everything because it was just faster and had a lot more memory to work with. 

So pretty much the added heft of the GPU and the back aches were for nothing.

TL;DR: from personal experience of going through university with a heavy laptop with a GPU, I would recommend against a dedicated GPU because other options will just be faster and have more memory.",1.0
g4gry3p,iowfgp,"Yeah you're not getting anywhere with a laptop. You'll have to get a desktop, an external GPU or use cloud. Just a heads up, free cloud services like Colab are unusable during midterms and finals - hope you have a good algorithm to manage your checkpoints, otherwise you won't get anywhere with the constant crashes.

I find popOS good for DL on linux, they have stable nvidia drivers and never really had problems on single and dual GPU builds.",1.0
g4h5ejy,iowfgp,I have used tensorflow just fine on Linux with an Nvidia GPU. Your information might be wrong.,1.0
g4hdupg,iowfgp,I have been using linux for deep learning (more than 2 years) just because it is easier to install things than windows. Never had problems with nvidia drivers so far.,1.0
g4hesp8,iowfgp,"Laptop GPUs have to be thermally optimized for running in tight, low ventilation spaces. They just don't have the power to be worth it. There is however a lot of affordable online GPU hosting. 

* https://colab.research.google.com/
* https://www.kaggle.com/notebooks
* https://gradient.paperspace.com/
* https://www.floydhub.com/

And of course more advanced options like AWS, GCP, and Azure",1.0
g4wcoax,iowfgp,"\&gt; I have heard nvidia drivers have poor support on linux. 

You heard wrong. Linux is the number one target market for AI and DL projects and Nvidia's hardware is at the top of the hardware stack for those tasks.",1.0
g4it7k4,iov9qy,"Hi, tbh I have never been able to make prefetch work as described in the doc, however I remember it was stated in the document ( at least back in tf1.x) that prefetch should be your last operation on the dataset. Also I can not see why you are doing (repeat) as your last operation, at least I always use that as the first operation like repeat() so the dataset will be repeated infinitely. This way you need to take care of num_empochs through fit operation. Again, having all these said, I still have never been able to make prefetch work! Good luck",1.0
g4jikde,iov9qy,"Hi, thanks for your reply. I exactly wanted to repeat the dataset for specific number of times, and wanted to repeat exactly the same number of batches from the start. So I think it could be at least after .batch() command. However, there was no reason to put it after cache() and prefetch().

Sad to hear that you also had not been able to use prefetch as they claimed on their guides!",1.0
g4g7qra,iouioz," Schirrmeister's paper ""Deep learning with convolutional neural networks for EEG decoding and visualization"" is a pretty good analysis of CNN's for raw eeg signal classification. There are a couple of relatively good review papers on deep learning architectures for EEG tasks as well.",2.0
g6m7j1e,iouioz,"Hey thanks for the reply!

I have actually been attempting to replicate the exact methods of both the Schirrmeister paper models (using BD-Deep4 and BD-Shallow) in a different dataset and am now looking to try using some other models (ChronoNet for example). I haven't been able to achieve the same level of performance (or anything above chance TBH) though acknowledge that the classification task and dataset are different.

Are you working in this domain yourself?",2.0
g6yijx9,iouioz,"Yep this is the research domain I work in. 

How are you pre-processing your data? (are you taking care of artifacts correctly?) 

How much data do you have and what kind of data is it? (are you using or can you use a sliding window? could you do any type of data augmentation?) 

How many channels are you trying to classify? (raw data classification is really hard if you are using a lot of channels (or without a LOT of data). would your paradigm benefit from channel or region of importance analysis?)",1.0
g75n3gk,iouioz,"I am fortunate to have access to a large dataset - 2 in fact. First is n=\~1000, using \~80000 2s epochs, de-artifacted. Have considered the possibility of using a sliding window of 0.5s to increase the number of epochs. 26 channels in use. The dataset has been analyzed in many papers previously, this is the first attempt at applying end-to-end deep learning models. We've been very thorough with everything up until the point of applying the DL models.

Now looking to use a larger database with \~4x as many recordings. 

I guess I am at a point that I'm questioning my application of the model or the model designs themselves. I intend to sink some time into visualising activation layers or developing 'heat' maps to get a feel for how the models are actually learning the data.",2.0
g76qxgc,iouioz,"Just FYI, an epoch is one training iteration through the entire dataset. So a smaller window size will give you more data points, not epochs.

26 channels of raw data may be too much, depending on how else or if you've done any further pre-processing. What's the sampling rate? If you haven't downsampled at all and it's, for example, 500Hz collection, that's 26000 values for each 2s data point. I'd look at a smaller window per data point just to reduce the input size.

It's cleaned EEG, but did you apply any other filtering? It's pretty common to filter around specifically applicable band powers (I think you could probably still consider this end-to-end, if that's the goal).

How many epochs are you training the classifier? What learning rate? What's the loss curve look like after a training session? (did the loss reach a lower plateau? Is it showing decrease in loss at all, or is it chaotic?) How did you do hyperparameter tuning?

You're having trouble with with dataset that's been analyzed in other papers, so where do your models differ from the successfully implemented models? You mentioned that no one has done end-to-end learning learning with this dataset, but end-to-end has been done elsewhere, so you may be able to glean some insights by studying those papers as well.

edit: are you looking at subject- or session-dependent models? Or are you throwing all subjects/sessions into the same classifier? Not necessarily a deal-breaker, but inter-subject and inter-session differences can be pretty difficult to overcome for some tasks.",1.0
g78ygnu,iouioz,"Clarifying - when I mentioned epochs I was referring to the epoch of EEG data- the segment of 2s of data being fed into the model. Not the ML epoch as you have outlined - sorry, didn't think to specify at the time. I'll replace ""EEG Epoch"" with EEG segment.

Sample rate is unaltered and yes is high at 512Hz. I did consider this and I have seen that other papers have down-sampled to around 256Hz or less... however the feedback I had when raising it was to try without down-sampling first. No other filtering or modification of the data has been undertaken - aside from data scaling in a manner similar to the Schirrmeister paper.

Training on ~50,000 2s EEG segments of data - Validating on ~12,000 and an untouched ~15,000 for Testing later. Standard binary classification task - can the model learn group A from group B. Multiple segments of data from each person so I have ensured that data from person x is only ever in one split (either Train, Test or Val). Also only 1 recording session per person.

Am able to achieve perfect overfitting on the Training Data with my own model - now starting to implement regularization - maybe where the answer lies. Loss curve is good in this model for training data but immediately rising in Validation data. I have also implemented EarlyStopping when testing early versions of models and 100 ML epoch runs when testing more thoroughly. Using the models from the papers I was able to get decent loss curves however never achieve overfitting on training data.

Hyperparameter tuning has largely been based off of intuition, persistence and guess work. Learning rate(static, schedulers), kernal sizes, implementing class_weights, init_bias, kernel_regularisation etc etc. All models already use high dropout rates (0.5) so I am now working through L1/L2 regularisation layers to observe their effect on performance in validation data.

Thanks for all of your input - not too many others I know who can provide insights and feedback at this level. I appreciate it!",2.0
g7ahcgi,iouioz,"Here are my thoughts on your clarifications:

* Yeah, 512 is really high so this could a problem
* Not filtering at all could lead to a few problems. Depending on your task and how each class was collected, you may encounter difficulty because of baseline drift (which can occur to differences in the temperature of the conducting gel, among others). I' d recommend HPF above 0.1 Hz minimum
* While you may have cleaned signals, it is very difficult to get rid of EMG, so typically people just LPF below 30, 40, or 50 Hz so they don't have to worry about it. I know you are trying to end-to-end learning and perhaps you'd rather not filter at all, but I still think you could consider this end-to-end as your data has already been cleaned. On a side note, how did they originally get rid of the artifacts? ICA? I ask because if they used non-real-time applicable methods, it can complicate your argument for end-to-end learning. 
* Can you clarify on how you split a subject's data? You say ""data from person x is only ever in one split (either Train, Test or Val)"". If you mean that data from one subject is only ever in Train OR Test OR val, this is likely your biggest problem. If the above is correct, you are trying to do transfer learning in that you are using data from some subjects to predict classes from other subjects. In fact, it's even harder as you are saving weights on validation loss (I assume), but the validation data doesn't contain data from subjects in the training data. I'd recommend first trying to train subject-dependent classifiers. So only a single subject for data in the train, val, and test sets (make sure your classes are balanced and that you are splitting depending on the session time, rather than randomizing (so, i.e. first 80% of both classes for train, next 10% of both classes for val, and final 10% of the session for test. You shouldn't randomize (if you are using a sliding window) because then the classifier will have already seen VERY similar samples in both train and test sets). Then move on to subject-independent classifiers. and then to transfer learning. But there, I'd still have the same subjects in train and validation at the minimum, as your model is probably only saving the the weights from the first couple of training epochs as the validation loss goes up very quickly (so not much learning is actually accomplished). If I understood what you said correctly, this is probably your biggest problem (especially since you are successful in fitting the training data).
* No problem! Pass it forward!",1.0
g86htew,iouioz,"Just a little update - my supervisors and I recently had a meeting with others who have quite a depth of experience in the field and their recommendations lined up perfectly with your first two points - we are now going to implement 0.1 LPF and a 100Hz HPF as well as reducing the sampling rate to 100Hz. So it's nice to now have two different sources that have recommended this strategy - there was some initial hesitancy to implementing this.

Regarding subject data and the splitting process, each subject had ~2mins of data recorded, which was then de-artifacted via a 3rd party before being provided to us. The artifacting process was then validated and found to be highly reliable - there are not any major concerns surrounding the artifacting process. The 2mins of data was split into ~2s epochs of cleaned data (unfiltered, 512Hz). You are correct in your description of the splitting process, and that we are seeking out ""transfer learning"". The models is training, for example on persons 1,2,3,4,5 being validated on persons 6,7,8 and will be tested on persons 9, 10. You say that this is perhaps the biggest problem, but the reason it was decided to undertake this was to avoid the possibility of the model learning and recognizing individual features from a single participant. This is really interesting, I'll have to think about this a bit more... I can see how this might be a big factor in limiting the performance of the model. Great insight! Thanks!",1.0
g4t6enx,iouioz,"I don't know whether you are classifying or searching for anomalies, but this is a time series and there may be other time-series research papers that are useful. Usually, there are much more useful variants formed by rebasing the series from time to another feature. My initial thoughts are that the FFT of the EEG may be a more useful input than the raw time-based data. With FFT this step wouldn't be an ML task of course. Depending on what you are doing, a spectral analysis (usually in ML anomaly detection libraries) may be better than FFT?",1.0
g6m5zjt,iouioz,"Hey thanks for the reply. The focus of the research is really to determine if an end-to-end model can outperform feature extracted data - effectively hoping to establish a model that can extract and utilise frequency information (or other features) without applying those filters ourselves. As the other commenters mentioned there are other papers out there but I haven't been able replicate or achieve the same degree of success that has been shown in such papers.

Thanks for the input!",1.0
g4g52xo,iotqco,"&gt;Now should I go straight to deep learning or should I continue doing the ml models in python only and read the academic papers?

Most researchers are not doing everything in pure Python, so if research is your goal, then it'd be a good time to start getting comfortable with a deep learning library (i.e., PyTorch and/or TensorFlow).  You can certainly do things from scratch and learn the inner-workings of deep learning that way, but at some point you're gonna have to learn a proper DL library and your ""manual"" work will really only serve as a reminder as to why most people don't do it that way.  


&gt;I want to be someone who researches in AI and not just sit behind a desk working a 9 to 5 job.

I hate to break it to you, but research is basically another 9 to 5 job but with less pay (give or take).  This shouldn't discourage you by any means, but it's important to understand that you'll only be paid to produce, and even if you go into pure academic research doing something you find interesting, you will still be at the mercy of whoever is doling out the grant.  On the flip-side, there are plenty of non-research careers that offer plenty of interesting challenges and room for problem solving that can require more critical thinking and creativity than many researchers ever put into their work.  For what it's worth: I'd rather be an engineer doing something interesting than a researcher doing something boring, which is a fate that many grad students can attest to.

I'll also add that DL research still requires a lot of menial work and engineering skills.  You may be focusing on different aspects of a project versus your engineering counterparts (e.g., you may not be concerned with scalability or end-user accessibility, etc.), but if you don't know how to engineer your research, then you'll get stuck at the same point as anyone else.  In my experience, grad students tend to really lack in engineering skills, which can really hinder their work.

If  you really want to get into research, then I think a more traditional (note: not necessarily better) route is to learn the math/theory on ""paper"" then use these proper libraries and tools to build the implementation.  You'll find that this how a lot of graduate schools structure their classes, and for good reason.  Although implementing stuff from scratch can certainly help you learn the theory, it may not be a very efficient way of doing so.",12.0
g4gj8yg,iotqco,"&gt; I hate to break it to you, but research is basically another 9 to 5 job but with less pay (give or take).

This is why I left academia, now it's just the same but with money.",4.0
g4ix4my,iotqco,"When you say grad students ""don't know how to engineer their research"" what do you mean?

I'm a PhD starting out and using DL in the health domain. I have a good field knowledge and serviceable python but no software design/engineering background and I do feel like it's impacting my work and I desperately want to improve.

Is this what you refer to? And if so, do you have any advice to develop those ""engineering"" skills?",3.0
g4jblz6,iotqco,"When I say ""engineering skills,"" at least in this context, I'm referring to the ability to implement an idea or concept, especially at scale (in the development sense). A lot of grad students (at least in my experience) are capable of formulating novel ideas without too much issue, but will fail to be able to demonstrate those ideas practically.

I've seen students struggle to wrangle data, write maintainable code, use remote servers, etc., and it really hurts their progress with their projects or research. Some of the time it is simply a matter of learning how to use something (like cloud instances or a new library), while other times it comes from being able to recognize how to solve something (like how to get pertinent data). I often see such issues occur in sequence which compounds the issues, e.g., a student writes ugly code using out-of-date libraries on their local machine using pre-processed data and will essentially hit a development wall where their only feasible option is to start over (i.e., they can develop an isolated portion, but fail to develop ""at scale""). I'll add that I've seen plenty of professors fall into the same issues, too, but their ability to pawn the work off on a student allows them to avoid it being detrimental enough to warrant having to learn anything new (for the most part).

As far as how one can overcome this, I'd say it basically just takes practice. Of course, practicing on an active research project is probably not the most effecient way to learn these things, but it seems like that's what ends up happening most of the time. I've seen plenty of older grad students learn to avoid or anticipate common engineering issues, but will still not be able to generalize or adapt to new things since they only learned their narrow set of engineering skills by solving narrow problems they faced.

Classes tend to do a bad job of teaching these skills (at least in my experience). I found that doing personal projects and working as a web developer has taught me skills that are really valuable to grad students and research. A lot of these skills aren't specific to ML (such as cloud engineering), but can be a critical component to ML projects (you'd be surprised how easy it is to build a simple web interface, and how many grad students with ML backgrounds wouldn't even know how to get started).

I suppose the typical approach to learning these skills is to find an internship or something. Doing projects in other domains (such as web dev) can also be a quick way to pick up a set of skills that carry over effectively. Obviously doing ML projects can help, too, but I've noticed people tend to focus on the parts they expect or are easy and neglect the ""auxiliary"" tasks that they specifically should be trying to actively learn. Expanding outward into other domains can help to prevent this.

I think following along with an online course or something could be a useful approach. You can knock out full courses in front-end development, back-end development, game development, robotics, etc. in a few weeks and learn enough that it's worth the time and effort.",2.0
g6mdr1o,iotqco,Thanks for the thorough reply! Very useful!,2.0
g4g91ef,iotqco,"\&gt;  I want to be someone who researches in AI 

Then you'd need a Ph.D. And with masters of science, you'd have to be an exceptional student to do research.",1.0
g4gsjig,iotqco,"That depends on where you live, unless you consider my 1.9 GPA exceptional. I guess what you wanted to say is you'd have to be exceptional to publish papers. But the thing is, people my age don't publish papers as students, but as interns, so again, has very little to do with academic performance.",0.0
g4fbbi5,iopwr3,"That’s the most common missconception in this ML/DL world, the fact that everyone is worrying whether or not and when they’re gonna run out of computing power. I’m a beginner just like you(been in this world for a year or so), I have an RTX 2060 Super, and it’s more than enough. Once you get to those research levels of computing and programming, you will by then know what sort of paralel computing to implement, or to train on cloud etc etc(if the graphics card is not sufficient). But for right now, if your budget allows you to buy that card, go for it and don’t worry about things that are out of your scope right now. When the time for upgrading comes, you’ll be experienced enough to know what to do about it",31.0
g4fmwdr,iopwr3,"Well said, sir",5.0
g4fdzoo,iopwr3,"It will work just fine, unless you get a defective card. It will make some fan noises, heat up the room a bit and be noticed on your electrical bill. But it's cheaper than cloud computing.

Source: I own a quad 2080 ti machine and have helpt build a cluster of 20x8 2080 ti at the lab for DL/ML.",6.0
g4fnixs,iopwr3,"Cool, thanks for sharing your opinion",4.0
g4gzx2y,iopwr3,The heat up room thing is very convenient in the winter by the way,2.0
g4g15c4,iopwr3,"I've had 2 1080 Ti's that I've been stressing the hell out of for years. With sufficient cooling they'll last practically forever. The chip itself won't die, but some other discrete component may fail over time -- capacitor, MOSFET, etc. That being said, if you get it directly from NVIDIA, they use pretty good components.

It will definitely last you until it becomes obsolete. I have graphics cards from 15 years ago that still work.",6.0
g4g3qa2,iopwr3,"Ok, so 'Founder's Edition' are more reliable, that is pretty useful information. Should I wait for RTX 3070 Ti instead, given that it will likely have 16GB vram? I'm still confused between 3070 (8GB), 3080 (10GB) &amp; 3070 Ti. You see, I would be practicing DL at home, while taking an online PGDM course on ML &amp; AI, and I would be gaming at 1440p (AAA games). Do you think 8GB 3070 will suffice or should I consider other options? If Ti versions would launch in next 3 months, then I guess I can wait.",1.0
g4g5nhq,iopwr3,"Depends on what kind of models you'll be training. Some of the larger models require a lot of GPU memory. If you are going to be using large/deep networks with a lot of layers or larger layers, you're going to want more VRAM, so I would wait for the 16 GB card.",2.0
g4g7bl9,iopwr3,"Super versions of RTX 20xx launched 9.5 months after release of originals, whereas Ti versions of GTX 10xx launched 16 months later. There's a good chance that Ti versions of RTX 30xx will launch in Q3/Q4 of 2021, which is way too long to wait, so I guess I'll have to consider 3080 because of 10 GB vram. I'll look at some benchmarks before deciding, but it's really hard to figure if 10GB vram will be sufficient or would I need more.",1.0
g4gant6,iopwr3,"Given the money you could buy 2 cards for 20 GB. I'm pretty sure TensorFlow at this point manages multiple cards fairly well. That's a lot of money though.

I have 22 GB of VRAM (with 2 cards) and I don't think I've ever come close to maxing it out with even the largest models pushed to their max. 

Most models will be below 10 GB I'm assuming. If you need more for a particular model, you can always just use a cloud solution.",2.0
g4gczc2,iopwr3,That clears lot of my doubts. Thank you for answering!,1.0
g4fbbo3,iopwr3,"GPUs are pretty reliable IMO. Our lab has dozens of GPUs (e.g. Titan X (Pascal), Titan Xp, and 1080) running 24/7 since 2016/2017 and they are still working perfectly fine. Most of the time they are idling but often we train models for a couple of days non-stop. On a quad-GPU workstation the temperatures are usually &gt;80C during training, so we are heating them quite aggressively.

If you have only one GPU and want to train occasionally, don't worry too much.",5.0
g4g1m9n,iopwr3,"You can have cards do mining continuously for years without issue. Also, the warranties are usually pretty good.",2.0
g4g3wjb,iopwr3,"If I'm not wrong, Nvidia's extended warranty is upto 3 years and I only have to register on their website for that?",1.0
g4g4hac,iopwr3,If it's like all the other cards like msi or evga then yes.,2.0
g4gj1p3,iopwr3,"I wouldn't worry about how long the card is going to last.  Unless you are running in an extremely hostile environment the card should last many years.  The first thing to fail is likely the fans and if you replace the fans the card should last many more years.

I train squeezenets and mobilenets on RTX 2070 cards, typically with around 200k samples.  For that particular use case I've found that two cards buys me nothing at all, so I reworked things so I can run two training different training runs simultaneously on the two different cards.

If you are training larger, fatter models (e.g. Resnet or Inception) multiple cards starts helping you more.

For the RTX cards, 16-bit training (in tensorflow ""mixed precision"") will buy you as much of a performance boost as two cards will.  You might have to fiddle with the shape of your model a bit to get the most out of it.

I'm planning to trade in my dual RTX 2070s for a single RTX 3090 in December or January.",2.0
g4gxzf6,iopwr3,"The most comprehensive article on  choosing a  GPU for deep learning has been updated with information for new cards. 

https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/",2.0
g4h3bw0,iopwr3,"This should be really helpful, thanks!",1.0
g4fad5l,iopwr3,Will it electrically last? Sure. Will it still be useful? Sure. How useful? Who knows.,4.0
g4femfi,iopwr3,"Mind expanding on what it means that a card will ""electrically last""? Also, ""how useful""... what context are we talking about?",-2.0
g4fkien,iopwr3,"Will it die on you? Nope. Will it become so outdated it is useless? Not for a long time. How outdated will it be in 5 years? Not a clue, and anyone who thinks they know is a liar or an idiot",2.0
g4g3q9o,iopwr3,"Are you trying to avoid explaining your remark on if it will ""electrically last""? Yes! Did anyone claim they could predict hardware evolution five years from now? Nope!",-3.0
g4g44wx,iopwr3,Oh for fucks sakes it is very simple: Electrically last = won't die = in 5 years if the GPU is kept under reasonable operating conditions if you plug it into the PCIe slot of a compatible motherboard and supply it with the requisite extra external power it will be capable of rendering a display. Jesus fucking christ you're thick,1.0
g4g56qw,iopwr3,Aha so will it electrically last = will the card electronics last. Yeah man I'm the one who's thick lol,-1.0
g4fgemn,iopwr3,"2 years ago I bought a 1080Ti for DL purposes. I have almost never used it for DL. The university provided a server with a 1080Ti on it. I am now applying for jobs and either they have gpus or they work on cloud. If you are a beginner, save your money and work on Google Colab until you get enough experience with DL before buying an expensive tool.",1.0
g4fv5ox,iopwr3,"same here, I bought a 2080ti for serious DL and some casual gaming and ended up doing serious gaming and some casual DL instead.",7.0
g4g8flr,iopwr3,"I got a 980 TI for that purpose when it was new. It was a dumb idea. When I used it enough hours to make the cost net out as a better deal than the cloud, I couldn't use the machine for gaming because it was always training! When I used it few enough hours enough to make my machine available for gaming, I may as well have just rented those hours from a cloud provider. When you're doing any cost tradeoff calculations, make sure to include your electricity cost (of the GPU AND the rest of your machine)

Anyways, a big question now, depending on which field you're in, is model scaling. Some models are getting huge, so RAM is a limiting factor, which suggests using a V100/A100. Maybe that trend holds and becomes a real dominant factor, maybe it doesn't.",1.0
g4fb9y8,iopwr3,"Hard to say as it has just been released, no long term data.

1080Ti was a great success from performance jump and being reliable card.
2080Ti suffered from inherent memory corruption issue, had to send my one back and sold the replacement. 

3080 does look like a nice card!
Get a GPU with a good RMA policy and possible extended warranty. 

For DL I would just get 2 X 1080ti (almost the same memory size as 3080) and wait a year for the 3080 prices to drop.",0.0
g4geyg8,iopwr3,don't lie to yourself,0.0
g4gjfvr,iopwr3,what do you mean?,1.0
g4fc5dv,iopwr3,"3080 is an excellent card for DL/ML.  Its almost twice as much performance as a 2080Ti.  I am sure they will release a Super or Ti version of the 3080 in the next 3-6 months and that will likely have more memory.  3080 has Tensor Cores, so it can do half-precision, which almost doubles your memory and is a lot faster than full precision.  It's an amazing value.",-2.0
g4flb0k,iopwr3,"Are you a teenager? Because, it's a device. It might break right right after the warranty period. It might last more than half a decade. I know people who still use a card from the 700 series.",-5.0
g4fk35s,iopqr4,"I recall seeing similar problems in ESRGAN’s paper and they removed batch norm for that. I think checkerboard artifacts also play a role here, you can have a look at distil’s blog post https://distill.pub/2016/deconv-checkerboard/.",1.0
g4fmadv,iopqr4,"thanks for your reply, but there is not deconvolution in the network of cartoonGAN, and I think it is not the  checkerboard artifacts .there is more generated images .  
[https://www.imgurupload.com/uploads/20200908/f813def280b1b3b2a46328d659d0d71ceffb4019.jpg](https://www.imgurupload.com/uploads/20200908/f813def280b1b3b2a46328d659d0d71ceffb4019.jpg)  
[https://www.imgurupload.com/uploads/20200908/ca33eb25900402c042f80ade2306deed5503998b.jpg](https://www.imgurupload.com/uploads/20200908/ca33eb25900402c042f80ade2306deed5503998b.jpg)",1.0
g4hbtlz,iopqr4,Its because Harry Potter can speak to snakes,1.0
g4i6kr9,iopqr4,"hahaha, nice idea",1.0
g4f61es,iof8wd,"I know it's probably not what you are looking after, but I would recommend you to read [this post](https://www.reddit.com/r/MachineLearning/comments/ioascy/d_which_gpus_to_get_for_deep_learning_updated_for) and its linked blog.",1.0
g4fel72,iof8wd,"Yeah, I viewed those already. I'm interested in GANs specifically. I think people who trained a bunch of GANs themselves would be able to give a helpful opinion.",1.0
g4gxmbv,iof8wd,"Get the 3090 if you can afford it, especially if you're trying to train GANs at &gt;= 1024 px, memory is really the limiting factor. 24GB is a huge upgrade from 10GB, which makes larger models possible. Remember that most of these models were initially developed and trained on cards with 16GB or at minimum 11GB.",1.0
g4j4l7x,iof8wd,So you'd say 1x 3090 trumps 2x 3080 for GANs?,1.0
g4rpa09,iof8wd,"not the person you're replying to, but 24GB RAM in the 3090 certainly future proofs you over the next few years. suggest 1x 3090 for training and 1x 1080ti for experimentation while your 3090 is training if budget allows.",1.0
g4t78br,iof8wd,"Yeah, I'm also thinking along those lines. Plus 3080 comes out sooner. =) Regarding future proofing, imagining the future I would guess that in a year they may roll out a card that would be 3090 equivalent with entirely usable 16GB at some $1000 price (not accounting for import fees and sales tax). At which point old 3080 would probably sell at like $100-$200 loss on secondary market. So effective price (with import and other fees) of purchasing 3080 now then swapping for new ""3080 ti"" 16GB 3090 equivalent would be around $1400-$1500, while directly buying 3090 would be about $1900-$2000. IMHO, future-proofing is a bad idea for these rapidly depreciating assets.

&amp;#x200B;

..Looks like mining has become somewhat profitable again..",1.0
g4vl0ig,iof8wd,"\&gt;future-proofing is a bad idea for these rapidly depreciating assets.

Depends on your total system refresh rate. Swapping out a brand new GPU every 2 years might introduce bottlenecks at the CPU / RAM / motherboard / storage / power / cooling level necessitating a near total system refresh to take full advantage? I'd rather have a massive system up front that is stable, well known to me, and can serve me 4-5 years. 

For ex.: 2x2080ti, i7 9800X, 64GB RAM in Nov 2018 for $5200. I have used this to great effect for academic pursuits the past 2 years, and expect another 2-3. Hope for another $5k system upgrade in another 2-3 years at RTX4xxx launch.",2.0
g4xo6bx,iof8wd,"Well, you could get a magnitude better CPU at same price from AMD since a year ago, so just a year into it. Leaps in CPUs and GPUs happen asynchronously. With CPUs and mobos you may also run into software license issues that will prevent upgrades if it's your desktop CPU, that's one thing to mind, though.

&amp;#x200B;

I'll probably still use my old i7 4960x CPU with 64 RAM I bought in 2014 as I don't think this will add any latency really since NVidia seems to have made image decoders and stuff run on GPUs now. So I expect very low load on CPU. As for having most GPUs run on x8 PCIE3 I think that will also not be an issue - even if I'll need them to communicate for multi GPU training I think I'll be able to shave off a lot of latency with some things like gradient accumulation. Once that will really become a bottleneck for me then I'll upgrade to some Threadripper or something. Currently run 4x 1080Tis on that server, decided to skip 2080Ti as 1080Tis were trading too cheap on aftermarket due to extra capacity from miners, so it was unreasonable to upgrade. Plus in terms of mining 2080Ti was almost the same as 1080Ti and it makes a lot of sense to mine when you're not training anything. But now they still cost the same as they did like 1.5 years ago on the aftermarket here where I live, even as I probably something like recouped their cost during that time through mining, and 3080 will come out at 2.5x that price but with probably 1.5-2x mining rate and higher effeciency and like 5x faster training if considering mixed precison, so upgrading to 3080 is basically a default action for me, the non-default one to consider is 3090.

&amp;#x200B;

IMHO, in your case I'd try to sell 2080ti's at a decent price and if succeeded, get 3080s or 3090 depending on what's better for you. You can wait for some 3080 ti or something, of course, but don't do it simply due to laziness. Check out how much you can haggle on your local aftermarket for those 2080 ti's. 3080 is going to be like 1.5 faster and you may not really have to pay a lot for that upgrade.",1.0
g505vni,iof8wd,"A six year old CPU, motherboard and RAM speed to feed a new GPU for the next 2+ years? Yeah, you're going to be CPU bound.

Mining on a GPU? Nope, ASICs is the only reasonable platform for that. I donate my spare GPU cycles to Folding@home, whatever I'd get from bitcoin mining is trivial and I'd rather further science with my electric bills. If you're mining on a GPU you are absolutely doing it wrong.

Buying top-of-the line GPUs every 24 months and selling off previous editions as soon as they're obsolete means you're selling in to a buyers market, and always paying a premium. You're also bound to 2-3 generations of older compute architecture which WILL bottleneck your new card.",1.0
g52h87a,iof8wd,"Ram speed on that one is much faster than on consumer cpus, as it’s 4 channel. But that doesn’t really matter all that much, because neural networks are bound by the speed of gpu memory and tons of operations. If your NN is bound by cpu memory, I would guess that it’s either you are doing some special thing like running simulations and doing “reinforcement learning” (or semi-supervised learning, or whatever, basically play learning), or, maybe, feeding it wrong in which case it’s not really a cpu problem and you’re going to add a lag no matter what. I donno, maybe you can get bound by cpu during inference in some special case, but for training, I’d listen to your scenario of how that is possible, because I can’t really imagine that. Also, it’s obviously not applicable to GAN training and for those my CPU and ram aren’t going to be a bottleneck for any foreseeable future whatsoever. Even old PCIE probably won’t for the next 10 years, probably even in the case I’ll want to do parallel training in 10 years, because I’ll just do a 1000 gradient accumulations for a single update and that’s it. Not that cards in 10 years are guaranteed to work on existing hardware, though.",1.0
g5qxioo,iof8wd,"Actually, maybe, keep your 2080 Ti's. For TF16 mixed precision training memory speed is the only bottleneck, as far as I can see. And in those terms 3080 is just 24% faster. Seems like 3080 will be available for buying at reasonable prices only in a few months, so.. Just ordered 2060 Super on the aftermarket. Will try and see how that one works for me and how good is this TF16 training.",1.0
g4j7esr,iof8wd,"One other big point for going for 3090 I see is that Tensorflow (but not PyTorch) seems to have decent grouped convolution kernels and now there are cool architectures that use them, like EffecientNet. Grouped convolutions are bound by memory speed and size.",1.0
g4ct3dw,iobuf5,Thanks,2.0
g4f6pll,iobuf5,Now just hoping RTX3090 isn't as gimped as 2080Ti was in RMA rates...,2.0
g4dh4vf,iobuf5,Your blog is returning a 500 for me,1.0
g4exb0u,iobuf5,"TLDR, get rtk 3090 if you can afford it.",1.0
g4f6j48,io6dmu,"For me, that would be when you are tackling a problem in which you are completely confident that overfitting your model is not a risk, either because it can't happend or because somehow it is not a problem.

Also another exception occurs to me, which would be when you have a loss that is not necessarily significant of the model's progression towards ""solving"" the assigned problem, such as happens when GANs use crossentropy. But that is generaly something undesirable on its own.",1.0
g4f7cbk,io6dmu,"Something I've had some success with lately - instead of early stopping I just accumulate more and more gradients as training goes on. The thing is that the viability of early stopping is network dependent. I'd rather use techniques that work on all architectures similarly. So far, creating a pacing function for gradient accumulation solves LR scheduling for me and the results I've had since I've started suggested it stops training after a while anyways because the norm of gradients converges to 0.

I learned early that early stopping can make things very bad if you don't understand your model. Even when I was finetuning a simple ResNet50 backbone detector it killed my performance (\~42mAP instead of 58 I have now) because I've had double descent going on, before it was even a paper or I read it. So I guess you should only use it if you understand your model well, understand that it can benefit your model and if it is beneficial (however given the existence of the double descent phenomenon I'd probably suggest finding other methods of regularization, if you think of model complexity as a function of epochs).",1.0
g4c5cr7,io5sn1,I am currently passing all the TF code of the deep learning specialization assignments in PyTorch and contextually trying to implement the results of some other papers mentioned during the course. I am founding it to be a rather useful exercise TBO and I am keeping on learning new DL stuff,2.0
g4c5q7r,io5sn1,"alright, thank you!",1.0
g4bppsp,io5htw,Isn't this an actual portrait of someone who resembles Keanu? I'm pretty sure this isn't AI generated,2.0
g4c281v,io5htw,"I don’t know sincerely . But it isn’t the only portrait I saw , there is plenty of them",1.0
g4bko0b,io5htw,Style transfer GAN,1.0
g4bl4go,io5htw,"I already used Style Transfer . But look at the right image , the content ( the face ) has moved to the side ,we can better  perceive his jaw .
The style transfer itself doesn’t change the content , it preserves it .
Here in the image , the content changed",1.0
g4bliw8,io5htw,"If that's required your request is not defined very well. Do you want some random distortion applied additional to Style Transfer? Probably not.

So what's the goal, actually? Where does that example come from?",1.0
g4blqzu,io5htw,"The request is well define : AI portrait , and it will be the first result you ´ll find on google .
I asked the question because I don’t know exactly what kind of method it’s applied ""precisely""",1.0
g4bp1gt,io5htw,"They didn't release a paper or anything, so all we can say is that it's a GAN trained to generate portrait paintings. 

There's nothing that keeps a GAN from changing or distorting the content of a picture, it's just that usually Style Transfer GANs are explicitly trained to not do that to the extent we see here.

I'd argue that's just a result of the dataset used.",1.0
g4cgrhv,io5b0a,Might want to try CycleGAN. It does domain-to-domain transfer and doesn't need paired images.,8.0
g4cgs2s,io5b0a,I would use StyleGan. This is the exact use case that StyleGan is for.,5.0
g4eg32u,io5b0a,I'm pretty sure https://portraitai.com/ uses CycleGAN,3.0
g4fkfkd,io5b0a,It doesn't =),1.0
g4bo4pr,io5b0a,"Some sort of face segmentation to figure out body/fave features, then closed end photoshop transforms to:
apply light
Blur background
Increase saturation in the iris 
Decrease clarity in skin
...whatever you consider photo treatment",5.0
g4bpse8,io5b0a,GANs or Autoencoders?,4.0
g4c0xj5,io5b0a,"There was a website called AI portraits ARS that was online for all of like two weeks.

It did not just take a picture and apply a paint filter and blur. It actually re-painted a similar image using a combination of many old renaissance(?) Style paintings. 

It went offline and said it would be back after upgrading but..its been like a year and I've never seen an AI do something like that since.

It would be cool if someone out there knew more about AI portraits ARS.",2.0
g4eg1dv,io5b0a,"https://portraitai.com/

It still works, but you need to get their app.",2.0
g4ennny,io5b0a,Wow! that is the one. Got the app and it's just as good as I remember. Maybe a bit more refined (and limited in scope) Thanks!,1.0
g4c6ql7,io5b0a,There are commercial packages like ‘portrait pro’ that basically give experienced photogs control over transformations and enable them to fine tune the body/face feature selections.,1.0
g4c8fng,io5b0a,My concern is test data - you’d need a plethora of real faces with their matching portraits,3.0
g4cm0yt,io5b0a,a form of GAN probably,1.0
g4ezvmg,io5b0a,Neural style transfer might work too,1.0
g4bk4pk,io1u8o,i'd stop training when performance on the validation doesn't improve anymore for a couple epochs. i think you should take a look at other performance metrics than the loss. mAP seems more descriptive of the actual performance.,3.0
g4b8c4i,io1u8o,"Just curious, What is the slight customization you have done?",1.0
g4b9sib,io1u8o,"I have added a bounding box rotation to the output. I am working in a project with images similar to aerial images, and hence the rotation is very benefivial",1.0
g63kp66,io1u8o,"This sounds super useful for my project, could you describe your strategy of applying this customization?",1.0
g4anzfp,inu5du,"FYI, this only supports single GPU for now, no DP or DDP",2.0
g4at06n,inu5du,From the FB Blog: https://ai.facebook.com/blog/introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy/,2.0
g49tny9,insgub,"I've been looking into building a 2x+ 3090 rig myself and what I've learned is the CPU will be important in not bottlenecking this GPU. If you're looking to have anything close to 7 3090s then you need a far better GPU, preferably threadripper 3990x.",1.0
g4svgl1,insgub,"CPU requirements are down entirely to your model and any data-prep steps that you do. It's perfectly possible to drive any GPU from a single core, if the training pipeline is efficient. Most pipelines are.",1.0
g4bsgp7,insgub,"Points left for you to consider:

➊ GPU size and using risers cables. You won't be able to fit a ton of 3x height GPUs so easily. Also, you might want to use a custom open-air configuration (like in [https://linustechtips.com/main/topic/1064204-7-gpu-rtx-2080-hybrid-liquid-cooled-cinemaoctane-render-supercomputer-rig-aka-thanks-plx-chips-one-of-very-few-in-the-world/](https://linustechtips.com/main/topic/1064204-7-gpu-rtx-2080-hybrid-liquid-cooled-cinemaoctane-render-supercomputer-rig-aka-thanks-plx-chips-one-of-very-few-in-the-world/)) instead of stock computer chasis. What specific GPUs you'll choose will also have impact. As for Linus, he's using a superexpensive water cooling system. Expensive, complicated and you hope that you'll be able to find a compatible system for your GPU. If you'll find a stock AIO watercooled GPUs however, you might forgo thermal troubles. On the upside of the Linus-style system, seems like GPUs will only take 1x height instead of 3x which means you won't need riser cables which is really good.

➋ With PSUs things aren't so easy as well. I used multi-PSU setups in GPU miners, but in that case things seemed somewhat isolated via specific 1x riser cables. Personally, unless I found a specially made extensible PSU, I wouldn't put multiple PSUs on a server. Seems risky. But that's my IMHO. I see some guys in one article you linked to do use that and I hope they know what they're doing. I'd recommend you sell your 2080 tis ASAP and buy 3090 - save on PSU and be able to put more power per watt, plus, having a monolithic system is preferrable in any workflow.

➌ Threadrippers have 88 PCIE lanes. 7\*16 &lt;&lt; 88. If you'll want to get to 16x PCIE4 for 7 GPUs you'll have to go for Epyc which is much more expensive. I think the only other advantage they have is 2x max RAM bandwidth.

➍ You should consider if you maybe need to link 3090 GPUs with NVLink.

&amp;#x200B;

As for ECC RAM memory, you probably don't need ECC memory. (In Threadripper case)",1.0
g4jcyer,insgub,"Hi!

Thanks for your points, they are spot on!

Ad 1: I was planning to use mining rig, yeah. Custom water loop is too much trouble with reliability, maintenance etc for me. This was intended to be functional build.

Ad 2: The PSU is exactly why I scrapped this build (see edit in original post). After consulting few people, and weighting in your comment, consensus among people experiences in the are is not to link PSUs unless they were designed for that. I'm gonna wait for 3090 reviews until making any GPU sales/purchases. Don't think 2080ti will go much lower than now.

Ad 3: For deep learning I'm not even topping up PCI 3.0 x8

Ad 4: I'm won't be using NV link for the foreseeable future, but good point.",1.0
g4xpui5,insgub,"Reguarding your dual Threadrippers, my guess is they will also be loud. I'd go with some single 3-gen instead, unless it's a mobo issue. 3'd gen is revolutionary.

&amp;#x200B;

In case of no NVlink whether I really need 3090 or 2x more 3080 is a question that tears me personally still. 🤔 You might want to skip that thought cause it'll drive you crazy. Too many variables.",1.0
g5p95fz,insgub,A good writeup [https://lambdalabs.com/blog/deep-learning-hardware-deep-dive-rtx-30xx/](https://lambdalabs.com/blog/deep-learning-hardware-deep-dive-rtx-30xx/),1.0
g49cfvh,inrj0g,"Here are the interactive Desmos graphs shown in the video, in case you want to try them out yourself.

Sigmoid: [https://www.desmos.com/calculator/r8hxsriucw](https://www.desmos.com/calculator/r8hxsriucw)

Softmax: [https://www.desmos.com/calculator/u5r0zgh3jg](https://www.desmos.com/calculator/u5r0zgh3jg)",1.0
g48sc5t,inne7y,"If you are comfortable with maths, you could try Ian Goodfellow's Deep Learning book.",4.0
g496cp9,inne7y,Thanks,1.0
g48viog,inne7y,D2L.AI,2.0
g496d3g,inne7y,Thanks,1.0
g47pmmm,inhrjj,I’m interested!,1.0
g47qjgp,inhrjj,Hey I'm interested too. Indian time zone works for me. Hit me up if you're still open.,1.0
g47qlx1,inhrjj,Count me in if you're still open !,1.0
g47scth,inhrjj,"Sure. I'd love to join if you're still open. Indian time zone works for me, and 1hr dedication won't be an issue. And yeah, Indian timezone works for me as well.

Hit me up in case you need my mail id.",1.0
g47syhv,inhrjj,I would love to join but I'm european timezone,1.0
g47vnxg,inhrjj,Interested,1.0
g47yh19,inhrjj,Interested,1.0
g48xc1g,inhrjj,Interested,1.0
g48y00i,inhrjj,count me in,1.0
g48ykac,inhrjj,I'm in. Need to learn this urgently. I'm in CDT.,1.0
g50g2ut,inhrjj,interested! I finished lesson one earlier this week and diving into lesson 2 this weekend. can you message me the link? the one you posted expired. i'm in PST,1.0
g45wpvf,in7wb5,I've used this before and it's not an editor but a code snippet share tool called Carbon -  [https://carbon.now.sh/](https://carbon.now.sh/),1.0
g45y8rw,in4f8s,"Im also looking for opinions on this as well. 2 other factors i would like to add:

Memory bandwidth:
2×760 GB/s vs 936 GB/s, so 60% more memory bandwidth with 2 3080s

Resale value:
Hard to come up with exact numbers, but when the 3080 TIs come out i expect roughly a 20% resale hit for the 3090 vs the 3080s (guesstimate), since they are obsolete

For me particularly, im interested in stylegan 2. Im leaning towards the 3090 because getting stylegan 2 to work for two 3080s may not be a good investment of time, i see posts about it on stackoverflow so theoretically its possible and i have a gut feeling it would be faster as my understand is memory bandwidth is the limiting thing here, its just risky and probably a time sink, and ive been away from ml for a few years at this point, so im looking basically for information that would tell me otherwise, ha. I think im going to have try to get stylegan 2 up on my 1070 to verify its possible. If its ez pz i will prolly go with the 3080s.",2.0
g45zxq3,in4f8s,"2x 3080 very attractive based on specs but I realised from this post 3080 do not support nvlink. This means we cannot utilise memory in parallel and it is a deal breaker for me. 3080 ti/super could be vastly superior on value for money but could take a while before release, if you’re willing to wait it will be the better option.",1.0
g4632sz,in4f8s,"ya i hear this. Nvlink does matter, but it doesnt appear to be a dealbreaker for what im working on, it seems like a 10% boost when properly configured if this article is correct  https://www.pugetsystems.com/labs/hpc/RTX-2080Ti-with-NVLINK---TensorFlow-Performance-Includes-Comparison-with-GTX-1080Ti-RTX-2070-2080-2080Ti-and-Titan-V-1267/ , maybe if was doing nlp, but if the model isnt omega huge i can just have smaller minibatches per gpu and pool the gradients (still annoying). And ya too many unknowns for me to wait for the TI, could be a year for all i know. I wish i had more time to figure this out feels like theres pressure to make the right decision and ive never even done multi gpu setups so its just overall frustrating. I will prolly rent out multi gpus and verify i can do what i want with 2x10gb. Annoying but what can ye do.",1.0
g6y752a,in4f8s,YOu don't do Nvlink for the boost you do it for memory pooling. and It is not clear if RTX30XXX Nvlink will allow for memory pooling. If it does it would eat away a the Quadro cards which is why it was limited on the RTX20XX cards.,1.0
g7cjry4,in4f8s,"Didn't know about that, thanks for pointing it out. Doesn't affect my decision personally but I was wrong to say that the 10% boost was the only benefit.",1.0
g6y6z2j,in4f8s,FE does not AIB does it seems. But the real question is if that Nvlink will be Full Nvlink not just limited to SLI like the RTX20XX series.,1.0
g65mci8,in4f8s,"3090 is reportedly 20% faster than 3080 on CUDA workloads.

[https://videocardz.com/newz/nvidia-geforce-rtx-3090-shines-in-blender-benchmarks](https://videocardz.com/newz/nvidia-geforce-rtx-3090-shines-in-blender-benchmarks)",2.0
g458c2w,in4f8s,May be wait for 3070ti or 3080ti. They reportedly will have 16gb VRAM. If 3090 is quite costly.,1.0
g4592pk,in4f8s,This is an option but it may be a year before they release Ti cards.,1.0
g47o33f,in4f8s,"I wonder where you get your ""reports"" from, but I'm pretty sure there will be no Ti's. Maybe some ""super""'s some veery long time into the future. I Highly doubt there will be a Titan. 3090 is probably how they decided to merge the two.",1.0
g47yadh,in4f8s,"3070ti was just leaked by Lenovo. It was all over the news. And btw ti's are not Titan.
Sources:
https://www.tweaktown.com/news/74935/lenovo-teases-geforce-rtx-3070-super-after-teasing-ti/index.html

https://www.techspot.com/news/86618-listing-points-possible-rtx-3070-ti-16gb-gddr6.html

https://www.tomsguide.com/news/nvidia-geforce-rtx-3070-ti-leak-teases-a-mighty-mid-range-gpu

https://www.pcgamer.com/nvidia-rtx-3070-ti-16gb-memory/

Just google. You will find like a 100 more sources.",1.0
g4dfv4u,in4f8s,"I do know Ti≠Titan, I'm saying there will probably be neither. All those sources cite same thing. Donno, maybe there will be some Ti, or some Super, like I said, they didn't really have any source for naming in those news you cited, but anyway, that's beside the point. I'd say yeah, might be the case that they'll release something better somewhere along the way. But it will probably be like halfway to a much better newer RTX 40 gen, so.. Might want to just grab a current GPU and later sell it and upgrade.. 3080 seems legit for reselling.",1.0
g469b4g,in4f8s,"What is not that clear for me is the FP performance on the new cards. Admittedly for the 3090 the FP 32 (non tensor?) is in the 30+ TFLOPS range while the FP 16 is in the tensor value of the is in excess of 250 TFLOPS?! what does that even mean? Why stage half precision gets tensor values and the single precision unknown!? Why also no double precision values!?

Can the 3090 be faster than the also freshly released A100?! That GPU as monstrous as it is has “only” 19 FP32 TFLOPS. Does this make any sense to anyone?",1.0
g6y7gqd,in4f8s,"also take note of this [https://twitter.com/RyanSmithAT/status/1301996479448457216?s=20](https://twitter.com/RyanSmithAT/status/1301996479448457216?s=20)  "" GeForce cards are still going to be artificially capped in tensor performance for market segmentation reasons. As with RTX 20 cards, FP16 tensor ops with FP32 accumulate is running at half the native rate. This leaves the door open to an Ampere Titan. """,2.0
g6y8nd6,in4f8s,"Note sure I follow as yes the statement makes sense but doesn't address my questions. On paper and in brute force, the 3090 looks faster than the A100 (FP 32 and FP16), which doesn't make sense. Also why no FP 64 numbers for the 3xxx series?",1.0
g6y8shg,in4f8s,there is a much longer breakdown on that tweet stream. It might help a lot to read the rest.,1.0
g6yeb7u,in4f8s,Will do. Thanks,1.0
g47oqbq,in4f8s,"A100 is probably much more optimized for neural networks and given that it has 2x transistors, my guess is that it's about 2x faster for neural networks, while 3090 is around the V100 level.",1.0
g47uwwf,in4f8s,Benchmarks will confirm 3090 performance vs v100 but my guess would be it surpasses it easily.,1.0
g484vvk,in4f8s,"Same thought, v100 does not stand a chance against 3090, and hope that nvidia io comes to deep learning for seamless drive to gpu communication, its gonna be on a different level, for instance 2080 ti had 28.5 tensor tflops, you get the idea where it stands, the bottleneck is going to be the transfer of the data from ssd/ram to your gpu.",3.0
g48oto7,in4f8s,This is exactly correct. While the 3090 is very impressive on its own.. any who has done deep kearning knows the bottleneck is in the data transmission pipeline and not the gpu itself. Rtx IO is a massive jump potentially,1.0
g5x1n4a,in4f8s,"&gt;  any who has done deep kearning knows the bottleneck is in the data transmission pipeline and not the gpu itself.

huh... I've never heard this. I recall reading that it's not a huge loss in performance if you're using 16x vs 8x PCIe lanes.",2.0
g4dbgkf,in4f8s,"I guess PCIE4 data transmission might be a bottleneck mostly in case you're parallelizing and not using NVLink, or your architecture is smallish and you feed tons of data constantly. Otherwise it probably shouldn't. Not with the new on-GPU DALI decoder [https://docs.nvidia.com/deeplearning/dali/user-guide/docs/](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/)

That depends on what you're training, though, of course.",1.0
g6y7kqa,in4f8s,yes. about x2 it seems,1.0
g6y7jbz,in4f8s,well looks like the 3090 is x2 the V100 on most DL work loads.,1.0
g6yfmrq,in4f8s,"Lol, how exactly did you find that? Very much depending on the exact network it's going to be either up to 2x worse - if heavy on FLOPS, or on par with Titan RTX which is slower than V100. At least until we trainsition to sparse kernels or something, where it might get a 2x boost or something. 30xx series has it's FP16 perf trimmed 2x for NVidia's market segmentation purposes.

&amp;#x200B;

Edit: No, it seems, actually, much less than 2x boost from sparsity, currently 1.3-1.8 depending on math intensity according to NVidia's numbers.",1.0
g6yqr0b,in4f8s,True. I ran a DLPerf benchmark and it scored 40 vs 20 for the V100. Same as wats used on vast.ai,1.0
g73c0dg,in4f8s,"If that is for FP32 then yeah, might as well be 2x V100. But it makes no sense to train in FP32 on anything newer than Pascal GPUs.",1.0
g75z4ok,in4f8s,That is interesting and very true.,2.0
g6gc3w8,in4f8s,"If you want to be safe next lets say 5 years using top of the line DL technology and not going to purchase titan lineup, I think 3090 is the only option.

Some famous Image model like VGG, NLP requires at least 8gig yet as standart, but if you're researcher who wants quick train test (assume you have organization and also have decent gpu server) I think in few years, the standard is going to get higher, even faster because nvidia's pushing it. Even some image model like mask-rcnn requires 12gig already to have decent generalize performance.

However I think 3090, It's not going to replace Titan lineup. Titan RTX has 576 tensor core where 3090 has 328 even if it's past gen. As good comparison. Next titan (If there will be one) is going to be better in purpose of deep learning.",1.0
g6gcuk1,in4f8s,"I am not sure Nvidia is releasing a Titan. 3080 super might be an alternative with 20GB but I don’t know when that’ll be released and if it will support nvlink. I am waiting for benchmarks, if they show less than 1.5 improvement over 2080ti, I’ll probably go for 2x 2080 ti.",1.0
g6gix9b,in4f8s,"As far as I know, Nvlink is not must for parallelize deeplearning workload with consumer-grade gpus. About 10% improvement over non Nvlinked gpus in famous image models, according to this post. enterprise-grade gpus scales better with nvlink.

  
[https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)

This post has Super deep analysis with case by case use cases including gpu clustering.",1.0
g6i6qz3,in4f8s,"For DL training, What’s your model size and training dataset size? If you don’t know, or are just starting out then get the 3080. You can even train on the CPU when just starting out. 

One 3090 is going to be better than 2 3080 for gaming, but 2 3080s is better for deep learning as long as your model comfortably fits in the 10GB of memory. 

Having looked into this before, using cloud is actually very expensive if you are doing allot of training. IIRC, last time I checked, 4 months of training time on Ronin can pay for one equivalent GPU server. 2 months can pay for the 3090, assuming you need a PC anyway.",1.0
g6y7wuz,in4f8s,hy take a look at [vast.ai](https://vast.ai) for cheap rentals. its limited to dockers but it works,1.0
g6y6vqx,in4f8s,Wait for the RTX3080 20GB,1.0
g70izdu,in4f8s,"Keep a lookout for AI benchmarks here

[https://lambdalabs.com/blog/tag/benchmarks/](https://lambdalabs.com/blog/tag/benchmarks/)",1.0
g47hero,in4f8s,Neither. Go cloud only for DL unless its an excuse for a gaming rig.,0.0
g482sw4,in4f8s,"Would you mind sharing your reasoning? I'm in a similar situation as this poster and am mulling my options, and am curious how either of these would compare with the \~$1/hr tier from AWS. Granted, you could get 700hrs of train time for the same price as a 3080 but it seems like it might actually not be that hard to get there?",2.0
g485i0c,in4f8s,"Anything you are going to do for personal projects, the AWS free tier or Google Colab GPU/TPU is probably more than sufficient starting out. The way GPU's are evolving (2080 ---&gt; 3090) its just not worth it to try and keep up and when you go to work somewhere you will want to be proficient in the workflow of Google/AWS etc anyways. Large datasets, big models and managing dependencies local is good to know but you are going to deploy to some sort of web service even for your resume/interview. Go pro.",3.0
g48ejmh,in4f8s,"Hey, thanks for the cogent and reasonable reply/advice! The argument that a pro will need to know cloud deployment anyway holds a lot of water.

I  just started playing with Colab yesterday, and it's shockingly nice, actually. At the end of the day, I'm not personally going to go pro on deep learning as a career (unless one of these ideas rattling around my dome actually turns out to be a good one!). So, I'll likely plonk down for a GPU (my old 760 is hopelessly out of date, and I do a bit of gaming), but for someone looking to make a career out of data science, I bet you could skate on Colab basically indefinitely! I hit the RAM limit once, but if you're smart about it I think you can stay under.",3.0
g4bk530,in4f8s,"Time is more valuable than money. While one waits on his single GPU for 20hrs training, there is someone using multi-GPU in the cloud and getting 10x the work done as well. the 20hrs  week turns into 10 mins. No brainer. Unless you like to wait.",2.0
g4np4tv,in4f8s,"Of course you don't even consider the trial and error part of optimizing a model. I have never made it to one-time bulls eye. I spent all google free credits in one month for a binary segmentation problem and I didn't even finish optimizing the model. If there is no company backing to this kind of investments, building a 5k-7k workstation is more profitable in the long run.",4.0
g4t5nvp,in4f8s,"Some companies are investing into 4/8 GPU per 4u custom build servers. We are killing the value as we can build much cheaper, much faster machines than whatever you can rent out at AWS/Azure.  Most of servers is paid off after 2-3 months vs Azure/AWS (of course there is a bunch of limits like I/O, transfer, number of connections that limits us from using that kind a ""cloud"" service, not mentioning much more downtime per year (including degradation of their service) vs self-hosted machines).

Of course you need a person or two for an administration, but it is peanuts in comparison to cost of AWS/Azure for a growing tech company (we pay less to purchase, build and setup  for our beefy database/cache servers than a single month of an AWS/Azure db machine (that is much slower anyway, and we are talking about £12k/month mark per machine)).

Thanks to that, we are able to undercut any competition with our offering and virtually TONS of money saved by deploying regional self-hosted DC  is spent on new devs to grow even faster...",3.0
g5qgwn1,in4f8s,I'd love to know more about this. Any links?,1.0
g48htrf,in4f8s,"Last I checked AWS free tier gives no long-term GPU access. Colab != GCP, training on colab gives you exactly zero experience with the GCP workflow. The jump in GPU performance doesn't invalidate older cards, nor is there any pattern suggesting that GPU performance will shoot up like this again (remember, the 20 series provided little improvement over the 10 series which is half of why 30 series can be as big a jump as it is reported to be).

If you're doing anything more than fairly casual hobby it is easy to use up more cloud credits than the cost of these GPUs. The card is like what, $700? So lets say 1000hrs of training to offset it. That's like 20hrs a week of training for a year. The GPU will last more than a year and if you're training models that are of reasonable size and complexity it is likely that you're going to exceed 20hrs a week (that's just over one training run for my work, and I probably do that 7+ times a week personally).

I have never understood why everyone thinks the economics of cloud work for individuals working on moderate scale. The entire premise of profitable gpu clouds are that the hardware cost is quickly offset by the price of cloud credits. Small stuff can get away with the free stuff they put out for goodwill sure, and big projects/companies can get commercial discounts, but the intermediates paying full price are getting the short-end of the bargain",3.0
g71iufr,in4f8s,Another option would be to buy your own system and host it on \[[vast.ai](https://vast.ai)\]([https://vast.ai](https://vast.ai)) for others to rent when you don't use it.  Or rather than not paying AWS that crazy amount of money just rent an 8 GTX 1080ti system for $0.9/h.,2.0
g6y80lx,in4f8s,noooo!!! AWS is burning money. use [vast.ai](https://vast.ai) man. a lot cheaper and you get docker container that runs directly on the hardware.,1.0
g4vy389,in4f8s,"While I agree that it's more cost efficient, I personnally like having the luxury of my own GPU to train models on. Having a discussion on this is useful for those who don't mind spending more than is optimal for the enjoyment of it (similar to spending way too much on a hobby)",2.0
g4de1ro,in4f8s,"Just in case somebody actually does that, please, take notice of different GPUs and their pricing (t4 vs v100) and take notice of spot/preemptible instances.",1.0
g4418jk,imzl8f,"The paper: [https://arxiv.org/pdf/2003.12039.pdf](https://arxiv.org/pdf/2003.12039.pdf)

GitHub with code: [https://github.com/princeton-vl/RAFT](https://github.com/princeton-vl/RAFT)",2.0
g44y2gt,imzl8f,"So if you're moving and things you're looking at are moving, how does optical flow work.",1.0
g45vf84,imzl8f,"Optical flow is the problem of predicting how pixels move between two frames. The basic version of the problem doesn't tell you (and doesn't try to tell you) *why* the pixels are moving i.e. is it apparent motion due to camera movement or actual movement in the world. That's why it's called *optical* flow and is defined as *apparent* motion.

Many algorithms estimate optical flow as a starting point and then run a segmentation algorithm to separate foreground (motion in the world) from background (apparent motion due to camera movement). 

The thumbnail image in the linked video shows a pretty good example: the yellow background implies camera motion. But I only know that due to further interpretation of the optical flow and the original color image on the left.",1.0
g43enzi,imtulb,Sounds like embedded or internet of things.,1.0
g43fjwl,imtulb,"There is not really a standard one and everybody uses whatever sounds good for them. Embedded computer vision is something I heard among the more EE, ECE focused crowd. If they are really edgy they call it Edge AI.",1.0
g42sz2p,imtulb,"I don't know if there's an official name but it's basically embedded systems + machine learning, so I'd call it embedded machine learning.",1.0
g432ufg,imtulb,"Edge AI.
Or ""Edge Computing"" for the general term",1.0
g42u4rn,imtulb,The field is called soldering accelerator chips to the PCB.,0.0
g4bnvqk,imqa78,"I think OpenAI's braindead strategy of stacking more and more transformers thinking it will fix stuff when the whole architecture is gimped is too rich for researchers lol  


Sad that we resorted to CV methods in NLP",1.0
g415bfa,imib62,Just go to aws,1.0
g454orq,imib62,"What? Just start a deep learning VM with gpu, you can literally do it in less than 5 minutes.",1.0
g3zf2ja,img84u,"Want to grease the wheels of your TensorFlow knowledge?

Welcome to this tutorial series for beginners on Deep Learning with TensorFlow. In this tutorial, you will be able to take part in a mini-project that will help you to master the nitty-gritty of TensorFlow.

[**Stay Skilled - Stay Ahead**](https://www.youtube.com/channel/UCtOOo75kcRq_IFNVpHOUeLA)",1.0
g3zr4ms,imd41p,Code base?,2.0
g41w66q,imd41p,https://github.com/Rudrabha/Wav2Lip,4.0
g3xi208,im5l5j,"my totally opinionated opinion on why PyTorch: because it is simple, small, very clean, has a great architecture, is well documented, easy to work with / easy to develop and debug, is very flexible, is very stable, has a large ecosystem of well maintained libraries/models/etc., is not breaking (almost) anything in their updates, has a friendly and knowledgeable community, is not very opinionated, gets a lot of things ""right"" and is easy to distribute and parallelize in various ways.",3.0
g3yj9nv,im5l5j,"I haven’t tried pytorch but heard lot of good things about it. Using Keras already feel easy for me, i wonder how easier PyTorch be. Excited!",1.0
g3xv70e,im4xc6,"If I understand correctly, you want to match a virtual node to one of the physical which best matches compute resource and bandwidth request right?",1.0
g3y9th9,im4xc6,Yup,1.0
g3yjjch,im4xc6,"You can keep 2 inputs, 1 your virtual node matrix(containing requested data) and another input as your physical network data of dimension [adj_x, adj_y, node_matrix_dimension] and the softmax output is appropriate. I don't have a lot of knowledge about networks, so I am not sure",1.0
g404pxy,im4xc6,"But my main problem is not the physical network input, it's the  the virtual network request input. 

In virtual network request input , I wanna mention the virtual network request which is a graph and also point out the virtual node that I wanna assign( virtual node of  virtual network request )",2.0
g3w5j2r,ilzpk7,"RTX 3080 does not come with NVLink, only RTX 3090 have that. So you cannot link RTX 3080s if that is what you mean.",6.0
g3x7bqa,ilzpk7,"&gt;NVLink

Does this thing is any better then default parallelization?",2.0
g3w6dch,ilzpk7,"this article seems to compare usage of multipleGPU with and without NVLink ([https://www.pugetsystems.com/labs/hpc/2-x-RTX2070-Super-with-NVLINK-TensorFlow-Performance-Comparison-1551/](https://www.pugetsystems.com/labs/hpc/2-x-RTX2070-Super-with-NVLINK-TensorFlow-Performance-Comparison-1551/)) So i guess a performace boost works. (of couse a little below the expected performance)

or am I wrong? (I'd use tensorflow.Keras)",1.0
g3w6erb,ilzpk7,"You're not wrong, Walter, you're just an asshole.",5.0
g3w76k4,ilzpk7,"It literally says in the article, those cards have nvlink and he used nvlink bridge.",1.0
g3w87a4,ilzpk7,"It does, but I mean the setups used for comparison (see picture:[https://www.pugetsystems.com/pic\_disp.php?id=56590&amp;width=800](https://www.pugetsystems.com/pic_disp.php?id=56590&amp;width=800))

the ones, where it says 2x RTX {GPU} without the suffix ""+NVLink"", are they also using an NVLink?

If not then it should be possible, or am i missing something out?",2.0
g3wce6r,ilzpk7,"For machine learning, your cards do not need nvlink/sli.",2.0
g3wvqxc,ilzpk7,"Honestly based on your description, I think a single 3080 should be more than enough for your needs. (Your dataset is tiny)",2.0
g3ww0j1,ilzpk7,"Other than that, 3090 if you need the extra 4gb of vram (which I doubt) else 2x 3080 if you want to train multiple models at the same time. Will probably be a bit smoother than on a single gpu, but also possible",2.0
g3x2z0h,ilzpk7,"EDIT: forgot to mention: my script is going to train 6-20 models which are interdependent to one another (datawise and predictionwise), so my guess is, that it would take around 30-100 days without pause, to run my full script. (the interdependence, makes it far less easy but not impossible to parallelize,but the amount of RAM used during preprocessing at the same time will probably be tooo big)",1.0
g3vjgyn,ilxpy1,"For AI experts, I was expected more analytical justification... and less feelings. 

This survey seems skewed to say the least. 

What about the large sums of money China is using to attract AI researchers?",3.0
g3vw401,ilxpy1,"I agree, and you beat me to the punch.  This quote kind of summarizes the article:

&gt;As a Canadian, I hope that the next AI hub will be in Montreal.

Most of the respondent's ""predictions"" are based on factors that could literally be used to ""predict"" where the next hub for any field could be.  Things like ""democracy, tolerance/multiculturalism, and freedom of speech, several world-class universities, affordable housing, English language"" don't really seem particularly specific to AI.

I'm not saying their necessarily wrong, but a more accurate title for this article would be ""We asked some AI experts what cities they wouldn't mind moving to"".

Edit:  Just to save someone a little time, the answer seems to be somewhere in Canada, like Toronto or Montreal.",2.0
g8aq409,ilxpy1,Montreal will be the next ai hub in my view based on recent decades openings of tons of research center from major companies. Also the ai infrastructure is there to take it to next level. Great minds like yoshua keep training his decipiles not only on his lab but also helping ai startups to flourish. Next ai Montreal program is just a tip of the iceberg. Also there is some kind of movement started back in 2017 https://youtu.be/Zfpsq7gIcvI for pushing the limit. So it will surpasses China in many ways in upcoming years in the field of ai.,2.0
g3vczrc,ilwsv2,"With sufficient ventilation it's not really a problem. Just make sure heat can escape your basement with something like an exhaust fan. Your hardware will throttle themselves when they overheat. Crashes from overheating only happen when hardware fails, or when the ambient temperature gets too high.

Backup in such a long process, however, is always necessary. Accidents happen.",3.0
g3x54f4,ilwsv2,"I think the point that needs to be addressed first is the question why it is impossible to reboot.

Afaik there is always a way. And there should always be a way.",1.0
g3xdfgf,ilwsv2,"If your PC is built correctly  your GPU and CPU should be able to run at 100% indefinitely without needing interruptions to ""cool"". If you are having problems with overheating, something is wrong with your system build, not your code.  


When you build the computer make sure you know the basics around system cooling -- where to put case fans and which directions air should flow. There are plenty of guides on internet and Youtube and in the pc gaming subreddits. Also make sure the room itself doesn't become a factor.  


Also closed loop watercooling is super safe and effective, what are you concerned about there?",2.0
g3xns90,ilwsv2,"thx for this answer. Im a little concerned because my workstation should run for 30+ days without rebooting. Its not build yet, but will stand in a relatively cool basement.

Watercooling concerns me a littlebit because of something running out. also transportation can be far more difficult. plus has to be inspected more often than an aircooler (i guess). 

can you recommend a guide on system cooling?",1.0
g4bnlj3,ilwsv2,"&amp;#x200B;

make sure you have good ventilation in your case. gamersnexus does tests for airflow in pc cases. there is absolutely no point in ""pausing"" computation, as soon as you hit the cpu and gpu with a high load temps will just jump up. if you are not comfortable with the temps you're looking at you can always underclock, but that shouldn't be necessary. 75°C to 80°C on nvidia gpus is to expect. if heat is dangerous to your system it will emergency shut down anyways. 

[https://www.gamersnexus.net/guides/3604-best-gaming-pc-cases-for-airflow-in-2020-right-now](https://www.gamersnexus.net/guides/3604-best-gaming-pc-cases-for-airflow-in-2020-right-now)",1.0
g4crfs6,ilwsv2,what are temps? (english isnt my motherlanguage),1.0
g4crk8g,ilwsv2,Temperatures,1.0
g3vb0ih,ilwsv2,"Under-clocking and under-volting can help. I train small cpu intensive RL models (5-6 hr training time) on my laptop. With stock clocks, the core temps often crossed \~95 C and it caused throttling. Now I artificially limit the max clocks to 2.8GHz (Theoretical Max-3.5GHz). The core temps now remain in low 80s and the training time is also improved by 10-20% because of no throttling. My laptop GPU does not support under-clocking / under-volting, so I don't have any experience in the GPU dept.",0.0
g3uql74,iltyq5,"3090s are triple slot. I haven't seen a motherboard with 4 slots, triple spaced.

No doubt someone will eventually make one for your use case, but given we only just know that the 3090 is triple width it might take a while.

Either that or one of the aib makers narrows it..",6.0
g3uuoth,iltyq5,Water cooling four of them could work if they have dual slot io shields.,5.0
g3v51x7,iltyq5,Good idea!,2.0
g3ut1ye,iltyq5,Thx for the info. Do you have an idea on how long does it usually take for manufacturers to release compatible hardware?,2.0
g3uyikm,iltyq5,[deleted],3.0
g3wtuqf,iltyq5,"Are you using risers of some sort, or one long board?",2.0
g3wu57g,iltyq5,[deleted],5.0
g3wuvvy,iltyq5,"I have an old crypto frame as well, I’ve held on to it hoping to make a ML rig eventually. My concern is the risers I have are either x1 or x4. 

Do you use similar risers without bottlenecks or do you have x16 ribbons?",3.0
g3wzwlw,iltyq5,[deleted],3.0
g3wzzsl,iltyq5,Gotcha. Thanks for the info 👍🏻,3.0
g43gx8n,iltyq5,"Wow! I never knew you could have more than 4x GPUs on a single device! What's the motherboard you're using?

I'm wondering whether it has 6x PCIe slots or you're using some other way to make that possible. (Believe me, whatever you say is probably gonna sound like magic to me, I never knew riser cables exist until I read them in another comment xD)

What about the CPU?",1.0
g3x7nwa,iltyq5,"Hi, I'm also looking to build 4x 3090 rig for rendering. I got experience with 4x Titan RTX on two motherboards.  One is older Intel based : Asus Z10PE-D8 WS and second AMD based : Asus Zenith  Extreme.

For the case I would suggest Thermaltake W100 or even better WP100 which is two cases connected into one (W100 and P100). The price is really low for the options it gives you.

The main components I would use for today are :

\-Asus Zenith II Extreme Alpha as a motherboard

\-AMD Threadripper 3990X

\-256GB DDR4 (8x32)

\-4x RTX 3090

\-Nvme ADATA Gammix S50 2TB

\-some HDD in RAID for example 2x 18TB WD HC550

\-2x Corsair AX 1600i power supply

Some old photos Intel based rig:

[https://ibb.co/gd6hr5G](https://ibb.co/gd6hr5G)

[https://ibb.co/jwqWxQc](https://ibb.co/jwqWxQc)

I know how it looks but I had to make it work in extremely short notice and I took a pic that day :) You may also get few aluminium profiles and make a rack inside this case for your 2 cards connected to the raiser cables.

Raiser cables are a must for 3090. I doubt that any company will make 4x 3slot motherboard in size. It's not going to happen. I wouldn't suggest going watercooling route because of the additional maintanance involved. Been there, done that. Not worth it IMO. If you would get proper case with big fans from Noctua you wouldn't have any problems cooling the setup under load and it would be reasonably quiet. The watercooling is neccessary only if you are planning to overclock your rig alot. Hope it helps a bit to anyone.

Also the power supply which is in the main W100 case in my photo could be put below to the P100 right beside the first PSU for more room for 3090 cards and mentioned before aluminium rack.

&amp;#x200B;

Custom case made from aluminium profiles I found on Redshift forum:

[https://ibb.co/q9kGr3z](https://ibb.co/q9kGr3z)",3.0
g42nsqu,iltyq5,"what do you do for a living? 

256Gb ram seems like overkill but i dont know how much you guys use ram",2.0
g43gcvq,iltyq5,"For me, I honestly, was just gonna go with the convention that you should have 2x RAM as much as you do have vRAM. Maybe 192GB is enough. But is it efficient to have modules of different sizes? e.g. (4x 32GB + 4x 16GB) What is the worse that could happen in such a scenario?",1.0
g47jzyp,iltyq5,Highres VFX simulations :) Believe me I would gladly get 2TB memory if I could afford it.,1.0
g43fkm0,iltyq5,"Hello, 

Thanks a lot for the detailed information. It helped me a lot on organizing my research and what to look for. I'm used to purchasing pre-built devices or the cloud and this is the first time I go with a DIY-rig approach.

So, I hope you'll excuse me if some of what I write doesn't make sense and for the many questions.   So, I decided to go with your specs as the base, and adjust as needed. Even though a budget isn't much of a problem for me, I want to make sure I'm gonna utilize a high percentage of what I decide to purchase. This is why I want to plan a full spec rig, but only purchase parts in stages (with the least need possible to replace already purchased parts).

&amp;#x200B;

Here's my, still incomplete, list of components as I still would appreciate the feedback:

**4x RTX 3090** Not much to say about this yet, until it's released. I might have to do a little research of Founders Edition vs. other manufacturers.
**AMD Threadripper 3960X**
I feel the 3960x is already going to fulfil my use case, but I'm still unsure about the # of PCIe lanes it supports, it's not directly stated in the specification. Is there a way I can infer this? When I googled I find websites stating x64 lanes, and others say x72. But if it's true it's 64x then it should be more than enough. I just need to make sure all 4x GPUs can operate on at least x8/x8/x8/x8 lanes.
*8x Corsair Vengeance LPX 32GB DDR4 DRAM 3200MHz*
For the RAM this seems like an excellent choice, it matches the CPU spec (DDR4) and the max speed the CPU can operate on. However, I'm still not sure about the DRAM vs. SDRAM part, yet.

**Asus Zenith II Extreme**
For the mother board, do I really need the Asus Zenith II Extreme Alpha since I'm not gonna go with 3990x? It seems the only difference is that the Alpha supports the 64cores. But I think Asus Zenith II Extreme should be enough for my use case, but I couldn't figure out if it supports manual/auto overclocking yet.

I had another idea, though I haven't followed on it yet. I was thinking maybe I should look for Server Motherboards and could find interesting specs? I mean, after all, I plan to install Ubuntu Server as SSH and Jupyter Notebooks access only should fit my usecase perfectly. I don't need the excess monitor/audio output and so many of old USB ports.

**Storage**
I still have a lot of questions regarding these, I probably will end up using the same products you suggested, but I wanna find alternative products to them to understand what makes your choices compatible with the rest of the equipment. One question I have, if I'm gonna have 4x GPUs and the NVMe SSD requires a PCIe, how will I be able to port it to the motherboard?

My initial setup is gonna be:
In addition to the CPU, Motherboard, Storage and case: 2x GPU and 4x 32GB RAM
And then if I see the need to, I'll add the remaining 2x GPU and 4x 32GB RAM.
I might even start with 1x GPU and add more as the demand increases.

There's still a lot of confusion, and sorry if that also appears in my writing, but as I go further and further I realize a good way to make sure of the specs needed is to actually have one already, so you know what you're missing (or the bottlenecks) and then upgrade.

Thank you!

**Edit:** I forgot to write, that you don't have to answer the above questions. I simply wanted to write them down, but I will be circling target subreddits for answers to these questions for each specific part/product. And maybe at the end of this, I'll write a post here detailing my final decision and the process that helped me come to it.",1.0
g44a2x2,iltyq5,"The 3960x supprots 88 lanes as per [AMD official website](https://www.amd.com/en/products/cpu/amd-ryzen-threadripper-3960x) (scroll down half page ""An unprecedented 88 total PCIe 4.0 lanes to meet large GPU and NVMe needs."").

NVME drives plug into their own slots, usually under plastic cover so won't be visible on most mobo pictures. They still take up to 4 lanes each if I remember correctly.

EDIT: As far as I know all Threadripper 3 motherboards support all CPU models including 3990x, where did you get info otherwise?

It would be great if you make writeup in the future!",1.0
g5zd583,iltyq5,"But TRX40 only has 48 PCIe 4.0 GPU lanes. Other lanes are associated with non-GPU components. That is why you see all motherboards either having a 16/8/16/8 or a 16/16/16 configuration, which totals to 48 in both cases.

[https://static.techspot.com/images2/news/bigimage/2019/11/2019-11-12-image-11.jpg](https://static.techspot.com/images2/news/bigimage/2019/11/2019-11-12-image-11.jpg)",1.0
g47jppk,iltyq5,"Ok, so:
-AMD Threadripper 3960X has 88 total PCIe 4.0 lanes. You are going to be fine with this chip. 
It's stated on AMD's site right below the Terminator screenshot : ""An unprecedented 88 total PCIe® 4.0 lanes to meet large GPU and NVMe needs."" (https://www.amd.com/en/products/cpu/amd-ryzen-threadripper-3960x)
-Corsair Vengeance LPX 32GB DDR4 DRAM 3200MHz - that was the ram I used on my AMD rig. No issues whatsoever.
-Asus Zenith II Extreme vs Alpha - both support all cores from 3990X and both have the same features including auto and full manual overclocking options. The only difference is higher rated VRM's. 90Amps for Extreme Alpha and 70Amps for the Extreme. You should be fine with cheaper Extreme but I would suggest getting the Alpha because it's the more futureproof solution. Motherboard is the most important component of the PC so it should be the best possible quality IMHO.
-Storage : ""One question I have, if I'm gonna have 4x GPUs and the NVMe SSD requires a PCIe, how will I be able to port it to the motherboard?""
You will get 5 NVMe dedicated slots with both Extreme and Extreme Alpha mobos. You don't have to sacrafice any PCIe slots for that drive to work. You need to bare in mind to which of those NVMe slots you are going to connect your drive because some of them are sharing lanes with the PCIe slots. The manual will tell you exactly how to avoid this issue.

Hope it brings you closer to the right choice :)",1.0
g49el14,iltyq5,"How about **7 (seven)** 3090?

In short:

* [**ASRock ROMED8-2T**](https://www.asrockrack.com/general/productdetail.asp?Model=ROMED8-2T) workstation motherboard (£600/$800) - 7 PCIE 4.0 16x slots (!)
* [**AMD EPYC ROME 7402P**](https://www.amd.com/en/products/cpu/amd-epyc-7402) processor (£1300/$1700) - 24 core 48 thread, 2.8GHz, 128 lanes, up to 2TB server memory
* [**Samsung 32GB 3200MHz ECC DDR4 RDIMM**](https://www.samsung.com/semiconductor/dram/module/M393A4K40DB3-CWE/) memory - (£160/$210 per stick)

I made a separate [thread here](https://www.reddit.com/r/buildapc/comments/inqpo5/multigpu_seven_rtx_3090_workstation_possible/) with full build breakdown. Note I have no experience with workstations, server CPUs or memory, so dunno if components actually will work together, needs more research.",1.0
g4424vf,iltyq5,"Did you have any issues with raisers? Any recommendation, things to look for?

I'm thinking these ones, but don't really know much about raisers:

[https://linkup.one/linkup-pcie-4-0-x16-riser-cable-x570-rx5700xt-tested-twin-axial-itx-vertical-ultra-gaming-pci-express-gen4-2020-universal-90-degree-socket-30-cm-3-0-gen3-compatible/](https://linkup.one/linkup-pcie-4-0-x16-riser-cable-x570-rx5700xt-tested-twin-axial-itx-vertical-ultra-gaming-pci-express-gen4-2020-universal-90-degree-socket-30-cm-3-0-gen3-compatible/)",1.0
g47i7t3,iltyq5,"That in your example should work just as fine. I had no issues with my raisers. They work like a charm. I got Thermaltake Riser TT Premium PCI-E 3.0 X16 - 300 mm (AC-045-CN1OTN-C1). 

What I would suggest is to doublecheck what lenght you need. There are (at least from TT) 200mm, 300mm, 600mm and 1m. I know that for 4x 3090 I would use my 2x 300mm and get 2x1m for the cards that are going to be offset to the side. Bare in mind that the raiser even though it's flexible, it's quite wide and you need to adjust it properly sometimes in tight space so it's better to have more lenght to play with than to try to bend it too much.",1.0
g47l8sb,iltyq5,Thanks for the info! I also found Linus video where he chained 15 raisers to 3m length. Much less worried now.,2.0
g5ki4hs,iltyq5,That's a Gen3 riser. It (might) throw a BSOD with Gen4 slots operating at Gen4 mode.,1.0
g5kuytz,iltyq5,I heard about it from some guy on YT but my friend used that raiser cable on his Asus Prime which is Gen 4 and he had no issues. Worth checking online before buying your particular setup.,1.0
g5m340s,iltyq5,"Can you give me a link to that Asus cable? Is it made for Gen4 or did it work out of spec? If it is the latter, then there is no guarantee that it will be able to handle the bandwidth of DL applications or when pushed to the limits.

In this video ([https://www.youtube.com/watch?v=QtoagBoelsU](https://www.youtube.com/watch?v=QtoagBoelsU)), the OP says that all except the thermaltake cable straightaway refused to work at Gen4 mode.

The only Asus riser that I can find is this one ([https://www.asus.com/ROG-Republic-Of-Gamers/ROG-Strix-Riser-Cable/](https://www.asus.com/ROG-Republic-Of-Gamers/ROG-Strix-Riser-Cable/)) and it says that you need to change the pcie mode to Gen3 from BIOS to get the cable to work. It is possible that it works out of spec, but then it might have stability issues.",1.0
g5sjqfh,iltyq5,I meant Thermaltake cable on Asus Prime board. Not Asus cable on Asus board :),1.0
g5ki16h,iltyq5,"I don't think you will be able to stick two 3090s on the motherboard and have room for two risers, like the way you did with the Titan RTXs. A 3 slot RTX 3090 on the first slot will block the second slot from being accessible. This means you would have to take three riser for each of the first three slots.",1.0
g5kvels,iltyq5,I was wondering about that as well. I think there might be enough clearance below the radiator on the 3rd slot though. The card is not full 3 slots wide. It only has bracket that wide. That’s not a problem. In 4x 3090 I would get 4 risers and connect them below the motherboard and off to the side just to have better breathing area. All you need is a big case and raiser cables.,1.0
g5m22er,iltyq5,"The third slot will indeed be free. But the second slot will be covered up by the GPU from the first slot. I know no TRX40 mobo that has a the top slot spaced for a 3 slot GPU. Also from the looks of Linkup's PCIe 4.0 ultra cables, it seems that it doesn't fit flush with the PCIe slot it's inserted it, and there will be a part of that particular end of the connector that will stick out of the pcie slot even when fully inserted. This will, therefore, collide with the fan shroud of the top GPU.

IMO, the only way out is to use 3 risers for the top 3 slots and have the fourth GPU plugged in directly to the bottom slot.",1.0
g3uqun4,iltyq5,I have been wanting to build one for a long time. I think the latest Threadripper CPUs should support your use case.,2.0
g89escr,iltyq5,"little late but there are multiple server class machines you can consider.    supermicro 4028 series and 4029 series for intel and they also have one with  AMD processors.  8 or 10  double-wide slots total so you can skip every other double one and get in 4 or 5 3090s  if you can actually buy them.   There are a few other companies as well e.g I just both two more from ebay  for 2450

&amp;#x200B;

[https://www.ebay.com/itm/4U-Cirrascale-GX-Series-Server-AI-8x-NVIDIA-Tesla-Multi-GPU-2x-Xeon-E5-2680-v3/133405006306?ssPageName=STRK%3AMEBIDX%3AIT&amp;\_trksid=p2057872.m2749.l2649](https://www.ebay.com/itm/4U-Cirrascale-GX-Series-Server-AI-8x-NVIDIA-Tesla-Multi-GPU-2x-Xeon-E5-2680-v3/133405006306?ssPageName=STRK%3AMEBIDX%3AIT&amp;_trksid=p2057872.m2749.l2649)

which have 8 double wide slots. 

 I have multiple of the 4028-tr and 4028-tr2  one with 4 and one with 5 titan RTXs in every other double  wide. Works just fine no thermal issues.  Even though titan RTXs are only double wide (so theoretically I might pack ore), they don't cool well if I don't leave an extra slots.      (I also have one with 8 1080tis).   

&amp;#x200B;

They sound like jets taking off but that is what the surver room is for",2.0
g441oa5,iltyq5,"I'm thinking the same, for now waiting for release and independent tests.

As for the case, you could look for 6u mining case, like this one:

[https://www.xcase.co.uk/collections/mining-chassis-and-cases/products/minestation-1-metal-construction-fully-enclosed-mining-chassis-for-6-gpu-easy-flatpack](https://www.xcase.co.uk/collections/mining-chassis-and-cases/products/minestation-1-metal-construction-fully-enclosed-mining-chassis-for-6-gpu-easy-flatpack)

Just be mindful of motherboard arrangement to make sure you don't need too long raisers or that cpu tower cooler doesn't block gpus etc. I think the one linked should be good with AIO cooler.

As for the CPU and Motherboard, really make sure you have enough PCIe lanes to support 4x GPUs. You will be looking at a Threadripper or workstation Intel CPUs. Mobo spec will usually say it supports Quad SLI or Quad CrossFire as well.

As for CPU/Memory, might be worth to consider 4x cores (8 threads) and 64GB per GPU. This matches what Amazon AWS offers on their P3 instances, which seem to be quite popular for deep learning.

Make sure to update us with progress!

**EDIT: Full setup I will be going for**

Motherboard: **MSI Creator TRX40** \- because of 10Gbe LAN. I was thinking about **Gigabyte TRX40 AORUS XTREME** as well because dual 10Gbe LAN would be nice if I ever need to store datasets on NAS. Good technical comparison of all Threadripper 3 motherboards: [https://www.youtube.com/watch?v=UT41-TdvF4c](https://www.youtube.com/watch?v=UT41-TdvF4c)

CPU: **Threadripper 3960x** \- 24 cores, 6 cores per GPU should be enough, can upgrade if required

Cooler: **Cooler Master ML360 TR4** \- there seem to be only two AIOs specifically for TR4 socket, second is Enermax and they have a super bad reputation. Need AIO because tower cooler will obstruct GPUs in my planned case.

Memory: **Corsair 128GB Vengeance LPX DDR4 3200 MHz** \- less RGB the better, easy to upgrade if required, 3200Mhz seems to be what Threadripper 3 supports out of the box w/o overclocking.

GPU: **4x 2080 ti** to begin with (already have), swap **1x 3090** after initial reviews. Probably EVGA brand because good customer support (personal experience). Confirm no unexpected issues, then swap more as needed.

Storage: probably **Samsung 980** when they come out, some spare SSD in the meantime

Case: **Minestation 1** ([link here](https://www.xcase.co.uk/collections/mining-chassis-and-cases)) - Mobo directly below GPUs, with AIO cooler everything should fit out of the box (or should I say ""fit in the box""?).

Raisers: **LINKUP PCIe 4.0 X16** Riser Cable ([link here](https://linkup.one/linkup-pcie-4-0-x16-riser-cable-x570-rx5700xt-tested-twin-axial-itx-vertical-ultra-gaming-pci-express-gen4-2020-universal-90-degree-socket-30-cm-3-0-gen3-compatible/)) - need to research more

PSU: need to research more - any advise or good resources on dual PSU setups?

I was thinking a lot about water cooling as well (never done it) and it seems like more hustle than worth it.

Hope this helps. Any feedback appreciated!",1.0
g4bmxwk,iltyq5,"last generation nvidia cards had some [serious issues](https://www.guru3d.com/news-story/nvidia-acknowledges-issues-with-geforce-rtx-2080-ti-fe.html) in the first production batches.  it may take a couple months until drivers are fully stable, and integration with the major deep learning frameworks is done.",1.0
g4zknsp,iltyq5,"I'm not sure you'd ever need 96GB of VRAM for any deep learning *research* purposes (and I say that as someone who does this for a living) unless its a server for an entire lab, in which case you should go for an A100 or two.",1.0
g5ki6m4,iltyq5,"Should we wait for the Zen3 threadrippers? When are they gonna be announced? Any ideas, guys?",1.0
g5mbl92,iltyq5,"I emailed [Linkup.one](https://Linkup.one) asking which one of their PCIe Gen4 cable they think is the best for RTX 3090. It seems that they have an upcoming release of an Ultra V2 cable that is be an ""even better fit"" for the RTX 3090. I am not sure what they mean by ""better fit"", so I will be posting more questions to them. I am copying and pasting the reply below.

""""""

Hello customer,

Thank you for your inquiry and patience. As recommended by our technical team, our Ultra line of riser cables will work best for the upcoming RTX 3090. We are also releasing an updated Ultra V2 line next year that will be an even better fit. Thank you for your attention.

Best regards,

[LINKUP.ONE](https://LINKUP.ONE)

""""""",1.0
g3uoa1j,iltgb3,"Thank you for sharing this! It's very helpful. 

I think that cloud GPUs really democratize deep learning. There are not needs in buying expensive PC with GPU to train my models anymore. But I also believe that machine learning should be democratized even more (for non-coders and citizen-developers). 

What do you think about no-code AI development platforms? Might it be the next step in machine learning?",1.0
g3yobfo,iltgb3," Thank you! Cloud GPUs can turn out quite expensive, too, if you use them a lot! The advantage of SageMaker is that it destroys the instances when the training is over. 

Personally, I don’t really believe in “no-code” AI - or at least I wouldn’t call it “the next step” in machine learning. I think right now we’re quite far from democratizing AI in the general sense. Hard as it is to write code for training neural nets, as well as to tune and diagnose them, it would be arguably way harder to even explain the task to the machine without code. Replacing lines of code with buttons won’t eliminate the need to understand the underlying principles of designing and training neural networks. 

But the good news is you don’t really have to be an expert in computer science to learn the basics of Python, and that’s pretty much enough to get started. In this sense, I feel that AI is already quite available to the general public, mostly thanks to the existence of frameworks such as TensorFlow and PyTorch. Second, there’s already plenty of inference services available, which people can use without having to train their own models. And then there are all the AutoML platforms, such as DataRobot, Dataiku, and the like, which make the whole process more clickable and visual, but it also means you’re constrained by all the interfaces and the abstractions the platform adds on top of pretty much the same code you could have written yourself. And all that still doesn’t eliminate the need for experts when the automatically created model eventually fails to meet expectations. 

I’m pretty sure, though, that there will be more and more “no-code/low-code” solutions on the market in the near future, but it will take quite some time for them to become useful. But that’s just my opinion. And I’m quite excited to see how the future turns out!",1.0
g3v19zn,iltgb3,Great tutorial thanks ! Have you planned a part 2 yet ?,1.0
g3ymayr,iltgb3,"Thanks for reading it! Yes, part 2 will also be out soon - hopefully in October. Stay tuned!",1.0
g3urdxi,iln1bk,"Thanks, I wasn't aware of this optimization.

Anyone know if Keras or TFLite include similar options? I assume not.

Would we do it in a loop roughly like ""if layers[i] is BatchNorm: layers[i-1].bias=False"" ?",1.0
g3xi92m,iln1bk,"I am not familiar with TensorFlow/Keras, but that's exactly what the dlib implementation does.

In this [thread](https://www.reddit.com/r/MachineLearning/comments/ikr8u8/d_pytorch_performance_guide/), I added a [comment](https://www.reddit.com/r/MachineLearning/comments/ikr8u8/d_pytorch_performance_guide/g3okyc7/?context=3) on how I normally do it with PyTorch.",1.0
g3qfmef,il9m2j,"If you’re looking to count individual tomates - get about 50,000-100,000 varied images, annotate each tomato in each image with a bounding box, then train YOLOv3 on that labeled dataset. 600 images is *nowhere near enough* to train a model like this.

Alternatively, if all you care about is “does this image have any tomatoes or not”, just create a labeled dataset (image has tomatoes, image doesn’t have tomatoes), and fine-tune something like a resnet50. Again, 600 images is not enough here either - think more like 10k-50k+ images.",1.0
g3qgdgf,il9m2j,Omg 10k. Well fuck. Thanks for your input. I couldn't find much data online. Ill probably use data augmentation.,2.0
g3qjimn,il9m2j,"Yeah, you generally need a ton of data for DL models to generalize well. It’s rough, haha.",1.0
g3qh00b,il9m2j,Sounds like a cool problem statement. Will you be interested in collaboration? Please DM me if there is interest.,1.0
g3qhpn2,il9m2j,Hey i have dmed you,2.0
g3rh8z2,il9m2j,"Well I think you are confused here, yolo networks are used for object detection, if you just want to detect if there are tomatoes or not, click around 400-500 more pictures with same background. But a classifier from DL with 1200 images seems quite difficult, and not recommended to be used in production. 

For fun you can use hand crafted features like color of tomatoes/pixel density of tomatoes or other stuff like that. 

Or you can use a pretrained network something like VGG which has been trained on jmagenet, as Tomatoes is category on imagenet(ig), this could be you best approch of DL.

Training a yolo on 600 is not something I can recommend.",1.0
g3rxd4v,il9m2j,"There are several academic papers on fruit finding in trees etc.

For example

[https://www.researchgate.net/publication/312298640\_Counting\_Apples\_and\_Oranges\_with\_Deep\_Learning\_A\_Data\_Driven\_Approach](https://www.researchgate.net/publication/312298640_Counting_Apples_and_Oranges_with_Deep_Learning_A_Data_Driven_Approach)",1.0
g3syp8k,il9m2j,"There was a wheat head detection competition on kaggle, you can use transfer learening from that data. Another post I saw augmented images by cropping out tomatoes, augmenting and pasting on random backgrounds to get more data, (used for training and original data for validation)",1.0
g4bpk1j,il9m2j,"tomatoes are pretty simple objects, i suppose a small dataset can already achieve decent results. yolov3 is a solid choice if you want to detect the bounding boxes of objects. it won't magically detect covered fruits behind others. i'd suggest you train a yolov5 model, it's easier to use than v3 and showed better performance in my experiments.",1.0
g4bpyhr,il9m2j,"Okay cool
Nice .. you're right. As I'm typing this I'm heading home after collecting 600 more pics.. we already trained a model with 450 pics with yolov3 .. loss was 0.45.. now trying f a better model n. I'll def try v5. Using darknet",1.0
g4bqaw2,il9m2j,"You should measure performance in a more expressive way. mAP is a typical metric for object detection, or even just the number of detected fruits if that's what you're after in the end.

V5 is based on pytorch.",1.0
g4bquf9,il9m2j,"Never heard map I'll be sure to check it out. I'm sure there's v5 on darknet.  I'll try to go with v4 if it's not there. 
Also is there any way to train my Images in 1024 x 1024 .. I feel like it might be better with accuracy. Or is 412 x412 fine ?",0.0
g4bsmju,il9m2j,Sounds like you need a more experienced advisor for this project,1.0
g3stkz9,il7syg,"DL is simply ML with &gt;1 hidden layer, no video required to explain it.",1.0
g3ppxco,il4sxj,"It’s kinda approximate, but that’s almost close to the max performance! You can use timeit to be more sure. Also, measure in your target device if you are not already doing it.",1.0
g40m4z3,il4sxj,"I've also checked the source code for many libraries, they just used the simple time.time() function.",1.0
g3pbe92,il3uil,"I'd give it a shot and ask Google directly.

"" Send us feedback!

If you have any feedback for us, please let us know. The best way to send feedback is by using the Help &gt; 'Send feedback...' menu. If you encounter usage limits in Colab Pro and would be interested in a product with higher usage limits, do let us know.

If you encounter errors or other issues with billing (payments) for Colab Pro, please email [colab-billing@google.com](mailto:colab-billing@google.com).""

[https://colab.research.google.com/notebooks/pro.ipynb#scrollTo=mm8FzEidvPs6](https://colab.research.google.com/notebooks/pro.ipynb#scrollTo=mm8FzEidvPs6)

Personally I don't think that measuring inference time makes a lot of sense. You probably do not use a dedicated cpu nor a gpu which makes your results dependent on the overall system load (but that's just a wild assumption). You could however just repeat your runs every hour and check how consistent they are. To give a more expressive estimate you can also report FLOPs or other metrics (here are some suggestions  [https://machinethink.net/blog/how-fast-is-my-model/](https://machinethink.net/blog/how-fast-is-my-model/) )",3.0
g3qb0nd,il3uil,"Ex- Google Cloud employee here. But I didn't deal with GPUs so this answer may still be wrong.

As far as I know, a gpu is dedicated to your run. You aren't sharing it simultaneously with other users. (Unlike cpu power and disk i/o which is likely shared). You may get a different GPU instance one minute to the next, but I think they are yours until the run finishes.

I believe the gpu being advertised is what you actually get. (Unlike CPUs. If you request an old cpu on Google cloud, it may be emulated on a newer cpu.)

Thinking about those cpu/disk considerations, they could still have a big effect on your benchmarking. Keep that in mind.

And the performance differences vs a desktop are still pretty big. Your code runs in VM which shares a physical host with many other VMs. The physical hosts are these crazy machines with stacks of custom many-core cpus and multiple video cards. It's such a different world, the performance won't be exactly the same as a desktop. But I'd guess it will be somewhere in the range of 75%-100% the same.",5.0
g40lvdq,il3uil,"Thank you for your reply. I read a paper about the performance of  Colab resources comparing to desktop architecture, I've quoted this from the paper "" *Results show that Colaboratory hardware resources can reach a performance similar to dedicated hardware. Results also show that it is wort to run experiments on Colaboratory in case the research group has no GPU more robust than a K80. Moreover, it is possible to accelerate other GPU-centric applications than deep learning related ones, with no need for CUDA runtime configuration. Besides the performance, it is interesting using this cloud service because it is straightforward to share notebooks with code and outputs.*""",1.0
g3nv7om,ikw35r,"Pooling layers have no weights. There’s nothing learned there.

But more than that, a pooling op is fundamentally not the same as what a convolution op is.",3.0
g3nyqjd,ikw35r,But what's the point of the filtering,0.0
g3o0538,ikw35r,Read up on regular convolutions - like blurring in photoshop.,1.0
g3q2iko,ikw35r,"Well, it is a genuine question. And I think I have already answered it in this post. [Why CNN?](https://www.aiunquote.com/post/100-fastest-ways-to-learn-deep-learning-project-3-why-cnn-most-intuitive-understanding-of-cnn)

In brief, Filters extract information from the image like Edges, Lines, Patters etc. These info that needs to be extracted are learnt during training. It depends largely on Data what info to extract from filters. 

Whereas Pooling does not extract feature, rather it takes the avg value of nearby n pixels so that the most important info in the image is retained reducing the image size. Which is req to make large models.

You can learn a lot more from my post where I have explained everything in detail.

[Intuitive Understanding of Why CNN?](https://www.aiunquote.com/post/100-fastest-ways-to-learn-deep-learning-project-3-why-cnn-most-intuitive-understanding-of-cnn)

I hope this helps. 
Thank you. 
Follow me for any future questions you may have.",3.0
g3ojrto,ikw35r,"A filter is used to detect lines or edges and after some successive layers its can start detecting even more complex shapes.
For example take a filter                                                 
|-1 0 1|            
|-1 0 1|                
|-1 0 1|          
This can be used to detect vertical lines or edges (here -1 means dark colour and 1 light colour).
Hope you are getting intuition behind it.",2.0
g3p8fxd,ikw35r,"Pooling does nothing but destroy/aggregate spatial information, it has its purpose but thats another discussion. The purpose of learned parameters in a neural networkis that you try to approximate whichever mathematical function could possibly take you from input to output, e.g. maybe the numbers which represent an image of a cat to the probability of a cat being in the image. Blindly pooling that input into a single value has no chance of actually turning in meaningful probability value. How you learn the right weights to approximate that function is through gradient descent which you should read about. Why use convolutions instead of just fully connected layers which learn parameters connecting every input to every output? 1. Too many parameters 2. Convolution is translation invariant. So I can use the same small filter to recognise a cat on the left side of my image and the right side. I don't need to learn weights for my filter to recognize cats at every position and more importantly I don't need a dataset which has cats at every position.",2.0
g3q5d7e,ikw35r,"Here's a narration of convolution followed by pooling.

Say I have a 5x5 pixel filter patch that represents something small like a human eye. Convoluting this filter will tell me how ""eye like"" every spot is in a new image. Now say I have 512 of these patches representing different things like hair, chins, pimples, etc. After convolution, I have now evaluated every spot in the new image for 512 different qualities.

Now this is getting untenable because I need 512x more memory to store all the new data. So I use pooling.
Before pooling, I had a map that told me how ""eye like"" each spot in the image is. After pooling, this map is much less precise and only tells me whether there was an eye in a general region of the image.
I've sacrificed a lot of data, but I've saved so much memory that I can keep going and add more layers and make an even smarter network.


Where it gets really unintuitive is when the next layer is a convolution. It's hard to narrate a convolution of a convolution. The 2nd convolution can detect bigger things (because it analyzes a 5x5 patch where each pixel was already generated from 5x5 patches). And it can detect combinations of things (because the previous layer already mapped out 512 qualities). In my face example above, a 2nd convolution may be detecting a combination of an eye with an eyebrow, for example.

And so on, with each convolution layer detecting bigger and more complicated things, and each pooling layer keeping the memory usage down.",2.0
g3oky3p,ikw35r,"? To create feature maps with learned filters.

If you only pool this is same as crappy resizing.",1.0
g3mm6qs,ikp04e,MobileNet isn’t that great. Try fine tuning a resnet50. You also might be overfitting.,2.0
g3ouhly,ikp04e,"I plan on training resnet50 for ""offline"" inference, I want MobileNet for real-time performance. I'll try testing earlier checkpoints on the camera images to see if I'm overfitting.",1.0
g3of5w8,ikp04e,"Have you tried not freezing the base layers? Or alternatively doing an extra-long fine tuning pass? Your classifications are so different than imagenet that it might need to repurpose a lot of the pretrained weights.

And are you using a patience callback to decide when to stop training? I often set patience to 10 or 15. (And make sure to restore the best weight after training finishes). https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/

I've also had situations where data augmentation hurt more than it helped, due to my own bugs. So try turning it all off and see what happens.",1.0
g3oup19,ikp04e,"Thank you for your detailed comment. I unfroze the base layers after first tuning the top layers. I used a patience callback and early stopping, but with patience set to 4 so I'll try increasing it. 

As for data augmentation, the noise and blur is randomly added third of the time and with different ""intensity"" so I hoped it will classify both low-quality and high-quality images well.",2.0
g3m40l9,ikohsd,That 3090 tho,20.0
g3m6gmv,ikohsd,I mean if they follow like they usually do you'll get a titan with 48GB coming soon.,3.0
g3mc6jf,ikohsd,Ahh goodbye RTX Titan superiority :(,2.0
g3m75yf,ikohsd,"Yeah, but its more of a budget constraint tbh, planning on two 3090s, one from my budget, and another from a govt scholarship(that i have to return after my thesis)",1.0
g3p9det,ikohsd,Why is this downvoted lol?,2.0
g3m8wy0,ikohsd,"Yep they are pricey. What you can do, if you aren't already, is when you aren't running any models/your cards are ideal, you can do crypto mining and make a little money to subsidize the cards.",3.0
g3n9xqg,ikohsd,"Is there a coin this still works with? I was mining back during the bubble and at one point the ASICs raised the network hashes to a level that GPUs couldn’t outpace their power consumption expenses. I haven’t been following closely since then, so if there is still a way to make a single GPU system profitable I’d love to know how. Winter is coming and I wouldn’t mind my desktop being a space heater once again.",2.0
g3nf8nw,ikohsd,Yep GPU mining is actually reasonably profitable right now. A lot of coins design themselves to be GPU friendly. You can check out www.whattomine.com,3.0
g3ngpgk,ikohsd,"Sadly this tells me it’s still not cost effective. The issue is the system power consumption. My GPU only uses 120w, but to have the computer on and running the power at my outlet is 250-300. I like to use [crypto compare](https://www.cryptocompare.com/mining/calculator/eth?HashingPower=29&amp;HashingUnit=MH%2Fs&amp;PowerConsumption=250&amp;CostPerkWh=0.12&amp;MiningPoolFee=1) because I can plug in the whole system power consumption. 

That being said, these new cards might get ludicrously higher hash rates making them viable in a single GPU setup.",3.0
g3o91qi,ikohsd,"It's not a efficient aa a heat pump, but you could think of it as free mining while heating your house with a really expensive space heater.",2.0
g3n8ylf,ikohsd,What I'm not sure about the 3090: can all 24gb be allocated to training one model? Or is one of those  x2 SLI-like cards where you would need to train 2 smaller models to get 100% utilization?,6.0
g3o5uqt,ikohsd,"Anybody got thoughts on which to go for? Maybe I should wait for actual reviews eh?

For reference, I’m just starting out with deep learning (gotten decent at classical machine learning tho!), but having fun with it and have been saving for a while for this generation of cards. Could maybe do the 3090, but that whacks the budget (and it’d be all of my “fun budget” for a while). Got a couple projects in mind, one NLP based (an upgraded version of something I got running decently with classical ML techniques) and one involving video classification (of maybe, say, 1080p 30 FPS video). 

How much of a hindrance would the 10GB of VRAM be on a 3080 vs that big 3090? How much performance improvement we talking about for deep learning stuff? After all it’s, you know, 47% of the price. All the internet discussion seems to center around gaming performance at present. TIA!",3.0
g3oottx,ikohsd,"Depending on how many hours per week / how many tflops per week you need, consider some cloud alternatives instead.

Google Colab (pro) can get you quite far and similar hardware isn't that expensive anymore and can really scale up.

But if you also want it for gaming, I would wait another 3 month or get it for Black Friday.",3.0
g3r18ct,ikohsd,Have nvidia cards historically dipped in price on black friday? (new),2.0
g3mwlsx,ikohsd,Are there any plans for the 3080 Ti edition?,2.0
g3n1qxq,ikohsd,"None have been announced, most likely the 3090 is the ti card for this year",3.0
g3nd8bh,ikohsd,The 3090 is the Titan equivalent. They are simplifying the naming conventions.,4.0
g3nmj5k,ikohsd,"I wouldn't be surprised if they eventually release a 48GB Titan for $3000 or something...

I feel like the 3090 is sort of between the xx80Ti and Titan.

And really, I think they're trying to be more competitive against AMD... The 2080 used a smaller die than the 2080Ti, and the 2070 used an even smaller die. Now the 3080 is a cut down version of the 3090 and the 3070 uses the equivalent of what would have been the 2080 last generation.

They're basically dropping prices from Turing's ridiculous levels without weakening their brand and reducing customer confusion at the same time.",3.0
g3oloci,ikohsd,"IMO, with this release, especially that 3070 is faster than 2080 Ti - AT THE SAME PRICE, they are not competing with AMD. They've blown AMD out of the water with this. I'm neither an AMD nor NVIDIA fan-boy but holy shit man NVIDIA killed AMD's GPUs \*\*and\*\* ROCm.",3.0
g3onlg9,ikohsd,"We'll find out when AMD announces their products. That said, if NVidia didn't expect AMD to compete with the 3080, I doubt they would have priced it at $700.

NVidia was very successful by releasing CUDA early and promoting it heavily. I think it's bad for the market for NVidia to have such a near monopoly; we would all be better off if more software used something open and cross-compatible like ROCm or HIP. NVidia just has so much momentum behind them and researchers who already have NVidia hardware don't have any downsides to sticking with CUDA, though.",2.0
g3nreme,ikohsd,[deleted],2.0
g3o6ti3,ikohsd,"The 3090 is $1k cheaper than the Titan RTX. That's why NVidia would release a $3k Titan, for those people who already paid $2.5k last gen and have to have the best no matter what.

I'm confident NVidia is at least considering it internally (pending Micron 16Gbit die availability). I have no idea whether they'll actually do it.",2.0
g3ouc3n,ikohsd,I really wonder if the RTX IO feature can be integrated into the DL frameworks to work like pseudo VRAM and allow for larger models with the 3080 and like.,2.0
g3psgia,ikohsd,"What are you guys' thoughts on the 3-slot design of the 3090? For my research group we worked with shared 4-gpu servers, but I don't think any of those chassis would be particularly viable for 3-slot GPUs. I think it's a bit of a shame, considering the specs are great for DL research (at least for groups that can't or don't want to afford quadro/tesla cards..).",2.0
g3ofsl1,ikohsd,"Daym, only 10gb on the 3080. This fucked me up :(

16GB for the 3080 would be my sweet spot with 12gb being the w/e place. The 3090 isn't really that much better (vRAM aside) to justify the price bump.

I guess i'll wait to see what AMD has to offer or i'll have to wait another generation.",2.0
g3opqsb,ikohsd,When is AMD coming out with their stuff? I want to make a PC build but not sure when to do it.,1.0
g3ospjx,ikohsd,"The rumors say that their graphic cards will be released mid October. I wouldn't buy a cpu right now since ddr5 is arround the corner and the new Amd socket as well. Not to mention that intel has to answer given the fact that amd took a big bite of intels market share which they probably want back. 

Buying a 3070 or 3080 now seems a no brainer, but amds new cards might be legendary as well (nothing wrong with waiting for the benchmarks).

My problem is that i am working with Cuda which means i can't have an AMD :(",2.0
g3s8wlj,ikohsd,I believe there will be higher memory 3080s from other brands. But that 3090 looks beastly,1.0
g3ooncv,ikohsd,When will it affect the pricing for 2080 TIs?,1.0
g3opp1q,ikohsd,Actually earlier today used ones started filling ebay and other sites between 450 and 700 usd,2.0
g3orvj1,ikohsd,What would be the safest bet right now until I wait for 30xx series cards?  I was thinking any dirt cheap card for my current ongoing build.,2.0
g3pfek5,ikohsd,"Lmao the pricing for the used GPU marketplace never makes any sense. All 3 of these new cards are 2080TI equivalent or much higher, with prices starting at $400. Why anyone would buy a used 2080TI for the same price is beyond me.",1.0
g3oprcp,ikohsd,[https://wccftech.com/nvidia-geforce-rtx-2080-ti-used-market-flooded-after-rtx-3080-rtx-3070-unveil/](https://wccftech.com/nvidia-geforce-rtx-2080-ti-used-market-flooded-after-rtx-3080-rtx-3070-unveil/),1.0
g3orrgi,ikohsd,Wow it's happening already.,1.0
g3mom1z,ikohsd,I mean people make whole rigs worth $1499,-4.0
g3nrj14,ikohsd,[deleted],1.0
g3o79kh,ikohsd,Did I say that nobody should buy the 3090?,2.0
g3n8h4g,iknun3,"Hi,

I have about 2.5+ years of experience of working on DL based research and development in govt. company as well as startups in India. Let me start by sharing my experience

Work exp:

CDAC (under MeitY): Part of GIST group at CDAC which worked on NLP for Indian Languages. One of the best environment I could possibly get as a fresher in DL. Seniors were really encouraging and the opportunities were also plenty. We had the freedom to pick the projects we wanted to work on. I worked on Indian language transliteration as first project. Further worked on translation, pos/ner tagging all using DL. Also got chance to explore Computer Vision. Worked on face detection, recognition and de-duplication (finding duplicate faces on a really large dataset). Crowd tracking and counting, crowd density estimation. 

NOTE: Though my experience was really good, it was mostly because most/all of the people around me and my seniors were really awesome guys. I have seen many guys complaining about the work too.

&amp;#x200B;

Startups: The startup I worked at was a typical Indian student startup where most of the guys didn't have a clue of what they were doing, the guy who funded became the CEO and was totally money minded and didn't have a clue of what the guys in the company were doing (just marketing and sales guy). Out of the 8 guys only 2 I found to be knowing what they were doing, everyone else was just there to play Foosball and making quick money. I left this place in 4 months.

Though this was my experience in one place, I have been closely following few startups who are doing really good job in their domains like Infilect, arya.ai or swaayatt robots in ML/DL in general.

&amp;#x200B;

Resources: Anyone working on DL has to have resources (GPU) and most companies working in DL do provide it to their employees (usually cloud).

&amp;#x200B;

If you are joining startups, it'll mostly be engineering work and running github codes initially. Though if the place is good, you'll have opportunities to go more into the latest stuff. But startup is not a place to do research, so its basically engineering/software development work, and being a DL engineer will limit your career options. 

The surge in DL jobs is highly unlikely its all a bubble. Being a good developer is much more valuable than working on DL and you'll get more RoI.

&amp;#x200B;

Competing with state-of-the-art: Swaayatt robots is working on cutting edge and has best in class proprietary tech. So does infilect. So basically anyone doing good is actually competing with the world and not selling something patchy. 

In research, institutes like IITM, IIScB, IIITH are having world class labs and are recognised world-wide.

&amp;#x200B;

Hope this answers your question.",2.0
g3neoph,iknun3,"Thanks a lot for this insightful response. Surely helped me gain perspective. Coincidentally, I'm an undergraduate at IITM, funny that I'm not aware of opportunities at my college itself.

Would definitely consider contacting you, for any further hurdles I might face. Hope you won't mind help me out.

Thanks..",1.0
g3omnlz,iknun3,"You're welcome. I'd be happy to help if you need anything else. Btw in IIT Madras, I think for RL there is R Balaraman sir who is a PhD student of Richard Sutton. Even heir CV lab is really cool in terms of research output, though I can't recall the profs name.",2.0
g3oo34u,iknun3,"Oh, wow. I know Balaraman sir. Never got a chance to take his course though. Seems a pretty exciting opportunity.",1.0
g3nlph3,ikjke4,"It often seems fine to use just batchnorm, or just dropout, or just an l1/l2 regulizer. (In order from more trendy to less trendy). They seem to reduce overfitting, through very different means. I think there's diminishing returns for using multiple. And you'd be adding more hyperparameters which will make it harder to perfect.

My experience was trying to improve an image segmentation U-net. A popular Keras version uses batch norm. The original paper vaguely mentions dropout in one sentence, so I tried adding it in various places. Didn't seem to help much at all. My time was way better spent on data augmentation and initially freezing pretrained layers.

But take a look at published models that are similar to your task to confirm what I'm saying.",1.0
g3ko75s,ikic3b,"Following the amazing turn in of redditors for previous lectures (almost 1000 total people registered - not bad), we are planning another free zoom lecture for the reddit community.

In this next lecture we will talk about image generation using GANs, the lecture is titled: ***Semantic Pyramid for Image Generation.*** Assaf Shocher from Google Research and the paper's author will give the talk.

**Lecture abstract:**

Google Research and Weizmann Institute of Science feature inversion model to generate image space representations from classification classes. The model provides a unified versatile framework for various image generation and manipulation tasks, including: (a) generating images with a controllable extent of semantic similarity to a reference image, obtained by reconstructing images from different layers of a classification model; (b) generating realistic image samples from unnatural reference image such as line drawings; (c) semantically compositing different images, and (d) controlling the semantic content of an image by enforcing a new, modified class label.

We present a novel GAN-based model that utilizes the space of deep features learned by a pre-trained classification model. Inspired by classical image pyramid representations, we construct our model as a Semantic Generation Pyramid - a hierarchical framework which leverages the continuum of semantic information encapsulated in such deep features; this ranges from low level information contained in fine features to high level, semantic information contained in deeper features. More specifically, given a set of features extracted from a reference image, our model generates diverse image samples, each with matching features at each semantic level of the classification model. We demonstrate that our model results in a versatile and flexible framework that can be used in various classic and novel image generation tasks. These include: generating images with a controllable extent of semantic similarity to a reference image, and different manipulation tasks such as semantically-controlled in-painting and compositing; all achieved with the same model, with no further training.

[https://arxiv.org/abs/2003.06221](https://arxiv.org/abs/2003.06221)

Project website: [https://semantic-pyramid.github.io/](https://semantic-pyramid.github.io/)

&amp;#x200B;

**Presenter BIO:**

Assaf Shocher is a deep Learning and Computer Vision researcher, working in Google Research and Weizmann Institute of Science.

Linkedin: [https://www.linkedin.com/in/assaf-shocher-271424b7](https://www.linkedin.com/in/assaf-shocher-271424b7)

&amp;#x200B;

**Link to event (September 8th):**

[https://www.reddit.com/r/2D3DAI/comments/ia66ct/semantic\_pyramid\_for\_image\_generation\_cvpr\_2020/](https://www.reddit.com/r/2D3DAI/comments/ia66ct/semantic_pyramid_for_image_generation_cvpr_2020/)",1.0
g3ko7dw,ikic3b,"[mp4 link](https://preview.redd.it/njz2iej8n3k51.gif?format=mp4&amp;s=3ce58b1f3eaa59692aa890bf5689fcccecadb8a2)

---
This mp4 version is 92.13% smaller than the gif (678.98 KB vs 8.42 MB).  


---
*Beep, I'm a bot.* [FAQ](https://np.reddit.com/r/anti_gif_bot/wiki/index) | [author](https://np.reddit.com/message/compose?to=MrWasdennnoch) | [source](https://github.com/wasdennnoch/reddit-anti-gif-bot) | v1.1.2",1.0
g3kjvpd,ikehds,"I am not sure if dl can be used for file compression in the classical sense (as in like compressing a file to a zip and the converting it back without any/much loss),  also, this might be computationally expensive as we need a specific size/chunk or format to process using a model, as deterministic approaches are working fine. Things for images might work (save only an embedding and pass it through a decoder to get it back, but there is still a huge loss of data and training of a practical model is difficult",2.0
g3k1qny,ikd3fm,"This really shouldn’t be an issue, you should be able to get it to converge. The real issue comes from controlling the class when sampling/generating from your model. For that you need a conditional GAN.",3.0
g3k94mm,ikd3fm,"My problem is not generating samples.

My problem is training a GAN on in-distribute data so that we get a model that we can feed new images to it and it will answer whether the new images are out-of-distribution or not.

The issue is my training data consists of multiple classes, yet, we don't know the class of images when testing. Thus, I don't think conditional GAN can help.",1.0
g3likjf,ikd3fm,"Ah interesting, a couple thoughts now rereading your post. 1) I hadn’t caught it the first time but it sounds like you could be experiencing mode collapse. This is where the generator learns to only produce one specific sample time that still fools the discriminator. This is a rather tough problem to solve and usually just comes from training more of these. 2) Anomaly detection or o.o.d seems like a weird application for GANs, you never “feed” new images to the GAN explicitly, you have it implicitly learn them through the discriminator. I am thinking some sort of autoencoder may be what you want. High reconstruction loss at test time would indicate an o.o.d as the features are things the AE model has never seen before.",1.0
g3t2hmn,ikd3fm,"1. I think mode collapse is the keyword I'm looking for, thanks!
2. If you're interested, here's the work I'm using in my project:  [https://arxiv.org/abs/1903.08550](https://arxiv.org/abs/1903.08550)",1.0
g3jygh5,ikd3fm,"Is this helpful 

https://arxiv.org/abs/1606.01583",2.0
g3k8al6,ikd3fm,"Thanks, I'll check it",1.0
g3jvkn3,ik9401,Well presented sir.,2.0
g3kvcak,ik9401,Thank you sir :),1.0
g3iaqhw,ijyqyc,"If you used the standard scalar then your input data has been normalized to zero mean, deviation of 1. You definitely don't want to sigmoid activate as it will truncate negative numbers.  You probably could get away with tanh but again risk truncating beyond -1 and 1. The best here is no activation.  Make sure to invert the scalar to get back to your original input data range.",2.0
g3hbcbr,ijyqyc,"Best way is to convert I/p image to 0 to 1 fp and then do a sigmoid at the output to make sure output is bound between 0 and 1. This can be multiplied by 255 for 8 bit per channel pixel values. Without activation function, loss functions ""may have"" instability with large output vs image 0 to 255 comparison. 0 to 1 limits/bounds loss range.",1.0
g3hi1yj,ijyqyc,"I had to specify  that I ´ma working with tabular data , specifically time series data. Sigmoid is pretty classic and tends to sature contrary to tanh activation where the range is bigger . It’s also depend on the preprocessing , I used a Standard Scaler , I don’t think that sigmoid may do the work",1.0
g3gfohg,ijvslr,Thank you for linking to this.,4.0
g3g2be8,ijt31i,"Batch norm is to normalize your outputs of a layer based on the batch, initialization is to make the initial state of training better for the rest of it, batchnorm affects the data (outputs of 1 layer which are inputs to other) and initialization affects the network(weights) and is needed to get appropriate training flow from the network side (not explode right away or start too slow), whereas batchnorm is a continuous thing while training",7.0
g3esdtl,ijmk0s,"Right now is a great time to take on a computer vision DL project, specifically AR. Currently the only notable projects in this field are things like object detection, real-time translation, etc. Mainly classification tasks. Something to try could maybe to build an object segmentation app. Current attempts have suffered from performance or accuracy so there's room for improvement. 

  


Pain points of these kinds of projects from my experience are:

  


• Developing a model that performs well in the wild 

• Making the model inference times acceptable 

• Getting data from app side to model, and interpreting the output 

• Making the pipeline close to real time",5.0
g3mz2tl,ijmk0s,Something like instance segmentation ? Seems like a great idea! Thanks!,1.0
g3o5chv,ijmk0s,"Yeah absolutely! If you have experience with AR development, things like ARCore and ARkit, there are opportunities to integrate DL with the depth APIs offered by these two toolsets. I don't know the stage of apples ARkit depth api, but Google's new AR depth api built on Unity has 0 DL applications (because it has barely a single application of any sort). It's something I've wanted to experiment with. Just something to consider, not sure of your comfort levels with any of this :)",1.0
g41evgq,ijmk0s,unfortunately i don't :( but i'll add that on my list of things to learn!,1.0
g3gdp6c,ijmk0s,Upvoted for better reach,1.0
g3myw8d,ijmk0s,Thanks!,1.0
g3fyf30,ijmjtc,"I think it's quite tricky to select features that are most predictive of something, but you can select those that are more representative of the variance in the data. You can look at PCA perhaps or related non linear dimensionality reduction methods, they usually aim at extracting a basis from the data that with fewer features can best reconstruct the data",2.0
g3g5uby,ijmjtc,"I m using autoencoder for segmentation using the latent representation . My reconstruction loss is quite good ( 10e-4 , with MSE + KLDivergence + L1_loss ) . But I saw that a low loss doesn’t generally  mean that my autoencoder performs well",2.0
g3gb1tq,ijmjtc,"KLDivergence? So are you using a VAE? If that the a  decreasing ELBO may just mean that you are getting closer to the proprio rather than increasing the likelihood of the data.

Also you should keep in mind that because MSE assumes gaussian noise around the target, you are smoothing out the actual error, which results in the AE or VAE filtering out high frequency variances (aka loosing a lot of detail/denoisong) which may not be ideal for your application",1.0
g3gb55b,ijmjtc,"No I’m not using VAE , I m using a sparse autoencoder ( SAE )",1.0
g3gcpj8,ijmjtc,"I haven't used them as much but I think they filter features even more keeping only the most informative axes (or at least I would guess that yo be the result of ""automatic pruning""), but also correct me if I am wrong, wouldn't you do L1 regularization or KL? Isn't both a bit too much? in which case I would expect the prior to be way stronger than the data's likelihood given the model which would indeed result in what you stated earlier (loss decreases but it doesn't necessarily learn what you want to learn)

If you need to choose between the two do you have anything suggesting that your data is laying in L1? if so why L1 regularization and MSE instead of MAE (seems a bit odd yo regularize in the L1 space but then quantify the reconstruction error in L2, perhaps you can just assume laplacian error instead of gaussian, and keep only one regularization term )",1.0
g3f70p1,ijlqni,"The 1080ti is the workhorse of DL. Its performance is as good as a 2080 for FP32 but isn't made for mixed precision. It does consume more power than a 2070 super though. I'm not sure if a 2nd hand GPU is worth it though, especially if it comes after a gaming workload.",3.0
g3f71jp,ijlqni,"Hi not sure if a 2nd hand GPU is worth it though, especially if it comes after a gaming workload, I'm Dad👨",-6.0
g3g0efq,ijlqni,The 3080 / 3090 are expected to be announced on Tuesday so would suggest you hold off for a couple days.,3.0
g3g1xpg,ijlqni,"The 3090 looks to be an absolute beast in terms of size and power consumption if any of the leaks are to be believed  
[https://www.tomshardware.com/uk/news/zotac-seemingly-leaks-rtx-3000-series-ampere-graphics-cards](https://www.tomshardware.com/uk/news/zotac-seemingly-leaks-rtx-3000-series-ampere-graphics-cards)  
 [https://www.tomshardware.com/uk/news/geforce-rtx-3090-rtx-3080-specifications-reportedly-exposed-in-vendor-spec-sheets](https://www.tomshardware.com/uk/news/geforce-rtx-3090-rtx-3080-specifications-reportedly-exposed-in-vendor-spec-sheets)",4.0
g3g46l1,ijlqni,Thanks. I think that is what I am going to do.  Apparently the new 3070 will have 16GB VRAM for an acceptable price so I might go for that.,2.0
g3gjx33,ijlqni,"i dont think thats true , i mean the 3080 only have 10gb vram according to leaks",1.0
g3tcz89,ijlqni,"Since the cards specs are out now there is immense value in RTX 3070 .  163 TFLOPs for 3070  vs 114 TFLOPs( **Tensor Perf. (FP16)** ) for 2080I  which is 42 percent gain for 3070 .

The only downside is 8Gb of memory  which should have been 12 GB IMO.",1.0
g3ucgmm,ijlqni,"there is a rumered 3070ti with 16gb so its coming , maybe after amd announce thier new cards",1.0
g3ftuhs,ijlqni,"It depends, the speed will be more-or-less similar for FP32 (i.e. ""normal"") models. With a new 2070S you're getting a warranty and faster inference/training in FP16/Mixed-Precision. With a used 1080Ti you're getting more memory allowing for larger models. This is a tradeoff, some code you find will work with FP16 but others won't. Some models you want to try will require more memory than others. You probably won't need your warranty but if you do have an issue you might be sad to not have it, and the second-hand market is always a gamble since you don't know how the card was treated.

Ultimately neither of these cards is in general ""better"", or even better in terms of well-rounded-ness. They're pretty close in value for the task. If they were both new the 1080Ti might slightly out-balance the 2070S on average, but its so close that the warranty really puts it on par.",1.0
g3g4llm,ijlqni,"Thank you. The warranty truly is an important concern - for almost all cards that I can find on eBay in my country the warranty has ended, for a the few other cards warranty ends in 2 months. 

I will wait for the RTX 30XX release though.",2.0
g3g4pum,ijlqni,"Be aware that typically warranty is only valid for the original purchaser. So treat any used cars as ""no warranty"" because it is entirely possible to have it rejected because you bought secondhand.",2.0
g3g5fum,ijlqni,Thank you for that insight! I will keep it mind when buying second hand.,1.0
g3ep2q2,ijlqni,"Depends on size of models you are interested in working with. VRAM is often the limiting factor on what can be accomplished in terms of model size. A 1080ti with 11GB VRAM will allow you to work with larger models than the 2070 at 8GB, it'll just take longer (and higher electricity bills) to get there.",1.0
g3errs7,ijlqni,"Thank you for your reply.

[This often cited article by Tim Dettmers](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/) suggests that although the RTX only has 8GB, it also has Tensor Cores which allow for mixing/ changing float precision to 16 bit and thus   
 \&gt; a 16-bit 8GB memory is about equivalent in size to a 12 GB 32-bit memory

Does that always hold true or will I actually not be able to train networks that would work with the GTX 1080 Ti?",1.0
g3eyhjh,ijlqni,"At this point, it's completely a matter of what you are trying to accomplish.

Code you pull from github or papers you try to duplicate will have different assumptions baked in. Do you want to git pull a repo and then spend time reworking it to 16bit precision if not natively supported, or do you want it to work out of the box?

What is the area of work you're interested in? Do you even need more than 8 GB VRAM? Have you tried prototyping your work in Google Coloab to see what you actually need? 8GB and 11GB isn't vastly different.",2.0
g3g4eoe,ijlqni,"Thank you. Your comment made it clear to me that I should first get a firm understanding of what I will actually need before spending money. 

Regardless of that, I will wait for the RTX 30XX release.",2.0
g3flrfe,ijlqni,It's only true when you use models with 16-bit floats. An 11GB model with 32-bit floats will still require 11GB on the 2070 so it won't fit.,2.0
g3e8q32,ijjpg2,"We’ve found that literally none of the object tracking approaches available in OpenCV are good enough for real world use in our applications. Besides being almost shockingly incapable of anything more than basic object tracking in very static environments, nearly half of them are slow as hell - slower than object detection DL models in some cases.

Walked away from OpenCV’s object tracking very disappointed.",1.0
g3euv42,ijhb5y,What does this have to do with deep learning?,13.0
g3g0ia7,ijhb5y,"Yeah, just report it. The sub has been invaded with non DL stuff lately...",1.0
g3er07a,ijgz6w,"Im a PhD student studying deep learning. Very focused on the maths side of things rather than application. I work 6 hours a day of focused work in two blocks of 3. 9-12 and 1-4. I do this 6 days a week with Saturday off. I feel like I'm learning and producing at the same level as the people around me who claim to work crazy hours.

I'm a big believer that a strict schedule can provide you with more freedom. Schedule the hours and remove distractions. People like to fetishize long work hours and all-nighters but nothing is worth that. Get enough sleep, spend time with family and friends, and exercise. Don't get sucked into the hustle culture",8.0
g3ff7xj,ijgz6w,Thanks for the advice,2.0
g3er1ql,ijgz6w,"Hi a PhD student studying deep learning, I'm Dad👨",-1.0
g3dr12o,ijgz6w,"I think when you start liking a field and love learning more about it, you stop questioning the need for free time, you just want to know more about it.

I, personally, love to drown myself in DL and it’s applications. So I don’t really feel “tired” when I do it.",1.0
g3ep7eh,ijgz6w,"As i learn, i realize how much more i dont know. I have been working in the field for a year, avereging over 12h/day + weekends, but that is my personal choice. I have always been like that. I could work just 8h/day, but then i wouldnt be able to achieve what i want, which is to be at a world class level. At night i stop to watch movies with my partner or play some games. I try to have 8h of sleep every day (most important thing). If i feel that i am grtting burnt out, then i take the weekend off and reduce my hours. I do miss exercising, and have no clue about how to fit it in my schedule...maybe 2 or 3 times a week at night, but i go for some walks with my partner here and there.",1.0
g3ffhn4,ijgz6w,Thanks for the advice,1.0
g3d18mt,ij92yu,[https://docs.fast.ai/tutorial.collab](https://docs.fast.ai/tutorial.collab)   check this one,1.0
g3dzmwp,ij92yu,"Thanks a lot, will do..",2.0
g39wdsl,iixqt1,"anyone can publish anything, the problem is getting peer-reviewed and be accepted whenever you publish it to (conference, journal etc)",16.0
g39yfo6,iixqt1,Full-stop punctuation should be important.,9.0
g3bj1h9,iixqt1,"Yeah use grammarly folks, or your whole peer review would be full of statement corrections. With a reject",1.0
g3blqex,iixqt1,"Oh sorry for that , corrected it",1.0
g39y7mt,iixqt1,You can always publish on arXive,3.0
g3a9ivd,iixqt1,[deleted],3.0
g3af4d9,iixqt1,"Maybe he is from germany and means ""at the moment"" or ""currently"", the translation in german would be ""aktuell"" which is spelled like ""actually"". ""Actually"" and ""aktuell"" are false friends",3.0
g3atuio,iixqt1,"It's actually (hehe) the same in Spanish, where ""actual"" in Spanish means current, despite being identical in spelling to the English actual. I wonder what that's all about.",3.0
g3afpea,iixqt1,[deleted],0.0
g3agf0u,iixqt1,".Oh, lol .Didn't recognized that",1.0
g3blin1,iixqt1,Oh sorry for that. Looks like first I need to work on my English :(,1.0
g3bdy7m,iixqt1,"Hey there :) I think it's great you're interested in publishing.

IMO, defining the contribution is important to be a publishing material. If the application of an existing method to certain problem is novel, probably you can start looking for a conference to publish it :) don't forget to review the literature the most you can, so that you make sure it doesn't exist yet.

Best luck!",1.0
g3bulm6,iixqt1,"I've published many. It depends a lot on what you are proposing, if it is novel and how you present it.

What exactly is your idea? Can guide you more then.",1.0
g3ay1x0,iiwh6j,"You’ll want to play around with the padding to get the shapes to line up.
If you’re using pytorch, transposeconv2d’s forward call has an output_shape kwarg which figures out the padding for you.",2.0
g3dagk3,iiwh6j,"Okay thank you. I'm using Keras with Tensorflow, can i do the same?",1.0
g38pf50,iisxua,GitHub with code &amp; paper: [https://github.com/jacobkrantz/VLN-CE](https://github.com/jacobkrantz/VLN-CE),1.0
g38qytb,iis7vp,"&gt;hardly distinguishable from the original sound

By a deaf person? 

Is this just an excuse to promote your low quality youtube channel?",2.0
g38rn36,iis7vp,Agreed,1.0
g38y7b2,iis7vp,Is this a video on how to run Python on colab? Probably should change the title.,1.0
g37o1n7,iim9gm,"1. http://neuralnetworksanddeeplearning.com/ (for concepts and coding from scratch)
2. https://fast.ai/ (for directly diving in)

If you are going with 2nd, do follow exact suggestions from instructor and dive into code a lot more to understand more.
If you miss even a single step you may lose yourself and you'll feel like you didn't learn much

My suggestion is to start with the first one and then implement some projects of your choice.

Tech stack:
Miniconda or anaconda python distribution
Their environment management is really good and can work as an alternative to going for docker
It also provides way to easily install libraries which you may need.

Common ones needed if coding from scratch are:

1. numpy
2. matplotlib

If going for more advanced stuff you'll also need
 Pytorch (personal bias) or tensorflow (equally good and easy after 2.0)",3.0
g38hpeb,iim9gm,"[https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2\_u4xOEky0](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)  I started with this, It covers all the basic concepts. After this I suggest you try to Implement a neural network using just numpy to get a solid understanding. Once you're done you'll have an idea on where to move next, but before that, pick a Deep learning framework (I suggest tensorflow or pytorch) . For image related models, I recommend playing around with MNIST dataset and then move to more focused topics.

When I started the availability of resources was quite over whelming, so I suggest you pick one and start doing. Good luck!",2.0
g39nkqn,iim9gm," [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials) 

Hi, also a noob here. I find the official TensorFlow tutorials are good to play around with. You can download the notebooks or quickly open them in Google Colab.",2.0
g3cgpeb,iim9gm,"First start with something you know will work. Like Retianet on Coco for object detection. Then go find yourself an image. 

Only after that is the time to start messing around with strange data sets and models. [Here](https://github.com/dataloop-ai/ZazuML) I give an example of running a simple retinanet with a toy dataset I subsampled from Coco (Check out the getting started section).",1.0
g55w7ei,iim9gm,"Just wanted to say thank you everyone, after reviewing and trying a bunch of stuff started with [Fast.Ai](https://Fast.Ai) and love it! Would've started sooner with [Fast.Ai](https://Fast.Ai) but there home page is so humble and unassuming and somewhat boring that one doesn't realize there accomplishments. If it did mention how they approach teaching and  what some of their students have accomplished I would've caught it sooner. PS also looking  at   [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)",1.0
g37mjjt,iim9gm,"Books. Learn the math first - Calculus and linear algebra for the very least. Then, Bishop's ML book and then when you reach the appropriate chapter - pytorch.",1.0
g382n0w,iim9gm,"Ignore this guy and just do a quick Google search.

There are hundreds of free courses, sample notebooks, and videos out there.

Start with something either focusing on Pytorch or keras/tensor flow. Do. Tranfser learning example. Backpedal from there to design a network. Then scale it up, fail, and learn about some of the math.",2.0
g3652v5,iiay3d,"I don’t think you need SLI for that, since the GPUs run independently. Other than that, your build should work.",2.0
g36ov0l,iiay3d,"Thanks for your answer. 

My workflow mostly consists of render/ CAD, we're already using gpus that have cuda cores in them. I'm aware that newer cards with tensor cores are more suitable for deep learning applications. However deep learning will be a suuuper small part of our work, maybe 5% at most. I was checking out titan rtx, quadro rtx6000 and v100s cards but they are insanely expensive, ranging from 2500$ to upwards of 10000$ per card. My reasoning is that a colleague of mine has 3 K80 cards for sale around 1000$ total. I'm just trying to figure out if it's worth it or if it's technically compatible hardware and software wise. I figured this is the place to ask as deep learning people are the most knowledgeable about these gpus. 

What do you think? Would I benefit from 3 K80s in a pc for my application at this price point? Or should I invest in newer cards?",2.0
g36kz3q,iiay3d,"Don't forget that the K80 only has passive cooling - it expects a server case to provide the airflow to duct the heat away. So, you'd need a lot of very loud cooling fans in your case to stop it from overheating - or you'd need to duct tape a high speed fan straight onto it. Either way it's going to be hot, loud and untidy.",2.0
g36pmeg,iiay3d,"Thanks for your answer. 

I checked out some k80 builds on a desktop. I'll 3d print some fan mounting brackets and I'll mount high speed fans on it. I have some space in my case and sound is not a great concern. 

My workflow mostly consists of render/ CAD, we're already using gpus that have cuda cores in them. I'm aware that newer cards with tensor cores are more suitable for deep learning applications. However deep learning will be a suuuper small part of our work, maybe 5% at most. I was checking out titan rtx, quadro rtx6000 and v100s cards but they are insanely expensive, ranging from 2500$ to upwards of 10000$ per card. My reasoning is that a colleague of mine has 3 K80 cards for sale around 1000$ total. I'm just trying to figure out if it's worth it or if it's technically compatible hardware and software wise. I figured this is the place to ask as deep learning people are the most knowledgeable about these gpus. 

What do you think? Would I benefit from 3 K80s in a pc for my application at this price point? Or should I invest in newer cards?",1.0
g37u7gf,iiay3d,"You'll notice the K80 has no fans. That is **not** because it doesn't need them, it is because it is expecting high throughput server fans to be pushing air over it. So make sure you have some kind of equivalent setup in your desktop, lots of airflow (probably ghetto-mount a fan or two to force air through the GPU).

You don't need SLI, that's a different thing that has little relevance outside (or tbh even within these days) gaming. You're using your GPU as a hardware accelerator, not as a graphics device",2.0
g368q33,iiay3d,"K80 are not great consumer cards. Its an old power hungry card. What it has is a lot of Ram. What you need for desktop though is speed first so you need modern architecture to use mixed precision to its fullest.

At this stage I would wait for the 30XX to get out and jump in based on how much youre willing to spend.",1.0
g36oone,iiay3d,"Thanks for your answer. 

My workflow mostly consists of render/ CAD, we're already using gpus that have cuda cores in them. I'm aware that newer cards with tensor cores are more suitable for deep learning applications. However deep learning will be a suuuper small part of our work, maybe 5% at most. I was checking out titan rtx, quadro rtx6000 and v100s cards but they are insanely expensive, ranging from 2500$ to upwards of 10000$ per card. My reasoning is that a colleague of mine has 3 K80 cards for sale around 1000$ total. I'm just trying to figure out if it's worth it or if it's technically compatible hardware and software wise. I figured this is the place to ask as deep learning people are the most knowledgeable about these gpus. 

What do you think? Would I benefit from 3 K80s in a pc for my application at this price point? Or should I invest in newer cards?",1.0
g36bwbu,iiay3d,K80s are quite outdated. You can use a K80 for free on Google colab,1.0
g36opof,iiay3d,"Thanks for your answer. 

My workflow mostly consists of render/ CAD, we're already using gpus that have cuda cores in them. I'm aware that newer cards with tensor cores are more suitable for deep learning applications. However deep learning will be a suuuper small part of our work, maybe 5% at most. I was checking out titan rtx, quadro rtx6000 and v100s cards but they are insanely expensive, ranging from 2500$ to upwards of 10000$ per card. My reasoning is that a colleague of mine has 3 K80 cards for sale around 1000$ total. I'm just trying to figure out if it's worth it or if it's technically compatible hardware and software wise. I figured this is the place to ask as deep learning people are the most knowledgeable about these gpus. 

What do you think? Would I benefit from 3 K80s in a pc for my application at this price point? Or should I invest in newer cards?",1.0
g36q2ka,iiay3d,"You'll have to custom build the cooling but yes, they will work. You don't need SLI.

You should be aware that power costs will be high compared to newer cards. If you are doing very long renders it might cost you more in the long run, no matter how cheap you got the cards.",1.0
g365tpp,iiay3d,"Would recommend against getting the K80, they are insanely expensive for the value they provide. Wait for the RTX 30 series if possible.",1.0
g36o7am,iiay3d,"Thanks for your answer. 

My workflow mostly consists of render/ CAD, we're already using gpus that have cuda cores in them. I'm aware that newer cards with tensor cores are more suitable for deep learning applications. However deep learning will be a suuuper small part of our work, maybe 5% at most. I was checking out titan rtx, quadro rtx6000 and v100s cards but they are insanely expensive, ranging from 2500$ to upwards of 10000$ per card. My reasoning is that a colleague of mine has 3 K80 cards for sale around 1000$ total. I'm just trying to figure out if it's worth it or if it's technically compatible hardware and software wise. I figured this is the place to ask as deep learning people are the most knowledgeable about these gpus. 

What do you think? Would I benefit from 3 K80s in a pc for my application at this price point? Or should I invest in newer cards?",2.0
g37ng0e,iiay3d,"I'm not too familiar with the hardware required for CAD/rendering. CUDA cores isn't the only measure, there has been tremendous improvement in performance per core in recent years. I would definitely go for newer consumer cards for deep learning.  K80 is ancient at this point and datacentre GPU's are way more expensive than their consumer counterparts (only difference is Nvidia milks datacentres by prohibiting them from using consumer cards). 

But [CAD seems to require double precision compute](https://www.digitalengineering247.com/article/redefining-whats-possible-with-high-fidelity-gpus/design) and [K80 seems much better than other consumer cards](https://www.microway.com/knowledge-center-articles/comparison-of-nvidia-geforce-gpus-and-nvidia-tesla-gpus/). You should probably post this question to some CAD/rendering focussed subreddit.",2.0
g375d42,ii8rkp,"Wow, looks really detailed and informative. Thanks for sharing :)",1.0
g348aj0,ii37d1,I’ve done a lot of this sort of work - first step is to ask whether the data occurs in the same place on each input or if it moves around? It would help if you could post some redacted samples...,5.0
g34go8q,ii37d1," Hey I have updated the description for a sample. Also the data can move for each input. More importantly, I would want to use the same model for different types of docs like invoices, medical docs, ledgers, etc.   
Is it okay if I DM you?",1.0
g34o4na,ii37d1,"Gotcha, non-standardized formats are much trickier and can’t really be done reliably. 

You can take a look at graph convolutional models - this might help: 
https://github.com/naiveHobo/InvoiceNet

Again, this sort of problem can’t really be solved reliably so you’re going to want to human review each one and spin up an interface to easily add any fields that couldn’t be automatically extracted.",3.0
g34oc4f,ii37d1,Alrighty got it. Thanks a lot. Any other resources/research papers/ type of models that you would recommend on this matter?,1.0
g34c1iq,ii37d1,I guess you can use the RCNN and then further apply image recognition to extract the text.,1.0
g34gp0p,ii37d1,"I'll check that out, thank you.",1.0
g34kt23,ii37d1,That sample is very structured and i think you'll be able to implement it.,1.0
g34l6c0,ii37d1,Do you know of any research papers/ resources that you'd recommend looking into?,1.0
g34mt1f,ii37d1,If these are all reliably scanned documents or even better auto generated PDFs it should be quite easy to only crop out the parts you need and run them through some digit recognition API (this is only if you know for a fact that they will all have the same exact format),1.0
g34o0wc,ii37d1,"Yeah the problem is that they won't be of the same form, since I want to run the model on invoices and receipts and reports of various establishments",3.0
g350kf2,ii37d1,You could first extract the type of document and depending on this use the right model. If your dataset does not contain all possible forms you are doomed eitherway.,1.0
g350vg9,ii37d1,Yea true that,1.0
g34s348,ii37d1,Is one-for-all model a necessary condition? Why you can't to classify docs first? Divide and conquer...,1.0
g34sm2j,ii37d1,"Yeah I'll probably develop models for medical report, one for invoice and so on
But I want those each models to handle the format changes in different inputs. For example, the name field may be on left for some docs and on right for some, with different font and formatting. Any idea how to go about this?",1.0
g350i7h,ii37d1,"It maybe not optimal, but if I had to do that I would start with CNN for the docs classification. Then I would visualize layers activation and weights to find the layer that represents main text blocks (it can't be too deep). They should be invariant to transfer. Then I would try to find text blocks areas by more specific model for each doc type, cut it, end use OCR for each block. Or maybe it be better to look for blocks from the start.... But it can't work if you have monolith text...",1.0
g35144o,ii37d1,"Yeah gotcha, would this work for tabulated data?",1.0
g3543yv,ii37d1,It should be.,1.0
g34z426,ihyhyl,Dont you have to also increase the learning rate ? I would think that what they do is update with the average gradient across all copies right ? But therefore you have less updates in 50 epichs with 8 gpus than with 1 gpu.,2.0
g35axwr,ihyhyl,"Many thanks for comment.

What I know is that it is stated everywhere that we should multiply batch size (by 8 in my case) so that each gpu will get the same data size in each batch as the single gpu scenario. But the whole data is also divided and therefore the steps in each epoch that the parameters are updated is reduced (in my case by 8). This is reasonable, as the rational for distribution was to reduce the simulation time. If the steps are not going to reduce, distributed training is just useless. Am I right?

Then comes the learning rate issue and I think you are right. But I am not using SGD optimizer, for which you should tune the learning rate yourself. I am using Adadelta, which is an self-adapting learning rate optimizer. Also I keep the simulation running until I see no improvement in validation accuracy and loss. So, why should I see such a performance drop in my case?",1.0
g35dev5,ihyhyl,"Even if you use an adaptive optimizer the starting rate still matters. Try different orders of magnitude and youll get different results. People still use schedulers for all optimizers as an additional example of the LR mattering. But it may be another problem, my suggestion is for you to try increasing it. I'm really not an expert on distributed training but it makes sense to me that your optimization need to be different. On 8 GPU do you have a very large batch ? Maybe look into large batch training tricks and optimizers ?",2.0
g35ky0b,ihyhyl,"Thanks again. My batch size is 1024 (which is multiplied by 8 for 8 gpus) with the total data size of around 550K samples. I will definitely try playing with LR. Do you have any specific resource that you can recommend for "" large batch training tricks and optimizers ""?",1.0
g35scno,ihyhyl,"You may want to delve into litterature and medium posts about LARS and LAMB optimizers. I can't really help you beyond that though as I haven't really delved into it much. But if you look at the LARS paper abstract it is basically your problem description: large batches lower accuracy.

Read about it a bit and if you think its useful then look around for the tensorflow implementation/function.",2.0
g35woj5,ihyhyl,Thank you very much for your great help :) I found some stuff and I really appreciate.,1.0
g32gewn,ihtt5f,[deleted],4.0
g32gqna,ihtt5f,"Yeah, I am weak with math. I just now almost bought the course. Thanks for the quick response.",1.0
g32i5zm,ihtt5f,[deleted],2.0
g32mlia,ihtt5f,Alright  I'll take the math courses and Python before diving into deep learning. I have a curiosity of neuroscience and brain health but I do not want to be a neurosurgeon or scientist.,1.0
g33y7ic,ihtt5f,"If thats your reason of doing deep learning, i wouldnt recommend you taking it, deep learning has hardly anything to do with neuroscience.

And as i quote someone i saw on reddit earlier, “biologically inspired is the deep learning equivalent of according to quantum physics”",3.0
g34abc3,ihtt5f,"Yes you can. And you’ll start learning about it as you go ahead. It’s not difficult, but don’t miss out the learning opportunity.

If you don’t understand something, I urge you to google it or look for an explanation on YouTube for it.",1.0
g3380hr,ihmjy9,How is the memory profiling implemented? How deep down can you drill into the modules when profiling?,1.0
g30eiol,ihii3i,Thank you,1.0
g30gne5,ihii3i,Could you also add the tutorials mentioned or some working examples?,1.0
g30hy4f,ihii3i,"These are all working examples :)  


Feel free to raise an issue if something's not working!",1.0
g30i1m3,ihii3i,Sure. Thank you,1.0
g3230an,ihii3i,this is great! Exactly what was needed! Keep up the good work!,1.0
g2zkgfg,ihb8sv,"Just curious, why predict something that can be calculated? Wouldn’t it make more sense to work on reducing the speed of the benchmark formulae, since the accuracy for existing formulae is 1.00? You want to create a model that would replace a function?",3.0
g30nra3,ihb8sv,Yeah but I want to try doing this with neural networks. Do you know about any models which could help me in this?,2.0
g6zbyc5,ihb8sv,"It's an interview question for a top tier start up. I've been working on it for a while, and I've ran into some difficulty doing so tbh.",2.0
g2yyxj7,iha6pa,On the surface that seems like pretty small data. You sure you really need that much GPU horsepower?,1.0
g2z4djv,iha6pa,"each sample 1k-15k rows, but 1-2mio of those samples\^\^",-2.0
g2zblb3,iha6pa,I think you're overthinking this. I think you'd be fine with a 2080.,1.0
g31upzt,iha6pa,2080 with or without ti?,1.0
g326y6n,iha6pa,"I think you'd be fine without it. But don't 100% take my word for it, haha.",1.0
g32h9yr,iha6pa,[deleted],1.0
g32i2xj,iha6pa,"hmmm , thanks for your answer. I consider LSTM timesequences  of around 1.000-15.000 rows &amp; around 10-100 columns)  and 500k to 1mio samples as deep learning\^\^  or am I wrong?",1.0
g32i3tx,iha6pa,"You're not wrong, Walter, you're just an asshole.",2.0
g2z330s,iha6pa,"&gt; I need to train multiple keras LSTMs on datasets that look like this.

&gt; Float or double sequences (of around 1.000-15.000 rows &amp; around 10-100 columns)

You could do this quickly with very ordinary hardware, nothing fancy required.",1.0
g2z4c7e,iha6pa,"each sample 1k-15k rows, but 1-2mio of those samples\^\^",-1.0
g307dbd,iha6pa,In general it would be also pretty informative for someone to give a comparison answer (if possible),1.0
g32h3kv,iha6pa,[deleted],1.0
g32i5gc,iha6pa,u got me ;),1.0
g2yy1om,iha6pa,"you might want to play around with your data in Google Colab to see memory utilization. On face value, you could probably pass with a far lesser GPU (GTX 1070Ti?)

you state ""Float or double sequences (of around 1.000-15.000 rows &amp; around 10-100 columns)"" but how many total sequences?",0.0
g2z4e1w,iha6pa," each sample 1k-15k rows, but 1-2mio of those samples\^\^",-1.0
g32nqb1,iha6pa,What is a mio?,1.0
g3fczb0,iha6pa,"sry, i meant a million \^\^",1.0
g788vn4,iha6pa,"one dont say a million, we use a capital G",1.0
g7qa5r4,iha6pa,"is that an insider? (I probably dont get it, bc Im not a native english speaker)",1.0
g2xs8ce,ih1zru,Check the anchor box size parameter options in the tensorflow model config file,1.0
g2zp1ge,ih1zru,Thank you. Will do.,1.0
g2zlskh,ih1c2r,The number of filters for the 1×1 conv will be less than the no. of feature maps of previous layers. So the dimensionality in the height and width will be the same but no. of feature maps will decrease. This will reduce the overall dimensionality.,2.0
g2whtof,igwsel,"I don't know why, and I hope I am wrong but I feel like they're overselling...",40.0
g2wihra,igwsel,"As musk is always doing, yeah obviously any step in that direction would be impressive, but he somehow always oversells his products.",20.0
g2wjl9u,igwsel,"I oversell my products too, because I worked on them :D I think if you are really standing behind what you sell, it is just natural",3.0
g2x4pi4,igwsel,"Ah yes, the good old mantra, ""oversell, underdeliver.""",11.0
g2wkc5t,igwsel,"it's not just that... He oversells for a reason... because if he does, his stocks soar and investor money pours in

...that being said... I wouldn't be so sure that they actually have a breakthrough... maybe they need money",3.0
g2xdn91,igwsel,"Catch-up for people not in the know...

[Neuralink](https://en.wikipedia.org/wiki/Neuralink) is a neurotechnology company founded by Elon Musk in 2016, focusing on developing high resolution and bandwidth brain-machine interfaces (*neural laces*, a la [The Culture](https://en.wikipedia.org/wiki/The_Culture)), with the stated eventual goal of allowing humanity to function as peers to artificial intelligence (and mitigate the existential threat presented), improving neurological medicine and enabling transhumanist cognitive enhancement along the way.

[This](https://waitbutwhy.com/2017/04/neuralink.html) is a very long (but very worth it) Wait But Why article explaining what Neuralink is all about in very understandable (and humorous) language, from first principles all the way up.


[**An integrated brain-machine interface platform with thousands of channels.**](https://www.biorxiv.org/content/biorxiv/early/2019/07/17/703801.full.pdf)

&gt;Brain-machine interfaces (BMIs) hold promise for the restoration of sensory and motor function and
the treatment of neurological disorders, but clinical BMIs have not yet been widely adopted, in part
because modest channel counts have limited their potential. In this white paper, we describe Neuralink’s first steps toward a scalable high-bandwidth BMI system. We have built arrays of small and
flexible electrode “threads”, with as many as 3,072 electrodes per array distributed across 96 threads.
We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute.
Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small
implantable device that contains custom chips for low-power on-board amplification and digitization: the package for 3,072 channels occupies less than (23 × 18.5 × 2) mm3
. A single USB-C cable
provides full-bandwidth data streaming from the device, recording from all channels simultaneously.
This system has achieved a spiking yield of up to 85.5 % in chronically implanted electrodes. Neuralink’s approach to BMI has unprecedented packaging density and scalability in a clinically relevant
package.

**Implementation**

Basically a chip is surgically implanted into the scalp ( the N1 ) and there are threads ( electrodes ) coming out from the chip that go down into the brain. Wires to power the chip are embedded/burrowed in the scalp and go on to form a inductive loop under the skin behind the ear ( like the wireless charging coil inside a phone ). A wearable device is put behind the ear which transmits power to the coil wirelessly ( like a wireless charging pad ). That device contains the batteries and provides the power. Also contains the brains that receives the signals from the chip wirelessly.

[Diagram](https://i.imgur.com/L14Ykau.jpg)

[Wearable](https://i.imgur.com/54iwkMZ.jpg)

**Progress**

[Last year's update](https://youtu.be/r-vbh3t7WVI) they showed [thread inserting machine](https://i.imgur.com/KxRLEFa.jpg) , [threads](https://i.imgur.com/CvzGl4c.jpg) , [chip](https://i.imgur.com/LDqE09h.jpg) and [current design](https://i.imgur.com/wbr7tCB.jpg) like the [one they put in their lab rats](https://i.imgur.com/XZcPsMa.jpg) and other details that can be found in the video.",11.0
g2wh3ue,igwsel,"""The breakthrough could help achieve Musk's ambition of augmenting human intelligence and abilities, which he claims is necessary allow humanity to compete with advanced artificial intelligence"" 

this would be amazing. can i get the ability to draw flawlessly please?",2.0
g2yypbk,igwsel,Advanced ai.. the same ai which fails when I switch the lighting ?,3.0
g2wp8ks,igwsel,Yes me too I'd love to be able to draw.. so that I could make uhh.... drawings ( ͡° ͜ʖ ͡°),-2.0
g2xlxfn,igwsel,why is this in a deep learning sub?,1.0
g2y5xx4,igwsel,"With Nueralink, we'll be able to connect ourselves directly as the top or bottom layer of a deep neural network instead of tediously interacting via keyboard and screen /s",2.0
g2wkt6i,igwsel,"Brain computer interface, hum, does a mouse and keyboard count? My brain moves my arms which interface with the computer.

Also, don't those headbands that move the mouse pointer already exist?",1.0
g2xc8ea,igwsel,"It would be nice if the ""brain computer interface"" allowed you to know instantly the capital of Zanzibar and the text of the 14th Amendment, but I suspect this will be more along the lines of ""Siri, turn on the lights"". We'll see.",1.0
g2zbr3e,igwsel,There has been brain-computer interface in research for a while now. Any idea on what'll make this stand out?,1.0
g2x3yc3,igwsel,Yeah well no.,1.0
g2yvhqj,igwgm6,"This probably says more about me than the quality of the papers, but it feels like we are just making stuff up now by taking the same 50 terms and throwing them in a random order and all the PhDs are doing is running random search over a seed and overfitting some proprietary data set and almost none of these papers are repeatable or have applicability to anyone outside of their lab. 

“Combinatorial differential non-linear hyperbolic transformer activation for differential federated learning with reverses autoencoder at scale achieved 0.7% better performance on standard GPT dataset with deep learning.”

But maybe I’m dumb.",1.0
g30cwu9,igwgm6,"So I don't think this paper is of that type, but I do feel like this is the case for a lot of papers in ML. It's the unfortunate nature of what the field has become and this definitely needs to be addressed.",2.0
g2vqqdy,igpofo,"the thesis is supposed to be the focus on one topic, that’s simple just start doing it, instead of asking random dudes on the internet.",3.0
g2vcwss,igpofo,Have you considered anything along the lines of dealing with dataset shifts?,1.0
g2vd38g,igpofo,What is dataset shift?,1.0
g2x754v,igog6m,Can you elaborate on your question?,1.0
g2zcr2x,igog6m,Why need beta and gamma? Is it just for the case that mu and sigma are zero(so it can offset extra epsilon)?,1.0
g2v6ef1,igobay,They're very useful for things like guided backprop,2.0
g2v9zaw,igobay,"Thanks for mentioning this use case. I probably should have added more examples of use cases to the video.

If anyone has any other good use case for hooks, I'd be interested to hear them.

Another use case example is that I think PyTorch's internal quantization features use the forward hooks on modules to examine the range of the values that get passed into and out of the modules so that it can determine the best quantization parameters for when it needs to convert between the higher precision floating-point numbers and the lower precision integer representations of the values.",2.0
g2t4iq3,ige3cd,"Looks amazing!
How did you make that red/blue filter? I'm used to the box object detection. This seems quite impressive..",2.0
g2t7f32,ige3cd,Semantic segmentation may be.,2.0
g2t8f1f,ige3cd,Mask-RCNN,2.0
g2wjfjd,ige3cd,I've used YOLACT instance segmentation. Frankly I got no clue how they did it.,1.0
g2t2ln5,ig9ji8,All he does is have a dry cough?,2.0
g2sif20,ig8dim,"It's not exactly what you're looking for, but maybe it's close enough: 

* http://vision.deis.unibo.it/~smatt/Papers/IROS2018/iros-2018-monodepth.pdf
* https://arxiv.org/pdf/1910.05547v1.pdf",2.0
g2rv71l,ig5byu,"I would say that it depends a lot on your application. For instance, in your computer vision example, if you’re planning to have the model run on something like a vehicle, chances are that you won’t have enough memory or compute to run 50 different models. On the other hand, if you want to identify the global effect of a specific feature on your target variable, you’d go for one big model. Another parameter would be your training set: if you have sufficient data for all different “cohorts”, then it’s possible that multiple smaller models will give you better predictive performance.

TL;DR: hard to tell in a vacuum, depends on application and any constraints you have",4.0
g2s0cg0,ig5byu,"Okay, that's a good point. Let us assume we have no memory constraints.

Would the model be better or worse wrt. attempting to learn a more wide rather than narrow distribution? Loss-wise

My intuition says,  the support on a wide distribution would likely be focused on the areas of large density, whilst this effect would be less pronounced in a narrow distribution that may attribute more weight towards lower density areas. (assuming some mixture of gaussians for instance)",1.0
g2thfwn,ig5byu,"Intuitively, I think I'm with you. I guess the differentiating factor is whether there is something ""inherently different"" in the distributions of the data that you will pass to the different model. If that's the case, then two models learning two different distributions will be an easier learning task than one model trying to learn both. If not, then you're just reducing the amount of data that each model sees. Another thing is noise: a single model seeing all the data will smooth over the noise (avoid overfitting) better than partial models. And then, finally, you can take it to the extreme and just go down the bagging route, where you have a lot of smaller models, but aggregate their predictions together",1.0
g2tonfy,ig5byu,"Hmm yes, very good point. I wonder if bagging DL models is efficient - thanks for your reply!",2.0
g2qpnkp,ifz1qo,Nope,7.0
g2s3jyy,ifz1qo,"i don't know of any programming domain where swift is relevant other than software for apple products. it might be a nice language, but apple isn't known for open source sofware. they even created their own graphics api [https://developer.apple.com/metal/](https://developer.apple.com/metal/) instead of contributing to open standards.",2.0
g2s66ed,ifz1qo,"Swift is really cool, but unfortunately it was never really a cross platform language. Instead its entire ecosystem is built around the apple proprietary walled garden so naturally it will never be relevant to anyone outside the apple asylum. And there are not enough skilled people and companies willing to vendor lock themselves like that. If it were more open and cross platform from the beginning it could have been huge, but it wasn't, so its irrelevant.

Which is a bit sad because its such a good language and we really need a replacement for Python but Swift will not be it.

There is still [Swift for tensorflow](https://github.com/tensorflow/swift) btw, but its development is almost non existent and is largely based around an immature interop layer with python. Since there is no cross platform support for swift they had to built an entire standard library from scratch, gave up and just relied on python to do most things. Plus of course you then have to tie into the whole Tensorflow bloat nonsense, something a large percentage of people are actually moving away from. So very nasty double vendor lock (google and apple). I suppose we can actually be happy it has been so unpopular.",2.0
g2skx59,ifz1qo,Thanks. This is a pretty good explanation! I guess I'll still learn Swift for other purposes but it seems no other language is on the horizon yet?,1.0
g2td5yc,ifz1qo,"Swift is potentially a really good programming language for Deep Learning applications. If you want a detailed explanation on why this is the case, read this article from Google: [https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md](https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md).

The only problem, I think, is that it's not broadly used and supported, which makes it less appealing, at least today, compared to other programming languages like Python.",1.0
g2tho56,ifwfs4,"Check out fast.ai, this guy covers almost every thing you need to know to get started. Deep lizard is also good.",2.0
g2thpal,ifwfs4,"**I found links in your comment that were not hyperlinked:**

* [fast.ai](https://fast.ai)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g2tho56&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",2.0
g2qovjj,ifvmtv,"Write a quick python script: 

First Manually find background frame (might do this per hour if lighting changes)

Then loop over all frames and subtract current frame from background - if total difference is small then delete the frame. 

You might want to blur and downsize the frames first. 

This kind of simple motion detection has worked well for me for this sort of situation.",1.0
g2qposf,ifun56,"Wow, great channel. Def worth a sub",1.0
g2pktd0,ifsys3,You are going to learn how to code before you do any of that 😁.,4.0
g2pkzar,ifsys3,"Of course, I have studied C,Java, Html,xml,sql and css",1.0
g2pmdw4,ifsys3,"In the simplest terms, DL is ML which extracts it’s own features. To accomplish this, there are things you need to do beyond traditional ML, for example having even more examples to learn from.",3.0
g2plzhf,ifsys3,"I would say that learning ""traditional"" ML (which includes shallow neural networks) is preferred before learning DL. It will give you a broader understanding of important concepts and about the different families of ML algorithms (read ""The Master Algorithm"" by Pedro Domingos).

Also it could be a good idea to choose an application field you're interested in, such as Computer Vision, Natural Language Processing,  bioinformatics, etc.

Enjoy your learning :)",2.0
g2pmgab,ifsys3,"A book recommendation is worth a thousand words. Thanks!

So, should I pick NLP before or after learning traditional ML?",1.0
g2pq72l,ifsys3,"
Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
by Aurélien Géron



https://www.amazon.com/dp/1492032646/ref=cm_sw_r_sms_apa_i_89.qFb07HZWKN",4.0
g2pz3ov,ifsys3,"You're welcome, it's a great book, ane gives you a good perspective ot what ML is, and its 5 different families.

Yeah, why not, if that's what interests you the most :) the important thing is that you like it.

I am actually in computer vision, so I'm not very familiar with NLP, however, I know that the area has been studied from other perspectives different to ML (the same happens with vision), so probably you'd like to have a grasp of that knowledge and then dive into ML + NLP, finally followed by DL + NLP.",2.0
g2q7rfd,ifsys3,"Again I'd say you could go straight to nlp with deep learning. What's happened very recently in the field of NLP is that all of the hand crafted solutions that were the best we had in the past, have been completely destroyed by newer architectures. Architectures which share little with older approaches. 

In fact these new architectures are so new, they are not yet even very complex yet (although they are rapidly complexifying so its a good time to get in). At least not when compared to the volumes of research involved with older approaches. Specifically I'm talking about Transformer models which use attention. The paper 'attention is all you need' essentially killed all older and more complex techniques. 

It's very much my unpopular opinion that clinging on to these older methods is akin to continuing to visit the library because you like the 'real paper feel'",1.0
g2teixa,ifsys3,"First of all, you need a very good understanding of Python, because the best DL libraries are based on it. Then, acquired that, there's a mathematical foundation that's mandatory if you want a deep understanding of the field; fortunately, you can easily find a lot of good resources, like tutorials, blogposts and even books ([https://mml-book.github.io/book/mml-book.pdf](https://mml-book.github.io/book/mml-book.pdf)) on the topic. So... good luck and have fun!!!",1.0
g2q715y,ifsys3,"While what people say about learning good underlying principles from general ML is true, it's certainly not true that you need to understand old ML algorithms to use deep learning.

Given sufficient data and compute, deep learning makes traditional ML redundant in most cases (unless model interpretability is important).

If your interests in deep learning are purely practical and not academic, you honestly barely even need to understand how neural networks work to use them. In many ways they are mathemagical. That being said understanding the underlying theory may help you make better design decisions while making deep networks. Although even expert designers use a lot of guess work in these things. Designing deep Neural Networks is more of an art than a science. 

In summary, you can go straight to deep learning, however if you are wanting to get hired based off of this skill, you probably won't, since it honestly takes minimal skill and effort to follow some deep learning tutorials and get incredible results in image recognition or language processing. However if you're looking to incorporate it into projects, go straight in, and learn it top down would be my advice. 

One more thing one of my lectures told us:
'the path to becoming a data scientist is to call yourself a data scientist'",1.0
g2qgl07,ifsc1u,Tried to help but your image is not showing up or show up as broken. Maybe check the survey link?,1.0
g2qk4ym,ifsc1u,"Sometimes it shows as broken, but page or image refresh will fix it. Anyways, thanks for feedback :)",1.0
g2p3nc9,ifjxww,"There's a lot of ways to do this.  I standard approach would be to create a Flask app which loads the model and make requests to the app for inference.  A good way of doing this is to create the application as a Docker image and host it someplace like AWS or Heroku.

Here's a [small repo a put together](https://github.com/nathanmargaglio/Deployable-Model) a little while ago that demonstrates this.  The approach I describe is far from optimal, but it may be a good, quick place to start.  If you need something more production-grade, you'll need to do a lot more work to setup the infrastructure, but the basic idea is the same.",1.0
g2tjboc,ifjqm9,"Wow, this is really cool..",1.0
g2owmeu,ifjbe4,"Correct me if I'm wrong, but journals don't have submission deadlines, conferences do. If you want to publish in a journal, just pick your favorite and submit whenever you're ready.",1.0
g2p7k8v,ifjbe4,And what about conferences?,1.0
g2q7k7k,ifjbe4,"Depends on what exactly you want to publish.. for machine learning / deep learning you can have a look at https://aideadlin.es/?sub=ML,CV,NLP,RO,SP,DM and http://www.guide2research.com/topconf/machine-learning. If you want to publish in your domain / field of research (like life sciences, computer vision, automotive, and so on) you can specifically look for popular conferences in that field. It's really hard to say which conference is suitable, it absolutely depends on your paper.",2.0
g2nzanf,ifixi8,"1. Says on website that it includes all content of revamped course + the part 2... so a lot more content. 

2. Depends on your preference based on your learning style, but either methods should suffice given your commitment. 

Personally, I’m going with the book :)",1.0
g2nhh3d,ifgidf,"Here are the links to the code and the interactive 3D visuals shown in the video.

GitHub code for visualizing how the logit values that are passed into the softmax function change over time as the model is trained with SGD (stochastic gradient descent) or the Adam optimizer:

[https://github.com/elliotwaite/softmax-logit-paths](https://github.com/elliotwaite/softmax-logit-paths)

&amp;#x200B;

Desmos 2D graph of softmax for 4 classes:

[https://www.desmos.com/calculator/drqqhtb037](https://www.desmos.com/calculator/drqqhtb037)

&amp;#x200B;

Geogebra 3D graph of softmax for 2 classes (with derivatives and Gaussians):

[https://www.geogebra.org/classic/qhdd4afw](https://www.geogebra.org/classic/qhdd4afw)

&amp;#x200B;

Geogebra 3D graph of softmax for 3 classes (with derivatives):

[https://www.geogebra.org/classic/ps9gwjav](https://www.geogebra.org/classic/ps9gwjav)

&amp;#x200B;

Geogebra 3D graph of softmax with Gaussians for 3 classes:

[https://www.geogebra.org/classic/vgwaw7wr](https://www.geogebra.org/classic/vgwaw7wr)

&amp;#x200B;

Geogebra 3D graph of the shape of the softmax input space for 4 and 5 classes:

[https://www.geogebra.org/classic/emjn7pmq](https://www.geogebra.org/classic/emjn7pmq)",2.0
g2nk45g,ifgidf,Great work!  Loved the visuals. I was able to immediately grasp the softmax method of determining probabilities. I got lost a bit as the video progressed but was still interesting.,2.0
g2nmqoq,ifgidf,Nice! Glad you got something out of it. And thanks for the feedback. I might have to take some more time in future videos to explain the concepts in more detail so they are easier to follow.,1.0
g2nqrz1,ifgidf,Excellent demonstration. I love intuitive visuals for such fundamental concepts,2.0
g2nrmsa,ifgidf,"Thanks, u/DanielagainDaniel! :)",3.0
g2nzgsd,ifgidf,"Very cool, well done Elliot!",2.0
g2nzjqa,ifgidf,"Thanks, u/alxcnwy!",1.0
g2ntgz7,ifggka,"If you’re using python, you can use the pdf2image library. But the library is just using poppler under the hood so you’ll need to install that too.


https://stackoverflow.com/questions/46184239/extract-a-page-from-a-pdf-as-a-jpeg",2.0
g2nazm9,ifb2su,"Is it always on a flat surface? You could just detect keypoints in the original image and compute homographies between that and each following frames. Then, just apply these transformations to the edited image to reconstruct the original video with the new start image.",1.0
g2nf3pl,ifb2su,"It is not always a flat surface - the drawing can be done on a paper slightly wrapped around a pole for example. 
I am not familiar with the techniques you mentioned - will look into it!",1.0
g2mkx8i,if9ing,Link: https://github.com/dataloop-ai/ZazuML,1.0
g2m1w7h,if7ko5,"Hi :) I'm glad you're interested in learning this amazing field.

Just in case you haven't done it yet, I would recommend to first start reading and learning about traditional computer vision; grayscale images, histograms, filters, color analysis, fourier analysis, and so on. The ""Digital Image Processing"" by Gonzalez is a gold standard in image processing/CV. If you're already familiar with Machine Learning maths (linear algebra, calculus, statistics/probability), then learning these topics won't be that hard. I would be a good idea to also code some of the algorithms in the book (MATLAB or OpenCV are good options to manipulate image data).

The reason why learning all of these, often called ""old school computer vision"", is to provide you for a broader understanding of what Deep Learning algorithms might be doing behind the scenes. It will be also easier for you to come up with new ideas, and incorporate those concepts with your DL models.

After that, you can start learning more in depth about Neural Networks. Although you're not required to do it everytime, programming one from scratch once is a great experience. After that you can choose the DL framework that better suits you (TensorFlow, Pytorch, etc.) and experiment from there. When you get familiar with popular DL models, such as AlexNet, VGG, ResNet, DenseNet, you should start reading some papers to have a grasp of whatever improvement has been donde on Convolutional Neural Networks/Recurrent Neural Networks/Generative Adversarial Networks/Autoencoders. 

An important thing, Computer Vision has tremendously grown in the last decades. The huge amouny of knowledge gathered so fat often looks intimidating. Trying to be a specialist in video, medical imaging and 3D vision at the same time (to name a few subfields) might be impossible and is really not practical. After reading the basics, you can choose your favorite area(s).

Also, remember that Computer Vision can be approached by other paradigms as well :) probabilistic, evolutionary, physics-based methods, and so on. It's fascinating overall.

Enjoy your learning! 

PD: learning Machine Learning basics before Deep Learning is always a good practice.",2.0
g2m1pr1,if7ko5,"1. Find a computer vision problem which interests you
2. Start solving it
3. See what knowledge you’re missing and learn",1.0
g2lqtlu,if6spr,"Multivariable calculus, linear algebra, probability and statistics. You'll also need python, numpy, pandas, matplotlib.",13.0
g2lr5an,if6spr,"Computer science: programming, algorithms, data structures... (the more complete your foundation in comp sci, the better) 

Math I'm not sure what your background in Math is but if we're talking about college-level math then calculus, linear algebra, statistical models and probabilities. Algebra, geometry, trig those are pre-requisite to the pre-requisite, for example you may not directly apply trig in ML but it is an elementary concept to know.",7.0
g2lznlc,if6spr,"Math. Lots of calc, linear algebra, stats/probability.",4.0
g2lyh4y,if6spr,"Hi there! 
First of all congratulations on taking your first step towards machine learning! 
Moving onto your question, actually all you need is determination and will to learn ML. You can learn all other technicalities as you grow. In the current time, ML is a never ending field and you might get to a stage where you have some knowledge of machine learning yet you still have to learn about lot of algorithms. This is because the ML field is advancing rapidly. New research papers come out everyday and its overwhelming to see a lot of stuff, but it can also make you feel worthless. This was my personal experience when I started studying ML on my own. It will get difficult. You might also get irritated. You will find people who know much more about ML than you. But the key is not to stop. You will get to your goal eventually. 

Sorry if this was not you were looking for.
If you want to know the technical prerequisites like maths and programming, there are tons of videos and blog posts out there. 
Here is my blog post about how to get started with ML:
[Blog post](https://link.medium.com/yVvRpSdVb9)
Hope this helps you.
Feel free to ask anything!

Happy Learning!!",3.0
g2m8iku,if6spr,"statistics. I can't stress that enough. its one of the most recurring theme in every serious work. Apart from that, linear algebra and some bit of knowledge about calculus helps.",2.0
g2mcurx,if6spr,"Hi, I was in a exact same situation 3 months back, I also surfed a lot about the pre requisites for starting ML and honestly I got confused after looking at a list of them. Finally I started ML (without covering any of the pre-requisites required, I just had the knowledge of Python, Algebra, Matrices and Statistics before hand). Today I am familiar with all the prerequisites which I was 'supposed' to complete before starting ML. Just start learning and whenever you face a new term, Google it and you are good to go. 
I have just 3 months of experience in ML so don't decide only upon this post. 
All the best!",2.0
g2nva9b,if6spr,"Learn Python for 6 months and then start watching fastai. The instructor always explains the very simple math theory you should have learned in high school. After that course you have solid understanding and can continue to other courses and start doing something.

If you try to learn something like calculus or algebra, it’s going to become boring really soon. Coding is maybe the only prerequisite I would recommend. Then just figure out other stuff as you learn or build.",0.0
g2mmtrv,if6kg6,"Hi, I would recommend deep learning specialization by [deeplearning.ai](http://deeplearning.ai). Along with it, do the tensorflow in practice specialization, once you have reached course 4 of deep learning specialization.   Before that you need to know some basic concepts of Machine learning like training set and test set, bias and variance. For that, you refer to a dre ng's machine learning course on YouTube. You don't have to go through everything in ML course, you just need to know the basic ML pipeline.",1.0
g2mmun9,if6kg6,"**I found links in your comment that were not hyperlinked:**

* [deeplearning.ai](https://deeplearning.ai)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g2mmtrv&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",2.0
g2lebpy,if484s,The background can help detect the same object in different sizes as it gives some sort of spatial perspective. To a certain extent background does help make the model invariant to scale and improves its performance.,1.0
g2lg4og,if484s,If the distance to the camera is at the same length in every picture ?,1.0
g2lgx91,if484s,I read some about it here: [https://www.quora.com/Is-there-an-advantage-to-subtracting-the-background-of-images-while-training-a-convolutional-neural-net-such-as-mobilenet-so-that-it-gets-more-robust-at-picking-up-the-objects-and-essentially-becomes-background-agnostic](https://www.quora.com/Is-there-an-advantage-to-subtracting-the-background-of-images-while-training-a-convolutional-neural-net-such-as-mobilenet-so-that-it-gets-more-robust-at-picking-up-the-objects-and-essentially-becomes-background-agnostic),1.0
g2lojyc,if484s,"Yes this does seem like a nice way to put it. If your segmentation task does not require identifying the plant in the first place, background isn't necessary. The segmentation classes will now include only the relevant parts of the plant.",1.0
g2j0223,ieqlwr,"I’m a bit confused. But what you should have are masks with values that correspond to a certain class. For example, mask pixel value ‘0’ = background, pixel value ‘1’ = muscle, pixel value ‘2’ for some other class, and so on until you Nahverkehr a number representing each class you want to classify. Does this answer your question or did I misunderstand what you asked?",1.0
g2j1ges,ieqlwr,"Yes! This is what I’m referring to, but I’m asking how to perform the process of this form of labelling, do I just hard code these values? And if I do will it not mess up the rgb values of the pixels? Also, once these pixels are set, how should I arrange my input/output data. In the past my input array was just all of the ct scans in array (# of img , 512, 256, 1) and my output array had 9 seperate channels for each muscle group, so (# of img , 512, 256, 9), this method of organization however did not work out for us. Do you have any links to tutorials regarding the class labelling your referring too ? Or any suggestions on what to google? Thanks again !",1.0
g2j2zs6,ieqlwr,"I don’t have any links that really describe this, but I can try to describe it myself. 
So I used deeplab but it should be essentially the same or very similar input formats used. You feed in individual images and a corresponding (but separate file) mask image. The mask image will have each pixel labeled as I described in the first comment, and you pair it with the image which it is fitted to. This is during the training stage only. During the inference/prediction stage you just feed in images without the mask file. The output of the network should be a mask image for each image you feed in. So if you give the network 1 image to perform a prediction it will return a single mask image where each pixel will have a value of the class the network thinks that specific pixel belongs to.",1.0
g2jauuf,ieqlwr,"Ah that makes a lot of sense , I will try that , thanks for the help !",1.0
g2ihyc6,ieofwo,[deleted],3.0
g2jerja,ieofwo,It is only good for newbies.,11.0
g2j4tz2,iennxt,interesting work but in my opinion if in the image training were introduces truck image it would were more like the movies,1.0
g2i0yxg,iemk1c,"You would need to find very precise definitions of “analyze the data” and “come up with a solution.” I’m no DL expert, or any kind of expert, but it doesn’t sound like a good beginner project regardless.",1.0
g2i04yy,iemg5y,"DL is just ML with &gt;1 hidden layer, no need to overhype it.",2.0
g2iojzn,iemg5y,"Thanks for replying, I'm really confused on how shall I begin with DL since I have covered all the basics and algorithms of ML, but almost 50% of sources say first master ML, take projects and then start DL whereas the other quote ML basics are enough. I'd love to have your insights on this.",1.0
g2l3k59,iemg5y,I don't understand what you're asking for. Does it matter if you start from theory or projects? Why not just follow through with a course and start with what it tells you to start with?,1.0
g2mdtnc,iemg5y,"Thank you for replying, I've taken up 2 courses on ML but what I'd like to know is shall I like 'start' learning DL or keep on strengthening my ML?",1.0
g2mp4b0,iemg5y,"I guess it depends on whether you want to understand the basics (you'll find lots of stuff in deep learning that also exist in other ML fields) or if you want to be able to quickly make your own deep learning model. There is no right and wrong answer, just how much time you'd like to spend on it.",1.0
g2ohy2f,iemg5y,Alright! 👍🏼,1.0
g2ja3k6,iemetv,"Suggest the [DeepLearning.AI](https://DeepLearning.AI) specialization courses on Coursera. Very clear, you get exposed to a fair amount in an organized fashion and you walk through it in a good way.  I had read/watched/done a bunch on my own prior, but this set of 5 courses tied a nice bow around it and made me feel ready for more advanced topics.",2.0
g2kmb8t,iemetv,you should take a look at jeremy howard's [fast.ai](https://fast.ai) program. it is probably one of the best approaches available to learn about deep learning. the new version of the course was released last friday and the companion book is available on amazon.,2.0
g2i7rht,iemetv,"Work through Goodfellow's book ""Deep Learning"", it's a great textbook that covers all the basic topics. After that you will be well-equipped to read the newest papers as they come out if you want to be on the cutting edge.",1.0
g2gy9xz,iej5pb,"Code: [https://github.com/mks0601/I2L-MeshNet\_RELEASE](https://github.com/mks0601/I2L-MeshNet_RELEASE)

Paper: [https://www.catalyzex.com/paper/arxiv:2008.03713?fbclid=IwAR1pQGBhIwO4gW4mVZm1UEtyPLyZInsLZMyq3EoANaWxGO0CZ00Sj3ViM7I](https://www.catalyzex.com/paper/arxiv:2008.03713?fbclid=IwAR1pQGBhIwO4gW4mVZm1UEtyPLyZInsLZMyq3EoANaWxGO0CZ00Sj3ViM7I)",1.0
g2i01uv,ieigpj,"You can try padding shorter length audio files to match max duration file, or maybe take a specific length windows of the features and then average the results (or whatever depending on your target).",2.0
g2i1o2d,ieigpj,"Yeah I did the second ,took a specific length window of the frame and then calculated mel spectogram.It's returning a fixed size 2D array now.thanks for the advice.",1.0
g2havmt,ieg2je,"If you had completed Ian Goodfellow's book then Do this:
Currently DL field is going on so much fast that you can't cope up with everything. So first Decide which topic you want to further study like CV, NLP or RL. There are many subtask  so Pick One and then Go to Website PapersWithCode https://paperswithcode.com/. Find paper with more star and Read and Try to Implement it. They also provide code for 99% Papers. Every is coming up with new tricks so you need to understand what equation are saying for practical purpose. 
Sorry for My bad English.",9.0
g2hffif,ieg2je,This excellent advice. Paperwirhcode is one great resource for papers.,2.0
g2hkvlh,ieg2je,Yes you are right paperwithcode is all time best. Thanks for suggesting : ),1.0
g2ggpne,ieg2je,"I can't recommend direct resources, but, in general, when you start looking into advanced concepts I would recommend first doing a youtube search on that keyword and filtering by playlists. 

You will either find seminars, or university lectures related to those topics. Then you can try to look through standalone videos.

I just did a quick search on meta learning and representation learning, and both seem to have some lectures related to them on youtube from Stanford and Berkeley. 

Other than that, check out some universities which conduct research in those areas and look at the courses they offer in those areas. Often the course will have details on associated textbooks that could be referred to, if at all. That might be a good starting point too.",4.0
g2hk5tg,ieg2je,"Thanks for suggesting that YouTube one.But any place where some one has mentioned that deeplearning has these concepts 
Multitask learning
Multimodel learning
Representation learning
Federated learning
Adveserial learning 
Meta learning
Etc
Etc

Thanks",1.0
g2h86pi,ieg2je,!remindme 2 days,1.0
g2hbgh2,ieg2je,"There is a 18.0 minute delay fetching comments.

I will be messaging you in 2 days on [**2020-08-24 15:07:23 UTC**](http://www.wolframalpha.com/input/?i=2020-08-24%2015:07:23%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/deeplearning/comments/ieg2je/resource_recommendations_for_advanced/g2h86pi/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdeeplearning%2Fcomments%2Fieg2je%2Fresource_recommendations_for_advanced%2Fg2h86pi%2F%5D%0A%0ARemindMe%21%202020-08-24%2015%3A07%3A23%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20ieg2je)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
g2eswwx,iebjst,"Training will last for about 45mins after you close browser. 1.5 hours if you have pro. These times can change in a whim, they’re not set in stone. Depends on how google is feeling/demand at the time, etc

Unsure about your save code. I don’t use torch. Double check your save path. Not sure why you have strings prefixed with ‘f’. Is that needed. I don’t know. Also you definitely need gdrive mounted b4 saving


If you get pro training will last about twice as long. GPU speed has been the same on pro and not pro for me. Only because demand for best GPUs wasn’t high though. You may have different experience",1.0
g2evoth,iebjst,Hi. you can try to mount from google colab to your google drive and save your model,1.0
g2eyxol,iebjst,Hello. THank you. I already mount my googledrive and i can acquire dataset just fine. Saving the output is whats giving me problems. :(,1.0
g2i9cls,iebjst,"For saving models in Google colab [saving model](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_load.ipynb)
Saving with help of keras [saving with keras](https://www.tensorflow.org/guide/keras/save_and_serialize)",1.0
g2exqxf,ie9g8m,There are some suggestions about preventing overfitting. I think normalizing your input data and using regularization can help.,1.0
g2cupvx,ie39vc,"Hi all,

Following the amazing turn in of redditors for previous lectures, we are organizing another free zoom lecture for the reddit community.

In this next lecture Dr. Marc Eder will talk about his research - ***Maximizing Computer Visions's Field.*** 

This talk will introduce the emerging field of 360° computer vision, and provide an overview of the spherical distortion problem, highlighting how this distortion affects many of the highest profile problems in computer vision, from deep learning to structure-from-motion and SLAM. It will survey some of the existing work on the topic, and identify 3 guiding principles that drive a general solution to the problem. Finally, we will conclude with some opportunities for further research and some big picture takeaways from work thus far.

The talk it is based on his CVPR 2020 paper 'Tangent Images for Mitigating Spherical Distortion' 

git: [https://github.com/meder411/Tangent-Images](https://github.com/meder411/Tangent-Images)

&amp;#x200B;

**Lecture abstract:**

The advances in computer vision over the past decade are astounding when you compare to decades prior. If there is one shortcoming to the current engine of progress, it is that its field of view is still largely limited, in the most literal sense. Most vision algorithms are designed with undistorted, central-perspective images in mind. While this constraint is reflective of the prevalence of these types of cameras in circulation, this narrow field of view restricts progress to merely 30° - 60° crops of the world. Yet, not everything can be experienced, augmented, or understood from these small glimpses. This partial view cannot transport someone to another place, nor can it guarantee the context required to augment a scene or assist with a desired task. These applications require capturing a scene in its entirety: in all directions at once. With the advent and growth of commodity 360° cameras, it is now easy to obtain this type of imagery. However, these 360° images suffer from spherical distortion that is mathematically impossible to remove, and which has a powerful, deleterious effect on many algorithms' performance. As a result, it is imperative that we identify ways to reduce the impact of this distortion so that we may expand computer vision's field of view to the full 360°.

&amp;#x200B;

**Presenter Bio:**

Marc Eder recently completed his PhD in computer science at the University of North Carolina at Chapel Hill, where he was advised by Dr-Ing. Jan-Michael Frahm. Marc's research has primarily focused on computer vision problems involving 360° imaging. In particular, he has endeavored to identify new and improved representations for 360° images that can facilitate the seamless application of traditional central-perspective image algorithms. He has also employed this line of work for popular applications such as 3D indoor modeling. Recently, he co-organized the OmniCV Workshop at CVPR 2020, which brought together top computer vision researchers and engineers to discuss their work with omnidirectional images. Marc serves as a reviewer for CVPR, ICCV, and ECCV, among others, and most recently was acknowledged as a top reviewer for ECCV 2020. Before his doctorate in computer vision, Marc received a MS in electrical engineering at Boston University and a BA in history and Islamic &amp; Middle Eastern Studies from Brandeis University. This fall, Marc will be joining Yembo, a San Diego-based startup leveraging computer vision to transform the home-services industry. More information about Marc can be found at www.marceder.com.

This is a technical talk, prior knowledge of deep learning is advised.

&amp;#x200B;

**Link to event (September 1st):**  
[https://www.reddit.com/r/2D3DAI/comments/ia66so/maximizing\_computer\_visionss\_field\_of\_view\_dr](https://www.reddit.com/r/2D3DAI/comments/ia66so/maximizing_computer_visionss_field_of_view_dr)

(You can see other lecture we did in our sub-reddit /r/2D3DAI)",1.0
g2ccavm,ie0aq4,"It's impossible to answer without details.

The general answer is ""yes, you can"". But it depends on the complexity of the model and how long are you willing to wait for it to train.

Also, the distributability across the different cluster nodes will depend on the model architecture you choose.",3.0
g2cjmai,ie0aq4,"You're in Deep Learning.  (More than 2 hidden layers, many parameters)

Most likely not. The Pi has computing power similar to late 90s early 20s. Deep Learning became feasible with GPUs, not before.

Nvidia Jetson or Raspberry Pi with Coral or Compute sticks, yes, you can. Still small stuff though.",2.0
g2cndvy,ie0aq4,"You might run into some compatibly issues as DL libraries aren't compatible with ARM architectures out of the box . But since it's just for fun... have fun!

&gt;Python wheels for TensorFlow are [officially supported](https://medium.com/tensorflow/tensorflow-1-9-officially-supports-the-raspberry-pi-b91669b0aa0). This repository also maintains up-to-date TensorFlow wheels for Raspberry Pi.

[https://github.com/lhelontra/tensorflow-on-arm](https://github.com/lhelontra/tensorflow-on-arm)",1.0
g2jgxq1,ie0aq4,"&gt; I'm interested in building a cluster for fun. 

You can do it although I doubt you will be having much fun.",1.0
g2c8kip,idxoki,"[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/) as you suggest, is the best one.",3.0
g2kzpt5,idxoki,"Hello mate, I just completed Deep learning Specialization from Prof.Andrew and as far as i can tell, courses cover almost all theory and mathematical aspects of the topics they teach. Andrew never leaves any topic like as a black box. Even the programming assignments asks you to implement in *numpy* so that you can know the insides when you do projects in *tensorflow* or anything else. Assignments are perfectly tailored to get a deep insights. All you need is patience and don't rush it, and you'll have amazing understanding of the mechanics.",1.0
g2lly9o,idxoki,"Thanks man, that is what I needed to know.",2.0
g2lqfk9,idxoki,All the best mate!,1.0
g2bevdp,idugn0,You should at least have the basic knowledge of cv since most of the techniques are used in both fields.,4.0
g2bnwe7,idugn0,Thank you very much for your response 👍 yes so I'll go through it once.,1.0
g2bfzjr,idugn0,"I had a similar issue but the other way around. A more experienced practitioner will be able to explain this better but if you think that a CNN is exclusive to Computer Vision and an RNN is exclusive to NLP, that would be incorrect. To put it in better words, it is essential to understand all possible techniques that are available irrespective of which paradigm you wish to work in. The simplest example of one such instance (of many) is the use of CNNs in NLP, where the convolutions are performed on the word embeddings. If you were to ignore how a CNN works, you'd probably find it extremely difficult to intuit why they're used in NLP. For the reverse case, check out transformers and then Detectron 2. 

In essence, to answer your question if you wish to grasp and improve current techniques, an in depth knowledge of both fields is required.",2.0
g2bo0me,idugn0,Thanks 🙏  yes I totally agree 👍 I will study it at least briefly.,1.0
g2bksze,idugn0,"At my University when we were doing our group meetings in the NLP organization, we went through the pytorch tutorials. They start with CV and work up to bag of words which provides a good framework for how NN works. I’d recommend using that too.",2.0
g2bo2lp,idugn0,Thanks!,2.0
g2bnutq,idugn0,"Short answer: Yes

Long answer as the other people told you, the basic techniques are the same, nevertheless CV is conceptually easier to grasp than NLP because the signals are ""continuous"" they are numeric and non categorical which makes a better base to work on the NN internals and get to understand them first. 


NLP is quite complex from the beginning as not only the language domain is complex in itself, but also the signal is categorical and you need to first decide on how you'll encode and decode that signal and which of the many techniques and methods you'll use for that before even starting to get into the other NN part. The decision on how you'll encode will make or brake the rest of the implementation and the way you do that also means working on how you'll create your loss function. (Source, I've been working on this for some time now and I'm still working on testing different encoding methods)

 These functions in CV you can just start with an autoencoder for continuous variable or a one hot output of a class selector as MNIST for example.

Also the CV domain is more mature than NLP which means less distracting from all the possible directions in which things will go. 

There are many other details on why NLP is difficult but as somebody who has done CV a decade ago and started with NLP a couple of years ago (hobby mostly ) I would definitely recommend starting with CV.",1.0
g2bokdd,idugn0,Thank you very much for your complete and perfect response! Yes I am still a newbie in this whole deep learning thing and wasn't really sure what to do. And yes I'll continue the courses and go on with cv and then nlp. ✌,1.0
g2b72my,idsn7a,"Yes.
This is a good project to show to your professors.
Definitely shows your work on tensorflow and your understanding of deep learning in general.",6.0
g2b7a9g,idsn7a,Thank you for the feedback.,1.0
g2c2moh,idsn7a,"You can also try training your model without using .fit() call, that'll show that u do understand the gradient computation part in Tensorflow",5.0
g2c5zx2,idsn7a,Ok I’ll try that.,1.0
g2cnnak,idsn7a,I second this. Doing a custom training loop instead of the blackbox .fit call really helps in understanding what’s happening when training a network. You might even want to add something of your own in the loop.,1.0
g2bho73,idsn7a,"This is good, but try doing cifar-10 data set which is kind of tough to get a good val accuracy and you need to also solve the overfitting problem by regularisation or batch normalisation or dropout.",2.0
g2bk679,idsn7a,I think this is good. CIFAR-10 is already solved for a very high accuracy. You can't beat it unless you're benchmarking a new network architecture.,4.0
g2kysmu,idsn7a,I am a beginner just like you. It's really nice man. Cheers!,2.0
g2lhydx,idsn7a,Thank you,1.0
g2c9p8i,idrzv2,"I had to look up what L0 regularisation was as I don't remember ever hearing about it, but the first google hit about implementing it for network pruning states in [the abstract](https://arxiv.org/abs/1712.01312):

&gt; However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function.

Which seems like a not insignificant hurdle.

Also, I have looked into network pruning a bit (just a few days maybe, definitely not my area of expertise) and have never heard of it, so it's at least somewhat obscure.",2.0
g2cs4cy,idrzv2,"There is a pretty famous paper from ICLR or ICML back in 2018 fall, the paper showed pruning doesn't make sense, and I also took a chance to verify it. It doesn’t make sense.   
Read the paper for more understanding, I read and tried the author's code when it was released back in 2018...

[https://openreview.net/forum?id=rJlnB3C5Ym](https://openreview.net/forum?id=rJlnB3C5Ym)",2.0
g2i9t3r,idrb74,Thank you,1.0
g2g3z94,idp241,"how well does the approach generalize to other tracks, weather conditions, and vehicles?",1.0
g2krjqk,idp241,Very interesting results.  Are you planning on releasing the code?,1.0
g2a822r,idn6ve,"I don't have any links on hand, but I'd recommend using PyTorch (or Tensorflow if you prefer) to create the CNN. Unless you have some non-standard customization in mind, the library will do a lot of heavy lifting (lines of code, potential bugs, mental effort) for you.

Edit: Actually, maybe FastAI would be even better.",4.0
g2ahtve,idn6ve,"These two are good. First one has good introductory explanations. Even available as audiobook, which can help reinforce concepts. 


Deep Learning with Python https://www.amazon.com/dp/1617294438/ref=cm_sw_r_cp_api_i_vf0pFb3Q4WGNC

Deep Learning with PyTorch https://www.amazon.com/dp/1617295264/ref=cm_sw_r_cp_api_i_ge0pFb2ZTB6PE",5.0
g2aqcy8,idn6ve,Deep Learning with PyTorch is currently available for free as well from the official website - https://pytorch.org/deep-learning-with-pytorch,2.0
g2aykh0,idn6ve,"[Cs231n: convolutional neural networks for visual recognition](http://cs231n.stanford.edu/) is very good in my opinion. It's a course given at Stanford University. All lectures are free on youtube, with corresponding assignments in Python (solutions can be found online).

It starts by implementing all the building blocks of CNNs just in pure Python using numpy, and then from assignment 3 I think they start using PyTorch.",3.0
g2be91p,idn6ve,Excellent! Thanks.,1.0
g2asec0,idn6ve,"I recently started reading [this](https://www.amazon.com.au/gp/product/B082MBMFVF/ref=ppx_yo_dt_b_d_asin_title_o00?ie=UTF8&amp;psc=1)\*, has been pretty good so far. I have very minimal Python skills but I have been able to follow along with it. It focuses on TF2 with Python and has a chapter dedicated to CNNs.  


\*'Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and more with TensorFlow 2 and the Keras API' by Antonio Gulli, Amita Kapoor, Sujit Pal",2.0
g2asq21,idn6ve,You could take a look at Part:2 of Hands-on ml with sklearn and tf,2.0
g2ay223,idn6ve,Fastai v4,2.0
g295owk,idhw8n,"Some of the most famous and general algorithms ever conceived of were for specific application problems. Here is one example, but there are many others:

https://aip.scitation.org/doi/10.1063/1.1699114

Metropolis Monte Carlo sampling was designed to compute the equation of state of molecules, and now it shows up everywhere.

If you ignore applications, you risk missing important classes of problems which you would be otherwise unaware of.",13.0
g29lksz,idhw8n,"This may be true (or may not be true) but fuck MIT and MIT techreview basically.

They have the effin worst combination of low innovative output and highest level of arrogance.

They think innovation is supposed to only happen at MIT and if happens anywhere, it's wrong, bad, and needs to shutdown:

- Effin Marvin Minsky crapped on neural networks in the 70s and helped bring the AI winter. Cz neural networks weren't invented at MIT.
- Now Gary Marcus, and Noam Chomsky's disciples, are crapping on neural networks in all kinds of ways. Cz deep learning wasn't invented at MIT.
- And now this.

Meanwhile what they are doing in terms of AI? they're creating effin robot pets (media lab) that can blink and change facial expressions. Wow. Just wow.

Oh and by the way, they crapped on Aubrey de Grey and the SENS foundation for longevity advocacy, around 2005. And now longevity research is becoming the biggest biotech field and pretty much every big researcher is on board.

If you want any kind of innovation to grind to a halt, you ask MIT and MIT techreview to pay attention to it.",11.0
g297ygt,idhw8n,"I disagree with this perspective.

Most of the AI researchers are interested in understanding the fundamental principles for building AI systems. Many of us recognize that solving strong general AI is basically the long-term research goal of the field.  Some AI researchers apply AI techniques to solve ""real-world problems"" --- many AI researchers do both. There are publication venues that skew towards more fundamental work. Rightly so, these venues are less interested in publishing work that is not conceptually novel, and that only applies already established methods to a particular domain. There are plenty of venues that publish applied work that uses AI/ML as a tool to achieve a particular goal. This does not imply a systematic failure for research venues that are oriented towards fundamental progress. 

The author rightly chastises much of the AI community for regularly confusing novelty, insight, and fundamental progress with chasing marginal improvements to metrics on toy benchmarks. However, this is not evidence to support the author's stated position. Rather, this is simply evidence that the scientific standards of the AI community have been pathetically low over the past few years. 

It is a good thing that ML conferences mostly reject the slew of applied papers that pull a model from Pytorch zoo and fine-tune it for some domain-specific task. Publish that in Nature &lt;Insert-Field-of-Choice&gt; or whatever else community cares.",8.0
g292iqe,idhw8n,"Not sure if this belongs here, so apologies if it doesn't! I think the author makes a good point, but wouldn't agree entirely. I think the world, and the not just the ML community, can appreciate a novel application. Where as the world won't appreciate a new network architecture. Thus we have the research community to appreciate those advances. As well though, not enough credit is given to the unique applications that have emerged. Thoughts?",2.0
g2c2cqk,idhw8n,"Aren't applications of ML algorithms and the relevant novel algorithms  that are specific to that field, published in the journals of that field? I mean if you have a paper with applications of deep learning to geology, or with deep learning in medicine, the research should most likely be published in journals about geology and medicine, not in pure machine learning journals.",2.0
g2cej7r,idhw8n,"I would agree with you. I think the author is trying to argue that the pure machine learning journals recieve the highest praise, and shouldn't just be pure machine learning. I don't particularly agree with that. I do agree with them however that there is a sense in the community that applications of machine learning are *easy or trivial.* And that just isn't true. I think the author is mostly annoyed with the attitude that the community will treat applying ML or DL novel applications like a Hello, World.",1.0
g29sn7d,idhw8n,"Thats an interesting counter-point. I guess there's a place for a theoretically-focused academic program. Private industry is the place for real world applications, and I guess thats still developing since ML is so new",1.0
g29vpv6,idhw8n,"It seems that the author is mostly upset that there aren't more highly revered publications and conferences that focus on applied AI. Which is understandable, but again like you said there is private industry for that.",2.0
g29ut0t,idhlrx,What was the total cost?,1.0
g29uwnt,idhlrx,It is $3k+ budget pc. Thanks for asking.,2.0
g27zyol,idc9sz,"so your GPU is being used at all?  this happened to me before, Im trying to remember what the issue was.  

the CPU is used to load data into the GPU, so if have a lot of data and not many nodes in your network, you're going have a low GPU:CPU ratio.",2.0
g2916xg,idc9sz,"yeah, probably I am running on MNIST dataset",1.0
g282gdy,idc9sz,All the pre-processing is done on CPU.,1.0
g2897rl,idc9sz,Cpu might also be bottlenecking the gpu,1.0
g28owi1,idc9sz,sounds like you installed normal tensorflow not tenserflow-gpu,1.0
g2913bd,idc9sz,I followed this tutorial: [https://www.codingforentrepreneurs.com/blog/install-tensorflow-gpu-windows-cuda-cudnn/](https://www.codingforentrepreneurs.com/blog/install-tensorflow-gpu-windows-cuda-cudnn/),1.0
g2b1o4x,id7xcd,"For those interested, I asked the authors of the paper what was the reason for that.

Apparently, my mistake originates from the way warping is perfomed. I thought other papers actually used forward flow to warp but it was a minsunderstanding as they never actually state that, they simply state warping frame *t-1* to frame *t* without giving any precision on what flow they use to do so. I assumed, as the semantic of the 'backward' and 'forward' flows suggests, that the flow they use to do so is the forward flow, but convention actually tells you to use the backward flow from *t* to *t-1* to warp a frame from to *t-1* to *t*.

Thus when they say that they warp  *$O\_{t-1}$* using the backward flow, they simply mean that they transform the frame *t-1* into frame *t* using the estimated backward optical flow between the two. Therefore, the losses they define now make sens to me!

If someone has a nice course explaining why 'forward' warping is performed using backward flows I would highly thankful as I couldn't find any.

I hope this might help someone !",1.0
g2707g8,id5o4m,"A non-technical explanation (since I am no expert myself). Regularization aims to limit (trainable) parameter-space, so overfitting does not occur as fast.

What happens when overfitting to a data set? One way to think about it is that the model starts ""remembering"" data it has seen already and optimizes to fit perfectly to these specific data points, which can hurt generalization.

L2-Weight-Regularization now exponentially punishes large weights, therefore reducing the effective parameter range for each weight, which limits the model's capability to remember specific data points.

Most of the time (in my experience always) large weights indicate ""remembering"" training data (since there is no mechanism to prevent a perfect fit). In neural networks this is omnipresent, since the models often overparameterize (meaning we eventually have more trainable parameters than we need to actually fit the data).

There is also a good explanation in the [Bishop](https://www.google.com/url?q=https://cds.cern.ch/record/998831/files/9780387310732_TOC.pdf&amp;sa=U&amp;ved=2ahUKEwjE_7iDrKnrAhUKsKQKHZKrAgcQFjAOegQICBAB&amp;usg=AOvVaw3nNtNFD-wtsHCz1Lc-uHLK) in section Section 1.1: Exponential curve fitting.

Edit: The example you see in the Bishop with M=9 is an overparameterized model in combination with very high weights.",7.0
g28xtan,id5o4m,"&gt; L2-Weight-Regularization now exponentially punishes large weights

I get why you're saying it that way, but this is probably going to be super confusing to somebody who is already struggling. Here's how I'd put it:

It *quadratically* punishes large weights (penalty = k * beta^2 ). That equates to to a *proportional* decrease in the weight at each step under vanilla SGD if there's no loss function (next beta = current beta * (1 - 2 * k * step size)). Without a loss function, after t steps, it's: beta after t steps = current beta * (1 - 2 * k * step size)^t , amounting to *exponential* decay.",1.0
g28l19y,id5o4m,"The benefits of L2 regularization don't really have anything to do with shrinking the parameters. It has to do with biasing the loss landscape in order to inject stability into the problem. You can do this by regularizing toward *any* value, not just 0. It just so happens that 0 is the most popular (which is reasonable, but altogether unrelated).

When you overfit (or have correlated predictors; the situation is the same), this will blow up the standard errors of the coefficients in your model. With overfitting specifically, this is because you have few/no degrees of freedom left; small changes to the data would lead to large fluctuations in the coefficients. When you regularize, you're ""hoping"" that the gain from overfitting coefficients will be combatted by the penalty on the predictors themselves. In other words, you're hoping that the loss landscape of adding a slope/parabola to the loss function will wash out the little bumps and eddies that exist in the over-parametrized solution.

Note that regularization is not guaranteed to help an over-parameterized model. A much better solution would perform parameter selection and/or gather more data. Regularization is a much stronger tool against correlated predictors, imo.

With correlated predictors, large coefficient values occur because of trying to invert a low rank matrix -- your model cannot reliably separate the effects of the predictors, and it will ""overfit"" to the noise that distinguishes some specific values of the coefficients. For example: if A and B are strongly correlated, then there is almost no difference between 2A - B and 100A - 99B and 1000A - 999B. The only distinction between these possible values will lie in how they fit the noise -- which is the definition of overfitting.

So with all that being said, hopefully it becomes more apparent that you can regularize toward values besides 0! The strength of regularization isn't about ""making the values smaller"", it's all about adding stability to the loss function. We stabilize it by adding very basic, stable shapes - L2 adds a parabola, while L1 regularization adds a ""pyramid"" shape. These shapes don't need to be centered at zero - you can arbitrarily center them at non-zero values (imagine shifting the parabola left/right, etc), which could make the coefficients larger instead of smaller while achieving the same regularization purpose. In fact, this actually corresponds directly to specifying a Bayesian prior!

Also, there is another important lesson here: regularized solutions are not guaranteed to be ""more correct"". They basically introduce a decision rule: ""If there are many possible valid solutions for this model, pick the one that minimizes this regularization term."" But there is no guarantee whatsoever that the true solution is the one that minimizes that regularization! Again, the only purpose is to add stability, not to add correctness.",2.0
g273i6a,id5o4m,One way to look at it is that a weight matrix with large weights will have a larger spectral norm which intuitively means it is able to modify its inputs more violently. This is why we try to avoid with regularization. Another way to look at it is that we are trying to lower the Lipschitz constant (which is highly related to the spectral norm),1.0
g27nzot,id5o4m,"Think curve fitting.

If you have some 2D data points and you're trying to fit some curve in it, then big weights means that your curve can have very vertical portions and accommodate the noise in the data (since weights == slope).

If you penalize large weights, then you encourage your model to fit the data with more ""horizontal"" / flat curves, ie having less variations, simpler, and hopefully more general.",1.0
g28he8s,id5o4m,"To understand, think about the intent of the term 'variance'. In linear regression, say you would like to learn the weight vector. If I gave you dataset 1, you would learn weights 1, whereas if I gave you dataset 2, you would learn weights 2. If I ran a million trials, and gave you a different randomly sampled dataset for each trial, you would get weights 1, weights 2, ..., weights 1 million.

The average of all of these weight vectors is called say w\_avg. The average squared distance between any weight you learn from one dataset and w\_avg is a measure of variance. w\_avg will have the best performance among linear predictors, so we'd like to be close to w\_avg. If we allow the weights to be arbitrarily large (no regularization), then the variance can be arbitrarily high, as the squared distance is higher if the different weight vectors are large in size. If we regularize, then we make all of weights 1, weights 2, ..., weights 1 million \*smaller\*. This also means that w\_avg is now smaller as well, and may go down in predictive performance (this is called bias). However, because all of these vectors are smaller, they will have a smaller squared distance to the new w\_avg. Controlling how much smaller to make the vectors such that weights 1, ..., weights 1 million are close to w\_avg, but that w\_avg is not so biased that performance suffers, is called the bias-variance tradeoff.

In deep learning, of course, there are many more details. But this intuition from LR should do the trick.",1.0
g275kmz,id5o4m,"A student too myself, but here is my understanding of it. During the training of the model, large weights (w) will produce large weight gradients (w') and thus large weight updates ( eta \* w', where eta = learning rate). Thus, with each data-point the model trains on, it produces large weight updates in the direction of fitting this data-point. In other words, it tries to fit each data-point closely and loses its generalizing capability (ability to make small updates to weights and moving gradually towards lower loss). This of course, is the definition of overfitting. Thus, larger weights are a telltale sign of an overfitted model.

Large updates (whether a large weight or a large learning rate) also means that the model has a chance to overshoot the loss value minimum (or atleast produce large oscillations in the loss value), which is wasted compute. Again we prefer small updates so that we definitively move in the direction of the loss-minimum with each update to the weights (optimal use of compute). Hence we use regularization to keep the values of the weights small.

As mentioned above, I'm a student myself so please feel free to correct any errors! :)",1.0
g25zlrg,id0mjx,Check out the Neural Net Study Group discord server,2.0
g260sg6,id0mjx,"Oh okay, I’ve never heard of that. I’ll check that out, thanks!",1.0
g259fz8,icwo6v,Concatenate them. You could also use a feature importance approach to remove less informative elements of the embeddings.,2.0
g259qud,icwo6v,"Can you explain a bit, How I can do that and at which point should I do? But If I am not wrong then feature importance part is handled by multi-head attention right?",1.0
g25rkh4,icwo6v,How are you encoding the book?,2.0
g25sd3x,icwo6v,using vocabulary and then sending it to Autoencoder,1.0
g2794vi,icwo6v,"Are you using a bag of words of every token in the book?

How do you know you have an informative embedding of the book?

Encoding a book into a 200 dimensional feature space seems hard.",1.0
g26lfxi,icwo6v,"If your goal is to classify each book, why are you combining them? It seems like you would want a singular model to output embeddings for each book, and then feed those embeddings into a second model for classification.",1.0
g25qrep,icvjpa,"The FastAI library is built on top of PyTorch to make ML “easier” and faster to get good (sometimes very good) results in just a few lines of code. Digging in deeper into the functionality for more insight into how it does that is up to you. The library is on GH and is well documented. 

FYI, the FastAI v2 Deep Learning course will be released on Friday. Might be worth waiting to start with v2.",1.0
g26h6zt,icvjpa,"but I've always heard that some most companies prefer to form all these algorithms from scratch rather than using these libraries. 
so that's why I'm asking if it's worth learning the fast ai library.",1.0
g26kzx9,icvjpa,"Google pretty much owns Tensorflow while Facebook started and extensively uses PyTorch. Both are open source and considered lower level libraries that have similar performance. Keras is a higher level API that runs on Tensorflow, like FastAI runs on PyTorch.

Edit: to answer your question... it’s almost like asking should you get a PC (TF) or a Mac (PyTorch). You will be up and running faster with FastAI and more easily transition into PyTorch details later. The Keras to TF transition is not as simple if you need to get past Keras limitations.",1.0
g26ldjw,icvjpa,"so just to clear it up, FastAI is also used in the industry and I can continue with the course.",1.0
g26mg1a,icvjpa,"I do not believe “industry” would use FastAI for serious DL in production. They would go straight to PyTorch for better lower level control. 

FastAI is great for learning ML/DL quickly, pulling in your own data, and getting good results.",1.0
g26nxr8,icvjpa,"I'm sorry for asking so many questions. 
But, I'm confused now. 
As a beginner, should I continue with learning the fast ai library , or start with Keras and Tensorflow first?",1.0
g26o80b,icvjpa,"Again, that is like asking if you should buy a PC or a Mac.... it depends on what you want to do with it in the end. There was another post a while back with some good discussion that may help you... I'll see if I can find it...",1.0
g26oip5,icvjpa,"[Here](https://www.reddit.com/r/MachineLearning/comments/bd35be/d_where_does_fastai_as_a_library_stand_against/) is that other post I mentioned with the discussion... it was a year ago, so judge accordingly as things change fast in ML and even these folks may have different opinions today...",1.0
g265tv1,icubem,xinghaozong@yahoo.com,2.0
g26qs37,icubem,Would have loved it.  Just finishing the final week of the specialization in the next few days.,2.0
g27n6it,icubem,"Congrats! Yeah, I created this because I’ve taken too many online courses alone. Learning with others &gt;&gt;&gt;",1.0
g27d1sv,icubem,are you starting the course today or at the next date?,1.0
g27n3a9,icubem,Both! We’ve had enough signups to get started but some people can’t start immediately and have opted in for next round.,1.0
g24996a,icoag8,The color and shape bias in this training/test set seems pretty stark.,1.0
g24xzzf,icoag8,[deleted],0.0
g25jhhl,icju7d,I like to use !wget to pull in the dateset from its url,0.0
g26063j,icjjor,"Cool idea, my opinion is focus money on expensive hardware that groups of people can afford to purchase together like really nice gpus but also fpgas, robotics hardware, and- when it’s available- neuromorphic computing cloud time.",1.0
g22x98u,ichz5i,[deleted],33.0
g231gt4,ichz5i,"Yep, could easily hard code all those code snippets..",15.0
g23ve38,ichz5i,GPT-3 is a gigantic trained NN.,2.0
g23j87r,ichz5i,The Gpt 3 paper?,1.0
g248m9c,ichz5i,https://arxiv.org/abs/2005.14165,4.0
g24ff78,ichz5i,AI should first try to learn the indentation syntax fundamentals before anything else.,3.0
g25egbu,ichz5i,Oh then we can have model v model tabs-spaces flame wars yes please more of this. This is what the world needs rn,1.0
g25r536,ichz5i,At least the model Vi vs Emacs war will be easier: set two identical RL agents simple programming tasks and watch the Emacs Bot get bogged down writing LISP macros...,3.0
g24d0re,ichz5i,"That's fucking amazing, so are you going to become an overnight millionaire and market this thing?  Or are you like the GP3 guy from Openmind that comes here to show us something incredible... seemingly just to individually tell each person asking for source in the comments that you can't share the code?  =p",4.0
g24m0ao,ichz5i,"Um, link?",1.0
g254hxy,ichz5i,"Wait... which link, the one for becoming an [overnight millionaire](http://www.google.com/search?q=how+to+startup+software+company+marketing+revolutionairy+voice+to+code+products), or one for the [reddit post I referred to](https://www.reddit.com/r/deeplearning/comments/iblhzl/personal_gpt3_project_guess_the_movie_you_cant/)?

\*Edit\*  oh shit am I behind... just found out about tabnine and kite's autocomplete features.",1.0
g22s41r,ichpqw,"What you're looking at is actually a picture of your training set. The circles are one class, and the xs are another class. The blue line is your decision boundary. 

In the picture on the left, the decision boundary is essentially a straight line. It won't change very much if you give it different training data, but it won't be very accurate either. In the picture on the the right, your decision boundary is snaking all around your data, capturing the noise in your training set along with the underlying concept. If you had given your model different training data, the decision boundary would look drastically different, meaning that's high variance.",3.0
g239l2n,ichpqw,"And, just to add, ""high bias"" on the left means that the model assumes that data is **linearly separable** which is a very strong assumption -- so the model is biased already, without even looking on your actual data.",1.0
g24e54c,ichpqw,"Thanks, that makes sense. Could you clear up another thing for me? Wouldn't L2 regularization, by affecting the weights, both reduce variance and increase bias just as much? So why would this be an improvement upon our network? How do we know that making the noise less prominent won't also make our function less true to the underlying concept?",1.0
g254fim,ichpqw,"Sure. 

Essentially, what regularization does is force your decision boundary to be less ""curvy,"" as smaller weights mean a simpler model. You control how curvy you want your model to be by adjusting ƛ.

In general, increasing ƛ will decrease your variance and decrease your bias, **but not necessarily by the same amount**.  If you're overfitting (ƛ is too small), then increasing ƛ will reduce the variance a lot more than it will increase the bias. Conversely, if you're underfitting (ƛ too large), then making decreasing ƛ will reduce the bias more than it will increase the variance. 

If perturbing ƛ reduces variance and increases bias by the same amount, then rejoice for you've found the coveted sweet spot where you're neither underfitting nor overfitting.",1.0
g21vpqe,icdmm9,HI EVERYONE,0.0
g20e17z,ic4ucs,Speech models still got a long way to go,32.0
g20y1a0,ic4ucs,Two more papers down the line !,18.0
g20lcr5,ic4ucs,"Yeah, Obama is the hardest, by the way, female voices are better",5.0
g21e84p,ic4ucs,I'm not up to date on text-to-speech but isn't that just because we have more female data samples ?,2.0
g21niul,ic4ucs,"no, it's just because the voice frequency is higher and there is less data in one wav file",3.0
g22crcm,ic4ucs,"Did it say ""supprarp""?",1.0
g20lp8w,ic4ucs,"RIP Facts, 2020",13.0
g20zfrh,ic4ucs,F(acts),6.0
g21rwh4,ic4ucs,"What’s the name, any GitHub link?",6.0
g21yqj9,ic4ucs,+,2.0
g22fea5,ic4ucs,"Name is in the video 
https://parodist.ai/",1.0
g2324tm,ic4ucs,The parodist app (parodist.ai),1.0
g2325fh,ic4ucs,"**I found links in your comment that were not hyperlinked:**

* [parodist.ai](https://parodist.ai)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g2324tm&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1.0
g22p985,ic4ucs,fake voice or video?,2.0
g232313,ic4ucs,Voice,1.0
g22l6u8,ic4ucs,"&gt;And that's why I'm **surprad** to **enderse** Elon Musk as president

Well said, Mr. President. Well said.",1.0
g22lfzo,ic4ucs,"But for real, this shit's starting to get pretty concerning. We're gonna be in some real trouble in 2-3 more years...",4.0
g22wuw5,ic4ucs,"Reminds me of the time when a German voice imitator had a comedic radio show where he imitated our Chancellor at the time (Gerhard Schröder), called it the Gerd show and some people started complaining why our Chancellor had time to do such a show, including singing songs.   
Meaning: This stuff is concerning for quite a while now, since some people are easily fooled even by bad imitation (or even without any evidence - just tell them someone they dislike said something bad and they'll dig it). Which is btw why it would be important to teach kids about fact checking and spotting misinformation in school. Should be a basic class early on.",2.0
g232s7f,ic4ucs,"Yeah, that is why we do not open full access to the synthesis, only through templates",1.0
g22o4g2,ic4ucs,That’s why I’m so prod to enderse Elon Musk,1.0
g20xn44,ic16at,"Hi, I'm no expert either, but I can tell you that 30 samples is very limited. So the first thing you are going to want to do is find ways to increase the number of samples i think. Ask around i guess. You could also experiment with artificially generating more samples based on your 30 images by messing with colors, rotation, gamma etc. No idea if it will work for you, but I overheard my teacher suggest something like this to another student a few months back.

Other than that, it sounds like you already have chosen a framework (pytorch), which means you already have something that fails to do what you want. If you want help, people need to know what you have and how its not living up to your hopes and dreams. Lastly, once you have a clear and answerable question, try asking on stackoverflow aswel.",2.0
g21kamx,ic16at,"Ehh, I wouldn't worry much about trying to do transformations on the image. I'd imagine 30 is enough for a pretrained network. 

My advice is to check out the first lesson, maybe first two lessons, of [fast.ai](http://fast.ai) Practical Deep Learning for Coders.",1.0
g23gac2,ic16at,"Since you want to do K Means clustering, I'm guessing you want to make use of the pre-trained models to extract features from the images. You can then use these features to perform regular K Means.

&amp;#x200B;

So read about how to do feature extraction in PyTorch. You'll have to modify the pre-trained models to output features from an arbitrary intermediate layer. Once you have the features, you can use traditional ML libraries such as sklearn to perform K Means.",1.0
g1yz0y5,ibyn8l,"1. [https://pc-builds.com/calculator/Ryzen\_3\_1300X/GeForce\_RTX\_2060/0Nj1378B/16/100/](https://pc-builds.com/calculator/Ryzen_3_1300X/GeForce_RTX_2060/0Nj1378B/16/100/)  
Do you want two gpus because of speed or parallel training more models? 
2. You need to check number of lanes. One gpu can handle with no problem, but I don't know about two.
3. If you don't plan to train multiple models simultaneously than one better gpu is better than two ""average"" gpus.
4. RTX is better than GTX because of tensor cores.",3.0
g21bphl,ibyn8l,"u/matej1408 

1. ye 
2. Ok
3. Ok,  I think for tensorflow from what I understand two gpus seems to scale well with it
4. OK

thanks for the advice",1.0
g1wofw7,iblhzl,"That's really cool! Do you have an online demo which I can try, or a precompiled binary that I can download and run locally?",7.0
g1wyn58,iblhzl,"I'm really sorry, but as of now, I can't put this demo into production. This is because its a beta version of the GPT-3 model so you'd need an API key to make it work. Thank you for the support though 🙏",2.0
g1xiahw,iblhzl,Can you let me know how to get access to GPT-3?,5.0
g1yj3w7,iblhzl,"Yes, I applied for the OpenAI API waitlist in their official website.",1.0
g1yqbhf,iblhzl,How long did you need to wait?,2.0
g1z0uzv,iblhzl,"Emm, I think I waited for a week back then.",2.0
g1z2j3r,iblhzl,It's been several for me without any response. :(,2.0
g1zfjp3,iblhzl,"Well, there is another way you can accelerate your beta access ( its by sending an email to OpenAI's CTO, you can find more details provided by him [here](https://www.reddit.com/r/OpenAI/comments/hmdrbt/how_likely_is_someone_to_get_into_the_api_beta/) )",2.0
g1xlrc2,iblhzl,For a production system what would you feed as training data? Synopsis of the movies? Synopsis + IMDb user reviews including spoilers? I guess users would usually query details not included in the synopsis...,3.0
g1yj6de,iblhzl,"Well, for the GPT-3 model, it has been trained on a large corpus, so I'm guessing, many comments about movies have been in the training data. It's a few shots learner so it doesn't need a dataset, it just needs a few examples to pick up the pattern you're reaching for.",1.0
g1xfh3p,iblhzl,"This is so cool!! Can it return multiple options?  Like if I searched ""movie with Charlize Theron"" or something not so specific.",2.0
g1xgu9b,iblhzl,"Sure, if its trained on that task, it can do so",3.0
g1xks20,iblhzl,Is there a GitHub link for this project ?,2.0
g1yj7th,iblhzl,"I'm sorry, but it's using a beta version of OpenAI's API so I can't open-source it",1.0
g1y4jbw,iblhzl,Awesome! Any link to this project? I also want to try it,2.0
g1yj9fh,iblhzl,"I would love to post the project but as I said, it's using a beta version of OpenAI's API so I can't post it. You'd need beta access to make it work.",1.0
g1z9gpv,iblhzl,I was looking for a movie today to share with family. Could you search it for me please?,2.0
g1zfexr,iblhzl,"Sure, dm me the description and I'll hook you up",1.0
g1zlkju,iblhzl,"At this point im convinced people named mehdi are genius, i already know 2 that are absolute genius now im puting you in that bin too.",2.0
g1zozpv,iblhzl,"Aww, that's so sweet of you. Thank you for the support ❤",1.0
g21n8w8,iblhzl,"Hey guys, are there any minimum hardware requirements for running the GPT-3?",2.0
g23wmmg,iblhzl,"Not really, cause as of right now its just an API that accesses the real GPT-3 model.

As for the requirements for the real model, when it gets released, I really don't have an idea",2.0
g23xr7c,iblhzl,Thanks for the info bro.,1.0
g236zvh,iblhzl,You could create a bot and let it run in r/TOMT for testing.,2.0
g23wn2g,iblhzl,Great idea,1.0
g1zpt19,iblhzl,Cool what dataset you'd used?,2.0
g1zrldc,iblhzl,"Thanks.

That's the magical part! Its that it doesn't even need to use a dataset!

This model uses few shots learning to train, so it just needs a few examples to pick on the pattern, amazing isn't it?",2.0
g26wm5c,iblhzl,"Yeah, sure it is. 

So you didn't use any official dataset. But how much data you'd used for the sake of this demonstration? I'm assuming these are your test queries.

But great work and application, thanks for sharing it.",1.0
g1w97s4,ibk6ei,"Take a look about siamese networks. They generate a z embedding that is used to compare objects. Maybe you could do something similar using the same encodee to produce two embeddings, using a net or some math to mix  the embeddings and then generating the middle net",4.0
g1wgmkr,ibk6ei,"I'll take a look, thanks!",1.0
g1wg4hf,ibk6ei,"Check this out  [https://arxiv.org/abs/1904.03189](https://arxiv.org/abs/1904.03189), they demonstrate an effective way to embed images in to the latent space of a StyleGAN and then you can interpolate between the two vectors fairly easily. They achieve some really impressive interpolation results provided the underlying generator can actually generate the images you care about.",3.0
g1wle3d,ibk6ei,"It sounds interesting, so I basically do a mix of the previous image and the next one",1.0
g1wltb2,ibk6ei,"Yeah, find latent representations of each and then you can find a middle representation. Main issue is it assumes a smooth latent space. That's a property StyleGAN and its successors worked on extensively, so it may not work as well on more basic GANs where the latent space is more entangled.",2.0
g1wn7eu,ibk6ei,"Okay, thank you very much, very interesting idea",1.0
g22dfa4,ibetgl,"I like it. I just did a similar self-project using Deep Q-Table, then Deep Q learning.  The table approach worked really well given the small game-space.... the Deep Q learning was a bit trickier with similar problems as the agents tended to play the same (or equivalent) games repeatedly - I had to introduce a high degree of randomness to force learning of alternative game states.",1.0
g2d3wv9,ibetgl,"I guess deep learning is an overkill for this simple game. Look into Kaggle's ConnectX competition, that's a better challenge. I've just implemented MCTS and its worked really well",2.0
g1v2jsm,ibengm,"AMD is not invested, and It is a loss for them to focus on GPU for AI. The market share of Nvidia is 100% and trying to get a bite of the AI GPU share market will take years of investment.   
Nvidia has invested in all fields of research in AI and whereas AMD only a team in California consisting of few people working on Rocm (That’s all I know).",5.0
g1v73fd,ibengm,so RocM is the amd's invest for ai?,1.0
g1vio4e,ibengm,I don't know if it is good to call a mockup an investment since it is open source.,4.0
g1x5fe8,ibengm,"&gt; cuz I'm only 16 yo with stupid question 

That's not an excuse for not trying to speak in sentences, man.",1.0
g1ziisy,ibengm,Sorry man.😓😓,1.0
g1wsnd9,ibej84,No. They're not random.,2.0
g1u2t9h,ibaij2,"I was reading this paper earlier but am not too familiar with reliability but maybe you can get something out of it, 

http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_S3VAE_Self-Supervised_Sequential_VAE_for_Representation_Disentanglement_and_Data_Generation_CVPR_2020_paper.pdf",1.0
g1t4wns,ib7o2d,Doesn't the specialization has homework where you do the implementations?,5.0
g1t5yhp,ib7o2d,Yup it has. Which I completed.,-1.0
g1ulkrv,ib7o2d,There you go then,1.0
g1t8ehp,ib7o2d,Depends on the language you want to work with. MATLAB has a great deep learning toolbox to save you implementing the algorithms. Python also has similar libraries. Id recommend keras. Other than that it's up to you to choose an application. Computer vision is pretty good as a starting point,1.0
g1tbqe8,ib7o2d,"You need a business case.

Go try to detect game in a cornfield from images.",1.0
g1tf70g,ib7o2d,"I think after all there's a gap between this course and a reality. I would advice to keep in mind, that real business cases usually a long or midterm tasks that is hard to complete because of many reasons, so it's more like a process.

I would suggest you to read a book from MXnet authors ""dive into deep learning"" just to backup your knowledge.
And while you do this I would suggest either find on GitHub any working repo that is interesting for you, and try to retrain it with your data(but this mostly doable with unsupervised nets) OR try to find employer who will send you a task and data to check your knowledge, which is actually worked for me",1.0
g1u4j6h,ib7o2d,Read the new Fast.ai book. I also completed the deep learning specialization and it was amazing but highly theoretical. Fast.ai is primarily focused on becoming a DL practitioner and you will work through many projects in different domains.,1.0
g1t8krz,ib7o2d,"After doing the same course, I did the exercises given in the book - Deep learning with python by Francois chollet ( the creator of keras)",1.0
g1t1t08,ib791z,"Just thinking out loud here:
1. What kind of initialization are you using? Maybe changing that to something farther than zero can help?
2. More of a long shot: add a penalty term to your loss that is the proportion of zeros in your output times some weight?",1.0
g1t6juq,ib791z,"Alright, I figured it out. Most CNNs that output back into image space are using L1 or absolute value loss since the data is not sparse. But classification problems typically have very sparse outputs, like one hot encoded vectors. In that case, people use cross-entropy loss, which is a type of log loss. 

In my case, if I also use a log loss (in fact, my images tend to be B&amp;W anyway), this problem goes away.",1.0
g7b2swi,ib791z,I am having a similar issue using MATLAB. My input and target images are also sparse and I am running into the same problem. I have done some research into cross-entropy and have not been able to implement a working solution. How did you implement your cross-entropy loss function and backpropagation? Thanks!,1.0
g1t0c7o,ib70mq,I have a model that very accurately predicts tomorrow's share price for the NYSE. I can share it with you for a price. Please send me $10 billion and I will give you a diagram with trained weights.,59.0
g1t3mko,ib70mq,"Lol my bad, probably shouldn't have asked for free implementations :p",7.0
g1uv0rb,ib70mq,[deleted],3.0
g1vd0zj,ib70mq,Exactly. They would have made like Ed Thorp when he figured out how he could take the house down with blackjack.,1.0
g1twps6,ib70mq,BS! Trained weights cannot be stored in a diagram,6.0
g1uknf7,ib70mq,He said diagram with trained weights. He never meant a diagram of trained weights.,4.0
g1t0m7z,ib70mq,"There's a whole class at Georgia Tech called Machine Learning for Trading. Granted it does not  cover Deep Learning but it gives you a very good introduction to the Industry.

[https://classroom.udacity.com/courses/ud501](https://classroom.udacity.com/courses/ud501)

I am not in this particular domain but I would assume the money would lie in feature engineering (highly paid Quants?) and then using that in conjunction with Deep Learning.

Nobody (Hedge Funds) is going  to put their secret sauce on a Medium Post.

&amp;#x200B;

e.g of an old Paper, not free but you could probably find it with some research.

Adaptive use of technical indicators for the prediction of intra-day stock prices

[https://doi.org/10.1016/j.physa.2007.04.126](https://doi.org/10.1016/j.physa.2007.04.126)",19.0
g1t0gep,ib70mq,"Yeah, a lot of people is just showing how close to the real  graph their predictions are by just predicting one day. It's worth nothing.

I don't think anyone will share a model that let's them really win money in the market. I think those models exists, tho.

I've experimented with some things, but I couldn't make any strategy that was better than just buying at the start and selling at the end of the period.

But, if technical analysis works, I'm sure that a model could learn that.",17.0
g1t67d0,ib70mq,key: “if technical analysis works”,21.0
g1u2asy,ib70mq,Aren't there entire industries based around that assumption?,1.0
g1u39mp,ib70mq,"Yes, they're located right next to the global headquarters of homeopathy and divination.",15.0
g1wmkzv,ib70mq,"Of course, they are horoscopes and astrology for men.",2.0
g1ups1t,ib70mq,"I think it works in like 60% of the time. The problem is that if you get a lot of consecutives failures, you lost everything",1.0
g1u26c9,ib70mq,Isn't a day's worth of predictions at a fine enough resolution good enough for day trading?,2.0
g1up4if,ib70mq,"The problem is that they make a daily line plot, with the real values and the predictions, and they say the model works because the lines are very close. The problem is that if you just use the value of the previous day, it will give you the same pattern but with 1 day offset, and it will be very close to the real values. It gives the illusion that you are predicting a lot, but in reality is not predicting anything.

A good way to really test the accuracy, it's just checking for higher or lower. If your model predicted higher than the previous day, and the next day is higher, you have a point. If you can achieve more than 51%, you have something there.",2.0
g1whbvu,ib70mq,I saw a demonstration of a stock prediction model that had excellent training metrics but really bombed IRL. The reason was the model used sophisticated deep learning techniques but was just predicting the raw stock price. The model pretty quickly learnt to just predict the previous day's value which was generally very close to the subsequent day's value. You have to make sure the problem formulation is correct before applying ML which is just going to exploit mistakes made in there,1.0
g1t9tcm,ib70mq,"There are many problems with stock prediction, its a hugely dynamic system, in constant feedback, high noise to signal, and its adaptive.  its not the typical ML problem.  it is closer to weather forecast, then forecasting what particular humans will feel (not do) based on the predicted weather forecast.

I know there are performing ML models, but they are locked behind NDAs.  I would suggest you look at regime forecast/ and decision trees.  ML is better used in risk management than explicit asset price prediction. 

Best published prediction model: T = T-1",15.0
g1t5lk1,ib70mq,I feel anything for stock prediction is a huge scam,7.0
g1u045u,ib70mq,"IMHO it depends... What is the model taking as input? Thousands of news business articles, balance sheets of the companies, and other relevant data that might drive the fundamental value of a company?
This model has a shot at working but building it it's gonna be super expensive.

Is it taking as input what the stock prices were yesterday? Then it's BS",5.0
g1tfslf,ib70mq,"""All models are wrong, but some are useful"" \~ George Box",4.0
g1tb7ko,ib70mq,"I am pretty sure it is possible.

But definitely NOT with the simple hardware and little data we have.


Since all companies depend on each other it is probably necessary to model the whole stock market at once. And I am not talking about the US market but world market.


Edit: missed a not in 2nd sentence.",2.0
g1u2n64,ib70mq,"Something that you also realise if you try to get into this is that even just historical data (at fine enough timescales) is very hard to come by, despite it being public",2.0
g1ue4s0,ib70mq,"Sorry I missed a not in my comment.

I meant that it is currently impossible.",1.0
g1u3mah,ib70mq,"Yes, it's just scam. Perhaps you see all these ""experts"" on Linkedin sharing their ""success stories""... remove the connections and you'll be fine",2.0
g1ubud6,ib70mq,"Could it work if a model read the major news for a day, and then was given the resulting market for the rest of the week, and then it was fed several years worth of news and resulting markets?

It could possibly learn some understanding between current news and how it impacts the market, and thus trade accordingly. There may be insights into how news impacts the market that humans usually would overlook.",2.0
g1uzeok,ib70mq,"Short answer: Yes.  


Long answer: It is far worse than you think. At least the papers all boil down to an innocent Prediction of yesterdays prices. Or for some more comedy someone deep learns the calculation of a moving average. Just peeking under the hood of buy and sell side systems gets you a proper horror show: Trivial probability tables are presented as highly advanced machine learning. Some systems even have a real neural network inside. Full page sized advertisements are taken in financial journals and magazines out to brag. And that is their only function. The outputs of the network are never used.   


The good news for you: For ""some sort of trend prediction"" you can use the classic MACD or DMI indicators. Many decent and open source implementations are available.",2.0
g1vzuv8,ib70mq,"It does work, but like most trades it isn't perfect, but it can give you an edge, it can also pork you.  No one that has anything that works for them is going to share it with you.",2.0
g28lkg5,ib70mq,"I actually built a stock return prediction algo and an economic forecasting algo both using custom lstm variants. None of the algorithms take previous target lags as inputs yet they have really good validation metrics.

It can be done.",2.0
g1t96ft,ib70mq,There is one hedge fund at the moment doing well using DL. I cant remember the exact company though. They use it a bit differently. They have about 200 different trading stratagies and the model predicts which one to use.,1.0
g1tft76,ib70mq,"Renaissance Technologies, perhaps?

Citadel Securities and Jump Trading are also among the major leagues.",3.0
g1v7nhc,ib70mq,"&gt;There is one hedge fund at the moment doing well using DL. I cant remember the exact company though. They use it a bit differently. They have about 200 different trading stratagies and the model predicts which one to use.

The Renaissance Medallion Fund has returned 66 percent annualized before fees over 30 years. No one else comes close to that. I'm not sure if what they do is considered a form of DL.",1.0
g1vdi2s,ib70mq,"I would argue that RenTech would use any methodology available for their disposal, including DL.",0.0
g1uvh4d,ib70mq,I have a model that's decent at predicting when to buy but weirdly only does it when the price is almost at all time low. I think there's some data bias somewhere.,1.0
g1uyf2q,ib70mq,You can train thousands of models and most likely you will get some that perform extraordinary well on the test set. It's all survivorship bias and test set overfitting. No guarantee that they'll continue to do well.,1.0
g1vaa9w,ib70mq,"If you have a model that is better than traditional approaches, you don't write a paper or blog explaining it, you use it or sell it to retire.

What you see published is always the people that tried and failed, but wanted to get something from all the time they sank into it.",1.0
g1vastn,ib70mq,Using NN for trading is interesting using RL but predicting stock price is bs since it depends on lot of factors.,1.0
g1vca6n,ib70mq,"Deep learning for predicting stocks isn’t necessarily a scam. There’s a lot of prop shops/hedge funds that are using ML and making a ton of money. These places have really smart people that get paid insane amounts of money that also sign stringent noncompetes and NDAs. They also have a lot of hardware and engineers doing crazy optimization to ensure that executions happen as fast as possible. 

So no, it’s not always a scam. No one that has a working strat is going to teach it to you, because if it works, why would you give it up?",1.0
g1vno1b,ib70mq,Worth watching https://m.youtube.com/watch?v=gjVDqfUhXOY,1.0
g1w9jv3,ib70mq,yes? You cant predict the unpredictable. There are many papers about it.,1.0
g1s8jnm,iaw492,"Huber and MAE don't heavily penalize outliers whereas MSE does. 

On problems with very difficult outliers, MSE struggles to reach a good solution because it's trying to keep the variance minimized. It sounds like this is the case for your problem.

Huber is squared at errors below some threshold, typically around 1. The reason for this is so the loss is differentiable at 0 (MAE is not). In practice, errors are rarely zero so this is not much of a concern.

Because you are using different loss functions, your loss value will be different. Eg, |error| /=/ (error)^2. You really just want to make sure the loss is minimizing or there's something wrong in your setup.",2.0
g1tfdf7,iaw492,"The loss is not decreasing, it is fluctuating. It increased to a value around 55 and then became more noisy.
The same setup is giving good results with other loss functions and the agent's visual performance is equally good for 2000 episodes.

In RL, usually loss value is not a good indicator of the agent's performance. 

Considering the above points could it still mean that the this performance is acceptable?",1.0
g1tfkmq,iaw492,"I'm not familiar enough with your problem to make that call. If your loss is fluctuating, the problem is either set up incorrectly or near convergence. To stop it from fluctuating near convergence, try decreasing the learning rate.",1.0
g1rmpgn,iaw492,one thing to keep in mind is that mse and mae values aren't directly comparable,1.0
g1qu921,iau7s7,"Hey, no problem at all, I bet everyone of us felt kinda confused when we're starting :)

OpenCV is a very popular Computer Vision (CV) library. You can use it in C/C++ and Python. It has many useful functions, many related to a traditional CV (aka pre-Deep Learning era).

Deep Learning is a broad term that groups many different techniques. Artificial Neural Networks are the most popular (Feedforward nets, convolutional neural networks, recurrent networks, LSTMs, GANs, autoencoders, etc.). Genetic Programming might also be considered  Deep Learning by some researchers (the evolutionary facet of DL). 

So, you might want to think of it as if OpenCV is  the tool and Deep Learning is the strategy. If you want to do DL you might also want to learn PyTorch or Tensorflow.",7.0
g1yxclc,iau7s7,Thanks a lot!!!,2.0
g1r3446,iasyct,Likely speed and memory consumption.,2.0
g1rdr6t,iasyct,"ResNets and SqueezeNets are older, have been studied more, and are thoroughly vetted at this point.  EfficientNets are relatively new and although I haven't had any direct experience working with them, I've seen others on this subreddit struggle to reproduce their results. 

TLDR: It takes time for newer models to become popular",2.0
g1rdvmx,iasyct,"Eh, half the time, just fine tuning a pre-trained ResNet is usually enough to get most of the jobs I need done, so I generally don’t feel the need to go outside the Pytorch model zoo.

That said - for object detection, EfficientDet is pretty dope. I do make the effort for that.",2.0
g1szjis,iasyct,"No idea about TF, but torchvision (where these would actually go) has some fairly strict requirements with regards to reproducibility before adding models to the library. Basically they must reproduce the published results while being trained in native pytorch/torchvision code. This is more difficult than one might imagine, and gets expensive for the larger models. Torchvision has open issues for these where it is being discussed, but basically the current state is no-one can meet the inclusion requirements",1.0
g1tvezs,iasyct,"They're not very well suited for real world problems. The use of grouped convolutions make it slower on edge devices like Jetson Nano. They're good for research, but not for practical purposes.",1.0
g1r4sf9,iaruid,How are you checking if they are learning? Try inner\_model.\_trainable=False or do it layerwise,2.0
g1qd2u1,iaqukc,"There are many factors to consider here apart from the market.

* If high pay is an issue, you will probably be disappointed by the European market, especially France and Germany. UK is a little better, Switzerland is the best in terms of pay. It is a trade-off between salary and working conditions, in my opinion, but net salaries in France are 60%-70% what their counterparts are in the US.
* How is your visa situation? If you have ways of moving to Europe and applying for a job from here, it might be easier than applying from abroad. Smaller companies might have a harder time hiring you because they will have less structure to help you settle, it is much easier for them to hire a local or someone from the EU, the bureaucracy hops are much smaller in this case. Rest assure, it is possible to come work in the EU, we are not savages that block any hope of getting a visa on the whims or the latest president.
* Have you checked how your countries taxing system applies to residents abroad? In France, there is an agreement to prevent double taxation with most countries, but, as an example, you will still be required to pay federal income tax in the US and other taxes, I do not know the details. You will have to take that into consideration, Uncle Sam will always ask its due as long as you are a citizen, other countries might do similar things.
* Doing a PhD is not completely straightforward either. France, for example, has a system of 2 years masters + 3 years PhD, where masters are very course-intensive and PhD is 95% research and no classes. Most academic institutions require a scholarship or funding to allow for a PhD, and **self funding is not allowed in most of them**. You are considered an employee of the lab during your PhD and the funding is your pay, this counts for social security and worker's benefits in the future.
* Do you have any support system in any European country, close relatives or friends? This is a more personal note, it is not easy to uproot and move continents, and it is especially hard to do alone. Many European countries are not the warm-embrace type of country and loneliness, depression and isolation are easily underestimated when you are making plans of your trip living still in your country.

And about the market, I would be able to answer this question in 2019, but now I have no idea whats happening. Most companies are holding off on hiring new people waiting to see if and how the world will end, there is a lot of uncertainty, especially with startups, on what the future will be. Many computer vision start-ups rely on a model that offers specialized services to bug companies, and in times of crisis these projects are easily cut by the big companies.

I don't want to discourage you, but this is a very big decision. If you are hoping to do that only for a short period, consider that a huge chunk of your taxes are going to retirement funds and social safety protections that you will not enjoy. They are transferable on an European level, but I am unsure if they can be used in other countries.

Hope this helps.",17.0
g1qyurq,iaqukc,"OK, I should have clarified this in my post: I *am* European, and already working in one of the relatively low-paying countries. I know I am badly underpaid even when taking that into account.

I'm not planning on staying here anyway, so I'm trying to get an idea of what I can expect abroad. I don't know much about the job market out of my country, which I don't care much about staying in.

Thanks for the advice :) But since I messed up in my post, most of it doesn't apply to my case.",5.0
g1qnrt3,iaqukc,What makes you think that the poster is American and subject to American taxes?,3.0
g1qsf3r,iaqukc,"Just my impression that 95%+ of reddit is either American or European, and since OP is not European, I defaulted it to American, I made edits to make the comment more inclusive.",5.0
g1rwper,iaqukc,Actually I’d be not at all surprised if the poster is European and saying that they are researching European options because that’s the citizenship they have.,4.0
g1t7htg,iaqukc,That’s the case :),2.0
g1s8uz6,iaqukc,"What kind of industry are you doing your work in? I'm very curious about what specific fields have the most growing demand for DL based computer vision. I have worked with ML for a couple of years and would love to work with computer vision at some point, esp in industry and agriculture (drone/robot vision for automation ideally). Anyone happen to know anything about this? I.e. what the market looks like, how to break into the field etc. Would love to know - 5 years ago I asked about advice for getting into ML here and it paid off!",2.0
g1q98s7,iaqukc,"I don't know about Europe, but I just saw statistics that 500,000 machine learning jobs are currently unfilled in America. So long as you add ML to your experience, your degree should be plenty in this environment. 

This is basically how I picked my career 15 years ago, except it wasn't ML at the time, it was just generic technology jobs. Follow these big trends and you'll be fine. ML isn't going anywhere, and is only going to get bigger. With this much unmet demand, you'll advance quickly, but you need to do the work, get the experience, and continue learning. Make sure you're well rounded with soft skills.",4.0
g1qq8sh,iaqukc,There is no way 500k ml jobs are unfilled in America alone. No way. Any link to support that claim?,3.0
g1qs4lt,iaqukc,"Oh I'm sure it was hyperbole or qualified, like ""500k over the next 10 years""

I'll see if I can find the article, hang on

Edit: not the article I saw but there are plenty talking about tens of thousands of open jobs today. 


https://www-forbes-com.cdn.ampproject.org/v/s/www.forbes.com/sites/bernardmarr/2018/06/25/the-ai-skills-crisis-and-how-to-close-the-gap/amp/?usqp=mq331AQFKAGwASA%3D&amp;amp_js_v=0.1#aoh=15975926452377&amp;amp_ct=1597592749811&amp;referrer=https%3A%2F%2Fwww.google.com&amp;amp_tf=From%20%251%24s&amp;ampshare=https%3A%2F%2Fwww.forbes.com%2Fsites%2Fbernardmarr%2F2018%2F06%2F25%2Fthe-ai-skills-crisis-and-how-to-close-the-gap%2F",4.0
g20hq07,iaqukc,"[M.Sc](https://M.Sc). in robotics with a master thesis in DL -&gt; 60+ job applications for ML/DL in computer vision / robotics, only 2 telephone interviews and 1 test. No offers. It sucks.",1.0
g213ede,iaqukc,"My impression is that you either need cool projects (like open source contributions or competitions), or a PhD, or both. But I'm trying to get a better picture of what's required.

Do you have work experience? That's the chicken-and-egg problem I'm happy not to have, since my internship led to a job right out of college.",1.0
g213u3n,iaqukc,"No work experience, fresh from the uni. And during masters, we don't do internships, only during bachelors.",1.0
g23fyth,iaqukc,"Not a great position, I guess. Are you applying to internships or straight to engineer positions?",1.0
g24hwkg,iaqukc,"Straight to full time jobs, I dont see a point to apply for internships and work for 900€ month with [M.Sc](https://M.Sc). degree in robotics.",1.0
g27n9b1,iaqukc,"From what I've seen, you might not have a choice. Experience is more important than the degree, and an MSc isn't that impressive in a field where PhDs are common. A few months of internship should help you. But good luck anyway.",1.0
g1q99i8,iaqukc,RemindMe! 2 days,1.0
g1qa5xv,iaqukc,"I will be messaging you in 2 days on [**2020-08-18 12:11:49 UTC**](http://www.wolframalpha.com/input/?i=2020-08-18%2012:11:49%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/deeplearning/comments/iaqukc/how_is_the_job_market_for_a_computer_vision/g1q99i8/?context=3)

[**4 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdeeplearning%2Fcomments%2Fiaqukc%2Fhow_is_the_job_market_for_a_computer_vision%2Fg1q99i8%2F%5D%0A%0ARemindMe%21%202020-08-18%2012%3A11%3A49%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20iaqukc)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
g1q6jvq,iame2m,"There's a guy on youtube I think he recently changed his channel name to **Artificial Images** or something like that.  He teaches machine learning classes, so there's two cool benefits to his method.  One is that he introduces all the shit leading up to StyleGan, like really holds your hand so nobody gets left behind.  

The other cool thing is that he covers the material in his videos repeatedly.  Makes it easier to look up specifics, and his (teaching skills...) video lessons improve each semester so I recommend starting with the latest tutorial on stylegan2.

Downside is he's not exactly concise, and there's no textbook, so get used to skipping back and forth in YouTube if you don't take notes or outline your tutorial work for future reference. 

[https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv\_AA](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA)",3.0
g1qsy3x,iame2m,Didn't know that channel thank you for sharing.,1.0
g2hxv6v,iame2m,"I like the Artificial Images channel, but I found that this guy (https://www.youtube.com/user/TheShoreOfLimbo) to be pretty good as well.",1.0
g2kdncd,iame2m,"Cool beans, looking forward to binging on his work.... since we're sharing, I found this guy's explanations monumentally helpful for understanding all the ""why?"" questions.   [https://www.youtube.com/c/YannicKilcher/featured](https://www.youtube.com/c/YannicKilcher/featured) 

He covers popular and landmark ML papers in 15 to 45 minute videos.  I get the feeling he's explaining the topic for the first time while recording the video in one take... but he makes it up through good use of diagrams and providing insight on an expert level.",1.0
g1nz67z,iaddbx,50mb .pdf?! damn nerds,2.0
g1prfhy,iaddbx,Honestly wtf. I can't get a goddamn CIFAR VAE working and then I see this. How would one even go about doing that?,1.0
g1li89n,ia8fec,Typically for heat maps you want to use pixel wise Euclidean loss,2.0
g1ljhf2,ia8fec,"I agree, that is why I went with mean square error first. But it seems that heatmaps are prone to class imbalance due to very small positive regions compare to image. I used a focal loss with cross entropy as an alternate loss, but the results weren't much better.",1.0
g1ll4s1,ia8fec,"MSE is not the same as pixel wise Euclidean loss, since the pixel wise loss is not taking an average. What were you attempting to gain by introducing focal loss or the cross entropy. Cross entropy is typically used for classification so not really sure how you were attempting to use it for this problem.",1.0
g1lm1yr,ia8fec,"Most papers on facial landmark detection with heatmaps talk about MSE, so my quick and dirty model implemented it. 
I tried the ce loss to see if image segmentation style binary cross entropy loss can help. I understand that segmentation is categorical and heatmap is continuous, but seemed like a good try. I used focal loss with binary cross entropy that is implemented as part of tensorflow addons package to try my experiment.",2.0
g1lwl1s,ia8fec,"I’ve never worked directly in facial landmark detection, only in body detection, but in the body detection sphere it is always Euclidean. Maybe if you could share a specific paper you are referencing I could take a look and make sure you are using the right loss function from the paper",2.0
g1mc7gv,ia8fec,"I am trying to replicate the results of this paper. I want to extend this to cover face and upper body pose.

[https://openaccess.thecvf.com/content\_cvpr\_2018/papers\_backup/Merget\_Robust\_Facial\_Landmark\_CVPR\_2018\_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Merget_Robust_Facial_Landmark_CVPR_2018_paper.pdf)

Since human pose estimation have long used heatmap based techniques ([Simple Baselines for Human Pose Estimation](https://openaccess.thecvf.com/content_ECCV_2018/papers/Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper.pdf)), I am looking for similar techniques in facial alignment.",2.0
g1mxv8d,ia8fec,Yea I’d need to read in more detail but there loss function is not MSE. It’s a weighted pixel wise squared loss but reminiscent of Euclidean loss,2.0
g1n617z,ia8fec,I think a quick shot way to train a network would be to use the standard pixel wise Euclidean loss and see how that performs,2.0
g1n7s47,ia8fec,Right! I read squared loss and assumed MSE. Thanks I will try your suggestion.,1.0
g1o4e67,ia806y,GitHub?,1.0
g1o9vsp,ia806y,Exactly,1.0
g1opjod,ia806y,"https://github.com/bryandlee/FreezeG
But it’s just a README and sample images. 
No code is shared. 😥",1.0
g1lskxk,ia5m92,Focus on projects where you can generate your own data. Lots to be done with physics simulations. DM me if you want specific advice.,3.0
g1kklq5,ia5m92,"I have just done a project on Neural Network Verification for MSc in Data Science - it's an emerging topic that's getting a lot of traction recently, worth having a look I suppose. Here's a link to a paper by my supervisor https://arxiv.org/abs/1907.01297",2.0
g1kl2g0,ia5m92,Sure. Thanks,1.0
g1lugyp,ia0mz5,Look at PSGAN (periodic spatial gan) they have a pytorch implementation on GitHub under their famos repository. The earlier gatys stuff works really well for generating individual texture images (maybe the best) but takes a long time to generate a single image. There might be some more state of the art papers out there that I'm not up to date on though.,2.0
g1iw8tb,ia0mz5,You can look at image stylization. For example Adaptive instance normalization is one good paper.,1.0
g1jfzaz,ia0mz5,"Looks for visual texture synthesis work by Gatys et al. that is a precurser to style transfer. Later studies improve the technique in terms of flexibility, efficiency, control etc.",1.0
g1iidf3,i9xxvu,What kind of model did you used ?,1.0
g1ikdsd,i9xxvu,"Point pillars for 3D object detection (the pretrained model for waymo open dataset was not available, so I decided to use the ground truth but I used point pillar 3D detections in the KITTI dataset). Real time panoptic segmentation from dense predictions for the Semantic Segmentation. In the future, I will generate a point cloud from a depth map obtained from a neural net and see how this all performs on that kind of data",4.0
g1iisv0,i9xxvu,"Very cool! Do you handle oncoming traffic differently from traffic on your side? Also are your predictions images or some other format that you project onto an image?

One confusing piece of info in the video- Recommended speed at the crosswalk was full speed at 2:40 lol",1.0
g1ilbvr,i9xxvu,"The default recommended speed is full speed (I assume that the car is always on the move, I guess you could adjust that if you have information from your path planner).

I simply generate a heat map around each detected object in 3D space and extend this heat map outwards. To make sure the heatmap is more accurate, I also project the 3d bounding box of the specific object on the different heatmap points to see if the intersection over union with the road is good or not. There is more detail on my LinkedIn where I trace my entire progress from start up till now.",2.0
g1m2gr5,i9xxvu,"Ok I see. So the heatmap is centered around each object and facing the same direction as the trajectory, but will always have the same magnitudes?",1.0
g1momfa,i9xxvu,"By magnitude, do you mean how far the heatmap is shown?",1.0
g1ph1l6,i9xxvu,Yes the size of the heatmap.,1.0
g1pmxjp,i9xxvu,The size and angular sweep of the heatmap are adjustable. I decided to keep it decently big here but you can adjust it in code.,2.0
g1ikp9h,i9xxvu,"The predictions seem quite smooth (or stable) in time, are you doing some kind of kalman filtering of the prediction or other filtering to stabilize it between time steps? Or, does the model inherently predict multiple time steps at once, and that's why its predictions are stable in time?",1.0
g1imr73,i9xxvu,I am using ground truths here because a pretrained model for this dataset was not available. I used Point Pillars for 3D Object Detection on the KITTI dataset using a pretrained network that I found online: https://www.linkedin.com/posts/sarimmehdi550_deeplearning-neuralnetworks-computervision-activity-6699450486002589697-icrh,1.0
g1imzp6,i9xxvu,Oh so you're plotting the label? Is that what you mean by ground truth,1.0
g1io4le,i9xxvu,"Yes. But for KITTI, I did 3D detection using a neural net. If I find a pretrained neural net for this dataset, I would have no trouble using that instead.",4.0
g1ijc13,i9tkix,"I’ll try to help, though I’m no expert on graph theory or graph data. 

Graphs are only one representation of non-euclidian data, but it might help to consider other types of non-Euclidean data, and then come back to graphs. Specifically, data which include three dimensional, real-world, geometric structure. Examples include point clouds, triangular networks (meshes), or molecular proteins. Graphs are similar to these types of data (meshes and networks in particular), albeit they don’t (typically) represent physical structures in two or three dimensional space, but represent entities and relationships within *some form of space independent of euclidian 2D or 3D space*. The distance from A to B (euclidian) isn’t important so much as the type of relationship between A and B (non-euclidian). 

Information about these data types, like their structure, relationships, and connections, are generally lost when represented in euclidian space (tabular, matrix, and/or array) via some transformation. For instance, you must transform a 3D geodesic into a straight line (loss of structure) or leave out connections between entities (loss of relationship). It turns out this lost information can be very useful in deep learning applications. Apparently there’s a niche sub field called Geometric Deep Learning that attempts to take advantage of data with these inherent relationships, connections, and geometric properties.",3.0
g1hgetl,i9tkix,Shortest path isn’t a straight line.,0.0
g1hhkrs,i9tkix,"I know the summary of non-euclidean space. When you're standing on a manifold (for example earth) shortest distance between two points is not the straight line, so it is a non-euclidean space.  But am interested in what non-euclidean data mean and why graphs are in non-euclidean space in machine learning perspective?",2.0
g1iptc2,i9tkix,"euclidean is a choice. When people have vectors in R\^n usually it's thought of as existing in Euclidean space; then you can define curvature of a vector field and size of any vector using normal metrics such as using dot product of 2 vectors (curvature) or L2-norm (size). But both notions only work as you're accustomed because you're saying the space is euclidean. In essence, you're specifying an \*assumption\* about your data by saying it's Euclidean, in that you're indicating the Euclidean metrics are suitable for measuring curvature and size of vectors in R\^n. This is actually a strong assumption, you're making claims such as this: ""take two vectors all of zero length N, say I take the second entry and make it a 1 in one vector, and in the second vector I take the (N - 1) entry and make it a 1"". In Euclidean world, both vectors have the same size, and the maximum amount of local curvature occurs between the 1-3 entries (vector 1), and (N - 2) - N entries (vector 2), and in both cases it represents the same amount of local curvature.

Now here's where non-euclidean signal processing comes in: what if the N entries of the vector are from a specific problem setting in which we might have some a priori knowledge specifying how any of the entries should differ? For example each entry represents an observation measurement at a point, and there's a collection of points (for all N entries) we take, and we know which points are nearby others? Well then local curvature should be measured specific to the different neighborhoods of sampling points, and not just the arbitrary ordering we gave in our R\^n vector. So for example, if you have the graph adjacenecy matrix which encodes the pairwise notion of nearness between all points then you'd have the correct way to measure curvature on your R\^n vector (Dirichlet energy of the signal using the graph laplacian).

In general, by knowing the data is observed on the graph implies the meaning of the information itself is changed; the information content required to reconstruct each entry in the R\^n vector is evaluated by how different it's entry is to other entries in it's neighborhood, and how this local neighborhood fits in to the entire graph. This perspective does generalize all of Fourier processing of time series vectors in R\^n, since the fourier modes are recovered as the eigenvectors of the graph laplacian constructed from a discrete graph that has a ring topology.

These are good papers but they cover much of the same ground: ""Geometric Deep Learning: Going beyond Euclidean data"";  ""The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains"". Obviously integrating this into deep learning is really interesting, but these papers on more pure ""graph signal processing"" are also really informative.",2.0
g1kmzvg,i9mzer,Where can I get it dude ? They look interesting,1.0
g1kuih9,i9mzer,Look in the YouTube video description,1.0
g1li2v1,i9mzer,Description for video,1.0
g1ll5zn,i9mzer,Thanks man,1.0
g1g1g8k,i9m8s5,GitHub/paper link?,3.0
g1g906b,i9m8s5,Video description,-1.0
g1gao8a,i9m8s5,AI. Hard at work solving the world’s most pressing problems.,2.0
g1gig4c,i9m8s5,Reducing fuel waste from shipped item returns would be a pretty nice impact on the planet.,9.0
g1gqj61,i9m8s5,"This won’t show true fit - this will make the clothes look as good on you as they do on the model, or whatever you’re wearing.",4.0
g1gvlez,i9m8s5,"Agreed. Seems this would just reduce the need for models and clothes to be in the same room. So perhaps transport fees (for humans, clothing). Would also reduce the photographer hours required.

Edit: It looks to modify the model’s body shape and features as well, if only slightly.",3.0
g1g8h1d,i9m8s5,Interesting,1.0
g1h8wqr,i9m8s5,This is epic,1.0
g204qiv,i9joun,"May be you can refer to UNET architecture, so that you can think about transfer learning appraoch using AE as it gives an idea on how each encoder layer interacts with decoder layer and which one to freeze and which one to allow in training. But really wondering how transfer learning possible in autoencoder architectures.",1.0
g22pqbv,i9joun,"Me too, I will try, thanks for idea :)",1.0
g1drcxd,i96m40,"So why would I use this, instead of something like Azure Forms Recognizer?",1.0
g1ewgod,i96m40,"Because Azure models are limited to the data distributions they were optimized for, and you cannot apply transfer knowledge, as microsoft doesn't give access to weights and models archs",1.0
g1ffhem,i96m40,"So that’s just not really true.

(1) you train forms recognizer with your data, optimizing it for the data distribution you care about

(2) you don’t think Microsoft isn’t leveraging transfer learning on a massive scale to make it work? Of course they are. You’re applying your data to their pre-trained model

It’s entirely possible OP’s product is better, but I want something a bit more convincing.",1.0
g1cwh5k,i95jmc,Yes definitely wait for the new cards. They're gonna be available starting next month.,2.0
g1d9jjm,i95jmc,This. Wait for the Ampere GPUs. Its very likely going to be a large jump in performance whilst having a more modern feature set.,1.0
g1cunm1,i95jmc,why 1200watts? are you planning on getting more gpus?,1.0
g1cuuzh,i95jmc,Yes. I was planning a sli config with two gpu. But low on budget.,3.0
g1e0ayn,i95jmc,"If this is the case, I highly recommend the blower style GPUs. The 2080 ti seems to run warmer than most GPUs already and you don’t want that hot air circulating in your box. 

Check out this blog by 
[Tim Dettmers ](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/) for more specific info if interested.",1.0
g1cyqwk,i95jmc,"Looks pretty good, lined up to the design I made with a couple of friends (also waiting to buy). What graphics cards to you plan on using? Also from what I heard two graphics cards in SLI is probably not worth it since you don't get a full 2x performance, and one graphics card might be enough.",1.0
g1cza1s,i95jmc,2080ti.. (msi/zotac). I heard you get around 1.5x to 1.8x perfomance.,1.0
g1d3r24,i95jmc,aren't Intel CPUs just a tad better at things like linear algebra? I am building a rig too and will probably go for the i9 10900k instead,1.0
g1d5e4m,i95jmc,Money wise if you optimize it to use all cores I found amd is just so much cheaper. Got a 2700x for £190 used 14 months ago and it outperforms my mates i7 which cost more.,1.0
g1d6eay,i95jmc,"Correct me if I’m wrong, but isnt all the linear algebra and calculus performed on the GPU...? 

CPUs are mainly for preprocessing... I could be wrong, I’m currently assembling a PC myself and this is what I learnt.",1.0
g1d81h1,i95jmc,"yeah, maybe. it sounds we are all on the same boat! I am reading that many pre-processing tasks are single-core operations where Intel processors are better and it does, in many cases, include numpy operations where Intel's MKL is just better optimised for most, not all but most, tasks. It doesn't look like there is a very clear answer anywhere so I just rely on the benchmarks I found online, i.e.  [https://www.phoronix.com/scan.php?page=article&amp;item=3900x-9900k-400&amp;num=5](https://www.phoronix.com/scan.php?page=article&amp;item=3900x-9900k-400&amp;num=5)",2.0
g1d5jna,i95jmc,This might help: https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/,1.0
g1dk09r,i95jmc,"If you're computing on gpu anyways you can go with a mainstream ryzen or 2nd gen threadripper and invest the money saved into a second gpu. Much more important for training speed.

Even though more cpu cores are good when pre-processing large batches, from my experience, the extra price you'd pay for the better single core performance of 3rd gen threadripper (compared to 2nd) is not worth it in this szenario.

Also pre-processing can be cached and then the only thing all those cores have to do is assembling the batches where single core doesn't really matter that much.",1.0
g1e5ikp,i95jmc,Get 32GB ram. 16GB is barely enough.,1.0
g1ej3c1,i95jmc,I am getting a 64gb ram.. sorry *4 was cut out of image,1.0
g1fqgk1,i95jmc,"Deep learning heavily depends on GPUs, why spending that much in a threadripper? You can get a Ryzen 3900x or 3950x at half the price or less and spend that money in better/more gpus. Also, amd will release the 4th generation of ryzen cpus next months as well.

If you are concerned about PCI-E lanes, the RTX 3000 series will support PCI-E 4.0 so you'll be able to easily run a SLI of 2xRTX 3080ti without bottlenecks even at 8x4.0 lanes. So there's no need for a threadripper processor in my opinion",1.0
g22vozv,i95jmc,"I'm building a rig for deep learning right now. Would say Threadripper 3960X is overkill, especially since you only have 1 GPU. 1900X is selling for \~$220 right now and 8 cores should be okay (will need to switch mobo to something with an sTR4 socket). Or a 1920X / 1950X. Instead of this expensive CPU, consider adding another GPU. Definitely would add at least 16 GB more RAM (64 GB total if working with large datasets). If you are sticking to 1-2 GPUs, you can get a lower wattage PSU. I'd also say you don't really need a liquid cooler.

That being said, if you never plan on upgrading this PC to &gt;2 GPUs, I would actually go with a Ryzen 7 3700X CPU and some corresponding motherboard. In all, I think you're chalking up quite a bit of unnecessary expense given the performance you're looking to get out of this.",1.0
g1e5s9p,i91gdy,"Interesting read! The NeurIPS reviews my team received were actually really fair and detailed for the most part.

I do understand the frustration as the traditional review process for ML journals is starting to break down. I like the tiered system idea. More specific venues as the first stage and then selection to general venues from the pool of already accepted work.

The problem is NeurIPS wants publications to be new and not published elsewhere (except arxiv).

Open review also seems like an interesting path forward.",1.0
g1cdskr,i8yxns,Why did you take two pictures of the same guy to demonstrate the effect though?,2.0
g1cdxsf,i8yxns,😂,2.0
g1cpacj,i8yxns,I try to make a deepfake „from scratch“ using pytorch. It kinda works but that shit is blurry as hell. Maybe someone can help me out with finding the right architecture?,1.0
g1bjvnm,i8yu2l,"I was surprised by how difficult converting a TF model into TFLite model and no surprisingly I was more surprised by how even more difficult converting a TF model into GPU acceleration ready TFLite model!

I had to try a lot things to make a GPU works on iOS. As a result, about 9M parameter YOLOv3 model runs with 15\~20 FPS on iPhone X.",1.0
g1bxd4h,i8wrdk,Nice video!,3.0
g1c7r90,i8wrdk,Thanks :),1.0
g1c7fx1,i8wrdk,"Awesome work, subscribed - please keep doing this!",2.0
g1cmmyt,i8wrdk,Thanks mate :) surely,2.0
g1biabv,i8wrdk,I am loving this trend of explaining research papers.,3.0
g1c7dcx,i8wrdk,Me too!,2.0
g1bcjar,i8wrdk,Thanks for the summary! Great video,1.0
g1camnu,i8wrdk,Thanks mate!,1.0
g1aqqz4,i8tlru,Machine Learning For Managers — What You Need To Know : A medium subscription is all you need.,1.0
g1akb0g,i8sai8,Learn to pick your battles.,50.0
g1b2krg,i8sai8,"What does ""basic""mean? It's just a word. You know stuff they don't know. They know stuff you don't know.

Actually I find it more relaxed to be underrated than being overrated.",10.0
g1b3wqu,i8sai8,"Yup, better to exceed expectations than to not meet them!",2.0
g1b91fc,i8sai8,"&gt;Actually I find it more relaxed to be underrated than being overrated.

Yes that's right. Being underrated and always giving your best work is a damn good feeling.",1.0
g1akipl,i8sai8,"I am sorry they are treating you that way. Let your work speak for itself. It seems like it’s a case of them trying to show dominance since you are “just an” intern in their eyes. If it becomes a case of bullying / ridicule, then I would suggest doing something more assertive but for now just stay calm and continue doing amazing work and let them see what you are capable of :)",17.0
g1b4o1a,i8sai8,"Great line “let your work speak for itself” ! Stay humble and show that you are eager to learn. ML/DL in academia is different from ML/DL in the corporate world. It’s not about state of the art performance but more about answering questions, delivering value and ability to productionize your models.",1.0
g1b518n,i8sai8,[deleted],15.0
g1c4vd4,i8sai8,"You did see the part where he's published, right? It's not dunning kruger; he's not claiming to be an expert or advanced, just to be acknowledged that he's not a beginner.",-3.0
g1b5kvf,i8sai8,"Agree with others, let your work speak for itself. My guess is that they’re underestimating due to you being an intern and not because you’re not competent. I typically don’t put too much stock in the new hires until they’ve proven themselves a bit regardless of how good their resume looks because I’ve worked with some people who produce really mediocre work who look great on paper. If you finish the work they give you quickly and the quality of your work is high, they’ll take notice.",3.0
g1biktz,i8sai8,"""Lion doesn't concern itself with the opinion of the sheep."" - Lord Tywin Lannister",2.0
g1bmhk8,i8sai8,"""...if they claim to be a wise man, then it means they surely don't know."" -- Kanasas

If I here someone claim to be an expert in something, I know they aren't. A true expert understands the vastness of what they still don't know, and thus won't claim expert status. Let them judge you by your work, not your words.",2.0
g1bjhm9,i8sai8,No need to be upset if you know the truth. Call them out on their mistakes when the opportunity exists. You will always have to prove yourself to every new group of people and it always takes some time (unless you are famous then you aren’t really new to that group).,1.0
g1bjyuh,i8sai8,"Here's my take but I preface this by saying I've worked in extremely competitive fields. They could be telling you this because they are insecure or they want to pay you less (and want you to accept,) or they don't ""know"" you; it could be anything. I wouldn't dwell on it as words imo mean nothing, pay is everything so if the pay reflects what seems basic, then leave. There's a lot of opportunities out there where you aren't shit on from the onset. I've worked at places like you mentioned and it generally doesn't get better, because the people that talk like that were in my impression doing it for some reason rather than it being harmless. Usually the reason was to have an excuse to pay less and dangle a carrot for you to work 2x as hard.",0.0
g1b8w3b,i8sai8,To be honest you don't owe anyone any explanation. Your work in coming future will show them what you are capable of and then they will be ashamed of how they treated you. This is no way to treat an intern (or anyone). I also joined a startup few months ago to work on ML/DL and my boss would tell others guy he is classical CV expert and all I knew was OpenCV (that gave me a lot of confidence and made the work environment so welcoming).,-1.0
g1bbm88,i8sai8,They always do that to interns in all companies around the world... It's just a shit hierarchical mindset. Don't worry about that.,-2.0
g19oii5,i8mne1,"Do we need these tutorials on this sub? I mean, if you ever try any of the libraries used for DL, all of these basic tutorials are already there.",4.0
g19oz1u,i8mne1,Valid but I thought of just putting it out there if someone thinks it's helpful for them they could use the resource.,2.0
g19pe02,i8mne1,"I'd understand if it was a new perspective on the well studied and implemented problem, but we already have quite a number of really well done and explained tutorial for this basic problem which don't even need any digging on the internet. I don't think reposting them here is necessary.",3.0
g19x6ev,i8mne1,"Hi,
can it be used to identify exact clothes from large set of images.",1.0
g1djn2v,i8mne1,I'm trying to figure this out too.   Any progress?,1.0
g197fd7,i8kltq,its per gpu the 3 usd/hour so 24/hour for 8 v100,4.0
g1acoqm,i8kltq,"That makes s lot of sense, thanks!
Even the sales person I called didn't know the cost was per GPU, or she didn't want to tell me.",1.0
g2dlmtn,i8kltq,[deleted],1.0
g2zuu3c,i8kltq,Do you have any link to your company?,1.0
g2zuy92,i8kltq,"Fluidstack.io , but now our price changed",1.0
g2zuz01,i8kltq,"**I found links in your comment that were not hyperlinked:**

* [Fluidstack.io](https://Fluidstack.io)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g2zuy92&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1.0
g2zv3kx,i8kltq,"I've already contacted you, maybe in around a month we'll be using your services as we're building our dataset",1.0
g3an7f5,i8kltq,"Thanks, can I have your email address please?",1.0
g183zkf,i8fkmi,"Things like padding, stride, and image size do not affect the number of parameters; so ignore those. The number of parameters for a layer depends on the filter size (5x5), the number of channels going in, and the number of channels going out.

The parameters are there to transform the input to the filter into a single vector, just like with a normal feed forward layer, except that the input is not a vector as usual but is shaped as a [filter height x filter width x input channels] tensor. The output will be a vector of shape [output channels]. 

Basically you just need to imagine that a filter is taking all of the [5 x 5 x input channels] values, turning them into a single vector, and multiplying that vector by a weight matrix and adding a bias. Given the first layer which receives 3 input channels (RGB) and outputs 50 channels (number of feature maps), how many parameters would you need for just that layer?",4.0
g187gdu,i8fkmi,"Thank You u/mtanti, it gave me the institution I needed, used the below formula to calculate

&gt;w = ((shape of width of the filter \* shape of height of the filter \* number of filters in the previous layer+1)\*number of filters)

Lowest Layer = 3800

Middle Layer = 187650

Top Layer = 1875500

&amp;#x200B;

Total Parameter = 3800 + 187650 + 1875500 = 2066950",2.0
g187s47,i8fkmi,Well done my friend. Keep it up!,3.0
g186r1v,i8fkmi,"First let's not confuse number of parameters with number of operations. A conv layer is a bunch of filters convolved over an image and its channels.

Things like stride and padding only affects operations and how the convolution happens but therefore have no effect on parameter number.

The number of parameter is simply the size of a filter times the number of filters. The size of a filter is the kernel size times the number of input channels.

Then add bias.",3.0
g1895rg,i8fkmi,"Thank you for the inputs. Apparently, I now understand how it works. :)",1.0
g17x9eg,i8f30q,"Any reason you’re not inferencing on a GPU? Yolov3 and v4 take maybe ~80-160ms on a good GPU.

CPU-based training and inferencing hasn’t been the norm for quite a long time now (specifically because it’s very, very slow). This type of work should really be done on a GPU.",1.0
g184l11,i8afru,Wait a month or so for ampere then decide,9.0
g18mnsz,i8afru,THIS.,2.0
g18nxsi,i8afru,now you have confused me :D,2.0
g18s0fh,i8afru,"On 1st September, nvidia will launch thier next generation of GPU. Its called Ampere. 

They are likely to be much faster, have much more ram, etc.

Some sites are speculating that the top gpu will have 24gb of vram.",3.0
g176fhn,i8afru,"I'm one of the founders of Lambda. We sell [Deep Learning Workstations](https://lambdalabs.com/deep-learning/workstations/4-gpu) and provide academic discounts. Just DMd you. Some advice:

* **GPU:** RTX 2080 Ti gives the best price/performance. It has enough VRAM to train every state-of-the-art convnet we've tested. It's my go-to recommendation for workstations.
* **CPU:**  Go with Intel Core i9 (e.g. 10900x). Intel Core typically beats Xeon in single threaded performance. Xeon is good if you need more than 256 GB of memory (which you almost certainly don't).
* **RAM:** Aim for at least 1 GB of system memory per GB of GPU memory. So, if you put four RTX 2080 Ti GPUs (11 GB VRAM each) in a workstation, you'll want 4 \* 11 GB = 44 GB of RAM.

EDIT to add AMD:

**AMD vs Intel**

* AMD offers more cores per dollar + PCIe 4.0.
* Intel CPUs often beat AMD in single threaded performance.
* The extra PCIe lanes offered by AMD help less than you might expect. Intel ""cheats"" by using PCIe switches to give x16 lanes per GPU. A couple years ago, Lambda benched Intel vs. AMD across a number of models and saw almost no difference. PCIe 4.0 may change this story. We have to benchmarks coming soon.
* If it were up to me, I’d choose AMD.",18.0
g17j8eb,i8afru,"Many convnets dont fit on RTX 2080 TI if you do things beyond imagenet. If you work on video data, high res images, point clouds and 3D volumes (eg medical data), 2080TI is a poor choice. The price of a better GPU (say a Titan RTX) is very much worth it given how it often shortens development cycles due to less memory juggling. If the card is too expensive, you can apply for an NVIDiA small hardware grant.

Core i9s can be problematic due to number of PCIe lanes. Many Ryzen chips have better long term value because they would allow multi GPU setups in the future if you need it. 

The amount of RAM you need is likely not only a direct function of the GPU size. Just go for 64GB of ram as a minimum, 128 if you can afford it. Again, the price is a drop in the bucket given the shorter development cycles (e.g. RAM caching of augmented datasets can result in massive speed-ups)",8.0
g18w9tt,i8afru,"I'm a big fan of Titan RTX as well - it's definitely more future proof. As you mentioned, it boils down to data type and network. The RTX 2080 Ti will get you quite far with FP16 (though that comes with its own set of problems). Among grad students at places like Stanford, it reigns supreme.

**System memory**

The 1:1 system ram vs GPU VRAM rule-of-thumb is a minimum requirement recommendation (I updated parent post to make that more clear). You're right that RAM is cheap relative to everything else, so most people just max it out. For those curious, you want at least 1:1 b/c datasets are often preprocessed on the fly and then held in memory before being loaded onto the GPU. If you have less system memory than GPU VRAM, then there is potential for CPU/storage bottlenecks.",3.0
g17mryg,i8afru,"How about a multi gpu approach in rtx2080ti series for example, it seems titan or tesla series is out of my budget?",1.0
g1831pp,i8afru,"Yes, that would work, but would need a bit more development and techniques such as model paralelism (maybe using gpype/torchpype). Single GPU with large memory is still easier development wise. 

Nonetheless, I'm arguing that the cost difference between a 2080TI and a Titan RTX (~$1-1.5k) will save you loads of dev time, which is very valuable in a research environment. I lead a academic research lab and we tend to overspend hardware wise to avoid silly time wastes caused by hardware limitations. For example, $1.5k is the cost of roughly 5-10 days of work for a junior researcher salary. I can guarantee that if you do anything more complicated that imagenet, you will waste a lot more than 10 days trying to fight hardware limitations.",6.0
g18o6ds,i8afru,"Yes it would be better to get lesser in quantity but higher in quality, i agree..",2.0
g17uotf,i8afru,"Imagenet is not SOTA anyway? 

Currently I don't expect NNs to grow much larger than 11GB. Maybe for NLP...",-1.0
g1839ew,i8afru,"They already do. Maybe not classification nets, but others definitley do. For example, GauGAN/SPADE from Nvidia requires 8GB+ of VRAM per image, and this is a 2D only application. The ""video"" version of the same network is now over 32GB vram.",2.0
g177ezd,i8afru,Thank you i have sent you an email.,2.0
g17mfh6,i8afru,Lambda is great. Go with advice above,2.0
g17tfir,i8afru,"No offense but Intel over AMD in 2020? Especially for not gaming purposes? Doesn't make any sense.

Apart from that I totally agree.",2.0
g19353r,i8afru,Hah. I would totally choose AMD over Intel. I sort of let the OP box me in with the “Xeon vs i7” statement. I edited parent to add my thoughts on AMD.,2.0
g17anhw,i8afru,Does lambda provide discounts for educational institutions?,1.0
g17apiw,i8afru,"Yes, we do!",2.0
g18aj0e,i8afru,"Wait until Sept 1st when the new NVIDIA GPUs are announced. It's likely there will be a consumer card that has 16GB, and possibly even 20GB of memory. Right now you'd need to buy a Quadro to get that, so this would be a huge upgrade in value per dollar for a workstation.",2.0
g18fjdc,i8afru,"Yeah the quadro 8000 is the only big card out there right now. the A100's you can get from some cloud providers but its still in alpha.

Pcie 4.0 + new announcment cards will be nice",1.0
g19eti4,i8afru,"We use two 2080 ti in our setup. The ability to train with mixed precision with RTX cards is really nice.

Tim Dettmers has a great blog on what hardware could be beneficial based on your goals.

[Tim Dettmers](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/)",2.0
g18smaz,i8afru,I meant i made my mind and now i am thinking y not wait 😛,1.0
g1969mm,i8afru,"Sidebar, why do you want on prem machines? I just got done setting up a research system for my lab and for our use, AWS was way cheaper and we were also awarded research credits which made the whole system free actually.  Is it a use the money or lose it situation?",1.0
g196jkm,i8afru,Yes indeed the budget lapses at the end of fiscal year! Btw how did you get research credits ? What grant can I apply to?,1.0
g19cthj,i8afru,We have a small lab and we’re awarded 35k.  I can check later tonight and add more context for our system.,2.0
g1adv9p,i8afru,That would be super helpful,1.0
g1afx4n,i8afru,"I think it was literally just AWS cloud credit application here https://aws.amazon.com/research-credits/

Also, our system is roughly a cheap M5.4xlarge (0.75/hr?) used for testing on sample data up most of the time, I shut it down at night if I remember.  Each user then runs their own jobs primarily through Sagemaker (with spot instances) or straight spot instances.  Data sits in s3 along with model checkpoints or cached partial results and logs (if we get booted on a spot we spin back up and restart).  Configs, envs are on EFS.  Everyone has their own AMI (usually just deep learning AMI + some stuff) and specific AMI's for tasks. 


I monitored GPU usage with a script for like 6mo and our usage was surprisingly low.  It was basically like 10% utilization for 80% of the time then ABUSE ALL GPUS (32x v100s) for 20% of the time.  So using a burstable system makes WAY more sense for out team.  Also, who runs jobs when you're writing or trying to address all the damn comments ugh... GPU's just sitting there.  If we bought a system we'd also have to manage it and we dun want to pay a data eng.",1.0
g17ynh7,i8afru,Why not pay for AWS compute time? A p3.xlarge EC2 instance is probably the minimum to get by and goes for $3/hr.,0.0
g180a4v,i8afru,"I am currently using both Microsoft Azure and GCP, VMs with Tesla V100 for my tasks .. but i have secured funds for workstations and i want to utilise it to build an in lab facility ..",5.0
g1946pn,i8afru,Or use [Lambda Cloud GPUs](https://lambdalabs.com/service/gpu-cloud). Better specs than p3 and half the price.,2.0
g198hcv,i8afru,Will definitely try it!,1.0
g17v54m,i8afru,Why don't you set up a server instead of individual workstations?,-1.0
g17vdlf,i8afru,What would that cost and would i be able to run multiple tasks on it simultaneously? I don't have a big budget .. can you point me to somewhere,0.0
g17w707,i8afru,"If you can have 5 workstations with Titan RTXs, then I guess you can have a pretty nice computation server, with your budget.

You can actually build your own DL server or you can call up any of the big companies like Dell, etc. - they build such servers as well. There are new, smaller companies which can assemble a new server as well. Just google.",3.0
g197n6f,i8afru,"Basically you mimic the big company’s in your lab this way. You build/buy one crazy good machine and split it up in virtual machines for your people. Takes a little more setup time but the flexibility/scalability might be worth it in the long run. 

You may even be able to utilize performance of idle VMs this way and boost the VMs that are running stuff; but I’m no expert on this.",2.0
g1aky93,i8afru,"Since, you're trying to figure out a setup for 5 RTX Titan workstations - that'd mean at least $18k-$20k conservatively. Just call some local PC company and see if they can assemble a lesser version of [this](https://www.nvidia.com/en-us/data-center/dgx-a100/?ncid=afm-chs-44270&amp;ranMID=44270&amp;ranEAID=TnL5HPStwNw&amp;ranSiteID=TnL5HPStwNw-v5.ST2Q02J6Rj1Hz8RFhqw) \-  Maybe with one A100 (40 GB VRAM), 1 AMD EPYC 7002/Xeon, 128 gigs of RAM, and as much of hybrid storage memory you want. Should be within similar price (maybe call NVIDIA for a academic discount or something).",1.0
g18cjqm,i86y6v,"From my experience that can be quite hard. Many papers just don't go enough into detail for re-implementation to being with. The step of choosing what paper to implement is therefore very important.

Significant papers should provide code the implementation that was used to generate results shown in the paper. Being able to reproduce results is an important sign for high quality research. 

If you are not confident in the topic yet I would suggest to start by getting existing code to run, understand it in detail and modify it to your needs.",3.0
g17or21,i86y6v,"A great way to do this is to find a group of graduate students who can help/guide you. Since you're an undergrad, see if yoou can sign up for a 'research' class for credit under a professor. I did this for a couple of semesters when I was an undergrad and it helped tremendously. The professor you choose to work with will help guide you too.",1.0
g18o5l8,i86y6v,"In my experience it varies from paper to paper. Not all papers are written equally. "" [**In a survey of 1,500 scientists**](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970), 77% of biologists and 87% of chemists said that they had tried and failed to replicate published research findings."" Now whilst that figure is more about science then machine learning it shows the general consensus. There are a lot of papers that are either near impossible to replicate (missing a few key details) or are lying about their results. 

So my advice is to try replicating famous papers, ones that countless people have been able to replicate without difficulty. Once you have a better understanding on how people write up their project in papers, it will be easier to replicate other ones.",1.0
g174v2l,i86y6v, What is your educational background?,1.0
g174vrl,i86y6v,3rd year undergraduate,1.0
g175bwn,i86y6v,"Okay honestly speaking, I would highly recommend pursuing a PhD/masters in ML/RL/DL. The chances of one getting hired as a deep learning engineer are slim to none with just an undergrad ..and as far your post goes, you need to able to do far more advanced stuff than implementing a deep Q  to be able to find a job as a deep learning engineer. Sorry for the harsh words. I tried that as an undergrad myself but wasted a lot of time applying for jobs etc with no success. Everyone said I needed to know a lot more than basic CV/DL skills.  Then I enrolled in a PhD program for CV to learn more.",3.0
g17c6um,i86y6v,"Do not listen to Life_Breath. Ridiculous advice. Unless you are actually interested in pursuing a PhD. Do not make a decision like that simply because some stranger on the internet told you it was required to do work in a field that has many examples of self taught individuals. Personally, I think your best bet is to read the new Fast.ai book. In the later chapters, it breaks down complex ideas from papers and explains them in code.  Here it is, [Deep Learning for Coders with Fastai and PyTorch](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527/ref=mp_s_a_1_3?dchild=1&amp;keywords=Deep+Learning+for+Coders+with+Fastai+and+PyTorch&amp;qid=1597231022&amp;sr=8-3).",1.0
g17c7cc,i86y6v,"**I found links in your comment that were not hyperlinked:**

* [Fast.ai](https://Fast.ai)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g17c6um&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1.0
g191fmq,i86y6v,"While I love Jeremy Howard's lectures and learned a lot from him, its mostly about how to use [Fast.AI](https://Fast.AI) instead of a deep dive of Computer Vision.  There is a huge lack of theory in his lectures.  I was left completely clueless on CNNs when I underwent an interview for a fellowship.",1.0
g1bz22c,i86y6v,"Theory is something you should be able to understand before implementing papers, understanding what is to be done is important.",1.0
g14ldjs,i7vq1r,"Looks awesome. Is it totally free? I clicked on the curriculum link that required me to sign up via Google or LinkedIn. I did not sign coz Usually that's followed by premium subscription messages. Is it free or some stuff will be behind a pay wall.? If there are premium services behind the sign in, how much for that? I liked what I saw, very interested.",4.0
g155ib8,i7vq1r,"It’s totally free! The sign-in is just so we have a way of helping people track their performance on exams and questions :)

We built this platform to help people prepare for their interviews based on our experiences successfully getting ML and data science jobs at different companies. Check it out and let us know  how we can improve it even further!",3.0
g15fw6t,i7vq1r,Oh cool. Thanks for the reply. Excited to try it out.,1.0
g15a4qq,i7vq1r,[deleted],2.0
g16i5rl,i7vq1r,Thanks!,1.0
g15l2z6,i7vq1r,Very cool. Thanks for contributing this to the community!,2.0
g16i85k,i7vq1r,Absolutely! Happy to help :),1.0
g1636xh,i7vq1r,"Very cool, just signed up!",2.0
g16iamh,i7vq1r,Glad to hear that! Let us know about your experiences using the platform!,1.0
g16zkbk,i7vq1r,[removed],2.0
g16zywk,i7vq1r,Glad it's useful. Let us know if you have any feedback/comments!,1.0
g170w2w,i7vq1r,Had been looking for this for a long time. Will sign-up today and check it out.,2.0
g182ijg,i7vq1r,Thanks! Let us know if you have any suggestions or feedback :),1.0
g16i1w4,i7vq1r,Will we also get the solution after the test?,1.0
g16ieb9,i7vq1r,Yes we have solutions for almost all the questions. If it would be helpful to the community we can get the last few solutions out as well.,1.0
g16rcbn,i7vq1r,"Ok , it was a nice idea",1.0
g171uxm,i7vq1r,"Great UI and content. You might have to look into integration with back-end compiler. Have submitted python coding solution multiple times, still stuck at ""Running code..."" phase.

Thanks!",1.0
g182fdw,i7vq1r,Thanks! And thanks for the suggestion. Can you point out which question is stuck at that phase so we can look into it?,1.0
g18trbf,i7vq1r,"Numpy Calisthenics II

Thanks for looking into it.",1.0
g19tejw,i7vq1r,Thanks for the info. Was this mobile or desktop (and what browser)?,1.0
g1amdy2,i7vq1r,Desktop with Firefox browser.,1.0
g1ef2ba,i7vq1r,Got it. We'll check it out. Thanks for the info!,2.0
g18cwhv,i7vq1r,"Is there only one question in the Deep Learning track?

&amp;#x200B;

[https://www.confetti.ai/questions/40-0](https://www.confetti.ai/questions/40-0)

&amp;#x200B;

Video: [https://imgur.com/a/TX2JavG](https://imgur.com/a/TX2JavG)",1.0
g18iqyd,i7vq1r,"No there's a lot more! :) If you go to the ""Questions"" tab and filter by ""deep-learning"" you can find all the questions on that topic.",1.0
g13toow,i7t1zo,To reduce overshooting.,1.0
g13tx04,i7t1zo,But shouldn't we take large steps to escape a flat region?,1.0
g16weix,i7t1zo,"The region is mostly flat. You don't try to escape the flat region, you try to enter in a cavity in the flat region.",2.0
g13bnlb,i7q4wl,The paper:  [https://arxiv.org/abs/2006.03511](https://arxiv.org/abs/2006.03511),1.0
g136l6v,i7p5gr,"Give a portrait face, move the gaze up

Github linking: [https://github.com/zhangqianhui/GazeAnimation](https://github.com/zhangqianhui/GazeAnimation)

arxiv: [https://arxiv.org/abs/2008.03834](https://arxiv.org/abs/2008.03834)

&amp;#x200B;

Paper has been accepted by ACMMM2020",1.0
g132ape,i7notd,"Lol I Googled the title of your post and about 8 papers showed up immediately, have you looked at those?",20.0
g15uydg,i7notd,I looked at some but they were not what I searched for. Can you propose one?,1.0
g15xbwc,i7notd,"Unfortunately that's the extent of my ""helpfulness"" 😅 Hope my original comment didn't come across badly, I was honestly considering that you might not have looked!",1.0
g137nm0,i7notd,"Did you actually check how many operations would a  NN take, even for fixed-size matrix, relative to existing approximate eigendecomposition algorithms like QR factorization?",3.0
g13eec7,i7notd,"You had better research what the standard method for inverting a high dimensional matrix is

AFAIK, they are iterative approximate methods

CF krylov subspace",3.0
g1332t8,i7notd,"With the exception of recurrent neural networks, most machine learning architecture takes an input of a fixed size, I was wondering how you would address this? One lazy fix I can see to this is to interpret the matrix as a stream numbers with the last entry being followed by an escape character. This way you can train an RNN-like architecture on many different matrix sizes. I believe this would be hard as RNNs have a problem keeping track of very long dependencies. 

If you plan to learn a matrix of a fixed size then this no longer is an issue, however you still run into the issue of having no guarantees on the worst case or even average case performance of your model. With deterministic matrix inversion techniques, worst case performance of the method can be bounded using the condition number of the matrix. Also unless you are working in a bounded subset of $\mathbb{R}^{n\times n}$ or are sampling matrices from some non-uniform, exponentially decaying distribution, you will not be able to use the empirical error average as an estimate of average case performance, even in the large sample regime.",2.0
g137q0x,i7notd,"So, in order to come up with a somehow useful network, you need a big amount of large matrix/inverse matrix pairs, which is computationally expensive in the first place. And you of course would have a high dimensionality problem, which you cannot escape by using convolutions for instance. I doubt that estimator would work great, but happy to be shown otherwise.",2.0
g13oxxx,i7notd,"This is is a bad idea for many reasons, not least of which because the times when you actually need the inverse matrix are extremely limited.",1.0
g144wye,i7notd,"Look up implicit restart Lanczos iteration. You’ll realize that outside of linear algebra 101, no one is inverting large matrices for SVD.",1.0
g166x5x,i7notd,KDD 2019 Network Density of States,1.0
g12lrmp,i7l5xv,"- do you do anything to the learning rate?
- are the records shuffled?
- are the records shuffled between epochs?",1.0
g12urfu,i7l5xv,"*  do you do anything to the learning rate?   
 I use Adam optimizer with initail learning rate 0.001
* are the records shuffled between epochs?   
Yes, I thing this might be the reason. But why?",1.0
g13g9z3,i7l5xv,Just a thought but is it possible youre reporting the loss for every batch and are averaging it over the batches over time and only resetting at the end of the epoch?,1.0
g1enkqx,i7l5xv,"I might got the explanation. In every epoch, the training datas have been trained once then the model would be optimized. And in the next epoch, because the same data have been trained in previous epoch so it would have less loss in this epoch. And so is others datas, but they maybe have same degree of declining, then make the loss curve looks like this.",1.0
g1er45k,i7l5xv,"If you can share your code, I can take a look. I can't understand what you're trying to say",1.0
g1eyhyy,i7l5xv,"Forgive me for not being good at expressing in English. The original code is an example of a textbook.

[https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/ch04/train.py](https://github.com/oreilly-japan/deep-learning-from-scratch-2/blob/master/ch04/train.py)  

Thank you.",1.0
g1ft5ia,i7l5xv,"The trainer code looks fine. Can you try reporting the loss only for full epochs and compare? I think some intra-epoch variability can be expected. What's weird is that despite the shuffle every epoch the same pattern appears in all the ""steps"" of your loss plot",1.0
g1gshvy,i7l5xv,"Thanks, I would make a try.",1.0
g1jrbr2,i7l5xv,"I have training the model again with more epoch. Here is loss plot I save. 

[https://github.com/KarPhoon/loss-plot](https://github.com/KarPhoon/loss-plot)",1.0
g78ro7v,i7l5xv,"I think your explanation is correct!  According to your explanation the steps indicate overfitting, i.e. the model remembers the training data points without generalizing to the other datapoints.  This causes a sharp decline after each epoch.",1.0
g1325y7,i77hld,"Note for Keras users: iNNvestigate provides some other XAI methods https://github.com/albermax/innvestigate

But very nice to include GradCAM, however since there are GradCAM+++ and GuidedGradCAM+++ you might want to look to include these as well and an Activation Maximization based method, if you do not just want to understand the decision, but the learned class. Keep up the good work.",2.0
g138kbm,i77hld,"I would definitely include GradCam+++ module in the next version of this package. I am thinking of adding Feature Inversion too. 

Thank you for your valuable suggestions.",1.0
g12a7hc,i77hld,Note for pytorch users you have utkuozbulak/pytorch-cnn-visualizations.,2.0
g10grd9,i77hld,Nice! Will definitely clone it and try it.,2.0
g11k3b0,i77hld,Thank you so much. Please give me more suggestions for improving it. And if you're facing any issues please feel free to contact me.,1.0
g11kgsf,i77hld,Will do. Already starred it.,1.0
g11oxjt,i77hld,Thanks mate ☺️,0.0
g10l1vh,i77hld,"Nice! Looks interesting but as a none expert in CNN, could you provide some explanation on each components? What is their use? How do you use them? Etc",2.0
g11jz5y,i77hld,"My python package is relatively a very small package containing only 4 classes. First class is GradVisualiser, it helps users to visualise gradients using Guided Backpropagation. Actually the main purpose of this algorithm is to tell us what are the gradients which are having a positive impact on our task (such as classification).  

Next class is GRADCAM. It helps users to generate heatmap. I have also added the paper reference for this algorithm. You can use this as a overlay on your own image. This will help users what are the pixels having a positive impact.


Next class is Vanilla gradients. This call generates vanilla gradients for users.

Last class is Intermediate Features. This class is a little buggy but it helps users to visualise intermediate feature map of CNNs.


I have also attached an ipynb file as a tutorial for using all these classes. Please look into that for more details.",2.0
g10n01j,i77hld,Thanks for the tool. Was looking for this for a very long time.,1.0
g11hvya,i77hld,"Thank you so much. If you have any suggestions  or if you are facing any problems, please reach out to me.",1.0
g10cnyl,i72boz,"It seems the implemented networks for keras and pytorch are different from each other.
How can this be used as a performance 
comparison?",2.0
g12b9pq,i70m45,"How exactly are you also flipping the keypoints ? Because there is an easy mistake to make here. 

Let's say you are looking at the left eye. If you simply mirror along the Y axis both the image and the landmark then your landmark will be on what used to be the left eye... but is now the right eye ! And so on top of mirroring the coordinates you have to also permute the coordinates.

Example: right eye x=10 left eye x=20 on the left of the picture. After mirroring coordinates maybe you get right eye x=120 left eye=110 on the right of the picture. Then you have to swap the two to have right eye=110 and left eye=120.

If X_k is right eye make sure it still on the right after flipping.",2.0
g12vgh6,i70m45,"Yeah, I totally got your point, and **this is the exact reason why model is not learning**.

Everything is working now 👍

Thanks a ton.",1.0
g0z5ewi,i70m45,"Just flip the labels too, should solve the problem for you.",1.0
g0z5qwo,i70m45,I'm already doin it.,1.0
g0zp7px,i6zske,would be nice if you could add coding for it as well,2.0
g0zpr7y,i6zske,That’s a good suggestion. I am planning on hands-on playlist as well that I will record on my laptop. Currently this is on iPad which is the limitation as of now.,1.0
g12nqfl,i6zske,there are plenty of codes for deep learning and machine learning. so i suggest dont do that. what is missing is the coding for fundamentals. that is what is missing. if you ask anyone to build a classifier they can find the code in a minute. but if you ask write a code for MGD they are like i dont know. ao try to focus on fundamentals not just hands on,2.0
g0y4oms,i6u942,"Idk if this may sound clique, but based on the reading/chapter, I will look out for keywords, ideas, or themes to focus on. I will then take my notes based on these simple phrases or ideas and then expand my notes off of these",4.0
g0zsixs,i6u942,"What I like to do is take notes in ""levels"". That is, I first read the paper without worrying about details and get the bigger ideas. Write that. I then reread the paper going a level down, detailing the ideas I had previously written. I repeat this as long as I believe there are details I didn't grasp. 

I also think it's very important to try reimplement at least part of the paper. At least for me no matter how many times I read something, if I don't do it myself I won't get a good understand of it.",2.0
g0yultb,i6u942,"Something that I've been doing recently that's helped me is to force myself to handwrite notes while reading the paper and then type them in a different format in a spreadsheet (with some columns like summary, main contributions, critiques, etc.). This forces me to take notes in the first place, but also to go through my notes and make sure that I can concisely state what the paper was about it, what it's main contributions were, and some questions and ideas for future directions I have about it. If your read a lot of papers, you're bound to forget some of the technical details, but in my opinion being able to write 8-10 sentences about the paper helps me organize my thoughts so that the framework is there for going back later and diving back into more mechanical aspects.",1.0
g0y7pyd,i6pwll,"Hmmm doesn’t seem to work on mobile (there’s no search button), but seems like cool idea :)",1.0
g0x4hvx,i6ourt,"Why not try to do something with RL + CV, or RL + NLP?",1.0
g0x5mwu,i6ourt,"Thanks for the comment. Is RL + CV or RL + NLP even applicable? In my knowledge, RL is mainly applied in games (go, dota2) or robotics control. Is there any paper that explore RL + CV or RL + NLP that I can reference to? 

Thanks",1.0
g0x9h9r,i6ourt,I mean many of those RL-based approaches to playing games rely on CV to observe the state of the game - so it’s highly applicable. Take a look at a lot of the openai gym stuff.,1.0
g0x9kr7,i6ourt,You can use RL for attention based models. CV and NLP are both applications of deep learning while RL is a technique.,1.0
g0yue56,i6ourt,"[rl + cv](https://youtu.be/PYylPRX6z4Q)

I wish I could spend my time doing stuff like this.",1.0
g16gv00,i6oo23,"&gt;image thresholding. It simplifies the image for easy analysis.

It simplifies nothing as it doesn't disentangle structure, texture, and lighting.",1.0
g0yk2fw,i6mybu,"Yes, you can find my review here: https://paste.ubuntu.com/p/MbDF2DQqk2/",2.0
g0yl5hd,i6mybu,Thanks!,1.0
g0vva48,i6i092,Neural style transfer is something like this only right?,1.0
g0w8a9k,i6h67f,[deleted],1.0
g0w8vq0,i6h67f,Seriously... explain yourself OP,2.0
g0w8y0b,i6h67f,Wth the help of onnx we can deploy our models on different platforms,0.0
g0vho4y,i6fbjz,"Another article promoting ignoring the requirements of the actual application and just maximizing some artificial score.

Yay.

&amp;#x200B;

Btw. the textual description of mAP, AP and the figure below are contradictory.",2.0
g0ubegg,i5ygd8,"I wonder if this content is relevant or interesting. If only the OP wasn’t nothing more than a link to a video.

Would this content be relevant to me as an introduction? Is this SOTA research only relevant to those directly doing this research?",0.0
g0v4msu,i5ygd8,"Hi, this content is relevant and a part of the Graduate Level course offered by DeepEigen \[Introduction to Robotics &amp; Visual Navigation\], and the video is a part of public lectures from DeepEigen, 

You can check out more: [DeepEigen Public Lectures](https://www.youtube.com/channel/UCZTd22jmaDPUM2E6QbWc5gg/playlists)

And as the content is from Module I: Computer Vision of the course, so yes for someone starting in Computer Vision the content is relevant as an introduction \[bit more if you see DeepEigen's public lectures each concept is explained mathematically, so that everyone can understand the concept with its mathematical significance\].

As the content is related to Computer Vision so, yes it is relevant to those who are doing research in Computer Vision \[Robotics, Deep Learning....\].",2.0
g0w3eq6,i5ygd8,Thank you. If only you had provided this information when you posted the content browsers of this forum could have read if it was relevant or not.,1.0
g0s7tyc,i5xtcp,Arxiv.org,5.0
g0s7uk1,i5xtcp,"**I found links in your comment that were not hyperlinked:**

* [Arxiv.org](https://Arxiv.org)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g0s7tyc&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",2.0
g0sgl5q,i5vheb,"It looks like you're trying to make money with affiliate marketing. They're all affiliate links, right?

In my opinion, this is low quality advice. You just picked the obvious courses from [coursera.com](https://coursera.com/) in an attempt to make money. Free advertisement on reddit. Good job.",2.0
g0sft6d,i5vheb,"I would recommend http://ocw.uci.edu/courses/math_4_math_for_economists.html . Don't mind the name, it is a great course for linear algebra and calculus part of machine learning.",1.0
g0rwk11,i5vebn,"I think deep learning can be both easy and difficult depending on situation.

&amp;#x200B;

If you are given a well-labeled dataset and need to make simply a good enough model for text/image classification? Easy.

If you don't have enough data or it is badly labeled, then most time will be spent on the fixing the data, time spent on the model will be the least.

&amp;#x200B;

But if you need to make a model with high performance, if it needs to be stable, if it requires changes to architecture - it will be difficult.",4.0
g0s85v6,i5vebn,"because of the enormous amount of fancy looking demos on Youtube or papers, people already got used to most of the things. Tell them that There's a difference between knowing the path and walking the path!! \[Reference\]([https://www.youtube.com/watch?v=Kz40vwcTGFo](https://www.youtube.com/watch?v=Kz40vwcTGFo))",3.0
g0sxeb6,i5vebn,Exactly.. I couldn't agree more.,2.0
g0tpga6,i5vebn,"I think it's correct. The way we practice deep learning nowadays is more like we practice PyTorch or TensorFlow or whatever. Not for anyone fault, it's just how it is. It's unclear if anyone in the world even has any idea how deep nets truly work. 

The actual ""difficult"" parts of DL often have nothing to do with DL. Hardware issues. Scaling issues. Data issues. None of this has anything to do with deep learning, they are just necessary evils.",1.0
g0v95ox,i5vebn,"I think almost every profession has its challenges. However, sometimes I forget this, and it's only after talking to somebody in that profession about what they do that I begin to see the complexities they face. I think everybody has a bit of a general bias towards either thinking that other professions are easy or hard, and often not a small amount of ego gets tied into that bias. So I wouldn't let it worry me.

Based on that, I think one thing that can help is to allow others to walk a small piece of the journey with you. Talk with this colleague about what you do on a regular basis. The next time you encounter a head-scratcher problem just call them over and ask if they have any thoughts. Either it really is easy to them and voila, you have a potentially good idea to solve the problem - or - they will struggle with the problem too and they are your ally in the trenches rather than being the guy just saying it is so easy. And if they just shrug it off and decline without good reason - well, there's a good chance they're bluffing and now you know.

Good luck!",1.0
g0rh8nr,i5tgyg, Virtual handball,1.0
g0qnxq1,i5mvqm,"I’d treat this as a multi-class, multi-output problem, run a sigmoid on the elements in your last layer and use whatever the library you’re using offers for that type of problem. Looks like nn.BCEWithLogitsLoss would work in pytorch (from this thread: https://discuss.pytorch.org/t/is-there-an-example-for-multi-class-multilabel-classification-in-pytorch/53579/8)",3.0
g0qojhq,i5mvqm,"Basically treat the output classes as independent, then just take the one with the highest probability, difference between this and the multi class cross entropy being that you won’t have a probability distribution across all of your classes, but an independent probability for each class. 

You could also potentially independently calculate class dependent thresholds to target some specific precision/recall targets for each class. Piggybacking on another comment here, you might classify something as a tool if that class probability is very high, but the class “eating utensil” might need a lower threshold because it’s a more specific label (and might have more  specific contextual clues in your input data)",1.0
g0qd761,i5mvqm,"Personally, I would assign the pattern to all the categories, so that the ""most correct"" response (i.e., the one with the smallest loss) is generated when the model associates the pattern with all possible correct categories. For example if the possible categories were TOOL, INSTRUMENT and FRUIT, and one of the patterns was a SPOON, which is a kind of tool, but also people use spoons as a rustic instrument, then it is arguable that associating SPOON with both TOOL and INSTRUMENT reflects a more thorough knowledge of the item. It would also force the model to learn the features that are distinguishing for a category (e.g., why can a SPOON be in two categories, but a SCREWDRIVER is only a TOOL?)",1.0
g0oebmk,i5dnp5,"🔥 Want to master statistical computing?

💪 Learn to analyze data and perform analytical operations like a pro with the R programming language.

👇 In this [**OdinSchool**](https://www.youtube.com/channel/UCtOOo75kcRq_IFNVpHOUeLA) free tutorial series on R programming language by Varun Rao, Programming Language Expert, you will learn some of the most crucial fundamentals of the R programming language:

⭐ Inbuilt functions

⭐ Operators in R

⭐ Data exploration in R

⭐ Data structures in R

⭐ Main data manipulation

⭐ Data visualization using R

⭐ Histogram

⭐ Density plot

⭐ Bar graph

⭐ Hands-on training on R

⭐ Box plot

⭐ Scatter plot

⭐ Facet grid

⭐ Mini-project on R

Like what you see? [**Please Subscribe to OdinSchool**](https://www.youtube.com/channel/UCtOOo75kcRq_IFNVpHOUeLA) and get more free tutorials. Hit the bell icon to stay updated.",-1.0
g0ns0pb,i5af28,"Wow nice stuff 
Do u have a GitHub link",1.0
g0pm55g,i5af28,"lost opportunity, why no 🖖or🖕",1.0
g0nw96b,i5a8ep,"Hahaha this is the most unique ""tutorial"" I've ever seen. The tiny guy in the corner slowly becoming more satisfied is hilarious.

This belongs on r/DeepIntoYouTube",4.0
g0m9xel,i4wbvc,"Awesome.

It makes it really easy to find mislabeled pictures:

https://polygrid.com/demos/imageNet#i=n00440382_1300&amp;d=13&amp;s=200&amp;x=69&amp;y=43    
https://polygrid.com/demos/imageNet#i=n00443231_7406&amp;d=13&amp;s=476&amp;x=58&amp;y=46
https://polygrid.com/demos/imageNet#i=n07607967_7594&amp;d=13&amp;s=476&amp;x=50&amp;y=59

Why TF are there so many mislabeled images in there?

Is this the first reasonable tool to browse them?",3.0
g0rbtfk,i4wbvc,First reasonable tool: to my knowledge yes,1.0
g0nb3r1,i4wbvc,This was really cool!,1.0
g0maciz,i4w005,can it do machine translation,2.0
g0mkokx,i4w005,"This is mainly for classification tasks. It might quite challenging to apply this method to machine translation directly as it's free text to free text. If you are interested in applying other semi-supervised or unsupervised learning method for MT, you can consider back translation, which only require a few or none translation pairs.",1.0
g0m7tzm,i4vh5c,"Awesome, very informative",1.0
g0n9zd0,i4vh5c,Glad you like it,1.0
g0l67nf,i4v2mh,"building a dl rig is cheaper than get even a mid-tier laptop(which is low tier on desktop specs)

if you are specifically interested in dl, then just run ssh into your dl machine, and you can use your laptop on the go.  

also, think about dl rig in terms on vram on gpu not necessarily compute. build a cheap gpu vector for carrying your gpu.  get a e5-1650v2 (6cores) for $20 and cheap ddr3 ram+mobo x79, and the rest on a quality psu and cuda-based gpu.  

ie.  

8gb cards - 1070, 1080,2060 super, 2070, 2080

11gb cards - 1080ti, 2080ti

16gb cards - v100, rtx5000

24gb cards - rtx titan, rtx 6000

48gb card - rtx 8000",4.0
g0kzdr2,i4v2mh,If you're serious about deep learning and gaming better assemble a pc with top end gaming gpu like 2080 ti. It will be way cheaper than using cloud + buying a mid tier laptop.,2.0
g0ld581,i4sdk5,What? Please show some of your actual data and describe the application,1.0
g0levny,i4sdk5,"Umm. Check this example.

° This feature is bad. I wish it could be better. | 2/5 | -0.6
° Please fix your app. | 3/5 | -0.2


^ Sentence| tag1 | label",1.0
g0lgk1k,i4sdk5,"So, sentiment analysis of comments + a 1-5 rating?",1.0
g0li3qf,i4sdk5,"Just an example. So basically sentence + tags to a class.
I need a representation for sentence + tags, what should I do",1.0
g0ll0nm,i4sdk5,"\-0.6 and -0.2 as results seem like what you want is not classification, but regression.

&amp;#x200B;

A simple way to combine the sentence and the rating would be to just do sentiment analysis on the sentence and combine the numeric output of that with the 1-5 rating. Could be as simple as a (weighted) sum for starting.",1.0
g0ll8c8,i4sdk5,"Sorry, but that for just for the idea.
I don't have cardinal values. I have textual tags. I shpuld have saod that before sorry.

So sentence , some textual tags, to predict a label.

Hence classification.

I have no clue how to go about it",1.0
g0lrgr8,i4sdk5,"Then why didn't you give an actual example, instead of some unrelated fake data?

If you think the data is too sensitive too share, then asking for free help on reddit is the wrong thing to do.",1.0
g0lrssv,i4sdk5,":(
My bad dude. I was trying to make things simple. My idea was to take a rough idea.
Sorry if I agitated you.",1.0
g0lsccj,i4sdk5,"A general rule of thumb: If you don't know what you are doing, don't try to abstract things. It will go wrong and make it impossible to help.

Just explain openly what you have and what you want to do. No proxy-questions.",1.0
g0lsunr,i4sdk5,"I've some product descriptions, with tags like what product are they, I need to classify them into classes of types of products.",1.0
g0j47d1,i4kxxk,I’d recommend checking out the data feeds of your local area. Your city county or state data has potential for you and your team to provide ML insights and new analysis. Correlate gov data with external sources for extra snazz.,6.0
g0kg9dg,i4kxxk,"Maybe, you can find these interesting.

1.Explainability on image classification/segmentation,

2.Poetry translation 

3. Action recognition from video

4.  Next word prediction for really long sentences

5. Low resource language translation  etc.",3.0
g0kpula,i4kxxk,"&gt;spend 1 month to form a team of 3-4 to do a DL project.

That depends much on your team.

Pick a real-world problem that your team is interested in.",1.0
g0l4o22,i4kxxk,deep fake,1.0
g0h63ic,i49zqz,Whats is the advantage over nvidia-dali?,1.0
g0hbdjb,i49zqz,"YogaDL has a different, complementary aim compared to Dali.  Dali is meant to be a high-performance data manipulation / augmentation API, where as YogaDL is focused on providing random access for datasets which results in efficient shuffling and sharding.  You could, for instance, use YogaDL to read from disk with proper random-access patterns, then feed that into a high-performance augmentation pipeline via `nvidia.dali.fn.external_source`.  So far, YogaDL has focused on working with tf.data input pipelines, so as of today that would require a little bit of customization, but that sort of flexibility of is what YogaDL is aiming for and will get easier in future versions.",1.0
g0hbqux,i49zqz,"&gt;Determined

Could I say yogaDL can be a better option when training a high amount of data on the cloud? rather inference it is focused on training speedup.",1.0
g0hcaka,i49zqz,"Yeah, YogaDL (and Determined) are primarily training-focused.

\&gt; yogaDL can be a better option when training a high amount of data on the cloud?

As I mentioned, I think you could use YogaDL and Dali together, as they are mostly aimed at different goals. But certainly a good use-case for YogaDL would be for efficiently accessing training data in a cloud environment.",1.0
g0hcfil,i49zqz,cool thanks. Are you guys maintaining any FQA? it is just slack?,1.0
g0he2bc,i49zqz,"&gt;FQA

Not sure what that is :)

Slack is a great way to talk to us / get support; there's also the [docs](https://yogadl.readthedocs.io/en/latest/) and a [GitHub repo](https://github.com/determined-ai/yogadl) where you can file issues / send PRs.",1.0
g0v9kok,i49zqz,Can you add as example on how to use YogaDL API with tf.Estimator API as well?,1.0
g0xsgad,i49zqz,"Sure -- you can find an example of using YogaDL with tf.Estimator (inside the [Determined](https://determined.ai/) training platform) here:

https://github.com/determined-ai/determined/blob/master/examples/experimental/trial/data_layer_mnist_estimator/model_def.py

If you want to use YogaDL with tf.Estimator outside of Determined, that would be easy to do by modifying the example above.",1.0
g143v7m,i43ztf," 

Transfer learning is a part of a machine learning technique that focuses on storing and gaining knowledge, i.e. learning and solving a problem faced in one project/model and applying it to other projects/models to minimize the issues and maximize the results.

For example – the knowledge gained to recognize cars can also be used to recognize trucks.

## There are two important types of Transfer Learning:

* **Near Transfer** – It is a technique where one can apply learnings from one project into another project, which is very similar.
* **Far Transfer** – It is a technique where learnings can be applied to another project in a different context. This type of learning is critical as it helps the users to apply those learnings in all kinds of the situation be it simple or critical

Read the in-depth [post here](https://www.yourtechdiet.com/blogs/transfer-learning-work/?utm_source=RD&amp;utm_medium=SM&amp;utm_campaign=RS)",1.0
g0gwcpt,i437pt,Kind of misleading title... It's not an NLP model... It's an architecture/structure originally used for NLP.,5.0
g0gx1lv,i437pt,"Exactly! I'm sorry for this ""shortening"" the YouTube titles only has 100 characters ahah!",1.0
g0hosqc,i437pt,Wouldn’t it depend on what data you trained it on? You can make AI do anything with enough training?,1.0
g0iprom,i437pt,"I totally agree.  Look at the bottom row, where rivers and lakes were added in to many of the re-creations even though the original had no bodies of water at all.",1.0
g0j13oy,i437pt,Link for paper?,1.0
g0jn1vb,i437pt,It's linked in my video covering it! You can find it on their project's page; https://openai.com/blog/image-gpt/,1.0
g0feyjr,i422xg,What a weird problem statement... Could u elaborate more?,2.0
g0fg3om,i422xg,"Just to clarify, you mean like [that](https://geekologie.com/2016/06/07/super-mario-plumbing-job.jpg)?",1.0
g0fogle,i422xg,Not entirely. Using custom built container and tubes. Really cool tho!,1.0
g0frlcy,i422xg,"I mean, I sort of get the idea for the assignment - but don’t you think you should go off and independently learn a bit about the math behind NNs first? There are a myriad of resources out there - I mean, just google “basic neural network”?",1.0
g0faojh,i416za,"1) Yes and yes
2) No it doesn't receives anything random features.",2.0
g0fdoq7,i416za,"1. Yes to both
2. You can design it however you want, but normally you’d link every neuron in each layer to every neuron in the next one: synapses handle the strength of the connection (which can be 0, disabling the connection). Using random information instead would be unusual though",2.0
g0fiwco,i416za,"1) You can set the number of neurons in any of the hidden layers independent of the others. That being said, the advantage of using the same number of neurons, is the ability of using residual connections, which in my experience have significant improvement in performance and lead to faster convergence atleast in the deeper networks.

2) In a multilayer perceptron, yes all the neurons in consecutive layers are connected. However, there are architectures (such as convolutional networks) that only connect some neurons. This however is not done randomly, it is done when there is some structure in the data and some notion of “locality”, like nearby pixels in an image or nearby words in text. 

You might also find “prunning” interesting. The idea here is to remove unnecessary connections for better efficiency. Because of weight decay many of the weights are very close to 0 at convergence, you can safely remove these connections without affecting the performance.",2.0
g0chbav,i3muc4,"Just in case you are don't know, there is a leaderboard with scores here:  [https://paperswithcode.com/sota/image-classification-on-cifar-10](https://paperswithcode.com/sota/image-classification-on-cifar-10) 

ResNet+ELU gives  94.4, Wide ResNet 96.11, TResNet-XL 99.",1.0
g0et6qu,i3muc4,"I am aware of the leaderboard and that several reported scores exceed the accuracy I have obtained. But please note that the maximum accuracies reported in those papers come from using either the 110 layer ResNet or ImageNet pre-trained checkpoints. In contrast, my results are reported for a 20 layer ResNet trained from scratch. For a more accurate comparison, the 20 layer ResNet + ELU gives 91.68, while the improvements that I have worked upon give the same model, an accuracy of 93.10.

I am looking to update my repo with results from the deeper models in the future. My hope is that the improvements seen in the shallow models, translate into similar improvements in the deeper model.",2.0
g0eu2zi,i3muc4,"Now I understand, good luck with your efforts then!",1.0
g0f4nal,i3muc4,Thank you!,1.0
g0f1hk7,i3muc4,What do you think about pyramidnet or resnext (8-20) layers.,1.0
g0f50eb,i3muc4,"Interesting! I hadn't considered using them up until you brought it up here. Tbh I've not used either PyramidNet or ResNeXts too much. But its definitely something that is worth trying. A custom ResNeXt would need to be designed, one that is at the same level of complexity (in terms of parameters and FLOPS) as the ResNet20. So, if you have a better knowledge about these models and are willing to contribute towards the repo by experimenting with these improvements, feel free to do clone/fork the repository and do so! I'd be very interested to know how/if they improve the performance!",1.0
g233kmz,i3muc4,I have updated the repository with custom ResNeXt models that have comparable complexity to the ResNet models. The addition of the ResNeXt blocks helped decrease the error rate to 5.32 (accuracy of 94.68). Do have a look at the repo and let me know what you think!,1.0
g11vwzw,i3muc4,"Some great results for those architectures, congrats! I believe you were using classical CIFAR training with train/val/test dataset, do you have any thoughts on what kind of results could you get with knowledge distillation from larger model?",1.0
g1ali1g,i3muc4,"Thank you! I have never tried knowledge distillation, but based on results in the existing literature, you would expect an increase in test-set accuracy. It is difficult to estimate how much this increase would be without actually conducting the experiments. [This](https://arxiv.org/abs/1812.01187) paper conducts such experiments on ImageNet and the authors note a slight bump in the accuracy (78.31 -&gt; 78.67 for ResNet50). However, this increase is likely to differ based on the dataset and model architecture. Do you have ideas on how to implement knowledge distillation / teacher-student learning in code?",1.0
g0bf0y1,i3hs9f,"To continue the tutorial I had presented here: [First Tutorial](https://www.reddit.com/r/deeplearning/comments/donip0/introduction_to_anomaly_detection_with/)

Here is a notebook to understand how to detect anomalies on time series.

[https://github.com/JulienAu/Anomaly\_Detection\_Tuto/blob/master/AD\_ConvAE\_English.ipynb](https://github.com/JulienAu/Anomaly_Detection_Tuto/blob/master/AD_ConvAE_English.ipynb)

You can view the notebook on: [NbViewer](https://nbviewer.jupyter.org/github/JulienAu/Anomaly_Detection_Tuto/blob/master/AD_ConvAE_English.ipynb)

I'll continue to improve the github in the future. Feel free to ask questions or comment.",7.0
g0bf1ib,i3hs9f,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/JulienAu/Anomaly_Detection_Tuto/blob/master/AD_ConvAE_English.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/JulienAu/Anomaly_Detection_Tuto/master?filepath=AD_ConvAE_English.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1.0
g0buhjq,i3hs9f,"This is awesome, thank you for putting this together and posting it :)",3.0
g0c8d9j,i3hs9f,Thanks ! :),1.0
g0ckd2a,i3hs9f,Awesome!,3.0
g0d0mb8,i3hs9f,Thanks a lot !,1.0
g0d4k4r,i3hs9f,Is this electricity usage or water usage?,1.0
g0ffikp,i3hs9f,"Not exactly. Connection to a website. But yes, we have the typical form of a time series produced by a human use :)",1.0
g0j9hvc,i3hs9f,"Cool, I work with water data and I use a 1d CNN to classify leaks in the network. This method looks very useful.",1.0
g0edqgw,i3hs9f,Nice,2.0
g0fk376,i3hs9f,"Any paper, blog reference for more depth analysis?",2.0
g0acl6f,i3agkp,"- Statisticians: ""wait, it's all statistics!""
- Computer scientists: ""wait, it's all programming!""
- Neuroscientists: ""wait, it's all neural models!""
- Mathematicians: ""wait, it's all linear algebra/optimization!""
- Physicists: ""wait, it's all partition functions/entropy""
- Electrical engineers: ""wait, it's all circuits/DSP/info-theory!""
- Aerospace (and electrical) engineers: ""wait, it's all control/feedback over sensors+actuators!""
- Game devs: ""wait, it's all GPUs/cuda!""
- Devops: ""wait, it's all docker/k8s!""
- Psychologists: ""wait, it's all psychology!""",32.0
g0akikj,i3agkp,"Agile PMs: ""wait, it's all kanbans!""",11.0
g0ak60c,i3agkp,"No, Computer Scientist goes at the bottom: ""Oh cool, Turing equivalence!""",3.0
g0ax1qk,i3agkp,Computer science is not ‘all programming’ lol,3.0
g0ba0bw,i3agkp,"- ""it's all programming"" =/= ""CS is all programming""
- ""it's all programming"" == ""data-science/ML/DL/RL mainly belongs to CS""
- ""it's all &lt;part-of-a-discipline&gt;"" == ""data-science/ML/DL/RL mainly belongs to &lt;discipline&gt;""
  - =&gt; tribalism, in-group/out-group, ""we were here first, so it's ours""",4.0
g0e3xzr,i3agkp,creative application &amp; distribution thereof... everything is math but that itself is useless conjecture,1.0
g8ovrde,jacdk8,Have you considered using some cloud GPUs instead of running locally? It will likely be a lot cheaper,3.0
g8ou424,jabsdc,"Umm as far as I know, you will be creating a vocabulary / using embeddings for your words. Won’t that be enough?

What is your use case?",1.0
g8ozv5n,jabsdc,"Thanks for replying. With the sentences, I have allied metadata which essentially is the context in which sentence was said... This is ordinal data and I want to include this context. So this metadata has to be one-hot-encoded and I want extract the features or ""include"" this context in the already encoded sentence",1.0
g8op1jk,jaaza1,Difficulties... like what? There’s a thousand things under “coding the model” you could be talking about here. There’s literally nothing we can help you with based on what you’ve told us so far.,3.0
g8otayr,jaaza1,Like deciding on what to do next and how to do next.,-1.0
g8ou2uu,jaaza1,"Do you not see a problem here with how you're asking this question?

There's literally thousands of potential decisions you could make. We cannot read your mind. We have no idea what types of decisions you're struggling with.

Regardless, your best bet here is to take - and really internalize - several in-depth deep learning courses.",4.0
g8opr4o,ja5sct,"Contrary to popular opinions, the DL book by Goodfellow is Not a good text for anything. You'd be better served taking a Mooc. 

On the theory side of things, I understand your frustration (I feel the same) but there is no other way other than to study those topics and practice them. If you want to be more rigorous in your approach, then look up the bachelors Applied math and stats program for Caltech and study the courses which you haven't before.",4.0
g8p4s0r,ja5sct,Yes it is a horrible book for a beginner. The book glosses over so much math it's impossible to learn from the book if you are a beginner.,1.0
g8nv7cx,ja5sct,"I'd start with ***Deep Learning*** by Ian Goodfellow, Yoshua Bengio and Aaron Courville.",9.0
g8ovw2u,ja5sct,Online version free btw,2.0
g8nztvj,ja5sct,it more about non convex optimization,4.0
g8oqh45,ja5sct,"https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html

This is a good introductory book on what you are looking for.
It would be a much easier read if you know probability well and understand at least some real analysis",2.0
g8pkqoo,ja5sct,This is another good one in a similar vein https://cs.nyu.edu/~mohri/mlbook/,1.0
g8o3q67,ja5sct,"Are you only interested in theory or do you have a practical goal in mind.

If the latter, as an engineer myself I started by using Deep Learning on Kaggle (https://www.kaggle.com/learn), then implementing it and reading papers at the same time.",2.0
g8ppblb,ja5sct,Following,1.0
g8nzczd,ja5sct,Commenting to follow this thread.,1.0
g8o7cq2,ja5sct,Me too.,1.0
g8oj66f,ja5sct,Follow this thread,0.0
g8okilp,ja5sct,Commenting to follow thread,0.0
g8pg2mp,ja5sct,Following,0.0
g8nbu94,ja1e1e,"This is from David Silver’s reinforcement learning course, no? If so I’ll try to find some notes that I should have saved that might help.",5.0
g8nhu47,ja1e1e,"He was able to explain how to verify a state value using bellman's equation but nothing on how to calculate each individual state... I'm genuinely confused. If you could, I would be absolutely grateful if you could share the notes with me.",3.0
g8nqcwq,ja1e1e,"For particularly small problems (like the graph shown above), the standard approach is through value iteration.

1. Initialize the states with the immediate rewards of that state. I believe in this example, everything starts off at 0 (but other problems there are desirable states to be in).

2. Update each state values based on the next best state (i.e., the action that has the best expected reward based on the state you land, make sure to include rewards/costs of taking that action). This step could also be according to some policy in which case you'll be updating expected transition rewards according to that policy (which could be deterministic simple sum of action reward + implicit resulting state rewards, or probabilistic where you use expected value of policy). Generally, each action has an associated probability of transitioning to some other set of states (non-deterministic transitions).

3. Repeat until convergence of values (updates no longer change state values).

Hope this is helpful. If you have a discount factor, it's important to include in the calculations of current state valuation = reward(state) + y(value of next state under our policy)",5.0
g8nqo22,ja1e1e,Exactly. Since you have the MDP fully defined you can simply use dynamic programming to calculate all the values.,2.0
g8nysln,ja1e1e,"This was well explained, thank you for taking the time to write this. I'm still a little fuzzy on the calculations though. So if I start at the node containing -1.3 (pretending that all nodes are empty). To calculate in order to get -1.3, would It be -2(reward state) + 1(gamma)? Where does the policy go into the calculation? I apologise I'm a little slow lol",1.0
g8oa88s,ja1e1e,This isn't Deep Learning,1.0
g8oarze,ja1e1e,Lol then what is it then?,0.0
g8okiic,ja1e1e,"Reinforcement Learning is a machine learning paradigm. You don’t need to use DL to solve these problems, in fact, in this case you have to use dynamic programming",1.0
g8n99fk,ja0ka0,The best bet would be to learn how to code before posting questions on Reddit about hypothetical things that will never happen in the future.,-6.0
g8mm9e9,j9r82n,"marked the video for later watch.

BTW is this is the paper from neurIPS 2020 which is receiving a lot of attention which seems to outperform CNNs?",2.0
g8mx3sb,j9r82n,"Yep, that's the one",3.0
g8niloj,j9r82n,"Thanks for marking it. This paper is currently under review at ICLR 2021. And yes, it's receiving a lot of attention because it achieves better results compared to CNN-based models when trained on a larger dataset.",1.0
g8l2aoh,j9pvnr,I released this blog post on end to end image classification last week. Let me know what you think about it :),1.0
g8kd128,j9jvgl," i assume u have done something related to nlp before

1. build a small Q&amp;A bot. a lot of people have done this before. resources are literally everywhere. 
2. kg-bert. if you are looking a bigger ( aka. cool) project , do some research on this one. [https://arxiv.org/abs/1909.03193](https://arxiv.org/abs/1909.03193), this is the paper's link. it is kinda cutting edge work, you gonna love it i guess.",2.0
g8kdmen,j9jvgl,"Yeah i have already made an Chatbot using RASA framework , it is a open source framework to develop textual and voice over chatbots

You can have a look at it . i have written a blog on that too :
https://medium.com/data-science-community-srm/developing-chatbots-with-rasa-intuition-to-implementation-39c1dd34274c",1.0
g8kt0bb,j9jvgl,then u should take a look at kg-bert. interesting research. still BerT but use triples as input.,1.0
g8k5e11,j9iwqz,It couldn't be one to one since you could invent any number of loss functions that could be used with a softmax final layer.,3.0
g8k8wvr,j9iwqz,No. All that matters is whats the input of the loss function. Whatever requires a probability needs a softmax (sigmoid if binary). The loss could be something completely irrelevant to cross entropy and would still require the last layer to be a softmax,3.0
g8k2puj,j9iwqz,"From my perspective the only thing that matters is, Sigmoid=bce, Softmax=cce/sparse cce, Linear=mse (and other mse variations). Other combos don't work theoretically and not practical imo. Feel free to correct me if anyone feels I'm wrong. Happy learn a thing or two.",5.0
g8lslrm,j9iwqz,Could you speak to why bce always goes with sigmoid? I would have guessed bce would work with any activation with the same output range.,1.0
g8lwdvj,j9iwqz,"Sigmoid will give you output between 0 and 1. Also if you notice the output dimensions for Sigmoid layer would be 1 (because it is binary). Hence, bce would be the only option here. Can you tell me other activation functions that gives you a single output between 0 and 1, other than Sigmoid?",2.0
g8m82et,j9iwqz,"I suppose you're right. I can brainstorm other mathmatical functions in that range [step function, saturated linear, (tanh(x)+1)/2] but I don't know any practical reason to use them for activations.",2.0
g8kdgj3,j9iwqz,"There is no such 'correspondence' since we need a loss function to account for proper handling of log loss and activation function for updates of our weights and biases while backpropagation. Every Activation function and loss functions are different for different purposes and sometimes, you will find they may work entirely in a different way! You have to explore them while working on a MODEL.

  
It all depends upon what sort of operation you are trying to achieve   
Like if you are performing a multi-class classification, then you have to use categorical cross-entropy as a loss function and softmax as an activation function. But we can also use different loss functions for Multi-class classification while keeping the activation softmax function as it is. So I don't think there is any such connection or link between them!",4.0
g8k7ip8,j9iwqz,How can I choose a loss function? What features/properties should I be looking for..?,0.0
g8k7xvu,j9iwqz,"There should not be any confusion choosing loss function. Because, most of the times you implement a network that's already been published on arxiv. So, they'll provide the best possible loss function for the given network.

If you are experimenting on a new deep learning technique, then you should check with different types of loss functions as well as optimization functions.",0.0
g8ky2n4,j9ivsw,Someone want to give a tldr (tldw?) for those of us who can't play audio right now?,5.0
g8kylsi,j9ivsw,"Sure, [here's the one](https://twitter.com/bhutanisanyam1/status/1309582455842639873?s=20) I wrote right after the interview",3.0
g8l20u5,j9ivsw,"Thanks!

TLDR: 3070 for students, 3080 for professionals. 3090 only if you are doing high resolution machine vision or pretraining NLP, where the extra RAM is useful.

Crazy detailed article: https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/

Interesting point: Nvidia made things a little awkward by dropping 1gb of vram from the 2080 ti to the 3080. Some existing pretrained models will be too big now. May require framework tricks to free up the extra vram.",2.0
g8l5l1i,j9ivsw,"&gt;Crazy detailed article

Thanks for the link, Tim Dettmers is the author of the article :) (The guest of this interview)

We're basically discussing the article + a Few FAQs here\^",1.0
g8kteho,j9ivsw,"Nice work. I’ve read Tims blog a few times and it’s great to have him talk through it. Thanks for this, keep up the good work.",2.0
g8kymin,j9ivsw,Thanks for watching and for the kind words! :),2.0
g8kuok0,j9ivsw,"Great job bhai..!! After reading his blog, it good to directly listen from him. You were mentioning about a community here in india, what community were you talking about?",2.0
g8kyndj,j9ivsw,"Thanks for watching :) 

I was referring to the ML Community in India, generally speaking!",2.0
g8kz0tw,j9ivsw,Oh. Thank you..!!,2.0
g8k8p48,j9hbho,"No.

It should be easy to see that it doesnt help by training with and without",1.0
g8j09r8,j9bi9v,Do u mean upsampling2d??,2.0
g8j2uxo,j9bi9v,"I want to do something like [this](https://www.researchgate.net/figure/An-illustration-of-two-types-of-unpooling-operations-a-simply-replacing-each-entry-of_fig9_323470988) , I can't do that with Upsampling2D",1.0
g8hy40r,j95938,"Some thoughts to try, in order from most likely to least likely:

Many of the base models in Keras come with their own data preprocessing function. Replace your data preprocessing layers with that one function call to guarantee correctness. https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception/preprocess_input

Is there a reason you're sampling a mid-level of the base model (add_5) and not its last layer? I've never seen that pattern before. (You've set include_top=false so the last layer should be safe to use).

I would leave out that global average pooling 2d while debugging. Then once your model is training nicely you can try it and see what happens to accuracy.

If you unfreeze the base model, does it make training progress?",1.0
g8k1acy,j95938,"Thanks for response.

I have tried other blocks upto 11 but didn't go till last. Since the domain of the data is different so I thought initial blocks will provide basic elementary image related feature. Going deeper in the architecture might result is abstractions particular to the domain (eg. imagenet has object images which is different from brain electrophysiological data). However, will try and visualize what is the last block's output just to unturn every stone.

Are you suggesting straight flattening the feature output from the basemodel instead of global avarage pooling. If yes I did it with no avail.

The suggestion mentioned in the first point is I think incorporated as the normalization layer is added which is the preprocessing step for Xception model.

Actually, I did try unfreezing during the first round of training but due to less data model didn't gone well. 

However, I would revisit all the points just to crosscheck. 

Thanks.",1.0
g8kdlq2,j95938,"Oh. Your images are brain scans? Then the imagenet pretrained weights might not be very useful. To get good performance, I bet you would need to train without freezing and provide more training data.

The strategy I use is to freeze the base model and train (to get ok weights in the new layer). Then reduce the learning rate 10x, unfreeze the base model, and train some more.
https://keras.io/guides/transfer_learning/


With the preprocessing, I would still try replacing this:

norm_layer = keras.layers.experimental.preprocessing.Normalization()

...

norm_layer.set_weights([mean, var])


With this:

x = tf.keras.applications.xception.preprocess_input(x)

So you can be 100% sure it's preprocessing correctly.",1.0
g8hj5f3,j92tlz,I wanted to know more about using multiple PCIE risers in a 4x GPU system that I am going to build for deep learning. I was searching for some information regarding this for a very long time but couldn't find much out there. Your post helped me a lot. Thanks a ton!,2.0
g8hkxrq,j92tlz,Glad it helped! Deep learning is different from rendering with r/rendertoken but some of it should work the same.,2.0
g8hkysu,j92tlz,"Here's a sneak peek of /r/RenderToken using the [top posts](https://np.reddit.com/r/RenderToken/top/?sort=top&amp;t=year) of the year!

\#1: [AMA with Jules Urbach, CEO of OTOY/RNDR - ask questions for the AMA at 2pm PDT on 6/23!](https://np.reddit.com/r/RenderToken/comments/hcztqb/ama_with_jules_urbach_ceo_of_otoyrndr_ask/)  
\#2: [deadmau5 &amp; The Neptunes - Pomegranate (Official Music Video) - You guessed it, all done in Octane and partially rendered on the RNDR Network](https://www.youtube.com/watch?v=aAvyS6HSa0A) | [6 comments](https://np.reddit.com/r/RenderToken/comments/hk3wsf/deadmau5_the_neptunes_pomegranate_official_music/)  
\#3: [$RNDR Wins!! Lowest price for GPUs in this render farm test.](https://twitter.com/MacDaffy/status/1299345559044722688?s=20) | [16 comments](https://np.reddit.com/r/RenderToken/comments/ii7kub/rndr_wins_lowest_price_for_gpus_in_this_render/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/fpi5i6/blacklist_vii/)",2.0
g8hl0pd,j92tlz,"2pm PDT happens when this comment is 6 hours and 55 minutes old.

You can find the live countdown here: https://countle.com/LHDgcLKln

---

I'm a bot, if you want to send feedback, please comment below or send a PM.",1.0
g8hnaxy,j92tlz,it just happened. bots are taking over the conversation.,1.0
g8go87l,j927ey,"You either scrape it or find an existing dataset. There is also the option of buying datasets, but if you're new it might be expensive for you.

In your case, https://www.kaggle.com/techsash/waste-classification-data is a good start.",4.0
g8h1389,j927ey,"Thanks

but where can I buy image datasets?",2.0
g8h2db3,j927ey,You could sign up on Microsoft for Bing APIs (paying some amount monthly) and use the key to create your own custom datasets.,2.0
g8h2wjt,j927ey,A free flickr downloader would probably be a better option,3.0
g8h6p6c,j927ey,"Unsplash released last month for high quality images.

https://unsplash.com/data",3.0
g8h8jdb,j927ey,"Ohhh, that's awesome. Gotta love Unsplash.",2.0
g8h9boa,j927ey,"That's great, indeed.",2.0
g8hfrrr,j927ey,**thank you**,1.0
g8gb352,j8zbbu,"Attention with ensembles. I don't think it will work they way you imagine it, as it will take too much memory to place in RAM and input sizes aren't the same. Also, there is absolutely no reason to do something like that, at least not in that way.

Routing has its benefits when you're routing for one task, because then you might end up developing multiple efficient subnetworks that end up making your model better than one without routing.",2.0
g8gdjdp,j8zbbu,"Its a difficult problem.. what you could do is, to have many ML/DL services and instead of have them all in one place, to implement a “smart routing main node” which automatically identifies the type of the input data and calls the corresponding ML/DL service.. at least it was a “near optimal” solution for me. Of course this solution will have some errors..",1.0
g8fcis1,j8x0rp,"Have a look at the fast.ai course, it’s free, amazingly good and very top down/hands on...",3.0
g8ge0oh,j8x0rp,As you are a youngling try to focus on statistics and probability rather than Deep learning frameworks or even deep learning. Start with maybe Khan Academy's college/AP statistics and probability.,1.0
g8i9igd,j8x0rp,Stanfords Computer Vision Course should be fun! http://cs231n.stanford.edu,1.0
g8jb549,j8x0rp,"Hi, I am currently actually doing the same thing! 

I am taking Udacity’s deep learning course and I am writing articles on medium about the concepts I find difficult. This really helps solidify the information! Additionally if something peaks my interest (like the basic idea of perceptrons) I try to build them myself and write articles about them.

I recommend maybe retaking the course, and maybe writing some articles on the concepts that are difficult. Since you have taken it once you should be able to skim through a lot of information. 

If you want to start building check out PyTorch 60 mintue Blitz : https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html

Maybe try the same approach a.k.a write about the things you find difficult!

If you have anymore questions feel free to message me.",1.0
g8koder,j8x0rp,"Hello! My name is Alejandro and I am 18 years old. I started in Machine Learning when I was 15 years old. My recommendation for everyone is to start with courses like Linear Algebra. Statics and probabilities. You need to understand concepts. Only that! You can follow me on all social networks as ""@lexandermorales"". 5 months ago I posted my video on: ""How to get started in AI if I am a teenager"".

[https://www.youtube.com/channel/UCAHSbPZyWCuXKyoM8rnnNYA?view\_as=subscriber](https://www.youtube.com/channel/UCAHSbPZyWCuXKyoM8rnnNYA?view_as=subscriber)",1.0
g8eo1qy,j8vc46,[deleted],1.0
g8eohr5,j8vc46,[deleted],1.0
g8eok90,j8vc46,Is ram important for sequence data in general.,1.0
g8ee8rf,j8tb9y,Use something premade like https://neuro-ai.co.uk you never have to touch cloud infrastructure it’s made for you. You’re charged only for the compute time on GPUs,1.0
g8fd0yl,j8tb9y,This is one of the reasons I’m building [seeme.ai](https://seeme.ai).,1.0
g8fbeoj,j8s3dn,"Some of these have pretrained ones and a GitHub to run them

https://paperswithcode.com/task/image-super-resolution


ESRGAN has one that I have used many times before.",1.0
g8gab51,j8lk2h,This is a great piece of work and very good engineering.,1.0
g8h9v1y,j8lk2h,"Thanks, I really appreciate your comment!

Hope you got some value from it.",2.0
g8hsjpv,j8lk2h,Curious if multiprocessing made any sense to get around the GIL? Maybe to complicated in the code flow?,1.0
g8ldbhq,j8lk2h,That’s a lot of FPS. Are you sure?,1.0
g8cqg5t,j8leeg,"Since deep learning of structured time series isn't my forte I won't try to answer, but I do have one or two best practices that should apply -  

1. No matter how you store the data, you probably want a way to store a local cache so you do not have to wait for the data when debugging or when you spin an instance. 

2. Don't wait for later, implement pipeline testing. I suggest GreatExpectations.

Hope this helps!",3.0
g8cu5uq,j8leeg,"thanks, I'll see what I can do.",1.0
g8avmg8,j8gb26,"neat, but what's this got to do with deep learning?",23.0
g8aw5w5,j8gb26,and the code isn't in the comments...,5.0
g8bqb4v,j8gb26,This has nothing at all to do with deep learning and should be removed from this sub.,9.0
g8brima,j8gb26,okay cool but why in this sub?,4.0
g8azc6f,j8gb26,"Well done!  
Always glad to see people using C#.

Recently just built a small camera app for computer vision on UWP in C#, hoping to add some prediction models to it later this year. Big fan of the language,",0.0
g8bpl8t,j8gb26,It'd be actually interesting to see deep learning application for building cartoon stories start to end by itself.,0.0
g8ac9yn,j8a0gy,"I think making your own projects is one of the things you could try. It's incredibly helpful, because in the course there's a lot of code that is just given to you, and while building something from scratch, you have to go through the entire process by yourself. I found it to help my understanding of both the models and the process. You could hop onto Kaggle and try participating in one of the competitions there, or just pick up one of their datasets and build your own thing with it.  

Personally, I've found the courses to be a great starting point. But it was only after finishing one or two pet projects that I gained some confidence in the knowledge. The projects don't have to be anything super fancy or reality shattering, it could just be something simple. The aim is to just get started. And once you do, you'll discover a lot of things that are done practically that the course didn't teach you (although for a good reason). This helped me when I finished the specialization, I hope it helps you too :)",3.0
g8axh3n,j8a0gy,Thanks a lot,1.0
g8altwo,j8a0gy,"
Hi There.  Glad to hear you finished the first step of learning deep learning. That's very clever of you. Now, I bet you have a basic understanding of deep learning. For instance,  the concept of gradient descent,gradient vanish... What is object detection, why LSTM have a better performance than RNN etc. Things like what I just mentioned is particularly essential for building up a much more complicated architectures like transformer and reading state of the art papers published recently. If you have no idea what I just talked about, please watch that specialization again. A robust basis of deep learning is the key to make groundbreaking works in your research journey.

  Now,let focus on what you need to do in the next step. Learning a deep learning framework like pytorch. There is a saying that practice makes perfect. What you just learned is just a bunch of theoretical knowledge. It is dramatically different in practice. A lot theories may not work as you read in the books.Also deep learning is still quite a empirical process, so it's important to gain some practical tuning hyper parameters experiences which you will never get from Ng's lectures. So learn a framework then apply it to your practice.
  By the way, when it comes to learning a framework, here are a series of questions and problems you are gonna need search the answer for yourself.
 1.which framework you wanna use? There is pytorch for research and TensorFlow for application in real world. It Its on what you wanna do with the framework. I am currently using pytorch.
2. Find some learning materials of learning framework.
3. Find some actual projects to practice.
For these two problems, I suggest you to look up in github.

 I still got a lot to share with you, but....

You have a nice day!

I am not a native speaker and i am currently practice my English writing skill. So I looking forward to any of your advice. Thanks a million : )",2.0
g8as5bd,j8a0gy,"Really appreciated that, thanks. I was about to start learning pytorch. Do you recommend a path for that?",1.0
g8axzeb,j8a0gy,"Read papers

You probably want to get a list of important architectures up until now and study those papers first, I don't know if that course goes that in depth (doubt it does).

Once you find something that you like or you get an idea for something, try implementing it. For an example, as I was reading up on RNNs I stumbled upon Differentiable Neural Computers. The concept of a memory networks interested me, so I kind of experiment with certain ideas relating to them ever since. Eventually by reading papers you're going to find something that sparks joy in you, but to know whether it is sane and worth you probably need to know if a better solution exists, and that requires reading a lot of papers.",2.0
g89s3i0,j8a0gy,"Have you finished the entire deep learning specialisation? Or just course 1(Neural Networks and Deep Learning) of the the 5 part specialisation? 

I am about the finish the specialisation and I was wondering about the same. I was thinking of taking up the tensor flow specialisation and simultaneously looking for an internship. 

I also looked into the deep learning nano degree on Udacity as a worst case substitute for an internship. But it’s quite expensive.

Some of my friends also suggested that I take up mini projects on the internet. Am probably going to consider that too.",1.0
g89sbyb,j8a0gy,I have finished one course which is the first couse of specialisation I guess. I didnt know it has five part. Nice to here that,1.0
g89sout,j8a0gy,It’s called Deep Learning Specialisation on Coursera. Have a look at it.,2.0
g89t1fz,j8a0gy,"Thanks, btw I don't know about Udacity but Coursera has financial aid opportunity. Take a look at that",2.0
g898erp,j87swm,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/XimingFeng/ddpg-stock/blob/master/DDPGTest2.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/XimingFeng/ddpg-stock/master?filepath=DDPGTest2.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1.0
g8971cc,j87evq,"Having worked with both, there isn't much of a difference if you're doing gpu training. The CPU is just there to preprocess and move data around at that point. I like my 3900X because it got me more cores per dollar and I do end up doing some work that is cpu intensive.",6.0
g89j1t5,j87evq,"Yes, CUDA works fine on AMD CPUs. It only cares about the GPU, not the CPU",5.0
g897js5,j87evq,I have R7 2700X and GTX 1660Ti. CUDA works fine.,4.0
g89r2af,j87evq,"You might be a little confused. CUDA is about GPU computation, so the CPU doesn't matter with what you're talking about. If you want to use CUDA then you need an Nvidia GPU though, so AMD CPU + Nvidia GPU (as you say, and as I have) is a good way to go.",3.0
g8atn3y,j87evq,"Always NVIDIA, but consider using something like [this](https://docs.neuro-ai.co.uk) instead if you really need the speed",2.0
g8a4nzb,j87evq,Thank you for your answers.  What should I buy between AMD 3900X or Intel i9 10900 (with a 2070 super GPU)?,1.0
g8a7mfs,j87evq,"Intel is historically known for single thread performance, while amd for multi core. As training models doesn't really require higher single thread performance, amd is the way to go. Personally, I would suggest you to wait for a while (if possible) and get your hands on the new zen 3 amd chips and/or the rtx 3000 series, as they are only slightly mor expensive than their predecessors, but provide a significant boost in performance.",2.0
g87ymwd,j7v57i,"Oof, these comments are a mess.  I'm not sure if the OP got the answer they were looking for, but just for reference:  


&gt;I want to precise that in my task ,images don't have classes ,they are just a bunch of images that cannot be classified . I can simply say that I have one class.

Although cross-entropy is often used for classification, that's not really how it's being used in this case.  You're not determining the class of each image, but the pixel value of each pixel in each image.  Pixel values should range between 0 and 1 for gray-scale images, so using cross-entropy makes sense as you're essentially trying to find the value in this range to which the pixel should be assigned.  There's plenty more to say about this, but the OP is misunderstanding the role of cross-entropy as a loss in this instance.

And you *can* use MSE, it just has different properties which may or may not be more suitable for this task.  In general, cross-entropy is better for pixel prediction for a number of reasons (other comments discuss this), but I've had success with both with regards to VAEs.",6.0
g880air,j7v57i,"Very helpful Nater ,thanks . What if I use sigmoid activation in my last layer , I would be able to get ride off the sigmoid_cross_entropy_with_logits ,isn't it ?",2.0
g8825mn,j7v57i,"Basically. [sigmoid\_cross\_entropy\_with\_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits) takes logits as input, applies a sigmoid function, then takes the cross entropy.  If you're already applying sigmoid in your final layer, do not use this.  Instead, use [binary\_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy) (with `from_logits` set to `False`).",2.0
g88fpsn,j7v57i,"Perfect , thanks a lot",2.0
g883utu,j7v57i,"Looking through the code from the TF website, they actually said explicitly that each pixel is modelled by a Bernoulli dist and that they did binarize the images. I guess that’s why they used binary cross entropy.",2.0
g884oqw,j7v57i,"Yeah, I haven't looked at the specific code, but that shouldn't change the use of binary cross-entropy.  Whether the images are binary, gray-scale, or RGB; pixel values should always have a value in `[0, 1]`, so the use of binary cross-entropy is always appropriate.",1.0
g87f1pu,j7v57i,"I don't understand why you are trying to avoid sigmoid + cross entropy at the output layer, but if you want to use mse you can do relu + mse. You would need to clip your results while doing inference in that case.",2.0
g87g4u3,j7v57i,What do you mean by clipping the results ?,1.0
g87g8bi,j7v57i,"If you use relu your output is unbounded, but the pixel values for an image is bounded. So you have to clip them (setting anything above 1 equal to 1, or however you are normalizing your data), if you want to use relu at the output layer.",2.0
g87gb7h,j7v57i,"Yes you’re right ,I can do a RGB mapping in that case",1.0
g898218,j7v57i,"From my exp, relu works awfully if you put it in the end like you said, so I’m guessing you’re not saying this from exp.",0.0
g89a2ta,j7v57i,"It really depends on the application. You shouldn't assume someone is talking bullshit just because your ""experience"" tells you otherwise.",1.0
g8b4au6,j7v57i,"Yeah, well, at least in this case the use case is quite clear.",0.0
g877mzd,j7v57i,"they just did that since they treated the images as binary I think, but if you just want a generic reconstruction loss MSE is fine, i've done it, it works (combined with KL divergence on latent distribution)",2.0
g87drq1,j7v57i,"No, they don't treat images as binary. They use sigmoid activation function at output (mapping to the range [0, 1]) and so they can use crossentropy. If you don't map your values to that range you can't use crossentropy.",3.0
g87dxxg,j7v57i,"By using cross entropy as the loss they're assuming the labels are binary. Yes, the output is continuous, but the target is to be binary",0.0
g87e175,j7v57i,"No they aren't. Do you know how crossentropy works? It's not just for classification, it can also be used for regression, like in VAEs or GANs. [Cross entropy for dummies](https://towardsdatascience.com/cross-entropy-for-dummies-5189303c7735)",1.0
g87e44v,j7v57i,"Lol, yes I do. What are you even talking about man? Binary cross entropy for an output that has a sigmoid activation is very natural.",1.0
g87e7zl,j7v57i,You claimed that images and labels are binary which is simply wrong. I think you don't know what you are talking about.,3.0
g87ed5x,j7v57i,"For the examples that person was mentioning it was grey scale images with range bounded between 0 and 1, so yes binary. You dont seem to make much sense. And if the labels were continuous between 0 and 1, then I mis-spoke.",-5.0
g87ef75,j7v57i,Haha bro you don't even know what binary means and you are trying to argue. Please stop embarassing yourself,5.0
g87eh97,j7v57i,"Dude you're really heated, try not to let it get to your head.",0.0
g87ek1g,j7v57i,"No I'm not, I'm just trying to provide correct information unlike you. It's very clear that you don't even know what binary means and you are still pushing it.",1.0
_,j7v57i,,
g87dc05,j7v57i,"Yes , I just wanted some opinions , which I thank you for ! When I try to use  the sigmoid cross entropy I ended up with an extremely high loss . Thank you again !",1.0
g898gwk,j7v57i,I’d recommend you look at some other people’s successful code or guides.,1.0
g898bq0,j7v57i,You should probably use Adam optimizer and LR on the order of like 0.001. You basically can’t train VAE with SGD/Nesterov/etc nonadaptive.,0.0
g89ad2i,j7v57i,"Wow I don't understand how certain you can talk like that, without even knowing the problem. SGD+momentum beats Adam in numerous occasions, quite likely including this problem.",2.0
g8b6hk5,j7v57i,Do you have any exp in training VAE? I’m quite certain of what the answer is,0.0
g8b6td9,j7v57i,I implemented a VAE with custom latent loss using only NumPy for my senior year project. So I know what I'm talking about. And you definitely sound like a pretentious ahole.,1.0
g8denwp,j7v57i,"Fair enough in that case. But non-adaptive optimization algorithms don’t work well on generative networks, nor VAE, nor GANs. Which is why it failed for thread author. I don’t remember well, I think they might have worked for non-variational plain autoencoder, but not for VAE. In some special circumstances, for example if your network is super simple, might work. Maybe that was the case with your numpy net. With better performing deeper network with residuals, it most likely won’t.",1.0
g8762bp,j7v57i,"I remember I tried both on fashion mnist dataset and had different results. As far as I remember, it was also weird to me that sigmoid loss worked better. I recommend you try both and look directly at results. If anybody would comment on why sigmoid makes more sense theoretically I would love to hear that.",1.0
g876mly,j7v57i,"I think the trick is with distribution tails.. With MSE your distance is always the same, not weighted, where with sigmoid cross if you predict class that should be 1 with something like -5 prior to activation, the loss will be huge. Its like stricter loss function.",0.0
g8789zz,j7tuft,"Weights are learned and changed automatically by algorithms like sgd, adam. Suppose u know how to change weights, how would u change it? There are so many parameters and infinite number of values to choose from. Maybe the learning has stagnated when accuracy isn't increasing.",4.0
g88tcrl,j7tuft,there will be no use of randomly changing weights during training as ultimately the trainer might converge to same point and get stuck as in your case. What you can try is cyclic or dynamic learning rate which varies from lower to upper bound and help you to overcome local minima or saddle point.,1.0
g86xj9t,j7tijb,Look at the documentation of the learning rate scheduler.,1.0
g86yocq,j7tijb,Already did. Unable to get useful insight.,-1.0
g875t55,j7tijb,"Well mate, we're not your personal coders. What exactly are you struggling with? There's quite a lot of info on doing this kind of thing that you can google.",1.0
g87vb62,j7tijb,Thanks for such a nice help,0.0
g87vfk9,j7tijb,"It's exactly as much help as your low-effort post here deserves. Ffs. Learn to help yourself, and then ask people when you have specific issues, *along with* a description of what you've tried so far.",1.0
g881rj7,j7tijb,So nice of you,0.0
g883ix6,j7tijb,"Pay me, and I'll be nicer.",1.0
g86z6n2,j7tijb,Look at your optimizer’s attributes. I think you can access (and change) the learning rate directly,1.0
g88zz3t,j7tijb,"Have a look at LambdaLR, https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.LambdaLR",1.0
g86qmnp,j7nwje,Have yo tried GradCam?,1.0
g86x1c1,j7nwje,"Just some ideas, as - afaik - this has only been implemented for classification so far. For semantic segmentation, I think you would end up with one saliency map per pixel if you start from the segmentation map. For one saliency map per segment, you would have to start from one of the inner layers, however, you don’t know if and where a segment as a whole is encoded. I also remember a paper from Google (Been Kim, i think, CVPR 2019) where she said that network saliency maps generated by GradCAM are not really trustworthy. Maybe you could look into LIME, which does not work with gradients but masks parts of the input and then looks at the effects at the output, which could effect multiple pixels at once in the segmentation map. For object detection, I think you would have to work with the layer before the bounding box regression.",1.0
g86xkcb,j7nwje,"Some of the existing methods are implemented in torchray, https://facebookresearch.github.io/TorchRay/",1.0
g86ulf6,j7nm83,No,1.0
g86uktp,j7nkby,No,1.0
g85tsdh,j7nkby,No. It’s not.,1.0
g86t6m1,j7mng0,This whole post barely provides more information than the headline,1.0
g86wybj,j7k3dm,"I'd call Attention an *architecture* because there many practical decisions when deciding to use attention. The *technique* of the attention function isn't going to be very useful just by itself. All the videos that I've seen that cover attention refer to it as the 'attention architecture'.


Some examples of decisions I've seen are: wether you need input embeddings; single or multi-headed attention; how many layers of attention; will you add a fully connected layer as output? The answer to these questions affect your design, hence architecture.

However, I'm not an expert and only added my two cents since nine hours have passed without a single reply.",2.0
g84wpox,j7inbs,Take a class on linear algebra,1.0
g85xn74,j7ig7y,"I know unrealistic right? However, I find their process of reviewing is slightly unfair. Why? 

First, the conference allows the authors to submit a paper that was already on arxiv or even allow them to post their paper during the reviewing process.  So, the latter isn't ""blind"" anymore. 

Let me give you an example. There are a lot of papers that have been uploaded to arxiv and are cited more than 50 times but never got accepted or submitted to conferences. Now, let us say you are an assigned reviewer, and was given a submission. First thing you could do is google the paper's name and hop! There you find the authors' name and their affiliations and that paper has already more than 120 citations. Let alone if these authors are from well known company/universities. What would you do? There would be a bias in reviewing the paper and will easily get accepted. I am not generalizing but this happens.   


Another thing is that, what is unfair in most conferences is that big tech companies are getting more involved in DL conferences. I think it is ok. But again it is unfair for purely university research work where the research students do not have the luxury of owning 10 GPUs, 10 TPUs ...  A research paper could take a year or two to be finalized in such condition. However, for big companies their research is done in 4 months due to a lot of collaborators, and of course abundant computational resources.    


I would like for big conferences like Neurips, CVPR, ICML, etc... to consider making two rounds of submissions: (1) For research that is purely done through university research (2) For research work that is done through big tech companies.

Anyway, I apologize if I seem to rant but I needed to get this out of me.   


Thanks for the post!",21.0
g86lgal,j7ig7y,"I definitely can see the necessity of two different selection processes for papers by industry and papers by univ. 

But is the bias in the case of already published and cited papers really bad ? I mean, doesn't a lot of citations and a brand name (big tech company) most probably will be equal to good quality research?",1.0
g8737bu,j7ig7y,"Not only that, even my shit tier Uni can afford to buy a lot of GPUs for research or at least rent some.

The point of research on unis is to make efficient, good stuff. Not inefficient, state of the art stuff. I have not seen A SINGLE researcher on my uni that is impressed or advocates for using large networks that need a lot of GPU hours to train. But that doesn't mean than 10 GPUs/TPUs is out of reach.",1.0
g86i36z,j7ig7y,I'll read half tonight before going to bed and the second half will make great bathroom reading tomorrow.,2.0
g84su8v,j7hxb4,"What you are asking doesn't make any sense. You are comparing oranges (editors) with shoes (resources).

Here is the thing which will connect both, enjoy:

[https://github.com/abhishekkrthakur/colabcode](https://github.com/abhishekkrthakur/colabcode)",2.0
g84vjxc,j7hxb4,"How does doing a deep learning project in python vscode differ from a google colab oratory notebook, that’s my question. I’m a undergrad whose still new to this stuff.",0.0
g877v0t,j7hxb4,VSCode will use your local computer while colab uses Google computers. Training deep learning is best done using GPUs. Other things like evaluation could be handled locally. You should learn to use both VSCode and Colab.,2.0
g87doa8,j7hxb4,"Even with colab GPUs things can become quite expensive, and the notebooks are not always the best for responsiveness. I locally host a Jupyter Notebook and then use [https://docs.neuro-ai.co.uk](https://docs.neuro-ai.co.uk), it just replaces the training function and deploys on a gpu with some nice graphs.",2.0
g84o9gr,j7hjff, Are they the same as you read.csv()? I'm still confused,1.0
g85cj4h,j7geyh,"Its a couple years old but you can start with this one https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.23730

In case you've never worked with it, I've never gotten good results classifying eeg that wasn't from a published dataset such as the one used in the linked to paper.",1.0
g84g6b8,j7f1c9,"Some potential reasons (not sure if these are right in your case): you aren't initialising the weights well (e.g. try xavier), or you aren't normalising the input data properly",3.0
g84glw2,j7f1c9,"Sure I will take a look at Xavier. I am using the below transformation to normalize the inputs:

`transform_train = transforms.Compose([#transforms.ToPILImage(),`  
`transforms.RandomCrop(32, padding=4),`  
`transforms.RandomHorizontalFlip(),`  
`transforms.RandomRotation(15),`  
`transforms.ToTensor(),`  
`transforms.Normalize(mean, std)])`",1.0
g84mn9m,j7f1c9,another thought: you seem to be doing data augmentation aswell at the same time. generally its a good idea to turn complications like data augmentation and any regularisation off at first just to make sure there aren't any bugs and the network is able to learn something.   after that if the network shows it can learn then you can gradually reintroduce those things,2.0
g84q2o9,j7f1c9,"Yea I will do those things. Also, I have applied the xavier weight initialization as below: 

`def init_weights(m):`  
 `if type(m) == nn.Linear:`  
`torch.nn.init.xavier_uniform(m.weight)`  
`m.bias.data.fill_(0.01)`

`model.apply(init_weights)`

But it doesn't change the accuracy for the validation set. Is it preferable to apply it for the Conv. layers as well?",1.0
g84pfeg,j7f1c9,"Well, batchnorm used for a reason.",2.0
g84qjtu,j7f1c9,"Yea, with Batch Normalization I am getting some decent accuracy. But I want to figure out why the model is getting stuck at 1% when I don't use the Batch Normalization layers.",1.0
g85lzi1,j7f1c9,"I cannot give you a theoretical explanation.

There were a lot of studies about why batchnorm so cool.

In general its fine then your model crush without normalization.

If you want to make it run without batchnorm (for whatever reason), you should try advanced weight initializings, dropout, activation functions, residual connections etc.

But in general big models do not work without normalization.

And sometimes big model do not work with certain types of normalization lol.",1.0
g85pnog,j7f1c9,"Yea got it. I am actually pretty new to Deep Learning, so was checking out how different architectures and functions work. I will be trying them. Thank You.",1.0
g84cnjl,j7f1c9,Why don't you include your code if you want to get an answer? Do you think we are psychics or something so we can tell exactly whats wrong?,2.0
g84d4sr,j7f1c9,Sorry about that. You can look at the code here: [https://colab.research.google.com/drive/1MJ5sBuUeirh1XQTshZi1amw\_j5cZ0syV?usp=sharing](https://colab.research.google.com/drive/1MJ5sBuUeirh1XQTshZi1amw_j5cZ0syV?usp=sharing),2.0
g84diw5,j7f1c9,"No need to apologize, just a suggestion for your future posts.  Hope someone has an answer for this",1.0
g846wsk,j7drox,"You can compare your solution against baselines, such as some naive algorithms or more simple features.",10.0
g84m14y,j7drox,Face the same problem and this is my approach. It's especially valid in deep learning solutions when simpler models or classical ml can be a close rival.,1.0
g84dxr8,j7drox,"If you're the first one to solve a problem then that's your contribution. In general, papers have to have a new/better solution to an existing problem or create a new problem and offer a solution. If yours is the latter then I don't see a need to compare to anything else since that's where your novelty is.",5.0
g84fww6,j7drox,Thanks for your guide!,1.0
g85ltbz,j7drox,I would put my efforts on explaining the problem you tried to solve and why you decided to focus on it. That's what I did when on my paper when I encountered a similar situation https://link.springer.com/chapter/10.1007/978-3-030-13342-9_15,2.0
g83ejhd,j7a1rx,"Does it really have to be a laptop? A workstation would be better for deep learning.

I always refer to Tim's article on this:

https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/

Also, he helps you pick the rest of your system setup based on your goals with Deep Learning.",9.0
g83eph9,j7a1rx,"I am currently a student and I stay in a hostel. So a laptop would be easy to carry and move around. Besides, I would also use the laptop for other work like presentations in college, etc.",2.0
g83ewqc,j7a1rx,"There are also cloud services (some are free), where you can train neural nets with code running in the cloud so you wouldn't be bottlenecked by your system specs.

First have a look at Google Colab, maybe also Kaggle's notebooks. There are videos and I believe tutorials online on how to leverage those shared GPUs and train NN's for free online.",8.0
g83gicl,j7a1rx,"In my experience, a 4GB graphics card is practically useless for Computer Vision using DL. A minimum of 8GB is good. I’d suggest to use Google Colab as it provides 12GB graphics card (sometimes 16GB) for free. That’s good for learning purpose and you can train good big models as well.",4.0
g83quwt,j7a1rx,Public clouds and a shitty Chromebook,4.0
g83f773,j7a1rx,"I recently bought a desktop mainly for personal projects in AI and Machine Learning. I still do most of my deep learning on colab/kaggle notebooks. The reality is - no amount of retail GPUs for students can match the ones available for free on cloud services.

Hell I’d even suggest against buying the 30 series if you wanna do deep learning...

But if youre looking to buy a laptop with a good GPU, you open yourself to faster preprocessing, loading, and maybe even gaming.

Dm if you wanna know more i guess, i built my own pc so i might be able to point u in the right direction

Cheers",3.0
g84nk47,j7a1rx,"4GB cards are practically useless for DL (your OS alone may occupy 1GB VRAM), rather spend the money on the fastest CPU (intel i7 H-series 6 or 8 core) you can get and 32GB of RAM which would allow you to at least debug on the Laptop. Also you don't want an HDD in a laptop when 1TB M.2 SSDs are ~100$",3.0
g83wjz1,j7a1rx,Use colab/kaggle notebooks. I have a 2080Super except I still use colab to train my models most of the times because the free GPUs they provide are far superior to my 2080Super.,1.0
g86xp4a,j7a1rx,"Same!! When did you buy the card? I bought it recently and i think i could’ve waited for the 3080. I regret the decision but then it would’ve been a little more expensive since GPUs cost higher in India (imported, luxury items apparently)

Do you think I’d be good for another 4 years with this card?",2.0
g87vx3q,j7a1rx,I bought it this July and regret it too since the 3070 came out a month later which has better performance for cheaper than the 2080S. I think I’ll wait another 4 years cuz currently it does it’s job well lol,2.0
g83f8vn,j7a1rx,"Checkout Tensorbook, maybe?
[tensorbook](https://lambdalabs.com/deep-learning/laptops/tensorbook)",1.0
g83d1zl,j7941f,Build a model that can analyze thousands of reddit posts and then suggest a random thesis idea.,10.0
g83dg1u,j7941f,Automatic hentai decensoring,4.0
g8k9kpp,j7941f,"&gt; Automatic hentai decensoring
can you plz elaborate a little bit please",1.0
g839fwc,j7941f,"What thesis? Bachelor, Master, PhD?",3.0
g83hnkt,j7941f,sir its master thesis,1.0
g83o720,j7941f,"I'm still learning the basics, so I'm unsure whether it makes sense or not to use deep-learning nor whether it would make a good thesis, but I have a problem that needs solving :). Maybe it will interest you.

For a [personal open-source project of mine](https://openpaper.work/), I have to deal with document scanners. The problem I have is that scanners provide an image, but are unable to tell which part of the image is paper and which is not. I would like to crop the image automatically according to the borders of the page (*not* the text borders ; this problem is already solved and it is not what I'm looking for ; I want to keep the page layout for possible re-printing).

Most scanning tools solves this problem by asking the user ""what is the size of the paper you put in the scanner ? A4, Letter, ... ?"". This is remarkably annoying to me and probably to others. Most of the time, it's A4/Letter, and sometimes .. I just have no idea what it is and I don't care. So I don't want to ask this question to my users. Right now I use a calibration mechanism + quick edition of the page, which is not that much better.

At first sight, it looks like a simple problem that could be solved with usual computer vision algorithms. [I've tried and mostly failed](https://gitlab.gnome.org/World/OpenPaperwork/libpillowfight#scan-border). 

It seems to me that most of my problems stem from the diversity of available scanners: For a same piece of paper, there isn't 2 scanners that will give you the same result (luminosity, contrast, dust on the glass, weird background, etc). There is also a bunch of possible scan resolutions (75 dpi, 300dpi, etc) and modes (Color, B&amp;W, etc).

Or maybe I'm just not good enough with CV algorithms ...

Examples:
* [Epson XP-425](https://openpaper.work/scannerdb/report/271/scanned.png): Lot of dust
* [Fujitsu fi-5110Cdj](https://openpaper.work/scannerdb/report/269/scanned.png): Returns a strangely long image with an annoying ""2 steps"" background.
* [Brother MFC-7360N](https://openpaper.work/scannerdb/report/267/scanned.png): This j***-a** cleans up the image so much that the page borders are hard to find even for an human.
* [Brother MFC-7360N again](https://openpaper.work/scannerdb/report/268/scanned.png)
* [Brother DS-620](https://openpaper.work/scannerdb/report/311/scanned.png): Black background instead of the usual white

Anyway, I was thinking of trying to solve this problem with deep-learning, but right now I'm short on free time ... :/

More examples can be found in [OpenPaper.work scanner database](https://openpaper.work/en/scanner_db/). Unfortunately, I doubt there are enough good samples at the moment to use deep-learning. So I'm currently thinking of improving the website/database and the [tool used to collect the scans](https://openpaper.work/en/scanner_db/#contribute) and then trying another round of crowd-sourcing :)",3.0
g83djkm,j7941f,"Because your question is broad, I'll suggest a broadly applicable topic: 

Interpretability and the methods thereof of Deep Learning models in Natural Language Processing/Language Generation/Image Detection/Object Detection/Image Generation. 

Any broad application context suits just fine.",2.0
g83hr9a,j7941f,"ok sorry for my broad question, i want to do thesis on object detection using convolution neural network (deep learning), can u please give me your precious suggestions",1.0
g83vd72,j7941f,"Well there is my suggestion already, just read the ""title"" and pick Object Detection from the options.",1.0
g8k9ova,j7941f,ok thanks,1.0
g839a57,j7941f,Wot m8,1.0
g83hntf,j7941f,what?,1.0
g83bhfq,j7941f,RemindMe! 24 hours,1.0
g83bi97,j7941f,"I will be messaging you in 1 day on [**2020-10-09 08:43:53 UTC**](http://www.wolframalpha.com/input/?i=2020-10-09%2008:43:53%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/deeplearning/comments/j7941f/thesis_suggestion_in_deep_learning/g83bhfq/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdeeplearning%2Fcomments%2Fj7941f%2Fthesis_suggestion_in_deep_learning%2Fg83bhfq%2F%5D%0A%0ARemindMe%21%202020-10-09%2008%3A43%3A53%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20j7941f)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
g84b29i,j7941f,"Doesn’t your advisor pretty much tell you what to do?

Take the journal articles you published and staple them together.",1.0
g8k9rgh,j7941f,you comment is not helpful,0.0
g8k9wig,j7941f,It was an option for me to take my journal articles that were published and present them as my thesis.  So I didnt have to write a thesis.,1.0
g8k9z6z,j7941f,ohhhh great,0.0
g8346q0,j77790,"Knowing math and python alone is not enough. This day to be a practitioner, you need more than that, and the DL industry moving toward a traditional software engineering.  Practical skill and knowledge in the DL framework are a must.",3.0
g832q5v,j77790,enter at the shallow end,2.0
g83dalp,j77790,I started with Zen and the Art of Motorcycle Maintenance. It’s a basic book on the metaphysics of quality. That’s some deep learning.,1.0
g81yrxy,j6z32g,How exactly did you add this LSTM? What is your base architecture? Adding random components can hurt performance just like it can improve performance.,5.0
g836ob2,j6z32g,"Base architecture in deeplabv3+, this works well on the image level problem. I added a convolutional lstm at a few different positions: after the encoder, after the decoder, etc...",1.0
g837g3e,j6z32g,"Seems to me like you need to train it a lot and the only place where you'd want to put the LSTM is after the backbone. Of course, you want to batch norm or layer norm after that output, you want to make sure the LSTM is big enough and that it has 2+ layers.

As I said you'll probably need to train it for way longer and you'll potentially need more data.",1.0
g83bxig,j6z32g,"By 2+ layers, you mean stacked lstms?
Any reason why training it end to end should decrease performance? 
After the backbone you mean encoder. If so, there was actually a [paper](https://arxiv.org/pdf/1905.01058.pdf) showing it was better after the decoder. Im taking those results with a pinch of salt, but I don't think it's obvious to do either.",1.0
g83d4jz,j6z32g,"&gt; By 2+ layers, you mean stacked lstms?

Yes, not sequentially.

&gt; Any reason why training it end to end should decrease performance? 

Well if you think of layers as messengers, every layer delivers messages in a different way. We use convolutions for images because they optimally deliver messages from an image to some representation. If the LSTM delivers messages suboptimally for a given task then it will surely screw up the performance. If it is large enough then it probably won't but since you need it to be overparametrized, you have to give it enough data so it learns the noise from it as well, and this usually means that you have to augment your data further or get a larger dataset.

&gt; If so, there was actually a paper showing it was better after the decoder. Im taking those results with a pinch of salt, but I don't think it's obvious to do either.

Think of it like this - why do you need an LSTM? Your LSTM basically only converts a sequence to a single element. It's sort of a summarizer. So where you place it depends on what each component does. In this case, I'd never put it after the decoder because we don't trust the rest of the network more than the backbone. The backbone is usually very well trained and if issues arise then you know it's the LSTM. If you put the LSTM after the decoder, then you don't know if it's the LSTM or the decoder. Not only that, the decoder output probably has more entropy than the backbone output, making training that LSTM harder. Generally though, even though it's logical to me that putting it after the decoder would give you better performance, as the decoder output is less generalized when compared to the backbone output, theoretically speaking it is suboptimal because you're not generalizing well, you're just using tricks to overfit on the output. You want your LSTM to give you a summary of what's happening based on the thing you've seen, not a summary of what's happened based on the things you've concluded. Sometimes the conclusion will give you better performance because they contain more information than the original information. But if it's that severe, you'd probably consider looking at a better backbone because it means that the rest of your network is likely hallucinating some of the input, which could be hard to fix once it starts hallucinating incorrectly. If your LSTM overfits on the backbone output, then you just have to increase the size and train it longer to counter double descent, or cut the number of parameters/layers so you don't overfit.

**tl;dr** putting the LSTM after the backbone should generalize better and be easier to debug",1.0
g82bc22,j6z32g,LSTMs typically need tons of training data to perform well. May be your dataset is just too small,2.0
g81zqbs,j6z32g,What’s mloU?,1.0
g81zy6b,j6z32g,Maybe IoU?,1.0
g836pbx,j6z32g,Mean intersection over union of the ground truth masks and the predicted masks.,1.0
g80srk5,j6tdlw,Nice! Do you include results from arxiv-sanity.com?,2.0
g815pkj,j6tdlw,&gt; arxiv-sanity.com thanks I had forgotten about this site :) No I don't I monitor various forums and social media channels for papers that are being talked about and who's sharing and talking about a paper helps with calculating a score for the paper.  https://42papers.com/about,2.0
g8274im,j6tdlw,"Signing up is the best way to stay updated I have an interesting roadmap with features like audio channels for collaborative paper reading or tools for authors to give talks on their papers, code sharing and a bunch more.",2.0
g8362dx,j6tdlw,What a wonderful idea! Will share this on other platforms too :),2.0
g83zxqp,j6tdlw,Thanks I appreciate your comment. It's been a labour of love,2.0
g8bds2y,j6tdlw,Great resource. Added to [https://github.com/BAILOOL/DoYouEvenLearn](https://github.com/BAILOOL/DoYouEvenLearn). Thank you for sharing!,2.0
g7zpzf1,j6lkrd,link to further info?,1.0
g81rhay,j6lkrd,GitHub link is in the post,1.0
g81dzdo,j6lkrd,[Link](https://github.com/NVlabs/imaginaire) to the repo,1.0
g7z6fxy,j6j9zc,"At our company, we are planning to open source CRAL. (cral.segmind.com). DM me if you are interested.",2.0
g7z6gso,j6j9zc,"**I found links in your comment that were not hyperlinked:**

* [cral.segmind.com](https://cral.segmind.com)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g7z6fxy&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",2.0
g7zt4fm,j6j9zc,I sent you a DM. Thank you.,1.0
g7yciqa,j6g2cj,What’s the resolution? The complexity of convolution scales quadratically here.,6.0
g7yal9m,j6g2cj,"If not doing it already,  try to use GPU.
CNNs use to require heavy training time.",5.0
g7yhr1g,j6g2cj,"If you're writing in Python, run it on Google CoLab for the free GPU.",4.0
g7yjpu6,j6g2cj,Yeah I’m in colab wasnt using a GPU,1.0
g7zdtkm,j6g2cj,I need resources on learning to use GPU for training the models. Any resource would be appreciated.,1.0
g80jvwn,j6g2cj,"If you have a GPU, a deep learning library that can use GPU (ie Tensorflow GPU distribution), and you have installed all of the required graphics drivers and dependencies (ie CUDA, cudnn, etc), then the training function of a neural network using that DL library will automatically use the GPU.

If you're trying to use the GPU for some other task not provided by the library, that's more intricate.",2.0
g7zbzka,j6g2cj,Checking timestamps in the pipeline helps. I had a problem where getting the next batch from generator took longer than the forward and backward pass.,2.0
g7zlz9t,j6g2cj,"Check out [https://neuro-ai.co.uk](https://neuro-ai.co.uk) free £100 credits on signup, and works with Colab",2.0
g7yeifr,j6g2cj,"* Make sure your batch sizes are as large as possible, given your GPU memory.
* Are you using data augmentation? If so, consider saving out pre-augmented training data, trading storage space for reducing the bottleneck that CPU-based augmentation creates.",1.0
g7ylfl5,j6g2cj,How long is it taking?,1.0
g7yn2nw,j6g2cj,I’m on epoch 40/80 and it’s been 5 hours....,1.0
g7ynlqe,j6g2cj,"Any reason you chose to do 80 epochs? Are you sure your model is still improving significantly after 40 epochs? You may want to consider some form of early stopping. This won't help with the time per epoch, of course, but it will reduce the total amount of time you spend training.",4.0
g7z4yk0,j6g2cj,I’ll try this,1.0
g7x2w0g,j68320,/r/learnpython,3.0
g7xpwp2,j68320,"Coding is really not very hard. Most of the best practioners taught themselves, and learned any theory later to hone their skills or just out of interest. I have yet to meet a great coder who thinks their education path was key, most curriculums are way behind the curve.

Just do it. You don't need to be a great coder to do interesting things in DL. You just have to keep at it.",3.0
g7x3qof,j68320,"Could  try [fastai](https://course.fast.ai),  a free course that comes with a free book, no sign up, or STEM PhD  required.",2.0
g7yopem,j68320,"You can start off with Deep Learning Specialization. A lot of people who are now working in this domain have used this as their starting point. You get a good fundamental intuition out of this course. And as per python, it is not very hard. You can try YouTubing it. There are several free 6+ hour contents on it. I would really recommend that you finish some of these free python courses from YouTube and go ahead with andrew’s specialization on coursera. Once you are done with the specialisation, you will be equipped with a good grasp of fundementals, which is feel is very necessary for anyone who’s trying to get into this field.",1.0
g7ww78a,j67r95,"Not overfitting.

To me it seems like the validation set is much easier that the training set. Could be that they come from very different sources, could be some kind of imbalance. Not necessarily a problem if the validation set is truly representative of the use case.",32.0
g7xj2pr,j67r95,"Many loss functions (unspecified here?) are a function of the quality of the solution \_and\_ a function of the prevalence of, say, the positive class. As u/trexdoor calls out, if the validation set differs in distribution from the training set, then the distribution of loss will likely differ as well. Binary crossentropy loss definitely has this property.

If you think the training and validation data should come from the same distribution, and that training is representative of the validation problem, then you have a bug somewhere.

All that being said, it's not strictly required to train on data from the same distribution as the validation or test sets. You can train on random noise if it gets you desirable performance on the test set and ""in the wild.""

If you're ok with the mismatched distributions, you can do a few things to validate your model is ok:

* train a simpler model that must have worse validation error, and confirm that it does
* train a null model whose loss should converge to the expected loss for that validation data set (eg cross\_entropy\_loss = - (p \* numpy.log(p) + (1.0 - p) \* numpy.log(1.0 - p)) where p is the prevalence of the positive class for a binary classification problem iirc)
* watch the shape over the course of training (pointed out by other comments)... if validation loss starts to rise you are definitely overfitting
* probe the model with more difficult metrics... for example, compute a precision-recall curve on the validation set or a test set; compare models by the full shape of the curve, rather than single point summaries; watch for weird kinks in the curve that are likely exploitation of some weak feature of the test set that won't generalize
* try some qualitative tests IRL... does the model make predictions you are proud of? \[harder than you think\]. Any head scratchers?",8.0
g7xn7vr,j67r95,Also it may happen because things like dropout turned off on validation.,5.0
g7y8ifg,j67r95,Or a regularization penalty during training that’s off for validation,3.0
g7z1xpv,j67r95,"https://github.com/wirelesshydra/Text-Generator/blob/main/LSTM_1.ipynb

This is the GitHub link it will give you a better idea. The training and validation are from the same source i have split the data into training - 67% and validation - 33%",1.0
g7zbxu6,j67r95,"In cell 12, before splitting the data into train and val, shuffle it (i.e. shuffle variables X and y, and make sure their mappings are preserved as well). This will ensure the train and val are coming from the same distribution.

EDIT: In fact, looking at a sample of the text it looks like it starts at “Chapter 1” and is coming from some book. You can imagine that the last third of chapter 1 could contain easier sequences than the first 2/3’s (e.g. more simple word usage due to resolution of plot or something). This will result in a shift of distribution from training to validation.",2.0
g7z1ygs,j67r95,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/wirelesshydra/Text-Generator/blob/main/LSTM_1.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/wirelesshydra/Text-Generator/main?filepath=LSTM_1.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1.0
g7wwwmb,j67r95,"You're not accidentally training on the validation set, are you?",20.0
g7wovmt,j67r95,"It doesn't mean overfitting, no. I'd be overfitting if the validation curve went upwards at some point. I'd continue training until you validation curve flattens - this means it can't be improved anymore and stop before it shows any upwards trajectory.

Edit: I just noticed that the loss of your validation loss is lower than the training set - did you not switch them accidentally? If not then something doesn't seem right",9.0
g7wnzzj,j67r95,My guess is you have leakage between your test and training sets. So it's simultaneously overfitting to both.,11.0
g87uviz,j67r95,"He doesn't have a test set, only a validation set.",1.0
g7wo0zx,j67r95,Are you sure you have the right legends on the graph?,3.0
g7wo556,j67r95,Yeah I'm sure about it.,1.0
g7woo6d,j67r95,"Are you sure the validation set and the training set are completely separate? It shouldn't really be possible to have consistently lower validation loss than training loss (if they are normalized, in absolute loss units you could though).",8.0
g7xqc87,j67r95,"Yeah usually it's the exact opposite.. loss on validation slightly higher. If it were reversed then yes this would be evidence of over fitting. In it's current state this just doesn't make much sense, and indicates the validation and training data must be fairly different distributions (with the validation data being a small subset or something)",1.0
g7xd1xx,j67r95,"Are you calculating mean instead of sum?
Are the batch size same for bith train and val set?

Just some basic sanity check",3.0
g7x7eiv,j67r95,"For these kinds of questions, OP should really describe how they have set up the training and validation data samples and how they were divided. This is key because the validation data should ideally be representative of the distribution of data that the network is trying to generalize for, but is distinct from the training data itself. People often seem to mess this part up and, for instance, only select a subset that is representative of a small range of examples or sometimes allow the network to see the validation data during the training phase.",2.0
g7z2zlx,j67r95,"https://github.com/wirelesshydra/Text-Generator/blob/main/LSTM_1.ipynb

This is the GitHub link i have commented the code for better understanding please go through it and correct me if possible.",1.0
g8053i4,j67r95,"Yeah, you're passing the entire labeled data to model.fit(), which contains both your training and validation data. It should be model.fit(X\_train, y\_train,  batch\_size=128,epochs=100,validation\_data=(X\_test,y\_test)). Your model is training on all the data, including your validation data. So, if it is overfitting, you wouldn't know because you're validating against data it has already fitted.

Also, the best practice is to have three entirely separate sets of labeled data: training, validation, and testing. You fit the model on the training data, use the validation data to monitor for overfitting, and finally measure the accuracy of the model against the testing data as the ultimate test to see if your model is generalizing. Testing and validation are best separate because by using the parameters that best fit the validation data we are biasing the model. We don't know how much that biasing affects the model's ability to generalize until we test it against data it has never seen before.",2.0
g7z3097,j67r95,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/wirelesshydra/Text-Generator/blob/main/LSTM_1.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/wirelesshydra/Text-Generator/main?filepath=LSTM_1.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1.0
g7x853m,j67r95,This seems like underfitting to me. Train it for more epochs. Overfitting would happen when the training curve continues to decrease but the validation curve starts increasing.,2.0
g7y22ds,j67r95,"Not to be a stickler, but underfitting would mean you stopped training prior to covergence, overfitting is when you train too long that your covergence point has already happened and your actually memoizing things.

In some conditions overfitting isn't all that bad as long as the real loss is ok.",1.0
g7xfjmh,j67r95,Do u have dropout layers in your architecture?? Because i have seen similar loss patterns when adding dropout layers.,2.0
g7xhn95,j67r95,Looks good to me.. try upping your validation set size,2.0
g7y0mt8,j67r95,"Seems to me like a data leak, where the validation set is on average easier than the training set.",2.0
g7x91gv,j67r95,Are you using keras? Because I remember something similar happening to me. It's the way loss is calculated that makes it more while training and less while evaluating on the validation set.,2.0
g7x9xdi,j67r95,"Agreed, I've seen this behavior before in keras.",1.0
g7yzn4f,j67r95,Yeah im using keras.,1.0
g7wzmx3,j67r95,"Not overfitting, but convergence is suboptimal. I would first use a very small amount of data and try to get the model to overfit (test on the same data as well) as quickly as possible by tweaking the hyperparams.",1.0
g7xe0dn,j67r95,There's not enough information to say for sure.  What do your data setup functions look like?  Are you scaling the whole dataset before splitting into train and test?,1.0
g7xhwbf,j67r95,How have you chosen the validation set? It should be a random selection.,1.0
g7xj64c,j67r95,Could you describe it in a detailed manner ?,1.0
g7xpc39,j67r95,"As others have noted before, if you are using dropout layers in your architecture, that could explain this behavior.

And here is a short explanation why:
Dropout ist a regularization that for each train step randomly ""deactivates"" neurons, i.e. sets their activations to zero with the probability you provide during configuration. The reason to use this kind of regularization lies in preventing that only a subset of neurons in a layer learn relevant information and the subsequent layer just relies on these more effective neurons to give the correct output.

As dropout is only active during training, during evaluation all neurons are available, which in the ideal case means that more useful neurons contribute to the output.
This in turn, can lead to the evaluation problem being easier than the training problem which manifest as the evaluation loss being better than the training loss.",1.0
g7zda6k,j67r95,"In case of overfitting, the training loss decreases and the validation loss keeps increasing. So no.",1.0
g7zeja4,j67r95,"First and most obvious thing to check (as other comments have said) would be to check if the test and validation sets are coming from different distributions. I’d probably reshuffle the data and rerun the training just to be extra sure.

Second thing is look for is if you overfit your hyper parameter tuning to your validation set. The way I’d recommend to check this would be instead of splitting your data 67/33, do 60/20/20 (or anything in that ballpark) and have a train set, dev set, and test set. Then build your model to perform well on your train and dev set, and reserve your test set for situations like this so you can see if that’s truly how it performs on now data or if your hyper parameters are just optimized to do well on your validation set.

Hope this helps!",1.0
g7wo0zq,j67r95,"Is this your first time in the field of DL?   


If it were overfitting, validation loss would be going up instead.",-5.0
g7x05iv,j66m30,Does this depend on the random seed? Did you tweak other hyperparameters? How does normal sgd fare? - these are some of the questions I would ask myself in similar situations.,1.0
g7x19wc,j66m30,"Adam if you're just deciding between the two.

Also try SGD with added momentum if you have the option.",1.0
g7xb8y0,j66m30,You can also try out Adagrad,1.0
g7w7zup,j65h41,Here is the link to the original paper: [https://ondrejtexler.github.io/patch-based\_training/](https://ondrejtexler.github.io/patch-based_training/),1.0
g7veceo,j5va2k,"I think the most direct route with a BS is to go for a SWE role that touches on deploying ML models. For example, Google doesn't have ML engineers, they have SWEs for that job.",5.0
g7us0d6,j5va2k,What kinds of jobs are you going for? Do your projects include an application of computer vision or a demonstrable knowledge of robotics?,2.0
g7utixi,j5va2k,"I usually apply to ML jobs that have a deep learning focus, but there are very few where I live so I suspect it is very competitive. I have one project that is an image classification of foliar diseases, not sure if that counts as computer vision though. I'm really interested in content generation with GANs right now and thinking of project ideas to bring to life rather than just slapping a model on a kaggle data set, as I'm taking the [deeplearning.ai](https://deeplearning.ai) course in them now.",1.0
g7vzcue,j5va2k,Yes of course. Look for applied research roles and come with a solid resume of project work to show you have real skills and not just academic knowledge. There are dozens of shops in each state that need people who have a basic understanding of models to implement SOTA for their projects but dont necessarily want to fork out the money for a PhD.,2.0
g7wexk6,j5va2k,Are there remote jobs? Do you know where the postings are?,1.0
