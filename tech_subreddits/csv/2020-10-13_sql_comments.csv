comment_id,post_id,comment,upvotes
g8pw0kl,jajecc,"Without knowing what your tables contain, here is a general thought: if I do

    from tableA inner join tableB on (tableA.id = tableB.id)

I should check to see if [ ](https://b.id)tableB.id is a primary key or unique constraint.

Example: I have three tables:

    student: student_id / name )student_id is unique)
    student_class: student_id / class_id (student_id + class_id is unique)
    student_major: student_id / major (student_id + major is unique)

I have one student id 22 name smith Smith. He is taking 4 classes (Algebra I / English Literature / Ceramics / Biology). He has two majors (Math / Biology).

    select s.name, sc.class_id
     from student s
      inner join student_class sc on s.student_id = sc.student_id
     where s.student_id = 22 ;

will return 4 rows (the 4 rows in student\_class joined to 1 row in student)

&amp;#x200B;

    select s.name, sm.major
     from student s
      inner join student_major sm on s.student_id = sm.student_id
     where s.student_id = 22 ;

will return 2 rows (the 2 rows in student\_major joined to 1 row in student)

&amp;#x200B;

    select s.name, sc.class, sm.major
     from student s
      inner join student_class sc on s.student_id = sc.student_id
      inner join student_major sm on s.student_id = sm.student_id
     where s.student_id = 22 ;

will return 8 rows (1 row in student \* 4 rows in student\_class \* 2 rows in student\_major)",2.0
g8pw1ca,jajecc,"Take a look at the data you're adding the join to. 

It sounds like either a) your join is insufficient or b) the data in the answers table is not as expected.",2.0
g8pygnj,jajecc,"I think the `SELECT DISTINCT` is likely messing you up, suppressing your row counts. I would suggest removing the `DISTINCT`, then building your row results table by table to see how your record count changes.",1.0
g8q0yb6,jaj2ks,COUNT(hands) from dbo.confusedUsers,1.0
g8p9npx,jae9rr,"WITH tbl1 AS (
    SELECT 
        rank,
        ID,
        date
    FROM
        src
)
SELECT
    a.rank,
    a.ID,
    a.date,
    CASE WHEN date_diff(a.date,COALESCE(b.date, a.date)) &lt;= 90 THEN 1 ELSE 0 END 
FROM
    tbl1 a
    LEFT OUTER JOIN tbl1 b
        ON a.ID=b.ID and a.rank=(b.rank - 1)

Something like that should work. Do a self join on customer id and rank. Then you can compare the dates on the same row. 

Of course date and rank are both reserved words and won’t work in some dbms. Also date_diff syntax is different for different dbms. I don’t remember how todo it in Oracle.",1.0
g8pewli,jae9rr,"Hey! Thank you for sending me this logic and taking the time to send this.  I tried this logic, but it still isn't addressing the underlying problem.  So, by doing a self join,  it is comparing Rank 1-2, 2-3, 3-4,....etc.  What I am trying to do is have a date compare to the last valid date for a coupon.  So in the above example, lets say I am trying to determine if Rank 4 is a valid coupon date, the last valid coupon date would be date three so the comparison for Rank 4 is Rank 3.  For Rank 5, the comparison would be Rank 3 since that is the last valid date.",1.0
g8pw1pe,jae9rr,"Make the self join query a CTE and then select from it. 

Select ID, MAX(date) 
Where valid=1",1.0
g8q01ck,jae9rr,Thanks for the response! The issue is I am trying to make the Valid Coupon field.  It does not exist. The secondary table is trying to illustrate what I am trying to create.,1.0
g8p59y6,jada4u,"always determine and plan on granularity of your inputs and outputs.

always include data grain in the output.

very simple check for duplication/filtering errors is that count of returned records is exactly the number you need, without any 'distinct' or any other dubious math.",2.0
g8ouo6d,jacrno,"I'm not entirely sure why it's complaining about the Over clause, however I did notice that ""pid"" is in your select clause, but not your group by clause... With any luck, mysql has just lost its mind and forgot how to report an error, and that'll solve your problem? Lol

Other than that, I don't see anything wrong with your code at all, looks good to me!",3.0
g8p3qkm,jacrno,"Just to clarify: You're running a version of MySQL &amp;ge; 8, correct?",3.0
g8p5lzg,jacrno,yes,1.0
g8pibie,jabp6x,I am interested in getting involved and learning more. Please add me on Discord: Jhunter1#9560,1.0
g8owasg,jabblx,"I've never heard of the framework in this video, but OData is a mature, Microsoft-sponsored, widely-used technology for this.",2.0
g8oxzs7,jabblx,"For those wondering what OData is, below is an example

```
GET ~/$crossjoin(Products,Sales)
                         ?$expand=Products($select=Name),Sales($select=Amount)
                         &amp;$filter=Products/ID eq Sales/ProductID
```

Taken from OData's website, to do the equivalent that I did in the video (more or less) ...",2.0
g8ox0fp,jabblx,"Yup, but it doesn't add authentication or authorisation, and it requires (some) coding - But yes, OData can be used for this in the .Net space. However, Magic is also (almost) as fast as Dapper - Implying it would probably significantly outperform OData. Hyperlambda endpoints are also _""async by default""_, implying the developer doesn't even need to know how async works. And then of course, Magic also creates an entire backend for you (check out its [main landing page](https://polterguy.github.io/)) - And an entire frontend for you too (check out a [screenshot here](https://polterguy.github.io/tutorials/theming/)).

Not to mention, a guy who knows OData would still have to learn the API for how it transpiles QUERY parameters into SQL - Sometimes also sub-optimal may I add, leaving the developer little or no control over the generated SQL. With Magic, it's a copy/paste operation from Microsoft SQL Enterprise Manager or MySQL Workbench ...

But you *are* right, OData is widely used, and Microsoft sponsored, so was ActiveX, Silverlight, FoxPro, IE6, Visual Basic and ASP.NET Ajax ...

... ;)",1.0
g8pa6pz,jabblx,Any chance you might post a link that lets me read about your brillant solution rather than watching a video? I hate watching videos.,1.0
g8pbts2,jabblx,"Hehe, sorry man :)

https://polterguy.github.io/tutorials/sql-http-endpoints/

I *should* have created some screenshots - Note2self ...",2.0
g8pdjua,jabblx,Thanks!,1.0
g8pe76b,jabblx,"NP, [here's how it will look like](https://servergardens.files.wordpress.com/2020/10/screenshot-2020-10-13-at-7.20.50-pm.png). I didn't have room on my screen for everything though, but at least you can see roughly how the process looks like. Basically ...

1. Select database type (MySQL or MS SQL)
2. Select database
3. Create your SQL and arguments to endpoint
4. Click _""create""_
5. You have an HTTP REST endpoint

Configuring authorisation can be done in the _""Authorization""_ textbox ... :)",1.0
g8oifi0,jaarxr,Just create a new database for each data set. Using different installations would be overkill (unless you want to practice installing Postgres as well),1.0
g8ojdlb,jaarxr,"Will do. Can you help me understand the hierarchy in the pic in my OP? What is 1. PostgreSQL and 3. postgres? Is 3 the user postgres? I read somewhere that once you setup a server it's good practice to create a user and give that superuser privileges. If I did, where would this new user be? Under 3b. postgres? 

1. Servers

2. PostgreSQL 13

3. Databases

4. postgres",1.0
g8ojsif,jaarxr,"No, that's the name of the database. 

When you initialize [a data directory](https://www.postgresql.org/docs/current/creating-cluster.html) (aka ""database cluster"" or ""instance"") a default database named `postgres` is created. 

&gt; I read somewhere that once you setup a server it's good practice to create a user and give that superuser privileges

No, don't use a superuser for your ""normal"" work. What you can do, is create a regular user and make that user the owner of the newly created databases:

```sql
create user warmduscher password 'very secret';
create database tutorial_1
   owner warmduscher;
```

Now the (regular) user `warmduscher` can create tables and manage everything inside the database `tutorial_1` without being a superuser.",1.0
g8oyn4g,ja8hle,SQL for Data Science on Coursera.,2.0
g8of4a2,ja8hle,What position is the internship for?,1.0
g8ogl4o,ja8hle,Datacamp/dataquest,1.0
g8onl7v,ja8hle,"These Reddit posts I've seen previously here may help. Note the course is no longer free but may still be of use

[https://www.reddit.com/r/SQL/comments/j9f9bs/free_sql_server_fundamentals_book_10112020/?utm_medium=android_app&amp;utm_source=share](https://www.reddit.com/r/SQL/comments/j9f9bs/free_sql_server_fundamentals_book_10112020/?utm_medium=android_app&amp;utm_source=share)

[https://www.reddit.com/r/SQL/comments/j12zlc/free_sql_course_on_udemy_with_realworld_exercises/?utm_medium=android_app&amp;utm_source=share](https://www.reddit.com/r/SQL/comments/j12zlc/free_sql_course_on_udemy_with_realworld_exercises/?utm_medium=android_app&amp;utm_source=share)",1.0
g8p1l84,ja8hle,"Personally, I enjoyed the exercises on [sql-ex.ru](https://sql-ex.ru) in the past.",1.0
g8o854a,ja8bae,"    SELECT [ProductID]
    FROM [Production].[ProductInventory]
    GROUP BY [ProductID]
    HAVING SUM([Quantity]) &gt; 1800;",1.0
g8o8g9q,ja8bae,Perfect thank you! Would you mind explaining why you need to group it?,1.0
g8o9k78,ja8bae,"The GROUP BY clause groups rows that have the same value (in this case  \[ProductID\] ) into summary rows so that aggregate functions like SUM, COUNT, AVG, etc. can be performed. You can't perform aggregations without grouping at least one column.

[https://www.w3schools.com/sql/sql\_groupby.asp](https://www.w3schools.com/sql/sql_groupby.asp)",1.0
g8o9m90,ja8bae,Okay thank you!,1.0
g8nh0ws,ja42mm,"Use the generate\_series function in PostgreSQL

[https://www.citusdata.com/blog/2018/03/14/fun-with-sql-generate-sql/](https://www.citusdata.com/blog/2018/03/14/fun-with-sql-generate-sql/)",1.0
g8nh4rb,ja42mm,"How is your current view defined? If it's a simple ""select * from table1"" that's all you need to do. Views run the query underneath them whenever you query the view, so if you hit your view after data was added your view will reflect that automatically.",1.0
g8nil4q,ja42mm,"currently there isnt any view, a select* would get me the 1st table.",1.0
g8nj97f,ja42mm,"Ahh, gotcha. I misread that as you were going to have new data inserted into the table each week, not that you'd ONLY have that data and you needed to create new rows against it.

I think there are a few different ways you could go about this then. This likely isn't the most efficient, but the first one that comes to mind would be to select distinct week from your calendar table and then cross join that to table1. This would give you one row for each week/product combination for the weeks you specify in your calendar subquery, and you could use a dynamic date filter to adjust the weeks over time as needed.",2.0
g8neyw1,ja3p1z,"Query looks sound to me, sure you didn’t fat finger something?",2.0
g8nnhae,ja3p1z,"you cant insert data into an existing table using select into..you need to use an INSERT INTO statement. 



drop the existing table in your database and run the select into statement and see what it does.",1.0
g8n593e,ja26q5,"Sql, because pandas means having to use python which I try to avoid. You can use your sql queries in any language.",1.0
g8nitc6,ja26q5,Good video. Subscribed,1.0
g8mxdj2,ja01gj,"The immediate cause are bad statistics. The ultimate cause is a poorly designed schema and/or indices that leads to statistics not scaling properly.

I don't think it's so cut and dry. CTEs and sub queries can and often will be faster than temp table due to the performance cost of storing the data in tempdb. And obviously, temp tables just aren't an option in views and table-valued functions, and no, I do not recommend forcing everything into a stored procedure.

This is where your DBA earns his paycheck - understanding/analyzing the query plan, add query hints if needed, and most importantly, understanding why the generated query plan falls short.",35.0
g8nmmyc,ja01gj,"&gt; The ultimate cause is a poorly designed schema and/or indices that leads to statistics not scaling properly.

Eh, I wouldn't go that far. It's merely an example of asking a system a question that it wasn't designed to answer. That doesn't mean it's a bad design. It's just that the design was created with other things in mind.

There are queries in my system where I'd literally have to join 30 tables to answer some of the questions I need to ask of them, and I'd have to use multiple self-joins and joins against calendar tables with inequalities as the join conditions. We're talking about tables creating near cross-joins against each other with filter conditions 8 to 10 tables apart. And the data needs to answer the questions being asked of it... but not with complete 100% up-to-the-minute data.

Like, there's a reason data warehousing is a thing. Data warehousing is in many ways, ""Hey, let's just make the temp table for this report permanent.""",3.0
g8nqymq,ja01gj,"""Every system is perfectly designed to achieve the results that it gets."".  Or something like that.",1.0
g8o7uru,ja01gj,"""perfectly designed"" makes me cringe. How many times the first words coming from a top manager were ""perfectly designed"", only to realise that the initial design was far from perfect and the subsequent tunes were all ad-hoc solutions to save a sinking boat.",3.0
g8oztzg,ja01gj,"&gt;  It's merely an example of asking a system a question that it wasn't designed to answer.

Hey, don't you know you're not supposed to live in the real world when you talk about DB design?

You're the type of person I'd want on our team. Understanding something may not be designed perfectly, but there are a million reasons why it is the way it is. Nothing pisses me off more than when we have a problem and someone's answer is ""well the system was designed wrong!"" It was built 20 years ago, and we're in the process of a rebuild, but we have to work with what we have. 

Also, for the most part, out system works pretty damn good, and that's a credit to it's original designer. We're just hitting weird edge cases with massive amounts of data.",1.0
g8nmsel,ja01gj,"It sounds like you are agreeing. Not recognizing when you need to denormalize data is very much in the ""poor schema design"" wheelhouse.",1.0
g8nsz05,ja01gj,"This is a great comment and one of my main points of frustration with our BI team. They heavily criticize whenever we denormalize data... but our designs work way faster than there's, they solve problems, and they do it more efficiently. They whine because it takes up extra storage... which is true... but who gives a fuck? Storage is cheap and we aren't talking about multiple TB's, we're talking a few hundred MB or possibly a few GB.",1.0
g8p0i56,ja01gj,"&gt; They whine because it takes up extra storage... which is true... but who gives a fuck

OMG been there. I was so pissed about a similar situation last week I said I'll fucking buy you a TB hard drive out of my own pocket just so I don't have to deal with this issue again. Somehow someone though 90 chars should be the limit for a name, to save space.",1.0
g8nhity,ja01gj,"This is why I think I'd be entry level dba at best.

Although I am getting way more familiar with the limits of CTEs and sub queries over temp tables",2.0
g8nsrz2,ja01gj,"I agree with you but only at a certain point. A simple CTE, or simple sub-query is fine. When you start throwing in a ton of strange joins, and a ton of work in your SELECT (like CASE statements, functions, whatever) then generally speaking (in my experience) a #table will perform better than running the entire query as a whole.

It's a doulbe edged sword though because it is simply easier to write a query with subqueries or as a CTE, and its only once you reach a certain point of complexity that it starts making sense to chunk the job out into #tables and creating indices on those #tables.

Other people may be correct that it's due to statistics, or whatever, but at the end of the day I don't have time for a lot of that shit and I've taken jobs that take hours to run and gotten them down to running in 15 minutes by breaking the job up into chunks.",1.0
g8omyyg,ja01gj,"&gt;  A simple CTE, or simple sub-query is fine.

Unless you're referencing that CTE/subquery more than once in the same query. As soon as you do that, it's time to think about using a temp table because you're now executing that whole CTE/subquery _each time_ it's referenced.

I took a 15-minute query down to under 90 seconds just by moving a subquery referenced multiple times out to a temp table.",0.0
g8n11mx,ja01gj,"There are a lot of scenarios where a temp table is not an alternative to a subquery so I wouldn't really consider them competitors in that sense. It's like saying buses are better than pickup trucks or tanks. It depends on what you're doing.

But yes, there are scenarios when you're doing batch processing like you're mainly talking about where putting things into a temporary table (or even a normal staging table) can be the most effective way of processing data. You see this approach a lot with ETLs. But if you're building a front end system where a query might be run tens of thousands of times a day, I'd question whether populating a temporary table each execution would make sense.",9.0
g8nh5v9,ja01gj,"Well I'd argue that if you have a query that's running that often, it's likely worth caching it, unless it 100% needs to be in real time.

Most of the stuff I've dealt with in that scenario, benefits greatly from even a 5 to 15 second cache, hardware permitting. But that's more the software developer in me, not the sql part. Trying to get a better understanding of the database stuff",0.0
g8myjky,ja01gj,The place where temp tables won't work is if you need to have one atomic piece of SQL in a script.,9.0
g8myz76,ja01gj,"This doesn't answer your question specifically, but in my organization, the data science team does not have rights to create temp tables.  So, we are forced to use CTEs and subqueries.",7.0
g8n0ypy,ja01gj,"Same for me... we can’t create temps and can’t generate Data-staging forms unless you are “IT” which sucks.   CTEs and sub queries may take a little longer, but makes it easier to read for new employees IMO.",5.0
g8ndth9,ja01gj,Rules like this infuriate me as a (SQL Server) DBA. I can think of no legitimate reason to have this rule unless you _want_ to spend more money on hardware and licensing.,6.0
g8ndpx7,ja01gj,"In SQL Server, if you can execute a `select` query you have rights to create temp tables. It's not possible to revoke that permission AFAIK.

By ""temp tables"" I mean _actual_ temp tables, prefixed with `#` or `##`. Not creating a ""real"" table in the database and then having to remember to drop it when you're done.",3.0
g8nqdau,ja01gj,Not sure why you got downvoted. I agree there is no way to restrict user access to the TempDb space. I assumed that the OP was talking about local and global temp tables where the table names are prefixed with `#` or `##`. I don't think some people understand there's a difference between a real table and temp table.,2.0
g8omtud,ja01gj,"Probably because people didn't check the flair to know that we're talking about SQL Server here.

* MySQL requires a distinct security grant to create temp tables.
* Oracle requires `CREATE TABLE` to create a global temporary table which is kind of a weird beast. But in newer versions, anyone can create a private temporary table which behaves more like a SQL Server temp table except that it's in-memory instead of materialized to disk. Oracle CTEs can be materialized, which probably leads people to think of and use them like read-only temp tables (prior to the availability of private temp tables).",1.0
g8ph2nc,ja01gj,That makes sense. I'm not too familiar with the other flavors of SQL. Working in MySQL without temp tables sounds absolutely miserable. Do you know if MySQL has equivalent alternatives to temp tables if temp tables are restricted?,1.0
g8nhdfk,ja01gj,"If I were running your team you'd have access to that. Maybe only a select few, but junior level people could develop with that notion, knowing it would get reviewed.

If I code review anything that gets to production and dicks it up, that's on me, not the person who wrote it",1.0
g8muggz,ja01gj,When dealing with systems that use the tempdb like you are describing to break things down that can reach a volume point where the tempdb becomes a bottleneck even with appropriate hardware and configuration. So I've recently had to go the other direction for more straightforward complexities to CTEs which tend to use RAM to relieve the tempdb.,7.0
g8nhpp2,ja01gj,"So I know this is dumbing down things, but are you saying if we threw more RAM at the server it could help? Because we have plenty not used right now in anticipation of growth.

I know more processing power isn't bad, but I've been constantly told by our senior person that what we have is fine, when it's obviously not, during peak traffic",1.0
g8nryik,ja01gj,If CTE can fit in memory why write it to disc in a temp table?,5.0
g8nwers,ja01gj,"It's possible that performance is tanking on the CTE due to limited RAM but the query itself could also not be great and the temp table allowed you to restructure the query.

Check out SentyOne Plan Explorer to review the execution plan of the query. It's the same plan you can get in SSMS but it's easier to navigate. Also ignore the percentage cost it assigns to nodes, it's frequently misleading. Missing indexes may also be factor.

I don't want to make it sound like temp tables are a bad answer though.  I still use them where needed. Best case you can make the queries as basic as possible and have the code do any business logic stuff instead of tempdb or CTEs. It really depends on the usage volume if your going to get bottlenecks by tempdb.",3.0
g8n3dmn,ja01gj,"Generally, CTEs are better for smaller tables (&lt;100,000 records) and are good for recursion.",5.0
g8ngpey,ja01gj,That would make sense with what I've seen working here,1.0
g8nllyr,ja01gj,"With less than 100k records, performance shouldn't be an issue.",1.0
g8nog6c,ja01gj,"If nothing else it’s less code to create a CTE than to  create a table. Also, IIRC there’s less server overhead using a CTE vs a temp table",1.0
g8n466z,ja01gj,"i don't come from the MS SQL world... are temp tables also physical tables in this DBMS? are there some special optimizations for temp tables? because if the answer is no (as in some other DBMSs) then I'd much rather stick with CTEs, unless as you said, the table becomes very large.",2.0
g8nf492,ja01gj,"It is just a physical table, in a special database (tempdb).",1.0
g8ngnf9,ja01gj,"They are physical tables in a reserved database (tempdb). You could bring down a server if you filled up temp db with a bad query and also had no limits on it, but that is kind of the point.

If you have your temp db restrictions in place, when someone tries to slam 50G worth of data into tempdb it fails, but the server is fine",1.0
g8negge,ja01gj,"In SQL Server, with the exception of recursive CTEs, CTEs are syntactic sugar for subqueries and they'll behave pretty much identically. Unless you start nesting the CTEs, in which case you'll eventually hit a tipping point and performance will start to suck.

There are cases where the overhead of a temp table may outweigh the performance improvement over a CTE by a small amount, but in situations where I have a choice between the two, I go for the temp table by default and then move to a CTE if it makes more sense.",2.0
g8ngb8o,ja01gj,"Yea they've always just seemed to be a better alternative to me. And I'm not a sql guru, it's why I just wanted input",1.0
g8nhil8,ja01gj,What about table variables?,2.0
g8nizpg,ja01gj,Root of all evil. Kidding.,2.0
g8onhpu,ja01gj,"They're OK for very specific use cases. Like passing a set of data into a stored procedure or function. But they have serious performance issues until you get to SQL Server 2019 because they can totally hose your cardinality estimates.

* People think table variables exist only in memory, which means they'll somehow be faster. Table variables, if they get large enough, will spill into Tempdb - which is where temp tables live.
* People think temp tables _only_ live on disk. But just like any other table, the pages have to be in the buffer pool to operate on. So as long as your temp tables fit there (likely), they're sitting in memory anyway.

I've yet to encounter a situation where a table variable resulted in significantly better performance than a temp table (To fend off the inevitable response: I'm sure someone has. I just haven't seen it myself). I reserve them exclusively for passing data in and out of stored procs because there's very few ways to do that.",2.0
g8nm1oj,ja01gj,I think they have their place. We just deal with large enough data sets where table variables lose their efficiency,1.0
g8oibe3,ja01gj,"Thank you sooooo much for posting this discussion, it has been so very helpful",2.0
g8mrry1,ja01gj,I agree completely.  Temp tables &gt; CTEs.,5.0
g8o669f,ja01gj,What's your reasoning? There's been other posts in this thread outlining which scenarios CTEs are better in.,1.0
g8p4cml,ja01gj,"Depending on your data size, CTEs are all in memory until they run out and spill over to disk.  Temp tables can be indexed is another benefit.  Also, execution plans are saved for queries that have temp tables, I’m not sure CTE queries have saved execution plans.",1.0
g8muzm3,ja01gj,"Agreed, but I also use table functions, views, and rework the JOINS in place of subqueries/CTEs. Using fewer subqueries/CTEs also helps with code readability.",2.0
g8nhkmz,ja01gj,My man,1.0
g8mvovr,ja01gj,"to me, using tempdb's is better if you're dealing with processes or have to take the data into different steps to transform it. if it is legitimately faster than there is nothing wrong with it.",2.0
g8nqlph,ja01gj,"I wish.  They're incompatible with Visual Studio, or at least for what I need them for.  But when I can use them, I love temp tables.  Much easier to debug a query too.",1.0
g8ohsye,ja01gj,no DBA likes temp tables actually :),1.0
g8onk6x,ja01gj,"_raises hand_

DBA here. I love temp tables.",2.0
g8opex9,ja01gj,"me too.

I have all my users drilled

        if object_id('tempdb.dbo.#temp','U') is not null
        drop table #temp;
    
The only thing I'd add is creating large temp tables without defining data types can lead to problems on larger datasets where type isn't constant (table a joining to temp on string column a = temp column that only had ints in it's generation dataset)",1.0
g8otdjr,ja01gj,What do you use instead?,1.0
g8ou4lc,ja01gj,"never temp tables.
either subquery factorisation or sub queries
or well constructed joins.

but im coming from Oracle. maybe for other databases there might be other practices.",1.0
g8owddz,ja01gj,Yea I think it's different in MSSQL,1.0
g8pe21r,ja01gj,"It’s different for SQL Server, temp tables are heavily optimized in SQL Server.",1.0
g8pepu5,ja01gj,"From a SQL Server point of view:

I reserve CTEs for when I need to use the same subquery multiple times in a single statement.

I reserve subqueries for very basic lookup things such as getting a label/name using some ID.

Otherwise I use temp tables. You can garbage collect them yourself as you go, as well as index temp tables. They’re incredibly useful.

Then again, I keep my development extremely simplistic even for the most complex of tasks. Each SQL statement in a long stored procedure would generally do one “thing”. Much like in general programming a function does one “thing”.

I prefer this approach because it makes it much easier to read months/years down the line when I open it back up, or anyone else does.

People writing multiple nested subqueries and throwing everything into a single SQL statement, unless absolutely needed, really aren’t doing any favors for themselves.",1.0
g8pgt0p,ja01gj,You pretty much share all the same view points I do as well.,1.0
g8n0ve2,j9zwa1,[https://stackoverflow.com/questions/17905873/how-to-select-current-date-in-hive-sql](https://stackoverflow.com/questions/17905873/how-to-select-current-date-in-hive-sql),1.0
g8nco7e,j9zpn2,"I've had to work on a few databases where the names of the tables and fields were not easy to work out what they are and only time analysing the data allowed me to understand what the fields were and how the tables were joined.  The advantage you have with this one is you do have a diagram of how the tables are linked.

As for trying to work out the naming of the fields, do a simple select * on each table and look at the data.",1.0
g8nuhsn,j9zpn2,"Use the sample questions and sql answers to determine what’s in the table. The question will tell you what the business logic is, the example SQL specifies the tables used and the code to implement the logic.",1.0
g8msffl,j9z3xy,"&gt; is there an easy way to dynamically add columns with the column name being the key name stored in the json field

No. 

One fundamental restriction of the SQL language is, that the number, names and data types of all columns of a query must be known to the engine _before_ the query is executed (i.e. while it's parsed). 

What you can generate dynamically is one row per JSON key, but I assume that's not going to help you.",1.0
g8mvyt1,j9z3xy,Got it- thanks for the heads up. Looks like I will use some Python on this then.,1.0
g8mgh3i,j9y7ho,"Do you know about ""LEFT JOIN""?",1.0
g8mgqrc,j9y7ho,"Nope, can you tell me about it?",1.0
g8migyo,j9y7ho,How would you do the query then?,1.0
g8mk3hk,j9y7ho,"It combines rows from multiple tables, using ""ON"" to specify how rows from each table are joined. Using left join you can find lesson hours without matching lessons.",2.0
g8ms0u0,j9y7ho,"Ah okay, but what's in the WHERE clause then?",1.0
g8o77ss,j9y7ho,"Form the query to join the tables without a WHERE, have a look at the results, and you'll see what the WHERE should be.",1.0
g8mbb95,j9wxen,"It's been a while since I've worked in Access but this may work:

    SELECT DISTINCT ssn
    FROM accdb
    WHERE date1 = date2
    GROUP BY ssn, moneyValue
    HAVING COUNT(ssn) &gt; 1",3.0
g8me1jq,j9wxen,"&gt;  select the ssn that appear at least twice

    SELECT ssn
         , COUNT(DISTINCT moneyvalue) AS moneyvalues
      FROM yourtable
    GROUP
        BY ssn
    HAVING COUNT(*) &gt; 1
    
&gt; From those results i want to only get the ones where date1 is equal to date 2

    SELECT yourtable.ssn
         , yourtable.date1
         , yourtable.date2
         , yourtable.moneyvalue 
      FROM ( SELECT ssn      
                  , COUNT(DISTINCT moneyvalue) AS moneyvalues              
               FROM yourtable    
             GROUP               
                 BY ssn          
             HAVING COUNT(*) &gt; 1 ) AS these
    INNER
      JOIN yourtable
        ON yourtable.ssn = these.ssn
       AND yourtable.date1 = yourtable.date2
       
&gt; From those results i want to get the results where there are different values in moneyvalue per ssn.       

    SELECT yourtable.ssn
         , yourtable.date1
         , yourtable.date2
         , yourtable.moneyvalue
      FROM ( SELECT ssn      
                  , COUNT(DISTINCT moneyvalue) AS moneyvalues              
               FROM yourtable    
             GROUP               
                 BY ssn          
             HAVING COUNT(*) &gt; 1 ) AS these
    INNER
      JOIN yourtable
        ON yourtable.ssn = these.ssn
       AND yourtable.date1 = yourtable.date2
     WHERE these.moneyvalues &gt; 1",1.0
g8o42wc,j9wxen,"&gt;SELECT ssn  
, COUNT(DISTINCT moneyvalue) AS moneyvalues  
  FROM yourtable  
GROUP  
BY ssn  
HAVING COUNT(\*) &gt; 1

Thank you, does not work in ms access however. It gives me a syntax error on the count.",1.0
g8m9306,j9ww7f,"I think it depends a bit on the DBMS being used. 

For Postgres, Linux is definitely the better choice, because many interesting tools and extensions are only available on Linux. 

For Oracle I don't think it matter, although I do have the feeling that the filesystem performance on Linux is a bit better. 

For SQL Server I can't tell, but probably Windows, given the fact that the Linux version doesn't have such a long history and many interesting tools are probably only available on Windows.",2.0
g8m8nqc,j9ww7f,"I’m not a DBA, but in my opinion Linux would provide you with a bit more experience just because of the command line. Really the gap is small and operating system should matter very little to a DBA. Why not both",1.0
g8nv3hr,j9ww7f,If you’re using big data choose Linux. Otherwise use what your dbms suggests.,1.0
g8m967q,j9wiuy,OK If I'm reading that XML correctly the data is stored in the attributes of the element Data. Data is an element of Level. Level has an attribute Number. Does that help?,1.0
g8mbuq5,j9wiuy,"Yeah so in a single row in the database I would have a value for 530, 540, 550, and 560, all sharing a single unique ID and time stamp. I'm trying to figure out how to say to put value 55.5555 in the 530 column and put 66.6666 in the 540 column.

I basically need to tie the number attribute to a column, as seen below:

https://i.imgur.com/HVqlvfX.png

So that screenshot is basically the goal",1.0
g8modct,j9wiuy,"So my thought is since you are using SQL Server is to do a pivot query against the XML. 

I would try using a CTE to parse out the XML data then take the results of the CTE and Pivot on that to get the result with the correct column headings and then insert that result into your database. This is a one query method that may perform poorly if you have a lot of data.

If the CTE does not work parse the XML into a temp table then pivot from there to another temp table if necessary then insert from that. This divides the work into smaller bites which can perform much quicker that the one big bite above. Also I usually add the appropriate index to each temp table as I go.

Good luck.",1.0
g8m28iu,j9vn5p,"&gt; Any advice on how to tackle this project?

throw each question into google, appending with ""in SQL""

then try out the queries that are suggested

i trust you have some way of verifying the actual numbers that your queries produce?  like, loading your sample data into excel and working out a few of the answers that way?  because running a query and getting some numbers back isn't enough, you have to know whether the answer is right or not",1.0
g8mk4qw,j9vn5p,I’ll have to check with my professor on whether the answers are correct. Good advice,1.0
g8mkt5o,j9vn5p,"&gt; we do not want to rely on the professor's help too much

no, seriously, dump your data into excel and do a couple of the problems there to confirm your query results",1.0
g8m9yai,j9usjw,"Sounds a bit like queues.

https://docs.microsoft.com/en-us/sql/t-sql/statements/create-queue-transact-sql",3.0
g8mmput,j9usjw,"With an index on \`eventflags\` it should be efficient enough.

If you're concerned about scale, there are better mechanisms. In MS SQL, queues (as u/phunkygeeza mentioned). Out of MS SQL, look at MSMQ, NServiceBus, and similar queueing mechanisms.",2.0
g8ldqsl,j9rhy0,"use a translation table

    model      phone
    -----      -----
    Galaxy     Samsung
    Epic       Samsung
    Intensity  Samsung
    iPhone     Iphone
    XS         Iphone

then your CASE statements all collapse into one

    CASE WHEN phone LIKE '%'||translation.model||'%'
         THEN translation,phone
         ELSE ...",2.0
g8lh5in,j9rhy0,Amazing! Thank you so much.,1.0
g8lk0ze,j9rhy0,you'll have to use that LIKE in the join clause as well,1.0
g8m8wi6,j9rhy0,"You could always do more grouping:

    table device_list
    device / model/ company
    galaxy s20 fe 5g / galaxy / samsung
    galaxy z fold2 5g / galaxy / samsung
    etc...

 If ""device"" contains all the distinct values for the device column in the other table, it becomes an equality join that can nicely use an index. Of course it also means that any time a new device shows up, it has to be first entered in the ""device\_list"" table.",1.0
g8nvjmy,j9rhy0,"This idea is correct. Keep a table with device attributes that you can reference later. Also have a report/alerting system when new device types are detected. 

The keys for that table would be manufacturer, and model. You could track screen size and OS information.",1.0
g8nf2vo,j9rhy0,"You could use IN. 
CASE
  WHEN device IN(‘y91’ , a35s’)
  THEN etc

Use excel to list your models, then in one  cell have “ ‘ “ and in another have “ ‘, “ then CONCAT them.",1.0
g8l36pn,j9pw7w,"Look up Django. It comes with SQLite built in. 

You can do anything you want with database querying.",1.0
g8lg3j5,j9prug,depends on how you configured it.   do you have automatic failover set on?,2.0
g8kugdx,j9o95t,Why not just use a date data type?,3.0
g8l3wv4,j9o95t,Agreed could definitely use this. Would it identify the impossible dates from the date set though ?,1.0
g8l6vct,j9o95t,"Case when inputdate &gt; current_date() then ...

You can opt to do that in the application instead of the database if you want.",1.0
g8kvs34,j9o95t,"first of all, `NOT LIKE` and `IS NULL` cannot both be true at the same time, so if you `AND` them, you'll get no results at all

my approach would be to use 

    CASE WHEN STR_TO_DATE(Date_Of_Birth,'%Y-%m-%d) IS NULL
         THEN CONCAT(Date_Of_Birth,' - Fail') 
         ELSE '' END  AS Date_Of_Birth_Flag",2.0
g8lkq4l,j9o077,"Oracle Text index will start with an exact match and gradually relax the search using wildcards, synonyms, and soundex. It will also normalize the characters to easily handle things like umlaut (Ü)",3.0
g8lqxcq,j9o077,"That is interesting, could you point me to some source?",1.0
g8mv37v,j9o077,"For full documentation have a look here: https://docs.oracle.com/cd/B28359_01/text.111/b28303/ind.htm

For a demo of it in action while building a web application in APEX, have a look here: https://youtu.be/XinLHvAzqzc?t=910

You can skip around that video as necessary. Audio could be better, but Carsten is an absolute Oracle expert",2.0
g8l1lj7,j9o077,"Interesting, thanks.",2.0
g8kuf50,j9m29n,"If you will need to look at every row that you are loading (ie you have no driving filters) then indexes are not going to help. I suggest, if you have a performance problem, look at where the time is going and aim to fix that.",1.0
g8lbs94,j9m29n,yes full scan basically on all just-loaded tables,1.0
g8mcrbk,j9m29n,Are you doing any filters so that it would be a good idea not to read the entire tables?,1.0
g8psady,j9m29n,zero filters,1.0
g8kngyd,j9ks37,"The psql is not particularly hard to play with. You start by exploring (read asking google) how to run a .sql script from terminal with psql. Or you could go to psql documentation. After that do the same but  using the credentials from, for example, the env variables.",1.0
g8knygh,j9ks37,"Note however that tools like pgadmin, dbeaver etc are extremely convenient. Knowing how to use psql in a shell or something like that it is important for example to prepare containers for a particulsr purpose. But to explore data etc I'd say pgadmin is far better.",1.0
g8ktioz,j9ks37,"I also like pgAdmin but I'm trying to become less afraid of the command line. 

Do you feel the same way about MySQL Workbench?",2.0
g8ksvhj,j9ks37,"If you know which SQL statements to use in pgAdmin, you already know which SQL statements to use in `psql`",1.0
g8kth64,j9ks37,Then are the only differences between PostgreSQL and MySQL setting up databases? Because I tried to navigate `psql` using `show databases` and `show tables` but I quickly learned that the correct commands were `\l` and `\dt`. How do I know there aren't other differences between MySQL and PostgreSQL?,1.0
g8kw3k3,j9ks37,"You didn't ask for differences between MySQL and Postgres - you asked about moving from pgAdmin to psql (`show databases` or `show tables`  doesn't work in pgAdmin either). 

If you extensively used the pgAdmin UI to find information about database objects, then yes there will be a learning curve. 

All commands that are available in psql (like `\l`) are [documented in the manual](https://www.postgresql.org/docs/current/static/app-psql.html) I highly recommend you go through all them at least once so that you get an understanding on what meta-commands psql provides.",1.0
g8kwea3,j9ks37,"If someone is new like me, should I start with MySQL? For example, is MySQL used more in enterprise than Postgres? I find it more logical to use `show databases` and `show tables` rather than `psql` commands.",1.0
g8kxdu1,j9ks37,"&gt; For example, is MySQL used more in enterprise than Postgres? 

This is hard to answer. Where I work nobody uses MySQL. But if you ask someone else they probably say that MySQL is used more than Postgres. Neither of them publishes reliable usage stats. 

I personally would always recommend to start with Postgres as it is a much cleaner product to work with and has more feature than MySQL. 

Whether or not something is ""more logical"" is also based on where you come from, e.g. Oracle has neither `show tables` nor a `\l` command - you need a `select` statement in `sqlplus` to achieve the same (the same is true for SQL Server).",1.0
g8k2byq,j9h6b0,"1., a &amp; c

2., b

3., b

4., c

5., b",1.0
g8kipkj,j9f9bs,Can't just get it in pdf?,4.0
g8lmkf1,j9f9bs,"Sorry I currently do not have it in PDF format, just kindle ebook format.",1.0
g8jx9uz,j9f9bs,Thank you so much!!,3.0
g8jzbqf,j9f9bs,You’re welcome! I hope you like it!,1.0
g8je9az,j9f9bs,Didn't work for Europe.,2.0
g8jf6lm,j9f9bs,Does it show as regular price?,1.0
g8jfxs5,j9f9bs,Unavailable :-(,2.0
g8jge0l,j9f9bs,Hmm. Can you send the link for your country’s Amazon site?,1.0
g8jjpx8,j9f9bs,I got it. Thanks very much. The link only works for the US. I used Amazon.co.uk,2.0
g8jjya0,j9f9bs,Oh okay great! Yeah that’s happened before in the past which is why I mentioned that you could just search part of the title for your country’s Amazon page. Found it for the UK: https://www.amazon.co.uk/Learn-SQL-Practical-Database-Fundamentals-ebook/dp/B07D5S2W4Y/ref=mp_s_a_1_1?dchild=1&amp;keywords=learn+sql+jacob&amp;qid=1602460964&amp;sr=8-1,2.0
g8k0zth,j9f9bs,Works fine for me in the UK: there was a link on the right hand side that took me to the UK store instead and it was right there,1.0
g8jrqmz,j9f9bs,Thanks man,2.0
g8jz8z7,j9f9bs,You’re welcome man! Thanks for the support!,1.0
g8k1fc1,j9f9bs,"I've not dug in yet, but it looks good - thanks. If nothing else, the chapter on Users/Logins/Permissions should be helpful - I primarily consumer databases as a developer, and another group in my department handle the permission and administration.

It's something I'd like to know more about, but never really have occasion to since I don't have access to do it myself. Obviously I have some idea what's going on, but I often find myself being unsure of exactly which permission I need to request...",2.0
g8k8man,j9f9bs,"Ah that makes sense. With limited visibility, it can be difficult to really pick up some of the specifics and gauge the level of access. The access doesn’t delve too deep since it’s a fundamentals book, but it gives an overview of server and database level permissions. Being that you’re a developer, you might know most of this stuff already, it may serve as a refresher or you may pick up something new haha. 

I won’t have much access to it at my company other than viewing permissions for server level roles and database level roles, so messing around on my own instance really gave me a better understanding of it.",1.0
g8kqn0f,j9f9bs,I got charged for it :(,2.0
g8lmiim,j9f9bs,Shoot. Can you see if Amazon is able to refund you? I can send you a DM the next time it’s free so that you can grab a copy.,1.0
g8ln7ci,j9f9bs,"Okay, thanks. I have requested refund.",2.0
g8lqs7v,j9f9bs,Okay sounds good. I will send you a DM when I have the next promo going. Thanks for the support!,1.0
g8j9189,j9e4vf,Book format is its own table (entity). A book can have 1 to many book formats.,3.0
g8jddk2,j9e4vf,"Creating a new entity was my initial thought. So when a customer buys the book, would a good trick be to append the format code to the end of the book id?",1.0
g8j7qxr,j9e4vf,Have a BookFormat table with titleID to join back?,2.0
g8j8p7t,j9e4vf,Are you suggesting to join the two attributes? Ex. A book with the id of 0001 that is in audio format would be 0001audio ?,1.0
g8j94h9,j9e4vf,"No your bookformat table would be
Titleid, audio, hardback,paperback,ebook
With a 0 or 1 value if it exists in the format type",1.0
g8jci2o,j9e4vf,"Ohh this is a great idea, thanks!",1.0
g8kmbd1,j9e4vf,"If you are designing an OLTP solution that should be normalized, simplicity isn't your goal. 3rd  or 4th normal form is. 

You have a many to many relationship between books and format. You have to bust up the M:M with a gerund table.

So, based on what you told us yields:

Book(BookID, BookFormatID, other attributes...)

Format(FormatID, BookFormatID, other attributes...)

BookFormat(BookFormatID, BookID, FormatID)",1.0
g8knivo,j9e4vf,What would be the difference between FormatID and BookFormatID?,1.0
g8m6zee,j9e4vf,"BookFormatID is the primary key of the BookFormat table. FormatID is the primary key of the Format table. I'm sorry. I haven't used this table notation since college (2002). Here is a clearer version.

 

Book(BookID PK, BookFormatID FK, other attributes...)

Format(FormatID PK , BookFormatID FK, other attributes...)

BookFormat(BookFormatID PK , BookID FK, FormatID FK)

&amp;#x200B;

Technically, in the BookFormat table, you could make BookID and FormatID a concatenated primary key, but I'm not a fan of that design pattern so I give BookFormat a single auto incremented value as a primary key.",1.0
g8jjf8n,j9e4vf,"How about a multi valued attribute? You can name it book format and it will be an attribute of the book entity. Might be the easiest way.

In saying that, I think there is no need to overcomplicate this. Book format can be a normal single valued attribute of the book entity. 

An example of this is cars and car models. Models can include BMW, Subaru, Maserati... A model would be an attribute of the car entity.",0.0
g8j4z92,j9duxm,"What would you say your level is after completing I need to learn sql I only know very basics......
Link to course?",2.0
g8j5fe6,j9duxm,It's takes you beginner to advanced I'd say. Go to Udemy.com.,1.0
g8jh5d5,j9duxm,"Which bootcamp was this? There are so many that I don't really know where to start. I do have some knowledge of it, learned it a bit in 2 classes I took at university",2.0
g8jh8fa,j9duxm,https://www.udemy.com/share/101WsUAEEed19UR3QF/,2.0
g8lf3du,j9duxm,"I would say practice what you learned using the Adventure Works db. That bootcamp is a great starting point, but it is only a starting point. Next, you should probably learn relational database concepts, how to performance tune a SQL query, and some general best practices. Also, figure out what career direction you want to go with SQL (Data Analyst, Database Developer, DBA, BI Analyst, etc).",2.0
g8lflt1,j9duxm,"I found this -  [https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver15&amp;tabs=ssms](https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver15&amp;tabs=ssms) 

What do I do from there?",1.0
g8lmdzj,j9duxm,"Download either the OLTP or Lightweight bak file. This is a backup of the database. Next, you need to restore the the the database to PostgresSQL, if that's the RDBMS you want to use.

How to restore a db for Postgres: [https://randomprogrammingstuff.wordpress.com/2017/10/13/postgres-restore-bak-database-backup-command-example/](https://randomprogrammingstuff.wordpress.com/2017/10/13/postgres-restore-bak-database-backup-command-example/)

[https://www.postgresqltutorial.com/postgresql-restore-database/](https://www.postgresqltutorial.com/postgresql-restore-database/)",1.0
g8k23jx,j9duxm,Apply for a job?,1.0
g8is359,j9cisq,"Looks nice, I will definitely try that",1.0
g8j6ov6,j99i31,Thanks for posting the video link and the proposal for similar videos.,1.0
g8hapnw,j94si4,"Select letter_grade, count(name)
From “table”
Group by letter_grade

This will give results like:
A | 10
B | 6
C | 3",3.0
g8hcdut,j94si4,"Thank you, it solved it.",2.0
g8hei2w,j94si4,You’re welcome,1.0
g8hkn34,j94si4,"Are we just doing their homework for them now?  Not even attempting to help them work it out on their own with a bit of help?

This is like the simplest GROUP BY query you can have.  OP needs to study a bit more.",3.0
g8hl15s,j94si4,"Understandable. I learn well by seeing the answer. That might sound strange, it might not make sense to some people, but it works for me",2.0
g8hmkkr,j94si4,"I’m doing it off Khan Academy, and I tried everything to get it correct. I literally spent 4-5 hours on trying to solve it and wasn’t able to get it right. Maybe I can take help from here and understand why I couldn’t get it. But you can keep assuming too, nothing wrong with it.",-1.0
g8hmtr1,j94si4,"4-5 hours on the simplest use of a GROUP BY does *not* bode well for you.  The most difficult part of that assignment was using a CASE to convert number_grade into letter_grade, and the directions gave you that much.",-3.0
g8htdv9,j94si4,"OK, but he's a beginner. I write complex queries and procedures now but there was a time when I first started out when I just couldn't get my head around joins. Imagine if I asked you for help and you said ""not understanding joins does not bode well for you"". I would have been discouraged and down on myself. Your attitude helps no one.",2.0
g8hp1q4,j94si4,That’s why I’m learning it. Furthermore I know about CASE coz I’ve learned Pseudocode in school. But they didn’t teach GROUB BY.,1.0
g8hrlzj,j94si4,Nice this must be new by khan academy,3.0
g8hdpy3,j946ts,Thank you for this!,3.0
g8iyx83,j946ts,"Thanks for sharing this, pretty informative.",1.0
g8jljvj,j946ts,This is amazing. Thank you.,1.0
g8fq8kf,j8zfw0,Does the orders table have the ZJLB_BOOKS field? Also why a sub query? Couldn't you just join the tables?,3.0
g8fr93y,j8zfw0,"no, it doesn't. that why i am asking for help. 

i wrote it another way and still have same issues 

UPDATE ZJLB\_BOOKS

SET ISBN = '1059831198'

WHERE COST = '20'",1.0
g8fshg1,j8zfw0,"I think you will first need to understand the concepts of tables, columns, and how to use them, before attempting update statements.",9.0
g8ft7vi,j8zfw0,"+=1 Tremendously so, OP.",5.0
g8j8nug,j8zfw0,"Spot on. No offence meant to OP but unless this is something where you are learning SQL away from work then you shouldn't have access to update. I would honestly suggest having your write permissions revoked if this is an actual work query, again don't mean to sound bad. 

Also always wrap any ad-hoc update statements in a transaction with the commit commented out first. Check the number of rows updated is as expected and then commit.",2.0
g8ftjgd,j8zfw0,"thats why i am asking for help.

so i can learn where i missed up at.",-1.0
g8g08iv,j8zfw0,"Mate, they told you where you messed up. You can’t update a field that doesn’t exist. If this is for a class you need to go back to the earlier work and understand table structures first before trying updates, deletes, or inserts. If this is for work please stop. You can destroy your data with one bad update if you haven’t kept a proper backup.",10.0
g8g2p75,j8zfw0,SELECT * FROM STRAIGHT_FACTS,5.0
g8hsz1i,j8zfw0,Saving this comment to use the next time someone emails me asking for help on their query.,3.0
g8hoq4y,j8zfw0,"OK Step 1. Do you know how to find the set of a columns in a table? If you don't know how to do that, you need to find that out first before you come back to ask more questions. In oracle I would type DESCRIBE ORDERS or DESCRIBE BOOKS (assuming that ORDERS and BOOKS are tables in my database).

Show us the columns in those tables.",3.0
g8ilvst,j8zfw0,"This is even more dangerous than the first. You will assign this ISDN to ever record in the table with a cost of $20.

You're missing the key piece of information that links the books table to the orders table. I'd presume there is a foreign key in the orders table that joins to the primary key of the books table.

I'd agree with others that a more fundamental understanding of relational databases would be required before you can successfully and safely write and execute update statements.

If you can provide more information (such as the actual h/w assignment), we will be more than happy to explain the required concepts.",2.0
g8fw41z,j8v63t,"The first thing to know about creating Oracle DBs compared to an alternative RDBMS like SQL Server or PostgreSQL is that creating DBs is a \_much\_ bigger deal.

PostgreSQL uses a single createdb command and takes about 2 seconds. For Oracle you need to use a UI wizard or a complex script and the actual processing to create the DB can take 30-60 minutes, depending on what hardware you're using.

The reason for this is that Oracle databases ('instances') are much more independent to each other than those in many of types of database. They have their own memory and storage structures and almost everything can be independently tuned between DBs.

To be honest I haven't installed Oracle on Windows in about twenty years, but I think the easiest way to get going is to find the Oracle Database Configuration Assistant (DBCA) and run that to create a new DB before connecting to it with sqlplus.

Alternatively, if you are comfortable with running a Linux virtual machine. Oracle offers pre-installed virtual machines you could download and run with Virtual box to avoid having to do a DB create at all:

[https://www.oracle.com/downloads/developer-vm/community-downloads.html](https://www.oracle.com/downloads/developer-vm/community-downloads.html)",3.0
g8gd577,j8v63t,"&gt; For Oracle you need to use a UI wizard or a complex script and the actual processing to create the DB can take 30-60 minutes, depending on what hardware you're using.

That was true up until Oracle 11. Oracle 12 introduced ""pluggable databases"" which are similar to the database concept in Postgres and it only takes a single CREATE PLUGGABLE DATABASE command to create a new one. But more often than not, you only need a single database in Oracle anyway.",3.0
g8edtsn,j8v63t,"So you have installed the software, but haven't created the db...try this https://hubpages.com/technology/Newbie-Guide-to-Oracle-11g-Database-Common-Problems",2.0
g8gd936,j8v63t,"If you installed Oracle, you already have a (default) database. Just create a regular user and create your tables as that user. There is rarely the need to create additional databases in Oracle and if you are just doing exercises then even more so.",1.0
g8gnl9f,j8v63t,"Attempting:

create user csnarr identified by this;

Result:

ORA-65096: invalid common user or role name",1.0
g8gui9n,j8v63t,"This means you were connected to the _container_ database, not the pluggable database. The CDB is only used for administrative purposes, t, the pluggable database for your actual data. 

You can use `show pdbs` to find the name of the pluggable database. Then you can e.g. use `alter session set container = ...;` with the name of the pdb, and create the user there. Alternatively connect directly to the PDB using your SQL client.",1.0
g8glujb,j8v63t,"Installing the Oracle XE software automatically creates your XE database, this is because you don’t have much choice in the matter. Not sure what instructions you’re following to connect to your database, but the installation should have set your environment variables so that you can just do:
“Sqlplus / as sysdba” in your command line and connect to the root container database (cdb$root), this container database is for mostly internal things and should not be used by users. You can switch to your useable pluggable database by running “alter session set container=xepdb1;” , this is the pluggable database that you should do your work in. In order to connect in sql developer you can tell it to connect to localhost, port 1521, service Xepdb1, or you can locate your tnsnames.ora file and add an entry for it so that you can connect with just an alias. Your tnsnames entry can be created by copying the existing one, modifying the alias (the bit before the first = ) and changing the service name it directs to  (service_name = xepdb1). If you’re still having trouble, please share the exact commands you run and the exact output you get (and what you expected to happen).",1.0
g8gpg73,j8v63t,"If you know of a set of instructions, that'd be great.  This college kind of says, ""Here's your assignment, figure it out.""

Again, I'm not asking for help with this assignment, just how to install the database and then access it, so I can work with it.

I go through the installation, screengrab the last model box that has connection information, open SQL Plus, 'SQLPLUS / AS SYSDBA', then 'CREATE USER csnarr IDENTIFIED BY this;', which gives 'ORA-65096: invalid common user or role name'.  That's as far as I can get.

I did find that C## suggestion, so I did that.  Then I wanted to make sure csnarr has SYSDBA privileges, but get this:  
'SQL&gt; GRANT SYSDBA TO csnarr;

ORA-65175: cannot grant SYSDBA privilege locally in the root' (edit down).

O.k., let's just see if I can connect at all.  Going into SQL Developer, Create Manual Connection, 'Test failed: ORA-01017: invalid username/password; login denied'.",1.0
g8gq7fl,j8v63t,You should be able to follow the instructions I’ve written in my post. Let us know if that doesn’t work and what exactly happens. I’ve recently resetup my home machine and those instructions are from memory.,1.0
g8i21xr,j8v63t,"Freaking out at 7am, I didn't completely follow your steps.  By goodness I'm in, granted my made account sysdba rights, and created a table.

Thank you so much, Possible!   And thank you all for taking some time to help!",1.0
g8i8nsc,j8v63t,"Glad you’ve got it sorted out, I see from another comment that you had the same difficulties that so many others have with had due to the instance being multitenant and container multiple databases even though only one is really for user use (and it’s not the default)",1.0
g8da0v9,j8rs1s,"Select user,symbol,sum(amt) as amt,price
From tbl
Group by user,symbol,price",1.0
g8dmply,j8rs1s, how can this be implemented to actually update the table to reflect the changes this query would make? thank you for your response :),1.0
g8da9po,j8rs1s,"You don't need to use `GROUP_CONCAT()` that function will concat together strings. It'd give you something like '2,1' - not '3'.

Here's a query that should work:

    SELECT UserID, Symbol, SUM(Amount) AS TotalAmount, Price
    FROM table_name
    GROUP BY UserID, Symbol, Price 

And an example sql fiddle: [http://sqlfiddle.com/#!9/0ca9b0/1](http://sqlfiddle.com/#!9/0ca9b0/1)",1.0
g8dedm1,j8rs1s,"i hope this isnt a dumb question but how can this be implemented to actually update the table to reflect the changes this query would make?

and thank you for your response it's helpful :)",1.0
g8ewvcz,j8rs1s,"Yeah for a query like this I wouldn’t update the table. If the data ended up in the db like this there’s probably a reason. Updating this would be complicated because you’d need to both make an update and also drop a row. If you’re dealing with a lot of records that’s going to be really inefficient and potentially cause integrity problems. But, in reality, data operations can go wrong and need to be fixed. So if I *had* to fix this in a table and it was a one time deal I’d create a new table with this query as using SELECT INTO, drop the table with the bad data, and rename the new one.",2.0
g8exva6,j8rs1s,Thank you! This was helpful. Username fits :),2.0
g8ey2a2,j8rs1s,Glad I could help! Good luck!,2.0
g8dyi76,j8rs1s,Why would you want to change the tables data?  Just curious because usually you just leave it alone and if you need to manipulate the data for display purposes you just do it in a select statement.,1.0
g8dzgl8,j8rs1s,I am doing this for a school assignment and that is a requirement :),1.0
g8e6ayt,j8rs1s,Ok well I would venture to say it doesn't happen much in the real world but you would do a table update somehow... Sorry I'd have to Google update table rows,1.0
g8eagjo,j8rs1s,No problem :) I looked it up and it seems pretty difficult,1.0
g8eav8t,j8rs1s,That's what I thought,1.0
g8e723a,j8qypt,"I know you said you didn't want how to make a website ideas, but you could make a website with a Db running in the background. You can have a login screen popup and then it will open up to your resume, you could have buttons that say for example ""Work history"" then when that button is clicked it would do an animation that says ""Select \* from workhistory"" and so on and so forth. It will show creativity plus SQL knowledge. You can host this all through github for free. What you need to do is stand out. As long as it is all in one website and it is easy to navigate you should be fine. 

[https://pages.github.com/](https://pages.github.com/)",3.0
g8d0mnc,j8qypt,GitHub projects are typically used for this purpose.,2.0
g8dgvt3,j8qypt,"So just db schema, and code is adequate? A person looking at this doesn't need to have access to data?",1.0
g8f1pn2,j8qypt,"At big companies interviews take time away from day to day work. Not many interviewers have more than 15-30 minutes to review your resume and LinkedIn.  They have to conduct the interview and then provide feedback.  All this time adds up. 

IMO: Just having a bit of code on github doesn’t mean too much. But being able to say I have project X hosted on github and being able to talk about how it works and what you’ve built is key. Then the interviewer can ask questions, and dig deeper if they are interested.",3.0
g8fh7qz,j8qypt,Thank you for your responses. I’ve never had to hunt for a job before so a lot of this is new to me.,1.0
g8ej41p,j8qypt,"I'd love to hear from hiring managers if they even care. Do you ever look at portfolios for SQL or BI jobs? I don't think they generally do. 

Webdev, sure. Database, not so sure.",2.0
g8ev16z,j8qypt,"I got the idea from other comments on this sub honestly. The issue is I have 12 years of experience, but no education or certs or anything like that. There's a slim chance that I could leave my current job with no reference, leaving all my work behind me, so I'd have nothing to get hired on.

I've heard that no one cares about your education to get a job as a developer, but I certainly need something to showcase that I'm not someone who just googled ""SQL"" and decided to fake my way into a job.",2.0
g8hp688,j8qypt,"When I hire interns, I typically look for a github account listed on their resume. Granted, it's usually not centered around SQL (usually Python or R would be more common in github), but I would love to see a github link.  Advanced SQL skills is one of the areas that is the most difficult to judge without conducting a formal test.  

So, to that end, it would be nice to see a github repository that includes advanced SQL that the candidate could converse about.",1.0
g8kuztk,j8p734,"OA here, just wanted to add that you can find all the details about the new support for multiple standby at [https://pg-auto-failover.readthedocs.io/en/latest/architecture-multi-standby.html](https://pg-auto-failover.readthedocs.io/en/latest/architecture-multi-standby.html) in the official documentation for pg\_auto\_failover!",1.0
g8cc5aj,j8o3bu,"Since your filtering on p.id, you can just use left outer joins and expect the same results.

Think of it this way.  You're including all results from c1 and c2 in the join evem if they don't have a match in p, but then your filter excludes any results that don't have pid=1.

If the id is guaranteed to exist in c1 and c2 then even inner joins would work, but the outer join will lead to still getting a row when it's in p and missing from one (or both) of the outer tables.

Full outer joins are pretty rare in production.  There's usually a filter somewhere, and as soon as you have one the full outer is a waste of resources.",4.0
g8ch67b,j8o3bu,"That’s what I’m trying to wrap my head around. 

I did left joins but the person wanted my results to return null too if there’s no match. So in that case, I need an outer join right?",1.0
g8ci1if,j8o3bu,"Left outer join will return all rows from the left, and if there are no matches on the right side it will return nulls from the right.  

You only need the full outer join if p.id might not have a 1 in it.  (In which case, where is that id defined, start from there.)

However, if none of the three tables have that 1 then even a full outer won't return anything.",1.0
g8ciwll,j8o3bu,"So if I were to run this query, and one with just left outer joins instead, and there’s a 1 in p.id, I’d get the same results for both queries? 

Sorry if I’m having a hard time understanding",1.0
g8cj0g1,j8o3bu,Try it and see!  (Yes you will.),2.0
g8ckgnl,j8o3bu,I was playing with it on w3schools but they won’t let me work with full joins. Any suggestions for a good sql sandbox?,1.0
g8cljjd,j8o3bu,"Oh that one is easy.  With sql server itself!

SQL server express edition and sql server developer edition are both free.  Install them, and use the included management studio for code.  Just install on whatever machine you want to practice on.  (Just don't use dev edition on any user facing software you might build - that's the license restriction.)

Stack Overflow makes their database freely available.  Download a copy (mind the size you pick or it gets huge), restore it to your own personal sql server, and you have a nice big database populated with real world data to mess with!",1.0
g8cmuqn,j8o3bu,"So if I’m understanding correctly. Since I’m already filtering for I’d=1, an outer join is unnecessary bc if there’s no match I’ll be returned with a null?",1.0
g8cnq7q,j8o3bu,"The no match would be eliminated because it would return a null on the left, which your where clause would eliminate.  At best the full outer would increase io load, though I think the planner would see through that and rewrite to left outer joins anyway.

You still need a left outer join though, since you want to get nulls for missing data from the tables on the right.",1.0
g8cp7i1,j8o3bu,Okay so I need the left outer because I need the p.id of 1? Am I understanding correctly?,1.0
_,j8o3bu,,
g8cs3ru,j8o3bu,"Also, what’s io load?",1.0
g8cluk0,j8o3bu,I thought full outer wasn't a thing in MS sql...  That's where cross apply comes in?,1.0
g8ckj46,j8o3bu,"The distinction you need is that a regular inner join (which is the implicit default), an unmatched record in any joined table will suppress the entire row, while an outer join will return nulls for the missing data instead of suppressing the whole row.

Fun fact:

You don't actually have to select your filtered column (or even table!) if you don't plan to use it anywhere.

You can even use an inner join instead of a where to filter.  I've seen people generate a list of ids in a tsv or temp table, and join to it without selecting anywfrom it.  Don't actually do it this way though, it reads a bit funny and can mess with performance.",1.0
g8ccsne,j8o3bu,"As a word of caution, if your parent child relationships are both 1 to many you'll run the risk of data duplication when you do a join to the two child tables.",2.0
g8cgpli,j8o3bu,"Could you elaborate kindly? Or point me to the resources, I’m a beginner. 

What type of join do you recommend?",1.0
g8chamc,j8o3bu,"If your parent ID shows up in child table 1 three times and in child table 2 two times, you would have 6 rows returned. 

The correctness of your query will depend on the context of the data you have and what question you're trying to answer. 

Without any other information we can't really recommend what needs to done.",1.0
g8chl0i,j8o3bu,It’s the foreign key/distinct column in the child tables,1.0
g8c1jh9,j8l4wu,"Your description isn't very descriptive.  What is ""friend"" supposed to represent?  Could your example data include some other values?",5.0
g8c5oq9,j8l4wu,"For example A is linked to B by hash1 in below example. Similarly the other relationships also linked by the same hash value. Now I want to traverse from B, I want to see all the linked accounts from B. 
A-B-hash1

A-C-hash1

C-D-hash1

So I should get something like below linkage of all accounts linked to B by the same hash value hash1. 

B-A-hash1

B-C-hash1

B-D-hash1",1.0
g8bxzvz,j8l4wu,"you meant this, right? 

    Acctid1 - Acctid2 - friend 
    Acctid1 - Acctid3 - friend Acctid3 - Acctid4 - friend 
    Acctid2 - Acctid1 - friend 
    Acctid3 - Acctid1 - friend 
    Acctid4 - Acctid3 - friend

&gt; I am trying self join 3 times

please, show your query",2.0
g8c5kwg,j8l4wu,"That's a very good idea. But I think I have to reform the question. For example A is linked to B by hash1 in below example. Similarly the other relationships also linked by the same hash value. Now I want to traverse from B, I want to see all the linked accounts from B. 
A-B-hash1

A-C-hash1

C-D-hash1

So I should get something like below linkage of all accounts linked to B by the same hash value hash1. 

B-A-hash1

B-C-hash1

B-D-hash1",1.0
g8ctwf4,j8l4wu,"So, you want to find accounts that share the same hash basically?",1.0
g8e0pmn,j8l4wu,"yes, but starting from 1 account and going all the way deep for that hash.",1.0
g8cxi12,j8l4wu,dude... [puff]... that's way too  much... [puff]... hash for me,1.0
g8by6pe,j8l4wu,"If i understand correctly you already have the data ordered in source table...
So just doing this:
SELECT acctid1, acctid2, status
UNION 
SELECT acctid2, acctid1, status",2.0
g8c5b18,j8l4wu,"That's a very good idea. But I think I have to reform the question. For example A is linked to B by hash1 in below example. Similarly the other relationships also linked by the same hash value. Now I want to traverse from B, I want to see all the linked accounts from B. 

A-B-hash1

A-C-hash1

C-D-hash1

So I should get something like below linkage of all accounts linked to B by the same hash value hash1. 

B-A-hash1

B-C-hash1

B-D-hash1",1.0
g8c8z4d,j8l4wu,"Can you post some DDL and DML? Also, the queries that you tried, the results you’d expect, etc. 

I’m having a hard time following.",2.0
g8bxoqk,j8l4wu,Commenting here to keep track of this thread.,1.0
g8bj9pg,j8j0fh,"Sorry to say it, but you really don't. 

I suggest you look into database replication if you need to keep track of changes between multiple hosts. 

If it's backup you want then there are existing MySQL backup solutions. 

If it's schema changes you want to keep track of then using mysqldump to export the schema to a .sql file which is just a plain text file that git will have no trouble with.",4.0
g8bkza8,j8j0fh,I've used git to version control my ddl by sticking the ddl scripts in a single repo.  It works fine.,1.0
g8b5g1h,j8iw24,"try moving your conditions from the WHERE clause to the ON clause of their respective joins

    SELECT SUM(PretProdus) as total_pretzet
         , evidentamembri.ID as evidentamembriID 
      FROM evidentamembri
    LEFT OUTER
      JOIN AngajatiSala 
        ON AngajatiSala.UserID = evidentamembri.ID
       AND AngajatiSala.SalaID = 1
    LEFT OUTER
      JOIN VanzariSala 
        ON VanzariSala.ConfirmatDe = evidentamembri.ID
       AND VanzariSala.SalaID = 1 
     WHERE evidentamembri.ID IN (5, 7)
    GROUP 
        BY evidentamembri.ID",4.0
g8b7mtr,j8iw24,"Works as an ace, thank you.",1.0
g8bipws,j8fg6k,"I can only answer from an analyst stand point but querying them is basically the same other than how you select the top rows, and Microsoft has []’s everywhere lol",3.0
g8cd0uq,j8fg6k,"Oh haha, that’s good to know :)",2.0
g8bm8yd,j8fg6k,"Check out Stack Overflow’s[developer survey for 2019](https://insights.stackoverflow.com/survey/2019#technology-_-databases)(Database section linked).  You’ll be happy to know that the three rdbms products you listed are the top three most prevalent amongst those surveyed :). 

These are all very mature software offerings with platform specific nuance. That said learning any of those will build development skills that are applicable across all three such as ACID properties, relational models, and syntax. Big differences come into play when you talk about administration. Though general concepts translate to an extent, enterprise scale administration require distinct platform-specific skill sets that only come with experience and prolonged exposure. 

This is a very general answer but I hope it was useful info!",3.0
g8cd2vw,j8fg6k,"Hmm yeah, thanks for your response!",1.0
g8c5x89,j8fg6k,"Think of them, DBMSs, as dogs—they’re all dogs, they all bite. They all bark.

The differences come by breed, maintenance expectations and ownership.",2.0
g8cd4g5,j8fg6k,Good analogy.. will be checking them out more :),1.0
g8b60la,j8dgxp,"NoSQL is an enormous umbrella term encapsulating graph databases, time-series databases, column-store databases, object databases and more. You will have to be more specific if you want a proper comparison.

Probably the most well known and notorious NoSQL database is MongoDB. Rather than ""tables"" it has ""collections"" and rather than ""rows"" it has freeform JSON objects of arbitrary structure and depth. There is nothing to enforce that each object has the same fields/columns in it, no such thing as a primary key or foreign key, no equivalent to stored procedures or views and no JOINs.

Denormalisation is kind of forced by the system. For example if you want to look up an Order and its OrderItems it would be tempting to model it relationally with an Order collection with an ID field and an OrderItem collection with each OrderItem having an OrderID, approximating a SQL foreign key. This is not idiomatic MongoDB usage as pulling together the Order and OrderItems requires two round-trips to the DB by the application- there are no JOINs so one query to get the Order and another to get the OrderItems. The ""proper"" way is to have the Order collection contain all the OrderItems within each Order object, requiring only a single round-trip to the DB.

You can see how this way of thinking tightly binds the database structure to the application as you design the collections based on what will be requested the most and denormalise those collections to minimise DB interaction. I find SQL DBs are more often designed in a data-first application-agnostic way where the logical objects are kept in separate tables as in the past multiple systems would be accessing the database concurrently so it didn't make sense for the DB to be structured for ease of querying for a single system. These days with microservices it is common for a single system to have complete domain over a single database so there isn't really a reason to keep the DB structure application-agnostic.

The entire ""dynamic"" way of doing things like this leaves a bad taste in my mouth and I can't tell if it's just because it's new or because it's JavaScript kids trying their hand at real backend work and piling technical debt up to their eyeballs before leaving for their next job after 18 months.",7.0
g8da6su,j8dgxp,"Could not agree more with your last paragraph. I don’t even see the up front draw of “performance” being sustainable long term as it grows and new functions/modules are added... let alone programmability. 

It just makes me uncomfortable as well. Thanks for your well thought out response.",2.0
g8egupz,j8dgxp,"The programmability is the major drawcard of these database systems in my opinion. The entire thrust of them is that you are an Amazing Startup Entrepreneur and you are iterating so fast in such an [aA]gile fashion you don't have time to lock down your database to a particular structure! You need to Move Fast And Break Things, otherwise known as garbage devs re-inventing the wheel and piling up technical debt that I get to remove for very good money.

With a good bit of experience behind me I can see the cycles coming around again:

It was mainframes doing the processing with dumb terminals simply showing the UI, back in the paleolithic 70s.

Then it was desktops doing all the processing, who needs to be tied to a server?- All the processing power is on your desk and at your fingertips! Wow, hope that 5MB hard drive doesn't crap its pants because your entire business relies on it!

Then The Cloud became a thing where servers do all the work and the clients - mostly mobile by this point - are simply displays and action sinks.

Then in a mini-reversal five years ago the fashion was to push web page rendering to the client to offload processing cost, but it turns out users don't like their battery draining twice as fast and burning a literal hole in their pocket.

Now it's Server-Side Rendering as if that's a new thing, with Perl on CGI (ran ebay for an embarassing amount of time) or C++ on IIS/ISAPI in the early 90's and Python/PHP and Java squeezing them out during the 00's it feels like devs today need a quick history lesson more than they need another Hackathon or GitHub stars or whatever it is this week.

Sorry you caught me salty",1.0
g89veae,j8acob,Select fields from table where field = value.  This is sql 101,1.0
g8cnrj1,j8acob,"3rd commit;

and i found out the other two. 

&amp;#x200B;

thanks",1.0
g8a9yo5,j8acob,"Maybe I'm misunderstanding something but:

* adding rows will involve an INSERT statement: [https://www.w3schools.com/sql/sql\_insert.asp](https://www.w3schools.com/sql/sql_insert.asp)
* Modifying rows will involve an UPDATE statement: [https://www.w3schools.com/sql/sql\_update.asp](https://www.w3schools.com/sql/sql_update.asp)
* The third one is a bit ambiguous without more context. It looks like you have this tagged for Oracle, so my advice for this specific point may be wrong (I'm a SQL Server DBA), but save the changes permanently to the database sounds like committing the transaction (the transaction being the statement you run - the UPDATE or INSERT or SELECT or DELETE statement)",1.0
g8cnlrj,j8acob,"yeah, i found out how. idk how i didn't see it.",1.0
g8aswnx,j884fp,"&gt; Is there a SQL agnostic way to create local variables

No, there isn't because SQL as a query language has no notion of variables at all.",1.0
g8bbi4j,j884fp,Then what about tables?,1.0
g8dhz51,j884fp,"Think of it this way:

SQL is a language, that has a standard syntax that is the same across the board. T-sql and Redshift  are both just a dialect of the SQL standard in the sense that it has taken the standard and has modified/added a couple (or more than a couple of I'm being honest) features that microsoft deemed desirable to make the language more robust. 

Therefore, if you stick with the SQL standard of creating tables (regular or temp), it should be compatible across the board.  


You can stick with the standard of

CREATE TABLE {#temptable}
(
Comlumn names, column data types, column attributes/constraints
)


Because in the [Redshift Docs](https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_NEW.html) it states if a table starts with # then it will be created as a temporary table. 

I hope this helps!",1.0
g897jp5,j874oh,"Computations on the right side of the equals sign in a where clause used to cause table scans IIRC, (performance issue) and since the year of getdate() won't change until next year it's best to calculate that once into a variable then use the variable in the query. As for all the date parts, if you do lots of summing or selecting by week of year it may be advisable to simply carry an indexed column with the week of the year in the table (you can load it with an insert and update trigger on a date column).",1.0
g8axxw1,j874oh,"ok great! Unfortunately I am not the DBA so I can't change the structure but I will totally use the variable (and already have). the DBA's have provided a date table that we can use (has calendar date, fiscal month, fiscal week, fiscal year, in both official languages, etc.)",1.0
g8b9kmo,j874oh,Joining to the date table will let you avoid calculating or sub-stringing so much during your query which could speed things up a bit.,1.0
g8pwj08,j874oh,"good thought! this calendar table suffers from a similar flaw, however, its greatly useful for defining an anchor point (Today, Today +/- time).",1.0
g88yn8h,j84rpj,Omg! What an amazing story behind postgres!,11.0
g891v7s,j84rpj,This is great except the random bit where they say Esperanto is superior to English,10.0
g896o06,j84rpj,Mi transprenos SQL super esperanto,4.0
g899wqs,j84rpj,"Fantastic read, I had no idea
Thanks for sharing! You got me wanting to look into the dvorak keyboard and QUEL, not that I will adopt it but just as a fun read",10.0
g8ac3ux,j84rpj,"If you’re into oddball stuff, check out DataLog.  I learned that in a database college course well before we dug into SQL...  It made SQL seem ridiculous.",2.0
g8a0u07,j84rpj,"The dvorak keyboard claims made by dvorak were never replicated by controlled studies. 

http://www.mit.edu/~jcb/Dvorak/#:~:text=Dvorak%20found%20that%20it%20took,had%20increased%20by%2068%20percent.",1.0
g8ao09v,j84rpj,"Lookup ""Workman layout"" for more survey level info, but about keymaps. It's really fascinating how many considerations could go just into arranging keys, especially when you factor in spending hours typing, and what sorts of things you're typing (code, novels, data, etc)",1.0
g89bosm,j84rpj,If QUEL is like SQL but with the table name earlier in the query for more query types so that code completion works better .... count me in,8.0
g89xaka,j84rpj,"In case you're curious about what Quel looks like:

https://en.m.wikipedia.org/wiki/QUEL_query_languages",6.0
g8a8hi2,j84rpj,"Interesting. Can anyone shed some light or even eli5 what they mean in this quote about ""composability"" and what this inherent limitation is? It looks like they're talking about the query order of execution (from, where, group, having,select...) but I don't see how that's related. 

&gt; … The language (SQL) is not very composable. This is a fact that most  SQL users are not aware of. The relational algebra that SQL is based on  is *absolutely* composable but SQL is not due to the inherent  limitation of the language (as it was designed to be natural  language-like). When you write ""select x from a where z"", you are  actually building something along the lines of ""from a"" =&gt; ""where z""  =&gt; ""select x"" in the algebra and you can actually compose each  portion separately. If you are familiar with dplyr, Spark or pandas you  would get this instantly.",6.0
g8bul78,j84rpj,"In SQL, clauses have to come in a certain order and cannot be repeated. So if I have one query:

    SELECT a,b,c FROM t;

And I want to add another table and some columns to it, I have to change the whole query:

    SELECT a,b,c,x,y FROM t JOIN u ON u.t_id = t.id;

But if SQL were ""composable"", then I could leave the first query unchanged, and add onto it:

    SELECT a,b,c FROM t
    THEN SELECT x,y
    FROM u BY u.t_id = t.id

To get the same result.

For an incredibly simple real-world case, consider conditional chained filters. 

I could write a base query:

    let q = ""SELECT a,b,c FROM t ORDER BY a;""

Now I want to add filters based on user selections:

    if(...) q += ""WHERE user_id = 1"";
    if(...) q += ""WHERE volume_id IS NULL OR media_id = 4"";

With SQL, these sorts of things need to be carefully managed to ensure that there is a single WHERE clause, it goes before the ORDER BY clause, and the boolean logic within each filter is encapsulated. But with a composable language, none of that would matter and you could trivially add arbitrary clauses without having to worry about what already exists in the query.",2.0
g8c19k5,j84rpj,"Ahhh I see, that actually clears it up. Thanks a lot!!!",1.0
g89fjnr,j84rpj,"Cool story, Hansel!

Thanks for sharing. (Not sarcastic)",2.0
g8atgjz,j84rpj,"I want to know what other great projects which lost or were abandoned. 
I mean
Operating system
Editors
Databases
Programming",1.0
g8ati59,j84rpj,Is ingres still available for windows 10,1.0
g8bq141,j84rpj,"So, if we have PostgreSQL, is there are PostgresQUEL somewhere?",1.0
g88g7tu,j82s99,"based entirely on [the manual](https://impala.apache.org/docs/build/html/topics/impala_conversion_functions.html#conversion_functions__cast), i would try

    CAST(CAST(yourcolumn AS CHAR(8)) AS DATE FORMAT 'yyyymmdd')",1.0
g88jri9,j82s99,"Telling me syntax error, note that my date has no dashes in it- all the examples seemed to have them",1.0
g88zmf1,j82s99,"&gt; note that my date has no dashes in it

note that my suggestion `FORMAT 'yyyymmdd'` has no dashes in it",1.0
g89fu48,j82s99,"To_date(Concat_ws(‘-‘, Substring(yourdate, 1, 4), substring(yourdate, 5, 2), substring(yourdate, 8, 2)))",1.0
g88eaha,j82mma,"SSMA (Sql Server Migration Assistant). Not sure if there it is kept current, but it was a tool that does a decent job of coverting oracle to sql. Not without at least some amount of manual intervention but it will make life a lot easier.￼",1.0
g87vgry,j7zd4u,"The lateral join is not a good choice in this case. It's essentially run in a loop, once for each row in the orders table. 

It's more efficient to aggregate the payments once for all orders, then join that result to the orders table. 

```sql
select o.date, 
       sum(o.items_ordered) as item_count, 
       sum(p.cash_amount) as cash_sales, 
       sum(p.voucher_amount) as voucher_sales
from orders o 
  join (
    select order_id, 
           sum(amount) filter (where type = 'cash') as cash_amount, 
           sum(amount) filter (where type = 'voucher') as voucher_amount
    from order_payment
    group by order_id
  ) p on p.order_id = o.id
group by o.date       
order by o.date;
```",2.0
g89xrpi,j7zd4u,"Thank you so much! Your answer really helped and is much more efficient. 

For anyone interested, I ended up asking elsewhere too. This was the answer someone else provided which seems to be more efficient.

That said, using ""filter"" instead of ""case"" as per the above answer seems to be a newer feature and preferred as it's easier to write.

```
SELECT  
  ""date"",  
  SUM(items_ordered) as item_count,  
  SUM(CASE WHEN ""type"" = 'cash' then amount ELSE 0 END) as cash_sales,  
  SUM(CASE WHEN ""type"" = 'voucher' then amount ELSE 0 END) as voucher_sales  
FROM 
  orders o  
  INNER JOIN order_payments op on o.id = op.order_id
GROUP BY o.date       
ORDER BY o.date;
```

EDIT:
The answer by the other person I wrote above is actually incorrect. The reason being that the output would show

```
date | item count | cash | card
2019 | 6 | 100 | 100
2020 | 6 | 50 | 50
```

Which is wrong as there were not 6 items sold in 2020. So it might be faster but you won't be able to accurately aggregate info on the order table.",1.0
g8axkz2,j7zd4u,"&gt; The answer by the other person I wrote above is actually incorrect.

Yes, because the join will return the `items_ordered` multiple time for each order (that has more than one payment) and thus the aggregate will sum it multiple times. That's why the two-step aggregation is needed.

It really helps to run aggregates that return incorrect values _without_ the aggregate (and `group by`), then have a look at the raw (ungrouped) data, e.g. by looking at a single order.",1.0
g87yrd7,j7z9ho,"You should be able to check if your parameters are null OR not like the two values

    WHERE 
        (
            SupplierHistory.Event Like '%' + @EventType + '%'
        AND
            SupplierHistory.ChangedByBuyerOrSupplier = 'Buyer'
        AND 
            SupplierHistory.DateTime BETWEEN @StartDate AND @EndDate 
        AND
            (@ExcludeStatus IS NULL OR BuyerSupplier.Status NOT LIKE '%' + @ExcludeStatus + '%')
        AND
            (@ExcludeStatus2 IS NULL OR BuyerSupplier.Status NOT LIKE '%' + @ExcludeStatus2 + '%')
        )",1.0
g887cm3,j7z9ho,"This doesnt seem to work - its duplicating the Status parameters so ExcludeStatusA and ExcludeStatusB are both listed twice (I changed the parameter names as having the 2 was causing it to go squiffy), when I try and leave one of the two parameters blank I get no returns again.

Below is the full code:

    Select SupplierHistory.[User],
      BuyerSupplier.SupplierCode,
      BuyerSupplier.SupplierName,
      SupplierHistory.Event,
      SupplierHistory.Detail,
      Convert(varchar,SupplierHistory.DateTime,120) As 'Date',
      SupplierHistory.ChangedByBuyerOrSupplier,
      Convert(varchar,SupplierHistory.DateTime,120),
      BuyerSupplier.Status,
      Current_TimeStamp As 'Report Date'
    From SupplierHistory
      Inner Join BuyerSupplier On BuyerSupplier.BuyerSupplierGUID =
        SupplierHistory.BuyerSupplierGUID
    Where SupplierHistory.Event Like '%' + @EventType + '%' And
      SupplierHistory.ChangedByBuyerOrSupplier = 'Buyer' And
      SupplierHistory.DateTime Between @StartDate And @EndDate And
      (@ExcludeStatusA Is Null Or BuyerSupplier.Status Not Like '%' +
        @ExcludeStatusA + '%') And (@ExcludeStatusB Is Null Or
        BuyerSupplier.Status Not Like '%' + @ExcludeStatusB + '%')
    Order By SupplierHistory.[User]",1.0
g889t28,j7z9ho,"Oh, are you leaving the parameter BLANK or NULL ?  They're two different things and in SSRS you have checkboxes for both... if blank, you'll have to do `@ExcludeStatusA = ''` instead of `IS NULL`... personally I like to allow a parameter to be NULL but not BLANK, as it adds the NULL checkbox that will gray out the field entirely while keeping a default value.",1.0
g88bpt7,j7z9ho,"Aaaah perfect, that works. Much appreciated!

I would give you an award, but I'm a cheapskate...",1.0
g888cku,j7z5mr,"how long since this box was rebooted?  Which OS?  

we use GMSA'a and just plug them in and they work.  I had some issues during install of SQL and just left the network or whatever account in during install, and then changed it later.

Also, after 30+ years in IT and 15 on SQL, i'd rather be a diesel mechanic.",1.0
g88dmoy,j7z5mr,"Windows Server 2016, and it was last reboot yesterday when I started fighting with this issue.

Yeah, I can actually get the GMSA to work for the Reporting Service, and the SQL Server Agent - but not for the SQL Server service itself, which is frustrating.

I've made sure that it has logon as a service rights. I've checked TCP/IP ports, and a couple other items as my Google-Fu has lead me toward, but I'm just having no luck.

It ~looks~ like my server is a early SQL 2016 SP1 build - maaayyybe CU1, but not CU2. So, maybe a weekend step-up to CU15 will help (or, if I get REALLY brave a backup and step-up to SP2 CU 15....) will help. 

But short of insight from here - I'm at an utter loss.",1.0
g893847,j7z5mr,"Since the gmsa works with RS, it *seems* good.  Maybe walk thru creating another acct and try it just to rule out some error made while creating the gmsa? 
(I don't create mine,  but I think it needs some perms in AD to specific folders,  etc.)",1.0
g8bpm6y,j7z5mr,"Ran all the missing updates this morning.

No change.

Did all my due diligence, and learned where to find the SQL logs (I *did* say I'm an utter noob here, right?) and found an error about the certificate.

Sure enough the private key for the self-signed SQL cert didn't have a listing granting permission to my GMSA.

So, I added that manually - and now things are working.

My only confusion now is that... I shouldn't have to do that manually, right? I must have missed a step in the process/wizard/something that was supposed to do that for me? If not, I'll just add this to my internal documentation, but I feel like what I've read never mentions this as part of the process.

Thanks for you help, however! And, y'know, good luck with pursuing that Diesel Mechanic job you dream of. ;)",1.0
g87k6is,j7xukr,"It looks like this column is written as varchar not date. It sorts ascending from 1st digit in the cells 0, 1, 1, 1, 2 etc.",14.0
g87ke8i,j7xukr,The column geboortedatum is written as a date,-7.0
g87u09u,j7xukr,Looks like a date but it’s stored as a string. Use a CAST(datecolumn AS date) in the ORDER BY and then it will sort in chronological order,12.0
g87tf1q,j7xukr,"But it's not a date type, dummy.",0.0
g87y8xg,j7xukr,Datepart function?,3.0
g87koid,j7xukr,"Order by strftime('%Y',&lt;dateColumn&gt;)",1.0
g87m10f,j7xukr,"[https://www.sqlite.org/lang\_datefunc.html](https://www.sqlite.org/lang_datefunc.html) 

I think the function expects the string to be in format YYYY-MM-DD",2.0
g87lv49,j7xukr,You need to parse the year from date and put that in the order by. Not sure which type of sql you are using but it could year(date) or cast or something else,1.0
g87o9px,j7xukr,"Sqlite, but year(date) doesn't work",1.0
g87omcg,j7xukr,"try

order by  strftime('%Y',date)",2.0
g87mwsa,j7xukr,"Your screenshot seems to indicate you are connected to an informatica database, if so this might help:

https://docs.informatica.com/data-integration/data-services/10-2/transformation-language-reference/functions/get_date_part.html

I think this example  would work:

GET_DATE_PART( DATE_SHIPPED, 'Y' )",1.0
g883emd,j7xukr,"If the dates in there as a vchar which it looks like it is then

ORDER BY RIGHT(DATE_COLUMN, 4) should do it.",1.0
g88b2n6,j7xukr,Maybe cast into date then yearpart()?,1.0
g87kiqv,j7xukr,Year(datecolumn),0.0
g87l56k,j7xukr,What do you mean? If I do 'order by year(geboortedatum)' it says 'no such function: year',1.0
g8awtd0,j7xukr,Do you use MySQL or something else?,1.0
g8870qg,j7xukr,order by year(column),0.0
g87qml3,j7xrey,"&gt; Can I just use joins instead of the set operators?

No, you cannot, or, rather, you cannot in an easy ( 'just') way.

So, first thing first, you can achieve the same result in SQL in multiple ways, usually.

It's sort of implied but operations in SQL affect metadata (structure) of the result set.

UNION/EXCEPT/INTERSECT keep metadata of the first dataset in the expression, JOINs change the metadata.

Another thing is that JOINs require conditions.

So imagine Table_1(i integer) and Table_2(j integer). To express '(select * from Table_1) UNION ALL (select * from Table_2)' via full join you'll need to write:

        select coalesce( Table_1.i, Table_2.j) as i
        from Table_1
        full outer Table_2 on 1=2 -- or another condition that gives FALSE every time

Similarly, for intersect you will need to write comparisons for each column in those tables.

More columns you have, the more cumbersome this becomes, it doesnt help readability and performance impact is unclear.",5.0
g87sr1o,j7xrey,"Thanks u/ichp for the comprehensive explanation. Just a clarification, when dealing with tables with the same schema I should use set operators then joins for others?",2.0
g87tqdh,j7xrey,"No, that's not it.

Use an operation appropriate to the result that you are trying to achieve.

In a somewhat broken analogy, while you can re-write multiplication via addition operation, it's not a practical choice most of the time.

P.S. you join/union/etc. operations are for datasets NOT tables. Some datasets are stored as tables and it is a convenience and a performance enhancement ""feature"".

(select a, b from table B where b.x = 'somesuch') is a valid participant in those operations.",5.0
g87m2cb,j7xrey,"Algebraically they do the same thing, so you're right to infer that an `INNER JOIN` would give you the same result set as an `INTERSECT`. The result set has a different format though, JOINs put the columns next to each other whereas UNIONs append columns of the same name.

&amp;#x200B;

`ANTI JOIN` isn't a thing though, with a JOIN operator you would express it as  `LEFT JOIN Table_2 WHERE Table_2.column IS NULL`",2.0
g87nd6b,j7xrey,"&gt;ANTI JOIN isn't a thing though 

Oh sorry. I came from R so I thought SQL had the same thing. I searched it, but it is there in images of the illustration of SQL joins, odd.

&amp;#x200B;

&gt;LEFT JOIN Table\_2 WHERE Table\_2.column IS NULL

Thanks for this solution

&amp;#x200B;

&gt;JOINs put the columns next to each other 

Is it also the case when the two tables have the same column names?

For example, I am combining the table of sales of two locations

US\_Sales and EU\_Sales have the same structure. If I join them, will they also be placed side-by-side?

&amp;#x200B;

Thank you very much u/vassiliy.",2.0
g87qr7d,j7xrey,"&gt; Is it also the case when the two tables have the same column names?

Yeah, it's dependant on engine, but the columns might be called something like:

id, saleItem, salespersonID, id_1, saleItem_1, salespersonID_1

(See below, turns out I'm wrong on this one!)",2.0
g87sder,j7xrey,"Which sql engine - Oracle, mysql, MS SQL, DB2, etc. -  (not some piece of client software) does that?",2.0
g87us0t,j7xrey,"I'm working in Oracle land and thought it was server-side but no, it appears to be client-specific, not server-side.

    column dummy format a6
    select * from dual w join dual x on x.dummy = w.dummy;

Returns:

    DUMMY  DUMMY 
    ------ ------
    X      X     

I'm a bit annoyed about that, something I thought I knew is clearly wrong :)",1.0
g87s8x6,j7xrey,"Ohhhh. It says different [here.](https://www.essentialsql.com/learn-to-use-union-intersect-and-except-clauses/) But I guess it depends on which DBMS. 

Anyways, I should just use set operations when working with relations with the same schema right then join for the other?",1.0
g87sznv,j7xrey,"What you are referencing there is UNION/EXCEPT, which requires all subqueries in the statement to have the same column names. The DB then returns all the relevant rows.

if you JOIN two tables together, you get a table which consists of all the columns of the two joined tables.

i.e. if you join employee (emp_id, dept_id, name) and dept (dept_id, dept_name) on employee.dept_id = dept.id, you would get a joined table that had these columns:

    emp_id, dept_id, name, dept_id_1, dept_name

And the data in that table would consist of all the rows where the relevant left hand side matched the relevant right hand side - i.e. where employee.dept_id matched a row in dept.

&gt; Anyways, I should just use set operations when working with relations with the same schema right then join for the other?

You should use the tool that's most appropriate for the job. In most circumstances one will be better than the other for the particular circumstances. If there isn't any difference in result or performance, whichever works for you.",2.0
g87tu9f,j7xrey,"I think all these things are getting clear in my head now. Thank you very much folks, love you all.",1.0
g87x14o,j7xrey,"I'll just point out :

&gt;  What you are referencing there is UNION/EXCEPT, which requires all subqueries in the statement to have the same column names.

The set operators only need the order and types to be the same, column names are irrelevant and taken from the first subquery.  At least that is how it works in Oracle.",1.0
g87rbxb,j7xrey,"I was totally going to say it is a thing... in R and some other frameworks.

Don't hate DPLYR, Hate the Game.",1.0
g87sjx0,j7xrey,"Depending on your flavor of SQL, ANTI JOIN is a thing.

&gt;Anti-joins (Impala 2.0 and higher only):

&gt;Impala supports the LEFT ANTI JOIN and RIGHT ANTI JOIN clauses in Impala 2.0 and higher. The LEFT or RIGHT keyword is required for this kind of join. For LEFT ANTI JOIN, this clause returns those values from the left-hand table that have no matching value in the right-hand table. RIGHT ANTI JOIN reverses the comparison and returns values from the right-hand table. You can express this negative relationship either through the ANTI JOIN clause or through a NOT EXISTS operator with a subquery.

https://impala.apache.org/docs/build/html/topics/impala_joins.html",2.0
g87sxxc,j7xrey,"Cool, I never knew about that",2.0
g87shk1,j7xrey,"In practice, I think the choice of one or the other may come down to readability and sometimes to the structure of the underlying data.

I find UNIONs are easier when the tables aren't related, even to the point of having different column names, something that can totally happen in the wild.

SELECT id FROM TABLE\_1  
UNION  
SELECT idnum FROM TABLE\_2;    

is an easier construct to read quickly than something like:

SELECT DISTINCT COALESCE(t1.id, t2.idnum) as id  
FROM TABLE\_1 t1  
FULL JOIN TABLE\_2 t2 ON (t1[.id](https://hd.id) = t2.idnum)

Not a big difference but I rarely see FULL JOINs in actual use -- so it's a bit unexpected -- also there is always the potential to screw up your ON clause and end up with more rows that you expect -- especially if you trying to do a true UNION ALL.   I find the set operators as a nice clean solution, especially as your WHERE clauses get more complicated for each table.",1.0
g87c8hp,j7vo5a,"Venn diagrams are the wrong way to think about joins!

The correct use of Venn diagrams is to illustrate the set operations you mention (intersect, union, except). 

Each table in a join is a new dimension. Think of a pack of cards. You have something like:

    suits CROSS JOIN ranks

To get 4 suits × 13 ranks = 52 cards. Other joins are mostly* just pre-filtered cross joins. 

More reading:
https://dzone.com/articles/say-no-to-venn-diagrams-when-explaining-joins",6.0
g87fbmx,j7vo5a,On point! Venn Diagram really confused me as they are both used to illustrate the set operations and the joins despite the join having different table structures. u/ijmacd I would like to thank you very much for this,1.0
g87ik1d,j7vo5a,"In fact Venn diagrams visualize the `except`, `intersect` and `union` operators, not the join operators",3.0
g85w6vn,j7n2o7,"We need more info on how the data is stored in the tables.

Edit: can you show us what the tables look like please.",1.0
g85zj4t,j7n2o7,"Student_id     

Student Id |Skills
---|---
1| R
1 | Python
1| Sql
2 | Python


Employer Id | Required Skills
---|---
1| R
1 | Excel

So that would be an example of the 2 tables. The first student would be a 50% match and the second student would be a 0% match with the employer.",1.0
g861nb0,j7n2o7,"Great! Thanks for showing the tables. 

The next question is to clarify what you’re looking for as that determines what the query looks like. I’m assuming you want a percent of students that have a matching skill with an employer, this would be the count of students with a match divided by a count of all students. Is that what you’re looking for or something else?",1.0
g85wuej,j7mahk,"Do you understand select, where, inner join, left join, group bys??? Hey start applying. They'll probably give you an assessment and if you understand those you should be good enough to start.",51.0
g861h3o,j7mahk,"Sorry to piggyback this, but if I understand those pretty well, as well as some basic knowledge on writing SPROCs and functions, as well as reading/modifying really advanced/intricate queries, what kind of position would I be looking for? 

My current job is more or less ""technical support"" for a very small business and I've learned SQL from the ground up to do the majority of my job - but we are slowly starting to develop away from the database for my position and I'm kind of worried about my job going away. 

What job title should I be looking for to keep doing this sort of stuff?",9.0
g86874h,j7mahk,"There are many paths you can follow if you are into databases. I have seen colleagues moving to Database Analyst then to Database Admin or DB Architect.

Business Intelligence and Machine learning are also interesting paths where your data modelling, DB and SQL skills will be very well appreciated.

I moved into software development after working as Database Analyst and I could say databases in general including SQL are a mandatory knowledge.",10.0
g88nkyu,j7mahk,"Thanks for the info. I'll have to do some research on what I actually want to *do*. I'm 30 and have no idea where to go, other than working at my current company and getting paid probably less than I would like to be paid for the knowledge I have.",2.0
g87pbiz,j7mahk,"Entry level DBA, or business intelligence/data analyst. DBA's tend to make more, but also usually have to deal with more stress, being on call, etc. If you had young me's advice I'd say go the DBA route and make all the money! If you want old me's advice, the stress isn't worth it. You can make plenty to live comfortably in pretty much any facet of IT and having a good work life balance is key",2.0
g88pjoq,j7mahk,"If you don't mind working out of hours, dealing with lots of stress and trying to herd cats then going down the DBA route can be quite lucrative.

Otherwise business intelligence tends to be more Monday through Friday and less stress. You can still make decent money but probably not rise up the pay grades as quick as a DBA. If you're up for getting more formal education in statistics and math then data science would be interesting and also lucrative.

If you're young and ambitious I suggest working at an agency. It tends to be stressful and demanding because they never seem to hire enough people to cover all the roles needed so you can get to wear lots of hats (too many) and get experience in lots of areas. That's what I did. I'm now chilling as a senior business intelligence developer.  It's good pay and comparably low stress. I could make more as a DBA but money isn't everything IMO.  

Just to add that in my experience knowledge of specifics is less important than mindset for most roles. If you're the type that can solve problems by seeking out the knowledge yourself you have lots of options.

If you want to chat about anything specific feel free to drop me a line.",2.0
g865np6,j7mahk,"Thanks, I think I'm ready. I have some experience using MS Access. I can debug and read code too. I still can't get an interview. (I'm not OP, BTW.)",3.0
g85wwx2,j7mahk,"thanks! what is ""group bys""?",-7.0
g85ya9x,j7mahk,How about you google it and come back here and teach us what you've learned about group bys and we'll help you get over any confusion you have.,33.0
g86w7oo,j7mahk,"You created [https://lmgtfy.app](https://lmgtfy.app), didn't you?",1.0
g85zjwy,j7mahk,"It’s for aggregates, like sum, count, min, max etc etc. Looking these up will help too.",8.0
g85zpeb,j7mahk,thanks guys! sorry if i overcomplicate things sometimes I but appreciate the help!,2.0
g865icn,j7mahk,It's how you organize your data. You can group by Sales Date or Category or something else. Hope that helps.,3.0
g86qiwx,j7mahk,Group by ;),1.0
g865oot,j7mahk,"My first job in IT consisted mostly in creating SQL queries for reports and charts. Most of the tasks involved designing basic selects joining multiple tables and group by operations (sum, count, max, avg, etc. for totals and all sort of calculations). Then I started creating stored procedures (for automating data migrations and transformations) and designing tables for new reporting solutions and finally I learned some admin tasks (in my case for Oracle, SQL Server and Postgresql). Then I moved to software development and these SQL skills are still very useful if not mandatory for my job. That's my experience working with SQL, I hope it is helpful for you.",17.0
g866im6,j7mahk,yes it was thank you!,2.0
g886ui7,j7mahk,Lucky,1.0
g888adh,j7mahk,Worth to mention that the process of going from basic SQL to BD admin took me about 3 years.,1.0
g86q74x,j7mahk,"Another avenue is a Data Analyst in the healthcare industry, I’ve been doing this for 27 years and have never lacked a job.",7.0
g86kczh,j7mahk,"You never will feel proficient enough. Just go for it and learn as you go. I'm 10 years in and always learning still, but happy I went for it early on, when honestly I was still struggling with joins. Stay confident and curious. Good luck!",6.0
g868avh,j7mahk,Next question: what's an entry-level database job these days? I used to be a data analyst but I've been out of a job (and actively looking) for years now and suspect I need to start over at the bottom.,5.0
g86p89p,j7mahk,"Probably looking at some form of ""junior"" position.

Check out the keywords ""integration"", ""engineer"" and ""programmer"" or ""analyst"". Having some additional skill set relating to ETL or BI will give you a leg up.",4.0
g86roo3,j7mahk,"Data analyst is still a common entry level job. There are also Business Intelligence entry level positions, but you have to consider having some basics on reporting tools like PowerBI or Tableau  as well.",1.0
g860t7j,j7mahk,"Like the comment on here said, if you understand basic select and joining tables with counts and group by statements I think that should be enough for entry level. Anything else I ever needed I just googled or asked someone and was able to pick it up, make sure you try to understand what each part of anything new is doing and before long you'll be a pro

Good luck!",4.0
g861vuc,j7mahk,"thanks! ill do my best, i love this reddit page!",5.0
g86eryc,j7mahk,"It is amazing, just last month a user shared their Udemy course which was free until the end of last month so I purchased that, I'm not sure how much the full price is but it's a crash course and would probably be great for you, happy to dig up the link if you're interested",4.0
g86eu9s,j7mahk,"sounds great, appreciate it",1.0
g86f04i,j7mahk,[https://www.udemy.com/course/sql-basics-crash-course-with-sql-server/](https://www.udemy.com/course/sql-basics-crash-course-with-sql-server/),3.0
g86akd4,j7mahk,Go on hacker rank and if you can do a couple of the hard SQL puzzles you're better than most people at SQL,4.0
g88itvu,j7mahk,"**Two stories:**

(1) An old friend of my uncle was a crane operator for over 40 years. How did he get that job? He applied to be a crane operator and had **no** experience. After 5 minutes in the cab trying to figure it out and pulling levels, they asked him to leave. On the second interview he made it 10 minutes. The third place he interviewed hired him.

(2) My ex-girlfriend's grandfather Clarence (a little guy) grew up in the Great Depression era. Jobs were very scarce. A couple of his friends grew up on farms and were taller and stronger. He accompanied them once to a lumberyard were they were going to interview for work. While they were interviewing, Clarence looked around and saw that the yard was in disarray, so while his friends were interviewing, he just nonchalantly started picking up and organizing and stacking wood. The boss of the place walked out with the boys and when they asked him who he was going to hire he said, ""I'll take the guy that's working.""",4.0
g8618f6,j7mahk,"Do you understand JOINS, UNIONS, and basic Aggregations?

You're ready.",3.0
g86asxk,j7mahk,"I have a one hour sql interview test I created for my intern, if you find those question easy and no challenge then you are very qualified",3.0
g86mb4x,j7mahk,"""jobs for databases"" doesn't necessarily require SQL. if you understand data structure and why a database is useful in the first place, you are already in the 1%.",3.0
g86hx1w,j7mahk,"These are more intermediate position questions aside from the first, but all of these are things you'll encounter if you take time to look things up and are someone reliable to deliver results.   I'm responding after your group by question, so you should also look at rownum using partition by : 

Do you know the difference between delete and truncate for a big table, and how it will affect a large table in production?

When do you use a temp table vs use a CTE (common table expression)? &lt;Think performance and what does the optimizer know&gt;

Assume you have to change a Key in a large production table from int to bigint (yes the table is that big) how do you do it?",2.0
g86pjed,j7mahk,"Good questions, but in my experience they are definitely more intermediate, and in some cases things you would look up when the need arises vs not feeling comfortable looking for an entry level job because you don’t know them off the top of your head.   

Because ‘databases’ is such a large topic, just knowing what SQL is and having any interest in it at all is practically the lower limit for entry level work.  Writing ‘select * from tablename’ and where to run it puts you ahead of most general people.  After that I would mostly look for someone who understands that you have to be careful and learn slowly and not randomly throw queries at production and I’d be reasonably comfortable working with you.",5.0
g86smzg,j7mahk,"True.  I did start by saying they were intermediate questions, and I gave other caveats; these are intended more as questions to get the them thinking about optimization, and when, or if it matters in a given situation.  My choice of questions come from a shop I worked in where you could start in SQL with no academics and just enough knowledge and interest.  This shop had only functional code reviews, if that, and no QA or peer QA with no mandate. Understanding your basic syntax is enough to get hired, but asking if you are going to negatively impact a business process or need to think how to do it faster, before you realize you might not have a safety net, can save a job.


I think we agree on practice and principle, but I'm just trying to rattle their head and tell them they might get DBO permissions before they know what that really can do.",1.0
g86t69t,j7mahk,"Very good points, and I was trying to include that as well.  Obviously no guarantees in life, but if I could chose between A) someone who knew a little more SQL and B) someone who wanted to be a little extra careful, all else being equal (ambition, curiosity, etc) I think I would chose B).",2.0
g86p5fy,j7mahk,When you get Itzik Ben Gal TSQL query book and you’re able to do all exercises without any problems.,2.0
g87b1zo,j7mahk,"Just the basics, aggregations and understanding of various joins I think.",2.0
g87r8o4,j7mahk,"You want /r/BusinessIntelligence  

Working on databases has very little to do with writing SQL.  SQL is for the people that need to get data out of thing you are setting up servers and managing backups of.",2.0
g8fezml,j7mahk,I think leetcode and hackerrank both have SQL exercises. Learn some fundamentals and then try some exercises on one or both sites.,2.0
g8k0ywd,j7mahk,Test your skills on the platforms like StrataScratch and LeetCode. They have thousands of real problems taken from the real companies. There you can improve your skills as well as prepare yourself for interviews.,2.0
g868ipo,j7mahk,If you know what Select \* means start applying and keep learning as you go. The practice of applying and interviewing will be valuable even for those jobs you don't get.,2.0
g86g81x,j7mahk,"Saving this reference, thanks for sharing the recommendations.",1.0
g85khp6,j7lvpo,"You need to separate the tables in a way that the columns that remain are only the ones that relate to the primary key.
The 1NF is pretty much sorting the table so no cell actually have any double values assigned or out of place.",8.0
g85sqh8,j7lvpo,[This](https://youtu.be/UrYLYV7WSHM) was the video that helped me the most to understand Normalization,4.0
g86dwh8,j7lvpo,"Microsoft has a great doc https://docs.microsoft.com/en-us/office/troubleshoot/access/database-normalization-description

First normal form is extremely easy to understand and anything after 3rd is not really used in practical settings (although i am sure some people do)",3.0
g8613nw,j7lvpo,"This poster is an excellent guide:

http://marcrettig.me/data-normalization-poster-1989/",2.0
g85nqw8,j7lvpo,Yep you are on track. Good work,2.0
g85n5ly,j7lvpo,"I don't know the theoretical definitions of 1nf, 2nf, etc., but those look fine to me practically speaking, though I'd normalize the City column as well. Others, I don't think you'll see much performance improvement from smaller varchar columns like State, all the YES/NO, etc., but theoretically you'd want to do everything.",-2.0
g85v1yj,j7lvpo,"Two-character state abbreviations are the *perfect* primary key for a US state. If you will never need any other data about states -- full names, population, number of polluted lakes -- then the only reason to have a state table is to reference it with a foreign key, to ensure that no typoed state abbreviations are entered. Replacing the two-character state abbreviation with a meaningless integer foreign key is worse than absurd.

Pulling the City name out to another table is usually not fruitful either. Typos in the City name are usually not fatal to getting stuff mailed.  Postal code typos are worse.

YES/NO is perfectly fine and does not need its own table. Instead, it needs a CHECK constraint so nothing gets entered there except YES or NO.",0.0
g85qcae,j7jdmw,"In my experience they'll almost always have some sort of exam or small project, and trust their own processes for assessing your skills. If they feel you need improvement, they'll be able to train you pretty quickly.

If you're looking to get into data analysis or engineering, consider this though-

Much to my own surprise, I've gotten much further demonstrating my familiarity with genres of data than general SQL skill- for example, in medical data, if you're vaguely familiar with ICD, CPT, NPI, DNC, etc codes, that seems to count much more in your favor than familiarity with aggregation, window functions, etc. Similarly, I'm sure that finance, marketing, etc. have their own jargon as well. Employers are generally confident in training, as mentioned above, and perceived expertise in their field is often harder to come by.

I haven't worked much with SQL in the context of web dev stuff before, so don't listen to me if that's the plan.",4.0
g85y1b5,j7jdmw,"Good to know! My experience was with subscription and financial data, which can be gleaned from my resume, but I could definitely state that outright.",2.0
g850c76,j7jdmw,Where are you located Op?,3.0
g85198k,j7jdmw,"Seattle, WA",3.0
g853kqv,j7jdmw,"Git is good to have as a feather in your portfolio. With covid messing everything up, try a website with a few projects on it including your resume IF you’re looking to get noticed by the big firms. 
Also jump in on LinkedIn groups in your area and connect online, post your git projects and ask for feedback. That should help you get noticed fairly quickly.",3.0
g853pcc,j7jdmw,"If you were in Boston area, I would have reached out to you for sure. 

Wishing you luck and you got this...keep us posted Op.",2.0
g85mdqv,j7jdmw,Don't mean to steal op's thread but I'm in nyc avail to locate. Currently interning with TMOBILE as an analyst with SQL experience. Would be interested in connecting!,2.0
g85n9ai,j7jdmw,Dm me,1.0
g852bea,j7jdmw,"Select city, state 
FROM country 
WHERE city = ‘Seattle’ 
AND state = ‘WA’;

Was the correct answer ...",2.0
g854xz3,j7jdmw,Make an app which is using a database. Knowing just sql is like nothing,-5.0
g85f8n4,j7itkt,"Pseudo code

Declare @DateTime = Your code

Convert (varchar, @DateTime, 112) + '-' + convert (varchar, @DateTime, 114)

Should do it. Honestly though I'd just leave that kind of work to your presentation layer.",4.0
g85nq50,j7itkt,Thanks this worked!,1.0
g8mj45o,j7itkt,"Actually this almost worked.

The time output above is YYYYDDMM-HH:MM:SS:mmm

However it's supposed to be YYYYDDMM-HH:MM:SS.mmm

There is supposed to be a dot right before mmm

Anyway we can do this?",1.0
g8ml45t,j7itkt,"There'll be plenty of ways to do it. Remember, it's a string at this point. You could so something like:

     LEFT (YourCodeAndMine, 17) + '.' + Right(YourCodeAndMine, 3)

For example. Have a look at the different formats SQL can output a datetime when you use CONVERT, and work out which one is easiest for you to work with.",1.0
g85ettb,j7itkt,"FORMAT() is the easiest solution. You can format a date anyway you want. Microsofts article is comprehensive enough I reckon.
https://docs.microsoft.com/en-us/sql/t-sql/functions/format-transact-sql?view=sql-server-ver15",1.0
g85hjk4,j7itkt,God i hate developing against 2008r2.,1.0
g85kbmk,j7itkt,"Haha I feel your pain, firm I work for only updated in the last few years. So many old work arounds, that can now be handled by a built in function.",2.0
g8mnhnb,j7itkt,"Thanks. I opted to convert the main time to format 108, then +'.'+ and then did a datepart to extract the mmm.",1.0
g84uabl,j7iftw,I am guessing the Nested Select for the schools should use a where clause to exclude the school you want to leave out.,2.0
g84w8oy,j7iftw,Thanks!! I had it inside the inner most select when it should have been one level up. I can't believe I couldn't see it before.,1.0
g84l2gb,j7gsrs,"What are you grouping by? Since you are already sorting using ORDER BY, can you just put a LIMIT 1 to return the single record? Maybe I'm not understanding what you've got going on here?",1.0
g84n7f7,j7gsrs,"Hi here is results at moment, what i want to do is get the latest version from each group

[https://imgur.com/a/udfYlwp](https://imgur.com/a/udfYlwp)",1.0
g84p1wr,j7gsrs,Like a max(htmltext.version) ? Then group by what you need - might need a sub query  to get all details,1.0
g84p53s,j7gsrs,Hi yes but stick on how to write it!,1.0
g859gqk,j7gsrs,"you need a subquery to find the latest version for each module, then join `htmltext` on both module and version


    SELECT tabs.TabName
         , tabs.Title
         , tabs.Description
         , tabs.KeyWords
         , tabs.IsDeleted
         , tabs.Url
         , tabmodules.ModuleTitle
         , modules.IsDeleted
         , tabmodules.Visibility
         , tabs.IsVisible
         , tabmodules.IsDeleted
         , tabmodules.Header
         , tabmodules.Footer
         , modules.ModuleID
         , htmltext.Content
         , tabs.TabPath
         , htmltext.Version
         , htmltext.StateID
         , htmltext.IsPublished
         , htmltext.LastModifiedOnDate
      FROM tabs
    INNER 
      JOIN tabmodules
        ON tabmodules.TabID = tabs.TabID
    INNER
      JOIN modules
        ON modules.ModuleID = tabmodules.ModuleID
    INNER 
      JOIN ( SELECT ModuleID
                  , MAX(Version) AS latest 
               FROM htmltext
             GROUP
                 BY ModuleID ) AS m
        ON m.ModuleID = = modules.ModuleID              
    INNER 
      JOIN htmltext  
        ON htmltext.ModuleID = m.ModuleID
       AND htmltext.Version = m.latest 
     WHERE tabs.IsDeleted = 0 
       AND tabs.TabName LIKE '%mexico%'
    ORDER 
        BY htmltext.Version DESC

note i removed the join to `contentitems` because you weren't using it",1.0
g86rb34,j7gsrs,"&gt;SELECT tabs.TabName  
, tabs.Title  
, tabs.Description  
, tabs.KeyWords  
, tabs.IsDeleted  
, tabs.Url  
, tabmodules.ModuleTitle  
, modules.IsDeleted  
, tabmodules.Visibility  
, tabs.IsVisible  
, tabmodules.IsDeleted  
, tabmodules.Header  
, tabmodules.Footer  
, modules.ModuleID  
, htmltext.Content  
, tabs.TabPath  
, htmltext.Version  
, htmltext.StateID  
, htmltext.IsPublished  
, htmltext.LastModifiedOnDate  
  FROM tabs  
INNER   
  JOIN tabmodules  
ON tabmodules.TabID = tabs.TabID  
INNER  
  JOIN modules  
ON modules.ModuleID = tabmodules.ModuleID  
INNER   
  JOIN ( SELECT ModuleID  
, MAX(Version) AS latest   
FROM htmltext  
GROUP  
BY ModuleID ) AS m  
ON m.ModuleID = = modules.ModuleID                
INNER   
  JOIN htmltext    
ON htmltext.ModuleID = m.ModuleID  
   AND htmltext.Version = m.latest   
 WHERE tabs.IsDeleted = 0   
   AND tabs.TabName LIKE '%mexico%'  
ORDER   
BY htmltext.Version DESC

Hi Thanks for that but i am getting an error on the code when running it

`&gt; 1064 - You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '= modules.ModuleID`              

`INNER` 

  `JOIN htmltext`  

`ON htmltext....' at line 34`

`&gt; Time: 0.001s`",1.0
g86sy8n,j7gsrs,Ignore that! Just found issue (double = =) Thanks so much!!,1.0
g84o43g,j7fn57,What does your data look like? What is the layout of the tables you're querying? What have you tried?,1.0
g89il5r,j7fn57,"CREATE TABLE `lessen` (
 `id_lessen` int(4) NOT NULL AUTO_INCREMENT,
 `groep` varchar(6) DEFAULT NULL,
 `lesuur` varchar(3) DEFAULT NULL,
 `vak` varchar(3) DEFAULT NULL,
 `docent` varchar(3) DEFAULT NULL,
 `lokaal` varchar(3) DEFAULT NULL,
 PRIMARY KEY (`id_lessen`)
) ENGINE=MyISAM AUTO_INCREMENT=1581 DEFAULT CHARSET=latin


CREATE TABLE `lesuurnr` (
 `lesuurnr` int(3) NOT NULL,
 `lesuur` varchar(3) NOT NULL,
 PRIMARY KEY (`lesuurnr`)
) ENGINE=MyISAM DEFAULT CHARSET=latin1

I’ve tried subquerys and different variables, but I can’t figure it out",1.0
g8amb7m,j7fn57,"Please read the sidebar. In the future format your code by adding 4 spaces at the front of each line so it looks like this:

    CREATE TABLE `lessen` (
     `id_lessen` int(4) NOT NULL AUTO_INCREMENT,
     `groep` varchar(6) DEFAULT NULL,
     `lesuur` varchar(3) DEFAULT NULL,
     `vak` varchar(3) DEFAULT NULL,
     `docent` varchar(3) DEFAULT NULL,
     `lokaal` varchar(3) DEFAULT NULL,
     PRIMARY KEY (`id_lessen`)
    ) ENGINE=MyISAM AUTO_INCREMENT=1581 DEFAULT CHARSET=latin
    
    
    CREATE TABLE `lesuurnr` (
     `lesuurnr` int(3) NOT NULL,
     `lesuur` varchar(3) NOT NULL,
     PRIMARY KEY (`lesuurnr`)
    ) ENGINE=MyISAM DEFAULT CHARSET=latin1

From this, can you show a limited data set, the output you are trying to get, and the query you have tried?",2.0
g8lhypt,j7fn57,"I tried this query

    SELECT `lessen`.`lokaal`, `lesuurnr`.`lesuur` 
    FROM `lesuurnr`, `lessen` 
    WHERE `lesuurnr`.`lesuur` = ""$"" AND `lesuurnr`.`lesuur` NOT IN (SELECT `lessen`.`lesuur` FROM `lessen`);`

English:

    SELECT `lessons`.`room`, `lessonhournr`.`lessonhour` 
    FROM `lessonhournr`, `lessons` 
    WHERE `lessonhournr`.`lessonhour` = ""$"" AND `lessonhournr`.`lessonhour` NOT IN (SELECT `lessons`.`lessonhour`  FROM `lessons`

$ is a variable for a lesson hour, like Monday first is 'ma1' ( Maandag is Monday in Dutch) and Tuesday 8th is 'di8'.

But it doesn't give me the output I want.

So the thing is when I enter a variable for a lesson hour, I'll get all the empty rooms for the entered lesson hour

EXAMPLE OUTPUT:

I want to see all the empty rooms for Tuesday 8th lessonhour (di8):

|Lessonhour|ROOM|
|:-|:-|
|di8|C10|
|di8|L22|",1.0
g8ljbqm,j7fn57,"I think that in the FROM part, you also need a table with all the rooms, Query for that is:

    SELECT lessen.lokaal
    FROM lessen
    GROUP BY lokaal

English:

    SELECT lessons.room
    FROM lessons
    GROUP BY room",1.0
g84ri1w,j7fn57,"Agree with the other guy. Show us your table structure, show us the query you’re working on writing for this assignment, and highlight where you’re stuck. We can’t do your assignment for you, but let’s help figuring it out.",1.0
g89imoi,j7fn57,"CREATE TABLE `lessen` (
 `id_lessen` int(4) NOT NULL AUTO_INCREMENT,
 `groep` varchar(6) DEFAULT NULL,
 `lesuur` varchar(3) DEFAULT NULL,
 `vak` varchar(3) DEFAULT NULL,
 `docent` varchar(3) DEFAULT NULL,
 `lokaal` varchar(3) DEFAULT NULL,
 PRIMARY KEY (`id_lessen`)
) ENGINE=MyISAM AUTO_INCREMENT=1581 DEFAULT CHARSET=latin


CREATE TABLE `lesuurnr` (
 `lesuurnr` int(3) NOT NULL,
 `lesuur` varchar(3) NOT NULL,
 PRIMARY KEY (`lesuurnr`)
) ENGINE=MyISAM DEFAULT CHARSET=latin1",1.0
g84zl3f,j7fn57,"""where COL1 isnull"" is probably the magic answer here, but we'll need more detail to proceed",1.0
g8lk0cf,j7fn57,See the comments :),1.0
g84ddcq,j7fdqv,"It sounds like you are pivoting data.

I wrote a post a while back on this, see if it helps:

https://www.reddit.com/r/SQL/comments/8wmsyc/pivot_and_unpivot/",2.0
g84ati0,j7fdqv,"what's the actual index?

because i think you need a **composite** index on both `event_cd` and `ce_dynamic_label_id`",1.0
g84bfzr,j7fdqv,"I guess my inexperience doesnt understand the index maybe like I should...

it looks like there are 4 indexes containing the event\_cd and 2-3 other fields (person\_id, 3 differnt date time fields) 

and the dynamic label index there is one and its that plus a date time field I could add if that helped? it is a valid\_til\_dt\_tm which is always 12/31/2100 235959 unless the result is inactive which in the case isnt possible.",1.0
g84bwha,j7fdqv,"do you oknow how to generate and analyze an EXPLAIN in Oracle? (i don't)

that will tell you which indexes (if any) the optimizer uses

if you have the ability to add another index, add a composite one on the two columns i suggested and see what happens",1.0
g84ch3z,j7fdqv,I do not and I do not have that ability.,1.0
g85o34i,j7fdqv,"(Disclaimer: I'm a MS SQL guy and know performance very well there - I've never tuned Oracle.)

Self joins on big tables often scale poorly.  When you do a self join, for each result on the ""left"" side of the join (with left being the side the engine decides to handle first), it searches the table on the ""right"" side for matching records.

Remember many-to-many is perfectly acceptable here, so the engine isn't stopping at the first match - it needs to return ALL the matches with certainty, and it doesn't know if the relationship is 1:1, X:1, or X:X.

And you're doing it 30 times.  Even with how ""smart"" the engines can work and having a small data sample that is absolutely massive.  The problem will grow exponentially as you store more data.

An index on ce\_dynamic\_label\_id and pulling out any other match conditions from the joins might help, but it's not the answer.

In your case, I think u/ichp has your answer.  PIVOT that data.  The engine will process the data differently, taking less time, less IO, and less RAM.

The other self-join situation to watch out for is next/previous event (if you're trying to build a delta chart off those stats, for example).  Window those.",1.0
g85og2k,j7fdqv,Thanks for the response. I am going to visit the pivot example and see what I can do.,1.0
g842lsp,j7e4ye,"use `concat_ws`

ie, `select concat_ws(',', col1, col2)`

it's short for ""concatenate with separator""",1.0
g843l55,j7e4ye,"Does each row have 2 columns for separate emails, or are you looking at a separate row for each email address?",1.0
g8472gm,j7e4ye,"You want the comma to be conditionally added (I've used CASE statements for this in MS SQL).  Alternatively, stick to the COALESCE and then wrap that monster in TRIM() functions to remove leading or trailing commas.

Though that concat\_ws function mentioned looks a lot cleaner, and if it works is totally what you should use for readability alone.",1.0
g84bee4,j7e4ye,"i'm not sure this will actually work --

     COALESCE (firstemail address) + ',' +COALESCE (secondemail address)

because COALESCE will return NULL if it doesn't find a non-NULL in its list of values

so if either of those email addresses is NULL, then the **entire three-part concatenation** will be NULL 

instead, try this --

     COALESCE(firstemailaddress + ',','') + COALESCE(secondemailaddress,'')

this way, the comma is not generated if the first email address is NULL",0.0
g84gn2t,j7e4ye,And if secondemailaddress is null?,1.0
g84i8zq,j7e4ye,"aw, crap

okay, this --

    COALESCE(firstemailaddress,'') + COALESCE(',' + secondemailaddress,'')",1.0
g84icxf,j7e4ye,"WAIT!!

that won't always work either

aw, crap",1.0
g85etpp,j7e4ye,"Yeah, you're going to need a CASE statement for this.

    CASE 
         WHEN firstemailaddress IS NOT NULL AND secondemailaddress IS NOT NULL THEN firstemailaddress + ',' + secondemailaddress
         ELSE COALESCE(firstemailaddress, secondemailaddress, '')
    END",1.0
g85qt82,j7e4ye,"yes!

thanks for this

that ELSE clause is fabulous",1.0
g83vun7,j7cmgs,"I'd go straight to a cloud/big data database for analytics if I were you (i'm a senior engineer at a national bank, using AWS)

Consider PrestoDB, Spark, Hive or Redshift

This book has some interesting analytics / data mining applications in pure SQL: https://www.amazon.ca/Data-Analysis-Using-SQL-Excel-dp-111902143X/dp/111902143X/ref=dp_ob_image_bk

I used that book in a course I taught 

Might be a bit advanced for you, however",10.0
g83w0s8,j7cmgs,Thank you boy_named_su for your suggestion.,3.0
g842hrx,j7cmgs,"For very basic SQL take a look at https://www.w3schools.com/sql/default.asp
Will give you a starting point and it's easy to follow.",5.0
g843jq6,j7cmgs,Thanks a lot. Will surely check it out,2.0
g84i2ia,j7cmgs,We have a platform we think is cool to learn on.  Skip over data loading and server running and get straight to SQL. Learn that other stuff when you're ready. [https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/](https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/),5.0
g84j18m,j7cmgs,Thank you for your suggestion.,2.0
g84nxcw,j7cmgs,"I really like https://www.sqlservertutorial.net/

My SQL was very much 'learn as you go' and going through that course made me realise there were some basic things I could've really used.

Level is very beginner friendly. My advice though - don't follow along with the tutorials, find yourself a dataset and TRY AND DO STUFF.

Practice joins, ctes &amp; sub queries, window functions etc. All this stuff done outside of a tutorial helps you *think* in sql. I can't tell you how useful this has been as I've moved much more into development as it helps you translate human data problems into solvable data models on a computer.",3.0
g84v7x7,j7cmgs,"Thanks jzia93, would this be helpful as i do not have a technical background, i am a commerce graduate.",2.0
g84tcbo,j7cmgs,Most of SQL are the same with some minor syntax difference. You should be fine. Here is to talk about joins https://youtu.be/P8hxoMQw7ig,2.0
g84vamk,j7cmgs,"Thanks, appreciate your help.",1.0
g854n0e,j7cmgs,"Hello I have a question, what should I use as a editor or practice?

I have csv files and id like to load them and then run practice things to go along with my coursera class, it would need to be able to run sql from a file and also load in data.. I'm using Watson studio but currently have been getting a 

""Uncaught securityerror: failed to read  the 'cssRules' property from  'CSSStyleSheet': Cannot Access rules""

From the server or web page, could this be my webbrowser?",2.0
g85kuy8,j7cmgs,RemindMe! In 3 days,2.0
g85kxxu,j7cmgs,Sure i will.,1.0
g85wi7n,j7cmgs,"I will be messaging you in 3 days on [**2020-10-11 21:42:34 UTC**](http://www.wolframalpha.com/input/?i=2020-10-11%2021:42:34%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/SQL/comments/j7cmgs/beginner_want_to_start_learning_data_analytics/g85kuy8/?context=3)

[**2 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FSQL%2Fcomments%2Fj7cmgs%2Fbeginner_want_to_start_learning_data_analytics%2Fg85kuy8%2F%5D%0A%0ARemindMe%21%202020-10-11%2021%3A42%3A34%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20j7cmgs)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
g83n1n2,j7b9g9,"I'm a DBA. People aren't looking at my blog because they want to know if I'm a competent full-stack developer who can build a CMS from the ground up. They're looking at it because I'm talking about database or powershell stuff.

I use WordPress because it's a simple means to an end - that end being me creating and sharing content.

Edit: Wow, that thread over there...what a mess.",3.0
g83t7gr,j7b9g9,"&gt;Edit: Wow, that thread over there...what a mess.

Ask 2 programmers and you get 3 opinions.",1.0
g83jtp3,j7b9g9,"Hello u/IberianIbex - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g83wurs,j7b9g9,"I've basically switched to only using jekyll / github pages / static site generators. Github pro is the best web host I've ever had! 

While there are a few quirks it is exceedingly easy to do stuff and using markdown files for almost everything keeps my blood pressure low.",1.0
g83jlp6,j7b6f9,"Sure, use [window functions](https://www.postgresql.org/docs/current/tutorial-window.html): 

```sql
select ..., 
       avg(the_column) over () as overall_average,
       avg(the_column) over (partition by second_column) as average_of_second_column
from the_table
```",10.0
g83kbey,j7b6f9,"Hello, this was perfect, I had never heard of window functions! Thank you very much",4.0
g83pon7,j7b6f9,Another life changed 😅,8.0
g84foh1,j7b6f9,"Window functions are the shit, the only problem is you wanna use them everywhere and eventually performance degrades",4.0
g83kfu9,j7b6f9,The [ROLLUP()](https://www.sqltutorial.org/sql-rollup/) function should do this for you,5.0
g84649p,j7b6f9,"Not OP, but that is perfect. Thank you for bringing this to my attention!",2.0
g87n57u,j7b6f9,"I think I learnt about it when I attended [this talk](https://www.youtube.com/watch?v=fpR7kwBZbBI), it's useful to follow events like that from time to time even as an experienced SQL person because you might pick something up that just makes your life easier",2.0
g83q5ih,j7b6f9,"Depending on what specifically you want, window functions or `CUBE`/`ROLLUP`/`GROUPING SETS` should work for you. For example, do you want something like this:

|  item_name  | item_category | item_value | category_avg | table_average |
| ------------|---------------|------------|--------------|--------------- |
|  dinosaur   | animal        |        1.0 |     1.500000 |      2.200000 |
|  chicken    | animal        |        2.0 |     1.500000 |      2.200000 |
|  strawberry | fruit         |        3.0 |     2.666666 |      2.200000 |
|  banana     | fruit         |        3.0 |     2.666666 |      2.200000 |
|  apple      | fruit         |        2.0 |     2.666666 |      2.200000 |

or this:


|  item_category            |  value    |
| --------------------------|---------- |
|  animal                   | 1.500000  |
|  fruit                    | 2.666666  |
|  NULL [category_average]  | 2.200000  |

(fiddle [here](https://dbfiddle.uk/?rdbms=postgres_13&amp;fiddle=e7c32a55ef956bf06a49bbeed19c601e)",3.0
g833f6k,j77qvw,"This feature is part of the SQL standard if you use the `USING` clause:

```sql
select *
from a 
  join b using (id)
```

[Online example](https://dbfiddle.uk/?rdbms=postgres_12&amp;fiddle=40223be7eac82bfada8dcdcac6603e84)


-----

&gt; I can't think of a situation where the second column join condition would be necessary, 

When doing an outer join this might be of interest:

```sql
select a.id, a.c1, 
       b.id is null as ""Row for B not available""
from a
  left join b on a.id = b.id;
```",3.0
g83b0h8,j77qvw,"Agree with this. Although when testing my query (I.e. data quality) my where clause will have the predicate b.id is null.

I’ll be trying to find where and why rows haven’t joined to table b, even though I expect them too.

I don’t actually need both a.id and b.id in my final result set but I do work with them both in testing my query.

Hope that helps.",2.0
g835qd5,j77qvw,"USING is for same column name, in that case?

Thanks for the outer join example!",1.0
g836jiv,j77qvw,"Yes, USING only works if the column has the same name in both tables.",2.0
g82zbgk,j75cdd,"Use HAVING avg(OPINION) &gt; x


Where x is the number that you want it to be higher then.

'Having' is part of the 'group by' function. Google it.",2.0
g838l6z,j75cdd,Thanks!,1.0
g830qr5,j75cdd,"Less familiar with Postgres, most of my time is spent with Oracle, but:

General notes:

-	the opinion column is text, and the avg function works on numeric values 
-	all columns in select should be also included in the group by 
-	with order by you’d use ASC for ascending and DESC for descending 

Then the task would be count the instances of ‘good’ for each place, count the number of ‘bad’ for each place, and then selecting places where count(‘good’) &gt; count(‘bad’)

If I were doing this I’d probably do something like:

` select * from 
   (select place, count(‘good’) as cnt_good 
from  opinion group by place, cnt_good) a,
   (select place, count(‘bad’) as cnt_bad 
from opinion group by place, cnt_bad) b
where a.place = b.place 
and cnt_good &gt; cnt_bad;`

e: apologies for formatting, on mobile",1.0
g838mpp,j75cdd,Thanks!!,1.0
g83jqik,j75cdd,"&gt; `count(‘good’)`

is the same as `count(*)`. The argument to the `count()` function is not a ""filter"" on the values to be counted.

You need something like 

```sql
... from (
  select place, count(*) as cnt_good 
  from opinions 
  where opinion = 'Good'
  group by place ) a ... 
```",1.0
g83z4rh,j75cdd,"Dope, that makes sense. Thank you",1.0
g830tbp,j75cdd,"Select PLACE, Sum(case when OPINION = 'Good' Then 1 Else 0 End) as good_opn
, Sum(case when OPINION = 'Bad' Then 1 Else 0 End) as bad_opn
From opinions
Group by PLACE
HAVING good_opn &gt; bad_opn;",1.0
g8391h7,j75cdd,Thank you!,1.0
g836zmw,j75cdd,"I can tell you how I would do it and maybe it can be a learning moment for you.

I would create three tables:

    CREATE TABLE places (id SERIAL PRIMARY KEY, place TEXT); 
    CREATE TABLE opinions (id SERIAL PRIMARY KEY, opinion TEXT);
    CREATE TABLE ratings (id SERIAL PRIMARY KEY, place_id INT, opinion_id INT); 

places:

|id|place|
|:-|:-|
|1|This Place|
|2|That Place|

opinions:

|id|opinion|
|:-|:-|
|1|Just Awful!|
|2|Okay I Guess.|
|3|Super Great!|

ratings:

|id|place\_id|opinion\_id|
|:-|:-|:-|
|1|1|1|
|2|1|1|
|3|2|3|
|4|2|2|

The easiest way to get an average is to use the 'opinion\_id' integer value to calculate the average rating. Ideally this would be a separate value in the opinions table but this works as long as the opinions table doesn't change, and why would we expect it to?

From this example we can see that 'This Place' is going have a rating of 1 and 'That Place' is going to have a rating of 2.5.

Here's how you would find that result:

    SELECT
        places.place AS Place,
        AVG(opinion_id)::NUMERIC(10,1) AS Rating
    FROM
        ratings
    INNER JOIN places ON ratings.place_id = places.id
    GROUP BY
        places.place;
    
    |       place | rating |
    |-------------|--------|
    | This Place! |      1 |
    | That Place! |    2.5 |",1.0
g83957x,j75cdd,"Thank you, I never thought about a table",1.0
g83k7bj,j75cdd,"You can use a filtered aggregate:

```
select *
from (
  select place, 
         count(*) filter (where opinion = 'Good') as good_count,
         count(*) filter (where opinion = 'Bad') as bad_count
  from opinions
  group by place
)  t
where good_count &gt; bad_count
```
The derived table (aka sub-query) is only used to avoid repeating the aggregate expressions in the `having` clause.",1.0
g84x9g2,j75cdd,"Aggregates are so difficult for me, I'll just need to keep practicing I guess. Thanks!!",1.0
g82qb2v,j73rfx,"https://aws.amazon.com/api-gateway/

Host your dB on a free tier instance, or just use Dynamo.",3.0
g84k3mc,j73rfx,"Are you guys on the same Network? You are using Apache, I take it? Did you open ports?",1.0
g81hz9x,j6z4mt,"I did some research recently on hierarchical data in SQL (e.g. employees and managers, where managers are also employees), and couldn't find anything that was comprehensive and up-to-date. A lot of other pages were created 8-10 years ago before the major vendors included features to help with this.

So I created a guide.

This guide has a list of different options for storing hierarchical data, how you would view the data, insert and update data, and sample SQL for each database vendor.

Hopefully people can find it helpful.",3.0
g81zfnq,j6z4mt,"thorough analysis, really well done",2.0
g82shij,j6z4mt,"I looked through your guide and it is missing T-SQL hierarchyid, which I use quite a lot.
I also use adjacency lists and bridge tables(BOM).",2.0
g84n7uq,j6z4mt,"Thanks, good to know! I made a reference to the hierarchyid at one point as it's a good SQL Server feature :)",1.0
g81tcis,j6z31p,"you're going to have trouble

what is the datatype of your ""date""?

i can imagine only VARCHAR",1.0
g82ityr,j6z31p,"My approach would be something like this:

Select distinct id, Date from tableA, a recursive CTE to generate the missing dates or a loop into a temp table.  I’m not sure what the lower and upper limits of the dates need to be so that is up to you we will call this B.

Select
A.ID  - - lag function to fill in missing IDs
B.Date
From A
Full outer join B on date",1.0
g81ilbt,j6xlyd,You will need to [contact](https://www.iso.org/contact-iso.html) the ISO committee if you want a proper answer.,2.0
g83x7mq,j6xlyd,"It's a shame how hard they are to get.

A user On DBA.SE recently tried to get some of the older ones and it was.. Interesting:

Https://dba.stackexchange.com/q/168595/45616",1.0
g84gxk6,j6xlyd,"Wow! [SQL-86](https://archive.org/details/federalinformati127nati/) was so simple, only about 100 short pages.",1.0
g8dpdip,j6wapq,I would be happy to help if it's still needed - no payment required. PM me and we'll get it taken care of!,1.0
g8p17dl,j6wapq,Appreciate the offer. Thankfully I got it ironed out.,1.0
g8pi4o7,j6wapq,Not a problem - glad to hear it all worked out!,1.0
g80y6vr,j6vxbl,"The usual way is to create all possible dates using `generate_series()`. 

Something along the lines:

```sql
with all_dates as (
  select dt::date as ds
  -- replace the upper and lower bounds with the range you want
  from generate_series(date '2020-01-01', date '2020-12-31', interval '1 day') as dates(dt)
), cte2 as (
  ... your query ...
)
select ...
from all_dates ad 
  left join cte2 on ad.ds = cte2.ds and cte2.name = '...'
```
It's important to move the condition on the `name` column into the JOIN condition, otherwise it will turn the outer join back into an inner join.",2.0
g817yi7,j6vxbl,How would I make it so that the upper bound is current_date?,1.0
g81icin,j6vxbl,"Well, just pass it as a parameter: `generate_series(..., current_date, ...)`",1.0
g80y18k,j6vxbl,"your `cte` CTE should generate all dates needed

there are [several ways to do this](https://www.google.ca/search?source=hp&amp;ei=QwR-X5_QHMOt_Qb5xYOoAw&amp;q=generate+a+range+of+dates+using+CTE&amp;oq=generate+a+range+of+dates+using+CTE&amp;gs_lcp=CgZwc3ktYWIQAzIFCCEQoAE6CAgAELEDEIMBOgIIADoICC4QsQMQgwE6CwguELEDEMcBEKMCOgsILhCxAxCDARCTAjoFCC4QsQM6DgguELEDEIMBEMcBEK8BOgIILjoOCC4QsQMQgwEQyQMQkwI6BQgAELEDOggIABCxAxDJAzoFCAAQyQM6BggAEBYQHjoICCEQFhAdEB46CQgAEMkDEA0QHjoJCAAQyQMQFhAeOgQIIRAVUKYLWO44YPM8aABwAHgAgAGRAYgB6BmSAQQyNi45mAEAoAEBqgEHZ3dzLXdpeg&amp;sclient=psy-ab&amp;ved=0ahUKEwjf_bnkiaPsAhXDVt8KHfniADUQ4dUDCAk&amp;uact=5)",1.0
g814fwg,j6vxbl,"The CTE does generate the date values that I would like, which is post 2018 til current\_date",1.0
g81am2d,j6vxbl,"oh, wait... 

why are you inner joining that same table inside `cte2`?",1.0
g83xcu9,j6tzoi,Maybe https://sqlkoans.com?,1.0
g80dd83,j6rofd,"1,2&gt; get familiar/practice a lot of (in my priority order):

a. CASE expression

b. windowed/analytics functions (OVER clause)

c. subqueries/derived tables/WITH clause

d. cross/outer apply (aka lateral joins)

e. output clause

f. temp tables

g. stored procs/table-valued functions

h. dynamic sql

3&gt; you can use custom SQL in tableau

4&gt; understand HOW your business is run and HOW and WHY certain database structures have been created and are populated. Being able to flip between logical/business process logic/quirks to technical implementation details (applications, databases, tables, shpreadshets, etc.) is, probably, the most important job skill for an analyst",32.0
g80h95k,j6rofd,"Not a BA, but I would think the LAG() function might come in handy for trending.

Could be wrong.

And dynamic sql is pure evil.  Except when there's no better way......:)",7.0
g80kqyr,j6rofd,"&gt;Not a BA, but I would think the LAG() function might come in handy for trending.

That's covered in window/analytic functions

&gt;And dynamic sql is pure evil

Not when you're automating things. Dynamic SQL is incredibly powerful and useful for abstracting out and creating useful patterns to automate the creation of scripts that you'll use over and over again.

For example, instead of copy pasting the same script over and over again with just minimal changes to the list of fields, table name, and filters to generate source data for ETL, I can store a set of parameters that allows me to include/exclude records for a specific entity and create a basic ETL package using dynamic sql.

I would then only need one SSIS package that looks at a table of parameter values to generate the SQL which supplies the source data. The dynamic sql can be inside a stored procedure which takes those parameter values to create the sql. I would then just execute that stored proc to get my source data.",5.0
g83dlt9,j6rofd,Also needed when you have to use DB links,1.0
g80l31f,j6rofd,"Thank you for this! I have SSMS at home so I think I'm going to download some datasets and practice, practice, practice!",2.0
g82b1gi,j6rofd,"Pick up this book. First 5 chapters will push you towards ""advanced"" querying. https://www.ebay.com/itm/Training-Kit-Exam-70-461-Querying-Microsoft-SQL-Server-2012-by-Ron-Talmage/392050812437?epid=114201729&amp;hash=item5b480ca615:g:UMIAAOSwBTxepr2q",1.0
g81flyk,j6rofd,"Honestly I don't think you're in as bad as spot as you think you are, different places demand different skillsets. You're considered a senior there because you know more than a junior, but at 3yrs experience you're still a very young senior. So you have to grow into the role.

&amp;#x200B;

Your value comes from your knowledge of the industry, what its processes are and what the important metrics are to measure them. I imagine that's why they promoted you. I know plenty of experienced analysts who don't have advanced technical skills because their company doesn't need them, their value again comes from their industry knowledge and being able to sit down with the CEO and discuss the current number with them, drawing their attention to the right things.

&amp;#x200B;

Regarding your questions:

&amp;#x200B;

1) When you do some ad-hoc analysis, you can write your queries directly against the database (with SSMS for example)  and get your result that way. This will make you comfortable with using SQL, and for more complex questions you are going to have to author more complex queries. Others have covered websites where you can practice SQL questions.

&amp;#x200B;

2. You're halfway there, honestly analysts don't need to know half of the things databases can do. /u/ichp covered it well, tbh I would stop at d), 99% of analysts aren't moving data around inside of a database that would required the rest of the list-

&amp;#x200B;

3. custom SQL in tableau like the other guy said.",10.0
g81gt3r,j6rofd,"Thank you for this post, man! It really made me feel a lot better with where I'm at. I am going to take your, and everyone's, advice here!",2.0
g82a5pm,j6rofd,This is good advice. I used to hire sr BA’s in a social service field and we would pick the person with industry knowledge and talent explaining data analysis. We can teach coding but field experts are hard to find. Focus on getting visibility as a data translator. That is a surprisingly scarce skill.,1.0
g80bz4r,j6rofd,You guys hiring any other Senior Analysts? I also posses these skills. Lol.,17.0
g80k21s,j6rofd,"We're fresh out of positions, sorry \*sweating\*",20.0
g80di7c,j6rofd,"Ya i used to be in that same boat years ago. And you're right you should feel very uncomfortable. Thing is they have promoted you with Junior level skill set to a senior level job and will inevitability and unexpectedly require you one day to start putting out senior level reporting.  

1) start demanding some training now! Get in front of expectations.  

2) don't wait for them definitely sign up for lynda.com ( now LinkedIn learning) to feel those gaps

3) have a honest conversation with your direct supervisor,  at least that way this person will bear some brunt of the responsibility when you will be requested to produce something beyond your skill set. 

This is corporate culture at its worst. Happened to me time and time again. I got smart.",10.0
g81wilp,j6rofd,"If you are serious, send me a pm. My company is looking for tons of analysts.",1.0
g80ln5t,j6rofd,"I wouldn't beat yourself up too much about being a ""fraud"".  If you spent time working with R and getting an MS in data analytics, you probably got a deeper background in stats and data modeling than most Sr Business Analysts. That's a valuable skill!

Unfortunately, that kind of background doesn't fit the day-to-day needs of a lot of practicing BAs -- heavy duty Excel data manipulation/cleaning and the *business* part of being a BA, i.e. understanding the money-making part of your business.

There are lots of lists of SQL beginner-to-advanced learning tasks, I would start tackling one of those but I would also focus on learning how to use the tools.  Get comfortable with SQL Server Management Studio.  I would also focus on getting deeply in touch with the data in the tables your working with.  You will feel like a fraud if you can't explain what a commonly used column or abbreviation is.",5.0
g80ozva,j6rofd,"Thank you for this response. I certainly feel confident in my statistics and modeling abilities when it comes to R and Python. 

I'll play around more in SSMS at my workplace and maybe download some stuff at home too. Thank you for the advice!",2.0
g80wl1t,j6rofd,I think this skill set is more valuable and rarer than intermediate sql skills. Maybe you don’t use it as much in your current role but you could find a higher paying role that leverages your data modeling abilities.,1.0
g826b6d,j6rofd,Ride the f***ing lightning bruh,4.0
g8081zt,j6rofd,CodeWars.,7.0
g8098rv,j6rofd,I'm an analyst and am envious of you being able to use SQL to link to data and make it look nice in Tableau. Any tips? :-),4.0
g809pin,j6rofd,"Thank you! Well, all I really do is load up Tableau, use their feature to tie into the database, and join 4 tables together. It does all the work for me and I don't have to write any SQL.",1.0
g80ekjv,j6rofd,"This is how I feel at work but without the advanced degree. Best way to get better is to create some work projects where you will be able to use advanced SQL for analysis. 

Would you say the Master's in data analytics is worth it? I am debating between a master's or a certificate program to hone in on my technical and stats.",2.0
g80jztl,j6rofd,"I would say it was worth it as I had no frame of reference for SQL or even knew what Tableau was. I felt like my degree taught me solid fundamentals for sure, but I don't think it's anything you couldn't learn in Udemy and Youtube tbh.",2.0
g80lxu3,j6rofd,Thanks! :) Appreciate your response.,1.0
g80lih5,j6rofd,"I use to work in healthcare, and Excel was the main tool we used. So I would say you're not unskilled in your industry, however you have a really nice title. 

I just moved to a tech startup and its night and day, I had to learned and level up on SQL and Tableau fairly quickly, but since I worked on it 8-10 hours a day, I was able to pick it up. I recommend youtube, definitely helped me out a ton. Don't just watch though, actually start a small project using your data. This will help out in the long run.",2.0
g813li9,j6rofd,"If you don't mind, what's the pay band for Sr Business Analyst?",2.0
g81ai54,j6rofd,Not OP but I was a BA a few years ago and was making low 70k. Senior might net you upwards of 90k where I was at but I find salary expectations vary almost as wildly as job expectations under the BA umbrella.,2.0
g81d5ny,j6rofd,"u/Cat6Domestique I live in a more rural area but I make 68k, which is about 15k above the median income here. That's w/ 3 years of experience.",2.0
g81ro5i,j6rofd,[deleted],1.0
g82eynh,j6rofd,"&gt; Sr Bus Anal

I hope this isn't the typical nomenclature",3.0
g825mmn,j6rofd,Iqr is about 75-90 for a sr around here.,1.0
g8277y5,j6rofd,You can easily make 140k+ in a metro area.,1.0
g836gqh,j6rofd,"'Easily' Chicago is a metro area. Positions like that exist, but its certainly the exception rather than the rule. More typically when I see it cresting 100k its no longer 'senior' it's 'lead', 'principle', 'manager' etc. Or really a PO position, or data scientist position. Another poster mentioned sr+pmp at a bit over 100k and I certainly see that, but that's getting into a bit of a different role than just sr. Titles are kind of fluid though so undoubtedly you'll see some listed simply as Sr since that's what that org tops out at.",1.0
g82867d,j6rofd,"At my company a regular business analyst get about \~47k while a senior business analyst get \~55k.  


And we have a lot of senior business analysts that don't have anywhere near the experience that OP above has. I'm still dealing with one that doesn't know how to create a pivot table after 12 months on the job. &gt;\_&gt;",1.0
g819m7m,j6rofd,"Sounds like a fraud to me. jk

The title is overused and miss-understood is really the problem.

It's fitting enough though, although BA generally means you gather requirements and work with developers to create solutions, however you are usually involved in creating some solutions yourself, especially reports and BI.

Seems like a lot of data analysts, report devs and the like get labeled as BA's but really don't perform BA functions. The do the BA work on their reporting solutions and that's it. I do it on reporting and everything else as well. All business applications. It's more like a Project Management and Communications job than a technically skilled dev job. Or somewhere in between.

To me it's stands for Bad Ass. and I got the Sr now too so now it's Sr. Bad Ass. It just means you can do anything. The question will be, how much time and how much money will it cost. 

With some formal education and access to tableau, you have some things that I didn't. Sounds like your doing fine to me. If you really want to be a full BA, see if you can get involved with any application development your company works on besides reporting. Like your ERP system or any business apps. Maybe they already have BAs for that idk. When you get access to both is where you can really take off. You'll see the whole picture from data capture to data consumption at all levels of the org. That's when you can really start making an impact.",2.0
g8241um,j6rofd,"I'm both a Sr. Business and Systems Analyst, with many years experience. 

If you have access to the dbase, just keep trying new and more complex things both with Excel and SQL. It's an evolution. 

Work on creating some basic derived value SQL queries(this will expand your SQL logic), then export the data to Excel and practise using Pivot Tables.  Try this for a while, and before you know it, you will be a tier 1 Business Analyst.

&amp;#x200B;

You're on a good trajectory 3 yrs in.

All the best 

&amp;#x200B;

 .",2.0
g80gmjb,j6rofd,"In SSMs go to query, and select “design query in editor” one of the overlooked benefits of this is that it writes the sql for you - you can then read it and learn it.

Also why not watch some of the videos from;

https://www.youtube.com/playlist?list=PLHoM3HFg-eVyOhIWlt5ikRP2eFOrABzxJ

Hope that helps.",2.0
g80jwmm,j6rofd,That is very helpful! Thank you!!,1.0
g80nl3j,j6rofd,"I’d also recommend learning about the schema, depending on your access in SSMS, you should be able to expand / collapse the tables and views. 

Have a look and the keys, and how they link the tables together. Not only will this help you learn which tables can be join without composites but also will point you in the right direct for faster / more efficient queries (most keys are indexed)

I would really recommend the videos, not all of it is relevant to an analysis, so cheery pick and ignore the ones you don’t take value from straight away. But ultimately the more holistic an understanding you have the better. 

Warning Don Jones isn’t an exciting orator.",1.0
g813d27,j6rofd,"As a senior business analyst, apart from technical prowess you will be expected to question why reporting exists, be able to articulate what business process is enabled by the provision of specific metrics and reports, be prepared to proffer alternatives / improvements, be prepared to inform the business of performance and opportunities. Evaluate tasks, processes, reporting from a SWOT point of view. The purpose of reporting / dashboards etc is to enable decision making around measurable actions - all output should be measured against this.",1.0
g813kyq,j6rofd,I've been trying to get an analyst position for a year. Howd you get started?,1.0
g81d2kk,j6rofd,"Weird journey. Started as a customer servic rep, 2 years later an Analyst position opened up and I had my Bachelor's Degree in IT. They needed a Jr. BA to help with HTML/CSS web design and basic excel reporting. I broke out from there and got my Master's and learned that their data was JANK! Convinced them to get Tableau, let me automate some reports, and the rest is history.",3.0
g81ogt3,j6rofd,"Interesting. I wish there were some real reports i could do for my current company. We're a psych facility for minors. I think it might be difficult, unless you could think of some small project ideas? I have a masters in business psychology and only used Tableau in grad school for some projects.",1.0
g823xn2,j6rofd,Some great ideas  and recommendations. You got this OP.,1.0
g82ex8z,j6rofd,You're good man.,1.0
g830bq0,j6rofd,"So look, I used to be in your shoes 8 years ago but I really got hardcore with SQL. I'm getting ready to take the next big step up, which you could take on the business side but not one the IT side. I've decided to specifically focus my talents into a very specific and lucrative niche which demands a very high degree of SQL knowledge as it relates to data science.

Anyway, if you want to get real with it, check this out: https://www.reddit.com/r/SQL/comments/g4ct1l/what_are_some_good_resources_to_practice_sql/fnx11mc/",1.0
g83ad6q,j6rofd,"What is this ""very specific and lucrative niche""?",1.0
g83axke,j6rofd,OLAP architecture for data science.,1.0
g83fa58,j6rofd,"Cool, thanks.",1.0
g84m0tq,j6rofd,You think this is lucrative and niche. Why so,1.0
g858gie,j6rofd,Because I can bill out at 300-500/hr for side projects and have a really well paying salaried position for a billion dollar finance company?,1.0
g85es18,j6rofd,"Noice. Are you making api calls from backend of a financial application, storing that in a data lake and transforming what you need into an olap dwh and then passing off the tables to data scientist who then use that to conduct analysis ? Would love to chat about how you tender work free lance and serve up tables to where you know your end of the work is done.",1.0
g85wt1v,j6rofd,"In my current project we don't use any API's but I've used them before. My current project leverages data from various systems and abstracts away the complexities to store it in a format that is much easier to explore, and which doesn't require the code to be overly complex. I'm a data scientist in my own right, but it's not the niche I'm looking to focus in.",1.0
g8402od,j6rofd,"Sorry if this is not related, but I came to this subreddit for some insight and found your post. I graduated from business administration about 2 years ago and worked at a company where I had no interest at all except for the money. I want to steer my career now in the field I actually want and it is to become a Business Analyst. Do you have any tips on things I should learn (for someone who has no experience at all in the field)?",1.0
g87x3mc,j6rofd,"OP here! Business Analyst is such a broad title, and every company is different in terms of skillsets. I guess it depends on what you want, but if you're looking to become a BA related more towards data, the path is usually 

1. Learn SQL
2. Learn a DBMS (Microsoft SQL Server)
3. Learn a data viz software (Tableau, PowerBi)
4. Learn the business and how you can provide value w/ data.

&amp;#x200B;

Hope that helps!",1.0
g87zf6m,j6rofd,"Thank you very much for your answer! Ill make sure I learn all that (I enrolled yesterday on an online course to learn SQL and MySQL, at least to an intermediate level).",1.0
g804auo,j6rcun,"Hello u/johnhammond010 - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g812q42,j6rcun,Fascinating,1.0
g82fti7,j6rcun,"Nice, good stuff!",1.0
g8027m7,j6qej7,"you need a ""left join"" instead of a join on table 3:

    LEFT JOIN tb3 ON tb3.id=tb1.user_id AND tb3.custom_field_id='34'

Left join (or left outer join) will return all results from the left table (in this case your two joined tables) and any matches from the right table

edit: also if you're getting 0 results that means there are no combinations that satisfy your original where clause (tb1.billing_state='TX' AND tb1.status='1' AND tb3.custom_field_id='34').  By using a left join, you'll get results, but you'll end up with Nulls for tbl3.value",2.0
g8073uz,j6qej7,This appears to have worked! Except the value that is being returned for each user is all NULL.,1.0
g807wyn,j6qej7,"Yeah, like I mentioned in the edit, it looks like you don't actually have any values matching all three criteria you're looking for.",1.0
g80dy8u,j6qej7,"Hmm, the problem is, I know that there are values that meet that criteria. I'm just doing something incorrectly. Thank you for the help, I'll keep trying!",1.0
g80el2a,j6qej7,"I got it! I'm an idiot. Your way (and the sub query way below) worked. [tb.id](https://tb.id) was not the correct value, it was tb3.user\_id on my table. Silly mistake. Thanks again.",1.0
g80fqzp,j6qej7,"""Silly"" mistakes happen. 

I recently spent a couple days going backwards through layers and layers of sql written by someone else to fix and issue where the final result was not returning a correct result.

Turns out there was a case statement looking for a value early on that had a typo in the string it was searching for.",2.0
g7zzu9k,j6qej7,"So make a sub query

 join (select * from tb3 where custom_field_id = 34) tb3 on tb3.id = tb1.user_id",0.0
g7zng1f,j6odyl,"The basic purpose of a join is to consolidate data between two tables, with part of the information present in one table and the rest present in another table. 

The general use of joins is as follows:

1) Inner join: if you want find the data that's present in both the tables then you inner join them on key (Mathematically A intersection B). This should ideally lead to decrease in row counts.

2) Left join: If you want to retain information in the left table (which will remain as is) and fetch information only of the values that are also present in the second table. The final row count will be that of the left table after the join

3) Full outer join: If you want to just combine the data from both the tables. The row count ideally increases and if the table don't have any common value, the row count will ideally be the sum of both the tables.


I've placed a reference links which you can refer to:


[geeks for geeks](https://www.geeksforgeeks.org/sql-join-set-1-inner-left-right-and-full-joins/)

[w3 school ](https://www.w3schools.com/sql/sql_join.asp)",3.0
g7znn85,j6odyl,"So we aren't using inner,left or full. My queries look similar to this ;
SELECT  community_name, date, block, beat, arrest
FROM crime 
JOIN crime
ON crime_id = community_area.community_name",2.0
g7zohae,j6odyl,"using ""join"" is the same as ""inner join"". the join key (ON crime\_id = community\_area.community\_name) needs to be the same in the two tables you are joining together. So if you want to get something for ""community\_area"" for each rows in ""crime"", you need to find a column in ""crime"" that corresponds to a column in ""community\_area"".

&amp;#x200B;

read the links /u/quatrolingo posted and try out examples there, it will help you undestand what you need to do with your data.",3.0
g7zo2jw,j6odyl,"Ok, I guess I misunderstood the question. In any case, to understand which tables to join, you need to know what information you are looking for. By the looks of the query you've shared, it feels you need to identify which crime was committed on which date and in which community. 

The joins should be on the key/level of the data. These are the record/combination of records that can uniquely identify a row in the table. 

If the level of the data is crime_id in one table then there should be one row per crime_id

Next thing is to identify the column in the second table which also has crime_id.

You join them both on crime_id and can easily fetch the required data. Sharing a snapshot of the data can help me further identify which lable/column to use for join",2.0
g80qhwv,j6odyl,Thanks for the help. It made more sense the way you explained it.,1.0
g80qvbe,j6odyl,Cool che! 👍,1.0
g7zn1ht,j6odyl,You will have to give a litte more info.,1.0
g7znh11,j6odyl,"I just dont understand the logic of joining tables , I have a homework assignment and its based on joining queries but I don't know in what order to join and how to label the tables I need to join. It was not explained how to know what labels to use and what order.",1.0
g7zsv0c,j6o7lz,"The [Stack Exchange](https://archive.org/details/stackexchange) data set might be a good fit, but not sure what the largest table in the set is. It's the one Brent Ozar uses in all his SQL Server demos, he [has files](https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/) you can use to directly load the data into SQL Server as well.",11.0
g7zwpxe,j6o7lz,Thank you so much! this is amazing... the largest data set is over 380 GB,2.0
g8075x3,j6o7lz,/r/datasets has some huge datasets available with lots of variety on subject matter.  This might not meet your requirement for a complex relational dataset.,3.0
g7z6e9v,j6l3kq,Try making the max date of schema.t2 a subquery after where condition in the first union table and vice-versa for the schema.t1 for the table below,3.0
g805da5,j6l3kq,"not sure how this is supposed to work, perhaps you could write the query you envisage

how will one subquery's max not exceed the other subquery's max?

OP wanted ""no side of the union has a greater date than the other""",1.0
g80czkx,j6l3kq,"
select * 
from
schema.t1
where
yyyy_mm_dd &lt;= (select max(yyyy_mm_dd) from schema.t2)
union all
*
from
schema.t2
where
yyyy_mm_dd &lt;= (select max(yyyy_mm_dd) from schema.t1)",1.0
g80hd2b,j6l3kq,"i think you're right, maybe i overthought this",2.0
g8074f5,j6l3kq,"    WITH maxes ( maxdate )
    AS ( SELECT MAX(maxdate) 
           FROM ( SELECT MAX(yyyy_mm_dd) AS maxdate
                    FROM schema.t1
                  UNION ALL  
                  SELECT MAX(yyyy_mm_dd)
                    FROM schema.t2
                ) AS u
       )
    SELECT yyyy_mm_dd
         , xml_id
         , feature
         , status
      FROM schema.t1
     WHERE yyyy_mm_dd 
           BETWEEN '2019-02-02'
               AND ( SELECT maxdate FROM maxes ) 
    UNION all
    SELECT yyyy_mm_dd
         , p_id
         , 'payment'
         , case when payment = 1 
                then 1
                else 0 end
      FROM schema.t2
     WHERE yyyy_mm_dd 
           BETWEEN '2019-02-02'
               AND ( SELECT maxdate FROM maxes )",2.0
g7ywlfc,j6it7k,"yyymm is not a valid date format, as far as I know, in any version of SQL. As alternatives, you could either use dateadd to convert all dates to the 1st (or last) day of the month (but first is easier), or you can FORMAT the date to your desired format, but that will convert it into a string.",7.0
g7z1upj,j6it7k,"What does ""in a date format"" mean? Do you mean you want to store the data in a table column using the DATE datatype? If so then Oracle will store it the way it stores dates. Where would you like to see it ""in a date format""?

If you want to convert a string in the format '10-Oct-20' to a date, you have to use the to\_date function with the correct format specifier

[https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/Format-Models.html#GUID-22F2B830-261E-4BF0-91FB-6A1DAFC6D0A3](https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/Format-Models.html#GUID-22F2B830-261E-4BF0-91FB-6A1DAFC6D0A3)

e.g.

    select to_date ('10-Oct-20', 'DD-MON-RR') from dual ;",4.0
g7zccnw,j6it7k,"Yes, I want to store the yyyymm format as a date  datatype.",1.0
g7zmcof,j6it7k,"There is no such data type. You should be using a date datatype probably truncated to month (so only 2020-10-01 dates, every date is for the first day of the month.)

Displaying this is another thing and there it will be converted to string. Additionally you could have a constraint check on the data so that the day of month is always 1.",2.0
g7z27bw,j6it7k,"Are you saying you want to store the year and month only, and not the day? If so, you can't  do that with a date column, to my knowledge. But you can use a date column and just ignore the day in your application, or if you're writing a report but don't want the day part, you could do a `to_char(dt, 'YYYYMM')` in your query. Hard to make a good suggestion without more context though.",4.0
g7zcgu5,j6it7k,Yes. I want to store the yyyymm as a date data type (I don’t need the day of the month).,1.0
g807kh4,j6it7k,"Basically it's not possible with the built-in date data type, but I would think for beyond 99% of applications you could just ignore the day part and have no issue.

To make up a random example, let's say you were keeping track of the count of federal holidays in the US per month, but you don't care about the exact date of the holiday. Maybe your table looks like this: [http://sqlfiddle.com/#!4/60da7/10](http://sqlfiddle.com/#!4/60da7/10)

Your application's UI has a month selector to get the number of holidays in that month. When you click on the month, it would generate the first query on the right panel of that sqlfiddle page. I wrote two other examples there, too, for the reverse - getting the month/year given another input.

Does that suffice for your purposes?",1.0
g7ytzyx,j6it7k,"Would datepart work for oracle? I know MSSQL I would just do datepart(YYYY,date)&amp;datepart(MM,date) 
Then you can cast that result as a date",3.0
g7yz6nk,j6it7k,Datepart doesn’t exist in Oracle,1.0
g802fet,j6it7k,Heartbreaking.  Then an extract could do it,1.0
g7ywod8,j6it7k,"I don’t think you can store parts of dates as a date type. Why don’t you store as char or int then do to_date?

Edit: well I don’t think you can do that either because like I mentioned earlier , YYYYMM is only part of a date.",2.0
g7zqr9y,j6it7k,"YYYYMM is not a date and can never be a date.

https://docs.oracle.com/cd/B19306_01/olap.102/b14346/dml_datatypes005.htm",2.0
g813du2,j6it7k,Why not just force it to be the first of the month? And then document it within your business processes that the attribute forces a date of the 1st of whatever year/month is listed.,2.0
g7yzka3,j6it7k,"Can you trunc(&lt;&lt;date&gt;&gt;, ‘mm’)? It will still be oct 1, 2020, but being a date datatype requires having a day...",1.0
g83tn8k,j6it7k,"&gt; I want it like this 202010 in a date format, not int or char datatypes. 

If you mean you want to store values like '202010' (yyyymm) in a date data type, you can't do that. '202010' (yyyymm) isn't a date.",1.0
g7yhh8w,j6h1be,"Oracle separates objects by schema (aka user - in Oracle a user is also a schema). In other words Oracle separates objects by the owner of the object. There is only one ""database"".",3.0
g7ycvdv,j6eaze,CAREFULLY,6.0
g7xz4j9,j6eaze,"The question could use some more details. 

* How many of the billion rows are being updated?
   * If the number is much less than a billion: is there an index that is specific enough to make the selection rapid?
* Is the column the primary key, or a member of the primary key?
* Is the column part of a unique constraint? 
   * If so, it referenced as a foreign key by any other table?
* Is the column indexed?
* Are there any triggers on the table?",4.0
g7y034l,j6eaze,Wish I could provide more info but that's all I got. They were open to various solutions if I provided context for it. This is focusing on optimizing with big data so I'm assuming they are updating most if not all the data and I'm allowed to implement indexes if needed.,1.0
g7y3p4f,j6eaze,Depends on the number of records you are updating and the existing indexing.  If it's only a handful of records then the fact that the table has a billion records is basically irrelevant.  If you're updating a billion records then you may be better off doing a create table from select with a case statement to modify the column and then dropping the original and switching.  That saves all the redo and temp space required for an update and processes much faster.,3.0
g7xws3o,j6eaze,"The first thing I would think of is partitioning results on an index. Start a loop and update all records that start with 1 then 2.... if it's still too large, 10 then 11... I think I've been asked this before and it seemed like an acceptable response. I'm sure someone knows an optimal way to do it.",2.0
g7y7wct,j6eaze,By using the clustered index to find the row or rows to update. It's a trick question really. You want to do the least comparisons to find the row or rows to update similarly to how you may do the fastest select statement.,2.0
g7ygpfd,j6eaze,"I'll make sure to mention depend on the size of row that get updated, you might want to update it in batch so it's not going to explode your log file. and it will be faster as well..",2.0
g7xv7lv,j6dywv,"The sample solution is implementing an implicit join, which acts as inner joins. This method is an outdated way of joining tables.",7.0
g7xum20,j6dywv,"You're correct.

As far as I know, this is an old syntax of join, I haven't seen it used in a long time.

It's effectively the same, focus on the join though.",6.0
g7yjwfn,j6dywv,"&gt; As far as I know, this is an old syntax of join,

The join syntax you're accustomed to, `table1 join table2 on fieldA=fieldB`, was introduced in the SQL-92 spec. So the ""comma joins"" are approaching 30 years old and IMO shouldn't be taught as an acceptable way to code in 2020.

Present it as ""you might see this out in the wild"" but follow that up with ""please don't ever do this.""",7.0
g7y1od8,j6dywv,"I would shy away from using this website, as it's using outdated syntax. There was another post here the other day that referenced this site, and it also illustrated some issues with the site. The sidebar of this subreddit, along with a lot of other helpful posts, can give you more beneficial references and tutorials.",2.0
g7y3z1l,j6dywv,Will do. Thanks!,1.0
g7xpwhe,j6bxra,"Date = date + Interval ‘7’ day

My question is did you even try google?",1.0
g7xrp0f,j696c8,"Self referencing aliases e.g.

    SELECT
         mycol / 2    as calccol
        ,count(1)  as cnt
    FROM mytab
    GROUP BY calccol",19.0
g7y26z4,j696c8,"Is this supported in other DBMSes. If GROUP BY is evaluated before the select, how does it make sense?",2.0
g7yn45p,j696c8,"You don't have to ignore the SELECT clause entirely until you get past everything else. You already have to parse the query to evaluate it because you don't try to execute queries with broken SELECT clauses. Surely you can determine column aliases at the same time.

The only time it doesn't work is when you specify SELECT *.",3.0
g7zdk9e,j696c8,Aliasing window definitions for window function is also a good idea,1.0
g7yow98,j696c8,Teradata supports it.,2.0
g7zblub,j696c8,"Teradata often allows it or positional e.g.

    GROUP BY 1,2,3


etc.

I also never understood why we could not have

    GROUP BY ALL

Which would just be ""group by everything in select that is not an aggregate"". I know there are reasons not to but they are the minority of cases so it would be nice to have a shortcut.",2.0
g7y3415,j696c8,"This is huge, I would love to reference it later in the `SELECT` as well. Right now I can sort of get away with it by putting the value in a CROSS APPLY and then referencing it that way - but this does not work on window functions or aggregations.",2.0
g7zbr0m,j696c8,"Yes, allowing top down references would be awesome, just as syntax shorthand. Query Rewrite could expand it like a macro. Some sort of shorthand lambda function in the WITH clause would be good too.",1.0
g7x9rah,j696c8,Make PIVOT not completely awful to work with.,15.0
g7y37r8,j696c8,"At a minimum, it should not be necessary to know ahead of time what the values you are pivoting are, and have to hard-code them in the query. It should be able to dynamically handle changing values.",4.0
g7y4t2q,j696c8,Its the one thing that I'd rather do in Excell,4.0
g7ymsl2,j696c8,"No, this will almost certainly never happen. It would encourage developers to ignore first normal form and overuse EAV tables, and then it tells developers that it's okay to use SELECT * with a PIVOT, and it really isn't.",1.0
g7yofkj,j696c8,"Developers already do this in Python (pandas and pyspark) and analysts already do this in Excel. Tableau works extremely well with EAV data too.

I mean, to broaden this a bit, this is why JSON and NOSQL is popular today - rigid schema isn't always necessary.",3.0
g7ytbcb,j696c8,"Yeah, but those other languages aren't built around relations. No, not even Pandas. Not *really*. You're not interested in determinism the same way. You're not optimizing for relational algebra that lets you take any query and treat it like a relation. That model is what drives the power behind the data store for 95% of the world's data applications.

NoSQL and JSON/XML exist and have a place, but you're a damned fool if that's what you *start* with. All the alternatives to an RDBMS are either (a) highly specialized subsets (i.e., key-value stores, document stores), or (b) compromises that intentionally sacrifice relational design for much more narrow schemes that handle those specific tasks at those specific loads. And virtually all of these systems' implementation guides are documents describing how to understand or mitigate the consequences of the loss of the relational model.

There is a reason that it has been the championship of data storage for the past 50 years, and ""but PIVOT"" is really not a convincing argument for letting you not have to understand your data well enough to write a query. It's literally only a problem when you are stuck using report writing software that doesn't give you any procedural processing. The original solution to even doing a PIVOT was ""that's a display problem; handle it in the application.""",1.0
g7yzlxe,j696c8,"I dislike this line of argument intensely. It reminds me of the arguments people made how we shouldnt have TRIM() because bad developers would misuse it. Yes, developers will do stupid things with some functionality. That's on them. These are supposed to be tools, not consumer products for children.",1.0
g7zrnrb,j696c8,"Okay, how about this. It's the same reason you have to have unique column names for CTEs and VALUES table expressions.

How does PIVOT know that a column reference is correct? I need to do a table scan *before* I do a query plan? How do we expect that to perform? Do I determine columns based on the whole table, or do I have to take the WHERE clause into account?

What happens when I write a query with PIVOT and refence a column elsewhere in the query that in the future disappears because the data changed? What kind of error is that? Should the column be NULL, or is it invalid to reference that column entirely? Since we never ask for a list of columns, how do we know what's possible? How does it know that when you ask for a column named ""Naame"" that that is a valid column name and not an error? What if that column reference is three CTEs away? How do you plan a query execution when the PIVOT no longer has columns that can be determined just by execution of the PIVOT?

I put a PIVOT with a variable number of columns into a VIEW. What happens? How does the system evaluate what columns are valid in a consistent way? Does that mean I need ALTER VIEW permissions to be able to SELECT from the VIEW? Do we block PIVOT from being in the view?",1.0
g86othw,j696c8,"Yes, those are questions which would need to be established by whoever was implementing this functionality. But that is an entirely different discussion to ""Oh we shouldn't have this feature because people might misuse it"" or that it might perform badly.",1.0
g86sao6,j696c8,"I don't think you understand. Those are examples of developers misusing it. Those are examples of the consequences of breaking first normal form. When you put different entities into one field, you eventually want to split them up. But no matter how clever you think you are being you eventually hit a wall that you cannot overcome with deterministic relational algebra alone. You then have to pick between poor performance and wrong output. The problem here is not that dynamic pivot creates Catch-22 problems. The problem is that you can't solve all relational problems after not accurately telling the RDBMS what the entities you need to capture are. You can't just pick a behavior here because you'll just find situations where that's not just inconvenient. And worse, where it's *wrong* because you will need to make incompatible choices in the same query. 

That's why ""discouraging breaking first normal form"" is not just DBA pearl clutching. It genuinely can send a design down a blind alley and the developers won't see it if the system makes poor design choices in the tools that encourage poor design. Like PHP when it was popular.",1.0
g875nko,j696c8,"You're still thinking about this from the perspective of what developers will do with the functionality. I'm saying a lot of the work done in SQL Server is done in a context where it's basically irrelevant if the engine has to do another table scan or has to recalculate the whole plan or worse before a particular operation is run. 

Consider something like OPENROWSET  - if a developer wants to, they can do some incredibly stupid things with this functionality. They can choose to have all their tables on another server, even another database platform and then create views in SQL Server and try to pretend they are tables and write complex join queries between the views. And then they can cry when it's slow or just doesn't work. But that functionality is still incredibly useful in some contexts. And yes, if you write something like :

    SELECT a.*
    FROM OPENROWSET('SQLNCLI', 'Server=SomeOtherServer;Trusted_Connection=yes;',
          'SELECT *  FROM WTF.dbo.Foobar') AS a;

SQL Server has no idea what columns that will return until it runs. Moreover the below

    CREATE VIEW dbo.FooView
    AS 
     SELECT a.*
    FROM OPENROWSET('SQLNCLI', 'Server=SomeOtherServer;Trusted_Connection=yes;',
          'SELECT *  FROM WTF.dbo.Foobar') AS a;   
     GO
     SELECT * FROM dbo.FooView;

Is not guaranteed to return the same number of columns as the first statement because the latter will only return the columns it knows about when the view is created! Worse, if the remote table *drops* a column FooView will fail when queried because you have more column names defined than columns. But despite all this weirdness, this is still really useful functionality for certain kinds of ad-hoc work (which constitutes a big part of my job). Would you want to use OPENROWSET in a stored procedure which runs 100 times a minute - of course not. But not all functionality needs to be ""generalisable"" like that - and some other SQL variants seem more open to QoL functionality. 

And believe me, I'm not advocating violating first normal form when developing. Many years ago I tried to ""save time"" when building a classic ASP web site by saving multiple form values as a comma separated string in a table. And yeah. It was bad. Really really bad. Once a lot of data was in the system to unpick it all took longer than all other development tasks put together.",1.0
g7y4ozq,j696c8,You mean dynamic SQL?,-1.0
g7x37sh,j696c8,"I'd love to see a standardised source control integration. I know you can use liquibase, or visual studio, or one of the premium ones, but it's still madness to me that SQL doesn't have a go-to way of maintaining a CI/CD pipeline in the same way all other development does.",14.0
g7xf268,j696c8,"Totally agree, saying this for years. Should be able to remember the text for the last x previous versions of sps, functions etc. Should be baked in.",3.0
g7y2z2d,j696c8,"I was under the impression the industry standard was a SQL Server Database Project stored in Git, with DACPAC deployment.",1.0
g7ybkra,j696c8,"What's wrong with a Visual Studio SSDT project? It builds with msbuild, can be deployed with sqlpackage. I have everything integrated in a CI/CD pipeline, including tSQLt unit tests running and failing builds if a test fails.

SSAS... that is a bitch to work with and maintain a working CI/CD for.",1.0
g7yzayg,j696c8,"Nothing wrong, just the VS dependency. I do use Visual studio but for my work doing app development I prefer VS code - git just runs in the terminal so it's available anywhere.

Plus you've got difficulties when combining application data model changes and database changes, it's all more headache than I believe it should be.",1.0
g7zt7gt,j696c8,"There's this [SQL Database Projects](https://docs.microsoft.com/en-us/sql/azure-data-studio/extensions/sql-database-project-extension?view=sql-server-ver15) Azure Data Studio extension, though it's not as feature packed as VS's SSDT for sure. It sure be enough for maintaining project files though.",1.0
g7ztcv1,j696c8,That's actually really handy - thanks!,1.0
g7xblqd,j696c8,Would be nice to get native Regular Expression support in SQL statements. Thinking specifically in LIKE or PATINDEX queries.,13.0
g7xwfw0,j696c8,"1000x this.  This is literally the only thing about PL sql that I prefer, the regexp_like/regexp_substr functions in particular have been so handy to me",4.0
g7xqfcf,j696c8,Dark theme for SSMS that actually works,25.0
g7xshlz,j696c8,Azure Data Studio is a pretty good alternative to SSMS. I actually like it a bit more.,2.0
g7x7k8k,j696c8,"Ability to add a new column to a table and specify it's ordinal position.

A guarantee for Microsoft to continue supporting on-prem SQL Server instead of trying to push everything to Azure.",9.0
g7y1ide,j696c8,This has my vote!  How about adjust the prices back down to something sane for on-prem and stop assuming everyone can or should be in the cloud.,1.0
g832szn,j696c8,What makes you think they won't support on prem?,1.0
g7y98gl,j696c8,"**Better tooling**. This is so overdue it's bordering on tragic. 

SSMS,  and data tools/ssdt/bids/whatever SUCK DONKEY ASS. And they haven't really iterated on azure data studio since it was called SQL operations studio like 3-4 years ago.  

Microsoft, stop neglecting your data engineers.  We want the same sleek lightweight type of tooling that developers on other tech stacks get to play with.",8.0
g7wy1et,j696c8,"I *think* this actually exists now, but my biggest annoyance was trying to figure out which freaking row was causing my query to break because of truncation / data conversion issues... there's always that one random row that has a `ÿ` or something thrown in there.  If it showed me the problem string / value I could at least query the table and find the row myself.",6.0
g7x3874,j696c8,"Yeah, they fixed that in SQL Server 2019",4.0
g7y4lmg,j696c8,"&gt; but my biggest annoyance was trying to figure out which freaking row was causing my query to break 

It's absolutely stunning that this is still a thing (or not fixed until SQL 2019, as someone below said). How friggin' BASIC is it to tell you as much as possible about an error.",1.0
g7x8w2b,j696c8,"A BOMONTH() function that operates the same as EOMONTH() except for the first day.

I guess I could just go back and architect everything based on the last day of the month, starting with a date table.",5.0
g7y34ww,j696c8,"Date table!  Write it one time, read ~~forever~~ for the rest of the dates in the date table.",2.0
g7y4imu,j696c8,I have one. It's based on the first of the month. I could join on date to get month start but why not make it native?,1.0
g7y3d4t,j696c8,"This is very trivially avoidable, no? Just `DATEADD(day, 1, EOMONTH(date, i-1))`",2.0
g7y4g2r,j696c8,"Sure, it's easy to do... but why not have it native?",2.0
g7xk40q,j696c8,Native table level restores.,6.0
g7yj611,j696c8,IIRC that was a thing way back before 6.5 but was removed.,2.0
g7zaim9,j696c8,It was yes. Annoying that we now have to rely on third party software to do something that used to be included.,2.0
g7x1wz0,j696c8,"Support for ansi sql. Like “create table as” or nested CTEs, etc...",4.0
g7y3z6u,j696c8,Nested CTEs? That sounds like a maintenance nightmare,0.0
g7y3078,j696c8,"Number one for me, by a mile, is a query hint to materialize a CTE.",3.0
g7ycr16,j696c8,A QA team.,4.0
g7wy2hy,j696c8,In line with your query regressions I would like visibility into how plans are generated and understand why it changed.  Query store can help show what regressed but not why. Sometimes it is obvious why but sometimes it's a black box. It is so frustrating to have a plan suddenly get worse in production and have no idea why. I can't just add OPTION RECOMPILE to every query in a high volume system.,3.0
g7y779g,j696c8,"(Native) Always On aware SQL Agent Jobs so they fail over with a database. For open transactions to not be such a disaster - it shouldn’t prevent transaction log cycling, it shouldn’t prevent ghost row cleanup. Nested transactions to be implemented properly.",3.0
g7yfelb,j696c8,"Why oh why aren't Agent jobs AG aware?!

I have some convoluted step to check AG primary node on every single one of dozen of jobs.  And inevitably, someone screws it up everytime a job is added or changed.",2.0
g7ycgre,j696c8,"* regular expressions
* support for named, reusable windows (`SELECT AVG(val) OVER foo FROM table WINDOW foo AS (PARTITION BY bar ORDER BY baz)`, like in Postgres
* generate_series, also from Postgres; I'm so sick of `FROM sys.all_objects a CROSS JOIN sys.all_objects b`
* ... well to be honest A LOT of Postgres' programming features

And, for God's sake, XML output support for sqlcmd on Linux. This one is killing me, can't run unit tests in Docker because of it, it's just ridiculous.",3.0
g7ym9z9,j696c8,Make it easy in SSRS to have a subscription **not** run if there is no data to report.,3.0
g7x0bi9,j696c8,Already exists. Google for SQL Server automatic plan correction. It uses the query store to detect plan regression and force execution plans. It's not perfect but it does work. The catch 22 is you need at least one good plan in your query store to fall back on. And in my case I also had to turn on forced parameterization so SQL could recognize it was dealing with the same queries.,2.0
g7xlffw,j696c8,"A `RANGE()` function, so I don't have to write my own or use CTEs when I have only select rights.",2.0
g7y3gve,j696c8,Index Skip Scan as Oracle has would be nice. Probably less pressing then some of the other stuff mentioned already though.,2.0
g7ynuh2,j696c8,"I want a FILTER clause that works after the SELECT so that I can refer to analytic window functions in it without a subquery or a CTE.

I want FORMAT() to perform better in SQL Server. And I want MERGE to be atomic in the same.

I want recursive queries to be easier to write. CONNECT BY PRIOR from Oracle is nice-ish, but it's still awkward and cumbersome.

I want MERGE and IS DISTINCT FROM in the same RDBMS.

I want to be able to write the FROM clause first, then the WHERE, and then the SELECT, UPDATE, INSERT or DELETE.",2.0
g7zoap0,j696c8,Aggregate function like `First()`  and get `distinct` to work inside `String_agg` ex: `String_agg(distinct productname)`,2.0
g7xec59,j696c8,"Same as every other version, for enterprise manager to remember my fffing password.",2.0
g7yj8jw,j696c8,Move past SQL Server 2000 so you can ditch Enterprise Manager.,2.0
g7y0ezw,j696c8,"I would love something like RMAN in SQL server!!! Centralize backups keep everything organized and tiddy, tables restores, data restores.",1.0
g7y1mmr,j696c8,Don’t hold your breath they’re literally taking away back up features and azure,1.0
g7y1vop,j696c8,How about some additional default roles in MSDB for SQL agent execution and scheduling. Oh wait we’ve been asking for that since SQL2000￼.,1.0
g7yi4ym,j696c8,"A TRUNC() function that works with dates, and better handling of date math in general. Probably my biggest complaint over Oracle.

I wish I could do TRUNC(sysdate,'MM') to get 1st of month or TRUNC(datecol,'DD') to drop off time from a datetime. Instead I have to do ADD_DAYS(EOMONTH(sysdate,-1),1) or CONVERT(date, datecol).",1.0
g7yq3u6,j696c8,Declare variables in views,1.0
g7z4lbr,j696c8,The ability to allow multiple non-sysadmin users to manage the jobs of other users without hackish or cumbersome workarounds.,1.0
g7zdjb9,j696c8,Aliasing window definitions for window functions,1.0
g7xe2pk,j696c8,"`GROUP_CONCAT()`, but make it even better by allowing to specify the separating character you want to use. Something like how R's `paste(..., collapse = "", "")` works. I know there's ways to do it in MS SQL, but I always have to look up how to do it.",0.0
g7xv2tz,j696c8,Like STRING_AGG()?,3.0
g7xwu9c,j696c8,"Oh damn, yes! I learned SQL on 2008, we're on 2019 now, and that was introduced on 2017. I had no idea this existed...",3.0
g7yjg6q,j696c8,"Run some A/B tests on `string_agg()` vs. the `stuff`/`for xml` hack, especially on larger tables and be amazed at how much less I/O you're doing.",1.0
g7zbczk,j67rlw,"That's not valid MySQL syntax. Looks more like Postgres. 

In Postgres `date_trunc()` will ""round"" the timestamp value to the start of the week at midnight. This uses the ISO standard definition where Monday is the first day of the week.

So if `event_timestamp` contains `2020-10-07 17:18:19` this will return `2020-10-05 00:00:00`. 

The `::` is the (non-standard) cast operator in Postgres and converts the `timestamp` value (which includes the time of day) to a `date` value (without a time of day)",3.0
g88ybzy,j67rlw,"Thanks, this is exactly the information I was looking for!",1.0
g7wrgm9,j67rlw,vrooom!,1.0
g7wtr1i,j67rlw,"Well in PostGres, this will take the `event_timestamp` column and return the Monday of that week.

So if you have `2008-11-11 12:23:33` (a Tuesday), it will return `2008-11-10` (a Monday)",1.0
g7wgfvr,j65sui,"As noted by u/r3pr0b8, you can use date arithmetic to get you close to what you need (if using MS SQL, you can use `DATEDIFF`; other RDBMSs might support ""proper"" date/interval arithmetic). But, as u/ichp pointed out, you have to figure out your specific arithmetic, and you might need to figure out how to round your months.

 - The 1st entry (`2019-11-10` to current, which would be `2020-10-06`) could be either 11 months (just looking at the month field, November to October) or 10 months (since we wouldn't get to a full 11 months until `2019-10-10`).

 - The 2nd entry, `2019-03-10` to `2019-11-08` could either by calculated as 8 months (just looking at the month field, March to November) or 7 months (since you wouldn't get 8 months until `2019-11-10`).

Interestingly, this means you get different results using different methods in different RDBMSs. 

 - When using a construct like `EXTRACT(month FROM AGE(COALESCE(end_date, CURRENT_TIMESTAMP), start_date))` in Postgres, you get 10, 7 and 2 months. Fiddle [here](https://dbfiddle.uk/?rdbms=postgres_13&amp;fiddle=f6da17618713ae5eeb62904e86b264a5)

 - When using `DATEDIFF(MONTH, start_date, COALESCE(end_date, CURRENT_TIMESTAMP))` in MS SQL Server, you get 11, 8 and 2 months. Fiddle [here](https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=52cb0e4b346e96ea16afaf2251d7e629).

So you need to be consistent with your logic.

Finally, as u/ichp noted, the sorting (which rows appear first) is actually going to be the easiest. You just need to order your results. But it's not necessarily clear how you want to specifically order them&amp;mdash;do you just want the results with the most # of months at top, irrespective of year, or do you want to order by year, then # of months? If the former, are you using the year from the Start Date or End Date?",2.0
g7wak6u,j65sui,"use whatever DATEDIFF function comes with your particular SQL platform (which you didn't mention)

for current supervisor, use COALESCE to CURRENT_DATE on the end date",1.0
g7weogk,j65sui,"a. you need to figure out your exact date math: What is 'months between 2 dates' (why Phil's is 8 and Chris's 2, for example)

b. "" I want the query to prioritize""  - sounds like this is just the order by months descending. I would recommend getting a metric to capture the ranking/ordering as well (row_number or somesuch)",1.0
g845fju,j65sui,"Assuming that the actual intent is to rank supervisors in order of who was was in charge the longest, I think your best bet is to simply get the count of days, and order by that, descending. You can then, optionally, have multiple additional columns for how many absolute calendar months that spans, how many 30-day periods, etc.  

Deciding how to interpret ""months"" is really a formatting decision that should be made by the business, and needs to be specified by whoever is asking for the report. You can help them by pulling together the different ways to do it, and explaining the implications of each. For example, suppose that, in a non-leap year, Mike was supervised by Adam from January 31st to March 1st, by Phil from March 2nd to April 3rd, and by Chris from April 4th to May 30th. Adam was in charge for 30 days, which can be rounded to 1 month, spanning three calendar months. Phil had 31 days, spanning two calendar months, which could be rounded to one month, or shown as 1.03 30-day periods. Chris was in charge the longest, at 58 days, or 1.93 30-day periods, rounded to two integer months, but still tied with Phil at two calendar months, and loses to Adam with his three calendar months. Who should win? The answer is, it's not your call. Kick it back to the requester, and make them explain what they really want.",1.0
g7w9fl4,j65gn9,"    SELECT JS.Serial_Nbr
         , JS.CS_Type 
         , SH.VIN_Ref_Nbr
      FROM AA_Frame_Serial AS AA
    INNER
      JOIN MES_data_Job_Serial AS JS 
        ON JS.Serial_Nbr LIKE '%' 
                            + AA_Frame_Serial.Serial_Nbr
                            + '%' 
       AND JS.CS_Type = '36'  
    INNER 
      JOIN MES_data_Job_Ctl AS JC 
        ON JC.Job_ID = JS.Job_ID 
    INNER 
      JOIN MES_data_Scheduled_Order_Hedr AS SH 
        ON SH.Order_ID = JC.Order_ID",4.0
g7womid,j65gn9,"I want to thank you for the clean code (I am not a DBA by any means) I noticed I was getting an error on the line.

\+ AA\_Frame\_Serial.Serial\_Nbr

Only because it needed to be \[AA\].\[Serial\_Nbr\] other than that it worked flawlessly. Can I ask you some questions about the way this is written?",2.0
g7wov32,j65gn9,"&gt; it needed to be [AA].[Serial_Nbr] 

well done sussing that out

please, go ahead and ask",1.0
g80ccjr,j65gn9,I actually figured it out as I was typing my response. Damn you're good,1.0
g7wzq1n,j65gn9,"Haha, couldn't believe it when I saw those table names. I work for Magna and am in seating too lol. Glad you got it solved.",1.0
g80c7js,j65gn9,Not sure If we should feel sorry for each other or take down the post. =) Ex-Magna employee now working at a place that uses Magna's MAA.,1.0
g7x68oq,j64r30,"u/OkieDaddy one of the best resources on Postgres I found online is [this](https://coursesity.com/best-tutorials-learn/postgresql) 

Highly recommend",1.0
g7y9xt9,j64r30,"I started learning SQL and currently building a practice DB in Postgres. I have been using w3schools.com. For me, i like that i can reference commands and see an example of how it’s written, and i can try it on my own.",1.0
g7w42vj,j631cl,"You should probably it each new column on a separate line, it'd be a lot easier to read, you might see what the issue is if it's just easier to read.",3.0
g7vu11n,j631cl,It looks like you're missing commas between some of the columns/field names.,3.0
g83qca7,j631cl,"Today I examined the error closely: it states Error creating bean with name ‘h2Console’ defined in class path resource [org/springframework/boot/auto configure/H2/H2ConsoleAutoConfiguration.class]: Bean instantiation via factory method failed.

But I don’t understand why Spring can’t find the auto wired bean ? I thought we don’t need to specify autowired annotation anymore?",2.0
g83ss13,j631cl,"Sounds like you have a problem with spring. You might want to ask in a different subreddit about that. We only understand SQL and your SQL statement won't work because your are missing commas. How that relates to the error you're getting I have no idea, I don't use Spring. Good luck though!",2.0
g7vu2i3,j62ypv,"    SELECT parent_branch.Branch AS parent_branch
         , child_branch.Branch AS child_branch
      FROM tblBOM
    INNER
      JOIN tblBranch AS parent_branch
        ON parent_branch.Product = tblBOM.Parent
    INNER
      JOIN tblBranch AS child_branch
        ON child_branch.Product = tblBOM.Child
    INNER
      JOIN tblStatus AS parent_status
        ON parent_status.Product = tblBOM.Parent
       AND parent_status.Status = 'X'   
    INNER
      JOIN tblStatus AS child_status
        ON child_status.Product = tblBOM.Child
       AND child_status.Status = 'X'",3.0
g7vltfw,j615v9,"1, Students might have the same name: so SELECT DISTINCT is wrong here.

2, You cant SELECT a column that isnt part of the GROUP BY clause, i.e. you should GROUP BY student.NAME",1.0
g7vnh4e,j615v9,"You can if the GROUP BY column is unique, which a ID presumably is. See the example here:

&gt;We have just seen that the 1999 and 2003 versions of the SQL standard require that the columns appearing in the SELECT list are functionally dependent upon the groups defined by the GROUP BY clause. In other words, if we know that a column contains only one value for any given combination of values in the columns appearing in the GROUP BY clause, we may reference the column in the SELECT list even if it does not appear in an aggregate expression.

&gt;We've also seen that if we have a (primary or unique) key, all columns that are not included in the key are by definition functionally dependent upon the key. This means that if we include all key columns in the GROUP BY clause, we can reference any column we like in the SELECT list, even if they appear outside an aggregate expression.


https://rpbouman.blogspot.com/2007/05/debunking-group-by-myths.html",1.0
g7zgkrc,j615v9,"&gt;his means that if we include all key columns in the GROUP BY clause, we can reference any column we like in the SELECT list, even if they appear outside an aggregate expression.

ok makes senese. Thanks for the tips!",1.0
g7vk2gu,j61178,"Advantages - allows you to do hierarchy trees.

Disadvantages - added complexity but otherwise, none.",8.0
g7vzavr,j61178,Thank you,1.0
g7vu83j,j61178,"&gt; PostgreSQL allows the creation of foreign keys to another field in the same table

as far as i know, all databases allow this",5.0
g7vyf07,j61178,Thanks.,1.0
g7vkx0s,j61178,The disadvantage of not having a foreign key would be that a recipe can point to a non-existing master recipe.,3.0
g7vt7gg,j61178,Then let the foreign key value be null.,1.0
g7vxb5s,j61178,"Yes, absolutely. But I was referring to the disadvantage if no foreign key constraint is defined at all.",4.0
g7vye63,j61178,"Yes thanks. A master would have this field null as well. By default if this field is null, that particularly row could be treated as the master.",2.0
g7vofxf,j61178,"it's a good idea when you have a parent-child relationship within the table and you need to ensure referential integrity. you can also opt to ensure referential integrity in the data loading process though. so it depends on the situation.

&amp;#x200B;

a potential drawback is that you have to load data in a specific order. so you always have to have loaded the parent record before loading the child record. if you have very large files that you want to ingest in parallel,or the records in your source files aren't in the right order, this would slow things down considerably",2.0
g7vta5u,j61178,deferrable constraints can help with the ordering issues but they won't help with the parallel load problems.  Part of the problem there of course is that the requirement is that each parallel load run itself makes internally consistent changes to the db.,3.0
g7w2xc9,j61178,"they'd need to have some other way of making sure that the data being dumped into that table makes sense. when you build a system designed for analysis you can usually make sure of that or you'll clean it up at a later stage, with an operational system I'm not so sure I'd want to do that, but it's not my domain",2.0
g7w52kk,j61178,Thank you.,1.0
g7vyqbx,j61178,Thanks.,1.0
g7vyopf,j61178,"Thanks - very important consideration! In this case, the business requirement would leave the Master\_Recipe\_ID as null whenever there's no parent and we wouldn't have more than one layer of children. So first load would be where all Masters are null, then children get loaded.",1.0
g7w01ox,j61178,"Yeah that's a sound process. I cant speak for databases designs in an operational context (such as a webserver backend) as that's not my domain, but if you're using this database for reporting or analytics, honestly you may not bother with FKs as long as you can ensure elsewhere in the load process that all children must have a parent if that's a requirement. (i.e. a typical loading pattern for transactional data is to check reference tables for the correct member, and if they key is not present, you either discard the record or do something else with it such as logging it in a special table for later re-processing, depending on requirements)",2.0
g7vnz90,j60y90,"If I’m guessing your code correctly: You can join your two select statements together with a normal join using subqueries, or you can union all them together and sum the amounts. Eg 
Select usr, sum(amount) from
(Select from_user as usr , -1*sum(amount) as amount from transactions group by from_user
Union all
Select to_user as usr  , sum(amount) as amount from transactions group by to_user)
Group by usr",1.0
g7vs5i8,j60y90,"Thanks you, It works like a charm!",1.0
g7vfqek,j604q2,"I used temporal table in SQL and found it also has in MariaDB, you should check it out [https://mariadb.com/kb/en/temporal-data-tables/](https://mariadb.com/kb/en/temporal-data-tables/)",2.0
g7vh6ze,j604q2,Thanks for the link.Let me check it out,1.0
g7vif09,j5zaju,"Please make sure you check out the plans your system uses to execute the queries, that is an easy way of making sure you haven’t missed an easy performance tweak. How many rows do you expect there to be in your table per day? It takes 50-100ms to go to 10,000 of them via index (assuming it used an index and didn’t just sort the entire table). You would need there to be less than 5,000 rows for your day if you set up an index on time to fit your 25ms requirement  (assuming it does a read from index to table). You can include the story_id in the index to avoid the hop between index and table for each row: create index on story_counter (time, story_id)
This will (most of the time) give you the fastest run time when you want to look at a single column with a filter on another. You could also help it out by restricting the high value of time to   Unix_timestamp, this will help your optimizer believe that the index is the way to go. If that isn’t fast enough then you need to start thinking about it differently. If you have huge amounts of rows then you could store some of this result precomputed, eg you could have a table that you populate each minute (or hour etc) with the count per story_id. Each time this runs you only need to look at a smaller chunk of rows (you can still use that index suggestion from before, and you should index this aggregate table in the same way). When it comes to getting your last days figure you would just run a query against your precomputed set (with sum(count_col) rather than count(*)) and combine that with the live number for current minute/hour. It’s a bit fiddly and you’ll need to make sure your preaggregated table is always kept up to date but this sort of thing will make your final query almost instantaneous. Do understand though that users don’t really care about 25ms or 200ms, that will always be conceived as very fast - the problem is if you have many users trying to do the same thing at once. If you are expecting many users to want this result at once then to handle that (without using my other tricks) you can just have a single process updating a table as often as it can with the results of this query, your users just need to read this small table each time.",1.0
g7wrxx9,j5zaju,"You said a lot so I'll see what I can clarify/respond to.

- Not sure I understand what you mean about how my system executes the queries.

- A random sampling of the last 24 hours said 698,876 new rows were added.  And just doing the count took more than 5 seconds.

- I'm not sure how to set up indexes.

- When I load the webpage, I see a difference in page-rendering if the story counter is there or not.  I also have a query monitor testing the query speeds.  There are about 200 queries on every page load.  Almost every one is less than a millisecond.  Excluding this query, the slowest is around 20ms.  

- I guess if it's too complicated (I'm not a SQL-specific expert, as I'm the full-stack developer for the whole site), if I can walk away getting duplicate IP/story_id pairs filtered out with a reasonable query time, I'd consider this post a success.",1.0
g7vh3gz,j5z86m,"BIG difference. 

Each new connection you make requires that the database does quite a lot of work - it’s got to deal with making he network connections, doing the authorisation, setting up memory and process structures to deal with the new connection and that all takes a fair amount of effort 

Reusing an existing connection bypasses all of that. That’s why database connection pooling is an important consideration when designing and implementing applications.

Lanbda and serverless makes this slightly less controllable in some ways, but the general concept remains true.",12.0
g7wfu5z,j5z86m,"So when the client closes a connection and returns it to the pool , does something happen on the db side ? 

Regardless it's becoming clear relational database is not a good fit for Lambda.",1.0
g7w1pjg,j5z86m,"Depending on the database framework, the # of allowed open connections can be a finite number, so keep that in mind too. If you have 1000 users all using 10 requests per page load or whatever, you might end up with a bottle neck where people are waiting on available connections to open, to actually proceed.",2.0
g7v078x,j5xc0i,Use datepart(hour) function. Quick Google search with this will give you what you need.,2.0
g7v73wy,j5xc0i,Actually I figured out with  EXTRACT function but still not sure percentage one though. Thanks!,1.0
g7w5mo3,j5xc0i,"You will need to create a subquery with the sum of everything as a column, and in the main query use the new total column to divide against for percentage",1.0
g7yu6yv,j5xc0i,"&gt;SELECT requestor\_client\_id,COUNT(order\_status), SUM(total\_price) FROM vanorder  
&gt;  
&gt;GROUP BY requestor\_client\_id  
&gt;  
&gt;HAVING COUNT(order\_status)='1'

how can I sum everything on column once you need one solid column to group by..?",1.0
g7yxrn9,j5xc0i,"Check out this link:

https://www.essentialsql.com/get-ready-to-learn-sql-server-20-using-subqueries-in-the-select-statement/",2.0
g8abkdb,j5xc0i,Ohhh thank you so much. let me try again,1.0
g7uxmbf,j5x45d,We aren't here to do your homework for you. What part(s) are you struggling with specifically?,10.0
g7uyqgh,j5x45d,"Yeah, even I just explained the overall process and let them figure out how to write the query",7.0
g7uylz3,j5x45d,"I think I know the answer, but will help you understand the process and let you write the query yourself.

Q1) 

--&gt; First inner join the tables (media type and track) on id and Name = audio
--&gt; now create a new column of avg(Milliseconds) as avg_time
--&gt; create another column sum(Milliseconds) partition by Name as track_len
--&gt; select distinct of Name where avg_time &gt; track_len


Q2) 

--&gt; First filter the customer table for country = US and for those records join the invoice table to get the different invoice ids issued against each customer and then join it again with invoice line table on invoice id and bring in the unitprice and quantity

--&gt; once you've got this, multiple the unitprice and quantity to get the total amt_paid


--&gt; you table now should have customers mapped to 'n' invoices and against each invoice what was the total amt_paid for that invoice

--&gt; now simply aggregate the total amt_paid at the customer id level and order it by amt_paid desc to get the customers who've paid the most


Let me know if you are struggling with the writing the query itself and will help you accordingly",5.0
g7v9c73,j5x45d,"Newbie here trying to get some practice in. Why did you use the partition by clause for Q1? Couldn't you extract the track length in the subquery and then select name where track length&gt;avg length?

For Q2: You forgot to select customers that do not belong to a company. That where clause should be belong at the end right? Also, why take take the unit price \* quantity instead of the sum(total) from invoice?",2.0
g7v9uzz,j5x45d,"For Q1. You can do that as well, don't see a problem there.

For Q2. Thanks for pointing that out, you can run the entire query with all the customers and finally filter out the customers in US and not present in the company. Since I haven't seen the data, the total column could represent the total # of invoice or something else. That said, I could clearly see price and quantity present in the next table and suggested to multiply that. If the total column represents the same thing, as you mentioned, we can sum(total) as well.",0.0
g7v1ja9,j5x45d,"Im still struggling writing the query, specifically Q1",1.0
g7v1m1f,j5x45d,"Type in whatever you've written so far, and we can check it",2.0
g83hrtu,j5x45d,"    SELECT Name 
    FROM Track
    Inner Join Track on MediaType 
    WHERE MediaTypeid = ""audio""
    AND 
    WHERE  

?",1.0
g86e2eh,j5x45d,"    So are you not planning to follow the clear guidance posted by /u/quatrolingo just above

?",1.0
g8gj49w,j5x45d,how do you start off by inner joining tables ?,1.0
g8i4rbo,j5x45d,"&gt;inner join

[https://www.w3schools.com/sql/sql\_join\_inner.asp](https://www.w3schools.com/sql/sql_join_inner.asp)",1.0
g7v62ao,j5x45d,"Your teacher, textbook and reference material would be great places to start.  Make sure to brush up on subqueries in particular.",3.0
g7v2fox,j5x45d,[deleted],1.0
g7v4nav,j5x45d,"You're not going to gain anything from going through these motions and refusing to accept guidance over being handed the answer. You need to be able to break down and solve these questions when they come up to get any value from SQL in the future. If you don't want to learn this, you're spinning your wheels and may be pursuing the wrong field. 

That said, you can do this if you work through the guidance offered here. Don't psyche yourself out and quit before even trying.",4.0
g7unstx,j5vtkf,"you're grouping by artist and invoice, you should get one row per unique pair.",4.0
g7uonir,j5vtkf,"Yup that was it, forgot to remove Date from Group BY. Doh.

Thanks!",6.0
g7up2za,j5vtkf,"You've already filtered the data for 2010, then grouping it by date again will give you 'n' separate counts for 'n' separate dates. If you are only looking to aggregate the sales at artist name level, then we can drop the 'date' from group by",4.0
g7vcd2e,j5torp,"`count(some_column)` only counts rows where `some_column` is not null. 

So this query:
```sql
select count(some_column)
from the_table
```

is the same as: 
```sql
select count(*)
from (
  select *
  from the_table
  where the_column IS NOT NULL
) t
```

But you are selecting only rows where the column value is null and thus if you add `count(column)`, the result is  0",1.0
g7u9cp1,j5torp,are you sure your column is null?  Try looking for a blank. ( = ' ' ),1.0
g7u9t5v,j5torp,"I get the correct rows with the first query and the error “invalid input syntax for type date” when I type the following 


    SELECT  COUNT(column)
    FROM    Table
    WHERE   Column    =   '';",1.0
g7uatjc,j5torp,"Is column numeric?  I'm not sure why that wouldn't work even so.

are you saying that this:

 SELECT \* FROM Table WHERE Column IS NULL; 

gives you results but this:

 SELECT count(\*) FROM Table WHERE Column IS NULL; 

does not?",2.0
g7ub4jk,j5torp,Oh it worked with COUNT(*)! I had written COUNT(Column) which didn’t work. Thank you so much!,3.0
g7uytkl,j5torp,"Just a heads up count(column) returns a count of the non-null values. Count(*) returns all rows (filled or empty). 

Count(column) is also an alias for count(all column), where all translates to: “count all values, even duplicates, but don’t count the nulls.” It may be better to refer to count(all column) as the default behavior of count(column). 

Count(distinct column) will count all distinct non null values in your column

So, if your column contains the values (1,1,2,null) then:

Count(*)=4

Count(column) = 3

Count(distinct column) = 2",3.0
g7tzgt4,j5ru9a,I made this video to explain joins and subquery with visualization https://youtu.be/P8hxoMQw7ig. I think most people can’t remember everything that’s longer than 10-15 min so I made 3 separate videos in my channel to explain SQL with visualization https://www.youtube.com/channel/UCR5fQQoUywhm3tE3O0HSVVg here is the channel link,2.0
g7vaya8,j5rr5h,"Looks like it is in 3NF. 1st NF has two criteria - one, ensure all fields are atomic in nature. Meaning that one field doesn’t hold more than one value. Two, ensure there are no repeating fields. Looks good on that end. 2NF is to ensure there are no partial dependencies. Which occurs when you have a primary key made up of two or more fields. Good on that end. And 3NF is to ensure there are no transitive dependencies. I don’t see any transitive dependencies. Email your professor if you can.",2.0
g7vmfba,j5rr5h,"Tried to confer with som TA's and two lecturers, they all gave me some solution. Most of which did not offer a solution, only introduced several more tables, with no real fix. Edited the post to clarify a little.",1.0
g7ueihs,j5rr5h,"I don't really get what you're supposed to do...did the instructor give you this ERD? It looks like it's already in 3NF, except some of the relations to the Class table are on Class.ClassName instead of Class.Id.",1.0
g7vm83u,j5rr5h,"Thanks, the ERD is self made. See the edit in the post, please ask if there is something you feel like you are missing.",1.0
g7w804x,j5rr5h,"Is ExerciseReg supposed to be ExerciseRequirement, as in the exercises required for a class?

Because I'm thinking all you need is a one-to-many from Exercise to ExerciseReg and you're done. ExerciseReg is basically the middle table for a many-to-many relationship between Class and Exercise. Each class will have an ExerciseReg row for each required exercise.",1.0
g7tip49,j5pf4k,"You should be able to do a pivot using conditional aggregation and the `ROW_NUMBER` window function. Assuming a table `t` with your listed attributes, the following query should work (note the inner query, which groups together unique ticket-payers, and use of the `ROW_NUMBER` window function):

```
SELECT
	-- basic pivot using ROWNUMBER and conditional aggregation.
    ticket,
    SUM(total_payer_payment) AS total_ticket_payment,
    MIN(CASE WHEN rn = 1 THEN Payer END) AS Payer1,
    MIN(CASE WHEN rn = 2 THEN Payer END) AS Payer2,
    MIN(CASE WHEN rn = 3 THEN Payer END) AS Payer3,
    MIN(CASE WHEN rn = 4 THEN Payer END) AS Payer4
FROM
(
	-- make sure to group by ticket and payment
	-- in the inner query, to remove duplicate payers.
    SELECT [Ticket#], SUM(PaymentAmt), Payer, 
        ROW_NUMBER() OVER (PARTITION BY [Ticket#] ORDER BY Payer)
    FROM t
    GROUP BY [Ticket#], Payer
) AS t_rn(ticket, total_payer_payment, payer, rn)
GROUP BY ticket
```

working fiddle [here](https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=8456edf3ad0a72f24a50355716e8feca)

If you have any control over the database, I would recommend against having the `#` in your `Ticket#` attribute, sticking to alpha-numeric characters (e.g. `ticket_number`).",7.0
g7uuypz,j5pf4k,"I've provided my solution assuming that you need to aggregate the duplicate rows as well (if now then just make subquery of select distinct col_names before implementing this)

Select distinct
ticket#, total_payment,
Case when rnk = 1 then payer end as payer1,
Case when rnk = 2 then payer end as payer2

From


(Select

ticket#, total_payment,
Dense_rank() over (partition by ticket#, total_payment order by payer asc) as rnk,
Payer

From
(Select distinct
 ticket#, 
max(total_payment) over (partition by ticket#) as total_payment,
Payer
--this is to assign single total payment amt to each ticket#

From
(SELECT 
Ticket#,
Sum(payment_amt) over (partition by ticket#) as total_payment,
Payer 
FROM table_name
) agg_pay 
) agg_pay_2
) Rank_payer

This should ideally work, haven't ran it though.",3.0
g7ueugp,j5pf4k,"Select sum($), string_agg(', ', payer) from table",1.0
g7uewow,j5pf4k,Forgot the group by ticket,1.0
g7tkm38,j5pf4k,I’ll do a sum per ticket and put all the players in a single column using semi column or coma separator,0.0
g7tmcds,j5pf4k,"Possible implementation below, as well as a STRING_AGG solution, as you might have a lot of different payers possible and you might then need to generate the query dynamically and/or copy/paste a lot.

	DROP TABLE IF EXISTS #TicketSales;

	CREATE TABLE #TicketSales (
		[Ticket#] int,
		[PaymentAmt] int,
		[Payer] varchar(50)
	);

	INSERT #TicketSales
	VALUES
		(100,20,'John Doe'),
		(100,150,'Insurance ABC'),
		(100,100,'Insurance ABC'),
		(100,50,'John Doe');

	/* CTE, ROW_NUMBER and PIVOT solution */
	WITH CTE AS (
		SELECT
			[Ticket#],
			SUM(PaymentAmt) AS PaymentAmt,
			[Payer],
			ROW_NUMBER() OVER (PARTITION BY [Ticket#] ORDER BY [Payer]) AS row_num
		FROM #TicketSales
		GROUP BY [Ticket#], [Payer]
	)
	SELECT
		[Ticket#],
		SUM(PaymentAmt) AS PaymentAmt,
		MAX([1]) AS Payer1,
		MAX([2]) AS Payer2
	FROM CTE
	PIVOT( MAX([Payer]) FOR row_num IN ([1],[2])) AS pvt
	GROUP BY [Ticket#];

	/* STRING_AGG Solution */
	WITH PayAmt AS (
		SELECT
			[Ticket#],
			SUM(PaymentAmt) AS PaymentAmt
		FROM #TicketSales
		GROUP BY [Ticket#]
	),
	Payers AS (
		SELECT DISTINCT
			[Ticket#],
			STRING_AGG([Payer], ', ') AS Payers
		FROM (
			SELECT DISTINCT
				[Ticket#],
				[Payer]
			FROM #TicketSales
		) AS TicketSales
		GROUP BY [Ticket#]
	)
	SELECT *
	FROM PayAmt
	JOIN Payers
	ON PayAmt.[Ticket#] = Payers.[Ticket#]

	DROP TABLE IF EXISTS #TicketSales;",7.0
g7u2r1b,j5pf4k,I wish I didn't have to write SQL to support 2008r2... STILL!,2.0
g7vnvyw,j5pf4k,Well the combination of FOR XML PATH and STUFF still works and I have to keep using everywhere.,1.0
g7tgrbb,j5p22q,Just Google some courses and play bit more with code. I have learned syntax from sololearn and then went on to do SQL challenges on hackerrank and codesignal. I'm no pro currently but it was enough to land me a job as dbAdmin,4.0
g7tnhke,j5p22q,That is what I am asking what courses are actually helpful.,2.0
g7tlcl5,j5p22q,"&gt;Just Google some courses and play bit more with code. I have learned syntax from sololearn and then went on to do SQL challenges on hackerrank and codesignal. I'm no pro currently but it was enough to land me a job as dbAdmin

Thanks for the tips, really appreciate the insight. Can you give some insight on how long it took you and what your approach to landing your job was?",1.0
g7v5z50,j5p22q,"About a year, but I am also learning Java",2.0
g7v6v22,j5p22q,"Any reason going the Java route instead of Python

Sorry to badger ya! There's just so much info online and it gets difficult to wade through with little understanding. Appreciate you taking the time",1.0
g7wg2dk,j5p22q,There are more job opportunities for Java and it is used far more often by big companies.,1.0
g7tg1cn,j5p22q,[deleted],2.0
g7thlov,j5p22q,SQL server developer edition is actually free to use for testing and development ( and obviously learning). This is basically same as the enterprise edition,5.0
g7tnkcp,j5p22q,"I am looking for courses that walk me through the setup, not just databases.",1.0
g7tts2n,j5p22q,Microsoft has plenty of virtual labs for setting up SQL Server if you Google for them.,1.0
g7tv4o1,j5p22q,That's what I'm asking help for. I know how to google. I do not know what is actually helpful. It looks like I should've just posted on /r/learnsql.,0.0
g824lz9,j5p22q,Try Microsoft Virtual Academy - free to join and plenty of video tutorials and courses on SQL Server. So you can have free training and the free SQL Server Developer Edition to use too.,1.0
g7ucolo,j5p22q,Jose Portilla’s course on Udemy is pretty good. Helped me a lot when I started out.,2.0
g7uia98,j5p22q,"Thank you for link, I found another advanced sql course on Udemy that I am also looking at",1.0
g7tz5hq,j5p22q,For primary key and Foreign key concept https://youtu.be/ij5q88rrR38,1.0
g7u1de7,j5p22q,Thank you for the link. I will watch the video.,2.0
g7u2yvx,j5p22q,https://docs.microsoft.com/en-us/learn/,1.0
g824npc,j5p22q,The Wiki may help - http://www.reddit.com/r/SQL/wiki/index,1.0
g7te38r,j5nga9,"Does it have to be done in sql only? This is the exact type of thing that lends itself well to some sort of server side language like python, c#, etc.",1.0
g7waxwx,j5nga9,"It does not have to be in SQL only, but I constantly have issues integrating MySQL as opposed to MSSQL with Python..",1.0
g7tg1ka,j5nga9,"Window functions. [https://dev.mysql.com/doc/refman/8.0/en/window-functions-usage.html](https://dev.mysql.com/doc/refman/8.0/en/window-functions-usage.html)

Something like

    select c.order_id, c.item_id, c.item_price,
       case
        when c.order_line_number = c.num_items_in_order
         then c.order_grand_total
       end as order_grand_total
     from
       (select a.order_id, b.item_id, b.item_price, a.order_grand_total,
           row_number () over (partition by a.order_id order by b.item_id) as order_line_number,
           count (*) over (partition by a.order_id) as num_items_in_order
         from order a inner join item b on (a.order_id = b.order_id)
       ) c
     order by c.order_id, c.item_id ;",1.0
g7wl52j,j5nga9,"I've been messing around with this, making small adjustments here and there, but no matter what I do with this I keep getting a syntax error. I even get a syntax error just from this exact block and I'm really unsure why... everything looks right to me, syntax-wise.",1.0
g7wok0d,j5nga9,"How can someone help you if you don't tell us what error you are getting? Try running the internal part by itself, and see if that works.

    select a.order_id, b.item_id, b.item_price, a.order_grand_total,
       row_number ()
         over (partition by a.order_id order by b.item_id)
         as order_line_number,
       count (*) over (partition by a.order_id) as num_items_in_order
     from order a inner join item b on (a.order_id = b.order_id) ;",1.0
g7wpng5,j5nga9,"Sorry, I should have been more specific. I didn't specify because I honestly didn't think that the error message it gave me would be helpful at all. With the first block I tried, it just said ""select is not valid at this position"" for the first select statement. The only thing I tried that was able to change that message was getting rid of the comma right before 'case', which then just gave me the error ""case is not valid at this position.""

I tried running the internal part by itself, and it's just giving me "" '(' is not valid at this position"" for the '(' at the first 'over (partition'.

Sorry again for not letting you know exactly what the error was... I'm still trying to mess around with both of these blocks but the errors I described above are all I keep getting and they just don't seem at all helpful to me. Please let me know if you have any ideas. Thanks for taking the time to try to help me with this.",1.0
g7wqhf6,j5nga9,What version of MySQL are you using? Maybe you have an older version that doesn't support window functions.,1.0
g7wsbc0,j5nga9,"I'm using 8.0.21. When I looked it up, it said MySQL supports window functions (or at least ROW\_NUMBER()) starting at v 8.0, so I don't understand why it's not working. HOWEVER, I think I was just able to emulate the internal part with the following:

    set @row_number := 0;
    SELECT 
        @row_number:=CASE
            WHEN @order_no = order_id
    			THEN @row_number + 1
            ELSE 1
        END AS row_num,
        @order_no:=order_id order_id,
        item_id, item_price,
        order_grand_total
    FROM
        (SELECT item.item_id, item.order_id, item.item_price, order.order_grand_total
    	 FROM item, order
            WHERE order.order_id = item.order_id) as a
    ORDER BY order_id;

Output:

|row\_num|item\_id|order\_id|item\_price|order\_grand\_total|
|:-|:-|:-|:-|:-|
|1|101|201|$10|$35|
|2|102|201|$25|$35|
|1|101|202|$10|$40|
|2|102|202|$25|$40|
|3|103|202|$5|$40|",1.0
g7yhve9,j5nga9,"I have no idea why you couldn't get the window function to work. But you found a solution, so congratulations.",1.0
g7vej2c,j5nga9,"How about this? (Might need to be tweaked to work in MySQL)

    SELECT a.item_id, a.order_id, a.item_price, CASE WHEN c.order_id IS NULL THEN ‘’ ELSE b.order_grand_total END AS [order_grand_total]
    FROM item AS a
    INNER JOIN order AS b ON a.order_id = b.order_id
    LEFT JOIN (
        SELECT order_id, MAX(item_id) AS [item_id]
        FROM item
        GROUP BY order_id
    ) AS c ON a.item_id = c.item_id AND a.order_id = c.order_id",1.0
g7svvae,j5mg7t,"Will need to see an ERD / Database diagram, but probably looks like this:

SELECT \[name of product colum\] FROM \[table with product data\] WHERE categoryID = 2;

This will be different if categoryID is not stored in the same table as product data. Would also change if you do not want duplicate product names in your report.",5.0
g7sw7pn,j5mg7t,Thank you very much for your help,1.0
g7svnnv,j5mg7t,Is this a homework problem?,1.0
g7swgqg,j5mg7t,No I am just learning from a website called w3dchools and helps you learn from awls but I was stuck and couldn’t figure it out,0.0
g7sw14y,j5mg7t,Can you elaborate more?,1.0
g7sspde,j5lou7,"I can't help you with your problem, sorry. 

But I can point out that's not ISO8601. You need hyphens instead of slashes as shown in your screenshot.",4.0
g7sszsq,j5lou7,"Thanks :/  


How can I change the dash/slash in SQL?",1.0
g7t2jgw,j5lou7,"Replace(column, ‘-‘, ‘/‘)",1.0
g7trk58,j5lou7,"`date` or `timestamp` column do not have ""a format"". Any format you see is applied by the SQL client _displaying_ the values. 

You can either configure your SQL client to display `date` values in a different format or you can use `to_char()` to convert the value to any format you like, e.g. `to_char(dateadded, 'yyyy/mm/dd')` - not that `yyyy/mm/dd` is **not** ISO format. `yyyy-mm-dd` is the standard ISO date format. 

Your output looks like `psql`, I don't use that but I think the [DateStyle](https://www.postgresql.org/docs/current/runtime-config-client.html#GUC-DATESTYLE) parameter controls the output there.",2.0
g7saqup,j5j0gb,"For getting a hang of SQL subqueries, I used to practice on sqlzoo, why don't you try that?

[SQL zoo website ](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://sqlzoo.net/&amp;ved=2ahUKEwjSnsDkxJ3sAhX66XMBHWl-DasQFjAAegQIDRAD&amp;usg=AOvVaw18N8tWa_wEOlA-sYVAmme2)",6.0
g7saxtz,j5j0gb,Oh great. I will look at it. But does it have information about subqueries?,1.0
g7sc0x3,j5j0gb,"Yup, try the select within select section. It also has a quiz to test your learnings",2.0
g7ssenx,j5j0gb,"I'll make a pitiful attempt to explain them.  Who knows, it might stick?

Disclaimer:  This is for MS SQL, and as you have not flaired your post as I'm writing this or mentioned your DBMS I'm going to explain it for MS SQL.  The concept is the same for other languages, though syntax may vary.

Normally a query looks something like ""SELECT columns FROM table"".

You've probably noticed by now that the output of a SELECT statement looks exactly like a table.  Well, of course it does, because that's how you look at the contents of a table!

So, if you take your SELECT statement and wrap it in brackets, you can use the output of that statement as if it were a table.

    SELECT * From (SELECT Col_A, Col_B FROM table_a  WHERE Col_A &gt; Col_B);

(Note that the above sample is completely pointless - but it's handy for illustration)

In this case, boolean logic applies, meaning first we evaluate inside the brackets.  Get A and B from the table where A &gt; B.  The result of this query becomes the source table for the outer query, and you get an output that only contains Col\_A and Col\_B where that condition is met, because you're not looking at table\_a, you're looking at the output of that ""inner"" query.

Once you've met your current requirement for subqueries, look into the CTE (Common Table Expression).  It's basically the same thing but a zillion times easier to read.  Actually, look at the CTE anyway if you're still struggling - it might help make the connection for you.

    ;WITH CTE AS (SELECT Col_A, Col_B FROM table_a  WHERE Col_A &gt; Col_B)
    SELECT * FROM CTE

This CTE gets exactly the same results, but is easier to follow, especially when things start to get really complex.  (The CTE can also handle recursion, but that's well outside the scope of a subquery.)

Yes, you can use these in a join.  One table can be a regular table, the other can be a subquery/CTE.  Or it can be six subqueries if you hate the person that has to review your code.

Another example, and one of the few places I will use a subquery, is to look for items that exist or don't exist in a related table.

    SELECT A_bunch_of_columns From Table_A where pk IN (SELECT fk FROM Table_B);

In this case, the engine first gets a list of all unique entries for the fk column in Table\_B, then uses that list to filter results from the output of the outer query, returning all records in table A that have a corresponding record in table b.  While you could achieve this filtering with a JOIN, I've found that the query performance is consistently faster with this subquery method, sometimes by a lot.",8.0
g7sv3qa,j5j0gb,Thanks for this!,3.0
g7uqfry,j5j0gb,"Great explanation!

I think your last point on join vs sub-query can vary, though. Sometimes just because you need to update stats or something, which may not be in your control. From the perspective of other readers, though, I'd focus on comprehending sub-queries before worrying about query tuning. Start googling that once your queries take more than 30 seconds. Then you can play with whether or not a join or sub-query (or whatever else) works faster.",2.0
g7usz79,j5j0gb,"Yes.  Generally you don't need to worry about tuning for performance early on.  I worry that people may use the join unnecessarily though.  Personally I like the cleanliness of the CTE or even sub query in this case.  That way future you isn't looking at the code wondering why that join is there but nothing selected out of it, cleaning it up, and breaking the report.

I'm learning about a lot of performance no-nos that I remember being taught by instructors, and it's all stuff that will never manifest until your code has been in production for a couple years.  I'm hoping to safe future people like me from having to deal with poor queries so we can focus on indexes instead.",2.0
g7sv7dj,j5j0gb,"*SQL Queries for Mere Mortals* by John Viescas and Michael Hernandez has a very good chapter (Ch 11) on subqueries. This is a gentle introduction to SQL that has the advantage of being platform agnostic.  The chapter includes diagrams to help understand the order of execution and how the data flows through the process.

Unlike many analysts I've run into, I learned subqueries prior to temporary tables and common table expressions.",2.0
g7t093c,j5j0gb,Joins and subquery together https://youtu.be/P8hxoMQw7ig,1.0
g7t157o,j5j0gb,"Sql queries for mere mortals is a good one. I would suggest comparing and contrasting subqueries with joins, there is some overlap there and it really helped me understand both better",1.0
g7t50be,j5j0gb,"Think of the sub query as just another table you're joining to. So if you need info from table A and table B you would simply join on the key. It's no different with a sub query, except you're creating table B with the results of a query. 


Select a.order_number, a.item, b.customer
From table a join table B on a.order_number = b.order_number

But if you want a subset of the customers table, say only customers that joined after 2019, you can handle with a sub query, instead of in the where statement, like so:

Select a.order_number, a.item, b.customer
From table a join (select * from table B where customer_date &gt;= 2019) as B
on a.order_number = b.order_number

You wouldn't normally do this with a standard inner join because you could handle that with the where statement. Where you really start using subqueries is outer joins, so that where statements from outer joined tables don't adversely affect your results.",1.0
g7vafe5,j5j0gb,"I'd recommend StrataScratch platform. Here, you can test your code, modify it, and look up the answers if you need.",1.0
g7vfjeo,j5j0gb,Is it free?,1.0
g7vflw8,j5j0gb,SQL-ex.ru,1.0
g7shn11,j5ir46,"    SELECT products.name
         , products.price 
      FROM ( SELECT price
               FROM products
             GROUP
                 BY price
             HAVING COUNT(*) = 1 ) AS these
    INNER
      JOIN products
        ON products.price = these.price",6.0
g7snwat,j5ir46,"That should work perfectly. But just for interest's sake here's a version with window functions instead of a self join. 

    SELECT 
      name, price 
    FROM (
      SELECT 
        name, price,
        COUNT(*) OVER 
          (PARTITION BY price) AS count
      FROM products
    )
    WHERE count = 1",3.0
g7srqke,j5ir46,Thanks!!,1.0
g7so9b5,j5ir46,"you need an alias for your subquery, but otherwise, yeah",0.0
g7sp68l,j5ir46,What version of SQLite you running? Works for me as is.,1.0
g7sqrfc,j5ir46,"i don't have sqlite

i just assumed every database required the derived table to have an alias

sorry",1.0
g7srqyb,j5ir46,Thanks!!,1.0
g7s96x6,j5ir46,So if you have product a at 1 USD and product b at 1 USD then you don't want to see either of them?,2.0
g7sa3js,j5ir46,Exactly,1.0
g7s10v7,j5hd2v,"Nevermind problem was solved with  

    LIKE CONCAT('%',?,'%')",1.0
g7s2bdr,j5hd2v,ok,1.0
g7si1db,j5gmnk,"    SELECT tableA.id
         , tableA.currentSituation
         , tableA.moodStart
         , tableA.automaticThought1
         , tableB.id
         , tableB.allOrNothing
         , tableB.blamingOthers
         , tableB.catastrophizing
      FROM tableA
    LEFT OUTER
      JOIN tableB
        ON tableB.tableAid = tableA.id
    ORDER 
        BY tableA.currentSituation DESC

from your description of the probelm, you want all rows from A, with matching rows from B if any

classic left outer join",3.0
g7sb9dg,j5gmnk,[https://www.sqlitetutorial.net/sqlite-join/](https://www.sqlitetutorial.net/sqlite-join/),2.0
g7t0jt3,j5gmnk,Are you using any WHERE conditions? A LEFT JOIN will function as an INNER JOIN if you have a WHERE condition such as 'b.field = n',2.0
g7rxuqa,j5gmnk,I dont really got your question but as from where I understand it you want to get back all rows from both columns? I'd say full outer,1.0
g7s4q3l,j5gmnk,Could you tell me what was not clear about the question so I can edit it? Full outer join isnt recognized as a key word in android studio / sqlite,1.0
g7sw4ok,j5gmnk,"Inner join = only rows with a match in both tables.   

Left outer join = all rows even those without a match",1.0
g7rhoob,j5eei1,You can’t use the column alias that you create in the SELECT portion since it doesn’t get calculated until later on. The single quote denotes it as a text string and it’s failing to compare it to a number. You’ll need to repeat the calculation in the WHERE clause if you want to use it.,4.0
g7rhvm9,j5eei1,"To add to this \^, read up on the execution order of a SQL query: [https://sqlbolt.com/lesson/select\_queries\_order\_of\_execution](https://sqlbolt.com/lesson/select_queries_order_of_execution)

It will help you figure when you can and can't use aliases",2.0
g7riyy5,j5eei1,What do you mean in the last part? Repeat it again in the where clause? Like a sub query?,1.0
g7rn1di,j5eei1,"He meant this:

    SELECT  [order_num] ,[order_item] ,[prod_id] ,[quantity] ,[item_price]
    ,[quantity]*[item_price] AS 'TotalPrice'
    FROM [crashcourse].[dbo].[orderitems]
    WHERE [quantity]*[item_price]  &gt; 100",2.0
g7ruw98,j5eei1,"You can't use an aggregate in a Where clause. Google ""Having"".",1.0
g7rvh50,j5eei1,"It's not an aggregate function, it's a calculation. You are just multiplying two columns. Aggregate functions work on multiple rows within one column. HAVING can only be used if there is also a GROUP BY clause",1.0
g7rvtha,j5eei1,"Whoops, to early for this. My apologies to the OP.",1.0
g7rw42u,j5eei1,"No need to apologize. This is a subreddit for learning, which you can't do without making a few mistakes. I sure as hell make mistakes",3.0
g7t8xhf,j5eei1,"Thanks again! Can I ask, how long have you been using SQL and how long does it take to become a intermediate and advance level?

Thanks",1.0
g7tcffv,j5eei1,"I've been using SQL for 4 years but I would say that I'm between beginner and intermediate level. Subjectively, it would take 4 to 5 years to be intermediate. You're competence level in SQL heavily depends on the job you have/looking for and the type of data you're working with. Working with large amounts of broken data helped me learn the ins and outs of querying, database design, and optimization.

u/boy_named_su created a great list of SQL concepts that an expert in SQL should know: [https://www.reddit.com/r/SQL/comments/i69qx3/how\_to\_learn\_more\_than\_the\_basics/g0uchee?utm\_source=share&amp;utm\_medium=web2x&amp;context=3](https://www.reddit.com/r/SQL/comments/i69qx3/how_to_learn_more_than_the_basics/g0uchee?utm_source=share&amp;utm_medium=web2x&amp;context=3)",2.0
g7tj3sv,j5eei1,Thanks! There is much to learn for me. and for the link too.,1.0
g8bgxcd,j5eei1,"Hi, where does ""CASE""  fit in with the order of executions? Thanks",1.0
_,j5eei1,,
g7spm8d,j5eei1,"As pointed out by u/Achsin, the reason for the error message is in your `WHERE` clause: `WHERE 'TotalPrice' &gt; 100`. MS SQL is interpreting this as a string literal, rather than the name of a field. It's then trying to compare a string literal/VARCHAR to an integer (`100`), which is causing the error.

A couple of other points:

 - You only need to use square brackets around identifiers (e.g. `prod_id`) if the identifier name includes spaces, special characters, or is reserved. For example: `SELECT [update] FROM [you can have spaces in table names, but it's frowned upon]`. So usually, you can just use the identifier name. You can also use double quotes instead of square brackets, which is a SQL standard. But again, you shouldn't have to.

 - If you're querying the database directly, you don't have to specify it in the query. So you could write `SELECT ... FROM dbo.orderitems`.

 - Since `dbo` is the default schema, you don't _need_ to include it in your query. Although it can be good practice to always specify the schema. YMMV.

_Finally_, and this might be more complex than you want to get into right now, it _is_ possible to reference a calculated column directly in the `WHERE` clause, using a lateral join (`CROSS APPLY`/`OUTER APPLY` in MS SQL):

```
SELECT 
    order_num, 
    order_item, 
    prod_id, 
    quantity, 
    item_price, 
    calc.total_price
FROM orderitems
    CROSS APPLY
    (SELECT quantity * item_price AS total_price) AS calc
WHERE calc.total_price &gt; 100
```

Although this solution would be less efficient than simply repeating the calculation in the `WHERE` clause.",1.0
g7t8uom,j5eei1,Thanks,1.0
g8bbtl2,j5eei1,"Can you explain and expand on the last query/statement?

What is the Cross apply?
 Where ( [quantity] * [item_price]) &gt;100

gives the same as - your comment of

 CROSS APPLY (SELECT quantity * item_price AS total_price) AS calc 
  
  WHERE calc.total_price &gt; 100",1.0
g7rfjjx,j5e7dn,"Hello u/RogerSmithII - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g7ricd7,j5e7dn,you need to figure out where the psql executable is stored and add that folder to your path variable,1.0
g7rfdul,j5e6ca,"Hello u/camcode - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g7swcr6,j5e6ca,"When inserting into a table that has a trigger that will insert into another table, scope_identity() will return the identity value created by your insert while @@identity will return the identity created by the trigger. Generally you will want to use scope_identity(). There can be issues on older versions of SQL prior to SQL 2008 R2 service pack 1 where parallel execution plans may result in incorrect values coming back with both methods.",1.0
g7ra40j,j5cx8x,"Well that's, kinda some shit.

I'm MSSQL guy, but you could use UNION (or self join OR unpivot which is probably what the instructor is looking for?) to get 3 entries for each student then GROUP BY and AVG() to get their average grade along with a HAVING to restrict the results to AVG() &gt; 70

Sucks that this one got you caught up, try to always move on to try and get some other questions out of the way before coming back... It beats a 1/10.",18.0
g7rf128,j5cx8x,"It was an online test and I couldn't backtrack and it was timed.


How would you formulate it? I understand your points in those are the  values  I'm supposed to use but I don't know how to construct it and Google has not given me the answer.

If this was #2 and it stumped me I can't imagine the rest, probably worse.",2.0
g7rh93j,j5cx8x,"On mobile, hopefully the formatting works okay. I’d probably do something like this:

    WITH Scores AS (
    SELECT name, hist AS [Score] FROM grades
    UNION ALL
    SELECT name, math AS [Score] FROM grades
    UNION ALL
    SELECT name, scien AS [Score] FROM grades
    )
    SELECT name, AVG(score) as [Final Grade]
    FROM Scores
    GROUP BY name
    HAVING AVG(score) &gt; 70;",23.0
g7rhm1w,j5cx8x,"Okay, also got it wrong the first time so I recreated the table and tried it out.

What works is:

    Select a.name, AVG(a.math) AS ""FINAL GRADE""
    FROM (  SELECT name, math 
            FROM grades
            UNION ALL
            SELECT name, hist
            FROM grades
            UNION ALL
            SELECT name, scien
            FROM grades) a
    GROUP BY name
    HAVING AVG(a.math) &gt; 70;",14.0
g7rghy5,j5cx8x,"this is the basic use case of HAVING()

&amp;#x200B;

~~SELECT name, AVG(math+hist+scien) AS avg\_grade~~

~~GROUP BY name~~

~~HAVING avg\_grade &gt; 70~~

&amp;#x200B;

/u/SQLassie and /u/Achsin posted the correct code

&amp;#x200B;

\[edit\] I didn't get the first bit about the average correctly. going to edit this later. Does the test specify which DBMS you're using?",5.0
g7rgthy,j5cx8x,"That wouldn’t work, avg(math+hist+scie) will give you the average total rather than the average of the 3. Because there’s one row per name, you’d just get the total of the 3 columns, eg the 'Matt' row would have avg(math+hist+scien) as 240.",3.0
g7rgwe9,j5cx8x,"AVG aggregates across rows, not columns. Unless the students appear in the list more than once this will just return the total of their three scores.",2.0
g7rhr0a,j5cx8x,mysql,1.0
g7ricsz,j5cx8x,"Having never used MySQL before, it looks like you can't use UNPIVOT or APPLY operators, so the only way to transpose the data is to use UNION.

[Here's how I just did it. Assuming db fiddle is actually correct haha.](https://dbfiddle.uk/?rdbms=mysql_8.0&amp;fiddle=937ff97ce4157bdb2e5103278e9472c0)

This reason this may have seemed confusing is because the instructor is creating an artificial constraint on the problem by telling you that you can't use the very obvious solution, in order to test your skills either on data transposition or aggregate functions, instead of creating a problem that would more obviously call for these skills.",2.0
g7rkat9,j5cx8x,"This ran correctly in data studio thank you very much.

&amp;#x200B;

Why does the g prefix mean ? [g.name](https://g.name) 

What does ""Grades g"" mean? (as in why is the letter g there?)

What does the prefix un in [un.name](https://un.name) mean?

that is the last hurdle for me to fully understand",1.0
g7rm298,j5cx8x,"It's a qualifier since he's working with two different tables here (grades and unpivot). Basically he's telling the query in which table a specific column is: g.name is in the grades g table

He could also write grades.name but that's a bit lengthy so he gives the grades table the ""alternative name"" g in the FROM clause

Same with unpivot that got named un. So every column that has un. in front comes from the unpivot table.

I'm not sure it's necessary here but you might have to use it if you want to self-join a table.

For example:If you have a table with employee id, last name and manager id and you want to display the last name of the manager as well

    SELECT e.employee_id, e.last_name, e.manager_id, m.last_name
    SELECT employees e JOIN employees m
    ON (e.manager_id = m.employee_id)",2.0
g7roftt,j5cx8x,"Thank you for your time in explaining this, I finally understood it.",1.0
g7skq99,j5cx8x,It's called an Alias btw.,1.0
g7t8ybc,j5cx8x,TIL THANKS,1.0
g7t4pgt,j5cx8x,"/u/SQLassie is correct, it's not necessary here. I just consider it a best practice and pretty much always do this.

You can also format it like this:

    FROM Grades AS g

Either way is acceptable. You're creating an ""alias"". Similar to how you create a column alias

    SELECT g.name, 'math' as Class, g.math as Grade

In addition to the self-join use case, there might be a time when you JOIN two tables that both have a column with the same name (every table in my prod environment has an 'iden' column, for example). If I just typed

    SELECT iden, col1, col2,...
    FROM table1
        JOIN table2
          ON key1 = key2;

I'd get an error for an ""ambiguous column name"". Which 'iden' column? SQL can't tell.

Instead you need:

    SELECT t1.iden, t1.col1, t2.col2,...
    FROM table1 t1
        JOIN table2 t2
          ON t1.key1 = t2.key2;

Keep in mind, you only technically *have* to use the ""t1."" on columns that would be ambiguous, but I found I avoid problems in the long run by just doing it all the time.",1.0
g7t9i2z,j5cx8x,"Wow excellent explication, thanks!",2.0
g7rhgip,j5cx8x,"An unpivot seems to be the way you could do this? With dynamic sql if you want to account for the possibility of more columns being added in there future, but probably not needed for the question.
Unpivot to get a class and a result column, and then use the group by and having to filter on average &gt;70",1.0
g7rjgt3,j5cx8x,i guess they want you to use UNPIVOT to turn class names into a single column then regular AVG and HAVING. was unpivot covered in the class?,1.0
g7rjs2i,j5cx8x,Can unpivot be used in mysql?,1.0
g7s6ltu,j5cx8x,apparently no,1.0
g7rxe7h,j5cx8x,Cross apply.,1.0
g7riosi,j5cx8x,For all your future exams please make sure **the first thing you do is read the whole exam paper front to back** underlining in the question the key parts the question is asking you. This gets your brain thinking about what it needs to remember and allows you to identify the easy questions.,-2.0
g7rjnjg,j5cx8x,Can't read it all because it was a timed online test and no way to return to questions,3.0
g7rsl00,j5cx8x,"bloody hell, that's harsh",3.0
g7rllti,j5cx8x,"Couldn't this just be solved with the following:

    select 
    	Name,
    	(Math+Hist+Scien)/3 as 'FINAL GRADE'
    from Grades
    where (Math+Hist+Scien)/3 &gt; 70
        

Not the most fancy way but would solve the problem, or is there something I missed?",-1.0
g7rm8wz,j5cx8x,"This would be the most simple solution but it wasn't allowed in the test.

&gt;You cannot use (math + hist + scien)/3  to get the average.",2.0
g7ro99g,j5cx8x,"You didn't read all the instructions, (math+hist +scien) /3 is not allowed.

The point is if this was scaled into a large database, that formula isn't feasible.",1.0
g7rr2p2,j5cx8x,"&gt; (math+hist +scien) /3 is not allowed.

Well. That explains what I missed, and it makes sense. But where are the complete instructions? These are the ones at the top. I feel very stupid at the moment :)

&gt; This is a table calles GRADES which stores the grade of each student in a subject.
&gt; 
&gt; Show a query which results in the name of the student and the average for the 3 classes
&gt; 
&gt; BUT ONLY showing students whose average is more than 70.
&gt; 
&gt; Name the column for average for the 3 classes AS 'FINAL GRADE'",2.0
g7s3tu5,j5cx8x,"[https://imgur.com/a/db30C4a](https://imgur.com/a/db30C4a)

&amp;#x200B;

here is a screenshot of my post, maybe it doesnt show like this for all?",2.0
g7sdpt5,j5cx8x,That line starts with `# ` and does not render on old reddit.,5.0
g7seyi5,j5cx8x,I use reddit is fun mostly.,1.0
g7srmi2,j5cx8x,Thank you for this information! That would explain it.,1.0
g7t5jn1,j5cx8x,"[It renders for me.](https://i.imgur.com/dA75FX5.png)

I'm using Chrome on OS X 10.11, if that matters.",1.0
g7s2mge,j5cx8x,"That stipulation is not anywhere in this thread except for what I'm replying to right here.

If you're asking for help, you're doing everyone a disservice if you're not including all the instructions/requirements.",-2.0
g7s3p8y,j5cx8x,"[https://imgur.com/a/db30C4a](https://imgur.com/a/db30C4a)

&amp;#x200B;

dude here is a screenshot of my post , i highlighted the instruction with an arrow",3.0
g7sku8c,j5b0c0,I believe the WHERE is executed before the SELECT so yes.,2.0
g7uvpmw,j5b0c0,"Of course, thanks for reminder and conformation",1.0
g7qw1kh,j5ai4r,"I know a solution, but I'll just provide a hint.

As an intermediary step, try getting a result set that's just 1 through n, where n is the length of the input str.

So for the example you gave (19 characters), try to get:

     i  
    ----
      1
      2
      3
      4
      5
      6
      7
      8
      9
     10
     11
     12
     13
     14
     15
     16
     17
     18
     19

Then once you have that, consider how that might be useful...",4.0
g7r9aog,j5ai4r,Yes absolutely,2.0
g7r7iev,j5ai4r,"One of the first things I try to learn when I work with a new RDBMS is how to generate N rows using a with clause (aka CTE).

For example in Oracle SQL I can say

    select rn from (select rownum as rn from dual connect by level &lt;= 100) ;

to generate 100 rows.

&amp;#x200B;

In DB2 (and I believe also in MS SQLServer) you can do this:

    with my_rows (rn) as
      (select 1 as rn from sysibm.sysdummy1 a
       union all
       select b.rn + 1 as rn from my_rows b where b.rn &lt; 100)
    select * from my_rows ;

Once you know how to generate X number of rows, your problem is solved. In your case X is the length of the input string, and with a substring function you can pull out the appropriate character for row N.",2.0
g7res8h,j5ai4r,However there is one issue in order to use cte i need a table which i dont have is it possible to use cte with variable?,1.0
g7sant1,j5ai4r,"In SQL Server you can do a select without a table, like
select 'my string' ;",1.0
g7tjxlt,j5ai4r,"2 ways to do this without a build in string split function:

[The straightforward way using a WHILE loop](https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=7ababcb7ff08f6d315954feb6cbc43ca)

and the [fun way using a recursive CTE](https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=0948da633d14f725f046ab702746ec31).

Why would one choose a CTE for this when a while loop is so obvious? So you could [apply the logic to multiple table rows at once, if need be.](https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=fdbbec2466df4f166b38daef292f7c8e) I'm currently working on something that needs exactly this functionality, and this makes more sense to me than having 2 (or more!) nested WHILE loops.",2.0
g7tl3av,j5ai4r,Yep you’re absolutely right,1.0
g7qsr1s,j5ai4r,"Is this a ""puzzle"" or just your homework?

Either way, if you want help, at least show us some of the things you've tried.",3.0
g7qt12i,j5ai4r,"It’s puzzle , i tried string functions start end didnt get the amswer and for new line i saw char(13) and char(10) they give new line for each character and the issue is im having problem is alternative to substring which start at position 1 till end of a given character",0.0
g7r1to8,j5ai4r,I would probably try an unnest(split()) in Dremel/GoogleSQL,1.0
g7r2g1t,j5ai4r,It’s ms sql,1.0
g7r2sw0,j5ai4r,"There are usually ways to approximate the same functions if they don’t exist natively; hence they might have them in MS SQL but I couldn’t say for sure.

Edit:

Looks like STRING_SPLIT would do the trick without UNNEST


https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-ver15",1.0
g7r3e08,j5ai4r,"Im using 2019 sql and string_split didnt work select * from string_split('thisisyourstring',1) as result",1.0
g7r3trn,j5ai4r,"I would try (‘this is your string’, ‘’)",1.0
g7r3bxx,j5ai4r,"Looks like python script, less sql. The python library ""Pyodbc "" can be used to access sql. But whatever.",1.0
g7v1xvm,j5ai4r,"declare @string varchar(100) = 'This is a string'

;with CTE as

(

Select SUBSTRING(@string, 1,1) string, 1 as Position

UNION ALL

Select SUBSTRING(@string, Position+1, 1) ha , Position+1

from CTE 

where Position &lt; Len(@string)

)

Select string from CTE",1.0
g7w77u1,j5ai4r,"It's been a little while, so I will post the solution I hinted at here: [https://www.reddit.com/r/SQL/comments/j5ai4r/hey\_guys\_can\_anyone\_help\_me\_with\_this\_puzzle/g7qw1kh/?utm\_source=reddit&amp;utm\_medium=web2x&amp;context=3](https://www.reddit.com/r/SQL/comments/j5ai4r/hey_guys_can_anyone_help_me_with_this_puzzle/g7qw1kh/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3)

    WITH input AS (
    SELECT 'This is your string' AS str
    )
    SELECT substring(str, i, 1) FROM input, generate_series(1, length(str)) AS i;
    substring 
    -----------
    T
    h
    i
    s
    
    i
    s
    
    y
    o
    u
    r
    
    s
    t
    r
    i
    n
    g
    (19 rows)

I don't have MS SQL Server handy, but this works on Postgres. Easy peasy. However on MS, you have to replace generate\_series() with whatever is the best method to generate numbers from m to n on that database server. Last I heard (I don't follow SQL Server too closely so I may be out of date), you would need a ""numbers table"" to generate the numbers. [https://www.mssqltips.com/sqlservertip/4176/the-sql-server-numbers-table-explained--part-1/](https://www.mssqltips.com/sqlservertip/4176/the-sql-server-numbers-table-explained--part-1/) \- so assuming that's still the best way to generate numbers on SQL Server, you would need to replace generate\_series() with a query that selects 1 through length of the string from the numbers table.",1.0
g7qtp78,j5ai4r,"    VALUES ('T'),('h'),('i'),('s'),(''),('i'),('s'),(''),('y'),('o'),('u'),('r'),(''),('s'),('t'),('r'),('i'),('n'),('g')

Works in MySQL, PostgreSQL, SQLite. For MS SQL Server you need to wrap it in:

    SELECT * FROM (VALUES …) AS t (s)",1.0
g7qrqbj,j5aep3,What the fuck is this.,12.0
g7r6jlp,j5aep3,You have data stored in a database.  You need to read that data.  Or write new data.  Or create/modify the tables in the database.  SQL is how you do all those things.,3.0
g7qu3st,j5aep3,"Wait until the time comes. 

When you know, you know.",2.0
g7r5rvv,j5aep3,"In the off chance this is a serious question, watch this short [vid](https://www.youtube.com/watch?v=27axs9dO7AE&amp;ab_channel=DanielleTh%C3%A9)",1.0
g7qqzx8,j5a75u,"Did you try round(travel_price,2) ?",2.0
g7qsl8l,j5a75u,"(Travel_price,2) is the first one I tried. Results are:
5004	3.9637
5006	1.4610
5007	0.6174
5008	6.2800

So those are how the results are suppose to look like minus the 2 digits on the end. But I have failed to produce those results. U/wolf2600 suggest to look up the documentation on how round() is suppose to be. I have done that, and that is how I got this far. Unless I missed something.",1.0
g7qwwi2,j5a75u,I think you use the round function fine Maybe you should round the result of the calculation rather than one of the part - or round both part of the calculation,2.0
g7qreww,j5a75u,"Google: ""sql round function"".

Read the documentation on how round() is supposed to be used.

Always read the documentation.",1.0
g7quosb,j5a75u,"So the problem wants me to use the round function. However, I just need to eliminate extra digits. I need to convert it to a money format. So why does the problem want me to use the round function if I do not wanted rounded to a different number. I just can not wrap my head around this issue.",1.0
g7qv6no,j5a75u,"Convert your data type to decimal (5,2) to limit to two digits.",3.0
g7r7vmq,j5a75u,I am currently looking into how to convert data types. Cast and Convert do not seem to be working in this assignment however. I am very new to this. Thanks for your help.,1.0
g7r9kwt,j5a75u,"It also could be that it’s your multiplier that is affecting the outcome of your discount. Are you trying to achieve a 10% discount? 

Very glad to try and help. Every day I learn new and amazing things. I work with a small group on SQL dB management and querying, and the beauty of that is seeing how different minds will compose different queries to achieve the same results.",1.0
g7r1alz,j5a75u,"Did you do like I said and google ""sql round function"", then read the documentation on the proper use of that function?

Hint: your syntax is wrong.  Read the damn documentation.",2.0
g7r7jdp,j5a75u,"I have read the damn documentation. So this is what I have tried.

 ROUND ( numeric\_expression , length \[ ,function \] )  

This does not work by the way. Round(TRAVEL\_PRICE,2, 2)  if I add a comma and any number after I just get an error. ERROR 1582 at line 1: incorrect count in the call to native function ROUND. For some reason the quiz will not let my input a length. As far as the numeric\_expression, could that just be TRAVEL\_PRICE? or does it have to be an actual number, and if that is the case I have to set each actual number individually?

Only one that does work is  

SELECT TRAVEL\_ID, TRAVEL\_DISCOUNT \* ROUND(TRAVEL\_PRICE,2) AS DISCOUNT\_AMOUNT

This is my very first assignment, so I have no clue what I am doing. Ive searched on google and have gone through 4 other problems and figured them out with out asking for help. I am at my wits end, as I have spent 8 hours over the span of two days trying to do this one problem. I am starting to think the web based quiz is shit and will not accept the proper answer. I appreciate all the help but don't assume I am on here just looking for answers. I am struggling enough with my other classes and was just asking for some help, I did look at SQL sites over and over again.",1.0
g825qnd,j5a75u,"&gt; ROUND(TRAVEL_PRICE,2)

Exactly.  Length is the number of decimal places to round to.  TRAVEL_PRICE is the numeric expression.  Don't know what 'function' is, but its in square brackets, meaning it is optional.

If you want to round price *discount, that's also a numeric expression, so just stick it in instead of TRAVEL_PRICE",1.0
g7sly8j,j54ale,"Try

 SELECT DBA.IXPAR     ""Parent Item""

,         DBA.IXCHI     ""Child Item"" 

FROM   DEV.DBA

JOIN         DEV.IM ON  DBA.IXPAR = IM.ITEM ;",2.0
g7sq464,j54ale,"Awesome, thank you so much! And how would you add a second Join to get just Child Items, that are also in the Item Master?
So basically a filter of BOM Parent and BOM Child on the Item Master",1.0
g7swe3k,j54ale,"&gt;SELECT DBA.IXPAR     ""Parent Item""  
&gt;  
&gt;,         DBA.IXCHI     ""Child Item""  
&gt;  
&gt;FROM   DEV.DBA  
&gt;  
&gt;JOIN         DEV.IM P  
&gt;  
&gt;ON  DBA.IXPAR = P.ITEM  
&gt;  
&gt; JOIN         [DEV.IM](https://DEV.IM) C  
&gt;  
&gt;ON  DBA.IXCHI = C.ITEM

;",1.0
g7r2kxc,j54ale,Are you trying to get all bom items regardless of an item existing in the item list? In not sure what youre trying to filter.,1.0
g7rcxqr,j54ale,"Thanks for your reply!

I need all elements in DBA.IXPAR that are also listed in IM.ITEM

So all items of my item master should be in the BOM Parent column",1.0
g7sr1gs,j54ale,"maybe i haven't had enough coffee yet, but I don't follow. Could you throw up a quick example of the source tables and your desired output?",1.0
g7otevj,j50uz4,"It depends a bit. If there is just the one query, then yes, you will need to use the original column name in the where, group by, etc clauses.

But if the alias is defined as part of a subquery, when you reference the column in the main query you will need to use the alias.",4.0
g7otyn4,j50uz4,"It all comes down to the order of execution. You can't use aliases in the where and group by clauses because they execute before the select. You can use aliases in the order by though.

Order of execution: [https://sqlbolt.com/lesson/select\_queries\_order\_of\_execution](https://sqlbolt.com/lesson/select_queries_order_of_execution)",5.0
g7oufkq,j50uz4,Gracious and for that site! that is very helpfull. Have a great day!,2.0
g7p8xu5,j50uz4,I guess you can use the alias in the order by section,1.0
g7pnhix,j50uz4,"Generally yes.  If you try to aggregate, group, or sort, it might work but it probably won't.  I had a query where using the alias works in the order by, but not if I try to put a second column in the order by.

Your assumption is good - don't reference the alias in the same query and you'll be safe.

However if you wrap that query into a CTE, or it is a sub query, you use the alias.  I sometimes layer in a CTE for readability alone.",1.0
g7oxf6a,j50q55,"The term you're looking for is ""operator precedence"". It should be noted that this isn't just SQL, but (almost) all programming languages.

[This StackOverflow answer](https://hsm.stackexchange.com/a/2287) goes into it and links to a paper discussing some history around mathematical operator precedence (which boolean operator precedence seems to stem from).

TL;DR: boolean follows math, AND matches with multiply, OR matches with addition",32.0
g7osdj7,j50q55,"You don’t say what system you’re using but for MS SQL Server this page documents the order explicitly: https://docs.microsoft.com/en-us/sql/t-sql/language-elements/operator-precedence-transact-sql?view=sql-server-ver15

If you need an explicit order of operations you really should use brackets to make it explicit. Even if it works for you without brackets because of the order of precedence, you should make it clear with brackets for ease of future maintenance.",18.0
g7qde0h,j50q55,"You should really use the brackets no matter what to make it readable in my opinion.  Don't make the next guy guess at the way it is broken up, even if he knows the rules of how it works.",2.0
g7otsjv,j50q55,"&gt; You don’t say what system you’re using 

~all~ SQL databases evaluate ANDs before ORs",2.0
g7ovw2q,j50q55,"Fine, but it’s not really feasible to provide links to every single database product’s documentation, and a link to the SQL standard probably isn’t as helpful or clear.",1.0
g7oy5fm,j50q55,or you could just reassure OP that all SQL databases evaluate ANDs before ORs because SQL is based on Boolean logic which is used in practically every programming language ever,5.0
g7ossdc,j50q55,Thank you!,1.0
g7r7eno,j50q55,Aren't they called parentheses?,1.0
g7pj5zj,j50q55,"In logic, and making an analogy to arithmetic, is the equivalent to a multiplication and a sum, in the sense that the distributive laws apply in the same way:

p and (q or r) = (p and q) or (p and r)

p * (q + r) = (p * q) + (p * r)

If you have:

7 * 4 + 2

 you need an explicit way to get the tree of instructions. It can go:

- (7 * 4) + 2 = 28 + 2 = 30
- 7 * (4 + 2) = 7 * 6 = 42

The convention, for historical reasons, is that multiplication goes first.

And then it went into boolean logic.

One of the ""historical reasons"" is that, if you have:

- 4 + 2 = 1 * 4 + 2
And the obvious result is that  4+2= 6, so the multiplication by one has to go first.

Fun fact: If you have 3^0 = 1 * 1 * (3^0 ) = 1 * 1 * (three, zero times , so we just don't put it) = 1 * 1 = 1",5.0
g7p9u6j,j50q55,"As a general rule of thumb, always use the brackets if you're mixing and/or.  This gives you a visual queue to the processing order and if you do run into an engine that does not process in the order you expect, it won't matter.

For example, I do a fair bit of boolean logic in my sql, and until seeing this thread hadn't realized that sql processes and first because I alway bracket, evem if I'm looking for (a and b) or c.

If it's extra complex ill also add new lines and indents, but that's pretty rare.",3.0
g7pgcc5,j50q55,"Learned this in my sophomore predicate logic class when pursuing a computer science class.

Think about how the logic would execute of OR came before AND. Draw out some decision trees. It should be pretty obvious.",3.0
g7qndh5,j50q55,"Because math. 

AND is analogous to multiplication 

1*1 =1

True AND True is True

1*0=0

True AND False is False

0*0=0

False AND False is False

OR is analogous to addition, but with only one digit so you can’t go higher than 1, so

1+1 =1

True OR True is True

1+0=1

True OR False is True

0+0=0

False OR False is False

And we all know PEMDAS, so multiplication before addition, therefore AND before OR",2.0
g7p2nil,j50q55,"This is defined as such in Linear Algebra. And because OR is exclusive. Meaning that if you aggregate a group with AND, all expressions evaluated afterward are OR exceptions.",3.0
g7otvzq,j50q55,"&gt; is there any reason why AND is higher, in order of evaluation than OR?

yes, it's the same reason why in arithmetic, multiplication and division have higher precedence than addition and subtraction",4.0
g7osfrn,j50q55,Because it does not go left to right.  It is a arbitrary rule more or less,2.0
g7q3o29,j50q55,"Because it makes sense:

If x and y OR, if a and b, then...

If x or y and z, then...

The ""and"" joins them semantically in our heads, we have to go out of our way linguistically to specify ""if x or y, and if z, then...""",1.0
g7os1qr,j50kbr,"https://www.mssqltips.com/sqlservertutorial/162/sql-server-stored-procedure-with-parameters/

https://www.reddit.com/r/learnpython/comments/9o94ez/formatting_code/",2.0
g7os78f,j50kbr,thank you,1.0
g7nncfr,j4vvoy,"You’ve not given anywhere near enough info to be able to answer this effectively.

However - You’re trying to select ‘m.id from plays’ in your subselect in the last line. That won’t work. m is an alias for movies (line 2), so how can you select m.id from plays?

It looks like there should be a movie_id column in plays. I guess that’s what you want in the subselect - ‘select movie_id from plays’. 

I don’t know why you’d want the join in that case, unless you explicitly need some info from plays.",1.0
g7o7rqs,j4vvoy,"Select * from movies where m_id not in

(Select m_id from plays) ;

Or something similar, not sure how your scheme looks like",1.0
g7olks8,j4vvoy,"&gt;  Do you have any idea how I can fix this? 

i sure do

your FROM clause brings back all movies which have matching plays

then your WHERE clause says you only want movies from the FROM clause which have no matching plays

vwalah!  there's your problem!",1.0
g7ou9ir,j4vvoy,ie left join,1.0
g7ounr7,j4vvoy,"actually, gotta choose between left join or not exists subquery

can't do both",0.0
g8j1n7c,j4vvoy,"You can do both, because the not exists includes everything in the left joined table but not in the other tbl",1.0
g8j58dy,j4vvoy,"i don't think you quite get it

first. let's fix the syntax error in the OP's subquery

    where m.id NOT IN (select m.id from plays)

the subquery has no `m` table, so what the subquery should actually look like is

    where m.id NOT IN (select movie_id from plays)

agree so far?

okay, so the main query will return only rows from `movies` that *don't* have a matching row in `plays`

now tell me why you want to outer join to `plays` in the main query?

none of them will match!",1.0
g7nfusl,j4vvoy,"Having no idea what your schema looks like, this is what I can offer;

&gt;SELECT \*  
&gt;  
&gt;FROM Movies M1  
&gt;  
&gt;WHERE NOT EXISTS  
&gt;  
&gt;(  
&gt;  
&gt;SELECT \*  
&gt;  
&gt;FROM Movies M2  
&gt;  
&gt;WHERE [M2.id](https://M2.id) = [M1.id](https://M1.id)  
&gt;  
&gt;AND EXISTS  
&gt;  
&gt;(

      SELECT \*  
    
      FROM Plays	P1  
    
      WHERE M2.charId = P1.charId  

&gt;)  
&gt;  
&gt;)  
  
&gt;  
&gt;edit -i'm sorry, i've tried a couple of things and I cant get this ridiculous editor to render the code as intended",0.0
g7ollin,j4vvoy,"&gt; i've tried a couple of things and I cant get this ridiculous editor to render the code as intended

place four spaces at the front of each line of code",2.0
g7on3mi,j4vvoy,"Or for short pieces of code, \`surround it like this\` to get `inline code` like this

Annoyingly Reddit doesn't use ""code fencing"" syntax like most markdown does, which makes it much easier to add blocks of code without adding spaces in front of every line, eg

    ````
    long
    block
    of
    code
    here
    ````",0.0
g7ouh9s,j4vvoy,"i use a text editor when writing code

one of its features is ""group shift"" (not sure what it's really called) whereby you can place the cursor at a spot, say in front of the first column on a row, then shift/arrowdown to highlight a number of rows, and then space-space-space-space to insert spaces on every highlighted row

then ctrl-a ctrl-c, alt-tab back to reddit, ctrl-v and bob's your uncle

the best part about ctrl-a, ctrl-c, alt-tab, ctrl-v is that they're all done by the left hand alone, while the right hand stays on the mouse",1.0
g7ov3e5,j4vvoy,"Yeah it works if you’re copying code from a code editor, but it’s not ideal when throwing together a quick example on mobile or something",0.0
g7o9h3f,j4vvoy,"A few things

1. Your subquery should throw an error because 'm.id' is not in the table plays.

2. You have 'from movies m join plays'..... what kind of join do you intend to use there? **ALWAYS** specify the one you want. If nothing else, it will help the next person understand what you're trying to do.

3. If that JOIN is defaulting to an INNER, you won't get any results because only the records where the ID is in both tables will be returned. You're going to want either LEFT or FULL OUTER. 

4. You don't need to do a subquery here. You can just filter for rows where m.id is null. Either way will work, but looking for NULLs will be more efficient.

I'm not going to rewrite the query for you because this kind of smells like homework. But the error(s) here are pretty minor.

Lastly, in the *movies* table the ID column is called *id*, but in the *plays* table, the foreign key is called *movie_id*. That's OK for tiny db's, but when you get into real world sized schemas, it's better to use the full name everywhere, including the parent table. So in the *movies* table the column would be named *movie_id*. The computer doesn't care, but human beings reading the code, or writing queries and trying to figure out what relates to what, will greatly appreciate it... the less ambiguity and head scratching the better.",0.0
g7ov75p,j4vvoy,"a straight Join to Plays isn't going to work, because it's matching on movie_id value, but that is what you don't want to exist, right?

Get rid of the Join.  Use a correlated subquery instead, such as:

      Where Not Exists
      (select p.movie_id from plays p
         where p.movie_id = m.id);",0.0
g7pjx3v,j4vvoy,"Your IN list might contain NULL value , put a filter there might help",0.0
g7midv0,j4sbw3,"You could perhaps combine Employee and User accounts tables, since they both seem to be of internal-function, and differentiate them with a role.",1.0
g7mq1wh,j4sbw3,"If the employee and customer share the same columns as a user, why not just add those to the employee and customer tables. It's unnecessary complexity, imo.",1.0
g7nmml3,j4sbw3,most likely I'll  be adding columns that they don't share. I just added the minimal just to show real quick.,1.0
g7myixd,j4sbw3,"when a new account created, you have to add new records into 2 tables, one for user table and the employee table. The user table acts as a generic table for all users in a system (employee and customer in this case) if I understand correctly.",1.0
g7nmi59,j4sbw3,correct,1.0
g7oia67,j4sbw3,[deleted],1.0
g7phf9z,j4sbw3,"I see what your trying to do. But don't you think its good to have a customer? I know in the image the customer and employee have similar columns but it'll definitely grow. I just added the minimal for time sake. 

Also I would think employees to brand is a many to one relationship. An employee who works for x brand isn't going to have another job working for y brand.",1.0
g7mi68v,j4qslq,"How about an Employee table, a Company table, and an ""Assignment"" table that links an employee to a company. I'd also put a ""Primary"" field and store salary info in the Assignment table. If the salary can change, you might want to have another table for salaries and link those salaries to the assignments.",1.0
g7mnikv,j4qslq,"Okay, I got Employee entity, and Company entity. Both would have their own Id, employee Id, and company Id. Assignment entity would have employee Id assigned to company Id, and assignment would have its own Id. Salary would have an assignment Id, and attributes would be hrs worked and hourly wage. Would that make sense so far? Currently thinking how I’d also add benefits and deductions as a separate entity as well.",1.0
g7p0nhp,j4qslq,"I'd set it up as follows:

You have a situation where you have an end client that needs some work doing (an assignment), and a vendor that can supply people (permanent or contract) to work on that assignment. The permanent employees will have a contract of employment (which I'm going to call EmployeeTerms) which states salaries, benefits and work hours. The contractors may have their own contracts which I'll call ContractorTerms. The vendor may have fixed salary bands and benefits for employees so you could have an EmploymentPackage table that lists all those. Or the employees and contractors could negotiate their own terms, in which case the Terms tables would list the details separetely for each person.

So I'd have an EndClients table, a Vendors table, a Employees table, a Contractors table (which you might be able to combine into a Workforce table, depending on how many common fields there are between the two), and then an EmployeeTerms table and a ContractorTerms table, and maybe an EmployemntPackage table as well.",1.0
g7leodr,j4q429,That's not a valid single quotation. Just manually replace them and see what happens.,3.0
g7nhhst,j4q429,Thanks for getting back to me. Still doesn't play ball.,1.0
g7livv8,j4q429,"Do not copy sql code from MS office applications.. they have a ton of autoformatting (smart quotes, double hyphen, etc) which will create all sorts of problems.

In your example, some of the quotes are actually backticks.  Backticks and single quotes are not interchangeable.",3.0
g7nm31x,j4q429,Thanks for getting back to me. I'll take a look at this.,1.0
g7o8k5v,j4q429,"In Excel, if you copy your working code in one cell and your non-working code in one cell then use the formula =CODE you will get the character number for the first character of each. You can then use a formula to break down your codes one character each and you can start to see where the code differences lie.",1.0
g7p6mlp,j4q429,That's genius! Thanks very much for the help!,1.0
g7laysg,j4q0i6,"You need to use the 'having' clause it sounds like. Can you post what you have?

should be something like:

your query + having count(tag) = MAX(COUNT(tag))",0.0
g7lborz,j4q0i6,"I have: 
SELECT p.date,COUNT(t.tag)
FROM posts p, tags t
WHERE p. pid = t. pid
GROUP BY p. date, t. tag;

I'm not sure if that is they correct join to do but it's my best guess 😬",1.0
g7lcpw0,j4q0i6,"I generally write my joins with an actual join statement but I assume that statement is working as is. I think you would need to mess around with the query but something like this:

SELECT p.date,COUNT(t.tag) FROM posts p, tags t WHERE p. pid = t. pid GROUP BY p. date, t. tag HAVING COUNT(t.tag) = MAX(COUNT(t.tag));

&amp;#x200B;

Good source:

[https://www.w3schools.com/sql/sql\_having.asp](https://www.w3schools.com/sql/sql_having.asp)",1.0
g7ld1mw,j4q0i6,"Yes thank you, but MAX(COUNT(t. tag)) is giving me an error. Is there another way of finding that max?",1.0
g7leobf,j4q0i6,"maybe something like this: 

SELECT [p.date](https://p.date),COUNT(t.tag),MAX(t.tag) as max\_tags FROM posts p, tags t WHERE p. pid = t. pid GROUP BY p. date, t. tag HAVING COUNT(t.tag) = max\_tags;",1.0
g7mlqeu,j4q0i6,no... `MAX(t.tag)` gives the tag with the highest value,1.0
g7jzjjz,j4lcxz,"I use INFORMATION_SCHEMA queries a lot (or SQL Search); I use EXEC sp_help TableName for individual tables, I sometimes write dynamic sql to scan multiple tables if I'm trying to understand something very specific.",27.0
g7k0d3i,j4lcxz,Can you expand on that more and provide some steps/details?,3.0
g7ko8no,j4lcxz,"EXEC sp_help YourTableName provides information about that table, including a list of all the column data definitions and constraints. 

You can use the sys schema or information_ schema to query database structure information/ metadata. To do this you just write regular queries, except they're against the metadata. The easiest place to start is SELECT * FROM YourDB.INFORMATION_SCHEMA.COLUMNS WHERE COLUMN_NAME LIKE '%interestingthing%'. Google for more specifics, but basically everything structural is in there. You can find all the constraints, extract a list of all the stored procs,, get user info, etc. RedGate's SQL Search does the same thing, but with a gui. But if you force yourself to write the queries you'll learn a lot.

Then down the line you might want to write dynamic sql if you want to combine a metadata query with actual column content query in one go.  I don't do this often, but when I do it's to return column lists where I've done some kind of check on the data contents (like checking for null columns, or passing a list of IDs).",16.0
g7noaj0,j4lcxz,"EXEC sp_help YourTableName provides information about that table, including a list of all the column data definitions and constraints.


Does the above include the SP and views from a table?",1.0
g7ook9t,j4lcxz,"No, it's just the base table, not anything built on top of it. But I'm pretty sure the code to generate sps is present as a text string somewhere in the sys schema, so you should be able to wildcard search there. However, at that point SQL Search is probably a lot easier.",1.0
g7oxyt1,j4lcxz,"&gt; But I'm pretty sure the code to generate sps is present as a text string somewhere in the sys schema

yeah it's the ""definition"" field in sys.sql_modules. But I've been in roles where I haven't had permissions to see see that column for some reason!",2.0
g7js06d,j4lcxz,Redgate makes a SMSS plugin called SQL search that allows you to search across all items in a db.,10.0
g7o35ul,j4lcxz,"100 times yes. I could never do my job without SQLSearch. I manage about 50 applications, which involve hundreds of thousands of tables, procs, views, etc. Sometimes I need to find any instance of &lt;whatever&gt; and replace it with &lt;something else&gt;. I could never do it without this tool.",1.0
g7k0xuw,j4lcxz,"Can you share or provide more info?

Is it open source and also for T-Sql MS?",1.0
g7k330t,j4lcxz,Google isn't that hard [https://www.red-gate.com/products/sql-development/sql-search/](https://www.red-gate.com/products/sql-development/sql-search/),10.0
g7k86ai,j4lcxz,"Thanks, but I cant see and dont think its open source.  

Is there a way to do that within SSMS?",0.0
g7kar3y,j4lcxz,why does it need to be open source?,6.0
g7nno6w,j4lcxz,"As its for for commercial use, the other software may require licences?",1.0
g7ltzko,j4lcxz,So it's free. Of course.... Sql search is free anyway.,1.0
g7lwg7u,j4lcxz,"right, which is why i was asking the question because I knew it was free so if there is no other reason..",1.0
g7n9x6a,j4lcxz,"A lot of the tools related to SQL and ETL, and especially MS SQL Server are proprietary and costly. It's the one area of software development that just doesn't have a suite of free (gratis) open source options. This is why you've received a curt response from the other redditor about SQL tools being open source.

If you are using SQL Server then SQL Server Management Studio (SSMS) has a feature where you can right click on any table and show dependencies. This will show you all DB objects (tables, stored procs, functions, etc.) that use that table. It won't show you things like SSIS packages or external scripts that use the table in question but when you're going in cold to a database the dependency information can give you a general overview of how things hang together.",2.0
g7k89h6,j4lcxz,"First port of call should be ask people who work on the same DB!
Their knowledge will be invaluable.",9.0
g7jqr34,j4lcxz,"Having a full backend map for reference is very handy. Even better if you can make multiple maps that show only the tables touched by specific actions/transactions. Several GUI applications have decent wizards to help mapping, like SQLDeveloper, SQL Navigator or Toad.",6.0
g7k10xx,j4lcxz,How do you create that? a full backend map for reference is very handy,3.0
g7k2f5y,j4lcxz,Depends on what software you have at your disposal. There are a few open source programs out there just for database mapping,1.0
g7k8bc9,j4lcxz,"Which Open source and T-SQL Software?
I have T-Sql with the standard MS server management",1.0
g7jpw8b,j4lcxz,"Views are virtual tables, so they're easy enough to diagram. Some ERD tools will reverse-engineer them.

Procedures are more difficult of course. There are ways of representing them in diagrams, but these are less standardized.

If there are REFERENCES constraints defined, some ERD tools do a decent job of reverse-engineering these as well, and failing that, some can make inferences (which have to be reviewed by a human) about foreign-key relationships based on column names.",4.0
g7kbsii,j4lcxz,"Not sure of the context of your questions. Which perspective are you asking : 
1. DBA (managing DB)
2. application DBA(managing application) or 
3. analyst (trying to understand data) ? 

For 1, I would imagine you ask relevant people what the issue is and you can then look into them 
For 2, start of we a domain/ use case  that you are working on and as ask people who have used have used it to give you more insight. Start small and then grow your branch 
3. Apply 2, ask more people and grow your tree. 

If you cannot reverse engineer, create ERD as you go",3.0
g7kuqve,j4lcxz,I would recommend talking with people that use the tables. Ask a lot of questions and at least you’ll understand the most used tables. Guarantee most of the tables or procs aren’t even used or are relics of a time passed,3.0
g7kv5cw,j4lcxz,"Make use of the INFORMATION_SCHEMA views. Also a handy tip is to select a table name in your query and press alt+f1. Will bring up info on the table including the columns, their data types, constraints etc.",3.0
g7noirw,j4lcxz,"&gt; alt+f1
Thanks for that!  Does this also have info for the SP and Views?",1.0
g7kcyu2,j4lcxz,"I use MS SQL. I originally used addons that more senior developers recommended like RedGate's SQL Search. However as I became more experienced, I grew more comfortable figuring things out manually and querying the meta-tables like sys.tables and sys.columns for example. When I need to search across and within stored procedures for something specific, I might write something like: 

    select o.name, c.colid, c.text  
    from sys.objects as o  
    inner join sys.syscomments as c  
    on o.object_id = c.id  
    where o.type = 'p'  
    and o.name like '%haystack%'  
    and c.text like '%needle%';",2.0
g7kseo6,j4lcxz,Learn them 1 at a time. Just like any other thing you learn. Eat the elephant one bite at a time or however that saying goes.,2.0
g7l4m4g,j4lcxz,having the tables mapped out and or using the search functin (information_schema in sql server..prety sure every other db has something similar),2.0
g7l7jns,j4lcxz,"If it's a commercial application and you've got a support contract, most of the good ones have a data dictionary.",2.0
g7l7opp,j4lcxz,Look into db documentor (devart) and redgate sql search,2.0
g7lwvrc,j4lcxz,I don’t know the Sql server table with al the column names but get that into Excel. Then do a pivot table with columns as rows and tables as subs under those with a count and you will see how things work.,2.0
g7m25at,j4lcxz,I typically perform database traces while using the application.  And I’ll go through the source code as well. It can be agonizing as I click buttons in the application and watch the traces. If I have too I’ll decompile the source to make sure I am not missing any application logic. As you probably know documentation is typically old or wrong so yeah it can be grueling and fun and rewarding.,2.0
g7m9xd8,j4lcxz,"Think of it like moving to a new city and looking at the bus routes for the first time. It’s completely overwhelming, but you don’t need to memorize the whole thing. You just need to figure out the right route once you have a destination in mind. 

Once you start to learn the tables, build an ERD of your corner of the database and grow it from there.",2.0
g7noofi,j4lcxz,Thank you :) That is a great analogy! How do you advice in terms of building an ERD?,1.0
g7qezg5,j4lcxz,"The first time you query a corner of the database, map those tables. Lets say there is an items table. Do a describe on the table, and diagram each column. You'll probably know the primary key on it, and you'll know the foreign key that links it to other tables in your first query, so document that. 

As you continue to explore the database and learn new tables and links, build your map. Just keep maintaining it as a living document.",1.0
g7mtci6,j4lcxz,"you didnt state which DB.
but always use the information schema (PG) or all/user objects/tables (Oracle). every database has a way to query the objects themselves.
so build yourself queries that suit the nature of the DB. group them by subjects, size, FK/PK relationships, etc...

its also important when building a DB to design objects in such a way that their names can indicate a lot about what they are.  so you can just query all objects with %CUSTOMER% and you get all functions/views/tables/indexes ...related to customers.",2.0
g7kg7nn,j4lcxz,"Entity relationship diagram

Extract the DDL for all the tables and put them in a text file. Then you can search for constraints, indexes, columns etc...

Stored procedures / views - same as for tables. Extract the source code and store it in a text file.

You can put the source code (DDL / views / stored procedures) in a source control system like Github or SVN.

As other posters have said, learn how to use the catalog views in your RDBMS to query data dictionary information.

What RDBMS are you using? Do you know how to extract DDL? If you don't know how try a client GUI tool like dBeaver.",3.0
g7nnyjd,j4lcxz,"MS SSMS. 

No, how do I extract the DDL?

And thats a good idea, if i copy and save them to a text file

Thanks",1.0
g7pnzdf,j4lcxz,"I used these google search terms ""sql server management studio extract ddl""

&amp;#x200B;

[https://docs.microsoft.com/en-us/sql/ssms/scripting/generate-scripts-sql-server-management-studio?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/ssms/scripting/generate-scripts-sql-server-management-studio?view=sql-server-ver15)",1.0
g7o5717,j4lcxz,"80/20 rule applies here. 80% of queries/updates etc will involve 20% of tables.

Not as overwhelming as it initially seems trust me. I’m more dev than dba but my personal approach is this (assuming this is a db for some kind of user app): 

-	Find the users table and query that
-	Look for foreign keys in users table and run some joins to what you think are the tables the fk joins to
-	Find tables with the user id column and run some joins to those
-	These tables will also have foreign keys. 
-	Repeat 


Very basic start but you’ll see a decent handful of tables. Also if you’re on sql server sys.tables and sys.columns are your friends here",1.0
g7ox4ip,j4lcxz,"I've just started a new job and the first thing they asked me to do to learn the system was to look at the SQL Server Agent jobs on a couple of the servers. I found some jobs that seemed to be connected to my job description so I started looking at those, writing documentation about the steps they took and the procedures they called. Then I looked at those procedures and listed which tables were affected (which ones were read from and which ones were written to). I wasn't looking at the actual data or what the procedures were doing, just the tables that were affected. Then I did the same with the views, and updated my documentation to say ""this proc reads this table directly, and it reads this one via this view""

So ultimately I've got a cross-referenced list on a series of html pages where I can click on a server and see which schemas and tables I need to be concerned with, I can click on a table or view and see which procedures use it, and I can click on a procedure and see which tables it uses. The next step is to dive a bit further in and see which fields are used by which procedures.

However I'm only concerned with one reporting system, and there are a lot more servers with a lot more databases on that aren't part of my remit at the moment. But you have to start small and work your way up.

You can right-click on a table in object explorer and choose ""view dependencies"" and that'll show you where the object is used, but it won't show you if the object is referenced by a procedure on another server, and it won't show you if it's referenced by dynamic sql.

You can export the definitions for the procedures, tables and views into a text file and then search that using a text editor, that's a good way of finding out where objects are used as well.",1.0
g7p6mv2,j4lcxz,"Hey firstly congrats
 and that sounds great how are you finding it and will keep that in mind",1.0
g7pct8y,j4lcxz,"Thanks, I'm enjoying it a lot, only been doing the job two or three weeks so it's still really early days. Working from home too, which I love!",1.0
g7k29sf,j4lcxz,"Slowly. From data dictionary information, review of existing packages and procedures, and most importantly understanding business logic.",1.0
g7hjsl6,j4a82r,"Hello u/DevDBA - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",2.0
g7ivhyv,j4a82r,MS SQL,1.0
g7ohaxv,j4a82r,"If you read the wiki there is good stuff there.

I keep a ref page for articles, tools, and educational resources that might be useful too : https://expressdb.io/sql-server-resources.html",2.0
g7h1ha8,j46qnd,Group by doesn’t order the results  that is what an ORDER BY does,1.0
g7h5quu,j46qnd,Oh wow! so it serves no purpose what so ever without an ORDER BY?,1.0
g7higom,j46qnd,"It serve a purpose just not for ordering - it groups like data together when used with an aggregate function.
Without an order by the data may come order based on the indexes on the table",1.0
g7h2mqc,j46qnd,"That the query shows it in order is just pure luck, at least if is ms msql.

In order to sort it you need to add the order by clause by default it is ascending put you can suplly the parameter desc  if needed.",1.0
g7icnk5,j46qnd,"additional to this comment, if you want to order by more than 1 column, you can set the order differently for each columns.   
For example: you want it order it ascending for the warehouse but descending for the products if there are 2 products in the same warehouse, or after the warehouse you want to order it descending based on quantity.",1.0
g7rntyk,j46qnd,"The first code you wrote will return an error, because you cant SELECT a column without having it in GROUP BY.  
GROUP BY wont order the output, ORDER BY does; GROUP BY only group data for aggegration purpose.

&amp;#x200B;

In this case, yes, the second GROUP BY and the GROUP BY GROUPING SETS will return the same output. But normally GROUPING SETS is used for more than 1 grouping set, 

&amp;#x200B;

e.g. GROUP BY GROUPING SETS(

(warehouse, product),

(warehouse),

(product)

)",1.0
g7gw15g,j46iml,Are you modifying the column definition the data?,1.0
g7gwik7,j46iml,I'm sorry if this sounds so introductory. Let's say I wanted to change my birth\_date column to birth\_year by extracting the year.,1.0
g7h2k2n,j46iml,"Try trunc(birth_date)

https://www.postgresql.org/docs/8.0/functions-datetime.html

Formatting 

https://www.postgresql.org/docs/8.0/functions-formatting.html

Note that by selecting you aren't actually changing anything. You are only returning data in the desired format you specify. To actually make another column you would need to add another column and then populate it with an sql update statement.",1.0
g7hsrre,j46iml,Please do not link to the manual of discontinued Postgres versions. You can replace `8.0` with `current` to avoid link rot.,1.0
g7h1v8l,j46iml,I don’t know postgre sql but other platform I use you can right click generate sql as select and will list all column in a select query from there you just modify the birthdate column,1.0
g7hdlvf,j45kpi,"Do you just want a list of stores showing daily, weekly, monthly, quarterly profits for just today's date? e.g. if today was 25th September, Q2, you'd see all the sales for 25th September, all for the week beginning 21st September (all last 7 days, depends what you mean by weekly), all since 1st September to 25th September and all since 1st July to 25th September.

Once you get your result, what is the end goal? Who is it for and how will they use the result?

Reason I ask is because if it's going to an application (let's say Excel) then you probably only want to show daily sales with columns identifying which day, week, month and quarter the sales are for so they can pivot the data themselves and just drag in the date fields they want at the time.",1.0
g7ign0o,j45kpi,"This is for a project in one of my satay warehouse classes. It is the only one I am stuck on. The way it sounds is I need a list for store 1 and store 2 that shows the total profit by day, total by week, total by quarter, and total by year. 
This would all be super easy if it was excel but we have to learn how to use OLAP through sql. That is the goal for this project.",1.0
g7icann,j45kpi,"Hi,

I'm a new joiner in reddit and want to start helping with their queries. English is not my native language, so please bear with me.

First, I assume that you already knew about the standard SQL queries like select, where, and group by.

For you to get the daily, weekly, monthly, quarterly, you can use the date\_trunc function to transform your ""date"" column to the start of date type you want and then start to group by them.

If you want to union all the data, you can use 4 CTEs: daily, weekly, monthly, quarterly. and have 4 columns in each of them: date, date\_type, store\_name, profit\_amount.

And then in the final query, you just need to union all the 4 CTEs.  
With that, when you put your data to your end product, you can do the filter there how you want to see the profit for each store.  


From my explanation above, if you don't have any idea, I suggest you need to google  
1. SQL Group by  
2. SQL date\_trunc  
3. SQL CTE  
4. SQL Union  


Hope it helps.",1.0
g7oop94,j45kpi,Thank you for replying and giving me this info. I will check it out.,1.0
g7k8tjm,j45kpi,"IMO, you’d be best off making use out of the dimdate table, I’m assuming that’s why it’s there. After your select statement your query should look something like this... 

FROM FastProductSales AS SALES 
Then you’d left join sales with stores 

LEFT JOIN dimStores AS Stores
ON SALES.StoreID = Stores.StoreID 

then left join SalesDateKey with DateKey from the dimDate table 
LEFT JOIN DimDate AS DATE ON Sales.SalesDateKey = Date.DateKey 

then do another left join with the DimDate table using today’s date. 

LEFT JOIN dimdate AS DATE2 ON CONVERT(DATE, GET DATE()) = DATE2.date 

Then in your select statement 
You’d write 
SUM(CASE 
WHEN DATE.Quarter = DATE2.Quarter
THEN Sales.Profit ELSE 0 
END) AS QuarterSales, 

Repeat for each aggregate you need. 

Then GROUP BY StoreID at the end. 

You might need to do a HAVING date.year = date2.year as well",1.0
g7oon56,j45kpi,That Makes sense. I will have to try and implement that. Thank you for the help.,1.0
g7i4kf0,j454cn,"Order by col desc
Limit 3",2.0
g7hjl0b,j454cn,"there is no function that does this

however, a query will

post your table description and i'll help you with the query",1.0
g7mcgxr,j454cn,I think you can use either rank or dense\_rank as per your need.,1.0
g7jbpnc,j43zpj,"What is the workload? Pretty much all RDBMSs will have some driver you can use with c++. Your main source of performance is always going to be more dependant on the code you’re running rather than the actual DB being used. Different DBs will handle complex things in different ways but if you’re just loading data and selecting without massive complexity then they’ll all perform extremely well (10,000s rows per second on old hardware, millions with modern hardware). Just make sure you follow standard best practises - use arrays, use bind variables, don’t start a new connection per DB interaction.",1.0
g7ghmc7,j43tr5,"&gt; But instead of 1 date as a parameter, I need to supply a date range to this, and still pull all applicable dates less than or equal to the dates supplied 

this isn't as complex as it seems

    WHERE sometable.date &lt;= @dateparameter1",1.0
g7gkfiq,j43tr5,"Right but in my case @dateparameter1 &lt;&gt; a single date, it’s a range. Like ‘01-Jan-20’ and ‘31-JUL-20’.

I guess I can have 2 statements like &lt;=‘01-JAN-20’ AND &lt;=‘31-JUL-20’",1.0
g7hjbd0,j43tr5,but if a date is less than 2020-01-01 then it **gots** to be less than 2020-07-31,1.0
g7i4wrt,j43tr5,"Yeah this, just pick the most recent date of your range and use that.  Your two requirements are overlapping",1.0
g7icm28,j43tr5,Right but I can’t have precious years. So if the parameters are between 31-DEC-19 and 31-JUL-20 then I pull all dates from 2019 and all dates from 2020 before JUL 31. If the parameters are between 29-FEB-20 and 31-JUL-20 then I can’t have 2019 dates in my return.,1.0
g7oo3jq,j43tr5,"I still dont understand your requirements, it feels like you're overcomplicating.

So, from what I understand, get range between 31-DEC-19 and 31-JUL-20 actually means get all dates before 31-DEC for 2019 and all dates before 31-JUL-20 for year 2020?

Again, your requirements are overlapping.

Just get the latest date from your parameters (31-JUL-20) and pull all dates before this value and also put a filter for year in your two parameters.

So you end up with:

    tbl.date &lt;= '31-JUL-20' and YEAR(tbl.date) in (YEAR('31-JUL-20'), YEAR('31-DEC-19'))

I dont feel you should be using a range when you're not pulling a range, surely it confuses the logic.",1.0
g7gjmj1,j43tr5,"The way I understood it is you need records between a range and records before a certain date?  
You can pull your date range OR less than that date1",1.0
g7fy284,j42efw,"I cant copy and paste code i have saved in one note or even notepad! when i paste it it gives me an SQL syntax error . but when i copy and past it within the mysql workbench copy and paste works. even weirder , i copied the exact same code from mysql workbench

 and the from one note and pasted it into notepad, both seem exact the same but when i copy from the notepad after this excersise and paste them both (one first and then the other) into workbench, the one whose source was originally form Workbench works but the one from notepad does not unless i remove all spaces and add them in again and then it works. 

THIS IS DRIVING ME CRAZY, i cant use any of study code i have in one note or even notepad because wit wont work but the text is EXACTL THE SAME , help i am going nuts over this and i do not understand it (same behavior in visual studio code and popSQL so its not program specific)",1.0
g7i5gs2,j42efw,"Maybe it is an IDE problem, what one you using? 

Could try putting in notepad++ and turning on symbols so you can see line endings and the like.

Is it definitely the spaces causing it?",1.0
g7igixn,j42efw,"Yes definitly the spaces, I noticed that when I paste it into word, remove all formating and then paste it into sql it works, wierd but it shouldn't be like that",1.0
g7fq096,j41d6p,"&gt;Find the beers (names and manufacturers) sold in either Joe’s Bar or Sue’s Bar.

Multi-relational would look like this:

    SELECT beers.Name, beers.Manuf
        FROM beers
        INNER JOIN sells on sells.beer = beers.name
        WHERE sells.bar = 'Joe''s Bar' OR bars.Name = 'Sue''s Bar'

Subquery would look like this:

    SELECT beers.Name,beers.Manuf
        FROM beers
        WHERE beers.name in (
             select sells.beer
             from sells
             where sells.bar = 'Joe''s Bar' or sells.bar = 'Sue''s Bar'
          )

I'm not sure if these are actually the answer to the question, just giving you an idea that relational = JOIN and subquery = additional SELECT somewhere inside of your query.  You might need to join in the BARS table too... but from the table structure looks like the bar name must be unique",1.0
g7ft5nk,j41d6p,Thank you. That helps a bunch. I was thinking using join instead of a nested query was the difference.,1.0
g7fl5qp,j401y1,How to screw up text searches without being as obvious as dropping th index? Asking for a friend,22.0
g7fmmex,j401y1,Mess up the stats on the table,12.0
g7fre5m,j401y1,Drop primary keys...,8.0
g7frrt7,j401y1,Wouldn't that be obvious tho? I think the first thing I'd check would be keys and indexes,3.0
g7fwnnq,j401y1,"Yeah - Probably. It might be too obvious... maybe a random shuffle or something. Links appear to work but it might take a while to realize it’s linked to the wrong rows, etc",2.0
g7frnl3,j401y1,Can you please elaborate?,1.0
g7g1yur,j401y1,"I haven’t dealt much with indexes, but I imagine its like having two tables that can be related with a primary key. For example, suppose you have a table with book authors and a numerical author ID (e.g. Say author = Tom Clancy and the author ID is ‘1’). Another table would have a list of book titles by all the authors listed in the first table along with the author ID (e.g. book title = The Hunt for Red October and, in a second column the author ID = 1). Since Tom Clancy has written many books there would be multiple rows in the second table with author ID = 1. 

So dropping the primary key (author ID) in this case means you can no longer figure out which author wrote which book(s).

Changing the primary key would be less obvious as you could still select an author ID and identify related books (but the wrong ones - e.g. swap the author ID for a different value; like change 1 to 5 - perhaps you end up pulling books by Mark Twain)

Again - not convinced this applies to an index, but if you have an index then you probably have piles of data and somewhere inside would be primary keys.",3.0
g7obffq,j401y1,"I'm not sure your explanation makes any sense. 

An index is not the same as a key. An index is, as it's name implies,  list of all the unique values in a certain field and the row numbers that contain that value.

It's just a more efficient way for the DB to know what rows contain the various values. You can drop the index and the queries will still run, they'll just take longer.

If I want to find all books by 'Stephen King':

* With an index, the db can look for 'Stephen King' in the index and find out his books are in rows 5, 80, 352, 2256, and 5344, then go directly to those rows.
* Without an index, the db has to look in the *author* field of every single row of the table to find out which ones it needs. That's a lot more work. You can still run a query based on author name, it's just slower.",1.0
g7g6lvk,j401y1,Not sure if you were talking to me now because someone else replied and was talking about a different thing,1.0
g7fxqd4,j401y1,I think any monkeying with indices would be too obvious unless you expect your ex-employer hire someone totally useless to replace you. The best bet might be introducing (reintroducing?) tiny data quality issues into production data in places where they can't be trivially discovered all at once.,3.0
g7h8pce,j401y1,"And then leave behind plenty of guides with hard and fast rules that overlook those random exceptions.

""Yeah, just sum up anything without an R in the type field""

Oh yeah that works great.

3 days later...

""Well apparently this other setup uses L instead of R, but you have to cross reference the location info""

...I'm not bitter, you're bitter...",5.0
g7hlmyi,j401y1,What if you changed the PK from being integer to being a varchar? Values are the same but nothing works properly.,3.0
g7hxz3o,j401y1,Suggest they use Elastic or something else that requires another tech stack that no one has troubleshooted before.,2.0
g7g4q9p,j401y1,"this is every fortune 500 company ever. contractors are there just to clean up the mess from the previous group of contractors. even if you do the job right and to spec, in 3-6 months the reqs will change and it will have to be re-done",14.0
g7gdpi3,j401y1,anyone ever wondered what happens if you add extra row on sys.dual table ?  ( DO NOT TRY IT IN PROD! ),7.0
g7hjhtq,j401y1,I'll be starting up a VM tomorrow to play with this.,2.0
g7imf68,j401y1,"that's a new and exciting table i have never hear of...

edit: there is no such table, i feel somewhat betrayed.

edit2: ew oracle!",2.0
g7fuf9c,j401y1,"""That's gold, Jerry! Gold!""",5.0
g7fi1qu,j401y1,CHI Nah! Wanted me to start dropping sys tables.,10.0
g7g4zai,j401y1,"I install kill switches deep in code. I'm gone more than 7 work days in a row, all tables disappear",7.0
g7g7ls5,j401y1,"I put ASCII art in my code and comment it out. I just finished a database project with dozens of views &amp; sprocs, and in any of the significant ones you'll find some random ASCII picture, either a comic, or famous work of art, etc., most of which have names of business partners involved &amp; the date the code was deployed.

Serves a dual purpose. One, I think it makes the code beautiful, but more importantly it tells me who was involved n the relevant business decisions. We don't have comprehensive documentation yet (haha, probably never will), but these help serve as markers. For example, in one particularly complex piece of codes I have an ASCII penguin, because the head of a certain team uses a penguin as their avatar. This person wrote the book on the subject matter which the sproc deals with, and let's me/everyone else know that it was signed off on by the highest levels.

Later I'll use these markers as I develop documentation... if I ever have time.",14.0
g7g80h4,j401y1,Love this idea!! Stealing it! Add some pop culture to your scripts,2.0
g7g8kt2,j401y1,"If I'm waiting for jobs to run I'll Google things like, ""famous ASCII art"" and poke around. I try to find something relevant to the code itself, so for example in one of our views that is pretty crazy (if you select * from it, it will generate 1B rows and take a really long time to run) I included a Beavis and Butthead ASCII piece where they talk about breaking a computer, then signed my, and the initials of other people involved in building it, along with the date it was deployed.",3.0
g7g5wwo,j401y1,Lmfao this is epic,2.0
g7g63fk,j401y1,"To be fair, I only do it with clients who try to stiff me",4.0
g7itamb,j401y1,Complety fucks the DB and blames it on the next DBA.,2.0
g7oc28b,j401y1,He blames it on the previous DBA.,1.0
g7iwf1k,j401y1,"My approach is simple. I genuinely do the best I can, taking advantage of any opportunity to expand my skill-set, under the understanding that the skills I develop are worth a whole lot more than some job security at a replaceable job that seems me as a replaceable part. 

People in IT fields seem to constantly have this fear that they might have to take their 2-5 years of additional relevant job experience, and have to go through the trouble of finding a more senior job that pays better, with a better culture-fit, and more interesting projects. What actually ends up happening is that they inevitably jump ship anyway, and all the weird hacks and tricks they put into their code in the name of ""job-security"" end up as a genuine nightmare for someone that's probably not qualified to  understand even a fraction of the problems being solved, nor the reasoning behind such convoluted approaches. 

There's rarely much benefit in clinging onto the same job while you develop an ever-evolving set of advanced skills, while there's often a huge benefit to occasionally hopping onto a new project, if only because you can demand significantly more from a new employer for the extra years of experience than you could expect to get as a raise from one employer. If you can take that next job after learning a healthy set of useful skills and good design patterns, that's only better for you. By contrast, if you spent half your time writing code that belongs in a obfuscated code contest, you're just shooting yourself in the foot with a rocket launcher because the type of code you spent most of your time writing will inevitably affect the type of code you write in the future.",1.0
g8k7256,j401y1,"I've always wondered something. You know when sports announcers on TV come up with crazy stats? 

""LeBron has had 10 triple doubles in the last 3 years with total points above 20 when down by 10 in the 3rd quarter.""

Do they use SQL to pull back stats for this?",1.0
g7fjtzl,j401y1,"/* Returns the fucking Donald to the goddamn oval office.
*/ 
SELECT 
DonaldTrump AS president
FROM
Presidential_Candidates",-24.0
g7fqhk6,j401y1,"Please, just drop the table.",28.0
g7fswcf,j3zf20,"Haha, I got the job and he didn't ask me anything about PANDAS or SQL.",2.0
g7g2lni,j3zf20,"Honestly, if you can do joins and feel your way around Excel you're going to totally smash the requirements for this job. I work with people that have ""engineer"" in their title in a data-centric organization that haven't gotten there yet. You're on your way!",1.0
g7fxtlg,j3zf20,"If you don't know case statement in SQL yet, [https://www.youtube.com/watch?v=mHWb16MvyMs](https://www.youtube.com/watch?v=mHWb16MvyMs)",2.0
g7fciqz,j3z7yv,"what part are you having trouble with?

what have you tried?",1.0
g7fj2p9,j3z7yv,I can’t rationalize how to combine max and count white simultaneously joining tables to grab the name from agents. Similar issue for topic,1.0
g7g7vrv,j3z7yv,"    SELECT MAX(humpty)
         , COUNT(dumpty)
         , agentname
      FROM table1
    INNER
      JOIN table2
        ON table2.thiskey = table1.thatkey
    GROUP
        BY agentname",1.0
g7gp4wn,j3z7yv,This doesn’t filter by review date,1.0
g7hjgog,j3z7yv,"no, i was just showing ""how to combine max and count white simultaneously joining tables to grab the name from agents""

that wasn't supposed to be the answer to your assignment",1.0
g7hnylz,j3z7yv,"Right, the issue was all these concepts combined. The per day aspect is the problem.",1.0
g7mf027,j3z7yv,"I assume you already did the join part.  


**Question 1 (for each of points below is CTE)**

1. Query for count the ticket\_id
2. From CTE number 1, use row\_number partition by date order by ticket\_count desc, agent\_id asc.
3. From query number 1, add where condition where row\_number = 1

&amp;#8203;

    with
    cte1 as (
        select
            agent_id
            , name as agent_name
            , review_date
            , count(a.id) as ticket_count
        from
            ticket a
            left join agent b on a.agent_id = b.id
        group by
            agent_id
            , name
            , review_date
    )
    , cte2 as (
        select
            agent_id
            , agent_name
            , review_date
            , ticket_count
            , row_number() over(partition by review_date order by ticket_count desc, agent_id asc) as rn
        from
            cte1
    )
    
    select
        review_date
        , agent_name as top_agent_name
    from
        cte2
    where
        rn = 1

**Question 2**

1. I think you'll get the idea after you understand my answer for question number 1. Cheers!",1.0
g7f7jg0,j3yrci,"sorry, no tool recommendation, i just thought i'd point out that those integer datatypes ""like int(1), int(10), int(11)"" all map to the same standard datatype INTEGER

the number in parentheses is valid only where the database supports ZEROFILL, and gives the number of digits to show (zero-filled on the left if necessary)",2.0
g7f7yc2,j3yrci,"I've used SSIS to do this for me... I'll create a new blank SSIS project, setup my source data (DB2 was the last I used), have it create the destination table and bam, I get a `CREATE TABLE` statement with proper MSSQL data types.

The SQL Server Migration Assistant does this too, but again, not very ""simple"" to use.

I'm not really aware of any other tool, but it sure seems like it would be simple to create one...

Edit: here's one... http://www.sqlines.com/online",2.0
g7f9eb4,j3yrci,"what bums me out with SSIS is that it tries to guess the data type to use based on the content of the source and picks a suitable SQL Server data type. but you end up with a lot of text fields when you actually want something else.

&amp;#x200B;

that online tool would do the job pretty well",2.0
g7f9gsl,j3yrci,interesting tool you found there.   it is pretty cool. what did you search to find it?,1.0
g7fa27c,j3yrci,"`data type conversion db2 mysql sql server oracle`

Kept getting random articles on how to convert a data type in MS's TSQL / Oracle's PL/SQL when searching for just ""data type conversion"", so I figured I'd throw a couple of other DBMS in the search...",2.0
g7fbfwu,j3yrci,Nice lol.,1.0
g7fkem6,j3yrci,"to add another response to this one.  this would be a pretty simple tool to create, just needs the data to get started.  i think i may work on this, unless of course someone else can find something similar.   the sqllines tool you found works, but not in all cases.",1.0
g7f6jcv,j3yq66,"It's a custom piece of code that is stored in the database along with the actual data. This procedure/function can be called and perform calculations, return results or cause other effects to the data stored on the database.

Main difference to a procedure/functions/code that lives outside of the database (e.g. a Python application): Those need to transfer the data required to perform a task (or calculation). A stored procedure on the other hand, lives in the same location where the data is (in the database) and therefore can work with the data directly. This is often much faster than transferring data back and forth.

On the other hand, stored procedures add processing load to the database. The database often already is a central choke point for performance and from that perspective you want to keep away as much work from the DB as you possibly can.",8.0
g7f92t4,j3yq66,"The trade off you mention is a really important thing to be aware of. You can save time on the client if it's spending a lot of time waiting on data to be exchanged back and forth, but you also don't want to cripple the database especially if it's serving other clients / processes.

Another thing to be aware of is separation of concerns. Depending on the design of the process, it can be beneficial that the client app only needs to call a stored procedure to get back the data it is expecting. If any changes to the underlying queries need to be made, they are done in the stored procedure. As long as the output format is the same, the client won't even be aware if a change happened. 

Lastly (and this might be a Microsoft SQL server specific thing) stored procedures can be interesting with database permissions. You can have a SQL client that doesn't have INSERT privileges on a particular table, but it does have EXECUTE privileges on a stored procedure that inserts into that table. This is a way of ensuring everyone inserts data in a table in the same standardized process.",3.0
g7f9k7f,j3yq66,"If I were explaining this to Mrs. Heirophant, I would make it even simpler.

Let's say you're buying a watch online using your phone. The site needs to calculate whether you should get a discount. There are three places this calculation can happen: on your phone, on the website, or on the database that powers the site. A stored procedure is a program on the database that does the calculation.

Now there are advantages and disadvantages to each approach.

Personally, I never use them because it's just one more place to maintain code.",1.0
g7fb6kl,j3yq66,How about the difference between stores proc and views? Seems to me Views are simpler version of Stored Proc.,1.0
g7fniby,j3yq66,"A view is more like a ""stored"" and reusable `select` query. Like a stored procedure, it can be called from other places, but it only retrieves data from the db and doesn‘t allow data manipulation.

A stored procedure contains actual programming code (think algorithms, variables, loops, conditional branches) and can perform multiple steps, one after another or to say it differently: it runs instructions procedurally - hence the name.

Stored procedures are written in ""real"" programming languages like Java, Python, C# or vendor specific extensions to SQL, such as Transact SQL or PL SQL. These extensions add the missing parts to turn SQL into a ""real"" programming language.",3.0
g7joznt,j3yq66,"Thanks for the above, not OP.

Can I ask, lets say I have multiple Stored Procedures that does XYZ and all pull in data from a range of diff tables. 

Is there a way to search stored procedure and figure out which are contacting to where and which tables they are being affected?


So for example, in Excel I can click CTRL+Find and then search ACROSS the whole workbook and multiple Sheets.

In a stored procedure or a View - which has many different SQL and commands, how would you search across them or into them to find out and unpick them?",1.0
g7f8d1g,j3yq66,"A stored procedure is SQL code stored on the database and can be executed by a client by calling the procedure's name.

The simplest example of a stored procedure is just a pre-made query. Rather than having to send the same query to the database from the client, you can store the query in a stored procedure and call it from the client. 

In reality, this isn't really a useful case for using stored procedures, as the database server will cache execution plans for queries it encounters frequently. Where stored procedures become more powerful is executed lots of queries to process a lot of data and then just return the final result. This can be faster than exchanging back and forth between the client and server multiple times. Stored procedures can be given parameters like functions or methods in regular programming languages.",2.0
g7g5wy0,j3yq66,"It is code that can do anything you tell it to, when you want it to, without your hitting GO button",1.0
g7gt98k,j3yq66,"Its basically a function in a normal programming language.

Consider it a black box. Send inputs, return output. 

It can do anything you could want to do with sql. Inserts/updates/deletes/selects.",1.0
g7h532h,j3yq66,pre-stored subroutine that does stuff you need it to do. i like them but apparently its actually better to just execute the code. i prefer to have whatever language im using pass the string &lt;&lt;exec stored_proc&gt;&gt; as oppsoed to opening up a text file and doing all that jazz,1.0
g7hyv4l,j3yq66,I tend to tell non-technical business users I created a set of instructions that will automate a repetitive task and improve efficiencies.,1.0
g7gbc66,j3ygs0,"When you use the SSMS menu/dialog to do the backup, are you selecting the same options that are being used in the job? In the dialog, there's a button at the top where you can have it send the script to clipboard, file, or window. Do that and see what you get.

I'd suggest just running the job to create the one-off backup but it appears that it's not creating a unique name for the backups - you're overwriting your backup every day, you only have one day's worth of backups! This is not a good place to be!",2.0
g7hg42k,j3xa8d,"What's the issue you faced?

We just went from 2008 to 2019 and only issue was file system permissions on the new windows server which isn't even an sql issue.",1.0
g7en3q4,j3ub6g,"this is how to build a query

**step 1 (FROM)** -- analyze what you need and where it's located, then use that to write the FROM clause

this time it's easy, there's only one table

    FROM i_have_a_table

**step 2 (WHERE)** -- determine if any of the (joined) rows returned by the FROM clause need to be filtered out

this time it's easy, there's no filtering required, so no WHERE clause

**step 3 (GROUP BY)** -- is any aggregation needed?

this time it's easy, you want sums by location and day of week , so that's what you have to GROUP BY

    GROUP BY store, DAYNAME(specific_date)

**step 4 (HAVING)** -- are any of these groups going to be filtered out?

this time it's easy, nope, so no HAVING clause

**step 5 (SELECT)** -- determine which columns you have to return

this time it's easy, you want those groups and the sum of the quantity for each

    SELECT store, DAYNAME(specific_date), SUM(quantity)

**step 6 (ORDER BY)** -- do the results have to be sorted in any particular sequence

this time it's easy, no, so no ORDER BY clause

putting everything together, and adding a couple of column aliases to make the results easier to handle, here's the query

    SELECT store
         , DAYNAME(specific_date) AS weekday
         , SUM(quantity)          AS total
      FROM i_have_a_table
    GROUP 
        BY store
         , DAYNAME(specific_date)


by the way, this is all extremely basic

you really really should be able to write queries like this yourself",3.0
g7eohba,j3ub6g,This doesn't provide the average does it?,1.0
g7eqjes,j3ub6g,"you are correct, good sir or madam as the case may be

young Mister Living Life seems to be in some need of training, so i thought i'd see if he would catch that on his own

also the fact that he probably ~does~ want an ORDER BY clause",1.0
g7erig9,j3ub6g,"Thanks for confirming, I literally started learning sql this week so just wanted to check my own understanding.  The rest of the post was really useful for me.",2.0
g7ew3kz,j3ub6g,"Thanks again! Always saving me. But this is the part where I was struggling to be honest. I managed to handle all the steps, but what makes it difficult for me is calculating average of more than one sunday (as an example) because there is more than one sunday in the table...
I am more a c# developer so it gets time to get used to this kind of thinking because its completely different than what I am used to... thanks mate!",1.0
g7j4z07,j3ub6g,How can I do the average part? Please assist me here I am still stuck with this... :/,1.0
g7j5xga,j3ub6g,"i will assist you

find the mysql manual, look up functions, and see if you can find anything there that might be used to produce an average

you can't expect me to do ~all~ your homework",1.0
g7emlte,j3ub6g,I think Windows functions would be a Good Solution for this,1.0
g7emury,j3ub6g,I agree but I need to have it done with MySQL :/,1.0
g7eqa64,j3ub6g,"FYI MySQL has window functions

but you won't need them",1.0
g7ducd8,j3r2qg,"You've essentially already done all the work in your head. Vendors, Customers, and Employees are all distinct entities and do different things in your database so they should have separate tables.

Employees that belong to a Company should have a foreign key with the company ID. You said you're making an e-commerce platform, so you're presumably going to have an Invoice table, which would have foreign keys for the `customer_id` and `company_id` so you know who bought the product and which company sold it.

You might need to make an in-between table if you have employees who are employed by multiple companies.

If you want to properly research how to design a database I'd suggest looking up ""Database Normalization"" or Database Normal Forms",5.0
g7dwl04,j3r2qg,"Thanks! Appreciate the response. Would you say the user table should store for all types of users, which is the way I have it right now. Or have it so that each vendor, customer, and employee table has username/password. But that seems redundant.",1.0
g7dxmqi,j3r2qg,"Stick with a single table for all usernames and passwords. I would also consider including any other properties that are common across all the user types (if any).

When a users attempt to login you can use the same code for all authentication and once they have authenticated you can check and apply authorization.",1.0
g7dxr72,j3r2qg,"Gotcha sounds good. Since it would be one table for all users, would I have some sort of Id in the table to reference them (customer, vendor, employee)",1.0
g7dyvl8,j3r2qg,"You should have some type of Primary Key that is different than the username,  email. Most common would be an auto incremental integer and you can use that as your foreign key.

Can a user have multiple roles? E.g. a vendor can also be a customer. If so you may want to consider having a separate table that maps the users to their roles.",2.0
g7dzg28,j3r2qg,Gotcha thanks. Users can only be one role.,1.0
g7etkqw,j3r2qg,"You may want to seriously reconsider that long term. What a user is can well be fluid and may well change over time. Instead of think what ""type"" of user they are, instead think of what ""role(s)"" they play.",1.0
g7f4lti,j3r2qg,"Hmm I'll consider it but I just don't ever seeing that as vendors/employees would be using a website mainly and the customer would be using an iOS app. I just don't ever see a user role crossing over to be both.

Maybe a vendor/employee could be a customer but I wouldn't want a customer to become a vendor/employee",1.0
g7dug9f,j3r2qg,"Start off with three main tables: Vendors, Employees, Customers. Figure out the relationships. The Vendors table and Employees table will probably by a one-to-many relationship (1:N). This means that you will need a VedorsID column in your Customers table, which will be the foreign key. The Vendors table and Customers table will probably be a many-to-many relationship (M:N). This means you'll need to make an intermediary table that stores the associations between vendors and customers. This intermediary table will break the many-to-many relationship into two one-to-many relationships, which is necessary to maintain normalization in the main tables. From there you can expand and create other base tables like a Products table if needed.",2.0
g7dum4k,j3r2qg,This is a great answer. Lookup Database Normalization. It's actually not very complicated and will save you headaches.,2.0
g7dwfax,j3r2qg,"Thanks for the response and advice, I'll definitely do some more research!",1.0
g7dyc63,j3r2qg,"I don't think I fully understood your question the first time, but I found a Stackoverflow post that pretty much answers your question: 

[https://stackoverflow.com/questions/8479252/database-design-3-types-of-users-separate-or-one-table](https://stackoverflow.com/questions/8479252/database-design-3-types-of-users-separate-or-one-table)",2.0
g7dxrs3,j3qxdv,"If the system really can't handle that many separate statements, my first attempt would just try column edit mode in text editor to remove the start of the line (assuming they're all the same e.g. 
    
    INSERT INTO ... VALUES

Then add commas as necessary. 

I'm sure you can chop the files up by line using standard Unix utilities like head/tail etc. To make smaller batches.",1.0
g7j65qq,j3qxdv,Text editor could not open the file but a lil python script did the job,1.0
g7ds0jw,j3qv3p,"Knex.js for migrations/schema and query building.

Objection.js for an ORM (built on top of Knex).",1.0
g7e6suv,j3pdnh,"Those certifications are being retired in 4 months. They'll still be valid if you hold them, but they won't be offering them any longer.

The replacements are role-based Azure exams/certifications. And there isn't a 1:1 mapping for the old MSC* SQL Server certifications. In fact, the only one for database administration at this time is DP-300.",1.0
g7f32uz,j3pdnh,To learn joins and subquery https://youtu.be/P8hxoMQw7ig,1.0
g7d713w,j3n276,"What connection types are you using to call your queries (ODBC, ADO_NET, etc...)?",1.0
g7d8px1,j3n276,"mysql, its a local conection",1.0
g7d9mtj,j3n276,"Try adding a semi-colon to the query you stated doesn’t work. From a syntax basis, the query should run (even with the backticks). Backticks are specific to MySQL but take a look at the following thread: [Stackexchange MySQL backtick](https://dba.stackexchange.com/questions/23129/benefits-of-using-backtick-in-mysql-queries)

Or try to use the follow set command before calling the rest of your queries: SET sql_mode = 'ANSI_QUOTES';",1.0
g7dcj1v,j3n276,"TIL it's called a back tick

Thanks for your advice I'll look into it.",3.0
g7d43hq,j3n18q,"1,2,5,7 looks right to me. Where are you stuck?",2.0
g7dbxj8,j3n18q,Thank you! Just seconding guessing to see if 7 would be a possibility as well,1.0
g7d7ksw,j3n18q,"I'm not Op...

But shouldn't they all be possible?

If you assume that the 100 rows on table B are actually just 10 records with ten rows each, and they all match table A, then all of the answers are correct except 5.

If you assume a different scenario, then 5 is still possible?",0.0
g7d9hie,j3n18q,"3 and 4 wouldn't work even if there are duplicate rows. With a RIGHT JOIN and FULL OUTER JOIN, all the rows from the right table should be present in the result set, so there should be at least 100 rows.",2.0
g7db1y9,j3n18q,"But if the 100 rows are just 10 records duplicated, isn't the output ""10 records"" still true? (Even if your results actually do contain 100 ""rows"")

I thought the definition of ""record"" was ""a set of rows where the leftmost key matches""?


I've always kind of used the terms interchangeably in the real world, so maybe I'm just misremembering...",1.0
g7de74h,j3n18q,"You bring up a good point. The question distinctly uses both terms. In the real world, I've used them interchangeably too. Now I'm a bit confused",2.0
g7d8z67,j3n18q,"it's a terribly worded question and much will depend on how you interpret it.

If you a) read ""The relationship between A and B is 1:N"" as B _has to_ have a valid reference to A and A does not necessarily have a child record b) understand incomplete JOIN clauses as using the correct relationship columns c) assume that the point B prohibits the use of CROSS join

so if you take all the above, then there are only 2 possible answers.",1.0
g7dya0m,j3n18q,"1. If all rows from A match all rows from B then Ok.

2. If rows from A match only 10 rows from B in total  or if rows from A do not match any row in B then Ok.

3. If in table B there are only 10 rows that do not match any row in A, but the other 90 match, then OK, right?

4. If rows in A DO NOT MATCH, anything in B , then we get all rows from A, so OK.

5. Same as above, NOT OK.

6. NO

7. YES

This is my take on it.",1.0
g7daoxo,j3n18q,Learn the joins with business use cases https://youtu.be/P8hxoMQw7ig,0.0
g7cys1l,j3m9wh,"I haven't used Access in years but depending on what brand of SQL it's currently using the BALANCES in the order by might be causing the problem. Have you tried changing the order by to SUM(AMOUNT) rather than the BALANCES alias? It might be struggling to interpret the alias as it doesn't exist until after the query completes.

Alternatively just make a second query using the first one as the only source and then apply the order by to the second query.",1.0
g7czd9h,j3m9wh,"Hi, and thanks for the reply.  Been bashing my head against this one for a while now. 

I have indeed tried that.  However, I just gave it another it another stab.  The same problem occurred, but what's interesting is that the syntax for the error reference changed.  When I try to reference BALANCES, the Enter Parameter Value error comes up with just BALANCES as the reference.  However, if I changed my line to read ORDER BY SUM(AMOUNT) DESC, the error syntax now reads Query10.'BALANCES'.  I find that to be curious...",2.0
g7d0h0c,j3m9wh,"Did you use the query wizard to create this? I think in the case of Access it's probably the way to go for simple queries like this. The actual query Access creates is pretty stupid but it often easy than trying to adhere to Access's bonkers SQL syntax.

Also you might have missed my ninja edit alternative solution, removing the order by and creating a second query referencing the first which you *should* be able to apply the order by to.",1.0
g7d7gb6,j3m9wh,"You can’t sort by an alias. Sorting by SUM(AMOUNT) should have worked, but sometimes Access is so finicky. When Access is giving me a hard time sometimes I start with a fresh query and paste in the text I had. Also are you attempting a join here? Confused why your FROM has two tables.",2.0
g7dasim,j3m9wh,"Yes I was joining two tables. And yes, you’re right, it should have worked. Every single tutorial online was saying that it would work. I did find a workaround, which I mentioned in my reply above.",1.0
g7daoh2,j3m9wh,"I was trying to learn how to use the SQL setup (it was for a class) and the wizard didn’t help much. I ended up making it work by changing the Order By to reference column 2. Even then it freaked out on the word Balances in the Select field. For some reason it worked after I put apostrophes around Balances like so: ‘Balances’. But then it brought the apostrophes into the table view heading, too. Weird, but I made it work...mostly.",1.0
g7d1e9j,j3m9wh,Rusty on access but try : Order by 2 desc,1.0
g7dauri,j3m9wh,Thank you. Found that our right before your reply. Had to make some more changes to the SELECT line but I ended up making it work. Thanks!,1.0
g7cwj2j,j3m2ov,"It looks like you're joining one of the tables to itself. 

In case this is homework:
 
&gt;!INNER JOIN patients_hospitals ON  patients.patient_id = patients_hospitals.patient_id!&lt;

I'm on mobile so excuse name typos.",4.0
g7d0q4b,j3m2ov,Your first join is wrong. It's not joining the patients_hospitals table to the patients table like it should be.,2.0
g7dgrtg,j3m2ov,"Yes it should be something like

From patients p
Inner join
Patients_hospitals ph on p.patientsid = ph.patientsid

You know u can call you table through surname",1.0
g7cstsy,j3lpmb,"Your tag says MySQL, which should start with autocommit on. Have you had to turn it off for any reason? If so, have you tried committing after you do your inserts?",1.0
g7ctop8,j3lpmb,Yes I'm using MySQLWorkBench. I haven't touched any settings to my knowledge. Where can I check?,2.0
g7cz4y8,j3lpmb,"I'm sorry, I don't have any experience with that particular client!

But you could do your insert commands, end with the semicolon, then add the ""COMMIT;"" line after. See if that works!",1.0
g7czf28,j3lpmb,"Also, I notice that your message has you using ""select \* student;"" and I assume that's a typo. You'd want ""select \* FROM student;""",1.0
g7czk4v,j3lpmb,"Yeah, that was just a typo. I'll try the COMMIT line in a bit.",2.0
g7d033w,j3lpmb,"Just tried COMMIT;

Didn't work but I was really hopeful it would lol. I ended up messaging my professor to see if he'll have any idea. Was hoping for some immediate solutions since it's due tn but I'll just submit it late if I ever get this resolved.",2.0
g7d3ffp,j3lpmb,"What is the message  you're getting when you do the insert?  Does it say ""1 record inserted""??  Or does it give an error?

When you do your select *, do you get any records back?  Does that give an error?

What message do you get on the commit?",1.0
g7czp6t,j3lpmb,"Computer Science needs to be ‘computer science’
And as other mention make sure to check if you need to commit your work",1.0
g7czyy1,j3lpmb,"Yeah, sorry that was a typo as well. In the actual query everything is written correctly. I also tried COMMIT afterwards and still nothing shows on my table. I ended up emailing my professor, hopefully he has an answer smh.",1.0
g7d04t8,j3lpmb,Do you have any kind of message either error or 1 row inserted.  Anything?,2.0
g7d0fb0,j3lpmb,"No error (green validation mark), the response is ""1 row(s) affected"", and Duration/Fetch Time is 0.020 sec",2.0
g7d0low,j3lpmb,Bit of head scratcher- I apologize for asking but are you sure you querying the table afterwards,2.0
g7d2d3s,j3lpmb,"Your query program doesn’t limit a select to 1000 records by default by chance ?

Did try to select that specific student Id 

Select * from student where ID = 123",2.0
g7d3w1i,j3lpmb,"Yup, 2nd this.  Your mysqlworkbench likely has a configuration setting for max results returned.


Also try 

    Select count(*) from students;

And see it the count increases as you enter additional records.",1.0
g7d43qa,j3lpmb,A quick google search confirmed the 1000 default limit on workbench,2.0
g7f0w4j,j3lpmb,[deleted],1.0
g7f1eme,j3lpmb,Google is your friend. You can use a cte for that,1.0
_,j3lpmb,,
g7f1kdv,j3lpmb,"Yeah, I see now that my count is going up and I've created many duplicates. Would you know how to get rid of them so that I can make ID the primary key?",1.0
g7fyr43,j3lpmb,"Note: This will delete all records where there are more than one record with that ID... if there are 2 records for an ID value, BOTH the records will be deleted, not just 1 of the records.


```delete from students where id in
(select id from students group by id having count(*) &gt; 1);```


Any time you do a delete, you should run the same query as a `select` first, to see what records will be affected:

```select * from students where id in
(select id from students group by id having count(*) &gt; 1);```",1.0
g7f1f63,j3lpmb,"I accidentally responded without first seeing this so I deleted my other reply. Yeah my professor pointed out the same thing. I expanded my limit to 50,000, but there are 500,000+ students on the list so I can't see it regardless. However, with the query you just suggested I enter, I can see what I was looking for along with all of the other duplicate queries. How would I go about getting rid of them, because I want to make ID a primary key, but can't because of duplicates for '1001' (I used 123 as an example, and 1000 was the previous limit)",1.0
g7cnqjc,j3kz2m,Build a basic UI for inputting and querying data. Slap some logos on it and you have a fully functioning app.,8.0
g7cvuf5,j3k1jo,"    select C.id,
        C.last_name,
        group_concat(distinct member.first_name separator ', ') as names
    from
        (
            select *
            from family join member as B on id = B.family
            where B.first_name = 'fred'
        ) as C left join member on C.id = member.family
        group by id;

That works.",1.0
g7cevbj,j3hlic,"Figured it out by banging my head against it until it worked. I had the string,substring in the STRPOS reversed (I had substring and then string)",1.0
g7cfxc9,j3hlic,"Not sure how much control you have over the structure, but this sounds like something that should be handled on the application side. Also, I'm not sure about Presto, but Postgres does support regexes.",1.0
g7czq8q,j3hlic,"Zero control here since I work at a very large tech company and the table is owned outside of my department.

Regex is supported in Presto but potentially expensive in terms of CPU time and I was looking to avoid that. What I originally posted works, I just had the string/substring reversed in STRPOS.

Thanks for the response!",1.0
g7bk3vs,j3f7if,Is the source ok? I mean is the data in the tables ok to begin with?,1.0
g7bk8l7,j3f7if,Yep seems fine to me,1.0
g7bopyg,j3f7if,"You realize the data infers that they sold multiple of 1, and multiple of 2? You should have duplicate lines of the product ID, and they be separated by the different dates (sale prices). Looks good to me so far, unless I’m misunderstanding",1.0
g7bpcee,j3f7if,"Yeah, but when I try to join to get prices I get 4 lines of product 1, instead of 2. That’s what I meant by duplicating sorry if it was unclear.",1.0
g7bpmwy,j3f7if,"Oh ok. Umm I’m not sure why, left join should result in the same amount of records in the left table. It looked good to me, what you had there so far. The left join resulted in 4 rows, as the initial had.",1.0
g7bpwu7,j3f7if,"It’s to do with the dates the prices change not matching the sale dates, so I would need to join to prices on something like “sales date between min change date and max change date” but this results in the duplicates",1.0
g7br70z,j3f7if,"I’m very curious, so I’ll look when I’m home and at my computer. This is something I do very often, so shouldn’t be an issue for me to come up with your issue.",1.0
g7brcex,j3f7if,"cheers dude, appreciate it",1.0
g7caii8,j3f7if,"How is this?

https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=d44959232f6c21957859e11cef01675a",1.0
g7dqaoj,j3f7if,"Thanks for this! correct answer, more or less, they were after it by product id. Would  you have any idea on how to do this in mysql without using window functions? Thanks again, appreciate it",1.0
g7du1l7,j3f7if,mysql db here [https://dbfiddle.uk/?rdbms=mysql\_8.0&amp;fiddle=f58c923811789eec1030d01e2fe8825e](https://dbfiddle.uk/?rdbms=mysql_8.0&amp;fiddle=f58c923811789eec1030d01e2fe8825e),1.0
g7e0j30,j3f7if,"Idk MySQL unfortunately. However by product ID, you could just use GROUP BY sales.product ID.",2.0
_,j3f7if,,
g7bpkpx,j3f7if,"So my join to get the pricing is obviously wrong, but I’m not sure the correct way",1.0
g7brt3l,j3f7if,"if all you care about is product and revenue that's all you want in your final select statement.  You've got the join right, now just do the math in the select using an if statement wrapped in a sum.",1.0
g7brwup,j3f7if,"Here's a solution derived from your dbfiddle
https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=001b05558306f971b9c4f82dd97a95ff

It uses `LEAD` to find out the next date that a product's price changes, `COALESCE` to use a large number in case there was no next, and then `JOIN` so that the `sale_date` is between the `affected_date` and the next date.",0.0
g7bv33t,j3f7if,"I should have said, they specified no window functions were allowed. Thanks though",1.0
g7bsxzf,j3et3d,"Your code calls sysdate() twice, the ""right"" code calls it only once.  Theoretically the time could change between your calls, but that's unlikely. Also, with the ""right"" code making only one function call, it can theoretically run a fraction faster, though again it will be a very small difference.  In general though it's best to minimize the number of function calls you make when you can.",2.0
g7c8dkk,j3et3d,"Ah, gotcha. That makes sense. Thank you so much.",1.0
g7dk6t3,j3et3d,"Based on the two triggers, it seems the data type of hire_date is a string? (Bad idea in general but we’ll go with it). In your trigger, you compare this string to a date, so one side must be implicitly converted to the others data type for a comparison to be made - if it decides to implicitly convert sysdate to a string using a different format to how it’s stored in the table then you will likely get a wrong result. Having googled for the default date format in MySQL, you probably are getting away with it, but in general this is not true for other RDBMSs and others even allow a user to change their default date to string formatting which would allow your code to change behaviour.",1.0
g7b0flh,j3cmvr,"INSERT INTO
  myTable
    ( myColumn )
VALUES
  ( 'Car''s' )


Just double up the single quote to escape it. That being said -- whenever possible use parameter markers instead of actual values.",13.0
g7c8xev,j3cmvr,"The correct answer.  Just wanted to add how much I love single quoted values, so that 'This' is now '''This''' and you get to play Count-The-Cursor!^(tm) while tapping arrow keys.",5.0
g7cjy9y,j3cmvr,"Notepad++ and other programs have language parsers built in.  Quoted text turns a specific color from the rest of your sql and it very easy to tell where the unbalanced quotes are.

Edit a word",1.0
g7crsnf,j3cmvr,Pick a monospaced programmer font and it's pretty easy to tell.,1.0
g7cvifi,j3cmvr,Doubling up the single-quote is correct. Or in some languages you can use double quotes instead of single around the outside. TSQL is not one of them. This is where you’ll really see the value in a proper parser of your particular SQL language.  A good parser and IDE will color code the outer edges of your command or text and you’ll know much faster if you’ve left off one of the 38 different single-quotes in a line.,1.0
g7bnhal,j39y5g,"As a general rule, I never run a database on an external or USB drive if I can ever help it. Best case scenario is that life is difficult. Worst case scenario is catastrophic data loss. This is just not an architectural structure that SQL server was ever intended to be used on because all of the best practices assumed by Microsoft assume you aren’t doing this. 

You’re probably dealing with power management settings inflicting your pain, but USB driver settings might also affect you. 

A big thing to make sure you do when you make changes to the settings is to disable SQL server services while you make those changes. A failure to do so can corrupt your database.",2.0
g7b0s4m,j39y5g,"You'll need to disable / adjust the power saving setting for that drive / external drives in windows. A quick google should be able to help, you may need to adjust USB power save settings also. I recommend changing one setting at a time to find the one that is causing your pain.

I think it is most likely the Hard Drive Spin-down setting in Power Management.",1.0
g7b14x7,j39y5g,"Yes, found that turned it off, but once I did that SQL can't connect correctly and won't let me do anything to the database and gives me error 823..... And to do the DBCC CHECKDB! But when I do it's comes back with the same error!",1.0
g7btbd8,j39y5g,"https://docs.microsoft.com/en-us/sql/relational-databases/errors-events/mssqlserver-823-database-engine-error?view=sql-server-ver15#cause

If you have 823's, you have bigger problems on your hands. Hope you have backups of that database, because there's a decent chance it's been corrupted.",1.0
g7bu1pq,j39y5g,I don't car about that nothing was really on it but test runs! What I'm asking is there a correct way to move a database to an exterminator hard drive,1.0
g7adma4,j395q8,"&gt; Is this the right way to handle this situation?

yes, it is",18.0
g7adt5j,j395q8,Thank you!,3.0
g7cau8a,j395q8,"Oh my @DEITY, mtm is a bad way to model cashflow in such a situation. What you gonna link the payment to when you have no invoices yet? What you gonna link it to when you have several invoices issued at the same date? How much logic do you need to actually resolve the relationship correctly?

Money is a credit - debit type of transaction. Some of it is owed, some of it is paid. There is no need, nor benefit, to link payments to invoices. You just need account balance, and the list of invoices that are outstanding, or paid.",3.0
g7cg1y3,j395q8,"&gt;  What you gonna link the payment to when you have no invoices yet?

an interesting kind of business you're in to get payments without a corresponding bill/invoice/contract first. Protection, maybe?

i dont understand the other two question marks - what are the answers that you expect beyond the most obvious (each invoice/transaction has an id so having multiple invoices on the very same second shouldnt be an issue and it's up to business to define rules of amount allocation).

Sometimes, you are right, money is just that - a number that comes out of a single pile, sometimes there's a complex attribution rule and you certainly dont want to re-code that in ETL, biz workflow, ODS reporting, your KPI dashboards, etc. It is a super easy decision to argue for and take - run the rule(s) once, capture the outcome, done.",1.0
g7dkmg2,j395q8,"There's a number of scenarios in which you don't have an invoice when a payment comes in: booking mistake, system malfunction, customer prepaying their account. 

The thing these answers are not obvious, they will become more difficult over time as complexity of mtm allocation rules grows.",1.0
g7dzr7z,j395q8,"&gt; booking mistake

Report/log an exception, move on

&gt; system malfunction

Report/log an exception, move on

&gt; customer prepaying their account

In escrow/ deductible kind of situation, there is a bucket to allocate; if business NEEDS payments to be allocated it's their decision what happens in these cases - a default bucket of some sort.

Regardless, why do you think this scenario is problematic to begin with? There is nothing in a generic associative table structure that prevents you to have entities on either side without a relationship.",1.0
g7asns7,j395q8,"There's a few ways to approach this.  It's a challenge, and there's often no business way around it.

A payment needs to be able to cover multiple invoices.

An invoice needs to support multiple payments.

You are correct in the thought that you need an intermediary table to handle this.  A credit/debit table that tracks amounts applied to an account (not necessarily an invoice, though in practice that's how it works), and can have multiple rows for the same invoice or the same payment/credit.

Something like: CreditKey\_pk, Invoice, Payment\_note, amount.  That PK is the unique, and you should evaluate if the combo of Invoice and Payment\_note should be unique.

There should be a separate check somewhere that ensure all records here add up to the correct amount for a payment\_note, and the invoices themselves would peek into this table to determine if there is an outstanding balance.

You could also have logic that auto-applies the payment to the specified invoices, likely oldest to newest.  At the end if there's still balance owing on the invoices it tells you, and if an overpayment was made it can issue a credit note or look for more invoices on that client to credit against.  (This should be presented as an option in the UI, which also means this logic should be in the application, not the database.)",7.0
g7bjwiw,j395q8,"I always implement a unique primary key in my tables.  I tend to prefer using an autoident column, but I’ve worked with some compound keys as well.",2.0
g7bkaxu,j395q8,"I'm curious as to your business case here.

Are you building a full accounting software or is this just a project to track invoices and payments specifically?

Payments and invoices both affect the general ledger and then either the sales or purchase subsidiary ledger. 

Typically the different ledger types would be held in different sets of tables, with a control to ensure integrity between them, whereas the transactions within the ledgers would be held in the same table(s). E.g., Payments and invoices *transactions* would be held within the *subledgertransactions* table, with a field containing the ledger type and transaction type. 

You may have another set of tables, containing the specific documents. E.g., an invoice(and credit note) header table with a lines table along side it. 

Your theory on matching payment transactions to invoices is correct, where you'd have separate table containing the details of what payments match to what invoices. You'd probably also want to include the date of the match, which may be different from the payment date. If your credit control team hadn't received the [remittance advice](https://en.wikipedia.org/wiki/Remittance_advice) from the customer but needed to post the payment anyway, then may have to back and do the matching at a later date.",2.0
g7bq9l3,j395q8,"We have a complicated database system that I have become the inheriter of and sole maintainer of.  I don’t want to get into too many details.  Our business is benefits administration.

The way it works is that eligibility for benefits is based on monthly payments, however due to how things were done, not everyone pays on time or all at once.  Also due to some aspects of how contracts are negotiated that I have no control over and don’t fully understand, rates can change retroactively.

This means you can have all kinds of weird cases of how a monthly invoice can be paid.

I can’t change the rules, so I have to adapt to them.  Right now I’m playing with ideas to clean up some legacy hacks and cruft.",3.0
g7ab3t0,j395q8,"Hello u/Mastersord - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g7ab91m,j395q8,MS SQL,5.0
g7a9ibq,j38pa7,"&gt; AND revenue_in_000s &gt; '147699'

&gt; ncluding values less than 147699....WHY?!

because strings: '97' &gt; '147699'",4.0
g7aa7ec,j38pa7,Is there a workaround to the issue? Another function that will return the values im looking for perhaps?,0.0
g7aascw,j38pa7,"remove the quotes.  you're comparing numbers and strings.  there is no workaround, that is expected behaviour

edit: made a big assumption on your table DDL - please post your create table script",3.0
g7abxbf,j38pa7,"For the revenue in 000s column, the DDL is:
""REVENUE_IN_000S"" VARCHAR2(4000 BYTE) COLLATE ""USING_NLS_COMP""",1.0
g7aczju,j38pa7,"okay cool - check this page out [https://stackoverflow.com/questions/1146928/numeric-comparisons-on-string-column-data](https://stackoverflow.com/questions/1146928/numeric-comparisons-on-string-column-data) you'll need to cast as a numeric type eg:

    AND CAST(revenue_in_000s AS INT) &gt; 147699

As another user said though, if you've got things like ""NOT INCLUDED"" or ""blah blah blah"" as data in this column (which is allowed) you're going to have funny results.  best look at getting that field changed to an INT or appropriate",2.0
g7aejdu,j38pa7,Thanks!! Very helpful,1.0
g7abgnp,j38pa7,"the issue is that you want to treat strings as numbers but you are not telling oracle about that. Obviously, dont use a string literal ('147699') - use a number literal, for the field/expression you can use the to_number function (hopefully you are on 12+ version and you have the ""default"" clause for it, if not - google it, there are regex solutions to conversion errors).",1.0
g7a8osm,j38pa7,"I hope you should get rid of the quotes surrounding your numeric condition, unless you’re storing numbers as strings?",1.0
g7a9d94,j38pa7,"most likely applying the ""&gt;"" operator on a string field is messing it up, you need to fix that column so that it actually contains numbers",1.0
g7a9w0x,j38pa7,"Do you mean that the column should only contain numbers and not ""Not included"" values?",1.0
g7aami1,j38pa7,"I mean that you need to decide if it contains numbers or text. Storing ""Not included"" and numbers in the same column usually won't be a good idea because it will make operators such as ""&gt;"" act weird.

You probably want to replace the ""Not included"" values with either NULL or a 0 so you can define the column as INT. then your ""&gt;"" will work properly.",2.0
g7aapy7,j38pa7,Thanks so much!!,1.0
g7aa7oc,j38bqr,"I checked the answers to the other question and they seem ok. The real question is what resources do you have? What are DBs? Oracle , sql server? Do you have connection between them?
It is not a matter of code organization, if you write select and  insert or encapsulate them in a procedure. Please say exactly what's your scenario.",1.0
g7ai7rz,j38bqr,"Just MS SQL Server.

Background:  I have a query that pulls from \[Server1\].\[DB\] and I want the results to import automatically to a table in \[Server2\].\[DB\]. 

\[Server1\].\[DB\] query results will overwrite the current data in \[Server2\].\[DB\]'s table. 

Ideally, I would like this to be done automatically every morning. 

This is just one query out of many and the process will be the same. 

I hope this makes sense!",1.0
g7drfbo,j38bqr,"You need to create a connection between the 2 instances: that means you need to access the second db from the first - to be able to access a table from Server2 in Server1. 
In sql server i believe it is called Linked Server. So connect to server1 and create a linked server to server2. You could google how to do it o ask a dba to do it for you.
If you can manage that, you could create a job that does a simple insert select. Or create a stored procedure and schedule it.

If you cannot create a link server, then you need to make use of a scripting/programming language:
Connect to server1 and extract data
Keep data in memory or put it in a file
Connect to server2 and insert data

Or, as others have mentioned in another of your questions:
Write from sql server to a file( server1)
Write from file to sql server table(server 2)
Google how you could do that.

Hope it helps!",2.0
g7dshav,j38bqr,"u/Monstrish's reply is great. Just wanted to add the query syntax for linking servers and querying from linked server

    EXEC sp_addlinkedserver '[server]'
    SELECT * FROM [server].[database].[schema].[table]",2.0
g7dukbu,j38bqr,/u/snancyiv Here you go from /u/K0NGO . Easier than that does not exist :),1.0
g7fw6b6,j38bqr,Thank you both so much for taking the time to respond to this SQL noob 😭🙏🏻,1.0
g7amn0h,j38bqr,I assume you have created a linked table to the source server on the destination server. If not how are you querying the source server?,1.0
g7apw9m,j38bqr,I did not. I used the import feature with the query option,1.0
g7eq5o9,j38bqr,"Ahh. OK then I recommend you use Power Shell (PS). With PS you can copy the file from the other server, check it for consistency, then import it into your SQL database anyway you like.",1.0
g7fw767,j38bqr,Thank you!!,1.0
g79s08s,j35d2w,"Your tag shows MySQL but your example has SQL server 2019. Which one are you using? Also, if you are MySQL, which version are you using?

I ask because this would be really difficult to do in MySQL before MySQL 8.0.",1.0
g79tp6d,j35d2w,Sorry I meant Microsoft sql server. I am using version 2017 and up. I will update the tag,1.0
g7ag172,j35d2w,"Here's my stab at it.

    ;WITH cte_intake AS (
      SELECT ID, CAST(EventDate AS DATE) AS EventDate, EventName, 
      CASE WHEN EventName = 'Intake' THEN 1 ELSE 0 END AS IsIntake
      FROM EventsTable ) 
    ,cte_episode AS (
      SELECT ID, EventDate, EventName, IsIntake,
      SUM(IsIntake) OVER(PARTITION BY ID ORDER BY ID, EventDate) AS EpisodeNo
      FROM cte_intake )
    ,cte_ending AS (
      SELECT ID, EventDate, EventName, IsIntake, EpisodeNo,
      LAST_VALUE(EventDate) OVER(PARTITION BY ID, EpisodeNo ORDER BY ID, EventDate ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS Ended,
      FIRST_VALUE(EventDate) OVER(PARTITION BY ID, EpisodeNo ORDER BY ID, EventDate) AS Start
      FROM cte_episode )
    SELECT ID, Start AS EpisodeStartDate, Ended AS EpisodeEndDate, EventDate, EventName, EpisodeNo AS EpisodeNum 
    FROM cte_ending
    WHERE EventName &lt;&gt; 'Intake'
    ORDER BY ID, EventDate",1.0
g7cbvzy,j35d2w,"Thank you very much, your solution worked!",1.0
g79v0rj,j35amz,"[Works for me](https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=77e612d78f7889b2036f9c29b8ce5bf7)

Btw: there is absolutely no advantage in prefix table names with `t` or `tbl` or similar. See [here](https://dba.stackexchange.com/questions/154251) for a longer discussion.",12.0
g79wmak,j35amz,"\+1 on this. I fucking hate working on a database that has prefixed everything INT, TBL, CHAR, etc. It's completely pointless and makes writing queries a pain in the arse",10.0
g79xwqq,j35amz,"I usually ask those people if they prefix their Java classes (or whatever programming language they use) with `Class` as well or if they would name their son ""SonPeter"" or a file ""File Invoice.pdf"". 

More often than not, this convinces them.",8.0
g7ah1ew,j35amz,"i always ask people who name tables `t...` or `tbl...` whether they also name columns `c...` or `col...` and they go **""no, that'd be silly""**

and then i give them that [Walter Matthau look from Pelham 1-2-3](https://imgur.com/a/BMODaXB)",2.0
g7d87fz,j35amz,"I'd definitely agree that the benefits don't justify the cost, but I can see situations where it might have some benefit? Just a very fast way to see the data type...

The only time I've actually done something similar is when I have 2 columns with the same value and different formats though..",1.0
g7c8kp1,j35amz,"Agreed. 

But... I will confess that I do put a v\_ in front of my views so that I can tell when looking at the code what I'm dealing with. And I do put a lu\_ in front of static lookup tables (say, lu\_timezones or lu\_quarters that maps months to quarter numbers because my current company has funky fiscal quarters. I have also created custom tables to a data warehouse that I get from a vendor that have prefixes so I know they are mine and not the vendor's. 

Now you may say, 'but Hierophant, you created the damn thing, you should know whether it's a table that you might insert into, a static table, a vendor's table, or a view.' That's fair. And if I were more quick-witted and always coded sober, I'd consider changing my ways. But I'm not and I don't, so to quote the president, ""It is what it is.""",2.0
g7agx5k,j35amz,"It's nice for writing the models for entity framework, though. But yeah I just keep a window open so I can see the column type as I make the models.",0.0
g7c52nr,j35amz,"Ah so I ended up starting a new clean database and it ended up working for me, was so annoying as I stared at this for hours last night.

As for the prefixing table names, I know. Its a weird requirement for the class im taking. Otherwise I wouldn't use them because its a royal pain in the ass and it doesn't look as clean",1.0
g7dizst,j35amz,This piqued my curiosity. Is the code you put into your post directly copied and pasted from where you were working? Or did you retype it?,1.0
g7dultv,j35amz,"Yep, copy pasted. Its weird that it wasn't working for me but I got it figured out.",1.0
g79tdcv,j35amz,[deleted],3.0
g79vg45,j35amz,"&gt;This seems like it would work, right? It gives me an error stating, ""Invalid column name 'intPatientID'.""

What you've given above works for me. Are you sure that there isn't something silly going wrong, like running the INSERTs against the wrong DB or schema?",2.0
g79byed,j32g3r,"You need to install SQL Server Developer Edition. 

SSMS is the client for querying and managing SQL Server. It isn't the actual server.",3.0
g7adew6,j32g3r,"That makes more sense, thanks for the info!",1.0
g79h5wy,j32g3r,"SSMS = Database client that lets you query and manage the database

SQL Server Developer Edition = the database you need to install

&amp;#x200B;

Highly recommend you follow some guide in order to see what needs to be done, something like this:

&amp;#x200B;

[https://www.sqlservertutorial.net/getting-started/](https://www.sqlservertutorial.net/getting-started/)

&amp;#x200B;

Or find a suitable course on Udemy or whatever",2.0
g7adgae,j32g3r,"I'll give that a shot, thank you!",1.0
g79hyel,j32g3r,"SQL Server is required to host the database.

A tool such as SQL Server Management Studio (SSMS) is required to administer the database.

When you open SSMS the name of the SQL instance must be entered.

The link below gives evaluation versions of SQL Server:

[https://www.microsoft.com/en-us/evalcenter/evaluate-sql-server](https://www.microsoft.com/en-us/evalcenter/evaluate-sql-server)

[https://docs.microsoft.com/en-us/sql/database-engine/install-windows/install-sql-server?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/database-engine/install-windows/install-sql-server?view=sql-server-ver15)",2.0
g7adc3y,j32g3r,"Ah, the problem is between the keyboard and chair then. Thanks for the info!",1.0
g79b37l,j30iai,"I cross join a months table to a table with subscription start and end dates to produce a table with 1 row per subscribed month. 

An ""abuse of SQL""? That sounds suspiciously academic. Tools are tools.",3.0
g79hcmu,j30iai,"&gt; where Cartesian Product might be a better approach than Join?

Those are two very different things. I can't imagine a situation where both return the same result (unless both tables contain exactly one row)",2.0
g79hyji,j30iai,I meant Cartesian Product with a where clause.,1.0
g79m3nn,j30iai,"Then it's identical to a JOIN - it just uses implicit joins. It's not a ""cartesian product"" then. 

There are still some people who refuse to use the ""modern"" explicit JOINs which were introduced in the SQL standard nearly 30 years ago ('92).

But performance wise or logically there is no difference at all. It's just easier to accidentally write a _real_ cross join with the ancient syntax if you forget the join condition in the where clause. That's why most people prefer the explicit JOIN operator nowadays.",5.0
g7a475r,j30iai,But LeetCode execution shows Join to be slower than C.P. ... does it have to do with the site's backend &amp; query execution engine?,1.0
g7a9bmy,j30iai,That's hard to believe - at least if both statements are indeed logically equivalent. Which DBMS product are they using? The execution would reveal any differences.,1.0
g7acgen,j30iai,"I attempted both queries in MySQL.
But the results were displayed in JSON. They are probably somehow converting SQL query to programmatic form. Or using MySQL and converting the result set to JSON.",1.0
g78nbvq,j2ym7n,You could do this with cte and the row_number() function.,3.0
g78qkel,j2ym7n,"I'd use a subquery to order the results and get a field counting the number of items. I'm not familiar with Presto but this should do it in MySQL:

    SELECT user, GROUP_CONCAT(itemBought)
    FROM (SELECT user, itemBought, COUNT(itemBought) AS itemBoughtCount,
    ROW_NUMBER() OVER (PARTITION BY user ORDER BY COUNT(itemBought) DESC) AS itemOrder
    FROM tableA
    GROUP BY user, itemBought
    ORDER BY itemBoughtCount DESC
      ) as t1
    WHERE itemOrder &lt; 6
    GROUP BY user
    ;",3.0
g78bvvn,j2x1lw,"Hello u/meatworky - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g7y2pcn,j2x1lw,[Solution](https://docs.microsoft.com/en-us/answers/questions/114882/azure-analysis-services-how-to-add-role-members-to.html#comment-118457),1.0
g786jpt,j2vfpp,I like to use visual studio database projects and localdb for development. I do not recommend using a shared database development model,2.0
g78qy1g,j2vfpp,Liquibase?,1.0
g786zh2,j2vfmz,"Put your strings to match into some table structure (temp, cte, etc.) with priorities, join to your original table, get row number per ""value"" according to the priority, select where row number =1",1.0
g781dvk,j2u3tm,In what way did you find them underwhelming? Perhaps inadequate examples....,1.0
g77k4ch,j2sxz2,"&gt; (Details in the comments)

the details were not in the comments",4.0
g77k7em,j2sxz2,"They are though, I commented above",0.0
g77ki3f,j2sxz2,"sorry, yeah, but there wasn't when i first looked",2.0
g77kzkv,j2sxz2,"So first of all, any website/tutorial that gives you a query like this is questionable at best. 

 - ""people who have the same last name that checked out a book"", but don't reference seems like a badly worded question of dubious worth. 
 - The data model seems suspect, since it's not really properly normalized. There should probably be 1 table for people (to include first name/last name, and other attributes about the people), another table for books, and then a ""junction"" table for books being checked out by people.
 - The query provided uses old-style joins, which are deprecated and warned against by anybody with more than a passing familiarity with SQL. Ignoring any other problems with the query, it should be rewritten to something like this:

```
SELECT DISTINCT a.First_Name, a.Last_Name 
FROM CHECKED_OUT AS a INNER JOIN CHECKED_OUT AS b ON
    a.Last_Name = b.Last_Name 
    AND a.First_Name != b.First_Name
```

There are lots of posts here (not to mention the sidebar) describing good resources for learning about SQL. I would suggest looking at one of those.",3.0
g77loaa,j2sxz2,"But even then, when you duplicate the tables, how would any of the first names not be the same between eachother?",0.0
g77i453,j2sxz2,"Hello u/ratmsoadtmbg - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g77iro9,j2sxz2,"Ok so the question asks you to select the people who have the same last name that checked out a book, and this is the code they provide:

SELECT DISTINCT a.First_Name, a.Last_Name FROM CHECKED_OUT AS a, CHECKED_OUT AS b where a.Last_Name = b.Last_Name AND a.First_Name != b.First_Name

But how do they get it to return as the bottom results? Like how is there any first names between the duplicate tables that aren't the same?",1.0
g77j8cm,j2sxz2,Have you looked at their answer?,1.0
g77jhy4,j2sxz2,That code IS their answer,2.0
g77jsyo,j2sxz2,"I just don't understand how when you're splitting the table into table a and table b, the names wouldnt all be the same, therefore how does that != statment filter anything?",1.0
g77kwts,j2sxz2,"you aren't splitting the table, you are creating a second version of the same table, then it matches all last names, and shows where first names aren't the same.   


so first checks if last name is the same, if that checks yes, then checks if first name is different, if so, return last and first name. 

else go to next record.",1.0
g77lwfz,j2sxz2,"But how would the first name be different when its the same table? And by that I mean since both versions would assumedly have the same data and so if the query is comparing between the two, how would it determine ANY difference? wouldn't all the names be the same?

I'm also under the assumption it's comparing a row in A to the same row in B",1.0
g77mnda,j2sxz2,"so this step: 

    FROM CHECKED_OUT AS a, CHECKED_OUT AS b

creates two tables.   
Table A and Table B , they contain the same information, but read them as two seperate relations/tables.  


first record a vs first record of b, Chiou = chiou, but alex = alex, so fails  
then chiou doesn't equal anything else so that fails.   
Next record of a Lee = Lee, but justin = justin so fails, but Justin !=Tyrell, so print that first and last name...   


repeat until the end.",1.0
g77pgek,j2sxz2,So the code does read it hierarchically? Like it treats table A as the main table and compares each line to all of table B's lines before moving on to the next table A line?,1.0
g77pkm4,j2sxz2,Yes.,1.0
g77rshc,j2sxz2,THANK YOU SO MUCH This helped so much and relieved a fucking week long mental breakdown hahaha,1.0
g77q7ql,j2sxz2,"logically it does them all at the same, although it _could_ be implemented like what you are saying (""nested loops"") in a real system",1.0
g77tzp8,j2sxz2,Just trying to explain it as easily as possible.,1.0
g77poxy,j2sxz2,"Ok, this is simple but you’re making it over-complex, not helped by the rotten question and answer. 

This is a self-join. It’s joining the checked_out table to itself. By doing this, it creates an intermediate table with the following columns:

a.first_name, a.last_name, a.book_id, b.first_name, b.last_name, b.book_id

And it joins these together based on the conditions 

a.last_name = b.last_name 

And

a.first_name != b.first_name

I’m fairly sure you can work out from here how it works. Select distinct from that intermediate table.",1.0
g78hbzl,j2sxz2,"I think the problem is I wasn't seeing it as comparing one first/last name pair to all the first/last name pairs on the other table and THEN determining if it outputs that pair or not, because that's what seems to make sense to me, but is that what's happening with a self-join?",1.0
g7nv70j,j2sxz2,"That's what happens with any join. 'Self join' just means that we're creating a copy of the same table and joining it to itself. It's no different to joining two different tables.

A 'left inner join' (or just 'join') compares every row on the left with the rows on the right, and if they meet the join condition they are joined and the rows are added to the intermediate table.

A left join does the same but if there's no match, the left side is still returned, with NULL values for the right.

Right is the opposite - right side is returned with NULL for the left if no matches.",1.0
g7pieuh,j2sxz2,So there's different types of joins? AND NOBODY TOLD ME THIS? hahaha thank you that helps alot.,1.0
g77hlzb,j2saq7,"Datacamp. Sign up for a Microsoft outlook email account. Type in ""Visual studio dev essentials"" into Google. Activate your benefits. You will get 2 months of Datacamp for free",20.0
g77pvea,j2saq7,"How does this even work? Just click the top link for the ""Visual Studio Dev Essentials"" thing

&amp;#x200B;

Edit: NVM I got it lol",5.0
g79nnga,j2saq7,Cheers for this 👍,2.0
g77lpno,j2saq7,Khan academy has an awesome Intro to SQL course that has interactive sample coding built right into your browser! Free and no program installation needed,8.0
g78aecz,j2saq7,"I second this, it's how I learned also with SQL",1.0
g786fus,j2saq7,"I can vouch for vertabelo academy. You have to pay for the courses but I’ve a ton from their site. They even have courses dedicated to window functions, CTE’s, and some of the more advanced  SQL topics.",4.0
g78e6so,j2saq7,Sqlbolt.com - nothing but praise for that site!,3.0
g78rjbv,j2saq7,Pgexercises.com,3.0
g7845wj,j2saq7,I like Enki,2.0
g791tii,j2saq7,"DataCamp has interactive exercises to learn the basics. Once you understand the basics, also try Strata Scratch to practice SQL problems. This will help you in your interview and in daily work on job.",2.0
g7854om,j2saq7,Code academy,1.0
g78sioj,j2saq7,"To learn SQL with visualized data [https://www.youtube.com/watch?v=gjUNNHlMwWA&amp;t=1s](https://www.youtube.com/watch?v=gjUNNHlMwWA&amp;t=1s) 

Dataset to practice 📔Data Exercises for Beginners

[https://www.w3schools.com/sql/trysql.asp?filename=trysql\_op\_in](https://www.w3schools.com/sql/trysql.asp?filename=trysql_op_in)",1.0
g7az2p3,j2saq7,"YouTube course on database design:  http://youtu.be/e7Pr1VgPK4w

Normalization poster:  http://graphdatamodeling.com/resources/rettigNormalizationPoster.pdf

Free data models:  http://www.databaseanswers.org/data_models/

PostgreSQL Home:  http://www.postgresql.org/

PostgreSQL wiki:  http://wiki.postgresql.org/wiki/Main_Page

Sample databases:
https://www.postgresqltutorial.com/postgresql-sample-database/

https://www.oracletutorial.com/getting-started/oracle-sample-database

https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver15&amp;tabs=ssms

https://dbfiddle.uk/",1.0
g77jwdn,j2rv7t,"the norm?  no

DISTINCT is redundant because of the GROUP BY

the FROM clause is poorly written --

      FROM Kundenstatus
    INNER 
      JOIN Kunden
    INNER 
      JOIN Aufenthalte 
        ON Kunden.kundeid = Aufenthalte.kundeid 
        ON Kundenstatus.kundestatusid = Aufenthalte.kundestatusid 
    INNER 
      JOIN Organisationsstruktur 
        ON Kunden.kundeid = Organisationsstruktur.kundeid 
    
i know it probably works but it makes me cringe

there is much needless repetition -- for example, `AND (Aufenthalte.aktiv = 1)` occurs 10 times

the ANDs and ORs are such a shitshow (that's a technical term) that i cannot even begin to unravel them",5.0
g77ama2,j2rv7t,I wouldn't say this is the normal thing to do. Apparently this person is joining where a bunch of specified conditions are true.  Whomever wrote this query was looking for something specific between the tables he was joining. For readability purposes and probably performance it would almost be better for this person to write what he/she wants in one table as well as in the other table and then join that together. This guy/girl seemed to like to do things the hard way.,6.0
g77iv1q,j2rv7t,"I work with a ton of queries that are that size and bigger but that one is really weird. Usually the where is not even close to that size it looks like they were looking for something really really specific.

I’d have made ctes filtering out each table individually and then joined those, but that’s just personal preference and it’s going to be really hard to understand either way",6.0
g7800af,j2rv7t,That's the most insane where clause I've ever seen. No this is not normal.,3.0
g79kjic,j2rv7t,"It's not usual to write a query that way, but this looks like something that was generated by a CRM system to return some specific list or report, in that case it isn't surprising. The query logic isn't complex, it's just filtering a list of clients for a bunch of criteria.",1.0
g773p0q,j2r43p,Syntax for MySQL and Microsoft SQL is different enough it's probably not worth the time or money.,7.0
g77dhhe,j2r43p,"OK thanks. It looks like their usage/application is significantly different. Although from what I've read MySQL would be a good thing to pick up as well, just not sure as a financial analyst I would use it much, so maybe better to expand my skill set with something else.",1.0
g77fl47,j2r43p,"MySQL is used quite a bit outside of the corporate world.  It sounds like your company is a Microsoft shop so I would focus on T-SQL (Microsoft SQL Server).

The fundamentals are the same in both, as both MySQL and T-SQL support the ISO standard.  But they offer differences unique to their own engines.  So while the fundamentals are the same, they can deviate in it's implementation.",1.0
g77v863,j2r43p,"I see, I got a Udemy course for MySQL, which was like $14 at the time. Now it's like $95 suddenly, so the MSSQL boot camp one is $150. Maybe after a bit it will come back down, usually courses there aren't more than $20 from my past experiences, so not sure what happened. Maybe you get a discount up to a certain amount of courses and then they go to regular price?",1.0
g7892jc,j2r43p,"Can't help you there, I've never used Udemy :)",1.0
g77fkc8,j2r43p,"I’d disagree - focus on learning about joins and indexing rather than syntax, and it’s going to be pretty valuable for what OP does. A few function names doesn’t make that much difference in the scheme of things 

Although if OP can get a full refund and switch to a MSSQL course, obviously that’s going to be a better option",3.0
g77v1y6,j2r43p,Refund. SQL way more popular than MYSQL,2.0
g78fym5,j2r43p,I would say SQL is way more popular in the corporate world. MYSQL IIRC is dominating shares.,1.0
g776g5b,j2r43p,For querying it should be pretty much the same. I think they do select the top 100 rows differently but they’re basically the same,2.0
g77fhu3,j2r43p,I would switch to an mssql one. You’ll also become familiar with the environment which would help in regards to seeing optimisation,1.0
g77443h,j2nu6s,Use power query in excel should be no problem you’ll just update it when you need to and you can save it as with all the data in there. Or if your really really concerned about them trying to do it you can write a 10 line script in python to do it.,3.0
g77e9x1,j2nu6s,"Was guna say, sounds like a job for python",4.0
g7b526n,j2nu6s,What capabilities does python have to do this? I need to do this to multiple files in separate network folders for each user. Could python do something like that?,2.0
g7euvbk,j2nu6s,"Depends on how you approach it, but python can not only run SQL, but then modify your output in excel, email it or place it in a server folder, all with the script. Then you just schedule the jobs in task scheduler and tell your users when/where to look for an updated file. I recently did this with some warehouse data, where I pull the data with SQL,  divide and export to multiple excels, and loop through a bunch of emails to send  particular users their output, all in a 20ish line script.",2.0
g7kx59v,j2nu6s,"That is awesome. Will definitely look into python then! Are you able to format multiple excel sheets with excel tables, filters, etc. The specific formatting part for these reports are what I am having the most trouble with",1.0
g7mj4zn,j2nu6s,"Yep!  Check out the docs for pandas xlsxwriter. You can run sql, generate excel outputs, and then format the excel columns. Not sure about activating a filter per se, but you should be able to filter down to the desired data with sql/pandas before writing it to an excel. If you have a specific example of what you're trying to achieve, it would be easier to guide you to the right resource. 

 https://xlsxwriter.readthedocs.io/example_pandas_column_formats.html",1.0
g78qs47,j2nu6s,agreed - or if you're on Windows you can use PowerShell. The native COM for excel gives hella control on formatting.,2.0
g77zlla,j2nu6s,"Is there a reason you wouldn't use SSRS for this?. Build a report, then you can either schedule a job to create an excel file on the network, or email it, or give the people who need it access to run it on demand.",2.0
g77mllw,j2nu6s,"Does it need to be real time? If so this is a programming problem, not a sql problem. If like once a day or something will work, you could use SQL's exporter and just schedule a job",1.0
g77zsvy,j2nu6s,"As long as you don't require a refresh on opening the spreadsheet then everyone can use it even if they don't have access without a problem.

It will just give them an error if they try but the data that was there before will still be present.",1.0
g765s7b,j2m37n,"Your client should have an import option, create the empty table with the columns and appropriate data types you would be importing, then usually if you right click the table, the client should give you an import option.",8.0
g76et5x,j2m37n,"I tried on 2 popsql and bee studio, neither allow me to import directly. I have made an empty table and can copy the csv rows but would still need to add then manually. It’s over 1000 rows..",2.0
g76f7kl,j2m37n,"Maybe try Sequel Pro, it definitely has the option to import.",3.0
g76effm,j2m37n,"Your post is labeled with MySQL, and if that's correct, there is no INSERT BULK command for MySQL. So that's why you're getting the syntax error in response to that command. (That command is specific to MS SQL Server as far as I'm aware.)",6.0
g76fssy,j2m37n,"Is there no command that would do a similar task, insert rows from a csv into a table basically?",3.0
g76goxg,j2m37n,LOAD DATA LOCAL INFILE will probably work for you: [https://dev.mysql.com/doc/refman/8.0/en/loading-tables.html](https://dev.mysql.com/doc/refman/8.0/en/loading-tables.html),7.0
g7721ue,j2m37n,"Thank you so much, this sent me on the right track to getting it, just had to enable local_infile from terminal. Still couldn’t fix the dates because they follow a DDMMYYYY format, but that’s okay I’ll work with it since its just for myself. Thanks again, really appreciate it.",2.0
g77pjbd,j2m37n,Why not do a Java-Programm thats writes all the data into your Database.,1.0
g77sstg,j2m37n,"I don’t know how to do that, I’ve just recently started learning SQL, luckily I managed to do it :)",1.0
g77slxt,j2m37n,"We have a database that may work for you: [https://www.dolthub.com](https://www.dolthub.com)

We've spent a lot of time making CSV imports easy.",1.0
g768rd1,j2m37n,"Idk if your company has the resources for Microsoft SSIS or Alteryx, they’re relatively easy to import .csv files.",0.0
g76dvpm,j2m37n,Are they paid resources? I’m actually doing some self study so I don’t really have any company backing.,1.0
g76e4g2,j2m37n,"They are paid resources. I know with Alteryx, they have a month trial for the designer version. 
I’d probably use that, insert it into what you need and how you want it. Cancel the trial once it gets close to the month.",3.0
g76euhe,j2m37n,"I might have to try this, thank you",1.0
g769zn9,j2lqco,"it would help to see your table layout (table name and column names) and how the EOM is stored

avoiding joins is always more efficient than doing joins, assuming you don't have to do the joins, which in this case it sounds like, because you mentioned only one table",1.0
g76d0t2,j2lqco,"Select Date,
Company,
State,
Zip,
Sales
From trigger 
Inner join date dt on dt.dt_key= tr.dt_key
Inner Join sales sl on sl.sales_key=tr.sales_key
Inner Join location loc on loc.location_key=tr.location
Where date &gt;= last_day(add_months(date,-12))",1.0
g76j18v,j2lqco,"&gt;  From trigger Join date Join sales Join location

you're using cross joins??? because i don't see any ON conditions

and i don't see your table layout",1.0
g76k2rc,j2lqco,I was lazy to add them... I updated the reply.,0.0
g76owix,j2lqco,"okay, back to your original question

&gt; I can either ... Second, I can ... What would be more efficient? 

what happened when you tested these two methods?

what did your EXPLAINs reveal about efficiency?",1.0
g77v3ye,j2lqco,Much better to do the 4 lags sum those. We are good,1.0
g77akdy,j2lqco,Listen buddy you're talking to a bunch of analysts and DBAs you'll be lucky if we've had 4 dates between us,1.0
g75vqog,j2kmsf,It means the _table_ is empty.,5.0
g75x5nh,j2kmsf,"this implies that 'no data' meant 'no records returned', u/VSauceDealer",2.0
g760b07,j2kmsf,"Yep, I'm not sure what rapid sql is so if OP has selected a field which returns NULL for every record then it might consider that ""no data"". No idea why it would do that but without confirmation from OP no way of knowing",1.0
g7617o8,j2kmsf,"It doesn't say null, nothing shows up at all, only the field name. Below the field name there is nothing, not even a primary key.

Rapid SQL is just a program for sql, just like microsoft SQL.",1.0
g764any,j2kmsf,"&gt; Below the field name there is nothing,

That sounds very much as if the table is empty. 

What do you get for
```sql
select count(*) from the_table;
```",2.0
g7684xc,j2kmsf,"Ahh, should've thought about trying that. I got 0, even tho the query run for 1min.",1.0
g75ppzz,j2j7gf,"

Have a look at “join”, you can include all the columns that need to match (name, id, number?) as part of the join condition then just decide which table to take which columns from. There’s an Intersect clause but this doesn’t seem to be what you want, it means take two sets and only output rows that exist identically in both.",1.0
g75pwv9,j2j7gf,"You’re looking for an inner join: 

Select ID, Name, Height, Weight, Number
From View as v
Inner join live_data as l
on v.id = l.id",1.0
g76a9i0,j2j7gf,sounds like you want a FULL OUTER JOIN because Michael from `View` isn't in `Live_data` and Carolyn from `Live_data` isn't in `View`,1.0
g76o2rt,j2j7gf,"Hi, I actually used ""INNER JOIN""

SELECT a.E\_NAME, a.L\_NAME, a.REGION, b.datetime

FROM table1 a

INNER JOIN table2 b

ON a.E\_NAME=b.E\_NAME AND

a.L\_NAME=b.L\_NAME

order by b.datetime desc;

&amp;#x200B;

Thanks a lot!",1.0
g76p11p,j2j7gf,so you're just ignoring rows that aren't in both tables?,1.0
g76rilc,j2j7gf,Correct. I wanted that so I can type now a where statement and filter easily on a.region,1.0
g75wyk0,j2j5hw,"unpivot your records using column names as categories in the unpivoted result, use row_number to find 1,2,3,... etc. value",1.0
g75zpvx,j2j5hw,"Hi, thanks for your comment

That was my original thought but the SQL statement is up to 1240 lines and I'm working temp tables, and nested sub queries so the whole thing would get very messy very quickly

I'm just trying to get ideas as to work this our by keeping the same data format, it might just have to be the mother of all case statements... unless you can share an alternate solution

My data has an ID in col1, then columns 2 - 50 contain variable values. I can get the first value by using GREATEST() function, but getting the second and third is tricky. I had thought about a case statement to say if a column is the greatest then set it to 0, and then run a greatest on all other columns.

Anyways, thanks for your comment",1.0
g761ps0,j2j5hw,"&gt;  an ID in col1, then columns 2 - 50 contain variable values. 

what can I say? it appears you have used columns to represent an array or list-like structure. It is a common anti-pattern. My recommendation would be to retreat some steps and rework your statements/datasets to be more aligned with set-based logic and pivot data to individual columns in your presentation software/layer.",1.0
g75bzcf,j2g35j,"Disadvantages compared to? Maintaining unnormalised and redundant data?

Querying and processing takes longer with data in seperate tables, it's also potentially useful for working on unstructured data.

Apologies if I misinterpreted the question. Almost all table relationships are many to one.",3.0
g75ctah,j2g35j,Thanks that helped! So an example of longer query is making a many-to-many relation table into to a one-to-many by adding an extra table as a junction?,2.0
g75f8i6,j2g35j,"Exactly. Relational databases won't do many to many relationships and need to broken down exactly as you say with the extra table. 

Understanding normalisation is absolutely  key to understanding how/why to structure data and tables, the Wikipedia article is pretty good: [https://en.wikipedia.org/wiki/Database_normalization](https://en.wikipedia.org/wiki/Database_normalization)

If you understand that the rest is easy.",2.0
g75iy6d,j2g35j,Your the best. Thanks!,2.0
g75duav,j2g35j,"It’s *one-to-many*, not *one-too-many*. So no. 

(SCNR.)",-3.0
g74obom,j2cvr6,"If you know python, maybe check Spark?

SSIS is what I use everyday, but for another project we use Spark. I like both.

Edit: By the way, SSIS is a nightmare with version control like GIT or TFS if you are several people working in the same package",3.0
g74ofb2,j2cvr6,I’m new to SSIS. Can you provide some resources on setting it up and the advantages of using it please? Thank you.,1.0
g74pq73,j2cvr6,"Google and Youtube are your friends. 

But seriously, I started in a company with a big ETL in SSIS. So I never really started from scratch.

My first month was a crash course with some videos on Plurasigh (paid by the company and luckily coffee too).",4.0
g74pw6s,j2cvr6,Nice thanks! Do you think SSIS is still prevalent and is the go to tool for ETL? What advantages does it offer over ADF?,1.0
g74qaq3,j2cvr6,"I think Microsoft want people to migrate to Azure Data Factory eventually. Technically, you can run SSIS packages in it. In the background, ADF use Spark for some components (I think).

I use it for small projects. Cost/perf can be an issue.",1.0
g7604sf,j2cvr6,Do you recommend the Pluralsight course(s)? I watched a few things on Lynda since our company had free access available but was not impressed.,1.0
g760f8b,j2cvr6,"If your company pay for coffee, maybe...

I prefer tutorials in text.",1.0
g757w6v,j2cvr6,That version control comment is truth. I love working with SSIS but it truly is a mess if you work on a team.,1.0
g75sroo,j2cvr6,"We find some solutions. We use TFS and we lock dtsx files when edited. Usually, we try to sync our work to not be more than one needing the same file.",1.0
g766quz,j2cvr6,That makes sense,1.0
g74j1gd,j2c1c5,"Foreign, primary, surrogate, natural, composite, durable.",51.0
g74jgp2,j2c1c5,His guy selects.,18.0
g74k393,j2c1c5,But where is he from?,15.0
g7595gi,j2c1c5,"I don't know, but we should join him.",13.0
g75b048,j2c1c5,Maybe I am dense but I want a rank after joining him,7.0
g75c3va,j2c1c5,I want to be among the Top after joining.,7.0
g75ejpn,j2c1c5,I suggest we all form a union,9.0
g75uz7k,j2c1c5,Just in CASE WHEN you all decide let me know.,7.0
g7601jl,j2c1c5,I'd like to CAST my vote here too,6.0
g769xks,j2c1c5,Can we make this a group effort?,4.0
_,j2c1c5,,
g74qy70,j2c1c5,"Make sure you have enough memory, to remember it all. 

But on a serious note, try answering questions on Stackoverflow or DatabaseExchange.",4.0
g74r9ti,j2c1c5,Thanks,1.0
g764o7v,j2c1c5,"Based on your statement, I'm guessing you're possibly on operations side? If so, W3schools is extremely basic and is what I used to start. It helps to have a basic understanding of general coding syntax, though. If you're on IT side, I would definitely look into some schooling or some of the more technical sites that other people have linked.

I started off knowing no SQL and having no formal training in it. Now I'm well-compensated and am one of the main go-to people for sql reporting at my company. You'll have to work at it, and the more technically-minded you are, the easier it will be, but just getting the basics down and then having a read-only DB to play around in does wonders.

I learn something new all the time, usually whenever someone asks me (or I ask myself) how to make something better. Google is your friend. There are some reports I made even just a few years ago that I revisited recently and, while not terrible, were not optimized as well as they could be, and I'd thought of ways to add things to them that would enhance their usefulness. 

Practice, learn, and grow. If there are colleagues that are willing to show you some of their own queries, take them and deconstruct them to figure out how they work, and google the functions they used to get a feel for how the parameters work for each one and what the purpose of it is. Find ways to improve their query or to add something beneficial to it.

Good luck!",3.0
g7519jv,j2c1c5,[https://www.youtube.com/watch?v=mHWb16MvyMs](https://www.youtube.com/watch?v=mHWb16MvyMs) If you already know some Excel,2.0
g75r1t5,j2c1c5,"Easiest way to get started is to install sqlite3 locally.

You'll have some learning curve before you get used to the prompt (.tables, .schema, etc), but recent versions of sqlite are pretty feature-complete (window functions for example).

Start with a single table and practice selects, case and aggregates.

Then join multiple tables together.

Now do some window functions, I use dense\_rank all the time, for example.",1.0
g7655r1,j2c1c5,"What level are you at? Youtube has some really good videos that you can start with like this one 

https://youtu.be/HXV3zeQKqGY",1.0
g765rux,j2c1c5,I’m a beginner beginner. Ha,1.0
g766c7h,j2c1c5,Then the video is a great place to start.,1.0
g75hj2h,j2c1c5,"select-&gt;from
case-&gt;end
count-&gt;having
join-&gt;on
pivot-&gt;for
row_number-&gt;over
with-&gt;, 
and, or-&gt;()",1.0
g75ojy8,j2c1c5,"    FROM  

    JOIN (S) 

    GROUP BY  

    HAVING  

    all the expressions in the select at once  

    DISTINCT 

    SELECT 
   
    ORDER BY 
   
    OFFSET FETCH/ LIMIT / TOP",3.0
g76f8vc,j2c1c5,"i mostly focused on syntax word pairs that are most of my students usually take time to remember from the top of their head, like that CTEs need to be separated by comas or pivot's magic word is FOR while partitions is OVER. these things take time to stick more then the usual select from join where group by etc....",1.0
g76o1vj,j2c1c5,"Yeah, but you get those are just a Google search away. Logical order of processing helps you write what you want to achieve easily. If you are a beginner this is more important imo",1.0
g750stl,j2c1c5,If you are interested in SQL Server DM me.,0.0
g74twz9,j2b3p4,"&gt; I'm seeing that my ""actuals"" data are not being aggregated

...because it is literally not in your sql? (i.e  while you use sum() for ""af."" fields, you are using am. fields straight up).

also, why are you grouping by so many measure-type fields? if it has been just a crutch to get rid of the errors, it's a poor tactic and probably will catch up with you maybe in this statement maybe in your next.",1.0
g74fam0,j2b2he,"Sqlfiddle might be a good choice for you. You do not have to create a db, but you can create tables and do all the rest. 

http://sqlfiddle.com/


If you are used to python, a jupyter notebook with the sql magic and sqllite may also be a good choice.",2.0
g74k7sv,j2b2he,sqlzoo.net is my personal favorite. It is all browser based and gives you instant feedback.,2.0
g74k8or,j2b2he,"**I found links in your comment that were not hyperlinked:**

* [sqlzoo.net](https://sqlzoo.net)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g74k7sv&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1.0
g7giei4,j2b2he,"Thanks yo, these little quizlets are fun, right at my level.",1.0
g758oh1,j2b2he,"Here are 6 sites with practice SQL with built in databases, no need to install anything

With Exercises

https://mode.com/sql-tutorial/

https://www.hackerrank.com/domains/sql

https://www.sql-ex.ru/

Just some data to explore

https://fivethirtyeight.datasettes.com/fivethirtyeight - choose a dataset, select ""Edit SQL"",  write SQL

https://data.stackexchange.com/ - choose a site, review other people's queries, ""Compose Query"" to write your own

Just an environment, bring your own data

http://sqlfiddle.com/

--

Some other options for you since you know Python

There's a SQL course on Kaggle where you learn against a free Google BigQuery instance using Python and magic commands for SQL ..

https://www.kaggle.com/learn/intro-to-sql

And here's a binder to spin up a Google Colab and Jupyter notebook to interact with SQLIte


https://mybinder.org/v2/gh/jupyter-xeus/xeus-sqlite/stable

--

If you can install docker, there are  images with database software and preloaded datasets available

https://github.com/aa8y/docker-dataset

Just database management system (DBMS)

https://hub.docker.com/_/mariadb

https://hub.docker.com/_/microsoft-mssql-server

--

Lastly, I realize it's a struggle sometimes to just get started. If you need any help installing tools, getting data, or just what box to write your code in, keep posting here! We were all there once.",1.0
g73ygx7,j27stj,"Perhaps your table / index statistics are not up to date. If the column is indexed ( I've seen primary keys not indexed ) with usable table stats - then SELET COUNT( PKCOL ) FROM TABLE; should run quickly...



That being said --- why are you breaking up the count() into sets?",8.0
g73zt7b,j27stj,"Is the fact I'm currently INSERTing into those tables 100-200 rows/sec not letting update my table / index statistics and therefore making COUNT() longer ?

&amp;#x200B;

&gt;That being said --- why are you breaking up the count() into sets?

I wrote the command like that because I found it on some forums and it's the first thing that worked to count all rows in all tables.

Also, I see reddit changed my code for some reasons here is a screenshot of it : [https://i.imgur.com/TrfsrkR.png](https://i.imgur.com/TrfsrkR.png)",1.0
g741oyg,j27stj,"So, the 12 tables feels like you are trying to optimize too soon. These should be in one table.

If you are going to spread out the records ( and so divide the table and index buffers across 12 tables instead of one ) then you need to be aware that partitioning is really focused on increasing I/O. If you don't have disparate tablespace storage then you are unlikely to see benefits.",2.0
g742g90,j27stj,"If having multiples tables is that much of a problem, I'll merge them back once the current script filling them is over with it. I thought it would be the opposite, that tables being smaller, searches would be faster.",1.0
g76t50b,j27stj,"If you are going to be searching the tables _individually_ frequently, then perhaps splitting them would be a fair idea.

So far I'm not seeing a reason why you should, 48m rows isn't that big, and by splitting you have cost yourself a critical feature that would should give near instant results for the data you are querying. 

If the data was in a single table the query planner could simply return the row count from table statistics or index leaf count - which would bypass table locking contention.

By dividing them manually ( instead of utilizing a PARTITION ) you likely reduced the speed of access at you have disks having to seek multiple times per table / index instead of a few for one table / index.


You've done what many of us have done before ( myself included ) - attempted to optimize too early. Don't feel bad -- you can test the solutions against each other by created a new table and running a INSERT INTO SELECT cols FROM (TABLE1-12) and then seeing if performance is worse ( make sure to add your PK, indexes to match the existing tables, and run table stats ).",1.0
g75xov6,j27stj,"so to be clear, are you querying the table while inserting values in it ? if you do, likely the issue is your query is waiting for write lock to release before reading the table.  also how big is the table in term of row size ?",1.0
g74a2wx,j27stj,"This isn’t something you would generally do so performance shouldn’t be too important, and splitting your table into several other tables means you will have to mention all of them whenever you want to query it (unless you happen to know which table the correct table is). I’m not sure how valid the advice you’ve seen was. But anyway, this is a simple question - how long does it take to read X amount of data? With half the task being figuring out how much data needs to be read. Now, you’ve got indexes supporting a non-null column on each of these tables so (assuming there’s no tricks to avoid actually counting the rows), you’re going to need to read the entirety of those indexes. The size of those indexes can be queried (google it for MySQL) or we can take a guess based on your count - 43.5million rows. If you’re storing this column in an int data type then that’s 4 bytes per row, 43.5million *4 is 174million bytes, if we assume about 20% (pulled out of thin air) of your index is used for overhead that gives us 217.5 million bytes. MySQL will store this by default in 16k pages, so you have to do about 13,600 page requests (I can’t see anything about MySQL doing parallel/multiblock requests but happy to be corrected). Worst case scenario, this needs to visit your storage each time, and if this is taking 34 seconds then you’re doing about 1 read per 2.5 ms. Is that fast? Not in 2020, but in reality your indexes are going to be bigger than I guessed and there’s every possibility that your system is executing the query differently to just reading some indexes. Luckily, this sort of query is completely unrepresentative of what you should really be querying. I have multiple multi-TB tables in my production DBs, does the time it takes to count the rows in them matter? Absolutely not. Does the time it takes to fetch the handful of rows that fit the real query filters matter? Yes, and that has absolutely no relation to the time it takes to count every row.",5.0
g75xpy4,j27stj,"  I would generally do this with a CTE IN SQL Server (though actually I would get this info from `sys.partitions` or a DMV that already holds this information rather than counting) 

Syntax may need a tweak and Reddit did something odd with the code and it's too annoying to edit on mobile 

    WITH CountOfRows (rcount) AS (
    SELECT COUNT(id) FROM usersDB.\`0to50m\`
        UNION 
    SELECT COUNT(id) FROM usersDB.\`100to150m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`150to200m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`200to250m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`250to300m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`300to350m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`350to400m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`400to450m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`450to500m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`500to550m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`50to100m\`
        UNION
    SELECT COUNT(id) FROM usersDB.\`550to600m\`


    ) 
    SELECT SUM(rcount) FROM CountOfRows;",1.0
g7644w3,j27stj,"This won't solve all your problems, but just as a tip so you're aware, `SELECT COUNT(id) ...` is logically identical to `SELECT COUNT(*) ... WHERE id IS NULL`. Since `id` is a primary key, it's already guaranteed not to be null. So why the extra check?

I have a table at work (also MySQL) that has 48 million rows, so I ran a quick test to compare `COUNT(id)` vs. `COUNT(*)`. [https://i.imgur.com/UJuMMuS.png](https://i.imgur.com/UJuMMuS.png) \- I'd ignore the first run, but for the rest, you can see `COUNT(*)` is consistently a bit faster.

So again, this won't solve all your woes but you may be able to shave off a bit of unnecessary query time by modifying your query to `COUNT(*)`.

Another thing is I find beyond 99% of the time that:

1. Splitting a table into n tables is a bad idea. With most databases, there's some separation between the physical and the logical, so you could have one logical table with physical characteristics that help it run faster for your use case. For example my go-to thought when somebody says they want to split a table into several is have they looked into table partitioning? But is there even a tangible problem that this splitting is solving in the first place? i.e. I doubt any splitting or partitioning is going to help with this counting you're doing (although it may help with other queries, depending).
2. Making ANY architectural decisions based on something you ""heard"" is a bad idea. Identify the problem (are you even encountering it)? Look at the ways in which your database of choice solves this problem. Then test test test.

A final thought, to answer your main question, yes, it's totally feasible for counting a large-ish number of rows to take a long time. Spending a long time running a count should not in and of itself be a cause for concern. Sometimes people say, ""Oh it's such a simple command - just get a count. Why is it slow?"" It's a simple instruction sure, but there can be a lot of work behind the simple instruction. Imagine you work in a large Amazon warehouse that has practically everything under the sun. I'm your manager and I say, ""Get me a count of every item in the warehouse."" And I don't want you to just look up the number in an inventory system - I don't trust it - I'm telling you to really physically count everything, one by one. Could take you weeks. That's essentially what a query like this is doing.

Now if you had a database that supports parallel queries (MySQL doesn't) you could have several warehouse workers counting at once. Using Postgres on my Macbook Pro (so no hardcore server here), I just counted the same 48 million rows in 3 seconds with a parallel query with 6 workers...

    QUERY PLAN                                                                  
    ---------------------------------------------------------------------------------------------------------------------------------------------
     Finalize Aggregate  (cost=360459.03..360459.04 rows=1 width=8) (actual time=2996.822..2996.823 rows=1 loops=1)
       -&gt;  Gather  (cost=360458.41..360459.02 rows=6 width=8) (actual time=2996.581..3000.314 rows=7 loops=1)
             Workers Planned: 6
             Workers Launched: 6
             -&gt;  Partial Aggregate  (cost=359458.41..359458.42 rows=1 width=8) (actual time=2987.534..2987.534 rows=1 loops=7)
                   -&gt;  Parallel Seq Scan on test  (cost=0.00..339458.73 rows=7999873 width=0) (actual time=0.109..1763.749 rows=6857143 loops=7)
     Planning Time: 0.175 ms
     Execution Time: 3000.404 ms

(And as an extra bonus, I tweaked some memory settings so Postgres wasn't using out-of-the-box-defaults, and it speeded up to 1.7s, but you get the idea.)",1.0
g73vo8v,j26spa,"Use SELECT DISTINCT

PERCENTILE CONT is a row level function, like ROW_NUMBER(), so its returning a result for each row in your dataset",1.0
g75vxg8,j26spa,Yes! thanks!,1.0
g73v2e8,j234xb,"&gt; and exclude the last column as I do not need it

There is no shortcut to do that. You will have to explicitly list all columns in your SELECT statement. 

If you consistently want to query that table and exclude one (or more) columns, then create a view that only returns the columns you want and use that view instead.",3.0
g75jvzk,j234xb,"Hi, thanks for the clarification.",1.0
g72xpxe,j234xb,"That's not how CTEs work. 

I'm pretty sure there's no way to do it as you're attempting. 

What's the use case? If you need all but one column why not `SELECT *` then close your eyes when you get to that column? 😆

I'd look into [Dynamic SQL](https://www.postgresql.org/docs/9.1/ecpg-dynamic.html) if you're dead set on trying to make it work though.",2.0
g72ynr1,j234xb,"Hi, thanks for the reply.  
Imagine I have a table with 80 columns and I need only 75.  
It is easier to mention 5  columns that I do not need than specifying all 75.  


How should  I tackle such situation?  
I thought there was a way to use subquery/CTE to get names of relevant columns and put them the query after SELECT statement.",1.0
g73ihqf,j234xb,"I would probably discourage this practice for production code as the guarantee made to the client weakens from “you’ll always get columns a,b,c” to “you’ll get whatever columns except x,y,z”.

The way I’ve addressed this in the past with tables containing hundreds of columns is by writing bash or python scripts to generate and save the query.",2.0
g75jvfx,j234xb,"Hi, thanks for your comment!",1.0
g72gdzh,j1y5oh,That seems like a great solution! I can help you with the testing.,3.0
g72ptka,j1y5oh,Awesome! Thank you. I’m PMing you the link. Looking forward to connecting,1.0
g74uahq,j1y5oh,"I have similar issues, I'd be willing to give it a try.",3.0
g74v8jj,j1y5oh,"Awesome! Well not for the similar issue, but excited to have you use and share feedback on the platform! PMing you now",2.0
g74vigm,j1y5oh,Please let me know if you got the link!,2.0
g74w7t2,j1y5oh,"I did, thank you.",2.0
g72r5wy,j1y5oh,I’m interested in this.,2.0
g72sb2p,j1y5oh,Sent you the link! Can’t wait to hear your thoughts on it :),1.0
g72yy7h,j1y5oh,Sounds interesting indeed.,2.0
g7323bb,j1y5oh,Sending you the link!,2.0
g73axn1,j1y5oh,"Interested, we’re just about to embark down a data capture and discovery specifically for documentation. would love to try it out!",2.0
g73be9a,j1y5oh,"Sounds like a perfect fit! It’s definitely a more basic tool at this point designed for small teams not at the enterprise level yet, but I’d love you to check it out. Sending the link now.",1.0
g73b920,j1y5oh,interested,2.0
g73bh6z,j1y5oh,Sending you the link! Appreciate it 🙏,1.0
g73kyqa,j1y5oh,Definitely interested in this.,2.0
g73lics,j1y5oh,Great!!! Sending you the link,1.0
g73ps6h,j1y5oh,100% would love to see this.,2.0
g73tvkl,j1y5oh,Sending now in PM!,1.0
g73ybi3,j1y5oh,Yes! Would love to try this,2.0
g74rtnb,j1y5oh,Sent you the link!,1.0
g74bo8l,j1y5oh,This sounds awesome! I'm interested in trying it.,2.0
g74rphn,j1y5oh,Sent you the link!,2.0
g74efbt,j1y5oh,Interested!,2.0
g74rshk,j1y5oh,Sent you the link!,1.0
g74t3g9,j1y5oh,I'm interested to see this.,2.0
g74v0yl,j1y5oh,Sent you the link.,1.0
g74t7c6,j1y5oh,What does the data dictionary part do?,2.0
g74v5r9,j1y5oh,"Currently, the data dictionary just stores definitions of the data and allows users to comment and share insight at the column level in a table, I’m working on a feature to track column data changes over time, but it’s not yet deployed. I’ll send you the link in case you are interested :)",1.0
g74v7ml,j1y5oh,Thanks!,1.0
g769r1j,j1y5oh,i'll take a look as well too please. Thanks for sharing,2.0
g778qk2,j1y5oh,Sent it! Please let me know if you received it,1.0
g77d8q2,j1y5oh,"yup, got the DM
Thanks",1.0
g7737gi,j1y5oh,Interested!,2.0
g735e6n,j1y5oh,Interested!,1.0
g73a5s5,j1y5oh,Sending you the link!,2.0
g73v0h2,j1y5oh,"Really excited for all the interest! I pm’d everyone the link. I’m trying to limit users as I get started to ensure I can fix all bugs in a timely manner. After a few more beta testers, I’ll share the link publicly here.",1.0
g71yyic,j1y2lp,"As you and u/Wiccawill420 note, you need a `CROSS JOIN`. Since you're using MS SQL, appropriate documentation is [here](https://docs.microsoft.com/en-us/sql/t-sql/queries/from-transact-sql):

&gt; CROSS JOIN: Specifies the cross-product of two tables. Returns the same rows as if no WHERE clause was specified in an old-style, non-SQL-92-style join.

So you could do the following:

```
SELECT A.Name AS A, B.Name AS B
FROM Ingredients AS A CROSS JOIN Ingredients AS B
```

Note that you will have cases where both A and B are the same ingredient. You'll also have cases where the same combination is repeated, but reversed (e.g. 

```
apples, bananas
bananas, apples
```

To take case of both of these, you could use a regular `INNER JOIN` and specify the appropriate criteria in the `ON` clause:

```
SELECT a.name AS a, b.name AS b
FROM ingredients AS A
    INNER JOIN ingredients AS b ON
        b.name &lt; a.name
```",9.0
g71z6e5,j1y2lp,upvote for `b.name &lt; a.name`,5.0
g7252k8,j1y2lp,how does the logic behind `b.name &lt; a.name` work? I've only ever used &lt; and &gt; with numbers.,1.0
g72edcq,j1y2lp,"Pretty much the same as numbers, by comparing them (lexographically in this case). Per the [documentation](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/comparison-operators-transact-sql):

&gt; Comparison operators test whether two expressions are the same. Comparison operators can be used on all expressions except expressions of the text, ntext, or image data types.",2.0
g72huld,j1y2lp,"they work almost the same way on character data as on numeric data

the exception is that numeric data is evaluated as a single value, whereas character data is evaluated one character at a time, going from left to right

classic examples -- words in a dictionary, names in a phone book",2.0
g72it7z,j1y2lp,"somewhat related, somewhat offtopic -- ""Zoomers don't remember this but the phone company used to send out a book once a year that doxed everyone in your neighborhood."" @sigh_ontology on twitter",2.0
g724yii,j1y2lp,"The only thing I would change in your query would be the column aliases... you use the same A and B for both the tables and the columns. Might be easier to read if the column aliases were IngredientA and IngredientB. Other than that, totally agree!",0.0
g722lqk,j1y2lp,This should’ve been my moment to shine but I was late to the party,6.0
g727y8k,j1y2lp,"Damn, that would have been glorious.",2.0
g72dabn,j1y2lp,That's `RIGHT`: You got `LEFT` behind. Perhaps you should do some `INNER` reflection as to what your priorities are. But not all is lost: Maybe you can still provide a unique perspective on the problem from a `LATERAL` point of view.,-1.0
g72j4nn,j1y2lp,"&gt;100 names ... I need to find all possible combinations  
  
You know that's a really really really big number right?",3.0
g72nq42,j1y2lp,"It looks like he only wants two ingredient combos, so it's only 10,000. It'll be fewer if he removes duplicates.",1.0
g73mwty,j1y2lp,"It’s still a very big number. In fact, is 100!",1.0
g71wg1f,j1y2lp,"What you're looking for here is a cross join. Join the table to itself with no 'ON' statement and sql will join every line to every other line. Make sure to use table aliases

`SELECT a.Name, b.Name`   
`FROM Ingredients as a`  
`JOIN Ingredients as b;` 

Give that a try",-1.0
g71z7sv,j1y2lp,"Won't work, since an implicit `JOIN` is understooed as an `INNER` JOIN, which requires the `ON` clause. You'd need to explicitly use `CROSS JOIN`.",2.0
g71x7vk,j1y2lp,"I have to use `AS` when assigning the aliases. So when I try to do the cross join, it gives me a syntax error :(",1.0
g71xrhe,j1y2lp,"if you want a cross join, i suggest using `CROSS JOIN` -- an inner join with no ON clause is a hack

    SELECT a.Name AS IngredientA
         , b.Name AS IngredientB
      FROM Ingredients AS a
    CROSS
      JOIN Ingredients AS b",3.0
g71yh3n,j1y2lp,this worked! thank you!,1.0
g71y7gr,j1x08r,"A relatively obvious observation is that you couldn't have known in your business rule that deposit1 will align with debt1 and debt2, so associating portionlimit at that level seems to be artificial.

Since you are starting from this, there's a pretty good chance that this is an xy problem. My recommendation would be to retreat a step or two and spell out business rules for portionlimit.",4.0
g72iuov,j1x08r,"Definitely XY problem. This is a common accounting software issue, having a transaction be distributed over multiple accounts. The requirements should be spelled out clearly before development begins.",6.0
g71z18j,j1x08r,"I would handle this with a third table, something like transactions. columns would be like TransactionID, DebtID, Deposit ID, deposit amount, deposit percent. I would use this table to allocate each deposit to the various debts and then make a view that does the math on that.",5.0
g720iim,j1x08r,"* What would you like the end result set to look like? 

* What happens with 20 from Deposit1, since only 50 can be allocated on debt1 and 30 on debt2? Similarly with the 10 on Deposit2 where 40 is allocated to debt out of 50?

Recursive query going through each deposit in order (of date I guess, but you don't have it, so I'm going by depositID) will give you the debt balance easily:

This is SQL Server's T-SQL dialect, but I think SQLite supports recursive CTEs:

    ;WITH DepositIteration AS
    (
        SELECT ROW_NUMBER() OVER (ORDER BY DepositID) AS iteration
             , DepositID
          FROM Deposit
    ),
    Calc AS
    (
        SELECT 1 AS Iteration, DebtID, CONVERT(DECIMAL(10, 2), DebtAmount) AS DebtBalance
          FROM Debt
         
         UNION ALL 
        
        SELECT calc.Iteration + 1, calc.DebtID, CONVERT(DECIMAL(10,2), Balance.Remaining)
          FROM calc
          JOIN vDebtDeposit D2D
            ON D2D.DebtID = calc.DebtID
          JOIN DepositIteration 
            ON D2D.DepositID = DepositIteration.DepositID
           AND DepositIteration.iteration = calc.Iteration
         CROSS
         APPLY (SELECT D2D.DepositAmount * D2D.PortionLimit AS ContributionLimit) x1
         CROSS
         APPLY (SELECT CASE
                         WHEN DebtBalance &lt; x1.ContributionLimit
                         THEN 0
        
                         ELSE DebtBalance - x1.ContributionLimit
                       END AS Remaining
             ) Balance
    )
    SELECT * FROM calc
    ORDER BY iteration, debtid
    
    
[Working demo](https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=7db3b22fc3d4ce96128a320b77408cf2)",2.0
g8fmtgk,j1x08r,"Hi, sorry to take so long to get back to this. To answer your questions:

1. I would like the end result set to be a list of ""sub-deposits"" for each Debt-Deposit relationship. So that I can create a View from this and use it to track the progression of a Debt being payed off over time.
2. Any leftover values from Deposits are just that, leftover. Nothing happens with them.
3. I left out any ordering for the sake of simplicity. But I mean for the Deposits to be ordered by date, and the order of the Debts to be set via the application.

I've attempted to get your solution to work on my end. But I don't think SQLite supports this `CROSS APPLY` clause",1.0
g8j4i7i,j1x08r,"You don't need cross apply, it's nothing more than sugar in this case, you can just as easily move that `CASE` into the main query, here's a working [demo on SQLite](https://dbfiddle.uk/?rdbms=sqlite_3.27&amp;fiddle=78cebed773a36d736c7eda364c06097a).",1.0
g7265wu,j1x08r,"So I feel like you're post here lends to the argument I've made countless times on this sub, that sub queries or even CTEs, can become unmaintainable extremely fast, as your requirements grow and you need to introduce more business logic.

I think I understand what you're after, but can you just explain it to me in layman's terms? Semi technical, but what are you trying to do?

Because, this feels like a temp table and separate update statements (I'm thinking with ranks on both debts and deposits) would be the right solution. 

I've been solving problems like this, or maintaining existing scripts that handle things like this, as well as reviewing other's work, and temp tables have always seemed to work the best for things like this.",1.0
g72rrj6,j1x08r,"If you can add a field for the deposit as the owner ID of the lead deposit item. Also you  into a problem where multiple deposits are made within the transaction with one debit. 

I agree they need to be ranked but also consider the above situation. You should be able to accomplish what you are looking for with transaction order.",1.0
g72f0n3,j1x08r,Any reason you can't put this logic in your application code instead of trying to write a SQL statement?,1.0
g72g9ax,j1x08r,"I totally agree with the other posts that this is something that ought to be far more carefully specified.

But even if it were, it seems like the kind of thing that should be done in a true procedural language rather than force-fit into a SQL query.",1.0
g71u8m6,j1uc52,"The ways I have used it is with rank and dense rank.  

Think of it with a couple groups of people at work, you wanted to rank both groups separately and see who is at the top of both groups, you would rank them by let's say reputation and order by reputation desc and group by employee group.  It essentially splits your ranking into many little rankings from the same data.",2.0
g71wgok,j1uc52,"the use case: the ""old-school"" SQL wasn't great about mixed granularity/level-of-detail calculations, requiring extra group bys/joins to mix granularities; it wasnt also meant to process things in order - like running totals, also making something as common as finding N-th element a multi-level subquery ordeal.

So that's where &lt;analytical function&gt; OVER (...) comes in - to calculate an ""aggregate-like"" function in a different granularity from the main query  - i.e. I want all individual sales (granularity 1) as well as the total sales per region (granularity 2) in every single output record.

the gist would be that in the OVER clause ""PARTITION BY"" defines the granularity of your calculation (""per region"" part from the above) and ""ORDER BY"" allows for ordered execution (the simplest example being row_number - giving you a record number in some specific order so you can easily solve pesky n-th record questions).",2.0
g71ff77,j1uc52,"Think of it as aggregating with a group by without having to actually group the data together. If you do: select deptno, avg(sal) from emp group by deptno; you get the average salary for each department but because you are grouping you can’t see details about the rows that belong to each group (only at an aggregated level). If you use the analytics form, you can: select empNo, name, Sal, avg(Sal) over (partition by deptno) avg_sal_for_dept from emp; You can also just partition by null, meaning the aggregated data is for over the whole data set.",1.0
g71sseu,j1uc52,"Analytics queries allow you to split/group and then analyse the data in different ways to normal 'group by' statements.

This is my favourite link to explain the basics as it explains how to get the same result using non-analytic queries, even if it is simplistic:

https://codingsight.com/grouping-data-using-the-over-and-partition-by-functions/",1.0
g723uos,j1uc52,I found this [video](https://vimeo.com/289497563#t=16:50s) from Markus Winand very helpful.,1.0
g725rkf,j1uc52,It's like grouping data without filtering the data. It'll create a new column which gives the aggregation to each row for the group that row belongs to.,1.0
g73fv03,j1uc52,"[https://www.youtube.com/watch?v=KwEjkpFltjc&amp;list=PL08903FB7ACA1C2FB&amp;index=109](https://www.youtube.com/watch?v=KwEjkpFltjc&amp;list=PL08903FB7ACA1C2FB&amp;index=109) 

This really helped me in understanding the over clause concept.",1.0
g71ntz1,j1sej2,"&gt; Another option is to have a row for each relationship
&gt; ...
&gt; Person&gt;Bob is repeated 3.

so?  Bob has three relationships, so you ~have~ to store ~something~ three times

pay no attention to red herrings about requiring ids instead of names -- if 'Bob' is a Primary Key (and all that this implies), this one-row-for-each-relationships approach is the best

now all you have to worry about is whether the relationships is **reflexive**, i.e. if Bob is related to Dave, but Dave is ~not~ related to Bob",3.0
g714ze3,j1sej2,"Ideally you would have a third lookup table with all the people and their unique ids and build the relationship table with the ids instead of actual names. 

The first way you posted would be difficult to work with because it’s not exactly structured, they are comma separated strings.",2.0
g71zruw,j1sej2,"&gt; The first way you posted would be difficult to work with because it’s not exactly structured, they are comma separated strings.

To expand on this a bit, using comma-separated values opens you up to a number of potential issues:

 - There's no referential integrity. So a user could insert invalid values, like `Carl, Dave, Frank, Juliet`.
 - What if `Dave` changes to `David`? You'd need to manually update the comma-separated values, which would be a pain.
 - Querying the relationships would be a PITA. For example: __How many people is Bob related to?__",2.0
g71fl8w,j1sej2,"People are not uniquely defined by a name, so you shouldn’t be using that as a key. You would have one table of people which contains attributes and some unique key: you may as well make it a sequence generated key. Then you have your bridge table which just uses your people key.",1.0
g71zi32,j1sej2,"I think going into using a name as a primary key vs. a contrived key is missing the point, as noted by u/r3pr0b8. OP themselves noted this is just an example. They key thing here is how to represent the relationship (i.e. _not_ using comma-separated values).",1.0
g72gpca,j1sej2,"Yep, but the reason the OP gave for not going with the bridge table is because of the amount of repeated data. I was pointing out that the data needn’t be so large if it was stored appropriately so the apparent problem isn’t really a problem.",1.0
g70z50f,j1rksm,"Remove the semicolon after Products. Semicolon is meant to terminate a SQL statement. You’re getting an error because you’re writing two statements. The first one works, the second one that starts with where does not.",27.0
g71nyf1,j1rksm,Just curious which version(s) of SQL require a semi-colon?,1.0
g71p5oi,j1rksm,"Semicolons are the ANSI standard default separator in SQL. You can normally modify this if necessary (for example, in MySQL with the DELIMITER command).",7.0
g73gr8x,j1rksm,"Thanks!  Just a novice here, but have not come across them.  Most of my work involves relatively simple queries.",1.0
g716rc4,j1rksm,Remove first semicolon.,8.0
g70zc4z,j1rksm,"Remove the semicolon at the end of the second line.  You only want 1 semicolon at the very end of the entire sql
 statement.",3.0
g71lu6j,j1rksm,"Quite often, when you see an error reported, the actual error is with somewhere above where it says, so always look at the previous lines of code as well as the one where it thinks the error is. One extreme example of this is if you've put a BEGIN without an END, it'll report the error as being on the last line of your code because it can't match up the BEGIN/END pairs, whereas the actual END statement that's missing could be hundreds of lines higher up.",3.0
g71dqjt,j1rksm,"Not trying to be a dick, and I'm really glad you didn't delete this post so that other people can see it, but like 99.999999% of the time you get an error... the reason is because you wrote some bad code.

I remember when I first started diving into SQL from a background of multiple other languages. I would get so frustrated at times because it wouldn't do what I wanted or would throw random errors, that were nonsensical, such as needing to use a `;` when declaring parameters before starting a CTE. 

There is no logical reason for why, you just need to memorize these things.

Anyway, I remember one day getting really frustrated and my manager just very calmly asked me whether I thought my code was wrong, or whether I thought the entire Microsoft engine was wrong. Kind of made me look down and clench my butthole up tight, and then I went back to my desk and I figured shit out, and learned why I was wrong, and not the engine. 

It sounds pedantic but I think that is a good lesson for any aspiring analyst. It's **always** your code. Your code is **always** the thing wrong. It's not SQL. Don't blame SQL. Sometimes the reasons are annoying, such as a lack (or presence) of a semi-colon, which is fairly non-sensical, but it is what it is. These little nuanced rules are few and far between, and easily resolved with Google.",-3.0
g71edb6,j1rksm,"Not trying to be a dck, but you don't start a cte with a semicolon. You need to terminate your previous statement with a semicolon lol. Always terminate your statements with a semicolon",6.0
g71fg95,j1rksm,"I just mean it feels arbitrary. If i'm declaring parameters, or doing something above a CTE I might randomly have to use a semi-colon to get the query to execute. Or like how you might have to randomly name a subquery when you want to select * from it. 

In the end all of these just collect to me as nuances.",0.0
g71jnkf,j1rksm,"What Sql do you use, I'm curious. I use mssql and we can terminate variable declarations with a ;",1.0
g73a29b,j1rksm,"MS. I'm not complaining about having to use them, just pointing out that its a strange nuance. For example:

    declare @n int = 1

    select *
    into #table
    from table
    where field = @n

That works fine. This does not:

    declare @n int = 1

    with cte as (
        select *
        from table
        where field = @n
    )
    select *
    into #table
    from cte

But if you add a magic semi-colon it will.",2.0
g73mnl4,j1rksm,"The real problem is you not terminating variable declarations 😂
We should semi colons everywhere


Declare @ myVar int ;;;;;;",1.0
g73v28h,j1rksm,"That's not really what I'm saying, or my point. My point is that there are nuances that you need to learn. Little things you just need to do otherwise something won't run. Another example here:

    select *
    from (
        select *
        from table
    ) 
    where thing = thing

That won't run either without giving your subquery an alias, which makes sense, but this will run:

    select *
    from table

I'm not arguing that it should be any different, just pointing out that those little nuances are things everyone needs to remember and they exist above a level of simply learning what a query does, or writing a simple query to pull data.

I can explain why we need to use a datepart in a query, or why we need to join on certain conditions, etc. but I can't really explain these without just saying, ""that's the way it is.""

Why do we need a semi-colon? Because you're terminating variable declarations. Why do you need to do that for a CTE and not a regular query? Well... that's just the way it is.

See what I mean?",1.0
g75l3ak,j1rksm,"I believe we are ""supposed"" to end all our statements with a semicolon, but the ones without are also supported bc of backwards compatability. Also we need to name all table expressions ( CTEs, Derieved Tables aka sub queries, views ) because logically they are equivalent to a table and all the rules about tables apply to them as well. Like name of the table, unique column names, no defined ordering etc.",1.0
g75lgqz,j1rksm,"I mean that's fine, but it just boils down to answering a new user with saying, ""that's just the way it is.""

There is a fair amount of that in SQL as with any other language. 

My main point to the OP was that this shit happens, don't be embarrassed by it, and if something isn't working... it's your fault, not the engine's. I can't tell you how many times junior analysts come up to me and tell me that SQL is broken, or something like that, and then we look at their code and it's fucked up.

I have a running joke with my boss that anytime I complain about weird data, or some strange anomaly, etc., that he is supposed to ask me, ""how are you considering nulls?"" -- because at this point in my career if I'm really perplexed by some code... high degree of likelihood that it involves nulls I didn't properly think about / code for.

Like the first clue that your code is fucked up should be whenever you have a feeling that SQL is fucked up, or the data is fucked up.",1.0
g70tgea,j1qiwk,"You have to add a line that says group by then type every column that doesn’t say sum in it. But the formula for the column not the name so coalesce(am.brand,af.brand) not just brand",3.0
g712a6r,j1qiwk,This worked -- thank you!,2.0
g710yvp,j1qiwk,"After the join conditions and before Order, write GROUP BY 1,2,3,4,5,6,7,8

You need to group the aggregate values like the SUM here by non-aggregate columns",2.0
g712bu9,j1qiwk,This is very good to know -- thank you!,1.0
g717jz5,j1ppjf,"I think you overcomplicating a simple thing.
I think you need just group by product and sum noofitems. For the future, nested loops have high cpu cost.",1.0
g71nj4j,j1ppjf,"    SELECT productid
      FROM soldvia
    GROUP
        BY productid
    ORDER
        BY SUM(NoOfItems) DESC LIMIT 1",1.0
g71vu77,j1ppjf,"That makes sense, but the only issue I have is that there are two items which were sold in the same quantity, ""3x3"" and ""4x4"" both sold 5 items. I realize I could change LIMIT to 2, but that's a manual fix.",1.0
g71y1pp,j1ppjf,"ah, yes... i forgot about that!

okay, so subqueries it is

my hint for writing nested queries is to start with the innermost one first, by itself, and then add outer queries one at a time

have you tried that method?",1.0
g71z1yy,j1ppjf,"Yes, I've tried that. I think the innermost query that I have written works, and then I want to take the MAX value from that list, but I've got something wrong in that structure. 

Conceptually, I think I want to sum the number of items sold in all transactions, grouped by ProductID, then take the MAX value of NoOfItems, and then return the ProductID associated with that MAX value?",1.0
g71zf40,j1ppjf,"conceptually and actually

so your innermost query is

    SELECT productid
         , SUM(NoOfItems) AS sum_items
      FROM soldvia
    GROUP
        BY productid

can you take it from there?",1.0
g70lw3x,j1pkcc,Here's the SQLite leaderboard - [https://code.golf/scores/all-holes/sql](https://code.golf/scores/all-holes/sql),2.0
g70c7i8,j1ncy2,"Power BI certainly makes this attainable, but not sustainable without care. In my experience, Power BI is so easy to use, but that's also what makes it so easy to create a mess.

I suggest you have a strategy around standards, processes, naming conventions, parametrization, and IT review.

A good IT dept will empower their users with self service tools like Power BI, but have the processes in place to prevent the tangled mess it can become.

Remember, if you don't create what's necessary for a sustainable environment, instead you'll just be spending the time you saved, on supporting they mess they made.

Yes, with well organized and thought-out schemas, knowing when to use m vs dax, etc. Power bi will be able to handle a lot of ETL. This isn't a perfect science, however, and a lot of art from experience. Microsoft has a good article on star, snowflake, and single model tables, that you should hammer into people. It makes a huge difference",2.0
g70f7zd,j1ncy2,"awesome, thanks for the comment! We do have a very well designed enterprise warehouse so i'm hoping this helps keep everything organized. The majority of the team really doesn't want to learn to code, but they use access a lot with excel functions. They mostly use access for basic stuff like querying a single table, hardly any joins or anything like that. 

We have 1 or 2 people who might be good candiates for learning dax and M which should help build data models. i'm just hoping this works so they can explore the data themselves and bring in",1.0
g70hsqo,j1ncy2,"Using PBI dataflows and security groups, you can surface data from EDW to authorized business users in an easy and managed approach. PBI dalaflows is also good for bringing in data outside of EDW, giving full flexibility and agility in your data ecosystem. Key is having clear governance and process in place beyond the technology.",1.0
g70u162,j1ncy2,"In my experience, they'll always want something new and custom that's too hard to plan for. If I was in this situation I'd train them to use SQL or have their department hire their own SQL developer and give them access to a database with views they can query directly in SSMS.",1.0
g7127zv,j1ncy2,that is what I was afraid. Nobody on that team wants to learn sql. not sure if we have it in the budget to hire an sql developer.,1.0
g709xsq,j1ncy2,Why do you want to do this at all? If their excel users already why switch out of that instead of just teaching them to use power query and power pivot in excel?,0.0
g70a5q8,j1ncy2,its not my decision. IT gets crushed with requests for data sources and datasets and its becoming overwhelming to handle all the requests.,2.0
g70aino,j1ncy2,Yeah but power query is the same in excel and powerbi and you won’t have to train them on a completely different tool. It sounds like your it just doesn’t realize that it’s an option,0.0
g70awwq,j1ncy2,they're trying to move people away from excel and into a unified environment in order to share reports. its been a disaster with only using excel at this point,2.0
g70b8sc,j1ncy2,I’m not sure powerbi will make that part any easier but they should be able to load everything they need after a month or so.,0.0
g715i2s,j1mexp,"If this is in a proc, just have separate queries and choose which one to run based on the parameter.

Otherwise, a gross method you could use if the columns are compatible. Create a view that is the union of all 3 tables, and include a table name column.

    SELECT 'N_charges' AS tablename, transaction_charge AS charge, execution_date FROM N_charges
    UNION ALL
    SELECT 'B_charges' AS tablename, conformity_charge AS charge, execution_date FROM B_charges
    UNION ALL
    SELECT 'P_charges' AS tablename, conformity_charge AS charge, execution_date FROM P_charges
    

Then you can query that view WHERE tablename = 'B\_charges' AND execution\_date = '2011-03-22' or whatever.",4.0
g71bdfd,j1mexp,"Your problem is not the query, your problem is a wrong data model. 

Those three tables should in fact be just one table with a column named `charge_type` (or something similar). Then you just add `where charge_type = '${ID}'` to the query.",2.0
g702nh1,j1mexp,"Hello u/-SKoRM- - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g703sj1,j1mexp,PostgreSQL,1.0
g703ve7,j1mexp,Are you wanting to do this in a stored proc or is the control logic in a different language?,1.0
g7042z2,j1mexp,I’d like to keep it all SQL,1.0
g70n20m,j1mexp,"If this is the case, deaigns like this are generally bad practice.  Stored procs should be singular purposed in nature.  This allows the optimizer make the best choices.  If there are code trees the optimizer can pick a plan based on a different query houses within the procedure.  This is a case where being to clever can bite you in the end.


Remember SQL is not a programming language in the respect you are not telling the server what to do, only what you want the result to be and the engine picks the shortest path there.",2.0
g71cw8d,j1m8qz,"Depending on what exactly you will be doing, it might make sense to use a table that allows for many-to-many relationships.  
It would look something like this  


**DataTable**  
NodeId, title  
0, Dogs  
1, Cats  
2, Kittens  
3, Food  


**LinksTable**  
ChildNodeId, ParentNodeId  
2,1  
3,2  
3,3",2.0
g71r5mk,j1m8qz,You can use collection object,1.0
g733xdl,j1m8qz,"Just because you could doesn't mean you should. The standard way of many-to-many mapping is via a linking table, for example

CREATE TABLE links (parent_id int references pages(id), child_id int references pages (id));

or along those lines.",1.0
g706myq,j1m8qz,"Yes. You can use STUFF and FOR XML PATH, explained here:
https://stackoverflow.com/questions/31211506/how-stuff-and-for-xml-path-work-in-sql-server",1.0
g707i7k,j1m8qz,"Sure, use json",1.0
g70juc0,j1l09l,Just join on ID_Product and Id_Order,1.0
g70s103,j1l09l,That gives me six records because the invoice is joined on each row twice.,1.0
g70zzu2,j1l09l,"Select distinct will fix it. Also it's a terrible solution because it's a many to many relationship the way you have it here and in a transactional database it's almost surely going to give an incorrect result set at some point.  
Based on your question it seems like you're trying to align an order into a warehouse with the sales order to a customer. Unless you have a specific inventory flag on product that is tracked from receipt to sale you likely aren't going to get what you need.",2.0
g70y69k,j1l09l,"give your products in the order sequential number (per product, per order), give your products in all invoices tied to an order sequential ranges (using running totals). Then it's a join on order, product and order sequential id being in the invoice order sequential range.

In your case for order 2, product 7 you will have records 1,2 and 3. For invoices you'll have invoice 1 range 1..2, invoice 2, range 3..3;

your join will get order record 1 and 2 linked to invoice 1 and order record 3 linked to invoice #2",1.0
g6zu12b,j1jvyf,"pls mark your thread ""solved""",4.0
g6zjp14,j1ioeo,"I used DataGrip.  You can connect to all sorts of databases,  RedShift, MySQL, MSSQL.",1.0
g6zormz,j1ioeo,I'll give the 30 day trial a shot. Thank you!,1.0
g6zsaxy,j1hk60,"The stuff that I wish I had understood:

Lots of established programmers have a really hard time changing from procedural thinking to set-based thinking. This tends to be more of a problem if you're self-taught because the concepts that databases use are often missing from your background. Suffice it to say, as long as you're a beginner and you start thinking you should do big operations that are basically the same in the DB one row at a time or that you need a cursor, you're probably thinking wrong. Not always, but often enough to be a rule of thumb.

Programmers also tend to struggle with JOINs because they don't ever get the relational background. It sounds like you will get that relational background, but if you're starting early you may not have it. You don't need to know all the stupid symbology and diagrams and special language of relational algebra that they'll probably teach you. IMX, nobody out of school longer than a few years even remembers it. However, you DO need to know what JOINs do and what effects they have. Understanding the ubiquitous Venn diagrams is okay, but you really need to understand the concepts and what the actual effects are on your query output are. JOINs are Advanced Beginner or Intermediate, though, so you may not need to care yet.

You also don't need to understand the umpteenth level of normalization *exactly*. The first three and Boyce-Codd are the major ones that cover 95% of applications and 95% of tables in those applications. You do need to understand level 1 and know that level 1 is vital in essentially all real-world applications. There are reasons to break it, but they come at a high cost: a much less flexible range of types of queries. Level 2 is slightly less important, and level 3 slightly less important than that. Every level is an increasing tradeoff of complexity vs utility. If you skip level 1, you'll run into problems quite often where you can't easily write a query to do what you want that will still perform well. As soon as you're forced to deal with someone else's totally denormalized database you'll understand almost immediately why normalization is important. If you dump data into a blob of JSON, understand that that is breaking first normal form. There are many reasons to do that, but don't do it and expect that to magically scale as well or be easy to query even if the RDBMS has JSON or JSONB features.

Relational databases do not do hierarchal that well. You'll hear a lot about the impedance mismatch between objects and relations that results in ORMs, but what this problem ultimately boils down to are the rules for level 1 normalization and the fact that databases don't do hierarchal design very well. There are ways to store hierarchical data, but most of them suck. You will not easily overcome this problem when you hit it. It's been 50 years and there are still no *good* solutions.

The answer to ""what is the best way"" in databases is always *it depends*. And it really depends on a huge variety of things. Your hardware, your software, how your data is generated, how it's stored, what your demands are, etc. Virtually every aspect of *your specific application and your specific usage patterns for it* can affect performance and what works best.

And, the big one:

**There is no good way to *really practice* SQL.** It's very easy to write SQL that returns output. That's kind of the problem, because it's harder to tell when you're wrong. Until you have a very good understanding of the data in your database and until you know the business requirements are for your output, it is *virtually impossible* to tell if your output is useful, meaningful, or even what correct means. You'll have a pervasive feeling about that as longs as your understanding of the data and business are incomplete. Absolutely and by all means you should get the AdventureWorks database or one of the myriad of sample databases that exist and run queries against it to learn the basic beginner level behavior of the language. But until you understand your data well enough, and until you understand the business well enough, you will not really understand SQL. Until you yourself are formulating business questions that you can ask of the database and you understand the data and the business well enough to *know* the output well enough to validate it yourself, you will never really feel like you're good at SQL. You need to be able to tell the difference between an *almost* correct answer that is missing or duplicating records and actually correct output, and the process that you have to go through to change your query from *almost* correct to correct is how you *really* learn SQL.",20.0
g72aff3,j1hk60,Thank you for this. As a beginner myself I appreciate the guidance,1.0
g6zescp,j1hk60,"if your course is using oracle SQL i would start there, but otherwise a guide like https://www.postgresql.org/docs/8.0/tutorial.html will get you started. most sqls share most of the basics but each will have install/maintenance/query/performance tradeoffs. glhf",4.0
g6zrs6n,j1hk60,"Practical SQL is on Humble bundle at the moment

Edit: Available in the $8 tier. Here’s the [link](https://www.humblebundle.com/books/learn-to-code-the-fun-way-no-starch-press-books?hmb_source=humble_home&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_twos_tile_index_1_c_codingbookshelfnostarchpress_bookbundle)",2.0
g7104db,j1hk60,"For me, DataCamp is what got me basic to advanced SQL. They have a bunch of specialized topics that you can check out. And to practice real problems I used LeetCode and StrataScratch. Here, you can practice thousands of problems taken from real companies.",2.0
g71amgr,j1hk60,"Oh shit thanks for the direction on LeetCode and Strata scratch. I took the same path and learned on Datacamp and hit a wall and now I'm like ok so what now hah, than you!",1.0
g6zh7lp,j1hk60,"The course that helped me the most was the Stanford one, it's now on edX. This is the introductory course there are others after this

[https://www.edx.org/course/databases-5-sql](https://www.edx.org/course/databases-5-sql)",1.0
g6zxokl,j1hk60,This will be a good place to start https://youtu.be/mHWb16MvyMs,1.0
g6yrfd3,j1ex4f,Lol...bro try something and post what you can't do.. no one is going to do your assignment for you,3.0
g6yrias,j1ex4f,I am welling to pay 30$,-5.0
g6yrxtk,j1ex4f,"Well good luck with that.... You joined to learn sql but i guess you learned with money you don't have to learn sql 


I hope no one offers to help you and you have to do it yourself",3.0
g6ysxps,j1ex4f,Hahah ok sorry i upseted you lol,0.0
g6yrphk,j1ex4f,"I must be misunderstanding the word ""question"".... In this context it appears to indicate a step by step todo list....",1.0
g6yqnuv,j1ex4f,What are you prepared to pay for someone to do your hw? 😅,1.0
g6yi4hh,j1d3a6,"Or you could just have :  replace(venue name,’school:’,’’)
As the function indicates it will replace school: with an empty string  -",3.0
g6yj69b,j1d3a6,"Yes, this is the better way to handle it, both in terms of performance and readability for the next developer.",1.0
g71enzj,j1d3a6,"This does not replace the fields with “School” with blanks, does it?",1.0
g6yhwdj,j1d3a6,"    CASE WHEN venuename LIKE 'School:%' 
         THEN REPLACE(venuename,'School:','')
         WHEN venuename = 'School'
         THEN ''
         ELSE venuename END   AS venuename",2.0
g6yemj6,j1d3a6,"CASE WHEN VENUENAME = ""School"" THEN """" ELSE MID(VENUENAME, LEN(""School:""), LEN(VENUENAME) - LEN(""School:"")) END [Venue Name]

Untested. Just typing on my phone. Your syntax could be different, so you might need to fiddle with it, but it's that general idea.",1.0
g6yi2vx,j1d3a6,your ELSE is incorrect for schools like 'Junior Academy',1.0
g6yie8i,j1d3a6,"That's not in the brief. OP says it's either ""School"" or ""School: Example School"".",1.0
g6ymfd5,j1d3a6,"Thanks all for your help,  I'll give it a go",1.0
g6yqpio,j1bxg8,Well the title didn’t make a lot of sense,2.0
g6yrbgh,j1bxg8,"I apologise. The thing is I am unable to install MySQL workbench and other components and the reason for this I don’t have MS Visual c++ 2019 Redistributable package (x64). So, i went to install this but still I am getting an error and I don’t know why.",1.0
g6yri5r,j1bxg8,Unspecified errors are the best! What is the log file showing - does it show which package/component failed?,1.0
g6zy8vn,j1bxg8,Usually windows updates fixes similar issues. Are you up-to-date on windows updates?,1.0
g70vk8u,j1bxg8,I don’t think I am.. let me do that and check,1.0
g70w50j,j1bxg8,"Okay.. I am not technically sound in this. So, I will need a little more help. I still use Window 8.1 and updates are not available for it. What should I do?",1.0
g71c2oz,j1bxg8,"Updating to windows 10 is a solution but would be a long way.... for now ... Try installing MSVC 2015 manually (if the MSVC installation was triggered by MySQL installer) 

[https://www.microsoft.com/en-us/download/details.aspx?id=48145](https://www.microsoft.com/en-us/download/details.aspx?id=48145)",1.0
g71dzz2,j1bxg8,I tried it just now and again failed to install it.,1.0
g75hes2,j1bxg8,I can see a log file link on the installer when it fails (on the picture you posted).... My next steps would be to open the log file.... scroll down and find an error message..google that to find any solution,2.0
g6y4ybh,j1ax2f,"The performance aspect heavily depends on which DBMS product you are using. 

But usually the CTE isn't slower or faster than the equivalent solution using a derived table (assuming the CTE query is used only once). CTE are **not**  a performance tool. 

As a rule of thumb I'd say use a CTE instead of a derived table if you need to use that sub-query more than once in the overall query. Or if you need a recursive query.",10.0
g6yvxbf,j1ax2f,"In my team we have the strict rule of not using sub queries and to be fair its rarely necessary. A cte allows to chop up the problem in readable bits before joining it on your primary table of choice.

Performance wise it would debate that there aren't that many differences. I do have to add that I'm only a data engineer and by no means an DBA.

Hope this helps!",4.0
g6z25nz,j1ax2f,"CTEs are just named subqueries. 

Advantages are:

 * Readability
 * Ability to refer to CTEs more than once
 * Optionally Recursive

Possible disadvantage:

 * Lazy RDBMS might not have properly optimised CTEs to the same degree as subqueries in their initial supported versions.",5.0
g71z5de,j1ax2f,"i use teradata, i doubt its lazy rdbms",1.0
g71znph,j1ax2f,"So fortunately for you, no disadvantages then!",1.0
g722ah3,j1ax2f,"YEAH

&amp;#x200B;

THANKS MAN",1.0
g6zrkmv,j1ax2f,"I prefer CTE's over Sub queries for supportable code and CTE reuse. I've also found that if performance starts lagging I can quickly convert a CTE to a temp table and add an index to the temp table without having to alter a lot of code.

I have actually run across a few scenarios where sub queries outperformed CTE's so I will usually run tests with both just to make sure I am using the best solution.",2.0
g6ze7f5,j1ax2f,"FWIW I was always told to call FROM clause subqueries, ""Inline Views"".

CTE's are similar, inline named views, the main advantage being you can reference them multiple times in other CTEs and the query body.

A lot of recursive capabilities also rely on them but using a self reference.",1.0
g71ya3y,j1ax2f,"If you're using mssql, you will most likely get a similar plan for both of them. Referencing CTEs multiple times will go back to the base tables and the results of the ctes aren't cached anywhere",1.0
g71z3o6,j1ax2f,i use teradata,1.0
g6y0t9m,j1a62y,sql server might be not running. make sure you started the service.,6.0
g6ybx4q,j1a62y,This right here. Pull up the configuration manager and see if it’s listed as stopped. If you want it available automatically when you boot your computer make sure it’s listed as automatic rather than manual as well,1.0
g6ykxjy,j1a62y,"I also suggest looking into Windows networking logs. I had a problem similar to this and I just couldn't connect to SQL server.  


Then I noticed that the logs said the firewall was blocking all connections to SQL Server. A quick bout of loud cursing, a visit from a rather large neighbor who didn't want his family to hear said cursing, a change of the firewall settings and I was good to go.",1.0
g6yac4s,j1a62y,Verify the server in the connection string is correct,1.0
g6xttqo,j199y1,"I think you'd need a reference table with public holidays to do this fully. 

You can make a bit of progress by changing 20 to 28 to allow weekends.

Maybe the system vendor can help? Likely lots of clients would be interested.",1.0
g6xuafw,j199y1,"Thanks. There is actually such a table with “holidays” within the system, any ideas how I can incorporate that?

The vendor - officially at least - “doesn’t offer SQL support for bespoke queries”",1.0
g6xwm0j,j199y1,"You should be able to do:

Where date &gt; (some select that returns the date 20 working days ago)

So that's really your challenge, write a query that returns the date N working days ago.  I'd focus on this logic off to the side using your holidays table and whatever else you have available to calculate this.  Then once you know you can accurately calculate that date from a variety of different starting points (lots and lots of edge cases here to test of course) then simply plug that query into the where clause similar to shown above.",2.0
g6ybebb,j199y1,"you should attempt to get that subquery on the holidays table going

i just thought i would point out that if your `Datecolumn` is an actual DATE column, there is no need for fancy arithmetic shenanigans like converting it to FLOAT

this --

&gt; where (CAST( FLOOR( CAST(my_table.Datecolumn AS FLOAT ) ) AS DATETIME) BETWEEN dateadd(day,datediff(day,0,getdate())-20,0) AND GETDATE())

can be rewritten as --

    WHERE Datecolumn 
             BETWEEN CURRENT_DATE - INTERVAL 20 DAY
                 AND CURRENT_DATE",1.0
g6xo89f,j182fk,"You are dealing with known anti-pattern: the entity attribute value pattern which is quite hard to efficiently query in SQL. 

Your immediate question can be solved using a self join:

```sql
select t1.name, t1.datavalue as num, t2.datavalue as code
from the_table t1
  join the_table t2 on t1.name = t2.name and t2.dataid = 2
where t1.dataid = 1;
```

If you can't guarantee there is always a dataid = 2, use a `left join` instead",1.0
g6xvqnb,j182fk,"Hi and thinks for the suggestion. One simplification I made for my example is that the data is actually in a separate table so it actually looks more like this:

Table1

ID| Name  
1 | Steve  
2 | John  

Table2

ID | DataID | DataValue  
1 | 1 | 123  
1 | 2 | Abc  
2 | 1 | 321  
2 | 2 | CBA   

So to get the below I took a part of your solution and tried to add multiple joins to the second table

Name | Num | Code  
Steve | 123 | Abc  
John | 321 | CBA  

So essentially I now have 

Select Table1.Name, Table2_1.DataValue as Num, Table2_2.DataValue as Code,   
From Table1   
inner join on Table1.ID = Table2_1.ID  
Inner join on Table1.ID = Table2_2.ID  
Where Table2_1.DataID = 1 AND Table2_2.DataID = 2

But I'm getting the same value back in both columns, so it looks like this:

Name | Num | Code  
Steve | 123 | 123  
John | 321 | 321  

I really appreciate any help",1.0
g6xxxfw,j182fk,"So it turns out I was selecting both values from the same joined table. My mistake. Thanks though, wouldn't have solved this without you.",1.0
g6yqroj,j182fk,"Contrary to u/trulius's comment, a self-join is not necessary. You basically want to do a pivot/transpose, which you can do easily with _conditional aggregation_ (different RDBMSs have other syntax for pivoting results, but conditional aggregation is something that works in every RDBMS).

Given your table structure in a later comment, you can do something like this:

```
SELECT
    Table1.""ID"",
    Table1.""Name"",
    -- you can use either MIN or MAX here
    MIN(CASE WHEN DataID = 1 THEN DataValue END) AS Num,
    MIN(CASE WHEN DataID = 2 THEN DataValue END) AS Code
FROM Table2
    INNER JOIN Table1 ON
        Table1.""ID"" = Table2.""ID""
GROUP BY Table1.""ID"", Table1.""Name""
;
```

(working fiddle [here](https://dbfiddle.uk/?rdbms=oracle_11.2&amp;fiddle=ce447120e62d13913f6f7a33c628c81f)).",1.0
g6x7sqz,j152dc,[alias](https://www.w3schools.com/sql/sql_alias.asp) should be able to help,1.0
g6x8ktz,j152dc,"i got it, thank you!",1.0
g6x835c,j152dc,"I'm slightly confused about what you want. If you're just trying to differentiate the two columns then you can give them aliases in your query, which would better identify the data in each column. Overall, your query should have two joins between three tables.",1.0
g6wteg8,j131fg,I don’t recall it being different than in a standard query,1.0
g6wtp6l,j131fg,"I am working on a class project and it just isn't working I thought maybe there must be something special. Thank you for your response, I still have a week to solve it :).",1.0
g6wttu6,j131fg,"Maybe be more specific on what is not working 
What is the function doing and what is the process supposed to do",1.0
g71z8op,j131fg,"It's fine for a class project, udfs perform very poorly inside stored procs. You don't get parallelism and in general you get better performance by inlining the logic when possible",1.0
g71zanq,j131fg,UDFs are notorious for poor performance in MSSQL,1.0
g6wtbcc,j12zlc,"*Beep boop*

I am a bot that sniffs out spammers, and this smells like spam.

At least 100.0% out of the 6 submissions from /u/TravisCuzick appear to be for courses, coupons, and things like affiliate marketing links.

Don't let spam take over Reddit! Throw it out!

*Bee bop*",25.0
g6x11xx,j12zlc,Thank you,2.0
g6x1j6t,j12zlc,Thanks for sharing,2.0
g6xel8i,j12zlc,"I've been mostly self taught myself just searching for stuff as I need it. This course sounds amazing though, very happily bought and will recommend to others as well",2.0
g7ucbix,j12zlc,"Thanks, I appreciate it! Hope you enjoy the course:)",1.0
g6xmuvc,j12zlc,"Once I found out that my data analysis skills (primitive using only MS Excel) landed me better Product Management opportunities, I kept pushing further and learned SQL on my own to supplement my data analysis knowledge. I will use your course as a refresher as the content layout seems pretty much relevant. Thank you for sharing it for free :)",2.0
g6y1di1,j12zlc,"Awesome, thank you so much!",2.0
g77x9md,j12zlc,"Travis - I completed your course on Udemy today. I’ve been working in data 20+ years across various platforms and languages.   We are now leaning to SQL from some other tools at my work and my SQL was rusty.   I was on the lookout for a class and it was good luck that I came across your post when I did. For me your class did a nice job of refreshing on the basics and your examples in the latter half of the class did a great job in helping me do an ‘inner join’ in my thinking for translating some of my work from SAS to SQL.  I also appreciate that you included downloadable text resources with you code examples which saved me some note-taking.  

Thanks for creating this class, posting it here and the price.   Well done.",2.0
g7ubu5b,j12zlc,"Thanks for the positive feedback, I'm glad it was able to smooth your transition from SAS to SQL!",1.0
g6wumre,j12zlc,Cool man. Thank you for this!,2.0
g7uc8p2,j12zlc,"No worries, hope you enjoy the course:)",1.0
g6x09yz,j12zlc,"If I take this course, will it help with working on MS ACCESS assignments?",1.0
g7uc6cs,j12zlc,"Somewhat...the conceptual stuff like applying criteria, joining tables, and grouping/aggregation will definitely apply. When it comes to syntax however, there will be some differences...Access SQL is kind of an odd bird compared to other ""dialects"" of SQL, but the ""big six"" of select/from/where/group by/having/order by pretty much work the same.",1.0
g6wxgan,j12zlc,Thank you for sharing!,1.0
g6wobnz,j12aau,"In order to get the percentage of a total you need either: Something to compare against or a pre defined percentage amount to calculate. 

I.e do you want 10% of the total? Or do you want every record to be shown as a percentage of the total of all records?",1.0
g6woqnx,j12aau,"90% of the total, this is what i cant find answers to. everything is telling me to show a ratio but i need the percent of the total, not to have it broken down in a comparison or to see a ratio of what the other records are to the total.",1.0
g6wt56b,j12aau,"Total = SUM(MyField),

PercentCalc = SUM(MyField) \* 0.90,

AdjustedTotal = SUM(MyField) - (SUM(MyField) \* .90)",4.0
g6wkhz1,j11ixp,"What is clunky about foreign keys? And egads, don't create a new table per assignment. I can imagine nothing clunkier than that.

You would have one assignment table, and probably a response table (only one) that looks something like:

response\_id (PK) | assignment\_id (FK to assignment) | response\_text

You would insert a row into assignment for each assignment, and a row into response for each response to an assignment.",2.0
g6wyonf,j11ixp,thank u for ur reply. i will use foreign keys. sorry for my stupidity I'm new to this,1.0
g6w5xmp,j0zkaz,Have you tried any of these inside your mysql database?,4.0
g6w97g1,j0zkaz,I don't know how I would do that,1.0
g6wev27,j0zkaz,This guy [is a troll](https://www.reddit.com/r/mysql/comments/j0zt31/quiz_ive_been_struggling_with_this_and_would/g6wdhjx/?context=3). Don't engage him,4.0
g6wk1da,j0zkaz,Thats a lot of unrecognizable syntax.,1.0
g6w9l7c,j0zkaz,[deleted],1.0
g6w9r7r,j0zkaz,"I have tried googling them, Google really doesn't help me to be honest. And I would do the ones I can, but like I don't understand MySQL or php at all.",0.0
g6w9t7l,j0zkaz,"""The _____ function returns **the current data any time not the server in MySQL**.""

wut",1.0
g6w9zu7,j0zkaz,"Dude I don't even know, thats why I'm struggling too because the professor makes it so weird and confusing",-1.0
g6wafj5,j0zkaz,"you know about [da manual](https://dev.mysql.com/doc/refman/8.0/en/), right?

go look up the [functions](https://dev.mysql.com/doc/refman/8.0/en/functions.html)... i mean, seriously, scroll through them and find the one that fits this description",4.0
g6wy7tk,j0zkaz,"1. Ya motha
2. Ya motha 
3. Ya motha
4. Ya motha
5. Ya motha 
6. Ya motha
7. Ya motha
8. Ya motha
9. Ya motha
10. Ya motha",0.0
g6wd19l,j0xvds,You can have a daily snapshot of the table so you can see historical data,1.0
g6wdh71,j0xvds,How easy is it to restore an individual row from a snapshot? I am working in PostgreSQL.,1.0
g6wdlxr,j0xvds,Do you mean taking the most recent data? You can use window function or Max function,1.0
g6w39j2,j0xvds,"Is Id an identity column? And would title be your business key? If it's MS SQL Server, a system versioned table would do the trick.",0.0
g6wdcfg,j0xvds,ID is the unique identifier. This is in PostgreSQL. Not sure if it has an equivalent...,1.0
g6vj1n4,j0w6ey,"You can detach your current databases, move the files to the external hard drive, change the path in the database properties in SSMS, then reattach.",4.0
g6vch41,j0vmkk,"&gt;  is a schema where ""words,"" ""parts of speech,"" ""definitions"" all one column tables and ""syn_map"" is a table with a primary key of (synonym_word, part_of_speech, definition, primary_word) OK? Should I get rid of the 1 word tables? And if I shouldn't should I give the one column tables surrogate keys?

yes, no, and no",2.0
g6vehev,j0vmkk,"Thanks so much! Just to clarify, the reason the 1 column tables are good is just because it keeps things conceptually distinct? And is the reason the surrogate keys aren't a good idea because it needlessly complicates a 1 word table and requires joins",1.0
g6vfjcg,j0vmkk,"yes and yes

you're on quite a roll here",2.0
g6vcaq4,j0vipu,"You want an ETL tool.

Could be as simple as python or powershell. Or if you want to spend money, there are dozens of solutions (ssis being one of the the most frequent one).",1.0
g6vkn0d,j0vipu,"There are a ton of different tools in baseline oracle that will help you accomplish what you're trying to do. Here's two:

1 - Use UTL\_FILE to write out to a file and then access the file as an external table from the second database.

2 - Use a DB link from database 2 to query the data in database 1.",1.0
g6vl11v,j0vipu,For either of these do I have to have Oracle installed?,1.0
g6vofxr,j0vipu,"Both of those are database facilities. So db link for example, on db2 after you create the database link you would connect to the db and do select * from table@db1

If you don’t have access to the server, you would need a oracle client to connect to the db.",1.0
g6v3rx7,j0t55v,"I don't know if you'll even get past the automatic screening without SQL on your resume. Slap it on there and let them tell you if you don't know it well enough to put on a resume. There are plenty of Excel only analyst jobs that you'd still be in the pool for but a lot of those need VBA.

If you know python, java and R well enough to put those on a resume, then SQL shouldn't be too hard. Do some basic interview questions on hacker rank or leetcode. Look up what you don't understand. Make sure you can reliably answer basic interview questions like ""what join should be used in this situation"" or ""what is a DBMS?"".

For an intern, I'd say try to be able to comfortably answer at least 30/50 of these interview questions. https://www.guru99.com/sql-interview-questions-answers.html",8.0
g6v4016,j0t55v,Thank you lots!!,1.0
g6v2z6x,j0t55v,"I'd say yes. Mostly because you've done real world stuff that's data-oriented so you will know that data is not usually perfect.

You can learn the basics of SQL up front to prove commitment, then learn more during the internship.",3.0
g6v3b6t,j0t55v,"Would it be fine to put down ""Beginner SQL"" in my resume and just be honest about my very beginner SQL experience during interviews? Thank you so much btw!",1.0
g6v7ovf,j0t55v,Maybe? I would feel better if I saw some SQL experience and if you mentioned your Excel skills,1.0
g6upnya,j0s0e2,"Hotel operations not boring.
Availability, room types, special offers, integration with third parties like booking.com... there's lots to work with",9.0
g6w0tg4,j0s0e2,"Beginner here, how would that project work?",1.0
g6uq2bh,j0s0e2,"Ι know there's a lot to work on, I just feel like the data itself isn't so interesting, for example I'd rather work in something with environmental data etc. But on the other hand it'd be harder to work on without a reliable source of data like sensors whereas with the hotel I can make some mock data on the spot. I'll think about it some more, thanks!",0.0
g6uytw9,j0s0e2,You could buy a raspberry pi and a cheap sensor for temperature and make your own data!,4.0
g6uvbl1,j0s0e2,Oh. Well look around the internet. There's some free or cheap data sets out there. I've seen weather data for about $10 for a year's worth.,1.0
g6uvi4b,j0s0e2,"Great, I'll look into it too!",1.0
g6y1k59,j0s0e2,"Data.gov has ginormous data sets on lots of topics, all free.",2.0
g6v397o,j0s0e2,Why don't you work with something you're interested in? Do you have any hobbies or interests that you could draw inspiration from?,4.0
g6v3hfa,j0s0e2,"Mainly videogames, dnd and fantasy books. But I'm not sure what I could do with them.",1.0
g6v3peb,j0s0e2,"I haven’t played dnd in years, but I feel like it’s perfectly suited to a management system. Maybe even take a fantasy series you like and set up to set it up for dnd games.",3.0
g6v42nn,j0s0e2,"That's... That's genius! I could make a database for classes, races, spells, monsters etc! And make a web app for easy character creation and/or encounter planner! Thanks a whole lot!",6.0
g6vedyc,j0s0e2,"I was thinking about something like that for videogames. I play (amongst others) Overwatch, DotA and Tekken. There are a lot of informations about abilities, cooldowns, damage, HP etc (Ow, DotA) or frame data (Tekken) that you can make into table and play with. It could include scrapping some web pages, a thing to add to the project.",2.0
g6y1u6x,j0s0e2,"I hear that EVE players live and die by piles of spreadsheets. There's probably a use case for converting some of those into a proper db, cross-referencing everything, and then setting up parameterized queries for on-the-fly reporting.",1.0
g6v8jg9,j0s0e2,Something with probes and real time monitoring/analytics is a good option.,2.0
g6vfeie,j0s0e2,"When I wanted to better understand Postgres, I also found all the classics (school class/teacher structures, business org charts, etc) super boring and surface-level at best so I [created a multi-player game using only the database](https://github.com/Abstrct/Schemaverse).",1.0
g6wdwal,j0s0e2,"Hi, I would be interested in helping with whatever you come up with. I've started working with Python but I started with SQL.",1.0
g6uhee7,j0q42p,"Dynamic SQL is best ignored. It has a place, but any time you can solve the problem another way, avoid it. When writing code, make sure that the next person to touch it will easily be able to see what the code does. Dynamic SQL that's often tricky.

https://use-the-index-luke.com/, is a good blog that discusses performance tuning. I don't expect most analyst people to know how to tune. I do expect them to know when their query is poor performing.",1.0
g6ukjzw,j0q42p,"where do i start with this? some of it seems a bit intense, tried reading about Indexing. 

Thanks for the reply.",1.0
g6urhhx,j0q42p,"First, read the documentation for BigQuery provided by Google: it’s excellent. There’s also a book: [Google BigQuery: The Definitive Guide](https://www.amazon.com/Google-BigQuery-Definitive-Warehousing-Analytics/dp/1492044466/ref=sr_1_1) that goes a bit deeper and is an excellent complement.",1.0
g6wd7k0,j0q42p,Maybe the case statement can help you too https://youtu.be/mHWb16MvyMs,1.0
g6seghp,j0jh8u,Most of the normal commands work the same. If select count(*) from table didnt work something else was going on.,2.0
g70a2ol,j0jh8u,"Thank you! That was my initial thought as well. I'm no wizard, but competent enough to know that those simple functions should have worked.",1.0
g6ulxyy,j0jh8u,"&gt; Not sure if that is the fault of MySQL, but I'm still a bit put out by it.

highly unlikely

i'd look at what permissions they gave you

the differences in the two platforms are trivial -- the language is essentially the same",1.0
g70a94p,j0jh8u,"Thanks, I wonder if they'll provide any follow up or be willing to look into the permission settings.",1.0
g6pk8do,j0b69p,"Strong for a Jr would be writing queries with where filters, aggregates with a group by, an inner join and maybe cast a data type or alias work on the results to get things in the correct format. 

I would expect a Jr to be aware of the different type of joins, having, unions, sub query, case statements but not really know how to do it without googling. 

Things like creating or altering table schema, insert rows and delete data are left to more senior people.

Strong for mid level, I would expect them to know all of this and more.",38.0
g6po532,j0b69p,"Agree with this. Would add CTE’s and an understanding of underlying data structures and pipelines, ultimately how to connect data to end products like tableau and data studio.  

A plus would be identifying where queries are becoming slow and when to do data cleanup in SQL and when to do it end products. Maybe experience with GIT too but that’s more situational based on how the company operates.  Knowledge of this stuff would be good for a junior, being fluent and able to implement would be a path to mid-level",7.0
g6pretb,j0b69p,"When you say sub queries, would it be more that you’re referring to basic and more intermediate ones? Stupid question but better to clarify! Also, when do you actually use CTEs, how do you spot that you need to use it, what is the “giveaway” for that in tech question, for example?
Thanks a lot for the responds, guys, I really appreciate it",2.0
g6pyn44,j0b69p,"It’s always case to case. Sub queries are useful in one-off situations where you only need a specific set of data from a larger data set. CTE’s are more handy, for example I have table A with transaction data between an employee and a client based on codes. I can use a CTE for easy reference to join that table twice to another table that connects client/employee codes with names, the end result would be a table with client code, client name, employee code, employee name and the transactional data. I could use a sub query but a CTE would make the code “cleaner” and easier to manage.

Consider sub query for a single join while CTE’s are far easier to scale and reuse between queries.",2.0
g6q20bn,j0b69p,Thank you for the clarification and your time!,2.0
g6px3e7,j0b69p,I too am interviewing for an Analyst Position.  You just gave me a huge boost of confidence  because I know how to do these things.  I learned everything on the job and have no idea where I actually stand in terms of knowledge levels,6.0
g6q2w0m,j0b69p,"Good luck brother, at least one of us will pass and that’s you! ;)",4.0
g6q3wgt,j0b69p,Dude don't discredit yourself! I believe my biggest strengths are being trainable and adaptable. I'm going to sell that harder than my SQL knowledge.  You should do the same! Sell yourself!,6.0
g6q4e3f,j0b69p,"100% man, but does selling yourself really have that big of an impact if you do not show “great” sql knowledge? That’s where my doubt is. I will also have a good feedback from my current manager so that is good at least haha",3.0
g6q7o6y,j0b69p,"I wouldn't say my SQL knowledge is ""great"". Its better than I thought it was but I do know ill be competing with people who have years of experience on me. Gotta try and stand out in other ways on top of what you got",3.0
g6q8lmu,j0b69p,"You mentioned that the position is at your current company.  Have you built a reputation of having a good attitude, work ethic, and a willingness to learn?  If so, that will help you out if the new team is weighing you vs. another applicant, even if you are lacking experience in certain areas.  Plus, it's a much lower risk to the company (not to mention less expensive) to do an internal transfer, rather than an external hire.",4.0
g6qbejl,j0b69p,This.  This is how I got the position where I learned SQL on the job. I knew zero SQL going in. I did however have a reputation of being very dependable and trainable. I was very enthusiastic on how I would love to learn a new skill such as SQL,5.0
g6qfvbw,j0b69p,"Exactly the same man, hopefully it goes good :D",2.0
g6qfqyn,j0b69p,"Yup, all of the managers have good feedback on me which they regularly pass to my line manager. So far, everything is like very extremely good when it comes to that side but the knowledge is the only one lacking here :( hopefully it goes good, this is literally my only chance right now lol",3.0
g6q5is3,j0b69p,"For a junior position I would consider strong sql to cover understanding of the below:

Filtering using WHERE
Aggregation using GROUP BY
Filtering using HAVING (common interview question is why would you use HAVING instead of WHERE)
INNER, LEFT OUTER, RIGHT OUTER JOINS
Subqueries
CASE statements
DISTINCT
Common string, date and aggregation functions
Simple INSERTs, UPDATEs, DELETEs

Personally I wouldn’t consider CTEs among this group but if you  are asked when to use a CTE I would answer with something like:

CTEs can be useful as a replacement for a more common non-correlated sub query in order to make your code more readable, understandable and easier to troubleshoot. You are able to split out some of the code/logic into a separate table expression so that it can be joined to your main query, possibly in multiple places. It is important to consider performance if choosing to do this as often the query planner will choose to execute the CTE multiple times.  For this reason where performance is a key concern I would normally choose to use table joins so that the query can more efficiently process the data and make better use of any table indexes. 

CTEs can also be used to create recursive processes by joining the result of the CTE back onto itself within the table expression. This is useful for creating hierarchical data structures such as HR data where employees are linked to one another via a line manager in the same table.",6.0
g6q70dz,j0b69p,"Thanks a lot! What I’ve seen from an interview in 2019 for the same role (I think it is junior one), one of the question had the need of CTEs and the other one had window functions in it. I guess I’ll try to focus then on window functions for now and try to understand how to use subqueries, because it seems complicated at the first glance lol, thanks one more time",4.0
g6q9p1j,j0b69p,"Ok. It’s all relative...”strong” will mean different things across different people, teams, firms, industries.

You’ll find plenty of info online for window functions and sub queries. Always useful to try and make it practical and explain with an example. 

A common use of a Window function is to order data within groups (as opposed to order the results of the entire query). This is commonly known as the top N per group SQL problem. All experience SQL developers have heard of this some wouldn’t hurt to mention it. You could use an “OVER(PARTITION BY...) window function to group the data by one column and order the data by another. This would allow you to find, for example, the top 3 products sold by each salesperson. 

Subqueries are very common. There are two main types: correlated and uncorrelated). Uncorrelated subqueries are where the query within brackets can be executed as a standalone query, this is then joined to the outer query in the same way that you would join to a table. They are very common and in practice are not required in the vast majority of places I’ve seen then used. People use them to make writing the code easier and where they haven’t taken the time to understand the data model correctly. Table joins should be used 99% of the time. If I was asked to explain where an uncorrelated sub query would be used I’d probably say when analysing/testing data in small sets in order to understand the data model. Eventually to be replaced with table joins which align to the defined keys/indexes on the tables. 

Correlated subqueries are different. These cannot be run in their own as they are joined, via JOIN conditions or the WHERE clause, to the outer query. A common example of this is an EXISTS or NOT EXISTS. As these are joined to the outer query the query planner will usually process them in the same was a table so there is no detriment to performance. They can be more readable and concise than using a table join e.g. LEFT JOIN T2 ON T1.A = T2.A WHERE T2.A IS NULL",3.0
g6qdvzx,j0b69p,Well there goes that boost of confidence i had,2.0
g6qe80f,j0b69p,"Read my first post. I don’t think these are junior skills, I only explained them because the OP asked",4.0
g6qj1hx,j0b69p,Haha thanks for clarification that for me.,2.0
g6qk7sa,j0b69p,Hahaha sounds scary but thank you :),2.0
g77yysy,j0b69p,Ok. My interview went really well. Did you interview yet? If not definitely push that you are trainable and provide examples. I believed this stood out more than anything.,2.0
g78bnc0,j0b69p,"Oh good stuff man :) Happy to hear that! I didn’t have one yet, I found out the interview process so I will first have a home task before I get to the last step which is the live coding&amp;interview",1.0
g78sgmj,j0b69p,Thanks man. I appreciate the encouragement.  I found w3schools to be a great resource,2.0
_,j0b69p,,
g6pmof0,j0b69p,Know how to do window functions.,3.0
g6psfbl,j0b69p,"Different companies have different levels of expectation. If it’s for a junior role, I would be surprised if they weren’t willing to discuss what their expectations were and what could be taught on the job. CTEs and case statements are not that complicated, if you have a google around I’m sure you’ll get the hang of them in no time. Note that CTEs seem to have only been added to MySQL in version 8, that was a relatively recent release (although it’s possibly much more common to be running current versions of MySQL - you’ll find that a lot of companies will drag their heels with RDBMS upgrades!)",4.0
g6q2rsa,j0b69p,"I’ll make sure to clarify everything on Monday with both hiring manager as well as the recruiter in that case, thank you for your time to comment here",2.0
g6qphko,j0b69p,To learn about case statement https://youtu.be/mHWb16MvyMs,3.0
g6qpmai,j0b69p,"Thanks, will check it out after my shift tomorrow! :)",1.0
g6rjksb,j0b69p,whats your background?,1.0
g6rmv7p,j0b69p,"Work background is not even remotely close to this type of position but I’ve been doing lots of python and ever since high school 2nd year of high school (4years ago), I was in touch with it and I kind of have a brain for it in some way, if that makes sense",1.0
g6rpqck,j0b69p,"oh do nto worry then!

When I hire a junior, I epect him to know nearly nothing and I will teach him most of the stuff, as long as he has a good attitude and make an effort to learn",2.0
g6rqk6v,j0b69p,"Aw, well that’s really nice. I hope this goes the same because the very first step is a hackerrank test which is a very rough filter to avoid wasting DA’s time making home tasks etc.  This is what a recruiter told me and also, they would be very basic SQL questions :) Thanks for your input on this topics, gives me a bit of hope!",1.0
g6rlin4,j0b69p,Strong SQL abilities... junior analyst 🤣😂,1.0
g6ro56m,j0b69p,"It is most likely to push some kind of people back! Honestly, the hiring manager is a lead data scientist and they know what they are looking for, it’s not like HR wrote a job posting with absolutely no idea what they need :D Luckily it is internal move for me so it’ll be a lot different than an external one but hey, we’ll see brother :) we’ll see!",2.0
g6rp6y4,j0b69p,"FWIW our head data scientist doesn't know shit about SQL.

Still a joke description. No one with strong SQL skills is applying for junior roles. We hire senior analysts with basic to no SQL skills.",3.0
g6rq3ct,j0b69p,"Ok, this was my fault, I should’ve said exactly what he told me about this. I asked whether it is a JR. position and he confirmed it is, while adding the following:”the position is Junior and requires mainly GOOD sql skills”
I guess you can blame me for not clarifying enough!",1.0
g6se7m6,j0b69p,"Fair enough. In this situation I would think what they mean is being comfortable enough with SQL to use it to explore the data, and look at distributions.

For example say you have a semi-complex equation that isn't quite adding up... are you able to strip it down and use the general framework from the code to find edge cases, look at distributions of the data, etc.?

A lot of my role in data science is using SQL to explore data, ask the data questions, look for fringe cases and *then* write code which will solve for both the general and the edge cases. It's not so much knowing about CTEs, subqueries, unions, etc., as much as it is being able to dig in to the data using SQL and leveraging those functions.",2.0
g6tjc0t,j0b69p,"Literally saying ""hey I hope you're good but we don't have much money to pay you""",1.0
g6of946,j04dkn,"I highly recommend building a date table if you don’t already have one.  They’re very useful.

https://www.mssqltips.com/sqlservertip/4054/creating-a-date-dimension-or-calendar-table-in-sql-server/",7.0
g6ooe11,j04dkn,Seconded.  There's no point in making something a calculation like you're thinking when you can just save a table that contains this information.  You'll also have all of the date specific information available (based on your criteria) for each day as well.  It is a very useful table to have around.,5.0
g6ooowy,j04dkn,"Agreed!  I recently ran into a video from GuyInACube about Time periods, and I added a bunch of those dynamic columns (IsToday, IsYesterday, IsThisWeek, IsLastWeek, etc.) to my vw_DateTable view and MAN it’s been awesome for reporting.  No need to do those calculations at the top of stored procedures anymore.

https://youtu.be/8Mvr_AIw2DU",2.0
g6phj4g,j04dkn,"Thanks a lot for the info, I'll be looking into that :)",2.0
g6pjbaz,j04dkn,"Welcome!  And the video I posted below can be done in any language.  I took his idea and used DateDiff and datepart to make them happen in a t-sql view. 

https://youtu.be/8Mvr_AIw2DU",1.0
g6oqypr,j04dkn,"As u/AXISMGT suggested please use a Date table in your db or dimDate in your DWH for all practical purposes.

However, for the request as stated try this:

    DECLARE @indates TABLE (DateValue DATE);
    
    INSERT INTO @indates(DateValue) -- sample input date values
    VALUES
      ('2013-06-09') 
     ,('2015-12-01') -- notice both records are from same year
     ,('2015-02-09') -- notice both records are from same year
     ,('2020-04-09');
    
    ;WITH cte1 AS
    (SELECT DISTINCT -- removes dupe years
    	YR_VALUE = YEAR(DateValue)
       ,RN = DENSE_RANK() OVER (ORDER BY YEAR(DateValue) ASC)
     FROM @indates
     )
     , cte AS 
     (SELECT Dates = DATEFROMPARTS(YR_VALUE, 1, 1), YR_VALUE, RN
      FROM cte1
    
      UNION ALL
    
      SELECT Dates = DATEADD(DAY, 1, Dates), YR_VALUE, RN
      FROM cte 
      WHERE Dates &lt; (SELECT DATEFROMPARTS(YR_VALUE, 12, 31) FROM cte1 WHERE cte1.RN = cte.RN)
     )
     SELECT Dates, YR_VALUE
     FROM cte
     ORDER BY 1 ASC
     OPTION (MAXRECURSION 0);",3.0
g6rw432,j04dkn,"You're the real MVP. A dates table is the correct way to handle this, but it wasn't what the OP asked about. Nice job.",2.0
g6phhnt,j04dkn,"Wow that code is awesome ! Thanks a lot for your help. 

Indeed, using a date table may be simpler in general !",1.0
g6p7bz5,j04dkn,Thirding /u/AXISMGT's comment. Just make a date table and populate it. We did that a while back and it's been super useful,3.0
g6plo86,j04dkn,"Not sure how you're passing the dates into the script, but either way you need to first get them into a table. After that, I think this works for what you're looking for:

    DECLARE @Dates TABLE (Dt date);
    
    INSERT INTO @Dates
    VALUES ('20200201'),('20161201');
    
    WITH Bounds AS (
    SELECT
        d.Dt
        , StartDate = CAST( (CAST(DATEPART(YEAR,d.dt) as varchar) + '0101') as date)
        , EndDate   = CAST( (CAST(DATEPART(YEAR,d.dt) as varchar) + '1231') as date)
    FROM @Dates d
    ), RunningDates AS (
    SELECT b.StartDate, b.EndDate, RunningDate = b.StartDate
    FROM Bounds b
    
    UNION ALL
    SELECT StartDate, EndDate, DATEADD(DAY,1,RunningDate)
    FROM RunningDates
    WHERE DATEADD(DAY,1,RunningDate) &lt;= EndDate
    
    )
    SELECT *
    FROM RunningDates
    ORDER BY RunningDate
    OPTION (MAXRECURSION 366);

edit: this of course is assuming you don't have access to a date or numbers table, and aren't allowed to create one (which is the case in my environment).",3.0
g6pu2e8,j04dkn,Very smart way to decompose the problem ! I love it. Thanks for the inspiring code,2.0
g6qggcj,j04dkn,"Thanks, I hope it helps.",1.0
g6oafrf,j04dkn,"Kinda like this? You weren't far off. You'll have to figure out what the dates (coming from a table or what?) are and find the min/max.

Edit: set max recursion to it's highest value, first time I've played with that...

    
    DECLARE @StartDate AS DATE
    declare @enddate as date
    SET @StartDate = '2016-01-01'
    SET @enddate = GETDATE()
    
    ;WITH DateCalc AS
    (
    SELECT 
    	CAST(CAST(YEAR(@StartDate) AS VARCHAR)+'-01-01' AS DATE) AS StartDate
    UNION ALL
    SELECT 
    	DATEADD(day,1,StartDate) AS StartDate
    FROM DateCalc
    WHERE StartDate &lt; CAST(CAST(YEAR(@enddate) AS VARCHAR)+'-12-31' AS DATE)
    )
    
    SELECT * FROM DateCalc
    OPTION (MAXRECURSION 32767)",1.0
g6obnh2,j04dkn,"Hi ! 

Thanks for your help. It's close but not exactly what I need. Here you generate days through the years 2017,2018,2019. I only want to generate days for 2016 and 2020. Do you think its possibe ? or do i need to generate 2016,2017,2018,2019,2020 and then filter to keep only 2016 and 2020 ?",1.0
g6oegcn,j04dkn,"Ah, gotcha. You can get the distinct list of years and throw it through a while loop executing what you've already written:

    
    drop table if exists #dts
    select *
    into #dts
    from
    (
    	select '2020-01-01' as dt
    	union
    	select '2016-01-01' as dt
    	union
    	select '2020-02-01' as dt
    	union
    	select '2016-02-01' as dt
    ) a
    
    drop table if exists #distinctYrs
    select *, row_number() over(order by yrs) as rn 
    into #distinctYrs 
    from
    (
    select distinct year(dt) as yrs
    from #dts
    ) a
    
    drop table if exists #dates
    create table #dates
    (
    	dt date
    )
    
    declare @count int =1
    
    while @count &lt;= (select max(rn) from #distinctYrs)
    begin
    
    	DECLARE @startYear AS int
    	SET @startYear = (select yrs from #distinctYrs where rn=@count)
    
    
    	;WITH DateCalc AS
    	(
    	SELECT 
    		CAST(CAST(@startYear AS VARCHAR)+'-01-01' AS DATE) AS StartDate
    	UNION ALL
    	SELECT 
    		DATEADD(day,1,StartDate) AS StartDate
    	FROM DateCalc
    	WHERE StartDate &lt; CAST(CAST(@startYear AS VARCHAR)+'-12-31' AS DATE)
    	)
    
    	insert into #dates
    	SELECT * FROM DateCalc
    	OPTION (MAXRECURSION 32767)
    	set @count +=1
    end
    
    select * from #dates",2.0
g6ph1e8,j04dkn,"Awesome, thanks a lot for your help",1.0
g6m3see,izxzdi,"In your select statement you need to wrap distance difference in a max.

Select
Street_name,
Street_id,
...
Max(distance_difference) as max_distance_difference,
Elev_distance
From view/table
GROUP BY Street_name, Street_id, etc...

Then group by all the columns except for distance difference.
Just comma separate them,  but make sure not to include distance difference",8.0
g6marvd,izxzdi,"Just interested, what is the source?",2.0
g6pgr8g,izxzdi,"If you just wanna see the source table, [here it is.](https://imgur.com/a/7OEC9Z6)

But if you want to know what I'm doing, here's the long version. I'm actually working with a PostGIS database (spatial data) on a personal project.

Context:

I have over 100,000 street vectors in a dataset.  I'm currently working with a [small subset of streets](https://imgur.com/a/r23wL04), but will eventually run my query against the entire dataset.  I [generated points along the street lines at 5 meter intervals](https://imgur.com/a/Ch46xsb) (each point also includes the distance along the line that the point was generated from 0 to N, the starting point of the street is arbitrary and doesn't affect the query).  I sampled elevation values to the points based on where they were located on an [elevation dataset (DEM) green is higher elevs while blue is lower](https://imgur.com/a/2ndaNVE).  I'm basically using this SQL query as a ""slope/hill locator"" along roads by comparing the elevation and distance values.  You can probably already see that I'm gonna be dealing with millions of points, so a PostGIS database is far faster than using GIS/spatial data analysis software with a really slow/inefficient data format.  

I imported the point dataset as a table into Postgresql and that's where I'm doing the query. What I didn't show is that each sample point/node has a unique point geometry column that can be used in other kinds of spatial data analysis software (ie: QGIS, ArcGIS) and I can regenerate the street line segments that have these slopes.

I hope that made sense.  This is more of a GIS thing rather than SQL but I knew SQL was the way to go if I wanted speed with an enormous dataset.",2.0
g6o6i5k,izxzdi,"This would work in MSSQL... please try and see if this works in Postgres or not.

&amp;#x200B;

    ;WITH cte1 AS
    (SELECT subset_max_v2.name AS street_name
          , subset_max_v2.way AS street_id
          , subset_max_v2.id AS node_1
          , subset_min_v1.id AS node_2
          , ABS(subset_max_v2.distance - subset_min_v1.distance) AS distance_difference
          , ABS(subset_max_v2.max_elev - subset_min_v1.min_elev) AS elev_difference
     FROM subset_max_v2
     CROSS JOIN subset_min_v1
     WHERE subset_max_v2.way = subset_min_v1.way
    )
    , cte2 AS
    (SELECT *, DENSE_RANK() OVER (PARTITION BY street_id, elev_difference ORDER BY distance_difference DESC) AS RN
     FROM cte1
    )
    SELECT street_name, street_id, node_1, node_2, distance_difference, elev_difference
    FROM cte2
    WHERE RN = 1",2.0
g6pdju9,izxzdi,"This seems to have returned the same result as what I've got so far (like the first screenshot), but I greatly appreciate the help",1.0
g6mkx1m,izxzdi,You could use the window function Last_Value for each column you want the last value for. You'll partition by street_name and order by distance_difference within the function.,1.0
g6mytp6,izxzdi,"This is usually done using `distinct on ()` in Postgres:

```sql
select distinct on (street_name, elev_difference) street_name, street_id, node_1, node_2, distance_difference, elev_difference
from ...
order by street_name, elev_difference, distance_difference DESC;
```",1.0
g6pea7r,izxzdi,"This is it!  Thank you so much, it worked perfectly.  I took my original query as a sub-query and used your SQL on it, Tadah!",1.0
g6ku0vc,izsytd,"    SELECT yearmonth
         , COUNT(CASE WHEN A_purchases &gt; 0
                       AND B_purchases = 0
                      THEN 'tweedledum'
                      ELSE NULL END) AS A_only 
         , COUNT(CASE WHEN A_purchases &gt; 0
                       AND B_purchases &gt; 0
                      THEN 'tweedledee'
                      ELSE NULL END) AS A_and_B
      FROM ( SELECT yearmonth
                  , customerID
                  , COUNT(CASE WHEN productname = 'A'
                               THEN 'humpty'
                               ELSE NULL END) AS A_purchases
                  , COUNT(CASE WHEN productname = 'B'
                               THEN 'dumpty'
                               ELSE NULL END) AS B_purchases 
                               
               FROM X
             GROUP
                 BY yearmonth
                  , customerID ) AS d
    GROUP
        BY yearmonth",3.0
g6l7xli,izsytd,Thank you so much! This helped me immensely,1.0
g6kjj8m,izoxdp,"    SELECT *
    FROM help
    WHERE 'help' = 'do it for me'",5.0
g6k6rhz,izoxdp,You gotta do it if you wanna learn it,5.0
g6l6yde,izoxdp,"For sure. This is all pretty basic stuff. It might seem overwhelming OP, but if you focus on one task at a time you'll be fine. Just start at the top and work your way down. Google is your friend.",3.0
g6kjbwd,izoxdp,"whoa... TL;DR

what part of that [wall of text](https://en.wikipedia.org/wiki/Wikipedia:Wall_of_text) are you having trouble with?",2.0
g6kj0d5,izotgt,"    WHERE Name LIKE '% % %'
      AND Name NOT LIKE '%  %'
      AND Name NOT LIKE '%   %'
      AND Name NOT LIKE ' %'
      AND Name NOT LIKE '% '",4.0
g6mq4v2,izotgt,this worked!! thank you so much!!,1.0
g6khjp6,izotgt,You can use  CASE  and create three conditions. I'm doing a similar thing now in enterprise. Basically you want to have three conditions so google that.,1.0
g6klz9l,izotgt,"Ooh ooh ooh 

Where len(name) - len(replace(name,' ','') &gt;=2

And len(name) = len(replace(name,'  ','') -- remove double+

And left(name,1)&lt;&gt;' ' and right(name,1) &lt;&gt; ' ' --cant start or end with space 


Replace spaces, count the length.. 1 space per word. 2 spaces = 3 words.

Edit.. these other rules wtf",1.0
g6m9vk9,izotgt,"Does ms sql allow regular expressions? If so then something like

Where Regexp_instr(name, '[^\s]+\s[^\s]+\s[^\s]') &gt; 0",1.0
g6jtpvh,izmxqy,"Go into the database, look at the tables, columns, views, functions, stored procedures, indexes, replication settings, existing connections to the database, query store for top queries hitting the database/tables, row counts, etc... lots of stuff to explore.

Probably just a question to gauge how comfortable you are using the admin tools, maybe?",3.0
g6ju348,izmxqy,"thanks I'm not really that experienced in DBs so that was pretty helpful, the thing is that I have received some CSV files and was told to create a DB based on these files, then to explore them, which made me a bit confused because there isn't really that much to do.",1.0
g6kdm9c,izmxqy,Import them as tables using the data import wizard and then select * from the created tables and have a look at what's in the data. Have a look at of and how they link together.  Maybe check for orphan and duplicate records.,2.0
g6ks1bh,izmxqy,"good points, thanks.",1.0
g6jdnit,izkd0f,"Lets say your column is called Ref and the value is always 100. In your select query just write it like this

    SELECT A.Forename, A.Surname, 100 AS Ref
    From dbo.table

So its *value* AS *column name.*",3.0
g6jozwm,izkd0f,Thanks a lot!,1.0
g6jdm39,izkd0f,"As simple as:

&gt;`select 'MadeUp' as FakeColumn`  
&gt;  
&gt;`,RealColumn`  
&gt;  
&gt;`from WhatEver`",3.0
g6joybq,izkd0f,Thanks a lot!,1.0
g6jzcjx,izk4fy,"What kind of work are you doing, or looking to do?

&amp;#x200B;

SQL itself is useful, but you'll find you often need to pair it with something else depending on job title - a developer might need to learn SSIS for building new tables, putting out extracts, etc. An analyst might need to pair SQL with some like Tableau, or PowerBI, to create reports and visualizations. You might even benefit from dipping into web development.

&amp;#x200B;

Once you figure out what general ""track"" you're on, it's a little easier to point you towards good classes for it, though you can generally just search ""SSIS"" or ""Tableau"" on Udemy and find the high rated course.",4.0
g6jzscy,izk4fy,"Thanks for the reply! I'll check out Tableau and PowerBI. I work in finance, so I'm not necessarily on the developer track, but I'm trying to learn tools that I can use to help automate some of our processes and reporting. I know tableau would be particularly useful for reports and visualization in general.

Edit: grammar",2.0
g6k3o2t,izk4fy,"A financial analyst type role would definitely benefit from things like Tableau and PowerBI. Depending on what your organization is already doing, you can also pick up something like Python, and then use that with the pandas library to do some really neat and automated analysis/prediction. It can be a (relatively) easy way to wow people if they haven't seen it before.",3.0
g6kbwzi,izk4fy,"I'll check out Tableau and PowerBI first, I know some areas of my company already use Tableau, so I should learn it sooner rather than later. I'll try to pull up some examples of what python can do as well, I know about it, but I've never seen it in action. Thanks for the reqs",2.0
g6kjlm8,izk4fy,"PowerBI premium has paginated reports basically SSRS using report builder. I think you’ll find these much more flexible and fitting to building financial package reports. I’d argue most people don’t know what it can do, they think it’s just paged reports but it’s so much more than that. That’s why they’re locked in premium. Pro will only get you Dashboards and the new “reports”.",1.0
g6kkxu3,izk4fy,Python and PowerBi,1.0
g6jbher,izjze9,"I just a VScode workspace and use a folder and file structure to find stuff easily.
There are also many extensions for each SQL flavour to syntax highlight, autocomplete and execute them directly too.",10.0
g6l99ut,izjze9,"I couldn't find an extension (that would work) to connect to db2. Whomp whomp. 

I tried using a vsCode workspace but I couldn't figure out an easy way to search through all code in all files (whick I've got to have). I'm not saying it can't though... I'm just a total noob at vsCode and need to learn more.",1.0
g6mr860,izjze9,"Edit.... Find in Files you can also turn on regex searching

https://youtu.be/HnjDlqAdn2I",1.0
g6mrbpj,izjze9,Also VScode has fantastic git integration,1.0
g6jnur0,izjze9,"github is perfect

Storing code and its version history is literally what it's for. Just need to figure out how to integrate it in your day to day workflow so that committing updates becomes second nature.",10.0
g6kbbxj,izjze9,This. I’m surprised how many developers can’t imagine working without source control- except for the DB. Source control has saved my ass many times over the years!,1.0
g6kdd6f,izjze9,"Honestly if I had my way every movement of data, update of code, etc. would be logged. I should be able to look at a snapshot of business operations at any point in time.",2.0
g6l9kbd,izjze9,"I moved all my SQL statements and snippets into a repository and use Sublime Text as my editor of the file structure. So far, I think this may be a winner. Thanks for the input!!",1.0
g6jdfkr,izjze9,"i use [ultraedit](http://ultraedit.com), a great text editor that is aware of SQL (when you use the `.sql` file extension instead of `.txt`) and it will do things like colour code keywords and literals, match quotes and parentheses when you type them, and it has a great find and replace function

my sql files are stored in a simple Windows folder, one per project",6.0
g6jhu3y,izjze9,notepad++ another similar option to consider.,10.0
g6kjfi7,izjze9,Huge fan of notepad++,8.0
g6km9uw,izjze9,It's multi-file search and replace function is invaluable.,2.0
g6l9dv5,izjze9,"I may try that out. I'm currently using sublime text, but I hear about ultraedit pretty regularly.",1.0
g6jayva,izjze9,"Look into Jupyter notebooks, I think Azure Data Studio finally supports them.

The benefit of doing this is you can just click the run button and run the specific query from the notebook itself and the search works well if you add a text blurb or know what parts of the query to search for.",4.0
g6jdoau,izjze9,"https://docs.microsoft.com/en-us/sql/azure-data-studio/notebooks/notebooks-guidance?view=sql-server-ver15

ADS Jupyter notebooks was  one of the most useful things I've seen presented at our local user group (before the rona apocalypse). 

Think OneNote with code you can actually run.",5.0
g6jnk8c,izjze9,"I use VSCODE to edit my SQL. I work on multiple RDBMS's and one is a legacy beast with no extensions (I just use the ANSI SQL extension for that one). So I just keep each in separate directories. I use the git CLI for source control on each of them. I execute them using each program's native tooling. VSCODE is just such a sleek, full featured text editor, and frankly most text editors with SQL linting are going to be a way better experience than editing the files in each RDBMS's native tooling.",4.0
g6ku4tc,izjze9,I use voidtools everything to search for phrases in filenames and content. Has a nice preview function as well.,3.0
g6lac2j,izjze9,Never heard of this one! I'll check it out on Monday when I'm being paid. Thanks!,1.0
g6l8wok,izjze9,I work at place that restricts everything we do including installing software. To get around this limitation I just store all my code in .sql files on my laptop and created a python script that will search the content of all files in that location for any text. The python search is very fast and has never let me down.,3.0
g6jdmr5,izjze9,"[Aginity Pro](https://www.aginity.com/products/aginity-pro/) has a built in catalog for you to store and reuse code. You can even call SQL stored in the catalog in future code using the catalog name, invoked with the @ symbol. Free for personal use.",2.0
g6jeixz,izjze9,"Are you using a GUI DB program like dbeaver?  

Just put your .sql scripts in a DB project like you would with a project in any other language.  Then you can just edit + execute them all in one place inside your DB GUI.  

No need to copy and paste back and forth all the time into some separate notes program.",2.0
g6l9u3y,izjze9,"You guessed it. I use dBeaver, but I don't think I can search though all my code (at least, I haven't figured it out yet). I can search through a file at a time, but sometimes I'm not sure where a particular code section is but I'm really good at remembering certain pieces to search for due to some of my own coding standards. Am I missing a global search feature somewhere?",2.0
g6laews,izjze9,"There's this:  https://dbeaver.com/docs/wiki/File-Search/

Also keep in mind that that your dbeaver workspace is just a folder full of files, so you can always open that folder in vscode or any other editor too.

But makes sense to just leave them in the dbeaver workspace so you can execute them easily without all the copy and pasting.",1.0
g6lalmc,izjze9,Bruh! Thanks. I swear I looked. I guess I just hadn't had enough coffee yet. I'll check this out when I boot back up on Monday. Thanks for the help!,2.0
g6jdfmf,izjze9,"Thanks for asking this, I’ve been trying to find a decent solution also !",1.0
g6kb1fg,izjze9,"It costs money, but we bought redgate and I've made a bunch of snippets for our team and we are all loving it.",2.0
g6jfxcs,izjze9,"I use devart's sql complete at home (because of price) and redgate sql prompt at work.

The snippet managers in both those programs are fantastic. 

I havent used Apex, or know if apex has one. But apex is free.

Edit: mssql / ssms.",1.0
g6jhtdv,izjze9,"i used TOAD datapoint, and it's saves recent queries, then we keepa enterprise github repo of useful queries.",1.0
g6jzkj5,izjze9,I create templates in SSMS for commonly used stuff.,1.0
g6k4jmf,izjze9,"You mean [snippets](https://docs.microsoft.com/en-us/sql/ssms/scripting/add-transact-sql-snippets?view=sql-server-ver15)? 

I use those as well. Very useful to just rightclick and insert commonly used queries. I use one that gives me my 'basic' tables all joined together that I need very often and I have another snippet that I can use as 'surround' that gives me the pivot of the results (as I always forget that syntax).

If you commonly use SSMS, I'd highly recommend it.",2.0
g6knx5h,izjze9,"I save SQL scripts from SSMS, and push that into a git repo.

&amp;#x200B;

All 90 of them.",1.0
g6koub7,izjze9,"I just use the save feature in ssms and Microsoft's native file structure. 
 
I have a parent directory named ""where we left off"" where I can save any queries that I can't give a meaningful name/save location. Inside the parent directory, I create new subfolders which I title with the date (eg 2020-09-25, 2020-09-24 etc). 
 
Anything that doesn't go in these folders is either a) turned into a Sql object for reference by anybody, b) saved into a project related folder, or c) not worth keeping.",1.0
g6jac8f,izjme6,"So part of the problem is that your data's not properly normalized. Since everything's a `varchar`, you can have duplicate packages (with the same host) and other potential issues. _But_, assuming you have unique package/host combinations (i.e., for each host, each package is listed no more than once), you can just group by the package name and use a `HAVING` clause to figure out which packages are only on 1 host. So something like this:

```
SELECT packagename, MIN(hostname) 
FROM tablename 
GROUP BY packagename 
HAVING COUNT(*) = 1
```",2.0
g6jb144,izjme6,"Correct, I currently have two tables which are used for this ""system"":  
\- table\_1: Contains only the packages, only for 1 given host at a time.  
\- table\_2: Contains all of the dumped data from table\_1 which will always contain duplicates.  


I just entered the query you suggested in your comment (on table\_2), and it seems to do the job perfectly.  


Thank you very much for this, it has saved me a lot of time!",1.0
g6jc1yt,izjme6," Quick question, though, if I wish to designate which host I search for (only the unique packagenames), how would I go about this?  
I just tried:  


    SELECT packagename, MIN(hostname) FROM loggedinstalledpackages WHERE hostname = '$hostname' GROUP BY packagename HAVING COUNT(*) = 1;

However it was a huge failure, as it returned all packages relating to that host.",1.0
g6jcaos,izjme6,"Yes, it would do that, because you're specifying only 1 host. So each package will only be listed once (so the `COUNT(*) = 1` will be true for all packages). If you want to filter the list of 1-host packages to those residing on a specific host, you could add that to the `HAVING` clause. For example `HAVING COUNT(*) = 1 AND min(hostname) = '$hostname'`.

Although make sure you're using prepared statement/parameterized queries to avoid SQL injection.",2.0
g6jcupj,izjme6,"Thank you very much, that makes a lot of sense!  
Issue solved :-)",1.0
g6j9c3u,izjme6,"If each hostname only has one unique packagename that you need to expose, you could do this:

`select hostname, packagename from tablename group by 1,2 having count(packagename)=1`

this should return the hostname and packagenames where packagenames only appear once per hostname.

EDIT: you may need a distinct in the count,

`select hostname, packagename from tablename group by 1,2 having count(distinct packagename)=1`",2.0
g6jdvi8,izjme6,"&gt; this should return the hostname and packagenames where packagenames only appear once per hostname.

nope

if you have

    host1 packageA
    host1 packageB
    host1 packageC

then, because packagename is included in the GROUP BY, you'll get one result row per distinct package, i.e. 3 result rows where each one satisfies `count(packagename)=1`

adding DISTINCT to the count doesn't help",2.0
g6j9wml,izjme6,"Thank you for this!  


I applied your query, and it did work as expected, however, I am seeking to select 'packagename' which only appears on the given host, so for example:  


hostname = 'server\_a' | packagename = 'apache2'  


Where 'server\_a' is the only server to have 'apache2' installed on it.  


What you have suggested may be valid for another tab, so I will most certainly use it!",1.0
g6iilov,izer25,"Does it need to be online? Why not install mysql on your PC? That's the most free option I can think of.

I don't think setting up an RDS instance is too complicated really - just follow the setup wizard - but its true there are a lot of things to configure. Regardless, I don't think that would be cheaper than Digital Ocean. Does RDS have a free tier option? If so, I'd be happy to help walk you through the RDS setup process.",2.0
g6isl6i,izer25,"AWS RDS has a free for 1 year deal. Its a small database but should work well for your needs.  


Oracle (if you're interested in learning Oracle DB) has an 'always free' tier with two databases you can set up. Again, small but useful.",1.0
g6j66w0,izer25,I know that we use AWS for similar solutions. I haven't used it personally to set up  DB but I've noticed an option to use it for MySQL or Oracle DBs when choosing your specs. This makes me think that it would come with some preset configuration that might be useful.,1.0
g6jd1y9,izer25,"If you use SQLite, you can eventually stick it on pretty much any host, but it also works well locally.",1.0
g6iix7l,izcvma,"Pretty crude, but a regex solution is to match either the first four letters or the first three letters plus a number: `'^[A-Za-z]{4}|[A-Za-z]{3}[0-9]'`.",6.0
g6j94g5,izcvma,I don't think this captures the requirements in the post for either of the cases.,1.0
g6jjt8q,izcvma,"I'm not sure which requirements aren't met (at least from the small sample of data provided) but I think this slight modification would be an improvement anyway: `^[A-Za-z]{3}[A-Za-z0-9]{1}`.

Is there a particular issue you can see/foresee?",1.0
g6vjbuu,izcvma,"https://regex101.com/r/b7cPRR/1

Here, it arbitrarily takes the first four characters except for PG5FD where it fails to match, because the digit is before the fourth place.",1.0
g6jtw29,izcvma,Hey I appreciate the input! I’ll give it a shot! Thank you.,1.0
g6k6kxd,izcvma,"If the output isn't right, let me know. There are often edge cases that aren't immediately obvious and I'm sure there are other ways to tackle it.",1.0
g6iaczm,izcvma,"I mean, this seems like a case where you can waste a lot of brainpower being cute or you can just get it done through brute force. Just do

`CASE WHEN LEN (*whatever function fetches all letters before number*) &lt; 3 THEN LEFT ( *your string*, 4 ) ELSE *whatever function fetches all letters before number* END`",5.0
g6j2xfj,izcvma,This. Sql server - A patindex ‘%[0-9]%’ within the case.,1.0
g6i5s3w,izcvma,"Regexp_subtring (input, ‘[A-Za-z]+’,1,1) is close but doesn’t have 4 minimum characters. Anyone know how to do this?",1.0
g6i4570,izcn2z,"If it is helpful here is my entire code for the database


DROP TABLE computer_worker
;
DROP TABLE computer
;
DROP TABLE warranty
;
DROP TABLE worker
;

CREATE TABLE warranty (
    policy_id VARCHAR2(100) NOT NULL,
    policy_name VARCHAR2(100) NOT NULL,
    policiy_length VARCHAR2(100) NOT NULL,
    PRIMARY KEY (policy_id)
)
;

CREATE TABLE computer (
    serial_number VARCHAR2(100) NOT NULL,
    CPU_speed VARCHAR2(100),
    memory_capacity VARCHAR2(100),
    price NUMBER(7,2) NOT NULL,
    policy_id VARCHAR2(100) NOT NULL,
    PRIMARY KEY (serial_number),
    FOREIGN KEY (policy_id) REFERENCES warranty (policy_id)
)
;

INSERT INTO warranty VALUES ('1234', 'Liability', '10 years')
;
INSERT INTO warranty VALUES ('2256', 'Malware', '2 years')
;
INSERT INTO warranty VALUES ('5556', 'Liability', '5 years')
;

INSERT INTO computer VALUES ('555k1', '2.0 GHz', '1 TB', '500.00', '1234')
;

INSERT INTO computer VALUES ('665jk', '3.12 GHz', '500 GB', '250.55', '2256')
;

INSERT INTO computer VALUES ('223ij', '7.0 GHz', '2 TB', '5000.00', '5556')
;

/*
SELECT serial_number, price, policy_name
    FROM computer
    INNER JOIN warranty
    ON computer.policy_id=warranty.policy_id
    ORDER BY price
;
*/

CREATE TABLE worker (
    employee_id VARCHAR2(100) NOT NULL,
    first_name VARCHAR2(100) NOT NULL,
    last_name VARCHAR2(100) NOT NULL,
    birth_date DATE,
    PRIMARY KEY (employee_id)
)
;

CREATE TABLE computer_worker (
    employee_id VARCHAR2(100) NOT NULL,
    serial_number VARCHAR2(100) NOT NULL,
    hours_worked NUMBER (5,1) NOT NULL,
    PRIMARY KEY (employee_id, serial_number)
    FOREIGN KEY (employee_id, serial_number) 
    REFERENCES worker, computer (employee_id, serial_number)
)
;",1.0
g6icw7m,izcn2z,"Separate your FK creations. Add a comma after the PK creation.

I had to change VARCHAR2 to VARCHAR, and NUMBER to NUMERIC, but have changed them back below.

&amp;#x200B;

CREATE TABLE computer\_worker (

employee\_id VARCHAR2(100) NOT NULL,

serial\_number VARCHAR2(100) NOT NULL,

hours\_worked NUMBER (5,1) NOT NULL,

PRIMARY KEY (employee\_id, serial\_number),

FOREIGN KEY (employee\_id) REFERENCES worker(employee\_id),

FOREIGN KEY (serial\_number) REFERENCES computer(serial\_number)

)",1.0
g6qj74s,izcn2z,Hey that worked! Thank you!,1.0
g6hvevh,izb222,"You don't really need admin rights

https://www.tutlinks.com/install-postgresql-without-admin-rights-windows/

You could try sqlite, it has some differences (all SQL flavors are different, so you won't find a 1:1 replacement for postgres), but it's a lot more portable.",1.0
g6hk4by,iz991w,So what do you have so far? Work up the select statement to return the customer I'd first. Step this out. Once you have that select statement we can make a few minor changes to add an IN variable to a stored procedure.,1.0
g6hkqmt,iz991w,"the main problem and where im stuck, is that i dont know how to show all the people who bought to product, i have to use repeat because multiple people bought it.",1.0
g6hl9ip,iz991w,"Ok. So what are the tables involved specifically? And, what is the primary key and foreign key of each table.",1.0
g6hnpcf,iz991w,"Sorry, I'm heading out in a few, your SQL select statement is going to look sort of like this,

select c.id, c.firstname,c.lastname ,
   Sun(p.total)
From clients c
Join purchase p on c.x = p.x 
Join products pr on p.x = pr.x 
Where pr.id = &lt;whatever product id you are searching for&gt;

The .x values are the primary key of main table to foreign key of other table. For that you need to understand how to join tables.",2.0
g6hos2d,iz991w,"Thanks , your awesome!!",1.0
g6hpbwn,iz991w,"The table should look like this

Id -------name------------amout
2---------jhon--------------45000
3----------anton-----------65000

But y have to show multiple buyers at once.

That sentence works for that?",0.0
g6hatfr,iz814i,"What do you want to do with this? Are you planning on sending data to someone else, like in the .csv example?

The [PostgreSQL documentation is here](https://www.postgresql.org/docs/11/storage-file-layout.html), but basically, I cannot think of a single problem for which the best solution is for users to investigate storage on disk. There are almost certainly better ways for you to approach any question you might have.",1.0
g6hbrcg,iz814i,"Thank you for the prompt reply! I'll give a look at that link.       
One specific example is distro hopping. I have most of my files in my \`home\` partition, that way I can install a new distro and still keep my files. I guess I will find the answer to my question in your link, I'll comment here after that.",1.0
g6jbcii,iz814i,Copying files is dangerous although in theory it's possible if the architecture and version matches so it's very much not recommended. Additionally if the database is up you will end up with inconsistent data as it changes during the copy operation. Hence there's the dump &amp; restore operation that'd transfer the data over.,1.0
g6k67c8,iz814i,Thank you for explaining!,1.0
g6he3be,iz814i,"I just read the article in your link. It looks like most of the date is under \`/var/lib\`.     
Going back to my example, if I create a PostgreSQL database, and later want to send it to a friend, what would be the best way to do so?",1.0
g6hfuuf,iz814i,"Yeah, probably look into Database Backup / Restore options. Check out [this guy](https://www.postgresql.org/docs/8.3/backup-dump.html), affectionately known as ""dump"".

Physical storage itself tied in with all kinds of complications of transaction history / logs / locking, clustering / instances, partitioning, users, etc, etc, etc. All those features you say ""I'm glad the DB handles that so I don't have to"" - and the theme is, let the DB handle them :)",1.0
g6hgfjm,iz814i,Thank you for your help!,1.0
g6i3ien,iz814i,"Sorry to bother again but, any reason your link is one of the 8.3 version instead of the current one?  
By the way, this is the link to the corresponding to the current one:  
[1] : https://www.postgresql.org/docs/current/backup.html",1.0
g6i5u5w,iz814i,"It was what I most quickly found googling things, no particular reason at all, didn't pay a lot of attention to versions",1.0
g6hbw4b,iz791i,You can't use a column alias in the GROUP BY clause because the SQL order of processing.  SQL queries that have GROUP BY execute GROUP BY prior to the SELECT clause.,3.0
g6hcsz5,iz791i,You're right. So do you think it's okay to leave this redundance in the query?,1.0
g6hd6i4,iz791i,"Yes, it's fine.",2.0
g6h71yb,iz791i,"&gt; since I couldn't use the alias in the group by

really?

&gt; If it's possible to remove this repetition, would you suggest avoiding it?

yes

you can always use aliases with an **inline view** which is another name for a subquery in the FROM clause

    SELECT [Initial letters]
         , SUM(population) AS [Population]
      FROM ( SELECT SUBSTRING(name,0,2) [Initial letters]
                  , population
               FROM state     
           ) AS d       
    GROUP 
        BY [Initial letters]",1.0
g6h94eg,iz791i,"What the OP said is correct, you cannot use column alias in the group by. Once you select it from the subquery, it is no longer an alias in the parent query where it is grouped.",3.0
g6hkmbw,iz791i,"Although undoubtedly it's a creative way to avoid the problem I feel like this solution is not really practical in my case :/ but thanks anyway, might come handy some other time, maybe with more complex queries.",1.0
g6huhg0,iz791i,Interested why you think this isn't practical.,1.0
g6ijobs,iz72pd,"yes you guys are right, sorry about that.

here is the solution:

    mysql&gt; select avg(num) from(
        -&gt; select count(msg_id) as num from messages
        -&gt; where msg_type = 'USER_MESSAGE'
        -&gt; group by date(msg_date)) as subq;
    +------------+
    | avg(num)   |
    +------------+
    | 14322.7917 |
    +------------+
    1 row in set (0.54 sec)",3.0
g6h5eqt,iz72pd,nvm I got it ty.,0.0
g6h8iiy,iz72pd,That's good you figured it out. You should post your solution here too in case someone else comes along with the same problem.,8.0
g6hupcr,iz72pd,"What the other poster said. No matter how simple. Otherwise, you just meme yourself",5.0
g6ihw8v,iz72pd,Poat it here!,2.0
g6i8jnx,iz6mza,"If you are on mssql 2014.. replace stuff with STRING_AGG(). Its an aggregate function.


Your second picutre...

Let's your pic 2 output was a cte.


With cte as

(

Select area, Monday, Tuesday, Wednesday 

From picture2

)

Select area, ca.column, ca.val

From cte

Cross apply ( 

values 

(Monday, 'monday'),(Tuesday,'Tuesday')

) ca (val,column)



Will basically unpivot the data to match more closely to pic 1.  Cross apply ( values (originalcolumn, valueinnewcolumn) ) alias (aliasColvalue, aliasColName)  is basically an unpivot.

Edit: if you gave us aliases in your select, and what you wanted the final output to look like, we could give you better answers.",1.0
g6k3qlj,iz6mza,"I was able to solve this by using a control in a proprietary development library (DevExpress). I futzed the aggregation by wrapping the above query in a select statement and going MAX(ShiftStartTime) which since there was only one row for each result I was grouping by, works fine.",1.0
g6h664x,iz1n34,"Your company either doesn't understand what it is asking for, is taking advantage of you, or both.  Designing data warehouses is an entire discipline and data architects are a highly sought after commodity.  Generally data architects are not administrators, so it's likely a database administrator would be required to set up and manage your database after you designed it to keep it running and handle failure recovery.  That's without considering the orchestration and etl tooling required if you wanted too automate your data uploads. For example, managing type 6 scds from uploaded flat files (assuming they are uniform which is unlikely) would be very complicated given the multiple touch points on each record for each of your uploads.  Tell your company you want at minimum some training and a consultant to help with design and maintenance documentation.  If they can't bother with that then they are better off just using the spreadsheets.  I'm a data architect and etl developer and I can tell you that it would be very easy to get in over your head designing and building something like this, especially when the business isn't willing or isn't smart enough to invest in the required technical knowledge .",2.0
g6g5diu,iz1n34,"you can use any database to implement a data warehouse (which is what you are looking for)

some are better than others though, in particular ones with columnar data storage

you can use the MERGE INTO statement to load slowly changing dimensions

I think the best thing you can do at this point is to spend the time reading the Kimball books or  maybe Agile Data Warehouse Design",1.0
g6glgc1,iz1d76,"I believe what you want is for the users to enter the data via a view. Construct the view over a query joining the three tables. Then when they enter data the update will not occur unless all three fields are filled in. Not sure how they are entering data.

Either that or use two-phase commits to create a record. The user enters the widget and then decides he does not want it or does not know the region. When he backs out of the data entry have the code do a roll back to undo the record creation. If he finishes then do a commit.

In the commercial applications I work with they generally present this type of problem as a series of pop-up screens and the user is expected to complete each screen before the record is created. Once you get past the last screen the data from all screens is committed to the database.

I have also see commercial applications that have the database set a default value if none is given by the user. This can either be a valid value or a signal value. If it is a signal value then data quality reports are used to notify people the process is incomplete.",2.0
g7amgiy,iz1d76,"Thanks a lot for the input, makes sense.",1.0
g6g9u10,iz1d76,"&gt;  suppose what I'm dealing with is a cyclical dependency--can't have widget regions without a widget, can't have a widget without a widget region

This can easily be done using deferrable constraints: 

```sql
create table widget
(
  widget_id integer primary key, 
  color varchar(20) not null, 
  region_id integer not null
);

create table region
(
  region_id integer primary key, 
  name varchar(100) not null,
  widget_id integer not null
);

alter table widget 
   add constraint fk_widget2region
   foreign key (region_id) references region(region_id)
   deferrable initially deferred;

alter table region
   add constraint fk_region2widget
   foreign key (widget_id) references widget(widget_id)
   deferrable initially deferred;
```

Then you can do something like this:

```sql
begin transaction;
insert into widget (widget_id, color, region_id)
values (1, 'black', 100);

insert into region(region_id, widget_id)
values (100,1);
commit;
```",1.0
g7amjya,iz1d76,"Thank you very much, very helpful.",1.0
g6fv9jp,iz0jzp,"Sounds like you're looking for joins or subqueries

edit: noticed the tag and I'm not experienced with mySQL",2.0
g6fwgde,iz0jzp,"but it's different than joins in that they solve the queries not using the word ""join"" at all.

that's what's really making my head swim, I would've looked up joins and learned the crap out of em, but like our class only took ""the natural join"", it's basic SQL.

those query examples though are anything but basic, and since they don't use join, I honestly have no clue what they are.

thank you for the term ""subquery"", I searched that and YES! they do have that in the solutions, but add to that multiple tables instead of one, but it's a great start nonetheless thank you so much dude!",1.0
g6fxw95,iz0jzp,"&gt;  our class only took ""the natural join""

lord love a duck

nobody uses natural joins",1.0
g6gmokb,iz0jzp,"One way of avoiding using JOIN is to just list the tables then do all the JOINs in the WHERE clause instead. Personally I find this much more confusing.

Here is an example

SELECT A.Field1

,A.Field2

.

.

B.Field1

.

FROM TableA A, TableB B

WHERE A.IDFieldforA =B.IDFieldForA

In my example IDFieldforA is the primary key for TableA and is a foreign key in TableB. TableB should also have its own primary key for simplicities sake IDFieldforB.

The letter after the table is an alias for the table so you don't have to type out TableA.Field1.

Hope this is clear.",1.0
g6gnq81,iz06nd,So the new data is always additive? Is it possible to just build new tables for the new data and then use a view over all the tables? If this is going to grow you should look at putting stuff on different disks etc.,1.0
g6j4ivn,iz06nd,"Hey, thanks for your reply. That's not a bad idea. It would work especially well for all inserts after the first large one. Would you also split the first insert into different tables also? The first insert will be 100's of millions in batches of inserts. Any subsequent is about 0-3% the size. 

The selects are the biggest performance issues so perhaps separating the tables would lessen the long term strain but wouldn't it exponentially increase the select time due to the multiple tables?",1.0
g6ip3nd,iz06nd,"You disable the index then perform batch inserts/updates based on the values found in the index you just disabled? If that's correct, it seems really counter intuitive...",1.0
g6j4nsa,iz06nd,"Hey, thanks for your reply. 

So I asked previously in reddit or stackoverflow the best way to handle these massive inserts and was told that the index would massively slow down the inserts. The best way to avoid insert performance issues was to disable the index and then run it afterwards. The rebuild on 100 million records takes about 3 minutes.",1.0
g6j4ztc,iz06nd,Well the more efficient way is to do batch inserting. Are you doing the inserts in one single transaction? Or have you batched them into smaller buckets? Like 10M rows each?,1.0
g6k798g,iz06nd,"Hey, 

Thanks again for the reply.

So we batch them into 200,000 rows per insert. When I say 'WE', I mean 'I' :(

I made the decision to disable having read that performance would take a massive hit once you start getting into the millions of records in the table.

Do you think inserting 200,000 per batch would affect performance if the index was on? It currently takes about 70-80 minutes to insert 90 million records with the index off. It never occurred to me to try it with the index on.",1.0
g6kvhpw,iz06nd," Have you seen the execution plan for one of your batch inserts? The bottleneck could be from the source query and not on the inserts.
I have a feeling the updating of the index is not the issue here, but then again I'm not aware of your infrastructure and what sort of hardware you got",1.0
g6fu5jb,iz063r,"in MySQL `SELECT ... INTO` does not include creating tables

i believe that's MS SQL syntax",2.0
g6fwu15,iz063r,Oy🤦🏽‍♀️ do you know what the corresponding MySQL syntax is / where I can find that info?,1.0
g6fylkm,iz063r,"    CREATE 
     TABLE ttt2 ( ccca, cccb, ... )
        AS SELECT ccc1, ccc2, ...
             FROM ttt1 

&gt; where I can find that info?

in [da manual](https://dev.mysql.com/doc/refman/8.0/en/create-table-select.html)",2.0
g6fypim,iz063r,Thank you!!!,1.0
g6fe5fk,iyxez5,"&gt;  And then? I got stuck with my understanding here.

and then the WHERE clause of the outer query is evaluated

notice that the subquery is **correlated**, which means it makes a reference outside of itself to the outer query

basically, the outer query will return each row of `table1` if a matching row based on the value in `table1.col1` (disregarding the typo in your post) exists as a value in `table2.col2` 

&gt; Is WHERE EXISTS together is an operator, or just EXISTS. 

just EXISTS... sometimes NOT EXISTS

WHERE is a keyword that identifies the WHERE clause, one of the six possible clauses in an SQL query

`WHERE TRUE` is a perfectly normal WHERE clause, and you will also sometimes see `WHERE 1=1`, but these situations are for another thread",2.0
g6fn4yx,iyxez5,thanks a lot! this really helped,1.0
g6fe821,iyxez5,"starting from the bottom:

&gt; Is WHERE EXISTS together is an operator, or just EXISTS

technically, neither is. in SQL parlance, ""CLAUSE"" is more or less a part of a statement that contains multiple other variable parts, so 'select CLAUSE', 'from CLAUSE', 'where CLAUSE', etc.

""Operator"" is a symbol that denotes an OPERATION (+, -, AND, etc.). Usually (logical operators seem to be an exception) the term ""operator"" is not applied to a function-like name or anything that resembles an english word.

So, ""WHERE"" is a clause (a part of sql statement that will contain other syntax parts), ""EXISTS"" is a logical operation, it can be used in most places where a logical operation is permitted.

EXISTS takes a correlated subquery as a parameter.

&gt;  Postgres will execute the subquery first, then ...

logical flow of execution of a select statement is from -&gt; where -&gt; group by -&gt; having -&gt; select -&gt; order (you can see more detailed one in documentation).

So, Postgres executes (logically) the FROM clause of the outer query first getting the ""base"" result set, then for each record of the base result set executes the subquery, checks if any records have been returned by the subsquery returning TRUE and keeps base result set records where overall ""WHERE"" clause returns TRUE (there could be other conditions).

In your particular example, Postgres will keep rows where the subquery returns something.",2.0
g6fekh6,iyxez5,"&gt; EXISTS takes a correlated subquery as a parameter.

most often, but not always",1.0
g6ffnro,iyxez5,"i kind of wondered about this, did a quick check on ms sql - exists doesnt take neither values constructor nor a table-valued function.

What do you have in mind that's not a subquery?",1.0
g6fghpn,iyxez5,what i had in mind was a non-correlated subquery,1.0
g6fgr34,iyxez5,nitpicky ),1.0
g6fn3yq,iyxez5,"thank you, this helped me",1.0
g6f8bzf,iyxez5,"If exists comes back as true, then the row shows up. If it comes back as false, then it doesn’t.",1.0
g6fna9d,iyxez5,"ohhhhh !! thanks a lot. Most documents I read only said EXISTS returns boolean so my dumbass stuck with WHERE TRUE, and then what?  
Now it makes sense to me. Much appricated!",2.0
g6f5gk0,iyx03e,"Hello u/Aquamentus92 - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g6f6m9d,iyx03e,MS SQL,1.0
g6faisx,iyx03e,bad format? bad app to read whatever format you have?,1.0
g6fauye,iyx03e,"I dont think so as the exports used to be read properly without issues. Nothing has changed on the SQL automated sending side or mobile app receiving side, except for the fact that it is working one day and next. It is quite the conundrum.",1.0
g6fdv8x,iyx03e,Was there a recent update of the mobile app? Since the sql didn’t change I would investigate there,1.0
g6fewj5,iyx03e,"It's a possibility but hard to pinpoint it as the cause, as the SQL export gets sent to numerous people and there have been about 3 reports of the text file not reading properly. It's a strange issue because it happened in at least 3 different instances simultaneously for 3 different end receivers, presumably on different brands of device/apps",1.0
g6ff607,iyx03e,Oh I get it. Sounds like a pain - if some a receiving it fine I would request the specs of those who don’t - now if they are all different that will be even more fun,1.0
g6fgcva,iyx03e,"It's currently being requested, I'll post a second response when it comes in, thanks for your efforts.",1.0
g6g7i15,iyx03e,"At least 2 of them seem to be on the latest version of iPhone iOS using the native app for email serviced by Verizon, neither of which have been altered or updated since the beginning of these occurrences, so they say. As an additional, it looks like it is seeming to be this way with any text file they received, not just from SQL, so it may be an entirely different issue altogether.",1.0
g6fcg27,iywxdt,"Jesus, they are releasing faster than I can be bothered to upgrade!",8.0
g6g1ov0,iywxdt,Right - I was just stoked to get onto 12 in production. Sheesh.,4.0
g6g6now,iywxdt,well now is a good time to upgrade to 12 while the fools upgrade prod to 13 on the first day of release,6.0
g6h6cw9,iywxdt,"I'm still at v10 in my company lmao, too lazy to get IT to upgrade

Does anyone know if this would improve significantly improve QGIS performance on layers coming from postgresql?",1.0
g6fab3v,iywxdt,"Someone smarter than me please ELI5 how this is awesome. I’m sure it is, but I’m not educated enough",10.0
g6hajfk,iywxdt,"It's a new major version of an extremely popular free and open source database system, with new features and increased performance.",6.0
g6hy5t2,iywxdt,"Sorry, yes, I get that. I know what The database is. I just want to know what makes this version better in ELI5 terms. Thanks!",1.0
g6ifdng,iywxdt,"From the article: “PostgreSQL 13 includes significant improvements to its indexing and lookup system that benefit large databases, including space savings and performance gains for indexes, faster response times for queries that use aggregates or partitions, better query planning when using enhanced statistics, and more.”   

You’re welcome.",5.0
g6itu13,iywxdt,So it's faster. Got it. I thought there might be something else. Sorry for the trouble.,2.0
g6hfoa8,iywxdt,Thank you to all the amazing contributors &lt;3,3.0
g6hmztm,iywxdt,You're welcome!!,1.0
g6agyfd,iy3veo,"I've tried going into Tools and changing the font size, but that only affects the area where you write your script, not the actual UI itself",1.0
g6bl62q,iy3veo,Look at your display scaling settings. I think there is an option to auto fix older programs. Try that.,1.0
g6c5x9c,iy3veo,Is that in the oracle program itself or do I do that somewhere in my pc settings?,1.0
g6d0gag,iy3veo,It's in the windows settings.,1.0
g6dbhjz,iylnvs,"Most likely your date field is not a date field.  Can you confirm this?

Try explicitly casting it to datetime.

If your column is right but you're getting the wrong row, you need to rethink your query.  

Select stuff from table where pk in (select top 1 pk from table order by date column desc).",16.0
g6dg8c6,iylnvs,That fixed it. Didn’t notice that the column’s data type was varchar when I imported the data from the csv,6.0
g6dmcyr,iylnvs,"That'll happen with most ""easy"" import methods which also create the table if all you do is click through NextNextNext taking the defaults. Gotta watch out for those.",5.0
g6dce7i,iylnvs,What's the datatype of the column?,3.0
g6ey5lh,iylnvs,"look at the column definition for the column, maybe it's a varchar.  you may need to convert into a date.  for MSSQL use Cast(), Convert(). I don't play around with MSSQL much, so look up the function information.   

For Oracle it would be to\_date( datecol, 'MM/DD/YYYY HH:MI:SS  PM ') .  The Meridian indicator can be AM, A.M, PM, or P.M.",2.0
g6e9ntg,iylnvs,"Check out order by, and limiting results. You want to order by date column descending, and limit your result to 1.

Psst, I smell homework ... ;)",-2.0
g6eaxhi,iylnvs,"SQL dates should always be in the ""*yyyy-mm-dd*"" format. Anything that's not in that format is probably not a date as far as your DB is concerned.",-4.0
g6ftbzj,iylnvs,"If OP's date were stored as a string then yes,  yyyy-mm-dd would solve all their problems. However, actual date fields are handled internally and just displayed in whatever format you like. It doesn't affect sorting.",1.0
g6dfovz,iyl0wc,"Don't take licensing advice from randos on the internet. Ask your reseller's licensing specialist, or contact Microsoft. Start by reading [the licensing guide](https://www.microsoft.com/en-us/sql-server/sql-server-2019-pricing).

What does your application do that only serves two people but needs 16 cores? On the surface it sounds like you might be over-provisioned and it'll cost you way too much in the long run. Or is it the case that there are only 2 of you who are _directly_ accessing the server, but you have 300 people using web apps hanging off it?

You will need one CAL per user of the application(s) that access your SQL Server instance. You will _also_ need a CAL for each device accessing the server (your web server doesn't count, since the users will be accessing through it). And there's probably a minimum number of CALs you have to buy (they probably sell in 5-packs). So if you have 300 users of your web apps, you're going to get soaked by CALs. Many people think they can get by with CALs then do the math and realize they're better off with per-core licensing.

With core-based licensing, you have to license every core presented to the OSE (operating system environment). You're also required to purchase two 2-core license packs at a minimum. So yeah, with Standard Edition you're in for about $8K list price _minimum_.

You should check out Web Edition as well, see if it meets your requirements and pricing.

Don't take licensing advice from randos on the internet. Ask your reseller's licensing specialist, or contact Microsoft.",4.0
g6dji3q,iyl0wc,"Thanks for the reply this helps quite a bit, I've tried contacting Microsoft a couple times but I've always gotten someone who doesn't really know what they're talking about with long pauses in the convo so they can look up stuff....

I'll try to contact more Microsoft partners but a lot of them don't want to work with us since we're only a 10 person business.",1.0
g6dm6kr,iyl0wc,"After seeing /u/kagato87's reply, I went back and re-read the post. I misunderstood previously. You have a public-facing website/app with SQL Server on the back-end; I thought it was a web app internal to the company.

**You need per-core licensing. Full stop. Do not pass GO, do not consider CALs.**

Or, you know, throw it all up in Azure SQL DB/Managed Instance and pay for what you use. If that works for you.",2.0
g6exsf2,iyl0wc,"Web edition fits the bill here. It’s like express, but has sql agent. Only available via the SPLA program though, so you’ll need a reseller that does SPLA.",1.0
g6dkgzd,iyl0wc,"That's going to be expensive any way you slice it.   But at the same time, compared to the cost of that server, a DBA, and all that dev time behind your app the cost of licensing suddenly isn't that big.

Do confirm with a licensing rep, and don't waste your time with discount key websites.  In your case as you have a public facing website you probably will need per core licensing for all 16 cores.

CALs will be even worse as you need a license for every end useror device, regardless of how many layers of application servers you have.  This includes remote telemetry devices, end consumers of the website...",1.0
g6d5d82,iyji0o,You also need case statement [https://www.youtube.com/watch?v=mHWb16MvyMs](https://www.youtube.com/watch?v=mHWb16MvyMs) If you already know Excel this would be helpful,1.0
g6cw6db,iyhhnb, Everyone was a beginner,28.0
g6crp9g,iyhhnb,"I started in Excel. From there to MS Access where the queries are done graphically, i.e. drag and drop the tables to a design window and connect the fields for the joins and then drag and drop the fields you want in the result to the table below. Put any criteria below the field.

Given that I am old (pre-Google) I bought and read books on Access to get better. In one of the chapters SQL is discussed. I purchased the Dummies books to learn more.

My first exposure to SQL was Oracle and DB2. 

As far as on-line I did the Code Academy class and thought it is a good start but a little misleading about the real world. IMHO the class I took grossly over simplified data models etc but gave a good foundation.

I ended up getting a post masters in IT and we did a number of classes where SQL is the lingua franca. Oracle was the RDBMS. 

So as far as today, Code Academy is a good starting point since it is free. Other than that check your local technical college and see what they offer.",10.0
g6d3mi8,iyhhnb,[w3schools](https://www.w3schools.com/sql/) was where I started. They do a great job explaining the basics!,4.0
g6dv3c5,iyhhnb,"really helpful, they explain the basics considering you know nothing about it, and in simple English, if English is not your native language so this will be helpful",2.0
g6d15d3,iyhhnb,"Here's a good overview of different types of database platform:  https://db-engines.com/en/

YouTube course on database design:  http://youtu.be/e7Pr1VgPK4w

Normalization poster:  http://graphdatamodeling.com/resources/rettigNormalizationPoster.pdf

Free data models:  http://www.databaseanswers.org/data_models/

PostgreSQL Home:  http://www.postgresql.org/

PostgreSQL wiki:  http://wiki.postgresql.org/wiki/Main_Page

Sample databases:
https://www.postgresqltutorial.com/postgresql-sample-database/

https://www.oracletutorial.com/getting-started/oracle-sample-database

https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver15&amp;tabs=ssms",4.0
g6f1voo,iyhhnb,"I've started on a few learner courses but I just don't understand their application for what I want to learn it for.

I want to use power BI or tableau.

Let's say, for example, a ticket request system. Why is SQL used here?

Is it just to pull the data you want from a database and tableau does what you want? Or is there any editing of database with SQL (for this example)",1.0
g6fshwo,iyhhnb,"If you plan to work with relational databases (the most common type), you need to know SQL. This is true even if you use tools like Tableau or PowerBI. These tools typically generate SQL under the hood to talk to the database. You also need SQL if you ever have to access the database directly.",1.0
g6iqg07,iyhhnb,"So let's say I'm an analyst.

My goal is to create dashboards of number of tickets created, how many were raised on a monthly basis. 

Am I right in saying that I would be using SQL to take the data I want out, to analyse and display? 

How do I get to that data exactly, do I need to write an SQL query in, for example, MSSQL and then tell tableau to look at the data pulled into MSSQL?

Or do I write the query I want in MSSQL and then copy that query into Tableau and Tableau can access it that way?

I wish I could find some flowchart of how data flows and what each step does. I don't believe creating or editing databases is relevant for me?",1.0
g6cuaz9,iyhhnb,"I started by downloading mssql express (dev edition is free now so do that instead).. and built something that made sense to me.

I did an address book.

After I got the tables and data in... I started to think of ways to expand it. Not just the tables, but queries. So... how would I determine if one contact was the sister of another. How would I query that.

When I ran into an issue i searched for an answer.

Turned queries into procedures, started learning about code reuse, etc.

Eventually... i started another. Game consoles/games/publishers/genre/flags(co-op).. then another.. and 10 years later im still doing random stuff because it's fun now (Stockholm syndrome).

Do something that makes sense to. Your hobby. Music, games, cars... w/e.  Make up data if you can't find any, but i found that modeling and querying data regarding subjects that I knew helped me out a ton.

So my suggestion: hands on, get in there... build, create, destroy. Mess up so badly you have to start at square 1 again.

If you are passed smaller projects,  find a website and try to create a database that would run it. Like carmax or papa round murphy hut pizza.",2.0
g6e8rel,iyhhnb,The first thing someone told me when I first learned was to learn how SQL joins work.  It didn't click until I actually used it to visualize table relationships.,1.0
g6ek31n,iyhhnb,"I started off from Excel. Eventually I moved on to Power Query and Power BI. In the meantime, my old job required that we do analysis of SQL scripts. Then I changed jobs and starting working in SQL.

I already had some prior knowledge but it was pretty basic. W3 helped a lot, and then I started MCSA Database Developer.",1.0
g6f29z2,iyhhnb,Is it possible to embed a power view dashboard on a sharepoint site?,1.0
g6f51a2,iyhhnb,"I think so, never tried it myself tbh. I found [this](https://support.microsoft.com/en-us/office/power-view-in-sharepoint-server-create-save-and-print-reports-f616575e-02bf-46cd-b94a-b744245ef9d1) which will hopefully help you.",2.0
g6enx4i,iyhhnb,"The courses are great because they show you what SQL is capable of, but the real learning starts when you need to solve some problems yourself, which will force you to have to look things up and understand when and how to use them.

I thought I know query language quite well and recently started doing the practice problems on [hackerrank.com](hackerrank.com) and started learning more immediately.  

I would start with the basics at W3 and then start on the easy problems to get an idea of the syntax.",1.0
g6epmzt,iyhhnb,"Imagine you have a user interface that allows users to make CRUD operations on numerous models.  Make up a schema that would be efficient and practical to use against those models.  To come up with the models think of any business like an e-commerce, a website with different users and resources, articles, etc.  Then write all the queries you think may be useful.  Some may be helpers for your CRUD, but you could come up with statistical analysis, views, you may think of adding triggers, etc.

Everyone started as a beginner.  A lot of people don’t do much more with SQL than the basics.",1.0
g6f2hbz,iyhhnb,"I don't really know applications of SQL relevant to me.

Can someone explain to me SQLs purpose for example using tableau for analysing IT tickets?

Someone maybe explain the flow and what each does?",1.0
g6f2vyw,iyhhnb,I started at work. Boss(cfo) showed me how to do a simple select and i basically expanded upon it to meet business needs using w3 schools.,1.0
g6cg5ft,iyft4q,"SQL is interpreted by the engine into relational algebra, each engine has a selection of operations which it can apply to help solve the query.

The basics like Projection, Filtering, Sort, Merge etc. may be enhanced with a bunch of variants e.g. a Full Sort Merge vs Hash Merge.


 Using statistics about the data and metadata about where indexes exist etc. the optimiser will decide which order to apply the operations to produce the Query Plan. 

Most engines will provide an output of the query plan for you so you can see it all broken down.

Each operation will have its own complexity so overall the query complexity could be... well... complex.

Finally, as it executes the plan the engine might find it made bad choices. Some will continue blindly, others may adapt.",36.0
g6cotwy,iyft4q,"Also, if you want to see how it's actually implemented, [here's the PostgreSQL source code](https://github.com/postgres/postgres) (actual source [here](https://git.postgresql.org/gitweb/?p=postgresql.git;a=summary) but the Github mirror is a bit easier to navigate IMO)",11.0
g6y1blp,iyft4q,I find both difficult to navigate. How do I find the select statement implementation in those?,1.0
g6y3pyf,iyft4q,"I haven't really poked around in there for a while, but I believe it was in either [/src/backend/parser](https://github.com/postgres/postgres/tree/bda6dedbea599209048bc51115ecb2062ceb976c/src/backend/parser) or [/src/include/parser](https://github.com/postgres/postgres/tree/7559d8ebfa11d98728e816f6b655582ce41150f3/src/include/parser)

I'd note here that it's not the kind of codebase where you're going to dive in and find a nice `void Select(char* query)` kind of function that does all the select stuff though. That's why my comment was more of an ""and if you want to investigate further"" addition to a proper explanation, rather than posted as a top level comment in a kind of ""go find the answer out for yourself"" way - the latter would have been mean xD

Rather, queries are parsed by state, or nodes that have state maybe? I'm not quite sure how to explain it, in part because I'm not quite sure I understand it myself. There are some details [here](https://wiki.postgresql.org/wiki/Query_Parsing), and the actual parser used is called [Bison](http://www.gnu.org/software/bison/), which might be easier than trawling postgre's source itself",1.0
g6y4p6h,iyft4q,Alright. Thank you very much.,1.0
g6d8sm2,iyft4q,"To add a bit to phunkygeeza's answer:

The query compiler is trying very hard to come up with an optimal way of running your query. With the appropriate indexes in place, many queries will run in O(1) time (although O(n) in database space due to the index storage). O(n) queries (for n rows in a table) are often considered the worst case... table scans are O(n).

It's possible to get a query to run in O(n^2 ) time, but it takes work.",3.0
g6e1hn5,iyft4q,"SQL is a language to declare queries. Your question is a little off, because it's like saying ""what's the big O / time complexity of C++?"" - it's whatever you program into it.",1.0
g6cqcfe,iyft4q,Idk probably a for loop?,-16.0
g6ctki0,iyft4q,Found the NOSQL person.,13.0
g6cfg4o,iyfoem,"I'd probably try something like this:

    WITH first AS (
      SELECT 	device_id, 
      			timestamp AS first_timestamp
      FROM data
      GROUP BY 	device_id
    ),

    first_data AS (
      SELECT	a.device_id,
      			a.first_timestamp,
      			b.data AS first_data
      FROM	first AS a
      INNER JOIN data AS b
      ON a.device_id = b.device_id
      AND a.first_timestamp = b.timestamp
    ),

    last AS (
      SELECT	device_id,
      			MAX(timestamp) AS last_timestamp
      FROM data
      GROUP BY device_id
    ),

    last_data AS (
      SELECT	a.device_id,
      			a.last_timestamp,
      			b.data AS last_data
      FROM	last AS a
      INNER JOIN data AS b
      ON a.device_id = b.device_id
      AND a.last_timestamp = b.timestamp
    )

    SELECT 	a.device_id,
		    a.first_timestamp,
            a.first_data,
            b.last_timestamp,
            b.last_data
    FROM first_data AS a
    INNER JOIN last_data AS b
    ON a.device_id = b.device_id
    ORDER BY a.device_id;

Looks like it works in [DB Fiddle](https://www.db-fiddle.com/f/71fptdzJNKp5aug6zzXtMZ/0).",5.0
g6ch97u,iyfoem,wow! thank you a lot. this is exactly what I was looking for!,4.0
g6cnd8h,iyfoem,No problem! Glad it helped. CTEs can help you do all kinds of cool (and easy to read) things with SQL.,2.0
g6cgyht,iycjag,you seem to be calculating an average of standard deviations. Why would this result in an average across the whole sequence?,1.0
g6dycku,iycjag,"I was thinking that it would be useful to know the average distance between days of purchase for each customer in days, then calculate the average of all those average distances in order to to get the result with the following interpretation: ""On average, a customer makes a purchase every X number of days"".

It is possible that I still don't understand standard deviation :/",1.0
g6etqnu,iycjag,"Average of averages is, largely, a meaningless stat to begin with.",1.0
g6f7c3p,iycjag,"Ah ok. In that case, would you have a podsible solution for this challenge?
I've been thinking hard about sql problem, but I don't know what to do.",1.0
g6f8y4d,iycjag,"going to give you a formula - think you can manage the sql part?

so, let's try it with one shopper:

purchase1date, purchase2date, purchase3date, ... purchaseNdate.

days between purchase2date and purchase1date: purchase2date-purchase1date,

days between purchase3date and purchase2date: purchase3date-purchase2date

sum of 2 prior ones: sum of purchase1 to 3 days between = (purchase3date - purchase2date) + (purchase2date - purchase1date) == purchase3date - purchase1date.

ok, if you apply the same logic foward, the sum of differences in days between all purchases is simply max(purchaseDate) - min( purchaseDate).

the denominator in the average for a single customer will be number of purchases - 1

combine this all together and it is going to be sum of all max date/min date differences per customer divided by (overall number of purchases - number of customers).",2.0
g6fefbb,iycjag,Thanks for the detailed answer! I will try and use this info to solve the problem.,1.0
g6cwqzt,iycjag,I’d suggest using aliases so your code is easier to read,1.0
g6bukc7,iyc60w,"Will this single update statement not do what you want? Maybe I'm missing something.

    UPDATE trip
    SET available_tickets = available_tickets - ?
    WHERE trip_id = ?
      AND available_tickets &gt;= ?

I guess if you literally want to return 'Not enough tickets' then that won't do it.",3.0
g6bvpje,iyc60w,I need some sort of return value for my php script to determine if the transaction was successful,1.0
g6bxvmq,iyc60w,Can't you just check the `statement`'s affected row count after? `1` meaning a row was updated and `0` no updatable rows,7.0
g6c5grk,iyc60w,"Read up on how to use [TRANSACTIONs in MySQL](https://www.mysqltutorial.org/mysql-transaction.aspx/)

Typically you'll want to use the following pattern:

* Start transaction
* Generate data to process 
* Process data
* Commit if no errors/otherwise rollback
* Transaction is successful, verify data was processed if necessary
* Craft success message
* If rollback was performed craft error message
* Send message back to application",3.0
g6cf4rg,iyc60w,"Don't use an IF in your SQL query. You need this: https://www.w3schools.com/Php/func_mysqli_affected_rows.asp

Check for the affected rows from the update in your PHP code and then do whatever it is you need to do depending on that.",2.0
g6e96cn,iyc60w,"Thanks, in this case, that seems like an easy solution, but more generally speaking is there any reason not to use IF?",1.0
g6edyas,iyc60w,"There are plenty of reasons to use if, but when you're writing an app, it makes a lot more sense to keep that kind of logic in your application code and run the queries you need, when you need them.",1.0
g6c8ea9,iyc60w,"This is how i would tackle it.  I'd look to fold this into a stored procedure with an output parameter that returns the @ message variable.

    ''
    	-- SETUP THE TEST AREA  -- just load a few tickets
    	drop table if exists #tickets
    	create table #tickets (ticketno int identity(1,1),
    							available varchar(1))
    	Insert into #tickets (available)
    	select 'y'
    	Insert into #tickets (available)
    	select 'y'
    	Insert into #tickets (available)
    	select 'y'
    	Insert into #tickets (available)
    	select 'y'
    	Insert into #tickets (available)
    	select 'y'
    
    
    	-- setup
    	declare @ticketsToPull int 
    	declare @ticketsToReserve table (ticketno int)
    	declare @sql nvarchar(1000)
    	declare @Message varchar(100)
    
    	-- how many tickets?
    	set @ticketsToPull = 3
    	set @sql = 'select top ' + convert(varchar(10),@ticketsToPull) + ' ticketno from #tickets where available = ''y'''
    
    	-- build a list of tickets the top x eligable
    	insert into @ticketsToReserve (ticketno)
    	execute (@sql) 
    
    	if @ticketsToPull = (select count(1) from @ticketsToReserve) 
    	begin
    		update #tickets
    		set available = 'n'
    		where ticketno in (select ticketno from @ticketsToReserve)
    
    		set @Message = convert(varchar(100),@ticketsToPull) + ' tickets pulled.'
    	end
    	else
    	begin 
    		set @message = convert(varchar(100),@ticketsToPull) + ' tickets not available.  Nothing changed.'
    	end
    
    	select @message
    	select * from @ticketsToReserve
    
    ''",1.0
g6brtdv,iyc60w,"Instead of case use if else statement


If  @A &gt;= ? THEN 
UPDATE trip SET available_tickets = available_tickets - ? WHERE trip_id = ?;`

`ELSE 'Not enought tickets!'`

`END IF`

`COMMIT;`",0.0
g6epcjx,iyc60w,"When I try to do that, it says ' Unrecognised statement type. '

I added an image to the original post",1.0
g6b5yep,iy8qwa,"&gt; Let's say I have a string on every row which need to be manipulated : 'a,b,c,d,e'. I would pass it as a parameter with an ID to split it and return it as table like this : [xyz_ID, information #1],[...] for every row of my query in a join. Pretty simple.

could you please rephrase this

perhaps show the MSSQL syntax that you would be using for this",2.0
g6b7fwu,iy8qwa,"I have rephrase it, let me know if if clearer now.",1.0
g6b8fbf,iy8qwa,"that's so much clearer, thanks

i googled `FT_GetList` and got nothing, except a few references in something called Groovy

i don't think this is a MS SQL feature

and as far as i know there isn't an easy way to do the same in MySQL... you can do it with the `SUBSTRING_INDEX` function but you'd have to join to a `numbers` table to generate the indexes into the string list

i guess this isn't really the time to point out that a string list violates the spirit of First Normal Form...",3.0
g6b8p3q,iy8qwa,"FT_GetList would be something I would have created myself, I simply used it as an example for something I would need to create.

As for string lists, I work with other people database mostly, some dev simply don't know how to do it properly a lot apparently.",2.0
g6bpha8,iy8qwa,Ft_getlist looks like a UDF that returns a table.,1.0
g6buh5h,iy8qwa,"Can someone here come up with a solution 
..maybe but tldr Tvf are not supported by mysql.",2.0
g6cas0w,iy8qwa,"I had a similar issue.  No TVF support

I had 2 types TFVs i needed to replace, one.. i was able to make into views..

The other.. i had to use PREPARE and EXECUTE and (nvm on this temp.. this was my string split attempt) ~~CREATE TEMPORARY TABLE~~

Splitting strings was a nightmare...(substring\_index, temp tables and length counts  minus length counts replace( commas)... a nightmare... so I used the incoming CSV to do an IN (@CSV)

&amp;#x200B;

      @dynamicSQL = CONCAT('SELECT * FROM thingy WHERE thingyid IN (', @csv, ')');
      PREPARE dynamicSQL FROM @dynamicSQL;
      EXECUTE dynamicSQL;
      DEALLOCATE PREPARE dynamicSQL;",2.0
g6cjtet,iy8qwa,"Ended up with something similar, which suck to be honest. 
I also don't get why I need a damn procedure to use a cursor or even a declare of some kind. All of it take so much time.",1.0
g6bplb2,iy8qwa,"Have you looked into CTE?

I think you can probably build a CTE and then join to it the same way you join to your table function.

https://www.mysqltutorial.org/mysql-cte/",1.0
g6byh1u,iy8qwa,"Stuck with mySQL 5.7 for now (CTE are V8), no idea how people use mySQL efficiently so far. Guess they do all of it in the language calling the DB.",2.0
g6byz47,iy8qwa,"Haha fair enough.

Do you have any access to ETL tools?  Might be able to leverage those to better suit the functionality as well.

I had a similar issue On the oracle side.  To resolve, I just created a “get values from CSV” function and procedure which basically returned a cursor with the data in a table-like format.  Allowed my team to bulk import a csv and tie it to existing data.  If I could do it again, I’d just use an ETL tool and skip the headache.",2.0
g6c01b9,iy8qwa,"Will probably do something similar, it's a pain, but better then nothing.",2.0
g6c0gt2,iy8qwa,"Yes, I’ve found quite a few things on the Oracle/MySQL side that could be resolved with what SHOULD be native functionality. It’s helped me appreciate SQL server for its abundance of native features.",2.0
g6cjuvg,iy8qwa,I am just starting and I already see some of them.,2.0
g6cte68,iy8qwa,"LOL well, you can just familiarize yourself with the “nuances” and become a subject matter expert in it.   Then get a Job as an Oracle DBA and it’s welcome to planet money!  

I mean pain, planet pain.  Oracle is a niche product that costs 47K per core but at least the pay to specialize in it is good.",1.0
g6ex5x5,iy8qwa,"Not sure I would want to work for Oracle but hey, if the option is there.",2.0
g6exf9l,iy8qwa,"Sorry, I meant for a company that has oracle databases.  But oracle themselves aren’t out of the question!",1.0
g6bqi1r,iy8qwa,"a CTE for unstringing a comma-delimited list is harder than you think

i'd love to see your attempt",1.0
g6br5ji,iy8qwa,"Ease up chief, we’re all working toward a common goal here.

How about something custom build off of SUBSTRING_INDEX()?  The file pattern would have to be pretty consistent and it may take a bit of finessing though.  

https://www.gyrocode.com/articles/how-to-split-and-search-in-comma-separated-values-in-mysql/

https://www.got-it.ai/solutions/sqlquerychat/sql-help/data-manipulation/how-to-split-comma-separated-values-in-mysql-querychat/",2.0
g6bub75,iy8qwa,"chief???

also, ""may take a bit of finessing"" LOL",-4.0
g6bum81,iy8qwa,"Wow, what’s your deal?  I’m not sure what basis your attacks are coming off of but I’m not going to engage you further.

OP already has a custom function on the sql server side.  We can pretty much assume they understand that they need to do some custom work to get this done.  

They can also use an ETL tool instead if they need to.",4.0
g6bv7iu,iy8qwa,"you tossed a racial slur at me first

i chuckled at your simplistic ""finessing"" reply

and suddenly i have a ""deal"" and i'm making ""attacks""???

dude, get a fucking grip on your hostility

log off, go outside, let nature heal whatever's hurting you",-2.0
g6bvf0q,iy8qwa,"Lol wow. What are you talking about.

Know what? Here is some gold.  Hopefully helps you breathe a bit and stop misinterpreting words as racial slurs.   Hope you have a great day.",3.0
g6bw7xi,iy8qwa,"&gt; What are you talking about.

[""chief""](http://psac-ncr.com/zero-tolerance-racism-albert-dumont)

thanks for the gold, sparky",-3.0
g6c07x0,iy8qwa,"Huh, learned something new. Wasn't aware of the connotations of that phrase, always just took it as belittling when used in a negative light.",1.0
g6c10xr,iy8qwa,"Washington Football Club gets it

Kansas City Football Club doesn't (yet)

and then there's the Cleveland Baseball Club...",-1.0
g6bzt5k,iy8qwa,"Not a MySQL expert, been years since I used it and at the time this was absolutely not possible.

That pattern in your example requires a lateral join to a set returning function. I see that MySQL now apparently supports lateral joins: https://dev.mysql.com/doc/refman/8.0/en/create-procedure.html but I am not sure if it supports them when calling a function/procedure as all their examples just use lateral subqueries.",1.0
g6c098c,iy8qwa,As far as I know right now it's still impossible to do what I am trying to do.,1.0
g6aixxf,iy40h4,"    INNER JOIN tblInvoiceDetail ON tblInvoiceDetail = invoices.fldInvID

First, might there be a .fldInvID missing in this join?",3.0
g6ajsj1,iy40h4,"thank you... I actually had that on my query at the end of the post.

I have added another inner join to invoice the table invoices, but the result is pretty similar

    INNER JOIN tblInvoiceDetail ON tblOrders.fldInvID = tblInvoiceDetail.fldInvID 
    INNER JOIN tblInvoices ON tblOrders.fldInvID = tblInvoices.fldInvID AND tblInvoiceDetail.fldInvID = tblInvoices.fldInvID

I'm still getting multiple fldItemDesc for each individual orderNo",1.0
g6al2mp,iy40h4,"Show your whole query and the whole result table, and the whole result table when you `select *` rather than just a few columns. `fldOrderNo` is not a primary key so it is perfectly possible for multiple rows in `tblOrders` to have the same `fldOrderNo`, which is probably why you see it repeating. I suggest `ORDER BY tblOrders.fldOrderNo` so the rows for each order are sorted together.",3.0
g6aju3y,iy40h4,"Inner Join orders with Invoice   
Inner Join invoice with invoice details",1.0
g6akjh6,iy40h4,"Like this?

    FROM		tblOrders
    
    INNER JOIN tblInvoices ON tblOrders.fldInvID = tblInvoices.fldInvID
    INNER JOIN tblInvoiceDetail ON tblInvoices.fldInvID = tblInvoiceDetail.fldInvID 

This didn't work, unfortunately. I'm still getting itemDesc multiplied by every order no",2.0
g69uflj,iy08lm,"&gt; For some reason it shows the same as the second SELECT statement:

look at your subquery -- it returns ~all~ albums with at least three songs

but there's a problem in your outer query's WHERE clause -- it uses WHERE EXISTS

so basically, if **any** album exists which contains 3 or more songs, the outer query's WHERE clause will br TRUE

so what does the outer query do?  returns all albums

there's your problem",3.0
g69u4h1,iy08lm,"Exists work when you pass a value down to it.

So from album A

(

From song s

Where s.albumid = a.albumid

)

You have to make that connection. Or you will always get a ""postive"".  yes rows exist, its not linking the top album to the bottom, therefore rows ALWAYS exist.

Once you make the link from top to bottom query, it should work.

The 

exists 

(

Select albumid...)

Is misleading.

You could do. Exists (select 1... where a=s)",2.0
g69zixf,iy08lm,How would you write out that whole code? I very new to this and it's confusing for me. Sorry..,1.0
g6a2nbp,iy08lm,"Select *

From album a

Where exists

(

Select s.albumid, count(1)

From songs s

Where a.albumid = s.albumid

Group by s.albumid

Having count(1) &gt;=3


)",1.0
g6a2xbc,iy08lm,Thank you so much!,2.0
g6a33kl,iy08lm,"See the where clause in the exists? A.albumid = s.albumid


A at the top

S for the bottom query... thats where they connect",1.0
g6aeksm,iy08lm,"Hi again, now I understand it, but for some reason it doesnt pull out the tables from the query with this code:

SELECT *
FROM Album A
WHERE EXISTS
(SELECT AlbumID, COUNT(*)
FROM Song B
WHERE A.AlbumID = B.AlbumID
GROUP BY AlbumID
HAVING COUNT(*) &gt;= 3);",1.0
g6bbodu,iy08lm,"&gt;SELECT \* FROM Album A WHERE EXISTS (SELECT AlbumID, COUNT(  
&gt;  
&gt;) FROM Song B WHERE A.AlbumID = B.AlbumID GROUP BY AlbumID HAVING COUNT(  
&gt;  
&gt;) &gt;= 3);

You might want to use alias in your GROUP BY statement. Normally sql would warn about this.   
Make it clearer by writing:  
GROUP BY B.AlbumID  


Also I would put something inside the parathesis of COUNT (I work on SQL Server so might be different for MySQL). For example COUNT(\*)",1.0
g69qxx6,ixx2rj,Keeping an eye on this post since i have the same question,2.0
g69rzcx,ixx2rj,"look under **Example Databases** on this page of da manual --
https://dev.mysql.com/doc/index-other.html",2.0
g6akm68,ixx2rj,"Umm are you sure you have the book?

Right there in the foreword ...

http://data-miners.com/sql_companion.htm",2.0
g6bn3u8,ixx2rj,I have a pdf of the book. This helps. Thank you so much.,1.0
g6bh04y,ixx2rj,Here's a [comment](https://www.reddit.com/r/SQL/comments/da32sn/z/f1o3dy9) from an older thread.,1.0
g6a87yo,ixx2rj,Google,-2.0
g69psgo,ixwn79,"If you want to pull all the data into one column from three different tables, you union the selects together pulling each column from each table.  

Select column
From table 1
Union
Select column
From table 2
Union 
Select column 
From table 3



Use union for distinct values and union all for all data including duplicates.",2.0
g89evvb,ixwn79,Thanks bro !,1.0
g89g0sc,ixwn79,"I hope that worked since I didn't see you were using mysql, I use mssql",1.0
g89h3bd,ixwn79,"It did, thanks man !",1.0
g89ixvd,ixwn79,Glad to hear it!,1.0
g68p3fm,ixtnaq,trunc(date),1.0
g68pb4y,ixtnaq,Won't that just hide the timestamp? Thusly my case statement is still comparing time?,1.0
g68q0bn,ixtnaq,"If you're using trunc in the comparison, then you're working with the formatted data. So, no time",1.0
g68v6p9,ixtnaq,"I have never used trunc in a statement just in the select.

How would it work?

select

&amp;#x200B;

WONUM,

DESCRIPTION,

STATUS,

WORKTYPE,

REPORTDATE,

ACTSTART,

ACTFINISH,

NVL (MAXIMO.WORKORDER.FNLCONSTRAINT,MAXIMO.WORKORDER.TARGCOMPDATE) ""PM DUE"",

PERSONGROUP,

&amp;#x200B;

&amp;#x200B;

CASE

WHEN ACTFINISH IS null then 'COMPLIANT'

WHEN ACTFINISH &lt;= FNLCONSTRAINT THEN 'COMPLIANT'

WHEN ACTFINISH &lt;= TARGCOMPDATE THEN 'COMPLIANT'

ELSE 'NON COMPLIANT'

END AS COMPLIANT  

&amp;#x200B;

FROM

MAXIMO.WORKORDER

&amp;#x200B;

WHERE 

SITEID = 'OP'

AND

WORKTYPE = 'PM'

AND 

STATUS != 'CAN'

AND 

ISTASK = '0'

AND

REPORTDATE &gt;= '1/sep/20'",1.0
g68wa9e,ixtnaq,"You can use it almost anywhere.

WHEN TRUNC(ACTFINISH) &lt;= TRUNC(TARGCOMPDATE) THEN 'COMPLIANT'

seems to be what you're looking to use. Days precision is the default. You can also specify precision. See [https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/TRUNC-date.html#GUID-BC82227A-2698-4EC8-8C1A-ABECC64B0E79](https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/TRUNC-date.html#GUID-BC82227A-2698-4EC8-8C1A-ABECC64B0E79)",1.0
g68x72j,ixtnaq,Worked like a charm! Thank you so much!!,1.0
g6911dm,ixtnaq,"&gt; then you're working with the formatted data. 

`trunc()` does not change the data type, so there is no ""formatted data"" at all, it's sill a `DATE`

&gt; So, no time

The value returned by `trunc()` still is a `date` and thus still contains a time - but the time is set to `00:00:00`.",1.0
g694kqa,ixtnaq,"It's nitpicking for the purpose. A date is always a date, but to\_date(date, 'DD-MON-YY') is still equal to trunc(date). One is just simpler.",1.0
g69d92v,ixtjyj,"You could create a temporary table containing the employee details you want to filter on, and then join the main table to that.

    select * from mytable T
    join #Employeelist E
    on T.employeeid = E.employeeid
    and T.companyid = E.companyid",2.0
g69v4wr,ixtjyj,"Yea.


Either 

Where (employeeid = @employee OR @employeeid is null)
And (companyid =@company or @company is null)

To get 1 or all.. in each var.


Or.. pass in a csv


Declare @sql nvarchar(1000)='select from where employeeid in (&lt;token&gt;)'

Select @sql = replace(@sql, '&lt;token&gt;',@csv)

Exec sp_executesql @sql",2.0
g6chlh0,ixtjyj,"Table valued parameters, as the other 2 ways have been covered",1.0
g696ypu,ixsnzh,"Most certifications from software vendors show that you are at some level 'conversant' with their software.  It does show that you spent some time and money to go through the process to earn the certification.  

What does a hiring manager think when they see a cert on your resume/Linked.in? Well, if you do not have a CS degree or similar, it says that you have done some studying and mostly likely have some experience with that product.  So it does help, at least a little.  Depending on the level of the cert it may be used to balance out some lack of work experience.",2.0
g68tqp5,ixsnzh,"In some industries: Yes - in others: No.  It certainly won't hurt you.  And if you want more practice and a leg up on others competing for the same position, it's a wise idea.  Whatever you choose, go with a cloud track (i.e. Azure) for the highest earning potential.",1.0
g68xe3i,ixsnzh,Which cert do you think would cover a wider blanket of employers? For now I just want to get my foot in the door.,1.0
g68zph2,ixsnzh,"Please note that I am biased towards the Microsoft SQL Server / Azure side of the coin.  Many companies use this technology and its various forms.  There are a lot of jobs.  But other technologies (Oracle, Postgre, etc) also have merits.

Check this out:
https://acloudguru.com/blog/engineering/which-azure-certification-is-right-for-me",1.0
g69q6b1,ixsnzh,"to a recruiter, yes, but the technical interview is what you should be really practicing on",1.0
g68mfaw,ixrxgn,"Nice course, it should give you a good understanding of the basics and syntax structure. I have not crossed many companies using MySQL for BI though.

Is there any particular reason why you chose MySQL ?  


here are some resources to sink your teeth in. specifically for MySQL.

[https://www.mysqltutorial.org/advanced-mysql/](https://www.mysqltutorial.org/advanced-mysql/)  
[https://www.w3resource.com/mysql/mysql-tutorials.php](https://www.w3resource.com/mysql/mysql-tutorials.php)",2.0
g68p45g,ixrxgn,"The instructor cited some figure that said MySQL was fairly popular, and that it was better for learning syntax. I was hoping to make a pivot towards PostgreSQL after I finished if you have anything for that.

Much appreciated!",1.0
g6a1dmk,ixra5t,"Not to be a spoil sport, but it could probably be construed as copyright infringement.

Just thought I'd mention it in case you're concerned about that sort of thing.",3.0
g6a2wjk,ixra5t,"Hmm, thanks for the heads-up.",1.0
g68j2ig,ixra5t,Thank you. Much appreciated.,3.0
g69p9dw,ixra5t,I was browsing leetcode sql problems 10 seconds ago and was thinking like most of them are locked and this post popped up.,3.0
g6bbfbh,ixra5t,Seeing as you're scraping their website making paid stuff free I'd be weary about LC not being very enthusiastic about this whole thing.,3.0
g68q94t,ixra5t,"This looks awesome, I will test it tomorrow!",2.0
g69klfv,ixra5t,Thanks for checking it out!,1.0
g68vejf,ixra5t,Thank you. This is very helpful.,1.0
g68vyna,ixra5t,Yo I use python and also sql. Been trying to find good sql problems to practice on. I will check this out but thanks for making it.,1.0
g69kpxb,ixra5t,Perfect!! I hope you get value from it.,2.0
g6b23g6,ixra5t,"Great stuff, thanks for sharing!",1.0
g6bieez,ixra5t,"You expressly claim that an operation of this script you've built is to dodge the LC subscription?

How is that not theft?",1.0
g6cu6sj,ixra5t,"Madlad, many thanks",1.0
g69bog6,ixra5t,"That's really neat. Though I have to say I prefer https://dbfiddle.uk nowadays, more flexibility.

Edit: also, you went through a lot of trouble for that #1369 solution, this is simpler I think (unless I'm missing something):
    
    SELECT username
         , activity
         , startDate
         , endDate
      FROM (SELECT * 
    	         , ROW_NUMBER() OVER (PARTITION BY username ORDER BY startDate DESC) as rn
                 , COUNT(*) OVER (PARTITION BY username) as cnt
              FROM UserActivity
         ) sub
     WHERE rn = 2 
        OR cnt = 1",1.0
g69kcru,ixra5t,"
I’ve seen that UK url before and I always thought they were the same thing! I can now see they are very different, good to know about it.

With regards to #1369, the solution I provided is not my own. I linked it because my program will open up that website for the solutions tab.
Your solution looks similar to mine, though yours is slightly cleaner. Here was what I had come up with.

windows function:

&gt;select username, activity, startDate, endDate from
(select \*, rank() over (partition by username order by startDate) as ranked, count(\*) over (partition by username) user\_ct from UserActivity) tmp
where
case when user\_ct = 1 then ranked = 1 else ranked = 2 end

self join:
&gt;(select * from UserActivity where username in
(select username from UserActivity group by username having count(*) = 1))
union
(select distinct u1.* from UserActivity u1 join UserActivity u2 on u1.startDate &gt; u2.StartDate and u1.username = u2.username
order by startDate limit 1)",1.0
g68dhab,ixqzi7,"I usually use comments to do this, leave your declares uncommented, comment out all the query parts, then you uncomment your query parts one at a time and run the whole script.",3.0
g68ocfm,ixqzi7,Then in ssms you can fold the comment too.,1.0
g68cwoj,ixqzi7,"There is no simple solution to this problem I am aware of - it would be very interesting if someone suggests something directly addressing your concern.

However, I know exactly what you're talking about and while this may not be in your control, I have changed the way I write code, so that when I test or debug it, I am able to run it in stepped blocks, such that if I have 5 blocks of code, I would be able to run and test:  
1 - test  
1, 2 - test  
1, 2, 3 - test  
1,2,3,4 - test  
1,2,3,4,5 - test  


What this allows me to do is, go to the end of the last statement of the block I want to test, press ctrl+shit+home to highlight all code above it (including variables declaration and initialisation) and hit F5.  
The code is written as often as possible to be idempotent - allowing me to F5 the same block (or series of blocks) of code repeatedly, and get the same result.  


If you are testing other people's code, perhaps you could encourage them to code this way? It makes testing and maintenance much cheaper imo.  


Good luck!",2.0
g68d0uk,ixqzi7,"I've run into the same issue. In SSMS you can split the query window to see different sections of code at the same time. I will just copy and paste these to the block I'm testing then when done either comment them out or delete them.

The other practice I use is to just copy and paste the blocks to a new query window so each block is its own query tab. The only downside of this is merging the code back together.",2.0
g69vpc0,ixqzi7,"I have a crappy solution.. it works for me... its call the paste ring.


You can copy (single or multiple variables) and hit ctrl shift v multiple times and it will cycle through the things you copied.

Its not the hero we need.. but it gets the job done.


Ctrl c 2 different things and then hit ctrl shift V twice to see what i mean.",1.0
g67yrfe,ixp6gk,"I can't believe how similar this is to another post from just an hour ago.


https://www.reddit.com/r/SQL/comments/ixo6k5/how_to_filter_out_the_following_results/


The sub select [posted](https://www.reddit.com/r/SQL/comments/ixo6k5/how_to_filter_out_the_following_results/g67u89f/) by u/r3pr0b8 looks exactly like what I did here",1.0
g67zge7,ixp6gk,"depending on the actual situation, you might have to use `COUNT(DISTINCT ...)`

if some `grp` has two 'a' `val`s and no 'c', you'll get a false result",2.0
g6816km,ixp6gk,"I agree with this post. COUNT(DISTINCT val) and HAVING COUNT(DISTINCT val) &gt;= 2 will provide the most straight forward solution.

\&gt;= because the definition was 2 or more. The question here is how you will know how many distinct values you are looking for.

&amp;#x200B;

    WITH cte AS (
        SELECT 'g1' [grp], 'a' [val]
        UNION SELECT 'g1', 'b'
        UNION SELECT 'g2', 'a'
        UNION SELECT 'g2', 'b'
        UNION SELECT 'g2', 'c'
        UNION SELECT 'g3', 'a'
        UNION SELECT 'g3', 'c'
        UNION SELECT 'g3', 'd'
    )
    SELECT c.grp
    FROM cte c
    WHERE c.val IN ('a','c')
    GROUP BY c.grp
    HAVING COUNT(DISTINCT val) &gt;= 2 -- matches length of 'val' list",1.0
g682r1e,ixp6gk,"Thanks for the confirmation posted this above ...

 &gt; The actual situation is a table with a unique constraint on grp and val.

By 2 or more I meant that I might ask for (a,b,c) or (a,b,d,c) or any other permutation, but no repeats as I'm already filtering the inputs.",1.0
g68245v,ixp6gk,"The actual situation is a table with a unique constraint on grp and val.

But I like that COUNT(DISTINCT ...) is safer if I copy-paste later.",1.0
g67yz2b,ixp6gk,"I would do a ‘SUM(CASE WHEN Val in () then 1 end) ‘ and look for the sum being 2 .  
You can also do the same trick with a count distinct",1.0
g69be6m,ixoqmi,Learn SQL in 7 seconds. Learn SQL before finishing this sentence. Learn SQL before you were born.,0.0
g67yu12,ixoq6q,"If the following are all true, then everything is OK:

* It executes
* The expected output matches the actual output
* The runtime is within the expected amount of time
* The logging is sufficient for your use case

As a developer, you will find better and more performant ways of doing things as you learn. You may find in 12 months that you're laughing at something you've done there, and rewrite parts of it. You may find that in 12 months you are still happy with it. 

Without looking at your code, it's hard to give a definitive answer, but I would say that what you have described here is pretty standard.",6.0
g68002o,ixoq6q,"thanks, yeah i mean im more or less just curious as id like to know for professional reasons. i write sql all day at work and do some ETL but i know the way we do things at work isnt really 'best practice' per se (IT in banks in LOL..) i want to be able to answer this sort of question on an interview if i were to ever get one thrown at me in the future",3.0
g697d5n,ixoq6q,"Agreed. Some 10 years ago I wrote a web scraper to collect national  traffic congestion data. I used php for the scraping , the very ugly data massaging, and sql inserts into MySQL. I scheduled it by calling a url from a domain I own, including a ridiculously long string parameter as a sloppy means of security, via a wget cronjob on my openWRT router. I needed something that was turned on 24/7. It ran every 5 minutes. To my surprise this setup ran fine  for around eight years!. Apparently, the core of their site never really changed much.   

At the moment ETL is  moving away from monolithic tooling. The buzzwords I hear are coding with Python frameworks, powershell, or Spark. Code generation is key, and transformations are pushed further upstream making room for data lake type of implementations.",1.0
g69stnx,ixoq6q,"you can pretty much do everything with python but from what i was told, it is actually better to do the transformations on the server itself for scaling purposes.",1.0
g6altnf,ixoq6q,"Fair enough, but that was not my point. If it works conform specs, it is good enough. Very agile.  Many etl platforms really only generate SQL under water and, yes, transformations are ideally handled in SQL. For your use case it will probably be overkill but feel free to look at Informatica Powercenter, Talend or SSIS from Microsoft. Code generation with Quipu or BIML or whatever helps you to scale quite well. Probably overkill too, but still interesting. Good luck!",1.0
g67v485,ixoq6q,You might want some tables to track the activity and trap any error codes that may pop out.,3.0
g67zttq,ixoq6q,i do have some telemetry built in but this is a good idea'r.,2.0
g68nsyg,ixoq6q,"It's fine doing it the way you have, but try thinking pure SQL instead of a procedural program. 

Have a go at writing a series of views and plain old SQL to do your transforms.

The benefits are you can see your results any time just by doing a simple select. Most times the engine will do then right thing, optimally, including using temporary storage if it needs to.",1.0
g68xvac,ixoq6q,"What do you mean by ""try thinking pure SQL instead of a procedural program"" ? Stored procedures in a database ARE plain SQL.  You create temp tables inside of them and manipulate the data in those.",1.0
g68yodw,ixoq6q,"1) execute procedure 

2)extract data

3)load to temp table

4) perform cleansing update

5, 6 etc) more cleansing steps 

9) output data to final purpose 


What I mean is that is a procedure. Lose the stepwise approach, only move data if it is unavoidable.  You python scraper is mostly unavoidable to get your data.

Every step after that can be a view on top of that staging table. Your final output table can be a view.

Using ctes or chains of views you can even think of your transforms in logical 'steps' if you need to.

The data never moves from the staging table.

And btw, stored procedures aren't SQL. In SQL they are T-SQL or c#. In ORACLE pl/sql or Java. They are proprietary procedural languages.",-1.0
g67z5fp,ixoq6q,"I guess the first question is are you using python for the whole process, or transferring to a different type of DB? Seems like instead of SQL tables for all activities you could be using data frames in pandas and then loading final product into SQLite DB or another pythonic DB. Perhaps r/python is a better place to ask.",1.0
g67zrt4,ixoq6q,i use python to get the data from the website(s). one of the processes takes all of the data and puts it all into a dataframe. i know python pretty good i just know sql alot better (oof the top of my head for stuff like removing strings and whatnot),2.0
g684jsu,ixoq6q,"Without seeing more detail/clarifying the question, my intuition is that you would be better off using python from start to finish. SQL is very strong, as long as everything is pretty. From my limited scraping experience though, that's rarely the type of data you're working with i.e. removing strings, parsing etc.",1.0
g67u89f,ixo6k5,"    SELECT i_cover.* 
      FROM ( SELECT policy_number
               FROM i_cover
              WHERE cover_no IN (4610,4611)
             GROUP
                 BY policy_number 
             HAVING COUNT(DISTINCT cover_no) = 2
           ) AS these
    INNER
      JOIN i_cover
        ON i_cover.policy_number = these.policy_number",2.0
g67uu0a,ixo6k5,"&gt;Thank you!  This worked as far as eliminating the ones I wanted, but it is also including additional cover\_no 4552.  How do I exclude 4552?",1.0
g67v7cm,ixo6k5,"&gt; How do I exclude 4552?

add this at the end of the query --

    WHERE i_cover.cover_no IN (4610,4611)",1.0
g67vb6c,ixo6k5,Awesome.  Thank you so much.,1.0
g67s7pf,ixo6k5,"There are several ways - one would be 
Select blah blah from I_cover where cover_no = 4610 
And policy_number in (select blah blah from I_cover where cover_no = 4611)
Maybe not the most efficient or you
Could do it with an EXISTS clause as well",0.0
g67sdkb,ixo6k5,"SQL WHERE clauses follow normal boolean logic.

(A and B) or C

C or (A and B)

Or in your particular case

(A OR B) and NOT C.",0.0
g67n6fb,ixn8yq,I would give [HackerRank](https://www.hackerrank.com/) a try. Start with challenge #1 in SQL,1.0
g68fvbb,ixn8yq,https://www.sql-easy.com/,1.0
g6xfh2y,ixn8yq,Check out W3schools and StrataScratch,1.0
g67s9lz,ixmid6,Airflow,4.0
g67n1y5,ixmid6,What's wrong with SQL Server Agent?,4.0
g67n905,ixmid6,"Its beyond Just sql, we have large usage to trigger pre and post etl loads using tool.",1.0
g67njf3,ixmid6,"SQL Agent is fine for executing jobs outwith SQL Server, as long as you configure it securely.",1.0
g68nxng,ixmid6,"+1 for SQL agent, never found a better schedule tbh",1.0
g67sgbu,ixmid6,[deleted],2.0
g67sjyd,ixmid6,we are usinf Informatica DAC scheduler now and wanted to move away..,1.0
g67k190,ixmid6,Python with Windows Task Scheduler currently. Likely moving to Redwood In the near future.,1.0
g67l3oa,ixmid6,"You can schedule SQL jobs with [Magic](https://polterguy.github.io/) if you want to. I should probably document these parts of the woods better, but I'm sure you can figure it out after a couple of minutes if you fiddle with it ...",1.0
g677wv6,ixitnd,"Not exactly sure what you are trying to do here.  
Can you post a picture or give us an example of in-data and result-data?",2.0
g678yan,ixitnd,"[link to imgur](https://imgur.com/gallery/xHs6YHA) 

Here is the link I have a table of stocks. Here is the example. I live in New York and i sell books. I save them in 4 or 5 storages. I know that on the 5th of February I had 5 books in the 1st storage, and then 11 of March in the 2nd storage I had 2 books. And every month I have to show report to my headmaster and say how many stocks I have in the end of month. So overall I say. I have 7 stocks in the end of March. I need to get it for every month. Thank you in advance",1.0
g67lizc,ixitnd,"So I guess you want accumulated stock per organisation, storage AND sku (not sure what sku is).What SQL database are you using?

The following code was created in SQL Server to try and mock-up what you are trying to do. I'll do another post with a query you might be able to run straight away.

```
SET XACT_ABORT ON;
BEGIN TRANSACTION;

SELECT
	*
INTO
	#Storage
FROM
	(VALUES
		(NEWID()),
		(NEWID()),
		(NEWID())
	) X (StorageId)

SELECT
	*
INTO
	#SKU
FROM
	(VALUES
		(NEWID()),
		(NEWID()),
		(NEWID())
	) X (SkuId)

CREATE TABLE #Inventory (
	OrganisationID uniqueidentifier NOT NULL,
	StorageID uniqueidentifier NOT NULL,
	SkuID uniqueidentifier NOT NULL,
	[Period] date NOT NULL,
	CountAtEnd int NOT NULL DEFAULT(0)
)

DECLARE
	@Counter int = 0,
	@End int = 100;

WHILE @Counter &lt; @End
BEGIN
	
	INSERT [#inventory]
		(
			[OrganisationID],
			[StorageID],
			[SkuID],
			[Period],
			[CountAtEnd]
		)
	SELECT
		'F425A8B7-76F4-43DE-AB3F-E6EE7272AB5D',
		[S2].[StorageId],
		[S].[SkuId],
		[X].[Period],
		[X].[CountAtEnd]
	FROM
		[#SKU] AS [S]
		CROSS APPLY [#Storage] AS [S2]
		CROSS APPLY (
			SELECT
				DATEADD(DAY, ABS(CHECKSUM(NEWID()) % 364 ), '2020-01-01') AS [Period],
				ABS(CHECKSUM(NEWID()) % 10) AS [CountAtEnd]
			
		) X

	SET @Counter += 1;

END;

;WITH cte AS (
	SELECT
		YEAR([I].[Period]) AS [Year],
		MONTH([I].[Period]) AS [Month],
		[I].[OrganisationID],
		[I].[StorageID],
		[I].[SkuID],
		SUM([I].[CountAtEnd]) AS [CountPerMonth]
	FROM
		[#inventory] AS [I]
	GROUP BY
		YEAR([I].[Period]),
		MONTH([I].[Period]),
		[I].[OrganisationID],
		[I].[StorageID],
		[I].[SkuID]
)
SELECT
	[cte].[Year],
	[cte].[Month],
	[cte].[OrganisationID],
	[cte].[StorageID],
	[cte].[SkuID],
	SUM([cte].[CountPerMonth]) OVER (PARTITION BY [cte].[OrganisationID], [cte].[StorageID], [cte].[SkuID] ORDER BY [cte].[Year], [cte].[Month])
FROM
	[cte]

ROLLBACK TRANSACTION;

```",1.0
g67m5ct,ixitnd,"This might work directly on your table: (Not 100% sure about MySQL) Found this information about windows functions in MySQL that might help.

 [https://stackoverflow.com/questions/17664436/cumulative-sum-over-a-set-of-rows-in-mysql](https://stackoverflow.com/questions/17664436/cumulative-sum-over-a-set-of-rows-in-mysql) 

    ;WITH cte AS (
    	SELECT
    		YEAR([I].[Period]) AS [Year],
    		MONTH([I].[Period]) AS [Month],
    		[I].[OrganisationID],
    		[I].[StorageID],
    		[I].[SkuID],
    		SUM([I].[CountAtEnd]) AS [CountPerMonth]
    	FROM
    		[dbo].[Inventory38] AS [I]
    	GROUP BY
    		YEAR([I].[Period]),
    		MONTH([I].[Period]),
    		[I].[OrganisationID],
    		[I].[StorageID],
    		[I].[SkuID]
    )
    SELECT
    	[cte].[Year],
    	[cte].[Month],
    	[cte].[OrganisationID],
    	[cte].[StorageID],
    	[cte].[SkuID],
    	SUM([cte].[CountPerMonth]) OVER (PARTITION BY [cte].[OrganisationID], [cte].[StorageID], [cte].[SkuID] ORDER BY [cte].[Year], [cte].[Month])
    FROM
    	[cte]",1.0
g677n4t,ixitnd,Please send the image. I'm new to sql and it would be great to have something to practice on. Cheers,1.0
g679508,ixitnd,"Hi, I have attached link to images for sql. Please take a look at that",1.0
g67qjqi,ixitnd,"Are you only looking for that specific Sku Id? If you are, would adding storage 1+2+3 work for each month and filtering by that specific sku ID? I need more context as it is confusing. Not getting what is trying to be accomplished.",1.0
g6e8z78,ixitnd,"Select period, sku, (isnull(st1,0)+isnull(st2,0)+isnull(st3,0)) as total from “table” where sku=“milk” order by period desc",1.0
g6e92py,ixitnd,To get just the max date use max(period) might have to use distinct,1.0
g6iphef,ixitnd,The problem is my database is big. It was just an example,1.0
g67ya8o,ixdqcg,Check out this hack that I think shows you another way to do this that i find a lot faster - https://medium.com/narrator-ai/sql-hack-how-to-get-first-value-based-on-a-time-column-without-join-or-window-function-41eae3708c19,3.0
g67yheo,ixdqcg,"Btw this is done via a single line in the group by! No self join or min max cross joined or window function required.  
This trick literally saved me so much pain and you can add multiple values in the same wuery",1.0
g682n4j,ixdqcg,"Pretty cool hack, although... LTRIM in MSSQL doesn't have a second argument.

and it preformed slower than the cross apply

min max concat: [https://imgur.com/wkxrsf3](https://imgur.com/wkxrsf3)

min max concat with right()\* (replacing dates) [https://imgur.com/7mNMA8y](https://imgur.com/7mNMA8y)

fantastic idea though.

&amp;#x200B;

edit: I added this query to the video description as yet another way to do it.. with the statistics io pics.  1000 ways to answer a question, aiming to find the fastest.",1.0
g684c7w,ixdqcg,"Oh fascinating I guess grouping is a bit slower since it’s a row based data store vs a columnar warehouse ( Redshift, Snowflake and bigquery  all had this being more than 2x as fast)",1.0
g686oo4,ixdqcg,"I think ill do a part 2... taking this idea.. I put a columnar index on the primary table.  


Cross apply is still faster, but now the self - inner join is giving it a run for its money.  


Inner join dropped out of parallelism mode and they are neck and neck. since the inner join is only about 20ms slower on the elapsed, 5 ms faster on the cpu (average on 20 runs).. i think i would call that a tie.  


No columnar index. Cross apply, hands down.  
With a columnar index... tie between inner self join and cross apply  


window functions and this new group by still use the row based indexes and remain unchanged.",2.0
g684veu,ixdqcg,"Wouldn't surprise me if thats true. Each engine has its... thing...  


in MSSQL IN and EXISTS use the same execution plan in 90% of the cases...  


Use an IN in MySQL... (vs EXISTS or a JOIN) and the performance tanks.",1.0
g685ws9,ixdqcg,"Yeah each system and their ...thing...

I work for you a data company and we built an analytics software on top of warehouses so instead of supporting each warehouse independently we wrote a SQL abstraction that complies to each flavor of sql so it was fun to deal with all the little quirks of each system",1.0
g67e1g5,ixdqcg,[deleted],2.0
g67ulow,ixdqcg,"No problems. Always looking for more ideas. I was going to do one on how @ # ## tables all use tempdb... but its a super short video with not much substance to it aside from ""watch me insert data into an @ table, watch tempdb's pages grow"".  


If you have an island/gaps queries or a query youd like to see ripped apart.. tag this user in a post or dm me.",1.0
g66vnbn,ixd0lw,"If it's just swapping two values, something like this should probably do the trick

```
INSERT INTO table (Age, City, Zip, Sex)
    SELECT Age, City, Zip, (CASE
        WHEN Sex = 'Female' then 'Male'
        ELSE 'Female')
    FROM table;
```",2.0
g6766nj,ixd0lw,"This seems like the most straight forward answer.  
Considering you're using MS SQL you can also use IIF since there are only 2 outcomes.  
IIF(Sex = 'Female', 'Male', 'Female')",1.0
g67pc09,ixd0lw,There isn’t just two outcomes in my real example. This was just to give some theory of what I’m looking for.,1.0
g67pf8l,ixd0lw,There you go. Case will be your answer then 😃,2.0
g67f4c4,ixd0lw,"Yea except I don’t want to have to call out each column. The reason being I’m iterating through 10 tables and each table has different columns. 

I’m writing a deliverable script and it’s for a new product in mvp stage. We do envision making updates to these tables and adding a column here and there. I don’t want to forget to update this script as well. I’d rather if possible just bring in ALL columns and not specifically call each out.",1.0
g68ll5g,ixd0lw,"This answer https://stackoverflow.com/a/15344327 seems to do just that. Basically, copy your table to a temp table, do any updates there to just the columns you want to change. Then insert all rows from the temp table back into the original table, and drop the temp table.",1.0
g67rjvh,ixd0lw,Let’s say instead we were dealing with days of the week column in a table. I have a restaurant and just want to replicate my staff assignments from Saturday into Sunday. Obviously there are other days of the week. If I use this logic it doesn’t quite work.,1.0
g661q68,ixd0lw,"Which platform? If on oracle check out these:
https://dzone.com/articles/in-oracle-sql-should-you-use-case-decode-or-coales",1.0
g661ryx,ixd0lw,Sql server,1.0
g661xc9,ixd0lw,https://www.w3schools.com/sql/func_sqlserver_replace.asp,2.0
g662867,ixd0lw,Replace looks promising but not exactly sure how this would be set up.,1.0
g663tie,ixd0lw,Also there are 30 plus columns in each table and they are different across tables. However I want an elegant solution and do not  want to list out all columns.,1.0
g667aln,ixd0lw,"I have had a lot of times where I needed to list out all the columns before in sql server. Remember you can right click on the table, script table as -&gt; select to -&gt; new query window. This, plus multi line editing made it pretty quick. How many tables are you looking at?",2.0
g667gew,ixd0lw,"Well I’m actually trying to implement a customer job that will kick off and run a sql script. So I can’t just right click, script table...",1.0
g65ocxl,ix9v6l,"weighted average implies that you change how you measure stuff, in your example you are calculating just a normal average though: average of (50*20 + 50*40)/100 = 30",1.0
g67ebla,ix9v6l,"I must have worded it poorly, it's a weighted average because if offer 2 has $50 in sales and offer 3 has $50 in sales and offer 4 has $100 in sales the total would be
.3+.69+2.46=3.46 weighted average offer count",1.0
g67i4vv,ix9v6l,"you probably want to word it very carefully - what is the coefficient, what do you mean by  the ""count"", what offer # has to do with the result, etc.

e.g. you need to clearly specify how do you get from $50 to 0.3 (0.3 = xxx +/-/*/ln/etc ""offfer #2"" .... $50 )",3.0
g685b29,ix9v6l,"Well, I'm not sure how else to describe a weighted average by sales and offer, so I'll just have to try googling elsewhere I guess.  Thanks for trying to help.",1.0
g687s3y,ix9v6l,"here's the formula to calculate averages for a set of values Xi

      sum( Xi)/count(Xi)

here's a weighted average formula for a sequence Xi with corresponding weights Wi:

     sum( Xi * Wi) / count( Xi)

that's pretty much how you describe the weighted average - by describing your Xi and Wi sequences.",2.0
g65t12j,ix5mh1,I'd also like to have the total value labeled as 'Total',1.0
g65zvc8,ix5mh1,"Not sure where your nulls are or what is null that you are trying to rollup,

But a join to a left (so nulls are not disappearing...
And an isnull(theNullColumn,'NULL') might help.

Edit.. sorry. Oracle.. NVL(nullVolumn,'value')",1.0
g647rg2,ix35au,"      SELECT project
           , SUM(AR)                   AS sum_AR
           , SUM(CASE WHEN fee_type = 'fees'
                      THEN AR
                      ELSE NULL END)   AS sum_fees_AR
           , SUM(balanace)             AS sum_balance           
        FROM yourtable
      GROUP
          BY project
      HAVING SUM(balance) &lt;&gt; 0",3.0
g640xlr,ix1lj1,"     WITH 
       ten ( n ) AS
         ( SELECT 0 
           UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 
           UNION ALL SELECT 4 UNION ALL SELECT 5 UNION ALL SELECT 6 
           UNION ALL SELECT 7 UNION ALL SELECT 8 UNION ALL SELECT 9
         )
      , hundred ( h ) AS
         ( SELECT t1.n + 10 * t2.n + 100 * t3.n
             FROM ten AS t1
           CROSS     
             JOIN ten AS t2
           CROSS     
             JOIN ten AS t3
         )   
     SELECT the_date
       FROM ( SELECT CURRENT_DATE - INTERVAL h DAY AS  the_date
                FROM hundred
               WHERE h BETWEEN 0 AND 375     
              UNION ALL
              SELECT CURRENT_DATE + INTERVAL h DAY
                FROM hundred
               WHERE h BETWEEN 1 AND 375         
              ) AS w
      WHERE DATEPART(DW,the_DATE) = 4
        AND YEAR(THE_DATE) = YEAR(CURRENT_DATE)  
     ORDER 
         BY the_date",2.0
g640g5n,ix1hus,Sounds like a good case to learn something more about CTE's.,3.0
g63vb4r,ix1hus,"    SELECT
    CASE
        WHEN &lt;case_1_logic&gt; THEN 'Case 1 Label'
        WHEN &lt;case_2_logic&gt; THEN 'Case 2 Label'
        ...
        WHEN &lt;case_n_logic&gt; THEN 'Case n Label'
        ELSE 'Else_label'
    END AS Case_Group
    FROM table
    WHERE some_filters
    GROUP BY CASE
        WHEN &lt;case_1_logic&gt; THEN 'Case 1 Label'
        WHEN &lt;case_2_logic&gt; THEN 'Case 2 Label'
        ...
        WHEN &lt;case_n_logic&gt; THEN 'Case n Label'
        ELSE 'Else_label'
    END",2.0
g63w5fl,ix1hus,"In SQL Server, this can be simplified to not repeat the `CASE` logic

    SELECT Misc.Case_Group, COUNT(*) AS WhateverAgg
      FROM table
     CROSS
     APPLY (SELECT CASE 
                     WHEN &lt;case_1_logic&gt; THEN 'Case 1 Label'
                     WHEN &lt;case_2_logic&gt; THEN 'Case 2 Label'
                     ...
                     WHEN &lt;case_n_logic&gt; THEN 'Case n Label'
                     ELSE 'Else_label'
                   END AS Case_Group
        ) Misc
    WHERE some_filters = ...
    GROUP BY Misc.Case_Group

In Postgres, similarly with `JOIN LATERAL (SELECT ...) ON true`.",2.0
g6611jk,ix1hus,"Nice one, man. I will try this out in my queries too. Thanks to share!",1.0
g63vf9t,ix1hus,You just need to copy your entire case logic into the group by without giving it an alias in the group by section.,1.0
g63visv,ix1hus,Take out what ever you have in your GROUP BY statement and put your entire CASE statement in the GROUP BY.  Minus the 'as ...' part of course.  Now you are grouping by the assignments of interest only.,1.0
g640vcg,iwzzqw,"Good 'simple' stuff.
I do similar stuff frequently in my job.",5.0
g6465ls,iwzzqw,very nice!!,1.0
g65scgm,iwzzqw,Great writeup! Classic postgres at its best!,1.0
g665u1s,iwzzqw,This is super cool. I want to try using this on some marketing analytics Reporting I’m building. Thank you very much.,1.0
g69b92v,iwzzqw,"&gt;""I'm not a statistician and not a data scientist, I'm just a developer. ""  
&gt;  
&gt;  
&gt;  
&gt;Humble. Humble. Much respect lmao. This is cool. Thanks for sharing!",1.0
g6590vt,iwzzqw,Cool! What statistical model or algorithms are you using?,-4.0
g65oy3h,iwzzqw,There are nearly 30 pages of example code and methodology at the linked article...,6.0
g6397jv,iwxzs4,"Not sure about your error, but why is this a cursor at all? All you need to do is use the correct data type in the temp table and then use a subquery.",5.0
g63gulx,iwxzs4,"To clarify, the CUSER is ""Character User"", its a VARCHAR2 field. This ERP system is a pain in the ass to import data and this table is the only way to do it unfortunately.",1.0
g65und4,iwxzs4,"To clarify, why are you using a loop? The problem I'm responding to isn't that I think you're using a literal cursor. It's that you're explicitly avoiding set based logic.",1.0
g677vlb,iwxzs4,"Ah, mostly from lack of understanding and from modifying an existing script given to me by the ERP software manufacturer.",1.0
g68vl2p,iwxzs4,"I see you’ve already given up on the PL/SQL approach but just for completion. Your screenshot showed you were missing a semicolon, this was added and you hit this new end-of-file error. That’s because you didn’t submit the PL/SQL anonymous block to the DB before the end of your file (you were running this in as a script). You need to submit the anonymous block with a forward slash (/) on a new line. Yes, usually you shouldn’t use PL/SQL when you can just use SQL (as usual, there are exceptions to this rule of thumb like distributed transactions but you’ll know it when you hit them).",2.0
g69g0oc,iwxzs4,"This makes sense. The script I copied was buried in a larger script with a slash above and below.

Looks like the full script set CUSER41 to """" and then this script, and then deleted all entries where CUSER44 was ""ARINVT UPDATE""",1.0
g69jdzy,iwxzs4,"The slash above that block would be to submit the previous command and to tell whatever client you’re using (sql*plus, sql developer etc) that it’s ready to start the next command. I’m not sure if you need help with the other part of the script? It doesn’t look to be relevant to the problem you shared in your OP.",1.0
g6aypd4,iwxzs4,"Ok, I understand that part now.

And I'm wondering if that's why the full script didn't run either. I am using a back-end way to access the data. They have a front end way to enter commands that possibly runs sqlplus (I have to run a series of sqlplus commands at the command prompt for a 3rd way to access data as well) so makes me wonder if their script was meant to be ran under sqlplus.

And I have the rest figured out, I didn't need the """" fields and used a separate command to dump that temp table after I was done.",1.0
g6ayzpn,iwxzs4,"I can’t comment on the full script without seeing errors that were raised when you attempted running it, and probably the script itself. What do you mean back-end and front-end, you can only ever run SQL statements through some client process that talks to the DB, sure you might have different clients that do their own thing but it’s the same client-Server architecture.",1.0
g6b7ths,iwxzs4,"Their original script was for a different process to update tiered pricing, so it did a lot that I wasn't interested in.
The main block that I copied did 3 things. 1) it set some fields in the temp file to """" and set a field to ""ARINVT UPDATE"". 2) it looped through the temp file and set the ARINVT table fields to whatever was listed based on the ID matching in the temp file. 3) it deleted the rows with ARINVT UPDATE from the temp file, essentially cleaning it up. The full script had other stuff based on the tiered pricing.

Back-end vs frontend: I use dbVisualizer to access the database from the ""back end"" and can run any oracle sql command against the database as a dba. They offer a limited ""front end"" ""sql command"" window within the ERP GUI that allows specific commands to be ran. They also offer ""back end"" through a SQLPlus command prompt for importing a backup file.

The only method they offer and we have found to import actual data (like CSV) is to use their front end ""GL Import"" tool that dumps a CSV to this temp table. We have tried adding a temp table ourselves in their frontend and it either immediately deletes it, crashes, or throws a fit. I haven't tried adding one using dbVis. I just renewed to get version 11 of dbVis and it has its own import tool, so I might try that to be able to create a temp table myself and import direct instead of using their clunky frontend.

I understand what you mean that it is all the same, they just cripple the front end to limit back-end commands that can be ran.",1.0
g63jroy,iwxzs4,"Well, your screenshots show that you didn’t put a semicolon after your update statement, but your code here in your post does.

I’d suggest confirming that you actually did put that semicolon in your code after the where clause.  That’s where the error (end of file) is from.",1.0
g63k48i,iwxzs4,"Yes, added it to the 2nd attempt, same thing, seeing the EOF at that semicolon on the WHERE clause.",1.0
g63l742,iwxzs4,"This will sound weird.  But is there any chance you copied your code from somewhere?  I know that whenever I copy code from somewhere into toad, it gives me stupid formatting errors about invalid characters and EOF issues.   Things like invisible Carriage Returns really mess Oracle’s parser up.

Sounds dumb, but try writing the code by hand in a new window and confirming that every character you meant to put there is actually there.

Also, as mentioned above, you can get away with updating without the cursor:

        UPDATE arinvt a
        SET a.nuser5 = NULL
        WHERE EXISTS (SELECT to_number(cuser40) arinvt_id 
                                         FROM input_file_recn v          
                                          WHERE a.ID = to_number(v.cuser40));


Try running this above as a select statement to ensure you’re getting the right records before you update.

Hey thanks there /u/Josh_Your_IT_Guy for the gold!  Glad you got your issue fixed!",1.0
g63ujgc,iwxzs4,"Weird, your pulls an empty dataset in a select statement, but using the IN instead of EXISTS pulls the correct 17272 records. If I drop the where id=id part, it pulls all 33674 records.

This is a one time data dump, so I will try the IN statement in our test environment after I compare some of the select results to make sure its pulling the right IDs but it looks promising.

&amp;#x200B;

&amp;#x200B;

Edit, I should add that I tried typing it out from scratch just in case, had that copy/paste issue before on another script. something about UTF8 vs ISO and how it handled quotation marks bit me before.",1.0
g63uqmh,iwxzs4,"Sorry, corrected my statement.  I was trying to use the alias when it should be the function used to build it.

IN will work fine for a 1 time pull.  I would just advise against putting it into production.",1.0
g64dy53,iwxzs4,Worked! Thank you! Saving that script now,2.0
g64e8tp,iwxzs4,Great!  Sorry looking over it i realized I missed the last parentheses.,1.0
g64erz0,iwxzs4,"and the extra letter on the temp table, but I caught it",2.0
g64f2hp,iwxzs4,"Oh man sorry!  I’m all over the place lol.

Just as a heads up, not sure if you’ve ever used this but Oracle has a free 19c DB that you can use to do POCs.  http://livesql.oracle.com 
It’s a free 19c DB that lives right in your browser.  The Editor is on your browser too and it acts just like a normal DB does, without any licensing costs.  it’s saved my butt a few times!",1.0
g64g2r1,iwxzs4,"No problem at all

And haven't tried that yet, I will see if it can be beneficial. Our ERP system uses 11g (I know...) And has a few THOUSAND tables... (Not views...that's a few thousand more...) We have a duplicate test environment that imports an export from the production environment, so it allows us to test things before throwing it at production, but being able to tinker with just small chunks of data is nice. I have M$ SQL running locally, but if course Oracle does things just differently enough to drive you batty.",1.0
_,iwxzs4,,
g63wxh9,iwxzs4,"Ahhh, I see it now. Yes, that worked too.

Ok, time to play in the test environment.",1.0
g63gwxi,iwxzs4,"Oracle can be very picky. My guess is you are trying to update a record that does not exist in the destination table hence the end of file error. 

Still not sure why you want a loop here though as da\_chicken pointed out a basic update will work as well.",0.0
g63ixg6,iwxzs4,"checked again and nuser5 exists in arinvt, some are 1 and others are null.

Can I have an example of the update command you are suggesting? I thought I needed the loop to make sure it changes only the list of IDs given.",1.0
g63mint,iwxzs4,"Try

UPDATE arinvt             

SET nuser5 = null             

WHERE ID IN  (SELECT to\_number(cuser40) arinvt\_id FROM input\_file\_rec) ;",2.0
g63qvuw,iwxzs4,"Just as an FYI: IN should only be used for searching against hard coded values.  When using a sub query, EXISTS is the more efficient solution (especially with large data sets.

If this is a 1 time thing, then IN will do, but if it’s going to be used often or deployed, EXISTS is the proper approach.

http://www.dba-oracle.com/t_exists_clause_vs_in_clause.htm",0.0
g67mlcq,iwxzs4,"Wow I never knew this...

 *The Exists keyword evaluates true or false, but the IN keyword will compare all values in the corresponding subquery column.  If you are using the IN operator, the SQL engine will* ***scan all records*** *fetched from the inner query. On the other hand, if we are using EXISTS, the SQL engine will* ***stop the scanning process as soon as it found a match.*** 

&amp;#x200B;

I always thought when it hit the matching value it quit scanning when using IN. That just seems weird to me. But hey it's Oracle. I've spent enough time chasing down ORA-xxxx messages and rewriting queries to work around the quirks that I don't know why this surprises me.",2.0
g687lvc,iwxzs4,"Hahaha yep, two things that I’ve learned:

EXISTS performs much better than IN when using Subqueries and not hard coded values)


EXISTS performs much better than INNER JOIN, so it should be used to filter IN/OUT data if you don’t need anything from the joined table.

That last one has helped me optimized many queries where tables were INNER JOINED for the sake of filtering records (only return records where there is and isn’t a match) and not because a column was actually needed.",0.0
g68vy58,iwxzs4,"This is complete rubbish. The optimizer is perfectly capable of deciding how to execute the query for you in the most optimal way. As long as your query is correct and your statistics represent your data, you can be confident it will do the right thing.",1.0
g690lda,iwxzs4,Lol okay.,1.0
g63dxjm,iwxl67,"    SELECT DATE();

Also

    $ which sqlite3

By the way that's a *super* old version of SQLite in that GitHub repo and I'm not even going near the dodgy anonymous Dropbox binary. 

What features are you missing in the binary bundled with your version of Android?",4.0
g648jz1,iwxl67,"Hey thank you! 

However I realised that the problem in my case is that I'm not able to ""write"" the SQLite3 binary into /system/xbin. I am facing some kind of write restriction in that location, will have to figure it out somehow. 

And my phone doesn't have any sqlite binary by default, so had to download it. 

Thanks for your help!",1.0
g62smfb,iwuyjf,"The ease of this is exponentially proportional to the number of items you have to deduce an answer from. For a set of n items there are 2^n possible combinations of n or fewer items. For context if your store has 30 items in it you are already past a billion distinct possible combinations of items. 

You can probably tell that both options (either calculating on the fly or storing all possible combos for a lookup) would be pretty costly and inefficient at even small scale. 

To answer your question though, if you are dealing with anything more than a handful of items I would opt for a lookup table with a delimited list of items in one column and their combined price in another",2.0
g62syho,iwuyjf,"Think this is a variant of the knapsack problem  
[https://en.wikipedia.org/wiki/Knapsack\_problem](https://en.wikipedia.org/wiki/Knapsack_problem)",2.0
g62wh9e,iwuyjf,"The viability entirely depends on how big your problem space is, and how accurate your result needs to be. Other people have pointed out the 2\^n combinations, and the Knapsack Problem, so I'll throw in the 3rd link: the [P vs NP problem](https://en.wikipedia.org/wiki/P_versus_NP_problem). Basically, anyone getting a good complete perfect general solution to this problem isn't posting it to Reddit, they're claiming a [Millennium Prize](https://en.wikipedia.org/wiki/Millennium_Prize_Problems) for solving one of the biggest unsolved problems in mathematics / computer science.

If you've got a small number of things and you need an answer that's pretty good but not necessarily the absolute best, yeah you can do some approximations.

If you're trying to reverse-engineer spending habits based on the total spend of a supermarket bill, yeah, no, that's pretty much impossible.",2.0
g61y8zc,iwp02d,"It's too bad you can't add logging to your application, or otherwise debug it, but what you could start with is to log all SQL statements that hit MariaDB. Then you could see if the application is even attempting to insert the data you think should be there. (I assume there's no Microsoft SQL Server involved, but using the phrase ""SQL Server"" a few times might make a reader think you are referring to that.)

That may at least give you a clue about which end has the problem.

Some info about that: [https://mariadb.com/kb/en/general-query-log/](https://mariadb.com/kb/en/general-query-log/)",2.0
g63nphs,iwp02d,"Thanks for the tip. At least I got something.

After generating a log file and inspecting it in detail, I am highly suspicious of one point in the log.

Toward the end of the file there is multiple attempts to do below operations.

    200921 16:13:21     Query	Select update_counter from changes
                        Query	Select delete_counter from changes
                        Query	Select update_counter from changes
                        Query	Update entlab set data='string that contains non-latin characters'
                        Query	Insert into entlab values ('some id number', 897, 'string that contains non-latin characters')
    
    200921 16:13:22 - Same thing happens again with the same data
    .
    .
    .

It is trying to insert the same data but failing so it is constantly trying again.

When I look closely at that specific data, 2 things stand out.

1- It is in Chinese or Japanese

2- It is too long.

The length should not be an issue because there are longer entries with successful operations.

But the non-latin characters may be the issue here.

I am using  utf8mb4\_unicode\_ci for the Collation so this also should not be an issue.

Where am I going wrong?",1.0
g63wjz1,iwp02d,"&gt; the same data but failing so it is constantly trying

Do you see a specific error message?

&gt;I am using utf8mb4\_unicode\_ci for the Collation so this also should not be an issue.

Note that collation is not the same thing as character set (collation says how to compare and sort strings, essentially), although I think this collation necessarily implies the character set utf8mb4. (This comment is more of a side note than a substantial point.)

I've never used Maria, but in MySQL, last time I looked into this stuff, the server has a ""default"" client side character set, it sends that to the client. The client side can override that with its own. So if the application is in Java for example, the character set would be set via the JDBC url.

So TL;DR: you likely need to set the character set on the client. Just a suspicion I have - not really sure what's happening without an error message.",1.0
g6445m3,iwp02d,"Well,

I did try to get another logging with different results now.

I saw no errors. This time counter got stuck at 1043 and repeating itself.

I am attaching the log in case some kind people may take a look at it.

[http://www.filedropper.com/mariadblog](http://www.filedropper.com/mariadblog)

As for the charset,

The DB is set to utf8 but the client is confused I guess. It is constantly changing the charset.

The log file starts with 

    		    10 Query	SET NAMES utf8
    		    10 Query	SET character_set_results=NULL

then it tries 

    		    11 Query	SET NAMES 'utf8mb4' COLLATE 'utf8mb4_general_ci'
    		    11 Query	SET lc_messages = 'en_US'
    		    12 Connect	root@localhost as anonymous on 
    		    12 Query	SELECT `config_data`, UNIX_TIMESTAMP(`timevalue`) ts FROM `phpmyadmin`.`pma__userconfig` WHERE `username` = 'root'
    		    11 Query	SET collation_connection = 'utf8mb4_unicode_ci'

At this point, I am about to give up. For two days, I am tinkering with this.",1.0
g61qwah,iwp02d,nice,1.0
g64r4n3,iwp02d,"Okay, guys. Thank you again for your suggestions. I did manage to get it working but not with MariaDB10.

I installed MariaDB5, disabled MariaDB10 then BAM everything just clicked.

Lesson learned.

DO NOT try to use new DB servers with old school software thinking that it should work or I can make it work by tweaking here and there. 

Maybe, it will or like in my case, it most probably won't but most importantly; 

**IT AIN'T WORTH THE DAMN TROUBLE.** 

:)

Again, thanks everyone for their helpful tips. At least, I did manage to learn a couple of new things.",1.0
g5z2c1a,iwdeqw,"This has been a great funny and interactive tutorial which I have used and working through it.

The humour and simplicity is great. 

Has anyone else come across it and the channel?",10.0
g5z6wbj,iwdeqw,"Yes I’ve watched a bunch If their sql and python videos, particularly when I needed a different explanation than what we were taught in our course",6.0
g5zdeoo,iwdeqw,They have python ones too? I loved their sql vids,2.0
g5zaigl,iwdeqw,"I have been subscribed to their channel for almost a year now, they are awesome",3.0
g5zmmf3,iwdeqw,The subtly of the joke in that video was genuinely funny,2.0
g5zrv65,iwdeqw,Wow! This woman is awesome! How come I've never come across her videos while researching SQL and DBs,6.0
g60of4x,iwdeqw,"I watched her python videos, never got a chance to check out her SQL.",3.0
g61iw8z,iwdeqw,Damn socratia is amongst the best educational channels,3.0
g678eic,iwdeqw,Yes! They are amazing!,2.0
g64orw6,iwdeqw,"Hey hello! This is our channel! We're not filming right now because of the pandemic, but we definitely have plans to make more coding videos. We'd love to hear what you would find helpful.   
 I'm not on Reddit much at all, but a friend told us about this thread. Thanks a lot for sharing - it's really hard to find educational channels on YouTube. I search every day like it's my JOB and I'm still finding channels that have been around for years. So really, thank you so much for spreading the word.",2.0
g678e68,iwdeqw,Hey thank you for the great work!!! :),2.0
g6kbhch,iwdeqw,💜🦉,1.0
g5ypbac,iwbu2h,"I think the nested query is selecting areas that are greater than 0 from Y so long as x.continent=y.continent (this is used to pull the group of countries in the continent) and the where statement says to only keep only countries that have an area larger than all the countries in this group.

I believe an easier way to do this would be to use a group by continent and a having clause of max(area). I think you would need to put an aggreregate function around name too",3.0
g5yxbae,iwbu2h,"similar thread posted recently, see [my reply here](https://www.reddit.com/r/SQL/comments/irbau3/beginner_question_sqlzoo_answer_explanation/g4xr7ag?utm_source=share&amp;utm_medium=web2x&amp;context=3)",0.0
g5ywxwx,iwb2vw,"put the calc into a subquery, then use the alias in the outer query

*temp table not required* (they're expensive in every case)

    SELECT a
         , b
         , c
         , nrate
         , nrate * c AS nValue
      FROM ( SELECT a
                  , b
                  , c
                  , a / b AS nRate 
               FROM test
           ) AS s",3.0
g5yxtm8,iwb2vw,"Note to OP: when you get the inevitable suggestion to use a CTE instead of a subquery, this answer (with about 95% certainty) is how SQL Server is going to execute it anyway.",2.0
g5z88c7,iwb2vw,"Yeah, I think this is indeed the method I was looking for, it solves the ""repeat the calculation"" part perfectly, but on the ""make the query easier to read, follow and debug"", I think it's actually clearer at least in this particular case with a very small record set, to just repeat the calculations.  

If I have to do it again, I'll start with the Sub-query and work backwards. 

Thanks",1.0
g651wsm,iwb2vw,"I mean... if his source table has 100M rows you might find an #table to be a lot faster than using a subquery or CTE... also, this doesn't seem like you're simplifying anything from a code perspective although I'm not sure how the engine would handle it.",1.0
g65h3ss,iwb2vw,"temp table faster than subquery or CTE?

most definitely not in this case

&gt; doesn't seem like you're simplifying anything from a code perspective

on the contrary that's exactly what this does

OP wanted to resolve code bloat because ""the initial calculation is longer and more complicated than a/b, but it's reused about 6 times in the rest of the query""

in this case the subquery is a way to simplify the syntax -- note that the optimizer still resolves the whole query as a simple pass of the data

creating a temp table would be a waste of resources here",2.0
g65kwq3,iwb2vw,"Sorry didn't see the bit about the code itself being that verbose and thought you were adding a fair amount of lines to the base just to solve an a/b problem.

Even still I'm not entirely convinced a subquery will outperform a #table. I'm not saying you're wrong, but I don't know enough about it.",2.0
g5ygyxd,iwb2vw,"Put the results ina. Temp table, then just use that table",0.0
g5y4ku1,iw8u1f,Convert the date into a proper format (ISO) before inserting it into the database.,5.0
g5y7xem,iw8u1f,"I would do that, but this is a dataset from different sites that I didn't make, so I have to try and find a way to solve this issue.",1.0
g5ybfjn,iw8u1f,You could try to convert the date column with SQLite strftime() function on the fly.,2.0
g5yep8p,iw8u1f,"I did think about the strftime function a bit, but I haven't found anything interesting so far. I've found a solution now, but I'll go and search for a solution using strftime.",1.0
g5y5ycd,iw8u1f,"agree with others if you can get the import to the correct formate first that would be best.
but you can check the length, and switch your substr in a case statement",2.0
g5ydkmj,iw8u1f,"I found a solution from this link [https://stackoverflow.com/questions/40984650/reformat-dates-in-column](https://stackoverflow.com/questions/40984650/reformat-dates-in-column)

and I developed the query a bit more.

Just one thing: don't forget to change the name of the table to ""Sample\_Superstore"". If you import ""Sample Superstore"" without renaming it, this code won't work.

First step:

    UPDATE Sample_Superstore
    -- SET OrderDate = substr(Sample_Superstore.OrderDate, 6, 4) || '-' ||
    --            substr(Sample_Superstore.OrderDate, 1, 2) || '-' || '0' ||
    --            substr(Sample_Superstore.OrderDate, 4, 1)
    -- WHERE Sample_Superstore.OrderDate LIKE '__/_/____'

Second step

    UPDATE Sample_Superstore
    -- SET OrderDate = substr(Sample_Superstore.OrderDate, 6, 4) || '-0' ||
    -- substr(Sample_Superstore.OrderDate, 1, 1) || '-' ||
    -- substr(Sample_Superstore.OrderDate, 3 , 2)
    -- WHERE Sample_Superstore.OrderDate LIKE '_/__/____'

Third step

    UPDATE Sample_Superstore
    -- SET OrderDate = substr(Sample_Superstore.OrderDate, 5, 4) || '-0' ||
    -- substr(Sample_Superstore.OrderDate, 1, 1) || '-0' ||
    -- substr(Sample_Superstore.OrderDate, 3 , 1)
    -- WHERE Sample_Superstore.OrderDate LIKE '_/_/____'

Fourth step

    UPDATE Sample_Superstore
    -- SET OrderDate = substr(Sample_Superstore.OrderDate, 7, 4) || '-' ||
    -- substr(Sample_Superstore.OrderDate, 1, 2) || '-' ||
    -- substr(Sample_Superstore.OrderDate, 4 , 2)
    -- WHERE Sample_Superstore.OrderDate LIKE '__/__/____'

Fifth step

    UPDATE Sample_Superstore
    -- SET OrderDate = replace(OrderDate, '/', '-')

FINAL STEP:

    SELECT Sample_Superstore.OrderDate,
    CASE
    WHEN Sample_Superstore.OrderDate LIKE '__-__-____' THEN
    substr(Sample_Superstore.OrderDate, 7, 4) || '-' ||
    substr(Sample_Superstore.OrderDate, 1, 2) || '-' ||
    substr(Sample_Superstore.OrderDate, 4 , 2)
    ELSE
    Sample_Superstore.OrderDate
    END OrderDate_Corrected
    FROM Sample_Superstore

This is how it's done.

I will provide an explanation for any future viewers of this post a bit later in the week.

&amp;#x200B;

Btw: would there be a way to execute the whole code in one go:

    UPDATE Sample_Superstore
    SET OrderDate = substr(Sample_Superstore.OrderDate, 6, 4) || '-' ||
               substr(Sample_Superstore.OrderDate, 1, 2) || '-' || '0' ||
               substr(Sample_Superstore.OrderDate, 4, 1)
    WHERE Sample_Superstore.OrderDate LIKE '__/_/____'
    
    UPDATE Sample_Superstore
    SET OrderDate = substr(Sample_Superstore.OrderDate, 6, 4) || '-0' ||
    substr(Sample_Superstore.OrderDate, 1, 1) || '-' ||
    substr(Sample_Superstore.OrderDate, 3 , 2)
    WHERE Sample_Superstore.OrderDate LIKE '_/__/____'
    
    UPDATE Sample_Superstore
    SET OrderDate = substr(Sample_Superstore.OrderDate, 5, 4) || '-0' ||
    substr(Sample_Superstore.OrderDate, 1, 1) || '-0' ||
    substr(Sample_Superstore.OrderDate, 3 , 1)
    WHERE Sample_Superstore.OrderDate LIKE '_/_/____'
    
    UPDATE Sample_Superstore
    SET OrderDate = substr(Sample_Superstore.OrderDate, 7, 4) || '-' ||
    substr(Sample_Superstore.OrderDate, 1, 2) || '-' ||
    substr(Sample_Superstore.OrderDate, 4 , 2)
    WHERE Sample_Superstore.OrderDate LIKE '__/__/____'
    
    UPDATE Sample_Superstore
    SET OrderDate = replace(OrderDate, '/', '-')
    
    SELECT Sample_Superstore.OrderDate,
    CASE
    WHEN Sample_Superstore.OrderDate LIKE '__-__-____' THEN
    substr(Sample_Superstore.OrderDate, 7, 4) || '-' ||
    substr(Sample_Superstore.OrderDate, 1, 2) || '-' ||
    substr(Sample_Superstore.OrderDate, 4 , 2)
    ELSE
    Sample_Superstore.OrderDate
    END OrderDate_Corrected
    FROM Sample_Superstore",2.0
g5yf6xa,iw8u1f,"Glad you found a solution. Such a hassle and a perfect example for sanitizing data on the way into a database rather than the way out. (Not blaming you, just the person you got the data from)

Another possible solution would be to use `INSTR`.  For example:

    SELECT SUBSTR(OrderDate, 0, INSTR(OrderDate, '/')) ...",3.0
g5yiwu4,iw8u1f,"I will take a look at this later. I am a bit lost on how to use this, but thanks for providing alternatives.",1.0
g5yiiog,iw8u1f,"I found a solution for executing the code in one go: put a semi-colon ; after each sql statement, except the last one.

We get the following query:

    UPDATE Sample_Superstore
    SET OrderDate = substr(Sample_Superstore.OrderDate, 6, 4) || '-' ||
               substr(Sample_Superstore.OrderDate, 1, 2) || '-' || '0' ||
               substr(Sample_Superstore.OrderDate, 4, 1)
    WHERE Sample_Superstore.OrderDate LIKE '__/_/____'
    ;
    UPDATE Sample_Superstore
    SET OrderDate = substr(Sample_Superstore.OrderDate, 6, 4) || '-0' ||
    substr(Sample_Superstore.OrderDate, 1, 1) || '-' ||
    substr(Sample_Superstore.OrderDate, 3 , 2)
    WHERE Sample_Superstore.OrderDate LIKE '_/__/____'
    ;
    UPDATE Sample_Superstore
    SET OrderDate = substr(Sample_Superstore.OrderDate, 5, 4) || '-0' ||
    substr(Sample_Superstore.OrderDate, 1, 1) || '-0' ||
    substr(Sample_Superstore.OrderDate, 3 , 1)
    WHERE Sample_Superstore.OrderDate LIKE '_/_/____'
    ;
    UPDATE Sample_Superstore
    SET OrderDate = substr(Sample_Superstore.OrderDate, 7, 4) || '-' ||
    substr(Sample_Superstore.OrderDate, 1, 2) || '-' ||
    substr(Sample_Superstore.OrderDate, 4 , 2)
    WHERE Sample_Superstore.OrderDate LIKE '__/__/____'
    ;
    UPDATE Sample_Superstore
    SET OrderDate = replace(OrderDate, '/', '-')
    ;
    SELECT Sample_Superstore.OrderDate,
    CASE
    WHEN Sample_Superstore.OrderDate LIKE '__-__-____' THEN
    substr(Sample_Superstore.OrderDate, 7, 4) || '-' ||
    substr(Sample_Superstore.OrderDate, 1, 2) || '-' ||
    substr(Sample_Superstore.OrderDate, 4 , 2)
    ELSE
    Sample_Superstore.OrderDate
    END OrderDate_Corrected
    FROM Sample_Superstore",1.0
g5y7q7o,iw8u1f,"I was thinking a case statement could be good, but I had know idea what to do.

Thanks for providing some direction. I will post the solution if I get it done.",1.0
g5yboej,iw8u1f,"That's complicated because it could be the day, month or both that are one character long.",1.0
g5ysmdf,iw85ug,"Unless building and servicing PCs is your day job, find an off the shelf machine for the job. Nothing worse than being tier 1 support on a pc you built because there isn’t a dell or hp logo on the front.",6.0
g5z8z8l,iw85ug,"Why not put the database in the cloud, and save on hardware?

Furthermore why not Ubuntu Server?",4.0
g5y0jib,iw85ug,"Go Ryzen, but why not Zen 2?",1.0
g5y28lt,iw85ug,"Because i already have the 2600 cpu and board, And the difference between the 2600 and 3600 is minor.

maybe you're right and it's worth to install 3600 cpu with A520 board to make it more future proof just in case.

Thank you",1.0
g5ymcbu,iw85ug,"Who is doing backups? Are backups being completed consistently? Is this database mission critical?

And finally, is Azure SQL an option that’s on the table? You can offset licensing costs by using your on prem license, no hardware to worry about, no worrying about whether the backups are going offsite or not, easily scalable and you are set for the future.",1.0
g5yms4f,iw85ug,"It is not really mission critical database. I do local and cloud backups at night, So worst case scenario they will lose few hours of work.

His accounting software company just started few months ago offering cloud-based system solution which is AWS based, But from talking to one of their technicians, I understand that it is not as stable as I would like, So for the meanwhile they will continute to work locally on their own server and maybe in a year or two the software cloud solution will be stable enough to consider.

Btw the accounting software servers have been hacked about 2 months ago....

Thank you for your thoughts",1.0
g5ypfam,iw85ug,Have you tested the backups?,1.0
g5yrffa,iw85ug,"Yes actually I've recovered some db files 2 weeks ago which saved the client some grief because he is not doing the manual backup he's supposed to do every day.....

I backup 2 images of all the server's drive locally at different hours to 2 seperate drives and also use crashplan as online cloud backup.",1.0
g5ytt8t,iw85ug,"OK, let me ask this a different way.

*How* are you backing up the database? Because if you're just copying the MDF and LDF files, you got lucky. And you're reliant upon the _client manually backing up_?

Since you've upgraded the client to Standard Edition, you have SQL Agent available. Install [Ola Hallengren's Maintenance Solution](https://ola.hallengren.com/), schedule the backup jobs to run at least daily (preferably a daily FULL and periodic LOG with the database using the FULL recovery model so that you can do point-in-time restores), and then backup the files _that_ produces.",1.0
g5ywkh1,iw85ug,"I'm not the one that installs and maintain the sql database. That's what the accounting software technicians do and provide support for. I just install and maintain the hardware and OS.

I do make full drive images of the system drive and data drive on daily basis. (The sql database is installed on the data drive and not on the main OS drive, So it backups it same time as the MDF/LDF folders).

I will ask upon the new sql install for the software technicians to provide me with some automatic backup method they're familiar with just to have a proper sql backup.

Thank you",1.0
g5yx139,iw85ug,"So you've ""recovered"" the files but without doing a test restore, you've no way of knowing if those backups work properly.

Per the agreements between you &amp; your client and your client &amp; the software vendor, who is responsible for backups and ensuring that the backups work properly? Who is responsible for restores?",0.0
g5zgksh,iw85ug,"The client is responsible to do a manual daily backup as far as the software vendor is concerened and it ain't gonna happen with most small office clients. The software vendor is not really ""committed"" for restores, Because it is dependant on the existance and functonality of the backup the client is supposed to do manually.

I also am not responsible for the clients backup as per our agreement, But I do my best to maintain a reasonable backup scenario in case s#!t happens.",1.0
g5zk6nw,iw85ug,"You need to get those SQL backups scheduled (see my earlier post) so that the client _isn't_ dependent upon a manual backup. The day they forget will be the day the hard drive fails.

Without backups, everything is at risk. Having reliable backups that are known to work is a fundamental need for all database environments. And ""the client has to do it manually"" doesn't count because they _will_ forget to do it, or they'll do it a few different ways depending on how much of a rush they're in.",2.0
g5zru48,iw85ug,You the boss.,1.0
g5ypdmi,iw85ug,"Your client will be violating the Windows 10 license by using it as a server. 

Also, don’t put server duties on desktop grade hardware.",0.0
g5v6hmv,ivzys8,"Here's a good overview of different types of database platform:  https://db-engines.com/en/

YouTube course on database design:  http://youtu.be/e7Pr1VgPK4w

Normalization poster:  http://graphdatamodeling.com/resources/rettigNormalizationPoster.pdf

Free data models:  http://www.databaseanswers.org/data_models/

PostgreSQL Home:  http://www.postgresql.org/

PostgreSQL wiki:  http://wiki.postgresql.org/wiki/Main_Page",1.0
g5vat16,ivzys8,Thank you!,1.0
g5vpdn5,ivzys8,"https://www.postgresqltutorial.com/postgresql-sample-database/

https://www.oracletutorial.com/getting-started/oracle-sample-database

https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver15&amp;tabs=ssms",1.0
g5uqvhv,ivz3w1,"First parse both columns into 3 fields... then union all three sets together. There do where col2 = eng.

So you end up with 6 columns to start with and then select col1a, col1b union col2a, col2b, etc.

Make sense?",3.0
g5xuiq7,ivz3w1,"Try looking into string manipulations, I think PATINDEX() will solve this problem. You can use it in combo with a CASE statement. I’m currently on mobile but I can send a code snippet tomorrow.",1.0
g5uw0so,ivyu45,There is no difference except that &lt;&gt; is ANSI compliant. I tend to see developers that are more familiar with OOP languages like Java use != because that's what they're used to.,3.0
g5ynvjr,ivyu45,thanks,1.0
g5umgno,ivyu45,I believe they are the same though != is not recognized in Teradata for example,1.0
g5ynvb4,ivyu45,thanks,1.0
g5ujn0d,ivyf17,"Because % is a wildcard string and name is a value, and you use concat to combine them.",12.0
g5uk3uz,ivyf17,Oh! Thanks.,5.0
g5uk5c6,ivyf17,"`LIKE %name%` is not a valid string because it's missing the delimiting quotes, and will generate a syntax error

`LIKE '%name%'` is now syntactically okay, but it won't return anything, because it doesn't reference the `name` column, it's looking for a capital that has the actual letters 'name' inside it

you have to concatenate the wildcard characters around the actual `name` column values",17.0
g5ulajs,ivyf17,"Thanks, my man comes to the rescue again. :)",1.0
g6jlaxe,ivyf17,Just out of curiosity was is the name column representing?  Like the name of the country?  its odd that there is a column just called name in a table called world.,1.0
g5tzj83,ivuyu9,"This is a valid work purpose.  Contact IT support and ask them to either install PostgreSQL for you, or provide you the rights to be able to install it yourself.

If you don't have admin access, ask for it.",3.0
g5tz7k0,ivuyu9,"It’s pretty roundabout, but you may be able to get ODBC drivers through some sort of Python module that might not need admin rights.",2.0
g5ucppi,ivuyu9,"I currently use this approach, but then in R",2.0
g5tzgbj,ivuyu9,"Which operating system are you using? 

If you are using Windows, you can ""install"" Postgres without being administrator, but you won't be able to register it as a Windows service then. 

In a nutshell, you would download the [Windows binaries](https://www.enterprisedb.com/download-postgresql-binaries), unzip the archive and run `initdb` manually. To start Postgres you would then need to run `pg_ctl` rather than starting it through a Windows service.

Something along the lines

```none
unzip postgresql-12.3-1-windows-x64-binaries.zip -d c:\Tools\Postgres
c:\Tools\Postgres\pgsql\initdb -D c:\Data\Postgres -U postgres -E UTF8 -A scram-sha-256
```
Obviously you need to adjust the paths to your liking. It will ask you for the password of the superuser (`postgres`). You need that later to log in to Postgres and create a regular user account.


Once initdb is finished, you can use 

```none
c:\Tools\Postgres\pgsql\pg_ctl -w -D c:\Data\Postgres start
```
Note that you need to use `pg_ctl` to also cleanly shut down Postgres to avoid a recovery during the next start. 


Once it's running, use

```none
c:\Tools\Postgres\pgsql\psql -h localhost -U postgres
```

To start the command line client and create a regular database user that you can use to play with. You can also use any of the [many](https://wiki.postgresql.org/wiki/PostgreSQL_Clients) other SQL clients if you don't like the command line tool.",2.0
g5yi81m,ivuyu9,"Thanks, doesnt look that beginner but can learn new stuff this way",1.0
g5u1stb,ivuyu9,SQLite might be allowed. It is very lightweight and might be a good starting place.,1.0
g5yi5su,ivuyu9,"Tx, I will check this out",1.0
g5t0v8n,ivqnsb,Partition/row_number will do it.,5.0
g5t3uyk,ivqnsb,"This is the way OP. 

https://www.mysqltutorial.org/mysql-window-functions/mysql-row_number-function/",5.0
g5tvw1k,ivqnsb,This,1.0
g5tcq8z,ivqnsb,"Is #1 a hard requirement or can you simply ignore duplicates. if find that error tables are usually not needed unless one is trying to do something specific with those records. 

Assuming that #1 isn't a hard requirement AND that prod\_id is the auto-incremented primary key of the target table then you may find it easiest to do something like INSERT ON DUPLICATE KEY UPDATE.  Check out the docs here: [https://dev.mysql.com/doc/refman/8.0/en/insert-on-duplicate.html](https://dev.mysql.com/doc/refman/8.0/en/insert-on-duplicate.html)",1.0
g5ss21s,ivoeh1,SELECT TABLE_ROWS FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'mydata',2.0
g5tn3l7,ivoeh1,"this gives only an approximation of the number of rows

which may be good enough for some purposes

just don't rely on accuracy",2.0
g5u91lw,ivoeh1,Yes.,1.0
g5tgvs0,ivoeh1,"Is that a query you really run often enough that 2 mins is a problem? If you need a count(*) of all rows (no filters) then any index on a not null column will allow that to be read instead of the table - this will almost always be smaller than your table so will be quicker to read entirely. Now, if you *need* anything faster then you have to give it some thought - materialized views aren’t native to MySQL (afaik) but there’s ways around that, have a read of https://fromdual.com/mysql-materialized-views . But really, is it worth the additional overheads? Obviously the other solution to make reading lots of data faster is by improving your hardware - flash storage, many CPU’s so you can do it in parallel",1.0
g5ssp9x,ivoeh1,"Apply some indexes? It will take some time, but your query will be faster after this",0.0
g5ssc0u,ivoeh1,You can also do select count(your auto int id) from mydata,-2.0
g5srs2x,ivnnui,"&gt;  I could use NULLs instead, but I would really like to keep the string 'No invoices'.

use NULL inside the CASE, and COALESCE outside
    COALESCE(
      CASE WHEN EXISTS ( SELECT ... )
           THEN ( SELECT TO_CHAR(MAX(i.invoice_date)) ... )
           ELSE NULL END
         , 'No invoices' ) AS ""AP Last Invoice Date""",2.0
g5tgwu1,ivnnui,Thank you for that.,1.0
g5tmrj6,ivnnui,"oops, i meant to format that to make it clearer

     COALESCE( 
        CASE WHEN EXISTS ( SELECT ... ) 
             THEN ( SELECT TO_CHAR(MAX(i.invoice_date)) ... ) 
             ELSE NULL END 
      , 'No invoices' ) AS ""AP Last Invoice Date""",1.0
g5sjpwb,ivnnui,Change your * to 1,1.0
g5tgvm2,ivnnui,Thank you.,1.0
g5trbcg,ivn383,[This graphic](https://i.imgur.com/5ePLVwk.png) shows the various join types in SQL.,3.0
g5txmu4,ivn383,Thanks that's a really great graphic.,1.0
g5ty74y,ivn383,thank you for not posting a Venn diagram!!!,1.0
g5tz9p3,ivn383,"I hate the use of Venn diagrams to ""explain"" joins. They are useless IMHO.",1.0
g5u0o4a,ivn383,"bookmark this --

https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/",0.0
g5sru4b,ivn383,"that's an inner join, written with pre-1992 syntax",2.0
g5txdss,ivn383,I am sorry I didn't know that. I hope I didn't offend you with my lack of knowledge in SQL.,1.0
g5ty4wh,ivn383,"no apology necessary    ;o)

it's why we're all here -- to learn and to share",2.0
g5tj1cn,ivn383,https://advancedsqlpuzzles.com/2020/03/17/advanced-sql-joins/,1.0
g5txh2n,ivn383,Thanks.,1.0
g5tlbqq,ivn383,"Natural join is just syntactic sugar that I don’t think really ever gets used in real life (because it hides the real meaning). Natural join means join using an equality condition for all columns that exist in both tables. Eg for your tables, there are no shared columns so “select .. from student natural join majorin” will return the Cartesian join of the two tables (ie every row in student * every row in majorin with no filter). That is probably exactly why people tend to avoid natural join - you need to know the metadata in order to know what the query is going to do.",1.0
g5sfwdi,ivmhi2,"I found the free CodeAcademy course to be good, but honestly I didn’t really start learning until I actually used it in my job/data that I have the context for. 

In my opinion, here’s some basic stuff they will probably expect of you:
-know when to do a left or inner join
-know how to use aggregate functions (min, max, avg, etc)
-know how to create CTEs and subqueries

Just my opinion, though",11.0
g5sgl5k,ivmhi2,https://www.reddit.com/r/SQL/comments/g4ct1l/what_are_some_good_resources_to_practice_sql/fnx11mc/,6.0
g5smgqy,ivmhi2,"Wow man, what a post. I’m about to start learning MySQL soon (have purchased Udemy course some time ago, but waiting to get to know Python basics properly first - I’m afraid I will confuse stuff if I learn those two at the same time). Once I start learning, I think I will be interested in asking you some questions if you don’t mind...it’s just I’ve never seen anyone giving such thought out response that will actually help developing skills... you’re amazing!",3.0
g5snmce,ivmhi2,"I'm happy to answer your question, but I'm not that smart. Most of what I know about SQL I learned here by asking the broader community, Googling, looking things up, etc.

You gotta actually do it though.",2.0
g5tcdz1,ivmhi2,Thank you for sharing that post. Extremely thorough and great suggestions.,1.0
g5shj07,ivmhi2,W3schools is quite nice,4.0
g5sn448,ivmhi2,"Go get the Datacamp offer! It was 400 dollars and now 150, I think there is 1 day left to get it. Once you sign up, register for the SQL Data Analyst course which is 45 hours in total. Goodluck ;)",3.0
g5tay1h,ivmhi2,"Thank you, I'll check it out!",1.0
g5sdq3r,ivmhi2,Did you lie about your knowledge of SQL? Or were they aware that your knowledge of it is fairly basic?,7.0
g5takdb,ivmhi2,"That's a fair question. Nope, I didn't lie about my SQL knowledge. I was actually recruited for the position and from the start I let them know that my knowledge was limited. I was required to complete a SQL assessment as part of my interview process, which apparently I passed because... Well here I am with the job. But I was told that I would have to beef up my SQL. So now I'm here asking the best way to do that.",4.0
g5vzc6p,ivmhi2,"SELECT
This is where you pick your columns you would like to return

FROM
Is the table you like to query off of

WHERE
Is the conditions 

There, now you know basic SQL.
THE REST YOU LEARN ON THE JOB.",2.0
g66oqwm,ivmhi2,"Now, practice on LeetCode and StrataScratch. They'll provide you thousands of SQL problems to practice.",2.0
g6foevi,ivmhi2,"Oh thank you, I'm gonna check it out!",1.0
g6czzp3,ivmhi2,LinkedIn learning.  Zillions if video courses.   Sign up for free month.,2.0
g5t2fop,ivmhi2,"A little piece of advice which might help is to think only in terms of tables and the relationships they have together and which you want to represent. Your SQL code will always\* work on tables and produce a table as an output, even if you produce no output you can be producing an empty table, you are working with rows and columns. Every time someone you talk with gets an idea to make something, e.g. a queue, a cube, whatever it is, or to answer a query, it will have to be done using tables and relationships here.

\* things are a bit different with user-defined functions / stored procedures but this is more like an SQL variant/extension than SQL itself",2.0
g5t50c6,ivmhi2,"Two things to keep in mind as you learn SQL:

1. There are different variations of SQL depending on the underlying database system. MSSQL vs Oracle vs MySQL, etc. Think of it like dialects... British vs US English for example.

2. SQL differs from most other programming languages because it is Declarative. Meaning, your code is asking for What you want, but not How to get it. 

Ex: you order food at a restaurant, you say you want certain adjustments. But you don't give the line cooks exact step-by-step instructions on how to cook your meal.

Those two key fundamentals will help you in the bigger picture.",2.0
g5tb496,ivmhi2,"Thank you for those tips, that's very helpful!",1.0
g5rn070,iviw4g,Are you serious?,4.0
g5rn34x,iviw4g,Is there a problem???,-3.0
g5roieo,iviw4g,"No offense, but how old is your kid? One of the hallmarks of success is for a kid to learn on their own and use at least some effort to find answers. I understand that you don't have the capacity to teach him, but it's not like you can't Google some of these terms. I think people would be more responsive if you had explained more in your post what part of these questions is your kid unable to comprehend?

Here are some resources that will help your kid in solving those problems: 

* What is Normalization: [https://www.guru99.com/database-normalization.html](https://www.guru99.com/database-normalization.html)
* 3NF: [https://www.youtube.com/watch?v=\_K7fcFQowy8](https://www.youtube.com/watch?v=_K7fcFQowy8)",3.0
g5rp1dz,iviw4g,I am so sorry this is my first time using reddit for such a thing i mostly have his tutor help him out but he is fallen sick and also he is on a deadline so i had to what I can i am really sorry,-1.0
g5samha,iviw4g,"Type the text out (instead of having us look at tiny pictures) and show us what he's done so far. Then we can help.

As per the sidebar:

&gt; If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own.",2.0
g5r90vt,iveo5l,"I'm not really understanding why a properly structured SQL DB would not provide the views you're using as examples.

What you've currently described is some very simple variations in SQL queries.",8.0
g5rcegd,iveo5l,"Yes we already do simple analysis in sql ...we are creating a cloud solution to access data and want to aggregate it a better way i guess is the best way of saying it.....or put better easy to aggregate by different levels (ie customer or flight) which can be done in sql or python etc but I guess we just want it more dynamic. 


The more I think about it though I think we just need to keep it in sql and maybe flatten it more as we move it",1.0
g5s1fgs,iveo5l,"I'm still not sure you've really adequately explained your problem - you have to query the data differently to display different levels whether you're in nosql or in sql.

It sounds like what you're looking for is a simple business intelligence layer on top of your data - something like PowerBI, Qlik, or Tableau.

For context: I work for a very large online advertising firm - we log around 1.5 trillion bid requests per day into a SQL DB and transform that into hundreds of client specific reports at dozens of levels of aggregation. Our ETL is a bit different than most companies given the volume of data we're bringing in, but on the reporting side, it's (generally) nothing more than SQL and Tableau.

Prior to my current position, I worked for Sears where we (in brighter times) had around 3000 retail locations with thousands of transactions per day each, all feeding into SQL.

To this day, I have yet to find a place where noSQL actually does a better job than a properly configured and managed SQL instance. There are certainly times where noSQL was *easier* to implement, but getting data back out of it... that's a different story.",3.0
g5vxl0c,iveo5l,"pretty clear from these responses that there is no reason to go to nosql, but i am wondering how you have made reporting efficeint. basically we want to build easy client reporting solutiuons etc, but just using data as is makes reports take long etc. Wondering how you make your reporting easy",1.0
g5w623l,iveo5l,"You generally will want an intermediate layer for your reporting, rather than querying your production data. Depending on your stack, you may have a built in solution (SSAS/PowerBI if MSSQL), or you may need to rely on an external solution such as one of the BI platforms I listed above.

In our case, we create multiple reporting aggregation tables which are read by Tableau, or custom in-house tools. We also maintain a distinct and separate analytical cluster for our more advanced analytical teams, which contains a time-delayed mirror of our production data.",1.0
g5re9z7,iveo5l,"Shopping data is kind of the quintessential structured data. It is inherently relational (I.e products,customers, orders). 

Ask yourself this: do I need to update my data frequently? Do these updates need to be real time and consistent constantly? If the answer to these questions is yes I don’t think NoSQL is what you’re looking for.

Your post describes a need for performant aggregation. While NoSQL can deliver this so can traditional RBMS. As you say you’re moving into the cloud, I would encourage you to look into data warehousing and the ease of auto scaling/partitioning your data which can get you excellent reporting performance while preserving vital ACID properties of the type of data you’re managing

Edit:
Just to be clear, I’m not a NoSQL hater by any means. I have seen magic worked with elastic search. But I do think transactional sales data is much more naturally suited to relational models in most scenarios",5.0
g5ut7u2,iveo5l,"Great answer, from what I understand of the question posted, the data is perfectly suited for a relational SQL model, maybe the cost of MSSQL is a deterrent?",2.0
g5vnzen,iveo5l,"If MSSAL wasn’t meant to be MSSQL I’m afraid I’m not familiar with the acronym. That said, being a MS guy myself I’m not in a position to argue that full enterprise setups don’t become costly. Anecdotally, I will offer that I was recently part of a company team that was given an implementation pitch from the folks at mongo and suffice to say we were a bit staggered by the cost estimates. 

The fact of the matter is that once you’re at scale implementing an elite database is going to be expensive. In fact, I might argue that this is particularly true for NoSQL DBs as it is not easy to tune your indexes/data structures once they’re established especially compared to their main benefit which is to just scale horizontally (I.e. just add $ome more node$ Haha).  If cost is a major constraint free options like Postgres offer pretty amazing hybrid options. I believe up until like...2015 (don’t quote me on it) pg benchmarked faster json read/write performance than Mongo (full disclosure, if I recall correctly these were run on single node implementations).

To bring it back to OPs topic, if I were to offer a concise piece of advice it would be to research and select an ideal data model and then pick a platform that naturally supports it.

Edit: [Here’s ](https://portavita.github.io/2018-10-31-blog_A_JSON_use_case_comparison_between_PostgreSQL_and_MongoDB/) one company’s performance test for some pg VS. Mongo. Now, for the people working to parse TBs of data in subsecond times, the differences are clearly significant. But for people who are willing to wait a literal second and save 100% on DB software licensing, pg performance is super impressive. At least to me",2.0
g5xmk61,iveo5l,"Sorry that was a typo, it was indeed MSSQL. Great advise here!",1.0
g5rb452,iveo5l,"How is your data structured currently?  Is it a single table?  It kinda sounds like you're storing unstructured data in a sql database.

It sounds more like you need to get your data normalized.  One user one row.  One flight one row.  One booked seat one row.  Three tables.  (To over simplify it a bit.)

The structure of sql is it's real power, and unless you're working at massive scales nosql might actually be a step backwards.",4.0
g5rc5t1,iveo5l,"Its all structured and works fine for sql...we just want to do an efficient could etl solution to move from ms sql server and make the data easy to analyze...ie easy to look up by flight or by date etc. 

Something better than just moving the already aggregated data to the cloud  as is",1.0
g5rnpb9,iveo5l,"Gotcha.

Disclaimer, I'm a sql guy all the way through and I'm in the process of (successfully) tuning a lot of ugly reports.  I've leaned a lot about sql in this process and know it quite well.

I guess the real question is, then, how is the current solution failing you now? Or how do you predict it to fail?

For context, I have SQL tables in excess of a million rows at 75GB total (just the one table, about half the total physical size) with all kinds of constraints and keys, and I consistently get reports down into the low seconds per month of data returned (on the worst reports).

It is my (limited) understanding that nosql loses many of the techniques I've used to pull these rabbits out of my hat.",3.0
g5qv3jo,iveo5l," noSQL  allows for high-performance, agile processing of information at massive scale. It stores unstructured **data** across multiple processing nodes, as well as across multiple servers. As such, the **NoSQL** distributed database infrastructure has been the solution of choice for some of the largest **data** warehouse. 

I would say it really depends on how much data there is, as well as what type of data there is. Based on my assumptions it seems like just a SQL DB should be fine, but depending on your needs (performance, data storage, data size) you may want to go the noSQL route, just my 2  cents.",3.0
g5rejwi,iveo5l,Sounds like a SSAS dimensional model should handle the job?,1.0
g5qz0c0,ive1vh,Try ‘and not (column1 is null and column2 is null)’ this will remove where both are null but keep the ones where you still have a value in one of the columns. Not to sure on oracle syntax but should work.,3.0
g5r0vdv,ive1vh,Ill give this a try thank you,1.0
g5r224d,ive1vh,Alternatively and (column1 is not null or column2 is not null) should do the same depends on your preference,1.0
g5rv24t,ive1vh,"Shouldn't it be OR if you want to select records where either column (or both) has a value?

    and ( o2.order_id is not null 
              OR ce.event_id is not null)

If you use AND then it requires both columns to have a value/not null.",2.0
g5rvlio,ive1vh,It really is that simple I think. Its been a long week and an end of the day friday issue. I am 90% sure this will work ill try it in the morning. It has been a long week. Thanks for the input.,2.0
g5qpu86,ive1vh,"&gt; I want to only omit when both are null

that's the opposite of what you coded

to return only rows where both are null, you want this --

    AND o2.order_id IS NULL
    AND ce.event_id IS NULL",-1.0
g5qqlvb,ive1vh,If order_is is null and event_id is null I want that row removed from my result. Your coding is saying if they are null include them. Unless my brain has jusy fried over today.,2.0
g5qronb,ive1vh,"Put NOT in front of the nulls, to exclude the nulls.",0.0
g5qs13l,ive1vh,That is what I did the problem is if only one of the two columns is null the row is omitted. I only want to omit if both order and event are null.,1.0
g5qsuho,ive1vh,"If you take away your other criteria, so it becomes: 

where (o2.order\_id is NOT NULL and ce.event\_id is NOT NULL) 

What happens?",1.0
g5qti8w,ive1vh,Same result its excluding my rows were order_id is null but event_id is not,1.0
g5qw4yo,ive1vh,Is ce the correct table that you want to exclude?,1.0
g5qwfxi,ive1vh,The table names/alias are correct,1.0
g5qrzg6,ive1vh,"oh my

i misunderstood

okay, what you want is 

    AND o2.order_id IS NOT NULL
    AND ce.event_id IS NOT NULL",0.0
g5qsh7l,ive1vh,This is what I am using as stated in the OP.,1.0
g5quuk1,ive1vh,"that's weird

that's definitely what you want

are you sure they're NULL and not something else like a zero-length string or a blank?

what are the other conditions in your WHERE clause?",2.0
g5qw803,ive1vh,The query is quite lengthy and the database is massive (Cerner EHR for a large hospital network). I have added additional qualifiers so I am looking at one specific patient to test with. The patient in question has no order id but has an event id. If I remove everything from the where expect the line in question i get the same result. If I just use where o2.order_id is not null my patient shows. The problem is that it is omitting patients where just one of the two is not null. If I remove the line entirely it works but then my result set is way larger containing patients who have no order_id or event_id and when we are looking at hundreds of thousands of persons this is making a result set of 4k become a result set of 15k.,1.0
g5qxely,ivdzyg,"You can give your SUM() column an alias. You just can't have a period in the alias unless you put quotes around it.

    SELECT SUM(col4) AS name -- This is fine
    SELECT SUM(col4) AS ""col4.name"" -- This is also fine
    SELECT SUM(col4) AS col4.name -- This should give an error",3.0
g5qph6c,ivdzyg,"&gt; Is this statement true?

yes, but there are very specific conditions it's talking about

in your case, this --

    SELECT col1, col2, col3, SUM(col4)
    GROUP 
        BY col1, col2, col3

is perfectly fine and should not generate that particular error",1.0
g5r1n8v,ivbvk2,https://www.acxiom.com/blog/the-magic-of-segmenting-audiences-for-success/,5.0
g5rgxaw,ivbvk2,[removed],1.0
g5rk9ai,ivbvk2,"I think if you post the problem/question to this sub, people will respond.",1.0
g5q1a7j,iv5p0e,"First of all this is a bad idea. Figure out which is the base table and use JOIN to join the other tables to the base or to the proper table and put the joined fields in the JOIN clause and not the WHERE clause. 

The only time I use this is when I need a multiplicative or cartesian join.

 I had to take apart a query that used over ten tables listed like this with all of the join logic in the WHERE clause. Just doing that improved query performance.",1.0
g5sn9sx,iv5p0e,"&gt; Just doing that improved query performance.

No modern optimizer treats implicit joins (in the WHERE clause) differently than explicit JOIN operators. 

I would assume the performance improvement was due to the fact that there was a join clause missing in the version with implicit joins and you had to add it when using the JOIN operator. 

-----

&gt; Figure out which is the base table and use JOIN to join the other tables

For inner joins the order does not matter. Neither for performance nor for correctness. `from a join b on ..` will return exactly the same result as `from b join a on ...`. So it's not really necessary to figure out a ""base table"". But I do find it helpful when reading a SQL query if the ""main entity"" (or most important one) is listed in the FROM clause.

-----

&gt; The only time I use this is when I need a multiplicative or cartesian join.

I prefer a CROSS JOIN for that, so that it's clear that the cartesian join is intended. When just using `from a,b where &lt;some unrelated condition&gt;` it might simply mean the join condition was forgotten.",2.0
g5u9c9g,iv5p0e,"It shouldn't matter but it did improve performance significantly. The RDBMS is Oracle. This is my experience and YMMV. There were no missing joins if that is what you were implying. Moving from the where clause to the join did not change the result.

I personally find it very hard to follow when all the joins are in the where clause.

As you say there is usually a main table that everything else bases on and from a human readable and understandable POV this makes the most sense. As you note technically it does not matter. However when you have to support  a lot of queries it makes sense to follow some sort of style and having the primary table listed first after the FROM clause can give a big head start. Even if everything is an inner join after that.

I've never used the CROSS JOIN syntax. Personally I just forgot about it. But for reusable code that is obvious about the intent then I would also recommend using this clause as otherwise it just looks like a mistake when you come back to it months later and wonder where the JOIN clause got to. I will start using that in my code when I need a cross join in future.",1.0
g5prs3d,iv5p0e,"&gt; In this case is there a certain sequence i need to follow in where conditions ?

no

see replies under ""comment deleted by user""",0.0
g5paygv,iv5p0e,[deleted],-1.0
g5pbjfb,iv5p0e,Is there some logic behind the positioning of the tables too ?,1.0
g5pcjb4,iv5p0e,[deleted],0.0
g5phxcw,iv5p0e,"&gt; its best to just join tables depending on the number of columnar data you will be accessing

nice idea, but the optimizer will perform the joins in the order which it thinks is most efficient, regardless of the order in which you write them

in particular, it will usually start with the table that will return the fewest number of **rows** (not columns) and then join other tables after that

that's why some people say you should write your joins in that order if you can, because that makes it easier for others to read and understand

thus, this is misleading --

    SELECT ...
      FROM employees
    INNER 
      JOIN employee_projects
        ON employee_projects.empl_id = employees.id
    INNER
      JOIN projects
        ON projects.id = employee_projects.proj_id
     WHERE projects.name = 'Geronimo'

the optimizer will begin its execution by accessing the `projects` table first, because then there's only one row to be joined to the other tables

you might as well write it that way too --  

    SELECT ...
      FROM projects
    INNER 
      JOIN employee_projects
        ON employee_projects.proj_id = projects.id
    INNER
      JOIN employees
        ON employees.id = employee_projects.empl_id
     WHERE projects.name = 'Geronimo'",2.0
g5pct3o,iv5p0e,"Thanks for the information ! That was helpful.
I did have some trouble in a bigger query but now i am begging to think it wasn't related to this part",1.0
g5ph0a6,iv5p0e,"&gt; The complier will execute line per line top to bottom

not true

the order of execution is FROM, WHERE, GROUP BY, HAVING, SELECT, ORDER BY",1.0
g5phr8u,iv5p0e,"this is actually not true - most sql implementations have ""optimizers"" that will re-arrange the way the data is accessed and produce ""execution plan"" of the query. the access pattern can be different from what was specified in the ""from"" clause",1.0
g5p78zy,iv5bpx,"SQLite is probably the easiest for beginners, as installation is incredibly simple

That being said, PostgreSQL is a joy to work with. It mostly does what you expect, and the syntax is closest with the SQL standard

https://postgresapp.com/ makes it easier to run PG on Mac

Or you could try PostgreSQL on the cloud (such as AWS RDS) or in a playground service like https://www.db-fiddle.com/",17.0
g5pbr23,iv5bpx,"&gt; That being said, PostgreSQL is a joy to work with

This.",11.0
g5qo2h7,iv5bpx,"And really if your problem is struggling with installation, you probably won't be better off with SQLite long term vs just getting installation help on pg, MySQL, or ms sql.",1.0
g5pbxz8,iv5bpx,"&gt; Is PostgreSQL good for beginners? Or should I pick another one?

PostgreSQL (or just Postgres) is a good choice for beginners as it is probably the DBMS that is closest to the SQL standard. And it will grow with you as you discover more and more of its features. 

It is almost never the wrong choice (if you want a relational database server).",9.0
g5pw4uq,iv5bpx,"In your experience, do a lot of companies use PostgreSQL?   Or rather, what would look better on a resume/portfolio for job seekers?",2.0
g5q2yxa,iv5bpx,"It's widely used, maybe falling behind Oracle on largish databases. Of course this varies by field but I like the standards following approach and extensive features they have. Smaller services might use MySQL(Mariadb) and Windows ecosystems tend to pick the MS product.",2.0
g5srxb2,iv5bpx,For the UK government it's part of the default web stack across lots of departments. This ranges from small apps with thousands of records up to big ones with tens or hundreds of millions.,2.0
g5q660z,iv5bpx,"I think it depends on where you are and the industry you are working for. About 50% of the projects I work on use Oracle, 40% use Postgres and about 10% use SQL Server (luckily I don't have to deal with MySQL).

Postgres has also gotten a lot more traction in the last years. I see a lot more questions around migration from database X to Postgres than I did say 5 years ago.",1.0
g5qqurt,iv5bpx,"I'm currently building a database of baseball statistics using Postgres, it is a joy to work with. I would highly recommend, plus syntax is very similar to what you would see elsewhere.",4.0
g5si5bi,iv5bpx,"Fire up Python and Postgres, together they are a delight. Especially in the field you are planning to get into.",3.0
g5rgzby,iv5bpx,[removed],1.0
g5rjzmz,iv5bpx,"&gt; e please help with an sql school project my kid is unable to understand certain questions and i am not able to help him out

You should start your own thread with the question.",1.0
g5rk5b0,iv5bpx,I did but i was asked to to this because i can get more ppl to see about this btw can you please help? Are you familiar with something that's taught in university?,1.0
g5rku3b,iv5bpx,I dont know who told you to spam all the threads in /r/SQL and /r/learnSQL but please just create a thread in /r/SQL  which the image and question you provided in an earlier comment. The community will help if it can.,2.0
g5ozom0,iv4lrd,"Check out this video.  We created an open source library called JsonAutoService.  Makes it easy to build APIs 

https://youtu.be/FTn0IcZi_70

And here’s another one that explains the foundations and methodologies of how it works

JsonAutoService: A New Approach to Data Access https://youtu.be/Px-_i61fOnE",1.0
g5p7mu6,iv4lrd,"Hmm been looking for something like this, I'll scope this out - thanks",1.0
g5pvpbw,iv4lrd,"SSMS is only a client application for SQL Server. You say you want ""an API to go into SSMS"" - what do you mean by this? Are you fetching data via this API that you need to insert into a SQL Server database?

Do not access your REST API directly from SQL Server. You need an application/service that hits that API, then does whatever needs to be done to get the data into SQL Server. And that application should be running on another machine. Using SQL Server itself to do this is _technically_ possible, but it's a waste of extremely expensive resources, potentially a security problem, and not readily portable between environments (on-prem/IaaS/Managed Instance/Azure SQL DB).",1.0
g5q2hkk,iv4lrd,"Yeah, fetch data then use SSIS to take it into SQL Server database",1.0
g5qbqkd,iv4lrd,The only way (I know of) you're going to be able to interact with a REST API via *SSMS* is via the [CLR](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/introduction-to-sql-server-clr-integration).,1.0
g5t2wob,iv4lrd,"There are 3rd-party SSIS toolkits that will do it. CLR is almost never the appropriate answer.

And so we're all clear here, SSMS is not going to be doing this access. As I said above, it's a _client application_. All the work is happening on the server.",2.0
g5t33qq,iv4lrd,"A quick search for ""REST API from SSIS"" turns up this: https://www.c-sharpcorner.com/article/how-to-consume-web-api-through-ssis-package/",1.0
g5qbzcl,iv4lrd,"I think your options are pretty much:

* Hacking together some C#
* Using SSIS extensions, such as (not vouching for it, just Googled it) [this](https://marketplace.visualstudio.com/items?itemName=ZappySys.SSISRESTAPIWebServiceTask)
* Using SQL Server CLR

The first is probably the simplest, .NET supports REST interactions nicely and there are likely loads of blogs/code samples available online.",1.0
g68p39r,iv4lrd,"SSIS can do this natively with c# or vb, or grab a 3rd party component like JSONSource

The c# route sounds hard but you can find the code easily enough online and stuff it in a script component.",1.0
g5sk2b4,iv4lrd,You can call an api from SSMS and parse the results into a table,0.0
g5sk2qf,iv4lrd,"*You can call an api*

*From SSMS and parse the results*

*Into a table*

\- lookslikeanevo

---

^(I detect haikus. And sometimes, successfully.) ^[Learn&amp;#32;more&amp;#32;about&amp;#32;me.](https://www.reddit.com/r/haikusbot/)

^(Opt out of replies: ""haikusbot opt out"" | Delete my comment: ""haikusbot delete"")",1.0
g5qymsc,iv4l76,Some versions of MS Visio can do this.,1.0
g5qj6l6,iv3fv8,Just deserialize it with any programming language and write insert statements,6.0
g5qpl4x,iv3fv8,This. Batch your inserts.,1.0
g5owab2,iv3fv8,Isn't a .JL file Julia script? What does your JL file look like?,4.0
g5py93j,iv3fv8,No. it is a kind of json file delimited by newline,1.0
g5pyy1d,iv3fv8,"Probably isn’t the solution but it looks like the appropriate extension is .jsonl

https://jsonlines.org/",2.0
g5q123g,iv3fv8,How about parsing this to sql insert script and running just that? If the json is complicated or involves multiple tables then it might be a bit too much.,1.0
g5q9qxa,iv3fv8,"Not sure if you are familiar/can code with R, but you can use the jsonlite package to read the file and parse it into tabular form. From there you can use create and odbc connection through rstudio, and insert into the database with the DBI package.",1.0
g5oy5ud,iv168w,Are you using parameters for your INSERT statement?,1.0
g5p4yx3,iv168w,Yes. 1.5 million records in the first run then few records every week,1.0
g5phhpe,iv168w,"The number of records has nothing to do with whether you're using parameters.

If you're doing something like this:

    var1 = 5
    var2 = "";""
    var3 = ""C""
    cursor.execute(""INSERT INTO table VALUES (%s, %s, %s)"", (var1, var2, var3))
    
then you're making one big string and passing that to the database (""INSERT INTO table VALUES (5, ;, C)"") which will break since there's a semicolon in the middle, ending the statement. Doing it this way makes your program vulnerable to SQL injection and, as you've found, the values can break the SQL statement.

You want something like:

    cursor.execute(""INSERT INTO table VALUES (?, ?, ?)"", (var1, var2, var3))

In this second form, the string just has a literal question mark, and the variables are passed in separately as parameters.  Values with semicolons, line breaks, or strange Unicode will not break the SQL statement.",2.0
g5ormnw,iv12qt,"just call them indexes, and call unique indexes unique indexes",5.0
g5r4bay,iv12qt,"Ok thanks for the feedback.

But it's a little bit like...

* Question: Name any fruit that isn't an apple
* Answer: An apple!
 

:D",1.0
g68p7j6,iv12qt,a Napple?,1.0
g5oqtj1,iv12qt,"After spending about 30 minutes checking several synonym &amp; antonym sites, I think that the best (unofficial, mind you) terminology would be either:

    Public-Index

or

    Unbound-Index

or

    Boundless-Index

or

    Generic-Index

or

    Ordinary-Index

This is because, when you distill specific use of the antonym of unique into its broader concept, we are searching for a word that does not necessarily describe the token characteristic of exclusivity, so much as it pertains to its underlying state: restricted.

Thus, an index that is unrestricted (too verbose for the frequency of use that it would be likely to see) could be considered public, unbound, boundless, generic, ordinary, etc.

Ultimately, there are a nearly limitless number of options available (wordhippo.com and onelook.com are fantastic resources).

Personally, I will be going with:

    Nonique-Index

Partly because I just made it up, but also: 
It describes the term perfectly, has the same number of syllables, is quick to say and above all: it’s fun to say!",2.0
g5ou0i3,iv12qt,"These are some interesting options, thanks!

&gt; Nonique

I like this one too! I actually do a lot of naming like this where I come up with unique word by mashing 2x real words together and removing some letters from the middle.  It makes it very easy to search your whole codebase(s)/filesystems/whatever without getting irrelevant results or needing to think about any punctuation/special-chars.",2.0
g5q7xuq,iv12qt,I have never heard any of those terms. _If_ people want to distinguish they usually simply talk about unique indexes and non-unique indexes.,1.0
g5qk9sd,iv12qt,"Yes, like I mentioned these are unofficial terms that I feel are contextually appropriate substitutions for non-unique. This was more or less a way to burn 30 minutes while I waited for my insomnia to finally give in to sleep :)",1.0
g5r1tab,iv12qt,"ERwin used to call them ""[inversion entries](https://erwin.com/bookshelf/9.7.00/Bookshelf_Files/HTML/erwin%20Overview/index.htm?toc.htm?3709.html)"", but never explained why. I think ""unique index"" and ""index"" are unambiguous enough and fine.",1.0
g5r45pg,iv12qt,"Interesting, haven't heard of that one!",1.0
g5o36km,iuxsul,"I would probably not use Access to learn SQL in the first instance.

Access doesn’t use MS SQL - it uses a cut-down version called Jet SQL. It’s got some pretty annoying limitations and if you learn on it, it will teach you some pretty bad habits that you’ll have to unlearn if you move to another SQL environment.

I would download the free version of SQL Server, and learn on that. The concepts will be easily transferred to Access.",26.0
g5oc94t,iuxsul,"Absolutely agree on this. Start with SQL Server, or even PostGreSQL as a matter of fact.

Ms SQL has a lot more builtin functions, but either of the above should work.",8.0
g5oufqb,iuxsul,Agree 100%.  Download MS SQL Server [here](https://www.microsoft.com/en-us/evalcenter/evaluate-sql-server-2019?filetype=EXE),4.0
g5p2rhc,iuxsul,"I had to help someone with an access database, and I wanted to throw it out a window. I told them I could have setup sql express and sql reporting imported the data and made the report in a couple hours. While they told me they spent couple weeks trying to figure out access",3.0
g5nwkg6,iuxsul,"This guy has a good series of videos and is where i learned how to use forms. He gets into SQL, too. 

You can actualky learn quiete a bit from free Microsoft tutorials. Search the Microsoft sure",3.0
g5nzcir,iuxsul,"https://www.youtube.com/playlist?list=PLYMOUCVo86jEeMMdaaq03jQ_t9nFV737s

I forgot the link!",5.0
g5nzbg8,iuxsul,Which guy? I didn't see a link come through. Thanks.,1.0
g5o16tc,iuxsul,I find this useful. [w3schools](https://www.w3schools.com/sql/default.asp),3.0
g5o0nl1,iuxsul,I learned sql starting with MS Access UI. I would recommend learning basic SQL query before access. I think it will help understand linking data tables and reports..etc,2.0
g5o3moe,iuxsul,"Is basic SQL query what others have referred to as ""true SQL""? What would be a good starting point for learning basic SQL query, as someone who is not a programmer but works with programmers and has seen and worked with code, but not actually written any (if that makes any sense at all)?",1.0
g5o3s40,iuxsul,"This should get you some experience before jumping into access. 

https://youtu.be/7S_tz1z_5bA",1.0
g5o6cdk,iuxsul,Thank you!,1.0
g5nvy75,iuxsul,"Hello u/ragmats - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g5nz7s1,iuxsul,MS SQL,1.0
g5o1kj2,iuxsul,"The problem with using SQL in Access is that it's not a true MS SQL, but a bastardized SQL and VB. The concepts of tables and joins are universal be it in Oracle, MySQL, or MS SQL, so there is that benefit. The drawback is if you want to be able to know a more universal syntax than having Access as your learning base will be frustrating down the road. You'd be better off learning a ""True"" SQL and then learning to make it work in Access than the other way around, especially because in the vast majority of data roles no one uses a program with such severe limitations.",1.0
g5o3et6,iuxsul,"In that case, same question for a starting point for learning a true SQL. Got any recommendations?",1.0
g5o53qo,iuxsul,"There's a million books and websites that will get you started with basic concepts and syntax. Just by googling ""Learn SQL""  you can't go wrong. One big caveat to that is that it's really hard to retain and advance. If that's as far as you go it's really hard to truly learn SQL.

The best way to learn is just like with any other skill, by doing. It helps a lot if you can be forced to use it every day for work, but you can still do this for free if you want. Download SSMS 2017 or whatever free version along with SQL Express. This will put a free database server that works exactly the same (albeit with some RAM and storage limitations) as a professional grade MS SQL server. last time I checked though even the free express version let's you build databases 5x bigger than Access and will crush queries that took hours to run as well. 

Once you have your software downloaded take the time to researc how to set a local sever up and build a database and table within it. It's a pain, but it's a growing pain that will give you knowledge of how all these things work together. These are the things that simple online courses will never cover when they dump you in a text editor to write a query.

From there find a passion project. Track a sport, some stocks, anything that with regular data that interests you so you feel invested. Spend time finding out the process of importing data, incorporating it into your table structures, and seeing the updates in reports you build. Spin up a local SSRS, which is free with SQL Express, and build some more physical reports in it that update you on the information you're tracking. As you ask more questions of your data you'll confront new questions on how to write queries and find yourself approaching problems in new ways. It's all downhill from there.",1.0
g5o6ko7,iuxsul,"Thanks, this is great advice.",1.0
g5o66j0,iuxsul,youtube,1.0
g5p0qir,iuxsul,"I generally agree with the other comments regarding Access as a poor starting place to learn SQL.  On the other hand, if that's what you're seeing the most, it's a lot to learn a different database and try to play constant tranlation.

*SQL Queries for Mere Mortals* might be a good resource for your situation. The author teaches beginners using standard SQL (dialect neutral).  The book has small sample databases that you can download in multiple formats, including Access.

It covers the basics (clauses, join types, set operators, aggregation), but also includes quite a bit on subqueries. You won't be a SQL master or Access master, but you'll get a solid foundation.  When I was starting, I learned a lot from this book even though I used lowly Access.",1.0
g5p42z9,iuxsul,"I teach an intro to databases course at a college. For my first 5 years or so, I used access to teach. SQL was painful. 

I've since switched to using MySQL Community Server and MySQL Workbench. Much easier, better SQL, and great interface. Plus, MySQL has some great demo databases to use.",1.0
g5q0385,iuxsul,"Access is a unique tool. It combines a database as well as reporting and forms for application development. 

Access queries are visually designed but underneath the hood is Jet SQL. You can take a query designed using the visual design tool and have it show you the SQL code underneath. Jet SQL is a little quirky but for the simple stuff is the same as other SQL implementations.

Start with a query over just one table and do the view SQL. Then add some criteria and do the same. 

Access writes very conservative code and over uses parentheses IMHO. 

As others have noted, for pure SQL in the MS world, SQL Server is the tool to use.",1.0
g5qapt8,iuxsul,"IMHO, don't bother. Learning anything related to Access is not going to do much for your employability. Let whatever sorry individual who has to use Access in 2020 worry about it.",1.0
g5n48f6,iutibm,what are the chances that this is home/coursework?,3.0
g5nf3n8,iutibm,"it is a homework i did the relationships but i am not sure if its right.

here is my answer   

The main relationships are: 

· CLUB trains TEAM

One to many relationships because a CLUB trains MANY teams, but a team is trained by a single club. 

· PLAYER plays in a TEAM

Many to many relationship because one student can play for several teams. While a team has many players in it.

BRIDGE: LS\_ENROLLMENT: keeps the teams in which each student is enrolled.

· TEAM has player ENROLLMENT

· PLAYER has team ENROLLMENT

DIVISION has SEASON

Many to many relationship because a division have more than one season and a season have more than one division.",1.0
g5n8xwv,iut5ae,"On mobile so hard to see/test but think it’d be:


SELECT sc.CustomerID, pp.FirstName,pp.LastName,
STRING_AGG(CAST(ProductID as char),',') WITHIN GROUP (ORDER BY ProductID) AS ProductID
FROM Sales.Customer sc
JOIN Person.Person pp on pp.BusinessEntityID =sc.PersonID
JOIN Sales.SalesOrderHeader sh on sh.CustomerID = sc.CustomerID
JOIN Sales.SalesOrderDetail on sd.SalesOrderID=sh.SalesOrderID
GROUP BY sc.CustomerID, pp.FirstName,pp.LastName
ORDER BY sc.CustomerID",2.0
g5nigx0,iut5ae,"u/RedSeaPedestrian_RSP Thanks for this query! 

This line has some problem:

STRING\_AGG(CAST(ProductID AS nvarchar(max)), ', ') WITHIN GROUP (ORDER BY ProductID) AS Products

The query works great without the line **WITHIN GROUP (ORDER BY ProductID) AS Products** but the products are not sorted though I get a long list which is good.

Including this **WITHIN GROUP (ORDER BY ProductID) AS Products** is causing some syntax error which says Incorrect syntax near '('  though I checked more than 10 times and I referred Microsoft doc as well, same syntax is being used.

Example from Microsoft:  

SELECT TOP 10 City, STRING\_AGG(CONVERT(nvarchar(max), EmailAddress), ';') WITHIN GROUP (ORDER BY EmailAddress ASC) AS emails  FROM Person.BusinessEntityAddress AS BEA   INNER JOIN Person.Address AS A ON BEA.AddressID = A.AddressID INNER JOIN Person.EmailAddress AS EA ON BEA.BusinessEntityID = EA.BusinessEntityID  GROUP BY City; 

I do not see any '(' missing or any punctuation extra  in the code checked thoroughly comparing it to example from Microsoft.",2.0
g5nl5re,iut5ae,"Hmm, do you know what compatibility level the DB you’re running it on is? I think you might need 110 or higher

SELECT NAME, COMPATIBILITY_LEVEL FROM SYS.DATABASES",2.0
g5ob0he,iut5ae,Thank you! Using higher compatibility level database worked.,1.0
g5nofx3,iuryzs,"&gt;  Would modeling this data require a three-way junction table holding that information in addition to the others? Is that safe?

Yes. And it's perfectly safe, and when you have it you may not need your other junction tables.

You say effectiveness of a strategy in a biome varies based on the organism employing it. This can ONLY be described by a table with all three entities as primary key. 

The other junction tables (StrategyBiome, OrganismBiome, OrganismStrategy) can be jettisoned if they don't have any independent attributes -- if you have something like ""a dog always wears a collar in Biome ""Home"" no matter what strategy"", then you need a junction table, otherwise you don't.

If you do have any junction tables, you should have a foreign key from the triple table to the two-entity table, so that you never have any orphan rows.

[Here's an SQLFiddle](http://sqlfiddle.com/#!17/9fefa/9).

EDIT: Thanks for the gold! I think it's my first!",7.0
g5nomxd,iuryzs,Thank you,1.0
g5rhbff,iuryzs,[removed],1.0
g5rl2iu,iuryzs,"I wish I could, but my education is informal and piecemeal. I'd hate to pass on misinformation. If I were you, I'd post the questions you have with lots of detail to subs like r/learnprogramming or maybe stack overflow.",3.0
g5rlnc9,iuryzs,Thank you so much i will do that and i apologise for spamming,2.0
g5s6nwy,iuryzs,Not a problem at all! Respect to you for your efforts to help your son :),1.0
g5org3u,iuoynv,By “the SQL DB’s has been running in Demo mode” do you mean something application specific? There is no demo mode for SQL Server. Which users were lost? Application users that would be stored inside your DBs in your own tables or the logins used to connect to the actual Database? Data doesn’t just disappear when servers crash unless that data was only in memory - this would be incredibly unlikely. Did you accidentally overwrite the database files with something else (eg restore from an old backup) when the server recovered? Or perhaps the application restarted and thought it had to recreate all objects? Do you have database backups?,1.0
g5n917y,iunql4,"you could log ship to a stand alone instance from an AG,  but not to another AG.   That would essentially be a distributed AG,  which is enterprise only feature.",2.0
g5m0b0x,iunjpq,"It depends on the query, the database and the data.

I have written monstrously long and complex queries in minutes, and also taken weeks to perfect a shortish query to get around poor design and dirty data.

There is no answer to this question",10.0
g5o74w0,iunjpq,"Also let's not understate the time required to change a query that already does a thing to do a new slightly different thing instead.

Or figuring out why a query is doing the things it does (surprising or unintentional).

Maintenance often takes more time and is a larger component of the workload to writing entirely new queries.",2.0
g5nqyj0,iunjpq,This,1.0
g5m24l0,iunjpq,"what's a ""good query""? I guess if a query crashes the server it kinda makes it a bad one, but what makes a query ""good""?

What is that you were doing in these 10 hours? Did you need to talk to Adam Smith from Accounting for 3 hours to understand how billable entities are different from customers? Did you need to talk to Keesha Steelblz, the VP of Operations to grasp the workflow of some process? Or did you have to instant message Rajesh, who's working from Bangalore currently, 4 times to ask 'how do i join customers and orders tables'?",5.0
g5m7l4i,iunjpq,"Somewhere between 5 minutes and 2 weeks. It just depends on the query. And whether or not the database is busy.

""Getting faster"" is as much about knowing the data as it is knowing SQL. Both come with time.

If you're concerned about your performance, ask your manager or a teammate.",7.0
g5nmoy2,iunjpq,Anywhere between two minutes and two months in my experience,2.0
g5lrmzh,iunjpq,"Hello u/Front-Chocolate198 - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g5m0wth,iunjpq,"I just started working as a db analyst couple months ago, i had some prior sql knowledge. I wouldnt say that it has taken me 10 hours to write a query, but i had my share of problems when people keep changing the requirements etc. Also you might not have anyone actually telling exactly what you need to query. If you're not familiar with the tables, data in them and what exactly you're looking for i can see it taking 10 hours. Either way, don't sweat it, take it slow it's better that way :)",1.0
g5niht1,iunjpq,"Off topic. How did you find a job without 5+ years? I’ve been in my career as a IT/web dev liaison for 8 years, ended up doing SQL and python bootcamps for a year and I’m looking to find a db analyst role but everyone says I don’t have enough experience.",1.0
g5oq725,iunjpq,"Hi, well I'm working/living in Lithuania, job market here is quite nice and most people don't care about your ""experience"" on paper they care what you actually know/can do instead, at least this is my personal experience so far.",1.0
g5mtb9e,iunjpq,"As has been said, it depends on the query, the data, and what you're trying to accomplish.

Unless it takes you 15 seconds to type out the word SELECT, you stare blankly at the screen trying to remember what comes after SELECT, or you're googling the same thing over and over and over again because you can't keep it in your head, you're likely doing perfectly fine.

My first Window function took about an hour to set up (not counting time it took to figure out that a Window function was what I needed).  After that it took a few minutes and now it takes about as long as any other complex calculated output column (it is a fair bit of typing).

The key I'm trying to get at is that you should be getting faster as you go.  As you learn and get familiar with the data structure and the techniques to get the results you want, you will get faster.  As long as you're getting faster over time, you're fine.  People generally learn to crawl before they learn Parkour.

Now, if you're sitting there staring at the screen waiting for the query results to come back, maybe look a little bit into what's making the query slow, and ways you can make it faster (even if it's just limiting your result sizes while you're building).",1.0
g5nzbxc,iunjpq,"I wrote a pretty big one today that is good imo that took about an hour. Multiple ctes, lots of joins. Still have some things I would like to do with it, but it's still fantastic seeing as I'm hitting like 15 tables.

I did have to do a lot of database crawling trying to find the links between certain tables. Usually don't have to do this but one set of tables use another table to link them all.",1.0
g5lx91v,iunjpq,"It really depends on the query, but I’ve written some very complex stored procedures and never has one taken me 10 hours. 

But truly, it depends on how complex they are and how poorly the data is structured.",1.0
g5m12hw,iumw0q,"Free MS SQL has most of the tools from the paid version these days. It has memory and space limitations though.

How many MySql tools and IDEs did you investigate? Have you tried postgres?

Excel will read from pretty much any DB using PowerQuery. Loading FROM Excel is a pain in most db, there is moderately better support with the Access engine driver but that in turn can be loaded by any db that will talk to ODBC. There is always CSV of course as a go between",2.0
g5q5duw,iumw0q,"I've just been on workbench and my knowledge is limited beyond that. If seems like it may be over kill though. 

Do you suggest moving my data into access, cleaning it there, and exporting from access into mysql work bench?",2.0
g5q7j92,iumw0q,"No, sorry

To clarify, SSIS Excel loading relies on  the Access engine runtime components. They function as data access drivers that let you write import/ export packages directly from/to excel sheets.",1.0
g5lrhs6,iumw0q,"I’m in a similar position as you. I’ve taken to using MS SQL Server and loading in data from excel in tab delimited text format with BULK INSERT. I think there are easier ways if you’re using csv files but doesn’t work so well if your data has commas in it as mine usually does. But for the 100k-1M row files I’m using, bulk insert takes care of this in 3ish seconds",1.0
g5mhat0,iumw0q,If you already have the data I wouldn’t worry about doing it in excel. Loading from excel to sql is a massive pain. What are you doing now that you think would be better in sql?,1.0
g5q52qv,iumw0q,"2 main reasons. 1 is to continue to learn SQL and unlock it's full capabilities. 2 is I like being able to use SQL to ""declare"" specific queries without messing with pivot tables, vlookups, filters, etc. 

I like just commanding to view data a specific way...even if the file sizes I'm using work fine in excel",2.0
g5ng8pk,iumnrq,"    SELECT Columns 
    
    FROM
    (SELECT Columns, RANK OVER(PARTITION BY Date_Val ORDER BY Date_Val ASC, Created_Date ASC) AS Rnk FROM TABLES)
    
    WHERE Rnk = 1

Selects the oldest Date\_Val record, and then if you find matches, ranks them by whatever your Created\_Date was, generates 1 instance. You can add a Row\_number() thing too if you possibly have multiple things with the same date\_val AND the same Created\_Date.  It would just be:  


&gt;Rnk=1 AND Rn = 1",3.0
g5nhx47,iumnrq,"I'll give this a try, thanks.",1.0
g5lvosi,iumnrq,"""the item""? use a row_number() (... as ""RN"") to give sequential number to your records in whatever order is needed, then pick records with the RN= 1.",2.0
g5mbjx1,iumnrq,"I've tried this previously but it doesn't correctly solve the issue.

The underlying issue with my data is, the oldest item returned by MIN(date\_value) may *not* also be the record with the oldest created date.   So I need to be able to look at the MIN(date\_value) result set and then within that, identify the record with the oldest created date.",1.0
g5mc9oi,iumnrq,"&gt; I've tried this previously but it doesn't correctly solve the issue.

maybe you're doing something wrong?

&gt; So I need to be able to look at the MIN(date_value) result set and then within that, identify the record with the oldest created date.

you should order by both date_value and created_date in your row_number expression.",1.0
g5mf393,iumnrq,"Yes it's entirely possible that I'm doing something wrong, I'm definitely not a SQL expert.

I will try the row number and sort fix and see if that solves my issue.",1.0
g5mn6fh,iumnrq,"Otherwise, post a little sample set and give us the output you want and I’m sure we’ll be able to figure it out.",1.0
g5mpojo,iumnrq,Thanks but at this point I'll just get as close as I can to the correct result and then just fix the incorrect data manually in Excel.,0.0
g5lp9ov,iultse,"SSIS Data Profiling task and the Data profile viewer.

Probably just the null value test but could also do a histogram while you're  at it",3.0
g5loryv,iultse,"What are you counting as not populated, is it blank or null? 

Off the top of my head, it sounds like you're going to need to loop through every table/column combination, count the blanks/nulls and then the total rows and do the math. 

I'm VERY curious to see if anyone else has a shortcut for this.",2.0
g5mjpvw,iultse,"I don't really consider either populated, but I can change the empty stings to NULLs prior to anything else, so that shouldn't be a dealbreaker.   


There have been a couple responses so I'm working my way through those. I will be a super happy camper if I figure this out--the destination database for target mapping won't change (at least not regularly) so that piece isn't a huge deal (currently handling that portion mostly in Excel) but getting the percentage for each row/column combination is critical.",1.0
g5mb7ow,iultse,"    DECLARE @table_name varchar(100) = ''
	    ,@sql varchar(max)

    SELECT @sql = STUFF((
    SELECT (',' + 'CAST(COUNT('+column_name+') AS FLOAT) / COUNT(*) AS ' + column_name) 
    FROM INFORMATION_SCHEMA.COLUMNS m
    WHERE TABLE_NAME = @table_name
    ORDER BY ORDINAL_POSITION
    FOR XML PATH('')
    ),1,1,'SELECT ') + ' FROM ' + @table_name

    exec (@sql)

There you go.

Just choose a table you want to do this for. 

It will treat empty string data as actual data. You would need to build in a case statement to handle that if you needed but that should get you going.

The FOR XML PATH solution is provided since work is only on SQL Server 2016. STRING_AGG is unavailable until 2017",2.0
g5mgmvg,iultse,"Thanks! I appreciate it. This might be (probably is) a dumb question, but where is the output for this going? I tested it with a table and it runs successfully, but I'm not seeing any output, just notification that the command ran successfully. 

Also thanks for the note on the NULL vs empty string, I can convert the empty strings to NULLs prior to running this if needed so that isn't a problem.",1.0
g5mls9f,iultse,"It’s stored in the @sql variable, do a SELECT @sql after the exec to see the result query.",1.0
g5mq042,iultse,"Got it, thanks. I was trying that, but not running it as part of the rest of the statement so it was failing. Appreciate it!",1.0
g5mqfpk,iultse,"Thanks again. This does give me the information I want, but it is formatted with the columns as the column headers and the ratio as the row. I would like that number combined with 

`SELECT TABLE_NAME, COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS`

basically adding the result as a third column to that query. Would it be possible to modify this to accomplish that?",1.0
g5mva1c,iultse,"    DECLARE @table_name varchar(100) = ''
    	,@sql varchar(max)

    SELECT @sql = 'SELECT ''' + @table_name + ''' TableName, ColumnName, Ratio FROM (' + STUFF((
        SELECT (',' + 'CAST(COUNT('+column_name+') AS FLOAT) / COUNT(*) AS ' + column_name) 
        FROM INFORMATION_SCHEMA.COLUMNS m
        WHERE TABLE_NAME = @table_name
        ORDER BY ORDINAL_POSITION
        FOR XML PATH('')
        ),1,1,'SELECT ') + ' FROM ' + @table_name
        + ') p '
        + 'UNPIVOT (Ratio FOR ColumnName IN ( ' + STUFF((
        SELECT (',' + column_name)
        FROM INFORMATION_SCHEMA.COLUMNS m
        WHERE TABLE_NAME = @table_name
        ORDER BY ORDINAL_POSITION
        FOR XML PATH('')
        ),1,1,'') + '))  u;'

    select @sql
    exec (@sql)",2.0
g5mvfwi,iultse,"It ain't pretty or easily maintainable but that's what you asked for. 

If I had sql server 2017, it'll be much less work to clean up but enjoy.",1.0
g6gdvw3,iultse,"I'm super late obviously, but I really appreciate this. I was able to modify it to account for divided by zero errors I was getting on empty tables and also get it to run on all tables. Exactly what I needed and can't thank you enough!",1.0
g5lqf32,iultse,"Steps:

1. Get list of all columns for the table you want
2. Convert column list into single string delimited by a comma using STRING_AGG or another similar method to look like this:
    &lt;col1&gt;,&lt;col2&gt;,&lt;col3&gt;,...&lt;coln&gt; 

3. Create dynamic sql to look like the following:

    SELECT COUNT(&lt;col1&gt;) AS FLOAT / COUNT(*), ... FROM &lt;table&gt;

To do the above you'll need to do a string replace on your column list.

    REPLACE(@col_list,',',') AS FLOAT / COUNT(*), COUNT(')

Then add 'SELECT COUNT(' to the start of that string and then add 
' FROM  &lt;table&gt;' to the end

Then execute that Dynamic SQL. I think that pretty much does it.",1.0
g5nrk2e,iultse,"I guess my question is: why? What are you hoping to learn?

This doesn't seem like particularly useful information. Sure, I can see wanting to know how many NULL, missing, or default values are present for one or two tables. But in that case you should be able to just run a count of every record. 

But every table in a database? Are your redesigning the entire system?",1.0
g6gedpt,iultse,"Sorry for the delay. So I am migrating several databases from a legacy system to a new system. This serves to give me a clean list of tables that are populated, and within those, which columns are populated and which aren't. I'm also combing this with field validation in Excel to essentially create a map of X goes to Y, where Y can be any user selected table/field in the destination database.",1.0
g5m1dgd,iultse,Try using SQL “window functions”.,-1.0
g5m5eoa,iultse,How would that help?,1.0
g5n8104,iultse,"SQL Server Window Functions calculate an aggregate value based on a group of rows and return multiple rows for each group.

Here is an example:

https://jimsalasek.com/2020/03/05/calculate-percentage-of-total/",1.0
g5n90iu,iultse,"I was just wondering how they would be useful in the context of OP's question. Window functions are good for... windowing data. But OP needs a single result for each attribute of a table. 

I posted my solution elsewhere in the thread. Just curious if others have better solutions than mine since mine mixes both dynamic sql and the unpivot operator. Neither of which are fun to maintain.",1.0
g5lc757,iulae9,"I'm no Mongo expert either, but I think this will depend on the type of data you're looking at. My understanding is that non-relational databases are good for cases where you have unstructured data and I don't have requirements for complete consistency of data.

I can assure you that I have a dozen or so SQL Servers with thousands of queries per second running without problem. If those thousand queries per second were all handling raw image file data in binary, then yes it could ""break down"" in some ways, but that's what file servers are for.",8.0
g5lzef8,iulae9,"Probably a ""stacked"" comparison of saving a document structure 

Mongo would be fetching and saving a document

If compared to relational where document is broken down into a normalised equivalent as a consistent transaction,  then there may be a slight advantage in Mongo

SQL saving a serialised document as XML, JSON or BLOB I'd be very surprised if there was any advantage.

SQL summarising and or doing complex filters is unlikely to be beaten by a doc db

So work out your requirement and then choose the right engine for your solution. 

Only Sith deal in absolutes...",4.0
g5mn1lp,iulae9,"He's kind of right. For the more scientific explanation of this phenomena, you can search for CQRS. However, probably less than 2% of use cases have such demands, but currently 50% of all apps built are using document based database - Hence, it's the new *""cool aid""* for all practical concerns ... :/",2.0
g5labqm,iukjn2,"Add a transaction\_date column to Transactions table. Turn the Food table into a Slow Changing Dimension table by adding a valid\_from and valid\_to columns.

So now the food price has a validity period and you can check transactions against the proper unit price. 

Also check out the Wikipedia article on Slow Changing Dimension.",7.0
g5l7w7b,iukjn2,"Use a third table for the details of a transaction - what was sold, when, where, quantity, price what was sold, how many sold, and total price. I have not done food point of sales in 15 years but the key focus is to be able to report on each transaction.",1.0
g5lmp7s,iukjn2,"In practice, the order lines would often store the line unit price &amp; tax (vat). 

Line prices can differ for all sorts of reasons and it's awkward to join back to your products table to calculate the price as it was in certain circumstances. Some places offer different line item discounts or different price lists. 

But as has been suggested, you can do a price history and look it up that way.

For rounding/tax reasons I would tend to store the line data so you can at least reproduce the customer receipts exactly.",1.0
g5lcmqr,iukjn2,"so, what you are saying by ""if the food prices would be changed"" is that your ""Price"" attribute depends not just on Food but also on the timeframe.

the FoodPrice table would be (Food, Effective_Date, Price). Consequently you can eliminate the ""Total Price"" column.

It is normalized BUT in real life this structure is NOT convenient. Most of the time you will see effective ranges (effective_from, effective_to) (dear Cthulhu, please make people use one-side exclusive ranges more often, thank you).

There are also reasons to include the total price/amount on the transaction in the real world anyway - while it is not inconceivable that every business rule applied at the time of transaction would be known/repeatable but the more time comes the more this creates a burden on all of your future code.",1.0
g5kl9uq,iueyfn,"I would suggest to use Transact SQL instead of the wizard. 

It can be a job with 3 steps.



Step 1: performs backup of the database to a network folder which is shared and accessible to the credential being used by the SQL server agent.


[Jump to Using Transact SQL](https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/create-a-full-database-backup-sql-server?view=sql-server-ver15#TsqlProcedure)

Step 2: perform restore using the backup just created.

[MSDN link](https://docs.microsoft.com/en-us/sql/t-sql/statements/restore-statements-transact-sql?view=sql-server-ver15)

Step 3: delete the backup file if they are older than a certain number of days.


[Stack overflow link] (https://stackoverflow.com/questions/43269779/how-to-delete-files-on-the-directory-via-ms-sql-server)",2.0
g5l3jh4,iueyfn,"Thank you for your reply, I think this is way I will have to choose.",1.0
g5kgs2s,iues49,"give me all persons, and their nickname on twitter, if any

    SELECT persons.id
         , persons.name
         , nicknames.nick AS twitter_nick
      FROM persons
    LEFT OUTER
      JOIN nicknames
        ON nicknames.person_id = persons.id
       AND nicknames.system = 'twitter'

would not return the correct result if done in the WHERE clause

this is very common

note that for INNER joins, it doesn't really matter, although many people will put the filter conditions into the `ON` clause anyway, especially if there are many joins, as it makes the entire query more readable and keeps the `WHERE` clause free of cruft",7.0
g5nb7wj,iues49,"The ON is just specifying the JOIN conditions, where it's matching records. The type of JOIN also determines the matching conditions. The WHERE/HAVING clauses are where filtering occurs. It's more readable, a best practice, and I want to say that the optimizers are able to execute the query more efficiently this way. 

If I remember right, before JOINs were added the matching was actually done in the WHERE clause (could be wrong about this). These are called implicit JOINs and I would avoid using them. I imagine that for simple queries that use INNER JOINs, implicit JOINs likely won't hurt query performance as the optimizer should recognize what you're trying to do, but it's best practice to just use JOINs.",1.0
g5kfiit,iua093,What about using the official ODBC driver: https://odbc.postgresql.org/,2.0
g5laqrm,iua093,"Hi! Thanks for linking! I downloaded  psqlodbc_10_03_0000-x64.zip, but I'm not sure how to reference it in excel or ensure it gets updated to my ODBC driver....as I'm sure you've noticed I'm very new at this.",2.0
g5jh5s3,iua093,"The error sounds like a DB permissions error.  Check DB credentials.  Alternately, establish an ODBC connection first and have Excel use that instead.",1.0
g5jikvx,iua093,"Hey, thank you very much for your reply! The DB credentials were correct - triple checked em.

I haven't ever established a ODBC connection before - when it tells me to input a connection string - where do I find the information that I would need to input? I just need to input the driver, the server, and the database name correct?

I'm not sure what to input for the driver and where I would find that input. Would I be correct in assuming the server host name:portnumber would be the server input? (example I found online shows = (local)). Database one is self-explanatory - I assume it's just the database name.


Sorry for all of the questions - very new to SQL",1.0
g5jwkg3,iua093,"Glad it worked out.  Establishing the DB connection is often the pickiest part.  Once that is in place its usually much smoother sailing.  Google ""ODBC connection string"" - there is a ton of help tailored to each DB type.  For Windows, there are usually many drivers preinstalled.  If not, check the DB website (postgresql.org in this case), the drivers aren't hard to find.  Install the driver, then Windows will allow you to choose it when you try and create a new ODBC connection.  I prefer using ODBC (or JDBC) connections since you can store your credentials there.  Many apps (Excel, Powershell, SAS, etc.) can then use the same ODBC connection.  Good luck.",1.0
g5jwudk,iua093,It didn't work out unfortunately - the credentials were correct but it still gave me an error - will give your suggestion a try though - thanks!,1.0
g5jxufs,iua093,"Sorry, I misunderstood.  Yes, try the ODBC route.  It's best under Windows anyway.",1.0
g5kzd3b,iua093,"Sorry to bother you with a very stupid question, but I did a version check on DBeaver's PostgreSQL (since I didn't have it downloaded and it auto downloaded a version for me). This is the output:

PostgreSQL 11.6 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.3 20140911 (Red Hat 4.8.3-9), 64-bit

Does this mean the driver is PostgreSQL 11.6? Or is it ""PostgreSQL 11.6 on x86_64-pc-linux-gnu""? I've never made a ODBC connection before so I'm not 100% sure what specific info I need to input in each section.",1.0
g5l4pve,iua093,"No, it means that the Postgres **server** version is 11.6 (and is running on Linux).

Also: DBeaver uses Java and thus JDBC, not ODBC for the connection.",1.0
g5l7lpz,iua093,"I thought what I was connecting to is PostgreSQL and DBeaver is just a ""host"" of sorts that lets me view the data? 2 follow up questions then, if you don't mind

1. Is it possible to do a JDBC by default (without any kind of additional software) between DBeaver's PostgreSQL and Excel?

2. If the answer to above is yes, how would I go about doing it? If no, do you have any software that you can recommend?",1.0
g5l8uji,iua093,"&gt; Is it possible to do a JDBC by default (without any kind of additional software) between DBeaver's PostgreSQL and Excel?

I don't know DBeaver, but yes, it's just a SQL client (like `psql` or `pgadmin`) and it has nothing to do with Excel.

But I would assume the only way to access Postgres from within Excel is through ODBC. So Excel would be a ""SQL client"", just like DBeaver (leaving DBeaver completely out of the picture)",1.0
g5lyav6,iua093,https://help.interfaceware.com/v6/connect-to-postgresql-from-windows-with-odbc,1.0
g5jotl5,iua093,"I make hrrps://beekeeperstudio.io, you can run a sql query and export the results to Excel, not sure if that's good enough.

I've never had much luck connecting databases to Excel directly honestly.",1.0
g5js8zy,iua093,"Unfortunately, it's not what I'm looking for - already using DBeaver and it can export the results to excel; I already have an excel ""results template"" data analysis sheet set up and I need to have it linked directly to the database if possible",1.0
g5jd8za,iu8fwk,"Data Management by Richard Watson is a classic SQL book, it’s pretty short and easy to understand and can usually be found online for free",20.0
g5jopw1,iu8fwk,sql cookbook by anthony molinaro has hundreds of examples,4.0
g5jr8at,iu8fwk,Thank you so much!! Will also check this out!,1.0
g5jfuj0,iu8fwk,Thank you so much! Will definitely check it out!!,1.0
g5j7ojn,iu8fwk,"I started with PowerPivot. At the time we had a sql report developer on staff that I had to ask to write queries for me. Eventually he got sick of writing queries for me and showed me the basics of a SELECT query. Then IT gave me domain admin because they love me so access to everything.

That’s all I needed. Everything I learned was based on a request, a complaint, an idea. Something that I wanted to make better, more accurate, more insightful, easier, faster, etc

So I googled the rest, I also had access to a SSRS report library that I could open any report and copy the queries to reverse engineer them.

And VPN access from home helped a lot too. I spent a lot of my own time just because I wanted to learn it.

When I ran into urgent problems that I didn’t have time to learn how to fix, we hired consultants and said show me how you’d do this. Learned massive amounts in small time with great consultants.",10.0
g5jfsyg,iu8fwk,This is awesome! Thanks so much for sharing! Hahah love that IT gave you domain access because they liked you :),3.0
g5jielf,iu8fwk,"If you were a ditchdigger and asked if there was a better tool than a shovel, SQL wouldn't be a steamshovel. SQL would be a logistics transpo company. Not one that specifically moves dirt, but necessary if you are into the serious moving of dirt.",7.0
g5jllpq,iu8fwk,This is a great analogy!!,1.0
g5jwlod,iu8fwk,https://www.w3schools.com/sql/,5.0
g5jta9i,iu8fwk,"Take SQL for Data Science Specialization from Coursera, it has covered all the topics also practice SQL from leetcode and HackerRank.",5.0
g5jk29t,iu8fwk,"Depends on the data you're working with. Unless you have direct data base admin access, SQL won't do much for you. If you're access is limited to downloading reports fed to you from admins, it may be best to master excel and python instead. Plus learning those to can lead right to SQL down the line.",3.0
g5jlpej,iu8fwk,Thank you for the suggestion!! Hmmm guess I should prioritize beefing up my excel skills before looking at SQL,1.0
g5kvxh9,iu8fwk,"Data Camp. Should give you their first lessons for free, which let you play around in interactive terminal and write queries.

It's enough to get a context for anything you'll read elsewhere, or you can just pay to unlock the rest of the course if you're in the financial position to do so.",3.0
g5nxr42,iu8fwk,The best way I know so far is learning on DataCamp and practicing on LeetCode and StrataScratch. Now you're ready to mention SQL on your resume!,3.0
g5jiux1,iu8fwk,Maybe you can start with filters? [https://www.youtube.com/watch?v=gjUNNHlMwWA](https://www.youtube.com/watch?v=gjUNNHlMwWA),2.0
g5jlq2e,iu8fwk,Thanks so much for the recommendation!!,1.0
g5jp0mp,iu8fwk,and pivot table concept translated into SQL [https://www.youtube.com/watch?v=mHWb16MvyMs](https://www.youtube.com/watch?v=mHWb16MvyMs),2.0
g5jrbjl,iu8fwk,Thank you for the helpful links!!,2.0
g5jow0g,iu8fwk,https://youtu.be/7S_tz1z_5bA,2.0
g5jrcxq,iu8fwk,Thank you!!,1.0
g5jrge3,iu8fwk,Should get you started and interested. Take it from there on your own projects 🤓,2.0
g5ju3go,iu8fwk,Sqlbolt.com gives a good run through of the basics with interactive examples.,2.0
g5kd1qy,iu8fwk,[deleted],3.0
g61yzj0,iu8fwk,"After sqlbolt I would highly suggest downloading mysql workbench and creating your own database. Whether it's importing data or creating from scratch does not matter.  Obviously you could use postgreSQL or any others, MysSQL workbench was what I started with and found to be the easiest to navigate. There is a great sample database through mySQL as well, known as the 'Sakila Database'. This link I'm providing explains just what the Sakila Database is and several questions to help guide you with querying for specific data. [https://datamastery.gitlab.io/exercises/sakila-queries.html](https://datamastery.gitlab.io/exercises/sakila-queries.html) 

If this is too basic, let me know and I'll suggest some other options.",1.0
g5k1gtz,iu8fwk,Sqlbolt,2.0
g5k6dwt,iu8fwk,"I recently started using  [codewars](https://www.codewars.com/) which helps improve your skills by doing real challenges for multiple languages including SQL. You're able to filter by difficultly and popularity of the challenge and  after you complete a challenge you're able to see how others solved the same challenge, it's really helped me improve my queries.",2.0
g5k8lo7,iu8fwk,"I feel like when people like you say, 'a lot of data' they really aren't talking about a lot of data.",2.0
g5kbtnf,iu8fwk,"Because I was unhappy with most solutions I saw out there I went  ahead and created an online course over the last four months. I don’t want to self promote on here so I’m not going to give the link but udemy gives me 30 copies a month to give away for free and really all I want is feedback, and if it’s good feedback please leave a review on Udemy to improve my ranking. Just send me a message and I’ll be happy to get you A free copy. It’s pretty good if I do say so myself and it’s focused on ISO standard SQL and takes you to the intermediate level. I’m working on the beginnings of the advance version now.",2.0
g5miw4a,iu8fwk,Send me a link please.,1.0
g5kh341,iu8fwk,"I recommend to everyone who is interested to use the SoloLearn app. SQL is on there and that's where I learnt all the basics. From there, youtube videos helped expand on the basics and experience helped fine tune where I want to learn more. I watch Brent Ozar on YouTube nowadays just to pick up a thing or two that I may not have known before.

I started out with Excel and it's MS Query then branched out to SQL Server. I started with practicing in localdb and then started to import some of my Excel data to work on it in there. Now I have a mini datawarehouse with SSIS packages for regular ETL, security, stored procedures, indexes, use of temp tables, CTEs and variables and now just dabbling in some dynamic sql (under the guidance of many resources to make it as effective, efficient and safe as practical).",2.0
g5kmhl0,iu8fwk,"Learn by doing. Find a course online that actually takes you through writing the code.  


Or better still, get a simple database set up and mess around in that. It seems daunting to do at first, but its actually fairly trivial.",2.0
g5kqqyv,iu8fwk,"YouTube videos, and you can also get ur free SQL book https://books.goalkicker.com/SQLBook",2.0
g5kt2nz,iu8fwk,"I started off by asking peers to write it down on a piece of paper and then copying it. 2 days of this and I was well versed with the basic syntax. Learnt more as and when use cases popped up (again peer support) and also went through existing queries in the system. 

Fast forward 6 months and have been taking SQL classes as part of employee onboarding for the team. :)",2.0
g5r1ht9,iu8fwk,"I feel like so many new users get caught up by syntax and think that advanced developers memorize things.

Sure, I memorize some things... but I constantly Google other things like how to convert a timestamp to the first of the month. Part of learning SQL is learning how to perfect your syntax so that a query will run, but really its mostly piecing together snippets of code and integrating the syntax.

There are only a few random things you need to memorize, like using a semi-colon after you declare parameters if you want to run a CTE, or other little weird shit that you just have to do.",2.0
g5lhwpn,iu8fwk,not free but so far i took a $12 course on UDEMY then cross-referenced with w3schools. So far the very basics are starting to make sense. I want to do an Oracle course next and try and get certified. The job markets getting super competitive thanks to covid and SQL seems like a must.,2.0
g5jek3k,iu8fwk,"This gets asked nearly every day on this sub. Feel free to search/browse, there's more answers than you could ever hope for.",2.0
g5jg15n,iu8fwk,Okay thanks! I’ll search/browse :),1.0
g5kd2ca,iu8fwk,[deleted],1.0
g5kymf6,iu8fwk,Well you're fucked then.,1.0
g5je8ps,iu8fwk,What kind of work do you do ? What kind of data is that,1.0
g5jfybn,iu8fwk,I work in marketing and work with a lot of excel data! I did pivot tables a lot but feel there are probably greater efficiencies with mining large amounts of data than what I’m currently doing in excel,1.0
g5jg4yd,iu8fwk,Do you have sql server at work where you can play with ?,1.0
g5kaznd,iu8fwk,Honestly PowerQuery might be better to look into if your data sources are not SQL! Maybe ask over on /r/excel about that (if you are on a recent enough version of office that is),1.0
g5q78p6,iu8fwk,I need help in a school project,1.0
g5q79o8,iu8fwk,Someone please help me,1.0
g5j88kz,iu687m,"There are probably better ways to do what you want to do... but the syntax you are requesting is a pivot.


OK, so you have your top 10.


Select top 10 product, totalrevenue

From blahblah

Order by totalrevenue desc



What you want is a PIVOT.. but pivot is a bit of a pain with dynamic stuff. If you want 1 product per column, then you'll always have to know what products those are. OR pivot on a row number.


Select [1], [2]...[10]

From

(

Select top 10 product, totalrevenue, row_number() over (order by totalrevenue desc) rownum

From blahblah

Order by totalrevenue desc

) mainQ

Pivot

(

Max(totalrevenue)

For rownum in ([1],[2],[3]...[10])

) pvt 


That will give you 10 columns, 1-10 with total revenue as the value.


Change max(totalrevenue) to max(productname) or w/e... and your 1-10 columns would be your rank. Whatever is in [1] would be the top product for that slice of data.",2.0
g5j8dwr,iu687m,Happy Cake Day!,1.0
g5k1lrx,iu687m,"PIVOT! Haven't learned that one yet, seems like a good day to start. I'll give it a try and thanks for the suggestion, very helpful.",1.0
g5icydp,iu3li3,"Consider that, like most languages, line breaks in SQL are functionally equivalent to spaces (comments aside). The query you have written here could then be written as:

    select t1.*
    from t1
    left join t2 on t1.fk = t2.pk
    join t3 on t2.fk = t3.pk

Read this way, it becomes apparent that the query isn't read left to right or by line; without t1 established in the first line of the query, there is nowhere from which t1.* can be pulled.

https://learnsql.com/blog/sql-order-of-operations/#:~:text=Six%20Operations%20to%20Order%3A%20SELECT,developer%20to%20know%20this%20order.",2.0
g5kjyl6,iu3li3,"I still very confused.

So,  Is it mean that `( select t1.* from t1  )`  will executed first ? then the resulting table left join  `t2 on t1.fk = t2.pk`   then join  `join t3 on t2.fk = t3.pk`   ?",1.0
g5lh85q,iu3li3,"No. SELECT is the last statement to be executed. The query is *executed* in the order:

FROM
WHERE
GROUP BY
HAVING
SELECT
ORDER BY

JOIN is an operation executed as part of the WHERE clause.",2.0
g5kjz87,iu3li3,"**I found links in your comment that were not hyperlinked:**

* [t1.fk](https://t1.fk)
* [t2.pk](https://t2.pk)
* [t2.fk](https://t2.fk)
* [t3.pk](https://t3.pk)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g5kjyl6&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",0.0
g5ki7co,iu3li3,"Because otherwise your where filters wouldn’t be evaluated until all your tables had been joined together. SQL is compiled by your RDBMS and that will decide how it executed your code. All you need to do is tell it what you want, not the how you want it.",1.0
g5idn5b,iu38zw,"You can host a SQL DB on Azure for free. The free tiEr is fairly limited, but for a simple demo it should work ok.",3.0
g5j5snh,iu38zw,You might be able to get a free-tier AWS ec2 container to hold a rudimentary one. It'd be a lot easier to just set up port forwarding from your home machine though imo.,1.0
g5i4qxy,iu1pe3,"    SELECT minmax.Name
         , min.Date AS minDate
         , min.Score AS minScore
         , max.Date AS maxDate
         , max.Score AS maxScore
         , max.Score - min.Score AS Difference
      FROM ( SELECT Name
                  , MIN(Date) AS mindate
                  , MAX(Date) AS maxdate
               FROM yourtable
             GROUP
                 BY Name ) AS minmax
    INNER
      JOIN yourtable AS min
        ON min.Name = minmax.Name
       AND min.Date = minmax.mindate
    INNER
      JOIN yourtable AS max
        ON max.Name = minmax.Name
       AND max.Date = minmax.maxdate",2.0
g5jwcsd,iu1pe3,"Here’s another take. It uses the first/last value window functions. They’re useful for when you want one field, the score in this case, but it’s not the field you need to sort by, i.e. the date field. 

    ;with diffs as(
        select t.group
            ,last_value(t.score) over(partition by t.group order by t.date) 
                - first_value(t.score) over(partition by t.group order by t.date)[scoreDiff]
        from #table t)
    select d.group, d.scoreDiff
    from diffs d
    group by d.group, d.scoreDiff",1.0
g5in6q8,iu0xvw,There is no specific license key to use. When it gets to the license screen just click next. I have installed over 50 SQL instances for the company i work for. Never needed a key. just clicked next on that screen. Just had to make sure that we purchased enough CPU core licenses for when MS comes through and does a software audit.,1.0
g5j160a,iu0xvw,Thank you I appreciate the reply. Yeah I have never had an issue with the iso from VLSC but just wanted to cover my bases.,1.0
g5i5o4z,iu029i,"I suspect your c# process is just transaction heavy.  

What size is the log file after you do log backup?

Is it possible to switch the database to simple recovery (or bulk logging) during your archive process?",1.0
g5iefix,iu029i,"If i do a log file backup, it goes empty (I could shrink it down to minimum size).  

The odd thing I am seeing now is that the log files size limit was set to a value higher than the drive space.  To make sure I could do a trn backup, I set the limit to 200gb, and now it seems to be under control.  Not sure why that would ""fix"" it.  I'm trying to get my IT guy to  reset the Virtual Machine snapshot back to before we started testing and i'll see if setting the log limit without anything else is what actually lets it run.  

Very confused by this, haven't seen it behave like this before.",1.0
g5m9y1d,iu029i,"yeah not sure what it is.  But restoring the VM and running the archive process right away causes the Log file to grow 100s of GB larger than expected.

Restoring the VM and doing a Log file back up followed by a full backup and the Log file doesn't even break 10GB (which does seem a bit large for only archiving and deleting 3GB of data, but it works for me).",1.0
g5kihe9,iu029i,"Run the following:
Select name, recovery_model_desc, log_reuse_wait_desc from sys.databases
against your master DB.
What does it say for the DB in question. 
A hunch is you’ve got an old transaction that just hasn’t committed, so run “dbcc opentran” from that database to get information about the oldest open transaction",1.0
g5lh8wj,iu029i,"&amp;#x200B;

|name|recovery\_model\_desc|log\_reuse\_wait\_desc|
|:-|:-|:-|
|Data|FULL|LOG\_BACKUP|

  
No active open transactions.

DBCC execution completed. If DBCC printed error messages, contact your system administrator.",1.0
g5m9ye4,iu029i,"yeah not sure what it is.  But restoring the VM and running the archive process right away causes the Log file to grow 100s of GB larger than expected.

Restoring the VM and doing a Log file back up followed by a full backup and the Log file doesn't even break 10GB (which does seem a bit large for only archiving and deleting 3GB of data, but it works for me).",1.0
g5n3hf5,iu029i,"Okay so it sounds like it must solely be your process that is generating all that redo. Maybe break up your process into smaller steps and check which part is mostly responsible for all the redo. You’re doing lots of data copying in this, if that needs to be recoverable (according to the DB) then redo needs to be written for it. Perhaps your insertion into your temp tables is responsible, perhaps some of the deletion. If you do any of the DML row by row then you will find that your redo will be much larger than it needs to be due to overheads.",1.0
g5hb5u6,itxyop,"No, but Sounds fun. Please follow up later.",26.0
g5hb9qo,itxyop,lol you want to do it for me? I'm nervous as hell,8.0
g5in4be,itxyop,Part of the trick to this will be to turn the nervousness into curiosity.,5.0
g5k6nsq,itxyop,"holy shit man this is great interview advice. I'm always telling people to channel their negative energy in a different way. Can say I've never thought of this one.

Ty!",1.0
g5hd1ze,itxyop,"Don't stress about it! It's pretty standard. If you think they are going to hit you with the hardest problem you can imagine, that's pretty normal too. With a vague description like that.. it will fall somewhere between 'i can do that in my sleep' and 'let me turn on some music and grab some coffee, this is going to take a minute'. 

&amp;#x200B;

 But that's a normal job right? Sometimes you get to work, somethings on fire.. and you don't even know how / why it started. The fire was lit years before you arrived... you just gotta put it out.   


If the moment comes, and you are feeling a bit unsure, just look at the interviewer and say: ""Normally I have access to the internet, do you mind if i look some stuff up on my phone to make sure I'm providing you with the best answer I possibly can.""  Worst they can say is no... the best thing that can happen is they realize, you are just like them.",7.0
g5hdzb8,itxyop,"The problem is I get high anxiety in a testing environment. If I already have the job I can handle fires needing to be put out, but this is like taking the SAT or standardized testing for me. It often doesn't represent my skills/ability.",4.0
g5hf5hk,itxyop,"I find that good interviewers are generally sympathetic to that, and try to put you at ease, nudge you towards things they believe you're capable of figuring out but might be a bit anxious to realise in an interview context, etc.",3.0
g5itynn,itxyop,"&gt;But that's a normal job right? Sometimes you get to work, somethings on fire.. and you don't even know how / why it started. The fire was lit years before you arrived... you just gotta put it out.

It's just like riding a bike. Except the bike is on fire. And everything is on fire. And you're in Hell.",3.0
g5jcq96,itxyop,"If they say no, I don’t want to work there. Completely asinine to expect someone to know every aspect about SQL, or any language for that matter. I have to use stackexchange once in a while to look up some basic garbage. I can’t tell you how many times I’ve googled “sql find tables based on column name”",2.0
g5i2i7b,itxyop,I'm glad I'm not the only one that kicks on some tunes and says it'll take a couple minutes! 🤣,1.0
g5hcspq,itxyop,"I did something similar recently where I had some SQL exercises to do using Big Query, while sharing my screen with a panel. You'll probably find it will be a fairly straightforward problem, just make sure you speak your thoughts out loud, explain what you are doing and why.",5.0
g5hd8u2,itxyop,What an awesome exercise. I'd love an interview like this. Gives them a chance to see your skills and how you approach a problem,3.0
g5hblwl,itxyop,what position are you applying for?,3.0
g5hbv4r,itxyop,It's a Data Analyst role at a startup,5.0
g5hgwxu,itxyop,sounds like a blast.,3.0
g5ibtic,itxyop,"Good luck! I bet they're testing for functions and outer joins, maybe a sub query.",2.0
g5itrc8,itxyop,"I had a few of these during my last job hunt, and most of them were surprises!

The trick is to go into it dripping with positivity. Pretend your best friend has just called to ask if you can help them figure something out. The things you know, you know. The things you don't know, you don't know. In one of them I was asked to do something with a command I hadn't used before, so I shamelessly opened Google up right in front of them. No problem.

If they don't want you using outside resources, and you don't know, tell them, ""I know this and this and this about it, so I think the right move is this, and normally I'd double-check online with this search to bring myself up to speed.""

Remember, analysts look at problems to suss out a solution. Any interviewer who actually does this kind of work will recognize this process, because I guarantee that they do it regularly to solve their own problems. Anyone who wants you to know obscure commands in an unfamiliar flavor of SQL is playing gotcha, it's a jerk move, and you might be dodging a bullet if they don't hire you.

The interview that ended up getting me a job sent me a set of problems with a time limit to return them. I liked this approach because it really tested me without the pressure of someone watching. The time was enough for me to double-check some of my thinking online, but not long enough for me to enter their data into a local database for testing and blunder my way through it. I don't even know how many of the questions I got right, but I had fun and told them as much.  


Edit: OP, how'd it go?",2.0
g5jb8o7,itxyop,"I've never done that but it sounds like a great way to go.  To me, collaboration is key.  If I am in this role, and I need to engage the engineer to ensure their requirements are met, this is how I get to the root of what they are wanting or need to do.  

""I need to get a list of sales people.""  Sure.  That is fine but what is it about these sales people you are needing to know?  Are they good at selling these products?  Are they skilled in selling in this area?  Are they friends with the potential client?

The only way I can help them is if them engage with me directly so i can help them directly.

I would take this all day long over interviews that are ""If wanted to order my customers by how much they bough over the last 8 years, how would I do that? And why is that important?""

Work with me.  I am not your adversary.  I am here to help both of us win.",1.0
g5jdgo3,itxyop,I was asked in an interview how I approached data analysis problems.  I responded with: i ask where does it hurt and how much does it hurt.  Got the job.,1.0
g5heonr,itxwis,"I generally have three sections, cover letter, job history, and skills. My cover letter covers key points on my resume and usually addresses several points that were made on the job listing that were deemed important, but the flow and wording is mostly a template at this point. The job history is where I put a brief explanation of how I used technology and solutions to create value that were of key note, and my skills section details out my years of experience with varying technical or soft solutions.

I would use the cover letter to detail explicit skills, usage, and knowledge. If you have no job history and the project is there to supplement it, here is how I'd word it.

Using CRUD (Create, Read, Update, and Delete) operations, I curated a normalized MySQL data set ready for Power Bi consumption to (Insert value here). 

I'd then upload the SQL to GitHub along with the PowerBi file and probably some screenshots. Additional bonuses if you have a website where you can detail the process, screenshots, and code.",6.0
g5hrudy,itxwis,"Thank you, I have a portfolio where I share my work. How much code would suffice for the outline of the process? I'm not sure I still have all of the scripts for creating everything but I can replicate some of the table creations and views I created. I think an image of the schema design would be appropriate as well.",1.0
g5i085t,itxwis,"You could (re)design the tables in [https://dbdiagram.io/](https://dbdiagram.io/), which might help you to make the design easily visible (plus there is an option to export your diagram to MySQL code, which you could store on GitHub). 

Besides that, I agree with u/FoCo_SQL, just write a brief description of what you have done in your cover letter (bonus points if you can phrase it to align a bit with the tasks of the job description, but keep it honest :-)).

Good luck with the application!",3.0
g5kzjgl,itxwis,"On my resume, I list the skill similar to the example I gave you. I also have a few highlighted projects on my skills / education / certifications page. These are generally articles I've written that have been highlighted in other larger newsletters or in the SQL community. 

[Here is one article I wrote on a ""homework assignment""](https://jonshaulis.com/index.php/2018/06/09/skill-showcase-pet-store-oltp-database/) from an interview. I decided to turn it into a showcase piece. You can see my github and look at the files to see what all is included. It was a database design with basic CRUD operations as well and is very similar to what you are doing, you could probably use it as a template. (Cut out what you don't like and add in what you do like.)

Honestly, most of the places probably won't even look at the code that you list on your resume. (Unless you are a junior or getting in the door.) I think I've had maybe two interviews out of hundreds where they actually looked at my portfolio. This is not to discourage you though, my portfolio definitely helped me land those interviews. The individuals didn't take time to go through it, but they clicked it, saw I had a bunch of posts, and it checked the boxes for them to interview me.",1.0
g5k4gjk,itxwis,"The basic formula which I follow whenever I write my resume is:

""Accomplished X as measured using Y by doing Z""

Try to fit what you have (Experience/Projects/Achievements/Extra-curricular etc) into this formula.",3.0
g5kzp47,itxwis,Do you write in and focus on the value your accomplishments gave the organization or do you focus on the results and allow the reader to make their own decisions on the value gained?,1.0
g5pg7ha,itxwis,"Both but in order, first I write those accomplishments which created significant value for the organisation (if I have prior experience in the same field) and then results which I have obtained from my side-projects, hackathons, programming competitions etc which allows reader/employer to make their own decisions on the results/accomplishments gained.

I think both are important, the first gives the employer a brief idea about whether I will be valuable for the organisation in the specific job for which I have applied for and second one gives the employer an idea about even if I didn't pass the bar did I have pass the bar set for other positions at the organisation which may aligned with my other accomplishments.",2.0
g5h8p45,itx9ik,"classic use case for a CROSS JOIN

    SELECT table1.name
         , table2.date
         , table1.factor * table2.amount AS result
      FROM table1
    CROSS
      JOIN table2",3.0
g5hbk94,itx9ik,"Yep, looks like a Cross Join is what I needed, thanks!",2.0
g5h8s0q,itx9ik,"You want a CROSS JOIN, I think:

    SELECT t1.name, t2.date, t1.factor*t2.amount AS factored_amount
    FROM table1 t1 CROSS JOIN table2 t2",2.0
g5hbkr1,itx9ik,"That worked, thanks!",2.0
g5h8trk,itx9ik,"So you want a multiplicative join. So in the FROM clause instead of joining the tow tables on a common field just list the tables separated by a comma.

In the SELECT list the fields individually and do your calculation there.

Something like

SELECT NAME

,DAY

,MonthYear

,Factor \* DayAmount as AllocatedAmount

FROM PEOPLE, CALENDAR",1.0
g5h7ra9,itx5sz,"there's a section in the sidebar for ""learning sql"", here's the link to the wikipage directly:

https://www.reddit.com/r/SQL/wiki/index",4.0
g5h7tpd,itx5sz,Thank you!!,1.0
g5h6n4e,itx5sz,"I started with Oracle , they have great material and exercises as well. Try one of these? although now a days there are many online courses available at udemy, coursera etc.",3.0
g5h7ulo,itx5sz,I appreciate the feedback! Thank you!,1.0
g5h84at,itx5sz,No worries !,1.0
g5j1uqg,itx5sz,DataCamp.com is a good learning platform.  I completed the Data Analyst with SQL career track this year.,3.0
g5hb5rc,itx5sz,Maybe start with this https://youtu.be/gjUNNHlMwWA,1.0
g5hgq0p,itx5sz,"Pg exercises is a great website for practice. My recommendation would be to try to find a dataset you are interested in, and manipulate for different use cases or reports that you would like the data to be organized in.",1.0
g5h6llt,itwmhy,do you have the oracle client installed as well? also is your tnsname file not corrupted? those are the usual suspects.,2.0
g5h7z37,itwmhy,Well..u can always try installing it in a vm,1.0
g5h0o5w,itvo04,"I know how I'd do it in MSSQL, but in MySQL...

It looks like you want to use the functions NOW(), DATE_ADD(), and DAY(). Calculate the dates needed into variables and use those in the between dates. Subtract a month and the number of days to get the previous month's first day for the first variable. For the second date, subtract the number of days -1 additional day to get you into the previous month.",1.0
g5h12fn,itvo04,"If you run it on the 1st use between date_add(curdate(),interval -1 month) and date_add(curdate(),interval -1 day)

Something like that
Not sure about scheduling I don’t use MySQL",1.0
g5h378u,itvo04,"works only with date values, misses a whole day's worth of datetime values",0.0
g5h56r9,itvo04,Then cast it as date - just giving a basic direction,1.0
g5h8dch,itvo04,"&gt; Then cast it as date

thus making it non-[sargable](https://en.wikipedia.org/wiki/Sargable) and inefficient",1.0
g5mnjw6,itvo04,"You might want to check out [Magic](https://polterguy.github.io/). One of its features, is a scheduler, allowing you to periodically execute Hyperlambda tasks, which can be thin wrappers on top of SQL queries.

**Disclaimer** - I'm the creator of Magic ...",1.0
g5h2731,itvo04,"Dynamic dates are easy to set up.  

DATEADD(month(datediff(month,0,getdate()-1,0)
first day of previous month 

DATEADD(month(datediff(month,-1,getdate()-1,-1)
Last day of previous month. 

Put those in your where statement or declare dates and you should be Holden",1.0
g5h2m33,itvo04,there is no GETDATE() function in MySQL,0.0
g5h9bfc,itvo04,I just saw the MySQL flare... I guess instead of get date use NOW().  Thx for the catch,1.0
g5h320d,itvo04,"run the query with these conditions on any day in the following month

automatic scheduling would be a SQL Studio option, i guess

    WHERE datecolumn &gt;= CURRENT_DATE - INTERVAL DAY(CURRENT_DATE)-1 DAY
                                     - INTERVAL 1 MONTH
      AND datecolumn  &lt; CURRENT_DATE - INTERVAL DAY(CURRENT_DATE)-1 DAY",1.0
g5gkowt,itt51w,"This guy makes great video lessons. Find his mysql stuff
https://m.youtube.com/c/learnWebCoding/videos",1.0
g5gevk6,itruvp,"I'm not an oracle developer but quite experienced in MS SQL.

It's not possible with SQL to do this kind of query without using dynamic query.Using the information i got on this site i would do something in the style of:

`DECLAREsql_stmnt varchar(500);sql_columns varchar(500);`

`sql_columns :=  SELECT LISTAGG(column_name, ',') FROM all_tab_cols WHERE owner = 'OPERA' AND column_name NOT LIKE 'UDF%' AND table_name = 'RESERVATION_NAME' ORDER BY column_name`

`sql_stmnt := 'SELECT ' + sql_columns + ' FROM  reservation_name'`

`EXECUTE IMMEDIATE sql_stmnt`

&amp;#x200B;

Not sure if this will work in Oracle but I'm sure you can find a solution if you fiddle around with it.

\[Dynamic query\]([https://docs.oracle.com/cd/B12037\_01/appdev.101/b10807/13\_elems017.htm#:\~:text=The%20EXECUTE%20IMMEDIATE%20statement%20executes,and%20so%20on%20in%20advance.](https://docs.oracle.com/cd/B12037_01/appdev.101/b10807/13_elems017.htm#:~:text=The%20EXECUTE%20IMMEDIATE%20statement%20executes,and%20so%20on%20in%20advance.))

&amp;#x200B;

\[String join\]([https://stackoverflow.com/questions/4686543/sql-query-to-concatenate-column-values-from-multiple-rows-in-oracle](https://stackoverflow.com/questions/4686543/sql-query-to-concatenate-column-values-from-multiple-rows-in-oracle))",3.0
g5gfswm,itruvp,"thanks a lot! I did find some answers before on SO with dynamic queries, but I was hoping I just misunderstood OP and I don't need to use that in mine... seems very advanced

I will check it out then, seems there is no choice. thanks again!",1.0
g5g3fq2,itpsm0,"If I'm understanding your post correctly, it sounds like there's a many-to-many relationship between the players and matches. I would suggest starting with a table for players and a table for matches. These two tables should be unique. You can then add a third intermediary table that has a column for matches and a column for players playing in the matches. This table will have some duplicate data. Let me know if I need to explain more.

Here's a [link](https://dzone.com/articles/how-to-handle-a-many-to-many-relationship-in-datab) to an article about how to handle the many-to-many relationship.",1.0
g5g4my5,itpsm0,Yes this is exactly it. Thank you very much!,1.0
g5gmj53,itpsm0,I’ve also heard this intermediary table called a bridge table.,1.0
g5gcwwx,itmdtq,"I stopped reading here:

 &gt; the SQL data structure follows a structured pattern to store the data. ",9.0
g5g99dq,itmdtq,The difference is that MongoDB is webscale,5.0
g5gj99p,itmdtq,It scales right up.,1.0
g5gft7o,itmdtq,"I’ll give a shot at a TL;DR.

Storing customer name? Need info like customers phonen umber? Need to find sales related to only that customer? Hate having many rows for that one customer because the customer has multiple orders and multiple phone numbers?   SQL is the answer.

Want to fetch data about weather? Do not care that the rows have columns with duplicated value? NoSQL is the answer.

I am a beginner, so feel free to correct me.",1.0
g5g4d9w,itkorj,"Yup. Something like this:

    ALTER TABLE table1_name
    ADD FOREIGN KEY (foreignKey_name) REFERENCES table2_name(primaryKey_column);",2.0
g5f4uey,ithsre,[deleted],1.0
g5f5jhy,ithsre,"Nah, it's actually a javascript spread operator.  The SQL query is actually on the variable const sql.",1.0
g5f6wu0,ithsre,"I've had some issues in react/express, where it didn't like line breaks in my sql statement. I had to make the entire sql statement on 1 line.

You should console your sql const to see what it created. Easier to debug.",1.0
g5f7qtb,ithsre,"That's actually why I put my whole SQL query on one line already, just in case for whatever Node/Express was being weird lol unfortunately still getting that error",1.0
g5f83pt,ithsre,"Oh then I see. Its your replacement. 

I wish I had covered more of that in this vid https://youtu.be/6PchXL6rySA


But the code is: 

connection.query(""SET @SubjectFamilyId = ?, @SubjectFamilyGroupId_Tenant = ?"", [req.body.SubjectFamilyId, req.body.SubjectFamilyGroupId_Tenant], function (
(error, results, fields) {
   if (error) res.end(JSON.stringify(error));
	  res.end(JSON.stringify(results));
	});
});",1.0
g5f8k7k,ithsre,"sorry u/VuewsFromThe6  i wanted to get the code specifically from my .js and not try to mangle it on mobile.

edit: maybe const Customer should return [val,val,val,val]  see if that works.

edit 2: yea.. im pretty sure that is what you have to do.

const customer = () =&gt; {

blah blah

customerArray = [customer.customer_id,customer.contact_title, etc, etc]

return customerArray
}",1.0
g5fc42j,ithsre,"And that was my issue!  I was thinking that the statement would be able to recognize the object values, so just taking those values and making a new array of values made the insert work.  Thanks for pointing me in the right direction!",2.0
g5fcrzu,ithsre,No probs,1.0
g5f9zdi,ithsre,I was thinking it might actually have something to do with the javascript since it was working in workbench!  I'll try this and see if this does it for me,1.0
g5fa30y,ithsre,"The mysql error has ? In it.. tells me it ain't replacing it. I'm no js guy, but it looks like that customer is an object.. not an array.",1.0
g5djlms,itctxa,"So Data Analytics and SQL.

Corporate Data is stored in a database. You will most likely have various tools at your disposal to analyze that data, but you often need to write a query in SQL to access that data and pull specifically what you want. Otherwise, you'll be grabbing too much data, and wasting your time cleaning data before you can analyze it.

SQL or Structured Query Language is pretty easy to learn the basics yourself. Type in SQL turorial and try any number of them to learn the basic commands. Focus mainly on the SELECT side of queries, as you probably won't have write access to the database. I use SQL every day at my job, and it saves me countless hours of pulling data, and doing pivottables.

**Good Tutorials:**  


* [https://www.w3schools.com/sql/](https://www.w3schools.com/sql/)
* [https://www.tutorialspoint.com/sql/index.htm](https://www.tutorialspoint.com/sql/index.htm)
* [https://www.sqlcourse.com/](https://www.sqlcourse.com/)
* [https://sqlzoo.net/](https://sqlzoo.net/)

  


SQL server is Microsofts Enterprise Database Management System. For SQL Server, you'll want to link this section for MS specific SQL functions and information. [https://docs.microsoft.com/en-us/sql/t-sql/functions/functions?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/t-sql/functions/functions?view=sql-server-ver15)",3.0
g5ekfls,itctxa,"Thanks!

So let's say there's an IT service desk and people submit requests to a system. You would need to write  query in SQL to get the data I would want to see because there'd be loooads of data I wouldn't want?

By various tools you mean like Power BI or Tableau?

I think the tableau part of how to set up views etc would be simple enough, it's the accessing the data with SQL (or whatever else) and publishing it to a location that I don't really have a clue on. Is that kind of difficult?",1.0
g5erpn4,itctxa,That really depends on your company’s data and how they structured it.,1.0
g5fk4y6,itctxa,Wasn't turorialspoint spamming the living crap out of all database related subs last year?,1.0
g5fklui,itctxa,I didn't follow that closely. Maybe,1.0
g5dmy40,itctxa,I am more curious about your background. It's hard to give good advice without having some idea of the persons background.,1.0
g5ej4sz,itctxa,"Well, that's kinda a tricky question! :D

I am the excel data guy.

I make graphs and stuff in excel, formulas, conditional formatting...that kinda thing.

Have written some VBA but very specific and with the use of google mostly.",1.0
g5gqk0q,itctxa,"OK I had the same background years ago and had to learn databases the hard way using MS Access. 

If you are familiar with VLOOKUP then you are familiar with JOINs. There were times in Excel where I had to concatenate multiple columns from one data source to multiple columns in another data source to use VLOOKUP.  In SQL that would be joining two tables on multiple fields.

One of the hard concepts for me to bridge is knowing when to use a calculation instead of a field in a table. In Excel I would create lots of columns with formulas but in SQL you would very rarely store the results of a calculation in a table. Also Excel lets you abuse data types. For instance in a column of date users may enter TBD or some other text. In SQL a date must be a date.

Since you have written some VBA you will have some idea of putting things together in a sequence. 

If you are more comfortable in Excel and since your source of data in SQL Server you may be better served to learn Power Query. PQ is now part of Excel. I am taking a class on Udemy for PQ and I can't wait to use the new features. PQ masks a lot of the coding you would do in SQL. However I have found that SQL is much faster for basic manipulation of the data than using PQ's M language.

As far as reporting I used MS Access for years. You can do amazing layouts and tables etc. with MS Access. The BI tool for that is SQL Server Reporting Services or SSRS.

You best bet is to take some on-line classes others have mentioned and build from there. I personally love PQ as I can develop a query in SQL Server Management Studio (SSMS), then copy and paste that query into the PQ and it builds the table in Excel for me. Then from there I do pivot tables etc.

Good luck",2.0
g5g4vzv,itctxa,"In your case, SQL is a great tool for limiting and cleaning the data before you import it to excel. If you just bulk copy/paste the data from the company database to an excel spreadsheet, it will slow your spreadsheets and dashboards down greatly. Have you worked with Microsoft Access before?",1.0
g5ga42q,itctxa,No I haven't.,1.0
g5dusf4,itctxa,Are they offering any training? Seems strange for your company to move you to a position you don't know much about.,1.0
g5eiyb3,itctxa,I have been asked to find training I might need so will probably do an intermediate training in Tableau.,1.0
g5dcn2k,itc4av,"Hello u/waitinthefog - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g5dd3uj,itc2re,"there is no difference -- `AS` is an optional keyword

yes it is a kind of renaming -- they're called **table aliases**

there can also be column aliases",9.0
g5ddic1,itc2re,"There's no difference in the two. Leaving out the `AS` is simply short-hand, though it is generally best practice to use `AS`. 

&gt; Also, is it kind of ""renaming"" ?

The canon term for it is called [aliasing](https://www.w3schools.com/sql/sql_alias.asp)",4.0
g5dgkjl,itc2re,Clarity,3.0
g5ddjhq,itc2re,As,5.0
g5dvwdh,itc2re,Got it. Thanks all.,1.0
g5dxhig,itbx48,"You just need to

    SELECT SUM([Total])
    FROM
    (
    SELECT [Total]
    FROM
    (
    yourfirsttable
    ) a
    UNION ALL
    SELECT [Total]
    FROM
    (
    yoursecondtable
    ) b
    ) c",6.0
g5ekuw3,itbx48,You're a freaking genius.,1.0
g5dyqla,it9x0p,"My unscientific advice:

* Share your anxiety with your interviewer. Tell them you tend to get nervous whiteboarding. It's no great shame, many people do, and it gives you some breathing room.

* Begin by restating the problem in ""pseudo SQL"" form, as in don't start writing SQL right away, just explain it in English what you're selecting, what the join and filter conditions are, etc. Again, it's good practice to demonstrate understanding the requirements before jumping to a solution.

Do this even if you already know the answer they want! It will help you organize your thoughts and establish a baseline for the more difficult questions.

* if you get stuck, break the problem down to the smallest possible next step, snd write out queries to solve that. Simple example, but if they ask you to return the 2nd highest paid employee in dataset, and you're stuck, showing you know how to sort a dataset by a column is valuable. 

This demonstrates that you can write SQL, you can solve problems iteratively, and you know how to get ""unstuck"".

* Ask for help when you need it. The best way to do this is to 1) tell them what you think the right answer is, and then ask for confirmation.

Example: ""I *think* I'm supposed to use LAG here, but I dont recall the exact syntax, can I just write a general pattern here?""

This shows you can collaborate well, you understand the core ideas of their questions, and you're not afraid to ask for help.

Finally: while many interviewers and companies use whiteboard exams as opportunities for collaboration and seeing how you will work with their team, some of them treat it as a black-and-white ""do you know this or not"" closed-book exam.

I personally wouldn't want to work for the second kind of company, and if I'm being asked to participate in a Whiteboard session, I ask if it's expected to be a collaborative session or if it's a pure ""exam.""

So that being said, if you do go into a whiteboarding interview, and it turns out to be a silent slog of knowledge and syntax, just grin and bear it and don't sweat it if you bomb, as it's probably a terrible place to work.",1.0
g5lzeqd,it9x0p,"Just wanted to give an update: interview went well!

When it came to the technical stuff, it was kind of a run-down of general SQL related questions (types of joins, how indexes work, etc.), and then for the white-boarding session, they presented a really ugly query that would technically run, but wanted to know what I would change to improve it.  I was able to identify a bunch of stuff, and we ended up having a really good conversation.

Thanks for the advice and confidence boost!",2.0
g5es41v,it9x0p,"Thanks for taking the time to write up all of those tips- it means a lot!  The manager of this team seemed really chill, but she did warn that they needed someone with 'advanced' sql knowledge for this role, and that scares me a bit on top of the white-boarding.

I think you are totally right; write out my thought process, make sure I am tackling the query problems in smaller chunks, and don't be afraid to ask for help where needed.  I also think that you are right about the way the white-boarding session will go on... if it is just 'answer this, now answer that, also can you answer this?' without quality discussion along the way, it is probably not a great place to be at.  

Thanks again for the tips!",1.0
g5cy00m,it8xr2,"Hi new accidental DBA. First of all, look for accidental DBA on google.  You will find a lot of people like you. 

1 For standalone it is almost as simple as a KB. They are inclusive so you only need to install the latest. Do make full backups of your databases tho. Snapshot have limited use. Fine to restore your operating system, but unless your sql server was down during the snapshot, recovery may lead to more troubles instead of less. For a cluster it is a bit more complex. Read the documentation on how to do it. Or at least make sure that the patch is applied on both. Do not think:""I will do the next one tomorrow"". For rollback de-installing the cu is often good enough. If not, I would build a new sql server rather then rolling the snapshot back. But then I can deploy a new server rather fast thanks to scripting and knowing my environment. 

2 Honestly never downgraded a SQL server. I would just make a full backup of each db and restore them on the new standard SQL server. Do not take a snapshot. You are just wasting your time. Just move them one by one and tackle any problems with testing. There are a lot of things that can go wrong, from jobs not taken along, or old scripts running on a remote machine somewhere. And then there are the accounts, possible certificates, linked servers etc.. You will not find them all, so painfully discovering things you missed, is something you should expect.",11.0
g5dbhxh,it8xr2,"&gt; Do not take a snapshot.

To echo this: 

Honestly, just don't do snapshots in general. If you're up shit's creek enough where you're falling back on snapshots to do SQL Server restores, then there's a good chance I wouldn't ever trust whatever machine that snapshot was to not have messed with or corrupted the system databases. 

In SQL Server's case, it's just much more reliable and will be less cause for OP to have headaches in the future if they have an actual SQL Server backup maintenance plan setup and to not rely on snapshots at all.

Spin up new server/instance and restore from backup. Restoring from snapshot is way too janky.

/u/sarcasm_r_us - see if your company will pony up the cash to get you some proper training as a SQL Server DBA and check out [Brent Ozar's](https://www.brentozar.com/) website. He offers both paid video training and free training, as well as subscribing to his newsletter/blog for articles on SQL Server DBA stuff.",6.0
g5ds3xy,it8xr2,"These are both great replies. There are full guides on being an Accidental DBA and I'm pretty sure Red Gate has a good one. Ozar has written about almost everything and is pretty much always right with good explanations why.

In my opinion, your priority should be: security, backups and learning to restore full/t-log/differential, regular maintenance, data delivery and DR, current clients / new product initiatives, proper query design and database architecture, data warehousing, learning how the optimizer works and to tune for OLTP then OLAP, and actually implementing this into new architecture and design standards - in that order (mostly based on how important it is vs how long it can be left unlearned or unaddressed).

If you find articles by any of these people, you can probably trust them: Paul Randal, Paul White, Brent Ozar, Adam Machanic, Kendra Little, Eric Darling, and Gail Shaw. There are a lot more, but that's off the top of my head. Pinal Dave is usually good, but doesn't go into enough detail for me and ignore W3 Schools. MS SQL Tips, SQL Server Central's stairway series and questions of the day, and SimpleTalk are good once you have a decent handle on your day to day.

Good luck!",6.0
g5gngmx,it8xr2,"Aww, thanks!",3.0
g5gu0ew,it8xr2,"And thank you for so many years of sharing your expertise and training us! I particularly loved the 201 Buckets recently, it helped fill in a big gap in my understanding of stats and histogram distributions.",2.0
g5kmn77,it8xr2,My pleasure!,1.0
g5dtrvi,it8xr2,hey would you mind to clarify what would happen after taking a server snapshot with sql server running? I always stop all sql services before the snapshot but I'm curious about what can happen. thanks,2.0
g5dxmaq,it8xr2,"If you stop the services you are fine. When you have the services up and running then there are risks. Active sessions could be running, including internal and those will be rolled back. Data loss is of course the first issue, but you could be unlucky and interrupt a big update/insert/etc. The roll back could take way longer then you like. And I have seen issues with databases just hanging in the recovery state, but that was a long time ago and I was a fresh accidental DBA at that time. People that like snapshots have told me that they have gotten better, so might have been me or the old snapshot software.",2.0
g5dds4n,it8xr2,"In the same boat being relatively new.  Balancing Oracle, MSSQL (along with SSIS) and Mongo.  It’s a struggle.  I wish you the best, friend.",5.0
g5dsi5g,it8xr2,I wish both of you the best. You're dealing with quite a handful for someone who's pretty new. That's three separate and very different types of environments with a lot of unique headaches between them.,5.0
g5e6o8s,it8xr2,"For 2. Do what others have said. Stand up a new Standard environment, and then I'd reccomend looking into [dbatools.io](https://dbatools.io), a super handy powershell extension for sql server, Particularly the [Start-DBAMigration](https://docs.dbatools.io/#Start-DbaMigration) cmdlet. Super easy and super powerful. I used it when upgrading a 2012 sql server to 2017, and it made it a breeze. 

I keep the [dbatools.io](https://dbatools.io) in my back pocket now a days. they have a bunch of good little things to help automate stuff and learning powershell is never a bad thing.",2.0
g5efqjn,it8xr2,"Welcome, Accidental DBA!  Yes, that's a popular term, and it's where most of us start.  Usually the Accidental DBA is selected as ""the person sitting closest to the server"" but ""didn't run fast enough"" is also a good one.  :)

You asked about basic stuff.  Do yourself a favor, hook yourself up with the ""First Responder Kit"" here (it's a freebie):

[https://www.brentozar.com/first-aid/](https://www.brentozar.com/first-aid/)

Install it and run Blitz.  It'll give you a nice, prioritized list of things that need to be looked at.

While you're getting that, also sign up for the 6-month DBA training plan (on the same page).  It'll cover the foundations you need to not lose the data, and his presentation is top notch.  There are many authors of Blitz, but many people consider Brent to be the source.  It's a few minutes a week (or you can blast through it faster on his blog).

He has some paid stuff too - well worth it, especially if you can get the company to expense it.",2.0
g5eg13i,it8xr2,"For #1, CUs are pretty well vetted and the 2016 ones have been out long enough to be well documented.

For #2, I've used the uninstall/reinstall method successfully a few times:

https://www.mssqltips.com/sqlservertip/3079/downgrade-from-sql-server-enterprise-edition-to-standard-edition/

Just have full backups and documentation on users, passwords, security, etc... in case you need to restore from scratch for some reason... but as long as you do it correctly it's pretty foolproof.",2.0
g5f4ff9,it8xr2,"Another bright side is that you can now spell D-B-A, correctly. :)",1.0
g5kkcww,it8xr2,"Welcome to being an accidental DBA and the good news is, data is critical to every business so the more you know, it can help to boost your career long term. In the short term, if you need support with the Patching and SQL Downgrade, please respond to me and will be happy to set up a call - we need a little more info than what was provided in the post to get you the right answers! Also, you being anointed the ""DBA"" happens all the time and so much that I am speaking about this at the largest SQL Server annual conference -- this year it will be virtual -- PASS Virtual Summit 2020 ([https://www.pass.org/summit/2020/Learn/Session-Details/name/congratulations-you-are-now-a-dba/sid/104654](https://www.pass.org/summit/2020/Learn/Session-Details/name/congratulations-you-are-now-a-dba/sid/104654)) 

This session is called ""**Congratulations, You Are Now a DBA""** and will provide you with a road map for succeeding as a DBA. We will cover all of the basics that the typical DBA needs to know and focus on Day 1 and Year 1. By the end of this session, you will know what is important and have your daily, weekly and monthly checklist for administering SQL Server. Once you learn the basics and you are comfortable with the DBA role, you are in luck long term as data drives decisions and businesses. If you are attending the conference, please check out this session. Or if you can't attend the session and would like for me to send over some helpful materials or set up a call to talk through your issues -- please don't hesitate to reach out to me or check out resources on our site here: [https://fortifieddata.com/blog/](https://fortifieddata.com/blog/)",1.0
g5q7ctv,it8xr2,Can someone help me with school project,1.0
g5ct8pn,it819g,"So, I'm assuming you don't literally care about using an array (though you could probably do that too), and that your main goal is you just don't want to generate a subquery per input date.

You could use a VALUES list and plop all your dates right in there and join, for a first step...

Here's what a VALUES query looks like:

     
    mwdb=# SELECT * FROM (VALUES ('2020-02-01'), ('2020-03-01'), ('2020-04-01')) AS t(input_date);
    
    input_date 
    ------------
    2020-02-01
    2020-03-01
    2020-04-01
    (3 rows)

So we can join `my_table` to the above query, the key piece being the join condition of `the_date &lt; input_date`. We want to, as a first step, eliminate any `the_date`s that are equal to or greater than the input. (I renamed your column `date` to `the_date` . Hope you don't mind.)

    mwdb=#
    SELECT *
    FROM my_table mt JOIN
      (SELECT * FROM
        (VALUES
          ('2020-02-01'::date),
          ('2020-03-01'::date),
          ('2020-04-01'::date)
        ) AS t(input_date)
      ) input ON mt.the_date &lt; input.input_date;
    
     id  | color  |  the_date  | input_date 
    -----+--------+------------+------------
     001 | red    | 2020-01-13 | 2020-02-01
     001 | red    | 2020-01-13 | 2020-03-01
     001 | red    | 2020-01-13 | 2020-04-01
     001 | green  | 2020-01-02 | 2020-02-01
     001 | green  | 2020-01-02 | 2020-03-01
     001 | green  | 2020-01-02 | 2020-04-01
     001 | blue   | 2020-02-27 | 2020-03-01
     001 | blue   | 2020-02-27 | 2020-04-01
     001 | white  | 2020-02-16 | 2020-03-01
     001 | white  | 2020-02-16 | 2020-04-01
     001 | black  | 2020-03-02 | 2020-04-01
     001 | orange | 2020-03-10 | 2020-04-01
     002 | blue   | 2020-01-30 | 2020-02-01
     002 | blue   | 2020-01-30 | 2020-03-01
     002 | blue   | 2020-01-30 | 2020-04-01
     002 | black  | 2020-01-10 | 2020-02-01
     002 | black  | 2020-01-10 | 2020-03-01
     002 | black  | 2020-01-10 | 2020-04-01
     002 | purple | 2020-01-14 | 2020-02-01
     002 | purple | 2020-01-14 | 2020-03-01
     002 | purple | 2020-01-14 | 2020-04-01
     002 | pink   | 2020-02-13 | 2020-03-01
     002 | pink   | 2020-02-13 | 2020-04-01
     002 | green  | 2020-03-24 | 2020-04-01
     002 | white  | 2020-03-23 | 2020-04-01
    (25 rows)

Now we can get the greatest value of `the_date`  per combination of `id` and `input_date`.

First I'm going to put the above query into a CTE for convenience, and add a ROW\_NUMBER():

    WITH step1 AS (
      SELECT *
      FROM my_table mt JOIN
        (SELECT * FROM
          (VALUES
            ('2020-02-01'::date),
            ('2020-03-01'::date),
            ('2020-04-01'::date)
        ) AS t(input_date)
      ) input ON mt.the_date &lt; input.input_date
    )
    SELECT *, ROW_NUMBER() OVER (PARTITION BY id, input_date ORDER BY the_date DESC) AS rownum
    FROM step1;
    
     id  | color  |  the_date  | input_date | rownum 
    -----+--------+------------+------------+--------
     001 | red    | 2020-01-13 | 2020-02-01 |      1
     001 | green  | 2020-01-02 | 2020-02-01 |      2
     001 | blue   | 2020-02-27 | 2020-03-01 |      1
     001 | white  | 2020-02-16 | 2020-03-01 |      2
     001 | red    | 2020-01-13 | 2020-03-01 |      3
     001 | green  | 2020-01-02 | 2020-03-01 |      4
     001 | orange | 2020-03-10 | 2020-04-01 |      1
     001 | black  | 2020-03-02 | 2020-04-01 |      2
     001 | blue   | 2020-02-27 | 2020-04-01 |      3
     001 | white  | 2020-02-16 | 2020-04-01 |      4
     001 | red    | 2020-01-13 | 2020-04-01 |      5
     001 | green  | 2020-01-02 | 2020-04-01 |      6
     002 | blue   | 2020-01-30 | 2020-02-01 |      1
     002 | purple | 2020-01-14 | 2020-02-01 |      2
     002 | black  | 2020-01-10 | 2020-02-01 |      3
     002 | pink   | 2020-02-13 | 2020-03-01 |      1
     002 | blue   | 2020-01-30 | 2020-03-01 |      2
     002 | purple | 2020-01-14 | 2020-03-01 |      3
     002 | black  | 2020-01-10 | 2020-03-01 |      4
     002 | green  | 2020-03-24 | 2020-04-01 |      1
     002 | white  | 2020-03-23 | 2020-04-01 |      2
     002 | pink   | 2020-02-13 | 2020-04-01 |      3
     002 | blue   | 2020-01-30 | 2020-04-01 |      4
     002 | purple | 2020-01-14 | 2020-04-01 |      5
     002 | black  | 2020-01-10 | 2020-04-01 |      6

So we've assigned a number for every partition of `id` and `input_date`, where 1 is the maximum value of `the_date`. We only care about `rownum=1`; the rest could be random garbage for all we care. Remember we already joined on only values of `the_date &lt; input_date`, so this maximum `the_date` is the row we want.

So we just add a WHERE clause for `rownum=1`:

    WITH step1 AS (
      SELECT *
      FROM my_table mt JOIN
        (SELECT * FROM
          (VALUES
            ('2020-02-01'::date),
            ('2020-03-01'::date),
            ('2020-04-01'::date)
        ) AS t(input_date)
      ) input ON mt.the_date &lt; input.input_date
    ), step2 AS (
      SELECT *, ROW_NUMBER() OVER (PARTITION BY id, input_date
        ORDER BY the_date DESC) AS rownum
      FROM step1
    )
    SELECT id, color, the_date FROM step2 WHERE rownum=1; 
    
     id  | color  |  the_date  
    -----+--------+------------
     001 | red    | 2020-01-13
     001 | blue   | 2020-02-27
     001 | orange | 2020-03-10
     002 | blue   | 2020-01-30
     002 | pink   | 2020-02-13
     002 | green  | 2020-03-24
    (6 rows)

And that looks like it matches your sample ""correct"" output.",3.0
g5cxdgm,it819g,"Hey, thanks a ton for this, this is a pretty nice answer.

I think can improve your result; instead of partitioning at the end, we can simply do a `SELECT DISTINCT ON (ID)` with an `ORDER BY the_date DESC` and it'll grab the right rows.",2.0
g5f6h8h,it819g,"Cool! I would be interested in seeing the final, working query if you don't mind. I rarely think to use PG's `DISTINCT ON` feature. :)",1.0
g5cfklj,it6kwd,"I would absolutely look for a different POS package. You laid out the reasons well already. 

Running unpatched software connected to the internet is asking for trouble, and that seems to be a requirement for them.

Sql Server sounds like the least of the problems with that system.",27.0
g5cgpa5,it6kwd,"&gt;Sql Server sounds like the least of the problems with that system.

Well I guess that is both good to know and a bit concerning.

I wonder how many uses SQL Server 2005, but it might not be that unusual.  
So often do I read about corps using ancient software and hardware because it is so hard to keep up with new tech. I guess in this particular field, there are at least competition and good reasons for better solutions.  


I will definitely keep this in mind and see where we land at.",3.0
g5chxlx,it6kwd,"&gt; So often do I read about corps using ancient software and hardware because it is so hard to keep up with new tech

IMO this is a really weak excuse. SQL Server has a support lifespan of 10 years. Windows Server, about the same (maybe a bit longer even). Desktop Windows a bit less, but Microsoft puts a _lot_ of work into backwards compatibility.

All of which is to say that if it's ""too hard to keep up"" with a change _once every ten years_ to stay on current software, maybe the organization needs to find new people to manage the environment (including the option of outsourcing their IT group), migrate to the cloud so a lot of those ""no longer supported"" concerns evaporate, or just get out of the business altogether.",5.0
g5cij4o,it6kwd,"&gt;(including the option of outsourcing their IT group)

I think the supplier already did for some of the cases. They just could not handle our (and many other retailers) request of implementing a solution for handling 3rd party accounting software.  
Up until recently we had to manually put in the daily reg. closure report into our accounting software. It took them years to figure that out even though all the accounting software suppliers provide api\`s to handle that IIRC.",1.0
g5f5p8r,it6kwd,"Find a new POS vendor that actually gives a damn about their customers. Whatever pain/cost is incurred will be far outweighed by the gains from getting to current software, that works properly, on supported platforms, with a vendor that sees you as more than a steady paycheck.",2.0
g5emfi8,it6kwd,"It's not hard at all. The greed is what gets them. Companies will cut whatever costs they can to increase profits. IT gets cut ALOT because the owners usually don't fully understand the consequences of not keeping up with technology. Then the really funny part is when they get upset when their technology no longer meet their requirements and they start blaming the software. Now that were in the connected age, you can't get away with running the same old software forever even if it does meet the requirements, eventually, it won't meet the requirements of the interwebz and it'll stop working or worse become a terrible risk or catastrophe.",1.0
g5f5k3b,it6kwd,"But we're talking about _a software vendor_ that isn't putting in any effort to keep _their own product_ that they sell to customers up to date. Not OP's company.

The vendor is putting in minimal effort and just cashing checks from customers who don't have the wherewithal to move to something better, nor the ""pull"" to make the vendor do better.",1.0
g5ficca,it6kwd,"That’s true, but how’d it get like that? Who chose that company and package in the first place and what were the determining factors? 

I do agree that it’s the vendors issue but I think many companies in general get themselves into situations like this by choosing lowest cost vendors and not properly spending the time to choose the right software/vendor. 

My company made some choices that were incredible and paid off immensely although expensive and others that were lowest cost we still pay for in various ways everyday whether in cost or productivity. 

Worst thing is the employees just end up having to deal with the frustrations that come along with it. 

So like sorry nothing against op or anyone here. I just think that’s a valuable thing to say. Take your time and don’t choose the lowest cost simply cuz it’s cheaper. Set the requirements and make sure the software and support provider can meet them. It’s not an easy job and It’s usually rushed.",1.0
g5ciaii,it6kwd,"Hell yes, you should expect more from your ""supplier."" If you're still paying them to use this software, find new software. They aren't providing you with adequate support (by your own admission), you've lost sales because of them, and they're putting in zero effort to keep their software current. If they don't have a version that supports newer versions of SQL Server, they're not going to sell any more product at this point because you can't acquire 2005 or even 2008/2008R2 readily anymore, nor operating systems that will support them.

You're paying this supplier for a big box of _nothing_.

There's lots of POS systems out there, cut your losses and move to one that has a future.",8.0
g5cybg3,it6kwd,"SQL Server is not the concern here. The fact they're running 2005 shows they don't care about maintenance or security. For reference, I'm running a super legacy VB app with sql server but it's running on a win 2019 box with sql 2019",7.0
g5d29nr,it6kwd,How are you doing POS without being even remotely PCI compliant (unsupported and unpatched systems)?,5.0
g5ehaqm,it6kwd,"Yeah... This is wild.

If they are running SQL Server 2005, I'd venture a guess that no one competent has seen or touched their codebase in nearly a decade, let alone done any sort of security audits that a POS system would usually have to perform.",3.0
g5ellwa,it6kwd,I honestly can't believe they're still allowed to accept credit/debit cards unless they're using a third party device like a Veriphone system.,1.0
g5f9qda,it6kwd,That was my first thought.  There's no way they can have a system that stores any kind of payments and be compliant.,2.0
g5crrgp,it6kwd,"With as many POS providers out there, including a ton of newer startups, there is no excuse for being on Win7 or SQL2005.

Depending on how big your store is, start requesting proposals from vendors and see what you find. Never hurts to see what is out there. 

If you are smaller - start asking for demos or find stores (maybe not in direct competition) that are around your size and see what they are using.",4.0
g5d1h12,it6kwd,"1.) Just having Windows 7 is a HUGE security concern. Utilities like Konboot can outright override local passwords (at least that's how I did it in like 2011), and where it's been EoL for a while, who knows what other exploits have been found. As a short-term measure, I'd highly consider switching to Linux and then running the Windows 7-only software in either a VM or Wine so you can have better security around it, even if you do have to use Windows 7 eventually.

2.) You absolutely should expect more from your vendor. Windows 10 came out in 2015, and while I can understand the chaos surrounding Windows 8, there has been more than enough time for their engineers to get with it.

3.) Is SQL Server itself running on Windows 7 as well? Where Microsoft has Docker images for it (the 2019 one anyway, Linux support starts with 2017), you should be able to use whatever host OS you want within reason (maybe not BSD lol), and so you could easily get Linux, and then deploy that in a container, and migrate your data, and I can't see why that would cause an issue, though ofc test it first. I'm not sure how licensing for that would work, but I can't think of a reason for a system to need the old version specifically aside from that.",3.0
g5f6edg,it6kwd,"&gt; Is SQL Server itself running on Windows 7 as well?

If it is, I'm pretty sure it's a license violation (using Win7 as a server).

Licensing for SQL Server on Linux is the same as on Windows. Docker isn't entirely appropriate for production (it's better for dev/QA environments where you need to deploy &amp; tear down temporary instances quickly) but I'm sure someone out there is doing it.",2.0
g5cxksv,it6kwd,"You’re fine with SQL server 2005 most likely. It isn’t ideal but that would be the least of my worries. It’s highly likely you could upgrade to a newer version and your POS wouldn’t notice the difference unless they chose to implement very specific 2005 features that have been deprecated. My experience is that very very few software applications do that.

My real concerns are that you are stuck with a POS that is a POS. The fact that their front end can’t run on Windows 10 is a massive red flag. The UI design sounds dated and probably costs you money in terms of lost efficiency.

I would be making some archival backups of my SQL data and then start shopping around for a new POS and be prepared to be slapped in the face by reality when you learn that migrating your existing data from your current platform to a new one is either very difficult or impossible.",3.0
g5f6425,it6kwd,"&gt; You’re fine with SQL server 2005 most likely.

Not if you're handling credit cards.

&gt;It’s highly likely you could upgrade to a newer version and your POS wouldn’t notice the difference unless they chose to implement very specific 2005 features

Or worse - they explicitly coded in a version check and it'll fail to operate on any other version.

Since OP is in Europe, they're probably subject to GDPR to some degree. GDPR requires that you patch known vulnerabilities. There's at least one in the system that's being used here, I'm sure.",1.0
g5fd33y,it6kwd,"Fair point. 

Let’s be honest... there isn’t any POS software that is 10+ years old that is safe for credit cards or customer data. The retail industry’s track record on that is abysmal.

Still, I’m less concerned about the SQL 2005 installation than I am about the rest of the platform.",1.0
g5d85uk,it6kwd,It truly is a POS system.,2.0
g5fi0e7,it6kwd,"To add to the comments, the only reason I can imagine something still being on SQL Server 2005 is out of pure laziness.  Unless the system is using some of the commands that were removed in later versions (a very short list), or doing some extremely complex data manipulation (unlikely), there isn't a valid reason to not upgrade it.  Odds are that it will actually run fine, but they just haven't tested it.",1.0
g5cwaiv,it61tv,"give your debts and deposits consecutive ranges (using running totals), so debt#1 is 0..100, debt2 is 100..200, debt3 is 200..250, etc. deposit1 is 0..75, deposit2 is 75..195, etc.

once you have this, it's a join on the range overlap to see what portion is paid out and to which debts/deposits it applies",1.0
g5evrhr,it61tv,This is the kind of ingenuity I was looking for. Thank you so much!,1.0
g5cg1ou,it2n83,"Depends on how your app is constructed to be quite honest. 

Do vendors access the app through a different portal? If so, I'd recommend a separate table entirely. 

Do they access through the same portal, but then have a different version of the app based on user type? If so, then I would simply have a Vendor BIT field, and mark vendors as 1.

EDIT: For clarity, your database structure should support your application structure directly. Don't over complicate your database unnecessarily, but don't just cram everything into a single table if it doesn't make sense.",3.0
g5djuxw,it2n83,"They would access it through the same app but have access to different pages as well. Creating a different table is a problem even if they were using a different app because for things like messaging for example where the sender and receiver are both users how would I know whether one should be a foreign key from the customer or vendor table? I would need one uniform users table for all users would I? 

So you’re saying in my case given that they will have some different parts of the app I should set a BIT for a column called Vendor. How does this solve my problem of unrelated data. For example data related to only vendors such as company information etc should not be stored in this user table since customers will always not have those columns filled. This would seem like bad design",1.0
g5dme1n,it2n83,"You have a separate vendor table then. 

All users, including vendors, are in the User table. Vendors have an additional ID (VendorID) that relates to the vendor table. 

Data pertaining specifically to vendors is stored here, without clogging up the User table. 

Users who are only Customers will have this ID as NULL, and you can use this to determine if a User is a customer or a vendor.",1.0
g5eljp9,it2n83,"That would work if the customer didn’t have additional columns that don’t apply to the vendor. For instance I want to track the customers (only customers) location when they open the app and additional info about their car etc. Then I would have to set those columns to null for vendors, that seems like bad design.",1.0
g5ep0ee,it2n83,"You do the same for customers, another table. 

This is where SQL is really strong. You have a user table that handles login and user interaction, which has a customerID and a vendorID that will link off to customer and vendor tables respectively. 

Thise ID's will be nullable on the user table so that you can leave them blank when necessary.",1.0
g5c3j9n,it2n83,[deleted],1.0
g5c4kl6,it2n83,there’s no roles table it’s a roles column in the users table just to know what role they belong to and the two tables with the userid fkey are for data specific to customers or vendors like vendor company name details Etc that wouldn’t make sense to have all under users table since it won’t apply to all users,1.0
g5cdnzi,it2n83,"&gt;two main types of users, a customer and a vendor or service provider

This was confusing to me without an Oxford comma. I assume you mean that there are users (of which there are two types), customers,  and  vendor/service provider. Rather than my other interpretation that there are two user types: one called 'Customer', and the other called 'Provider'? A varchar column named UserType on a singular User table should work fine. 

&gt;if for some reason I need to join the corresponding customer or vendors table from a query on users I can’t

Why not? There's a UserID FK. I don't understand what you mean.

I'm struggling to understand the objective and the requirements from your post. If you can provide more details, maybe it will make more sense to me. You asked if it's more efficient to have one user table or two. Without any information, what if I just assume you have one record total for each user type? Both are plenty efficient in that case.",1.0
g5cenb5,it2n83,I don't see the need for an oxford comma as much as a colon instead of the existing comma. It was not a series.,3.0
g5cevp4,it2n83,"If I understand his hypothetical database correctly, there is a User table with two types of users, a Customer table, and a service/vendor table... right?",1.0
g5cfz0l,it2n83,"From what I see OP wants to know if he should make one or two tables.  Either a table that users and vendors are both listed on, or a unique table for each.

IDK how much overlap there is, I imagine name number address, but no idea what all they have.",3.0
g5cul02,it2n83,"Okay, thanks for the clarification.",1.0
g5dl8sj,it2n83,"I have updated my post to clarify a bit more. But what I was thinking was I need a table users for ALL users. Then two other tables related to users through a foreign key of user Id. These tables would therefore connect to the basic user info for all app users but include user type specific info like vendors would have company info and customers may have something else related to their role as customer.

What I mean with the joining is just that say for example I only have the user Id then if I wanted to get their vendor or customer info Id first query the user table to find if they are a customer or vendor. Say I get back that the role value from the role column in user table is customer then I’d have to make another query passing in the user Id to customer table to get back the relevant customer info. So basically two queries but I don’t think that’s a big issue as generally I will know if a user is a customer or not based on their location on the app and that can be managed through state.",1.0
g5ck42m,it2n83,"I like having a company table which joins to a contacts table. Vendors, customers, etc are stored in the company table
 and individual persons are in contacts. Manage access on the contact level. This allows companies to act as vendor and customer if necessary and a role table can manage application behaviour on the company or contact levels. I usually have contact level permissions overriding company level ones.",1.0
g5dlvlr,it2n83,"I’m confused, you have both customers AND vendors under contacts? For my case the customer would never be a company it would always be one individual user so I don’t see how it would work if I put both under company. Could you correct me if I misunderstood?",1.0
g5dnkfq,it2n83,"They're both under company. Each company has attributes like location, company name, etc.

And company has multiple contacts. Each contact references a company id.

We basically have hierarchy with two levels. The organization level and the individual level. 
&gt; For my case the customer would never be a company it would always be one individual user so I don’t see how it would work if I put both under company.

In your case it looks like you never have a customer that is an organization. It's always an individual.

I would probably still have a hierarchical view where instead of putting these customers into their own organization, I'd use the company table to create a dummy organization to handle individual customers.

There are probably purists in here that would find that method of modelling abhorrent since it misrepresents that relationship by saying that there's a commonality between your customers that may not necessarily be true.

One way to get around that would be to rename the company table to something that generalizes the relationship between your users and the grouping that they would belong to. 

You could then create roles and permissions that would apply to all customers by assigning it to the dummy organization. And individual customers can be assigned specific roles and permissions as well.",1.0
g5bqv4f,it1ql5,"Moving to a different industry with more upper mobility might offer you better career opportunities as industrial knowledge can be learnt much quicker than technical skills. I focus on data analytics with Python and SQL Server as my primary toolset, and I was able to land different job with significant pay raise in different industries; as long as what you know has a demand.",1.0
g5javib,it1ql5,As in moving to GIS in a different industry? Or something with SQL/Python in a different industry? I'm definitely not married to consulting.,1.0
g5bsgml,it1ql5,"BI is too broad. You may have luck searching for Business Analyst, Data Analyst, SQL Developer, Database Developer, etc. Good luck in your search.",1.0
g5jax55,it1ql5,"Awesome, thanks for the list! I'll start poking around and see if I can find any I look qualified for.",1.0
g5c57vp,it1ql5,"20 years ago I was nearly done with a degree in Geo Info Sci and became painfully aware that GIS jobs didn't pay very well compared to other jobs that used a lot of the same technical skills. I got a job as a database developer and haven't looked back. If you have any experience and interest in importing and exporting data files, you're partway to a career as an ETL developer already.",1.0
g5jaqca,it1ql5,"&gt;ETL developer

Thanks for the idea. I do some DBA and schema design stuff already, so that's an option, possibly.",1.0
g5jccjs,it1ql5,Good luck my friend!,1.0
g5c8q1o,isxk6z,"I enjoy it. 

I'm switching between python and  SQL with hopes of using them both when I learn enough.

I've learned more about sql than I ever would've and used my knowledge to solve some what I would call advanced queries.",3.0
g5cajl5,isxk6z,Cool thank you.  Which tract did you do?,2.0
g5cxnfi,isxk6z,"Will follow this. I'm also currently using SQL for simple tasks as reports and process support however, I feel that there's a lot more more to do with this. Btw, this is my 1st yr using SQL, so you could say I'm totally new to this.",2.0
g5alrqv,isuq12,search **supertype and subtype tables**,3.0
g5ahsq7,isu2jl,"You can find the first day of the current month using [EOMONTH](https://docs.microsoft.com/en-us/sql/t-sql/functions/eomonth-transact-sql?view=sql-server-ver15) and adding one day:

`SELECT DATEADD(DAY, 1, EOMONTH(GETDATE(), -1))`",1.0
g5bm1zz,isu2jl,"The method I found was DATEADD(DAY, -DAY(GETD ATE()+1, GETDATE()) (I'm on mobile so might have messed up the syntax). 

Basically, take the Day of today (e.g. 14), make it negative (-14), add 1 (-13), then do o DATEADD using that day and it'll take you back to the first. Hopeful if I messed up the code this explanation will help.",1.0
g5d0u5w,isu2jl,Thanks for the explanation guys!! Got it working!,1.0
g5apfkp,istubx,Personally on super complex queries I break them down into each component and load that subset into temp tables. I have taken horrible queries that ran in 30 mins down to like 1 min using temp tables vs multiple complex joins.,13.0
g5bi5lr,istubx,"Temp tables are one of the best tools you can use to test query execution and make changes to improve. Since everything runs in memory, it is much faster and much less risk in damaging the database you are working with. 

If you are working in an active dB, it is the best and safest way in my experience",5.0
g5bisot,istubx,Thirded. Temp tables are where its at.,1.0
g5bwn7v,istubx,"The first 250 pages of data might stay in memory, but after that it'll start writing to disk.

Edit: actually that is pre-2014. It is different now but I can't find any specifics.",1.0
g5a986e,istubx,"That’s going to depend on the SQL engine you use, its query optimiser and your code. Most of the time the optimiser should recognise they’re functionally identical, but every engine has its peculiarities. I would advise to run several tests and report back.",6.0
g5ac51i,istubx,"Thankyou! I use a variety of things such as tableau products, aginity, argos.... Primarily tableau prep right now. I'll do as you said and test it.",1.0
g5amuw0,istubx,"1) Identify where the time is going. 2) Eliminate/reduce that.
CTEs (in RDBMSs that actually can materialized them) are useful if you need to fetch the same set of rows multiple times in the same query. And it is faster to get the whole set once rather than applying whatever filters you have to do the joins. Perhaps a demo would make more sense. Select * from table_a a join table_b b1 on a.col_1 or b1.col_1 join table_b b2 on a.col_2 = b2.col2 where a.selective_column ='some value'. There is no common filter on table_b, so if you went down the CTE route it would have to be the whole of table_b. So the options are CTE: read all of table_b and store the necessary columns in memory/temp space and then read that twice (from memory/temp) in order to join it to table_a. No CTE, indexes on col_1 and col_2 so that the join can do a nested loop and only look at rows in table_b that match the join conditions. Here the answer is going to usually be the non CTE approach but if you didn’t have indexes then the CTE would probably be faster. Let’s say there’s an additional filter on both joins so they only consider rows in table_b that match another filter col_3 ='some value', now the CTE could also use that filter, if it’s a reasonably selective filter then an index on col_3 could be sensible. One fetch of all the rows in table_2 that match the col_3 filter and then hash joining that to table_1 twice could be faster than the two nested loop joins driving on the other two columns. If you know your data then the difference in performance for these two methods are usually very well known.",2.0
g5cpt62,istubx,I like starting with CTEs and if the performance gets bogged down they are easy to switch to temp tables. Usually with every temp table I also create the index on the temp table I need in the next query. Its amazing how much speed you gain and how quickly temp tables can be created and indexed.,1.0
g5aop52,istlm4,"I am not sure I understand your requirements correctly, but if you are looking for entries with only one occurrence of **b**, then maybe the [HAVING](https://www.postgresql.org/docs/12/tutorial-agg.html) clause is what you are looking for. Write your query as normal and suffix it with *HAVING COUNT(b) = 1*. This will give you only groups with one occurrence of b.",2.0
g5apam1,istlm4,I was using having but it isn't accurate. I dont know what I'm doing wrong. I can post my query later,1.0
g5axbrb,istlm4,Could you first query a CTE and do row over partition model order by date DESC.  Then select * from CTE WHERE ROWID =1,2.0
g5ayshy,istlm4,Do you have a small example of this?,1.0
g5b3ofi,istlm4,"https://www.sqlservertutorial.net/sql-server-window-functions/sql-server-row_number-function/
I’ll put something tomorrow morning when I open my laptop.",1.0
g5cvjd5,istlm4,"Example. 
Row_number() OVER (partition ID,YEAR,MONTH,STATE,COMPANY ORDER BY ASOFDATE DESC) as ROW ID",1.0
g5azis4,istlm4,"I used cross apply.. i put inner join lateral here because i THINK.. its the same. 

top code is my random data generator. i only get 1 or 2 records back.. probably because my peopleids / 1000 needs to be larger.




       DECLARE @table TABLE (userid bigint, dates datetime2, model NVARCHAR(1))
       DECLARE @count BIGINT = 1
       DECLARE @random BIGINT = 0
       
       WHILE @count !&gt; 1000
       BEGIN
       
       SELECT @random = ROUND(RAND()*(4-1)+1,0,1)
       INSERT INTO @table
              VALUES (
              -- random id 1-100
                     ROUND(RAND()*(101-1)+1,0,1)
              -- day sequence
                     , DATEADD(DAY, @count,'5/5/2015')
              -- a b or c       
                     , CASE @random
                     WHEN 1 THEN 'a'
                     WHEN 2 THEN 'b'
                     WHEN 3 THEN 'c'
                     end
                     
                     );
       SELECT @count = @count + 1
       PRINT ROUND(RAND()*(4-1)+1,0,1)
       END


     SELECT * 
      FROM @table main
      -- user has one count of b
     INNER JOIN lateral
         (
                  SELECT COUNT(1) bCount
                  FROM @table cnt
                  WHERE main.userid = cnt.userid
                  AND cnt.model = 'b'
         ) cnt
      -- and 'b' is the final item
      INNER JOIN lateral
         (
                  SELECT model
                  FROM @table isitB
                  WHERE isitB.userid = main.userid
                  AND isitB.dates = ALL(
                           
                           SELECT MAX(dates)
                           FROM @table maxdates
                           WHERE isitB.userid = maxdates.userid
                           )
         ) bdates

      WHERE 
         bdates.model = 'b'
         AND main.model = 'b'
         AND bCount = 1",1.0
g5b62ct,istlm4,"I think this will work if I understand your requirements.  But I'm new so feedback is appreciated.  Also on mobile...formatting...etc.

SELECT *
FROM table
INNER JOIN (

SELECT useid
,MAX(CASE WHEN model='a' THEN date ELSE 0 END)
,MAX(CASE WHEN model='b' THEN date ELSE 0 END)
,MAX(CASE WHEN model='c' THEN date ELSE 0 END)
FROM table
INNER JOIN (

SELECT userid
,COUNT(model)
FROM table
WHERE model='b'
GROUP BY userid
HAVING COUNT(model)=1
)
ON userid=userid

HAVING MAX(CASE WHEN model='b' THEN date ELSE 0 END) &gt; MAX(CASE WHEN model='a' THEN date ELSE 0 END)
AND MAX(CASE WHEN model='b' THEN date ELSE 0 END) &gt; MAX(CASE WHEN model='c' THEN date ELSE 0 END)
)
ON userid=userid",1.0
g5blew2,istlm4,"`Select userId, date` 

`From table t` 

`Where t.model = b` 

  `and Not exists(select from table o where o.userId = t.userId and o.Date &gt; t.date)`",1.0
g5c41yh,istlm4,"Maybe something like this? 

```sql
select user_id, max(date)
from the_table
group by user_id
having min(model) = 'b' and max(model) = 'b'
```",1.0
g5at7au,isthvm,"No, for the external table clause you must tell it what data types to convert the text data into. This metadata can’t use the pl/sql %type extension as that’s part of PL/SQL and not SQL. You can write your own DDL trigger to achieve the same though, but these data types should be based on the source CSV file and not the target table. You don’t need exact data type matches for an insert select statement, so you could just be lazy and create your external tables (or declared external tables in your views) as having varchar2(4000) and plain number data types... Your actual insert code looks a little risky though, relying on dbms_output for your logging and knowing about errors is not recommended, it could very easily not be enabled or just ignored. Deleting from the error logging table at the beginning is also asking for trouble, you would usually add an expression to the DML error logging clause so you can associate the logged errors with the batch they were attempted in. And then review them properly rather than counting them and deleting them.",1.0
g5axdrm,isthvm,"&gt; relying on dbms\_output for your logging and knowing about errors is not recommended

Thank you for your answer.  

When I run this procedure I see the output in the procedure log even when dbms\_output isn't enabled. I actually don't intend on updating this table, but with other tables that I do want to insert into, I have a different approach. The code generates a csv file of all the records that got rejected from being inserted for reasons other than that record already exists. Would you suggest I do anything else / different?",1.0
g5a2gf0,issagy,"Yes, there are several ways to do it:

1) Inner join the tables. Results will only be projects and employees where both exist.

2) Left join the employees table to the current project table (current project table in the FROM clause, Employee in the JOIN) and that will only show employees with projects.

3) Do a FULL JOIN with a WHERE clause that states the project ID (or whatever you use) IS NOT NULL (that's the statement, I'm not yelling, lol).

Any of those should work.",3.0
g5a3c3c,issagy,"You could also do a statement that searches for employees in the WHERE clause but that might be more processing-intensive (not sure, I don't know a ton about that stuff).

So something like:

SELECT \*  
FROM dbo.Employees  
WHERE EmployeeID IN (SELECT DISTINCT EmployeeID FROM dbo.CurrentProjects)

I do this sometimes when the second table is pretty small and not very intensive.",2.0
g5a38n6,issagy,"If you only care that the employees in question are working on at least one project and you don't actually care about the project data or the number of projects, then you could use an EXISTS.",2.0
g59qsxi,ispjz3,"So the data sets cover different date ranges, and you need to pull the same info from both?

I believe the operator you're looking for is UNION.

[https://www.w3schools.com/sql/sql\_ref\_union.asp](https://www.w3schools.com/sql/sql_ref_union.asp)

Query the two data sets separately, and use UNION to combine them.

Or create a temp table, query the first data set into the temp table, repeat for the second data set, then return the contents of the temp table.  I don't think you need to use a temp table here though.",1.0
g58zyao,isogi0,There’s lots of way to do this. Easiest is probably just add the column with the time stamp and alias it with another name. As a column not in the join it should give you all the data,3.0
g5a1rub,isogi0,"I would use three sub-selects or CTEs. Instead of generating a time series, I'd use a UNION of DISTINCT date values from both tables as the primary ""FROM"" and then LEFT JOIN to the two ""GROUP BY date"" sub-selects for each of the two tables. I'm not a Postgres guy though.",2.0
g5ah8oe,isogi0,"I think you could get a more helpful answer if you post the actual table structure.

&gt; Now table B has a row for 14/09/2020 at 08:00 however table A (which is the base table in the query) doesn't have that hour (it does have rows for that day however).  
&gt;  
&gt;So when I join table B onto table A I lose that 08:00 row and all the data with it.

You didn't say what columns you're joining on.  It sounds like you're joining on the date column and the time column.  Why not just join on the date column since the time value doesn't exist in all records?",1.0
g5b4etd,isogi0,Why can't you just do a union all? and then aggregations on that?,1.0
g58v0dj,isn8td,use outer apply and select top 1 for your procedure table/data source instead of a simple join,3.0
g59cm5d,isn8td,"What flavour of Sql are you running?
If I understand your ask - and Sql server - I would join on a sub query with a rownumber() over and order by date descending. On the join - where the rowid =1 and date is greater than today (or whatever you define the next procedure)",2.0
g68272v,isn8td,"I thought I had this working, but found a case that is returning two procedures linked to a single referral record. Below is as simplified as I think I can get the query. For a patient that has a referral on 8/11, it is still pulling procedures on 8/20 and 8/27. I am trying to only get 8/20 in the results, to show that a referral was made, procedure done, and how long from referral to procedure.

edit: I am using MS SQL Server

    SELECT 
        sq.[PatientID],
        sq.[RefDate],
        CAST(pr.[Date/Time] AS date) AS [ProcDate]
    FROM ProcTable pr
        LEFT JOIN (
            SELECT 
                ROW_NUMBER() OVER (PARTITION BY rt.[Patient ID] ORDER BY rt.[Date Entered] DESC) AS RowNum,
                rt.[PatientID],
                CAST(rt.[Date Entered] AS date) AS [RefDate]
            FROM RefTable rt
            ) AS sq
        ON sq.RowNum=1
            AND pr.[PatientID]=sq.[PatientID]
            AND CAST(pr.[Date/Time] AS date)&gt;=sq.[RefDate]
    WHERE sq.[PatientID] IS NOT NULL",1.0
g6853wr,isn8td,"Hey again.
So you’d want as a sub queue in the where clause or a 2nd table inner join something like
(Select min proc date where  patient is same and date is &gt; than today [or The date on the table, whatever you need])
That means your query will only pull through 1 day and the patient should only have 1 appointment . 

Be wary of edge case where data is bad and you may have 2 appointments on the same day.",2.0
g58xcaa,ismn54,What's wrong with running a [CREATE TABLE](https://www.postgresql.org/docs/current/tutorial-table.html) statement?,3.0
g58oriq,ismjvn,Business Analyst or Business Intelligence Engineer roles immediately come to mind. Note that in some companies these roles are now also labeled as “Data Scientist”. Couple years down the line you could get an MBA and go into product management if you like the business side more.,3.0
g58sshc,ismjvn,"Looking at the BI Engineer job description it really does look like something that I am interested in. At this point I am not totally ready to leave my current job but I definitely want to be prepared for the future. My current company is pretty small, so because of that I don't see much career progression moving forward.

Are these jobs that you have to start out as a Jr. BI Engineer with a couple years of experience? Is there a role in-between where I am now and the BI role that could better set me up for that career path?",1.0
g594z41,ismjvn,I would say it depends on the folks who are hiring you. If they think your experience translates directly then maybe you can skip the entry level.,1.0
g58sgyw,ismjvn,"I am in a similar position, I have been teaching myself because, I want to become a Data Analyst. It involves data extraction using SQL (which u know), analysis using Python/R and data viz. using r/Tableau/PowerBI. I think this role works both in the data side and in the business side.

I am not an experienced professional but this is the path I am taking.

Similar roles are BI analyst, Business Analyst like u/mabehnwaligali says. I wouldn't say that these roles are similar to a Data Scientist, as a DS does this and also ML, Deep Neural Nets.",2.0
g58k5oo,ism3s3,"Would this not work?

`SELECT` `r.Site, COUNT(CASE WHEN c.FavCandy = 'Reeses' THEN 1 ELSE 0 END) as Reeses, COUNT(CASE WHEN c.FavCandy = 'Reeses' THEN 0 ELSE 1 END) as Other`

Then you don't need to worry about your WHERE clause?",4.0
g58pwh5,ism3s3,"That was so close and was exactly what I needed to see. I had to alter it to:  

    SELECT r.Site, COUNT(CASE WHEN c.FavCandy = 1 THEN 1 END) as Reeses, COUNT(CASE WHEN c.FavCandy != 1 THEN 1 END) as Other, COUNT(c.FavCandy) AS Total

With the ELSE 0 part in there, it just kept giving me the grand total. 

Thank you so much for your help! You taught me about CASE statements, and you've saved the person I'm helping many hours.",3.0
g59vqao,ism3s3,"Yes, the flaw you've corrected is 1 and 0 both get counted.  You fixed that by nulling out the 0, which does not get counted.

Using SUM() instead of COUNT() would also have worked.",2.0
g59ujqw,ism3s3,"Happy you have your answer.  Dave's CASE method is the correct one.  There ARE alternatives, but this is the best one (speaking as someone who makes queries go faster).

To take it a step further, you can put that inside a CTE and do a little pivot to get the candy names down the left, if that fits your reporting requirements.",2.0
g58h6i7,isly16,"I'm not aware of any way to get them into individual columns without hard coding them, but can you work with a comma separated list or something? If so, STUFF (ms sql) may be what you're looking for.

We actually have report for a similar situation and the most performant way were ever able to get it was to create a temp table with all the columns and then manually updating each column based on the key thing.

I suppose you could problematically create that with a cursor and executing TSQL to add columns to the table, then a select based off of column names, but that would be getting pretty hairy at that point",1.0
g58jcbj,isly16,"Indeed I have a functioning way to do it when I can hardcode it:

    SELECT c.first_name, c.last_name, m.customer_id,
    MAX(CAST (CASE WHEN m.meta_key = 'phone_number' THEN m.meta_value ELSE NULL END AS VARCHAR(MAX) )) AS phone_number,
    MAX(CAST (CASE WHEN m.meta_key = 'email' THEN m.meta_value ELSE NULL END AS VARCHAR(MAX) )) AS email
    FROM customer_meta AS m
    JOIN customer AS c ON c.customer_id = m.customer_id
    GROUP BY m.customer_id, c.first_name, c.last_name

I'm looking into doing dynamic prepared statements but jeez what is this complicated for such a (seemingly) simple and common use case of a database?",1.0
g58n106,isly16,"Looks like someone gave you an example and I can't speak to it's validity, but the short answer is no, there isn't a simple way to do this because it's a pivot table and really takes a human looking at the data to make sense of it.

As I mentioned I'm pretty sure you could do it programatically with cursors and exec and what not, but I'd take a hard look at your use cases before you invest into that.

We thought about doing our reports that way but realized once the report was built, we may need to change what columns it has like once every few years, so we opted for hard coding them.

The other benefit, beyond trying to do some magic table create/update tsql, is that when we had weird instances where the client would say, well actually, for this date column, if it's january, change it to february. 

Having the updates just hard coded one col at a time makes that change easy, vs if it had been dynamically generated, now we're doing extra updates after the thing based on hard coded columns anyways",1.0
g58tl5q,isly16,"Yeah, I'm a bit at mercy of another system so I can't really determine the structure of the data that I get, but the amount of 'meta' fields are a lot and also varies... so I can't really do it elegantly in another way I think. As long as all the 'phone\_numbers' and 'emails' end up in the same column in the final result then it's ok.

I could just fetch all results and do it in a scripting language. But that kind of defeats the purpose of a database, not? I thought databases were made for this kind of stuff, but apparently the most simplistic use case is already 'complicated'. /rant",1.0
g58w2a4,isly16,"IMO this is the exact kind of thing where you'd want to do it with a scripting language. But, I'm in the camp of keeping sql calls as simple as possible and any complexity moving to whatever back end you're using. I think you'd get various opinions in here as to whether or not it should be done in sql

I've actually done this in some other software where I need to pull a dynamic list of columns and also display them in a specific order per user.

So my logic is basically, get ALL the data. Use c# to get the unique list of columns. Create a c# data table based off of that, bind it to my front end stuff that displays it.",1.0
g58iy7j,isly16,"You're looking for a dynamic pivot. Writing up an example with your data, one minute.",1.0
g58jyif,isly16,"Here it is. The part you care about starts at 'declare @cols', the rest was so i had a dataset to work with.

Also, you need to be on at least SQL 2017 to use this. You'll need to use STUFF instead of STRING_AGG (way more complicated of syntax than that sounds) if you're on a lower version:
    
    drop table if exists #customer
    select * into #customer from 
    (
    select 1 as id, 'Peter' as first_name, 'Smith' as last_name
    union select 2, 'Mary', 'Jones'
    ) a
    
    drop table if exists #customer_meta
    select * into #customer_meta from 
    (
    
    select 1 as customer_id, 'phone_number' as meta_key, '123456' meta_value
    union select 1, 'email', '[peter@smith.com](mailto:peter@smith.com)'
    union select 2, 'phone_number', '234567'
    union select 2, 'email', '[mary@jones.com](mailto:mary@jones.com)'
    ) a
    
    select * From #customer
    select * From #customer_meta
    
    declare @cols nvarchar(max) = (select string_agg( '[' +  meta_key + ']', ', ') from (select distinct meta_key from #customer_meta)a)
    select @cols
    
    declare @query nvarchar(max)=
    'select id, first_name, last_name, ' + 
    @cols + '
    from
    (
    	select * From #customer
    	join #customer_meta
    	on id=customer_id
    ) custs
    pivot
    (
    	max(meta_value)
    	for meta_key
    	in (' + @cols + ')
    ) as pvt
    '
    
    exec(@query)",1.0
g58s9ew,isly16,Wow thanks!! I'm using Azure so I think it'll work,1.0
g5822xv,isjjr1,"First, there’s a hard limit on the number of active cells you can use in Excel. A similar limit exists in SQL, but it’s much higher and determined by your hardware. 

Second, you can remake tables on the fly with a few commands in SQL, and then flush them from memory when you’re done with them. That operation is much more cumbersome in Excel.",4.0
g587m93,isjjr1,"I was working as SQL dev and my manager once said he didn't understand why he needed me (or the other devs) because he could the job in Excel.  Many of the smaller projects he could have (&lt;10-15k rows) been managed in Excel, but the large projects with several hundred thousand rows not so much.  Good luck trying sort a column or running any aggregation.",3.0
g5850g3,isjjr1,"From the data entry side of things Excel lets you get away with some stuff if you don't set it up properly.

For example, in a column of dates you could write ""Next Thursday"". Excel would be cool with that. SQL is a data strict environment. Try that in SQL and the whole row will fail.

In addition, SQL is better at joining data together. Excel has come along leaps and bounds in recent years with the introduction of Power Query but back in the Office 2013 days there were some serious limits on what you could do because of the calculation overhead.",2.0
g584541,isjjr1,"The companies database is going to be (or should be) in sql already so to even get it to excel you’ll have to run some kind of software to get the whole thing as an excel file.

It’s much faster to just write an sql query to get the correct data from the db rather than add the additional steps.

If the dataset gets to big excel wont be able to use it but sql will have no problems. And sql runs way way faster but that’s still only an issue on datasets that are big for excel.",1.0
g585zcw,isjjr1,"RBAR. SQL's power is set-based calculation. On large data sets you can get away with doing much calculation in the query, and letting Excel worry about formatting, tables and referencing formulas.",1.0
g58x4fh,isjjr1,"In a nutshell - each cell of an Excel spreadsheet is evaluated as its own discrete entity.  Relational structure and data types are generally not enforced.  This results in a very inefficient, CPU-intensive process that becomes very noticeable on even top-notch equipment once you get to a critical mass of row/column count and formula logic density.    


Excel does still have its place though - there is no quicker way to analyze a blob of structured data by injecting into a pivot table / chart, esp. if you can directly connect it to your SQL Server and pull normalized tables, adding relationships via PQ.  But there is absolutely no need to do any data wrangling in Excel whatsoever if you have SQL access to the data source.",1.0
g5990ei,isjjr1,"Give me an SQL query you want to automate and it will produce a report automatically in 15 minutes. Give me an Excel file and I'll need days depending on the level of cluster fuck it is.

The nice thing with SQL is that you can embed it anywhere.",1.0
g5gnlte,isjjr1,"In addition to what others have said, a relational database splits the data from the presentation. You can present data from your database in a huge number of ways, from bespoke applications you write, SQL clients, reporting tools like PowerBI or Redash, scheduled reports, even directly in Excel.

Excel files are typically viewed in Excel and are limited by Excel's capabilities. Try crunching millions of rows, writing advanced queries, doing anything geospatial, optimising slow functions. All of those are easy with an RDBMS and extremely difficult with a spreadsheet.",1.0
g6b5p69,isjjr1,"I use SQL from a long time and I believe that SQL is much faster than Excel. Moreover, it can take minutes in SQL to fetch the data as compared to Excel where it takes hours.",1.0
g58wzz3,isex5v,"Check out generate_series() function. I believe it'll solve your problems with dates without a row. Generate a list of consecutive dates of your choice and left join the data rows with that. Then group by the date and the country, run averages over those.",2.0
g5g5yu6,isex5v,Thanks! Will give that a go,1.0
g57gem9,isex5v,"&gt; if there is a more accurate way?

yes, a join",0.0
g577nwh,iscnf0,"One option is online tools if you just want to get familiar with SQL commands.  I just searched for ""sql online practice"" and found this one for example:

[https://sqlzoo.net/wiki/SELECT\_basics](https://sqlzoo.net/wiki/SELECT_basics)",12.0
g57avq2,iscnf0,Leetcode and StrataScratch are the places where you can practice the actual problems.,8.0
g57dtkg,iscnf0,"Is StrataScratch good? How does it compare to leetCode?  Looking for a personal experience with it?

 Thank you.",2.0
g57jtn1,iscnf0,"https://www.postgresql.org/

You don't need to install it, you could use e.g. https://dbfiddle.uk/?rdbms=postgres_12 if you just want to play around",5.0
g57ruv2,iscnf0,You can run [Aginity Pro](https://www.aginity.com/products/aginity-pro/) for free and download Postgres to run locally.,3.0
g581dgd,iscnf0,"I just started doing some self study courses (a couple weeks in) and I've been using SQLiteStudio, which allows you to Read/Create SQLite tables. (https://sqlitestudio.pl/) but if you want something more robust I've also been using DBeaver (https://dbeaver.io/).  It can access a whole host of different RDBMS.",3.0
g5896js,iscnf0,Microsoft's SQL server has a limited free edition (express) and a full edition (developer) you can't use in production. A management tool is included as well.,3.0
g58djp6,iscnf0,"Would highly recommend the latest edition of SQL Server developer. Learn how to create databases from external sources and manage the database in SSMS. Would also recommend taking a peek at Azure Data Studio, its like VS Code for SQL Server but with Azure connection capabilities.",1.0
g58iwbe,iscnf0,"Personally, I far prefer DataGrip by jetbrains. I would be free for students. But it might also be overwhelming for a beginner.",1.0
g57kwv8,iscnf0,Popsql is pretty simple to use,2.0
g58dpur,iscnf0,Get Adevnture Works and SQL Server Express from Microsoft,2.0
g5942y6,iscnf0,"For a GUI front-end to run against your database, dBeaver is a great tool with compatibility for many RDBMSes",2.0
g57x6oc,iscnf0,"I think if you have a mac, sqlite3 already comes with it by default, try just typinig `sqlite3` in a terminal.",1.0
g583f38,iscnf0,"please ignore the SQLite everyone is talking about. its not really a proper DB.

just use PostgreSQL or MySQL
both good, free, simple to install and actual DBs that are used in production by many companies.

also dont use sqlserver, it works only on windows which is not representative of the market.

as for IDE to actually write SQL. DBeaver. thank me later.",1.0
g583q9e,iscnf0,Am I wrong but isn't SQLite just a C++ copy of SQL but isn't actually sql?,1.0
g58jtfl,iscnf0,"SQL is a language, the thing you write like `SELECT * FROM table`. sqlite uses SQL. It's written in C (not C++), but so are other engines, like PostgreSQL. 

sqlite's strength is in its liteness - it doesn't need a separate service running, you embed the server in your application. So if you have a mobile app, you can have a database supporting SQL and many of RDBMS features in it. On desktop, you don't need a fully fledged external server to connect to, you just ship a database file with your app or create it on the fly.",2.0
g58kuay,iscnf0,"That's great info thank you! But suppose you do have a SQLite instance hosted on a separate server, can it support multiple users at once? I thought it was only transactional to one connected user at a time since I always consider it an offline db.",1.0
g58pjrc,iscnf0,"You do not host sqlite instances. They are just files, that you open using the library (e.g. `sqlite3_open` in C or C++). There is no network interface or anything like that. It can support multiple ""users"", like several processes accessing the same file, locally, for read purposes, but only a single process can write to the file at any given time and all others will be locked out.

The minute you start thinking ""should I put this sqlite file on a network share"" is the exact moment you should stop and start moving to a hosted rdbms, because sqlite is not right for you.",2.0
g58qjy0,iscnf0,"Okay good I'm glad you said that because it confirms all of my basic knowledge of SQLite. I was looking for an offline solution for a sql db for an Android app once and almost used SQLite, which would have been a good solution, but I didn't think it was anything more than a persistent local file on the machine. 

Thanks for all of the information, super helpful!! :)",1.0
g5725ci,iscnf0,"MySQL (windows only), SQLite, and PostgreSQL",0.0
g56t7zf,isb907,I am confused. You don't compile sql queries.,2.0
g56umlx,isb907,[deleted],1.0
g56x5oh,isb907,"SQL is plaintext scripting.  It is not compiled.

Not DDL or DML. There is no compiling.

`CREATE TABLE ..... `

`SELECT * FROM.... `

etc.  It's all plaintext.


If your instructor runs MariaDB. that is the DBMS database software.  That's the actual database... MariaDB is an opensource DBMS released by Oracle.  Other DBMSs are PostgreSQL, MS SQL Server, MySQL, Oracle Database, etc...  These are database systems.  SQL is the query language used to interact with databases.

If you're looking to install software of some sort, you're either looking for a server (like those I mentioned above), or a client, which is the software you run which connects to the server and acts similar to an IDE where you can execute your queries against the database.  DB clients include DBVisualizer, DBeaver,  SQL*Plus",2.0
g57qp1q,isb907,"Just to correct an unimportant part of this. MariaDB is a fork from MySQL. MySQL is open source but sponsored by Oracle, MariaDB was forked to have no ties with Oracle.",2.0
g56whyo,isb907,"MySQL is very easy to install and then use 'MySQL Command Line Client' or 'MySQL Workbench'  


You can install MariaDB on your Windows device by the way.",2.0
g56xy06,isb907,Sqllite is really easy.,1.0
g571rp9,isb907,"They're all pretty easy to setup and use .. mssql developer edition, mysql, SQLlite, etc... I will say postgres is a little more difficult to setup and get going with writing queries.",1.0
g57d3gw,is9ia7,"
    SELECT * FROM
    (
    SELECT Tabla1.Country, Tabla1.DATEF AS Dates, Sum(Tabla1.MaxImp) AS Impressions, Sum(Tabla1.SumCk) AS Clicks
    FROM (
    SELECT TopCountries.Country, TopCountries.DATEF, TopCountries.QueryF, Max(TopCountries.Impressions) AS MaxImp, Sum(TopCountries.[Url Clicks]) AS SumCk 
    FROM TopCountries GROUP BY TopCountries.Country, TopCountries.DateF, TopCountries.QueryF
    )  AS Tabla1 GROUP BY Tabla1.TopCountries.Country, Tabla1.TopCountries.DATEF
    ) as T1
    LEFT JOIN
    (
    SELECT Tabla2.Country, Tabla2.DATEF AS Dates, Sum(Tabla2.MaxImp) AS Impressions, Sum(Tabla2.SumCk) AS Clicks
    FROM (
    SELECT TopCountries.Country, TopCountries.DATEF, TopCountries.QueryF, Max(TopCountries.Impressions) AS MaxImp, Sum(TopCountries.[Url Clicks]) AS SumCk 
    FROM TopCountries WHERE TopCountries.QueryF LIKE '*open*' GROUP BY TopCountries.Country, TopCountries.DateF, TopCountries.QueryF
    )  AS Tabla2 GROUP BY Tabla2.TopCountries.Country, Tabla2.TopCountries.DATEF
    )as T2
    ON T1.Country = T2.Country AND T1.DATEF = T2.DATEF;",1.0
g57z0np,is9ia7,"note that `Tabla1.TopCountries.Country` is not a valid column name (you have 4 of these in your GROUP BYs)

i've simplified your query for you

please let me know if this works

    SELECT Country
         , DATEF AS Dates
         , SUM(MaxImp) AS Impressions
         , SUM(SumCk) AS Clicks
         , SUM(MaxImp_open) AS Impressions_open
         , SUM(SumCk_open) AS Clicks_open
      FROM ( SELECT Country
                  , DATEF
                  , QueryF
                  , MAX(Impressions) AS MaxImp
                  , SUM([Url Clicks]) AS SumCk 
                  , MAX(IIF(QueryF LIKE '*open*',Impressions,NULL))
                                     AS MaxImp_open
                  , SUM(IIF(QueryF LIKE '*open*',[Url Clicks],NULL)) 
                                     AS SumCk_open 
               FROM TopCountries 
             GROUP 
                 BY Country
                  , DateF
                  , QueryF ) AS Tabla1 
    GROUP 
        BY Country
         , DATEF",1.0
g56hvgq,is9ia7,")  AS Tabla1 GROUP BY Tabla1.TopCountries.Country, Tabla1.TopCountries.DATEF **AS Tabla1**",0.0
g56ieaz,is9ia7,"Thank you, but still broken.",1.0
g56luid,is9ia7,You also need to alias the second one,1.0
g57yw5r,is9ia7,no... `Tabla1.TopCountries.Country` is not a valid column name,1.0
g57yypg,is9ia7,"Not as much of a valid column name as yer mum
***
^I ^am ^a ^bot. ^Downvote ^to ^remove. ^[PM](https://www.reddit.com/message/compose/?to=YoMommaJokeBot) ^me ^if ^there's ^anything ^for ^me ^to ^know!",0.0
g56daou,is8n9z,"Half of the diagram is already done for you, as the question includes the tables and their columns. You just need to connect the keys. A quick Google search of database diagrams will show you what the finished product looks like. Identify the primary key in each table (Customer ID in the customer table for example) then find the other instances of customer IDs in other tables. Those are the foreign keys. Draw a line between them. Repeat for for each table.",10.0
g56hzzk,is8n9z,Thanks man this really helps me a lot,6.0
g56bp8c,is8n9z,I really like https://dbdiagram.io/home. It will allow you to specify your primary and foreign keys,5.0
g56i1hd,is8n9z,I’ll definitely check this out thank you!,1.0
g56inol,is8n9z,"Read the preceding chapter in your textbook.  If the exercises at the end of the chapter are having you create a database diagram, then the chapter will tell you how to do it.

Seriously... thats why you buy a textbook; it literally tells you how to do this shit if you just read it.",7.0
g56mv60,is8n9z,"Exactly. 

My daughter is middle school aged and will bring something like this to me for help. I make her show me the material and instructions she's received before helping her. More and more, there's no instruction at all. Her math class so far this year is, I kid you not, all done via rap videos. She tried writing down the words in the videos but couldn't follow it enough to actually do the homework assigned. 

At least this guy has a book.",4.0
g56xfbg,is8n9z,My professor had us jump from chapter 1 to chapter 10 and gave us 5 days to do this. I read the chapter and I don’t understand half the things they are talking about,1.0
g56y2no,is8n9z,"What does the chapter say about how to design a database diagram?

First of all, what is it used for?  Secondly, what are its components?  What do the components represent?  How do you decide what components are needed?  How are components related to each other in the diagram and how do you decide on those relations?


You are given 4 tables in the diagram and told to show the relations between them.  It's a database for storing customer orders.  Think about it logically... if you owned a business and you wanted to store your orders and you broke the data down into these 4 sets, how are the sets related to each other?  A big hint is to look at the names of the columns provided for each table.  Any column names appear in multiple tables?",1.0
g578gm0,is8n9z,"Biggest tip here is just keep chugging along. Intimidating new world at first, but it’ll come.",1.0
g579ema,is8n9z,"What's the name of the book? Looking for a resource to continue learning SQL Server, and indexing is right up that alley.",1.0
g58fpnq,is8n9z,Murach’s SQL Server 2016 for developers. I’m sure it’s a good book but my experience has been garbage with it because my professor had us jump from chapter 1 to chapter 10,2.0
g56i6xl,is8n9z,If you’re using MySQL you could use a create database command and then create the tables each one by one with commands as well.,-1.0
g55v15s,is6jom,"Do it both ways if it’s for a portfolio. Then you can make observations about the processes of both, benefits/drawbacks etc.",3.0
g56rdmz,is6jom,"It’s common to load the raw data into DB as staging tables, then do the necessary transforms (normalization etc) and store the output as your production tables. The staging tables can be kept for future debugging, or removed after a period of time. If cost is an issue then consider doing the staging tables in a data lake where storage is cheap.",3.0
g570h7v,is6jom,"Agreed with this method. I’ve also set up jobs to clear staging table data after a given retention period to deal with the space issue. 

For the problem we had, there were several 3rd party call centers sending daily logs. Combining their calculated reports didn’t work well as they all defined certain metrics by different standards. Gathering the raw data in staging tables and performing calculations on that allowed us to more accurately report on the data across sites.",1.0
g5644rg,is6jom,"In most applications, you don't really want to change your sql schema too drastically after everything's in there and you're using it. I guess in this case if you're not using it yet, you could theoretically import it into a bunch of temporary tables before doing the other work on it to change it around and then throw away the temporary tables....but that kinda feels messy. If it's possible to ""fix"" the data outside, then I might do that first just so there's no ""trash"" sitting around in the sql database after.",1.0
g58cc6z,is6jom,"I would highly recommend sticking with the project's recommendation or something close to it, if I am understanding it correctly.

Data belongs in databases. At this early stage, you don't know what your ideal schema is, you don't know how you'll use all the data you have, and you're probably not that intimately familiar with the raw data itself, structure aside. It's going to be much, much, much, much easier to understand how your data moves from its raw state to its final state--and to modify that transformation process--if you put the raw data in the database first. It's going to be more efficient too, almost certainly, but understanding how to manipulate data efficiently is a project of its own.

I like the top suggestion to do it both ways and compare, though. Excellent idea.",1.0
g54zhjl,is2m12,"try this [https://www.w3schools.com/sql/func\_sqlserver\_cast.asp](https://www.w3schools.com/sql/func_sqlserver_cast.asp)  


SELECT CAST(ROUND(...) as Decimal(10,4))   
FROM ...",3.0
g55y5c6,is1jh9,"If I understand correctly, you are overwriting a partition at a time, but the same partition got overwrite multiple times because of your bug. 

Don't think there are duplicates since very partition got overwirtten, but should be easy to verify, simply pick a few days and rerun your schema1.import\_table creation script.

Then check if the values are the same in schema1.final\_table.",2.0
g55ymiz,is1jh9,Exactly what I was thinking. I suppose if I did ‘INSERT INTO’ without ‘OVERWRITE’ then I may have ended up with duplicate data,1.0
g54w8h6,is0xs5,"    SELECT p1.name, COUNT(DISTINCT m.MID) count_ 
    FROM Movie m 
        JOIN M_Cast mc 
            ON m.MID = mc.MID 
        JOIN M_Director md 
            ON m.MID = md.MID 
        JOIN person p1 
            ON mc.PID = p1.PID 
        JOIN person p2 
            ON md.PID = p2.PID 
    WHERE p2.name = 'YASH CHOPRA' 
    GROUP BY p1.name) AS YC

    JOIN

    SELECT p1.name, COUNT(DISTINCT m.MID) count_ 
    FROM Movie m 
        JOIN M_Cast mc 
            ON m.MID = mc.MID 
        JOIN M_Director md 
            ON m.MID = md.MID 
        JOIN person p1 
            ON mc.PID = p1.PID 
        JOIN person p2 
            ON md.PID = p2.PID 
    WHERE p2.name &lt;&gt; 'YASH CHOPRA' 
    GROUP BY p1.name AS NOT_YC ON YC.name = NOT_YC.name 
    WHERE YC.count_ &gt; NOT_YC.count_;

I've made an attempt at formatting your first query, but it looks to me like you've got syntax errors, I don't think this would run.",2.0
g54wsda,is0xs5,This query is returning blank table. Can you please identify where is the mistake?,1.0
g54yjfb,is0xs5,"Well if you want to JOIN two subqueries like that then you need to have them in brackets and aliased. You've got aliases on both, but only a trailing bracket for the first query and none at all for the second query.

You're also missing a SELECT at the start to SELECT from the two joined subqueries.",2.0
g54zh8j,is0xs5,It should be like: SELECT *FROM (1st sub query) JOIN (2nd sub query),1.0
g551d4s,is0xs5,"Yes.

Just realised, you have another problem. The question wants you to find actors who've done more movies with Yash than with any other director, i.e. Yash's count is higher than that of the count for any other director.

But your second query doesn't get a count per director, it's just counting the number of films that the actor was in that Yash didn't direct. 

So what you're doing currently is ""Is Yash's count higher than all other directors put together?""",2.0
g551pup,is0xs5,"Oh, so what should be the 2nd sub query. Can you please suggest the modification?",1.0
g552d8o,is0xs5,"&gt;Oh, so what should be the 2nd sub query. Can you please suggest the modification?

Get a COUNT per Actor and Director instead of just by Actor. 

i.e. GROUP BY Actor and Director.

Edit: It doesn't matter if you do this in the first subquery, because you're restricting it to a single Director anyway. But it does in the second.",1.0
g553juf,is0xs5,That is group by P1 and P2.,1.0
g554jmy,is0xs5,Yes.,1.0
g555075,is0xs5,Thanks a lot.,1.0
g54tn1s,is0xs5,"This is hard to read. I would use code blocks for your queries, and also split them sensibly into separate lines. So for example, each table you join to could be on a separate line, SELEC could be on a separate line, etc.",1.0
g54xo1x,is0xs5,"And the second one, which is a lot of nesting.

    SELECT DISTINCT TRIM(name) Name 
    FROM Person p 
        INNER JOIN M_Cast c 
            ON p.PID = TRIM(c.PID) 
        INNER JOIN Movie m 
            ON m.MID = c.MID AND TRIM(p.Name)!='Shah Rukh Khan' 
            AND m.title IN 
           (
            SELECT DISTINCT title 
            FROM Person p3 
                INNER JOIN M_Cast c3 
                    ON p3.PID = TRIM(c3.PID) 
                    AND TRIM(p3.Name) = p3.Name 
                INNER JOIN Movie m3 
                    ON m3.MID = c3.MID 
                    AND p3.Name IN 
                    (
                     SELECT DISTINCT Name 
                     FROM Person p2 
                         INNER JOIN M_Cast c2 
                             ON p2.PID = TRIM(c2.PID) 
                         INNER JOIN Movie m2 
                             ON m2.MID = c2.MID 
                             AND TRIM(p2.Name)!='Shah Rukh Khan' 
                             AND m2.title IN 
                             (
                              SELECT DISTINCT title 
                              FROM Person p3 
                                  INNER JOIN M_Cast c3 
                                      ON p3.PID = TRIM(c3.PID) 
                                      AND TRIM(p3.Name) = 'Shah Rukh Khan' 
                                  INNER JOIN Movie m3 
                                      ON m3.MID = c3.MID
                             )
                    )
           ) 
    ORDER BY Name",1.0
g54y31m,is0xs5,The query is running but it is not returning any values.,1.0
g53qf93,irwwse,"You've got a few things going in here. SQL is a language, not a file format. SQL is executed by a database engine, if you've seen RDBMS that stands for relational database management  that's the engine. Common examples are MySql, Microsoft SQL Server, Oracle, Postgres, and SQLite. What you very likely have is a database file used by one of these systems and the proper way to view the data would be through the correct RDBMS, what it seems you're trying to do is extract string/text data from the file in a text editor like Notepad or VS Code or something. Sort of like trying to find text in a Word file while not using Word.

Others have suggested doing string matching on the text of the file, and any decently fully featured scripting language would work like PowerShell or Python. You could also use regular expressions and a tool like grep or see to extract the strings.

What I would do is figure out which database system the file came from, set it up there and just do a quick SELECT and then export the results.

&amp;#x200B;

Edit: Another idea might be to use the ""strings"" utility (sysinternals has one for Windows) which might give you a cleaner text file to work with and then you might be  able to use tools you might be more comfortable with like even Excel. Strings, if it works with your source file well enough, would put readable text on separate lines so then the exercise of identifying emails becomes as simple as checking if the line has an @ between some text.",7.0
g540y16,irwwse,"Hey thanks for the reply. Yes this is a database file, however I don't have a database engine.

So you are suggesting I use something like PowerShell or Python to do string matching for strings that contain @ or use the strings utility?",2.0
g54abyj,irwwse,"That's probably your best bet if you don't have an RDBMS engine to parse the data.  Because that database file likely consists of multiple tables with multiple columns within each, and it's almost a given that some of the data is going to be Varchar, meaning the lengths will change from row to row.  So you're not going to be able to just assume a fixed position within the file for the email data.",1.0
g53g131,irwwse,"When you say ""all the text"", what do you mean? Is there an email field in a table?",6.0
g53hq6e,irwwse,"No clue, I opened it in EmEditor and it's just a large amount of text. I am a noob at databases btw.

I am able to find emails within the files but I cannot really recognize a pattern or anything like that.",2.0
g53rxlf,irwwse,What makes you say this is a SQL file?  What is the file extension?,2.0
g5407h9,irwwse,"The file type is ""SQL File"".",-1.0
g54jw80,irwwse,What is the file extension?,2.0
g54oaex,irwwse,".sql

Is that what you mean?",1.0
g5523qh,irwwse,"You know what a file extension don't you?  You know .docx = word document, .txt = text document, .pdf = adobe portable document format, etc.

If this is a .sql file, it could be a SQL query.  Do you have SQL Server Management Studio installed?",1.0
g53hshn,irwwse,"Agreed. Is this like a varchar field or when you say SQL file, do you mean something like a .bak or are you asking how to use SQL to read a file?",2.0
g53m09s,irwwse,"If it is just a string, I would suggest to use a combination of a loop, instr and substr to search for @, then use the space before and after the word to find the text. Search, Print, Repeat.",2.0
g53mvhu,irwwse,"Thanks for the reply. How do I do that? Which software? lol

I am a complete noob when it comes to databases.",-2.0
g542bmt,irwwse,"I don’t quite understand the question. 

Can you post an example of what you have?",2.0
g543w89,irwwse,"Yes, what example would you like to have?

It's a database file from an old company that no longer exists, it contains user data, such as usernames and email addresses.",0.0
g5459pd,irwwse,Can you post a small snippet of the data? You can change the data so you don’t leak pii.,1.0
g545m88,irwwse,"Okay, so if I open it in an editor I get lines like this: https://imgur.com/kJYhY0X",1.0
g5471nn,irwwse,"Ok, I see. It’s a sql file with DML, I’d imagine there’s DDL at the top as well. 

This is what I would do:

1. install sql server express edition. This is free. 
2. Install sql server management studio. 
3. Create a database using SSMS 
4. Open that SQL file in SSMS and execute it in the database you just created. 
5. Post the DDL, the “create table” sql code at the top of your file here on Reddit, we need this to construct a query to get the data you need. 

If you need more detail, I (or someone else) can walk you through it. I’d imagine this might be overwhelming for someone new at this. It’s really not that bad though.",2.0
g547yd1,irwwse,"To be clear, you have a single 4GB file that looks exactly like the sample screenshot you posted? 

A .sql file is code to run against a SQL database, and does not necessarily contain any data. However the code you posted was syntax to insert hard-coded data into a database table. So I guess technically you gave the data. You would need to create a database and a table with the right number of columns and matching data types, then you could execute the code.  But tough to say what exactly needs to be done without seeing the rest of the code.  A 4GB .sql is MASSIVE given that they’re usually a set of commands used to extract data from a database, but the data itself.",1.0
g54owwi,irwwse,"&gt;To be clear, you have a single 4GB file that looks exactly like the sample screenshot you posted?

Correct, it's a massive amount of data which is why I cannot even open it in most editors and programs. But I don't really need to execute the code but rather just pick out the addresses.",1.0
g56urv9,irwwse,"To me, it looks like the data is encrypted. In the excerpt you posted there are a bunch of lines like this -

 insert into member_login values (.....)

These lines are loading all the data, line-by-line, into the member_login table. There are five values for each person, within the parenthesis. One is presumably a username, but the next two columns are bizarre strings of data. My guess is one is the email address in an encrypted form, and the other the password in an encrypted form.

Do you have any more info about the database structure, or any of the application code?",1.0
g58x4x7,irwwse,No I don't but there are plenty of email addresses visible in the document as well. It just seems like a lot is encrypted though.,1.0
g53sjmx,irwwse,We need more info here. Could you put here an extract of your file here ?,1.0
g5417n2,irwwse,What exactly do you mean by file extract? The file itself is rather big.,0.0
g5451bl,irwwse,"If you can convert it to a text file, you can extract the email data with python using RegEx.",1.0
g54bhg5,irwwse,If it’s database backup file it’s likely encrypted . If it’s a csv file it’s a flat file and they should have certain format you can follow to extract,1.0
g54fyhz,irwwse,"Took a look at other replies, and honestly it'll be way easier to just set up a database for yourself and import it than write a script to comb through all that. What your file is is actually a SQL script that inserts all the data one row at a time, and while you could hit that with some string manipulation, you'll lose the structure of the data, which you probably want.

if you have access to Linux, mariadb should be in the package manager for pretty much any distro, and docker containers are available as well for even more out of the box functionality. For Windows, SQL Server should be a free download. after you get those, you should be able to download a SQL client like DBeaver, import that file, export it as .csv, and then you can  manipulate it however.",1.0
g54og2l,irwwse,"&gt;you'll lose the structure of the data, which you probably want.

Well for the moment all I'm interested in is the email addresses, the rest of the structure is not so important right now.",1.0
g54x0lv,irwwse,"well, where it's all INSERT statements, they'll all be in single quotes and contain an '@' sign, so you should be able to hit that with some Python. Do something like

edit: I give up on the editor here and pasted the code: [https://pastebin.com/ai7Y4x2G](https://pastebin.com/ai7Y4x2G)

I'm far from a Python expert, and this is off the top of my head, so hopefully people will jump in and correct me in the very likely case this is wrong.

another edit: debugged it with a test case and it should be fine: https://pastebin.com/ZK5uTSid",1.0
g55og0u,irwwse,Thank you my brother I will give it a shot.,2.0
g572g0k,irwwse,"just took another look at that screenshot, and this may not work unless the emails are plaintext in there somewhere, it looks like things were hashed somewhere along the way. It may be a different table than what you screencapped though, so I'd still give this a shot and see if it finds anything.",1.0
g56edau,irwwse,.sql files are usually the text files where the query is installed. you need to load the data into a table and query it. this will depend on what kind of SQL database you are using (..there are a few...),1.0
g548mwy,irwwse,Just download python and pandas and do it that way.,0.0
g52b17k,irth91,"\&gt;&gt;   LABEL CURRENT MAX A 3 3 B 1 2 C 1 1 

I don't get how you calculate values. My understanding is that the result should be:

LABEL CURRENT MAX A 1 3 B 0 2 C 1 1",3.0
g52gyd4,irth91,"I'm sorry, but it's a formatting problem, I'll explain better...
 I have this table

**TABLE**
* LABEL | Datetime | HomeWin | Draw | Away Win
* A | 2020-02-04 13:24:06 | 1 | 0 | 0
* A | 2020-02-05 13:24:06 | 1 | 0 | 0
* A | 2020-02-07 10:24:06 | 0 | 1 | 0
* B | 2020-02-04 13:24:06 | 0 | 0 | 1
* A | 2020-02-09 11:24:06 | 0 | 1 | 0
* B | 2020-02-09 13:24:06 | 0 | 0 | 1
* B | 2020-02-13 13:24:06 | 0 | 0 | 1
* A | 2020-02-20 13:24:06 | 0 | 1 | 0

**STREAK RESULT MAX/CURRENT**
* LABEL | Max HomeWin | Max Draw | Max AwayWin | Cur HomeWin | Cur Draw | Cur AwayWin | 
* A | 2 | 3 | 0 | 0 | 3 | 0
* B | 0 | 0 | 3 | 0 | 0 | 3

[IMG Table](https://ibb.co/vJfCJGq)",2.0
g52lf69,irth91,"When you say consecutive. 

Do you mean..

A 1

A 1

A 0

A 1

Would produce a 2.. because there were only 2 consecutive wins? But a total of 3... yea?",2.0
g52lrsb,irth91,"yea, is correct!",1.0
g54b0nq,irth91,so the problem is not easy to solve?,1.0
g54q9fu,irth91,"Depends on how you want to solve it.
  
      DECLARE @t TABLE (label NVARCHAR(100), dates DATETIME2(7), homewin BIT, draw BIT,awaywin BIT)
      INSERT INTO @t
      values
      ('A','2020-02-04 13:24:06','1','0','0')
      ,('A','2020-02-05 13:24:06','1','0','0')
      ,('A','2020-02-07 10:24:06','0','1','0')
      ,('B','2020-02-04 13:24:06','0','0','1')
      ,('A','2020-02-09 11:24:06','0','1','0')
      ,('B','2020-02-09 13:24:06','0','0','1')
      ,('B','2020-02-13 13:24:06','0','0','1')
      ,('A','2020-02-20 13:24:06','0','1','0')
      
      DECLARE @ResultTable TABLE (LABEL NVARCHAR(100), Max_HomeWin BIGINT,Max_Draw BIGINT,Max_AwayWin BIGINT,Cur_HomeWin BIGINT,Cur_Draw BIGINT,Cur_AwayWin BIGINT)
      
      DECLARE @label NVARCHAR(100)
      DECLARE @current_label                        NVARCHAR(100)
      DECLARE @homewin BIT
      DECLARE @draw BIT
      DECLARE @awaywin BIT
      
      DECLARE @Consecutive_WinHome_Count            BIGINT
      DECLARE @Consecutive_Draw_Count                  BIGINT
      DECLARE @Consecutive_WinAway_Count            BIGINT
      DECLARE @Total_WinHome_Count                  BIGINT
      DECLARE @Total_Draw_Count                        BIGINT
      DECLARE @Total_WinAway_Count                  BIGINT
      
      DECLARE @MaxConsecutive_WinHome_Count      BIGINT
      DECLARE @MaxConsecutive_Draw_Count            BIGINT
      DECLARE @MaxConsecutive_WinAway_Count      BIGINT
      
      DECLARE @Last_WinHome                  BIGINT
      DECLARE @Last_Draw                        BIGINT
      DECLARE @Last_WinAway                  BIGINT 
      
      
      DECLARE curLabel CURSOR LOCAL FAST_FORWARD
      FOR 
            SELECT DISTINCT label
            FROM @t
            ORDER BY [label]
      
      OPEN curLabel
      FETCH NEXT FROM curLabel INTO @label
      WHILE @@fetch_status = 0
            BEGIN
      
      -- initialize data
                  SELECT
                        @Consecutive_WinHome_Count            = 0
                        , @Consecutive_Draw_Count            = 0
                        , @Consecutive_WinAway_Count      = 0
                        , @Total_WinHome_Count                  = 0
                        , @Total_Draw_Count                        = 0
                        , @Total_WinAway_Count                  = 0
                        , @MaxConsecutive_WinHome_Count = 0
                        , @MaxConsecutive_Draw_Count      = 0
                        , @MaxConsecutive_WinAway_Count      = 0
                        , @Last_WinHome      = NULL
                        , @Last_Draw      = NULL
                        , @Last_WinAway = NULL
      -- for each label.. order the records by date
            DECLARE curDates CURSOR LOCAL FAST_FORWARD
            FOR 
                  SELECT homewin, draw, awaywin
                  FROM @t
                  WHERE label = @label
                  ORDER BY dates
            OPEN curDates
            FETCH NEXT FROM curDates INTO  @homewin, @draw, @awaywin
      
            WHILE @@fetch_status = 0
                  BEGIN
      
      -- get total running counts
                        SELECT
                              @Total_WinHome_Count      = @Total_WinHome_Count      + IIF(@homewin = 1, 1, 0)      
                              , @Total_Draw_Count            = @Total_Draw_Count            + IIF(@draw = 1, 1, 0)
                              , @Total_WinAway_Count      = @Total_WinAway_Count      + IIF(@awaywin = 1, 1, 0)
      
      -- get total consecutive positives
                        SELECT
                              @Consecutive_WinHome_Count            = @Consecutive_WinHome_Count      + IIF(@homewin = 1      AND ISNULL(@Last_WinHome,1)= 1      , 1, 0)      
                              , @Consecutive_Draw_Count            = @Consecutive_Draw_Count            + IIF(@draw = 1            AND ISNULL(@Last_Draw      ,1)= 1      , 1, 0)
                              , @Consecutive_WinAway_Count      = @Consecutive_WinAway_Count      + IIF(@awaywin = 1      AND ISNULL(@Last_WinAway,1)= 1      , 1, 0)
      
                        SELECT @MaxConsecutive_WinHome_Count = IIF(@MaxConsecutive_WinHome_Count &lt;@Consecutive_WinHome_Count ,@Consecutive_WinHome_Count ,@MaxConsecutive_WinHome_Count)
                              , @MaxConsecutive_Draw_Count       = IIF(@MaxConsecutive_Draw_Count       &lt;@Consecutive_Draw_Count       ,@Consecutive_Draw_Count       ,@MaxConsecutive_Draw_Count      )
                              , @MaxConsecutive_WinAway_Count       = IIF(@MaxConsecutive_WinAway_Count &lt;@Consecutive_WinAway_Count ,@Consecutive_WinAway_Count ,@MaxConsecutive_WinAway_Count)
      -- put current values in the last bucket                  
                        SELECT    @Last_WinHome      = @homewin
                                    , @Last_Draw      = @draw
                                    , @Last_WinAway = @awaywin
            
      
                        FETCH NEXT FROM curDates INTO  @homewin, @draw, @awaywin
              END
                  CLOSE curDates
                  DEALLOCATE curDates
      
      -- new label, insert data
      INSERT INTO @ResultTable (LABEL, Max_HomeWin, Max_Draw, Max_AwayWin, Cur_HomeWin, Cur_Draw, Cur_AwayWin)
            VALUES (
            @label
            , @MaxConsecutive_WinHome_Count,@MaxConsecutive_Draw_Count, @MaxConsecutive_WinAway_Count
            , @Total_WinHome_Count, @Total_Draw_Count, @Total_WinAway_Count
            )
      
            
                  FETCH NEXT FROM curLabel INTO @label
      
          END
      CLOSE curLabel
      DEALLOCATE curLabel
      
      SELECT * FROM @ResultTable
      
      
Returns:

      LABEL      Max_HomeWin      Max_Draw      Max_AwayWin      Cur_HomeWin      Cur_Draw      Cur_AwayWin
      A      2      2      0      2      3      0
      B      0      0      3      0      0      3",1.0
g54xiel,irth91,"solution 2: (edit because copy paste issues)


       DECLARE @t TABLE (label NVARCHAR(100), dates DATETIME2(7), homewin bigint, draw bigint,awaywin bigint)
            INSERT INTO @t
            values
            ('A','2020-02-04 13:24:06','1','0','0')
            ,('A','2020-02-05 13:24:06','1','0','0')
            ,('A','2020-02-07 10:24:06','0','1','0')
            ,('B','2020-02-04 13:24:06','0','0','1')
            ,('A','2020-02-09 11:24:06','0','1','0')
            ,('B','2020-02-09 13:24:06','0','0','1')
            ,('B','2020-02-13 13:24:06','0','0','1')
            ,('A','2020-02-20 13:24:06','0','1','0')
              ,('C','2020-02-04 13:24:06','0','0','1')
            ,('C','2020-02-09 13:24:06','0','0','1')
            ,('C','2020-02-13 13:24:06','0','0','1')
      
      ;WITH denserank
      AS(
      SELECT Label, homewin, awaywin, draw
            , DENSE_RANK() OVER (partition BY label, homewin ORDER BY dates)      rankHome
            , DENSE_RANK() OVER (partition BY label, draw ORDER BY dates)            rankDraw
            , DENSE_RANK() OVER (partition BY label, awaywin ORDER BY dates)      rankAway
      FROM @t
      )
      , maxRankAway
      as      (
            SELECT label,MAX(rankaway) maxaway
            FROM denserank
            WHERE awaywin = 1
            GROUP BY [label]
            )
      , maxRankHome
      as      (
            SELECT label,MAX(rankHome) maxhome
            FROM denserank
            WHERE homewin = 1
            GROUP BY [label]
      
            )
      , maxRankDraw
      as      (
            SELECT label,MAX(rankDraw) maxdraw
            FROM denserank
            WHERE draw = 1
            GROUP BY [label]
      
            )
      , totals
      as      (
            SELECT Label, SUM(t.homewin) sumwin, SUM(t.draw)sumdraw, SUM(t.awaywin) sumaway
            FROM @t t
            GROUP BY label
            )
      SELECT t.Label, sumwin, sumdraw, sumaway , maxRankAway.maxaway, maxhome, maxdraw
      FROM totals t
      OUTER APPLY
            (
                  SELECT *
                  FROM maxRankAway
                  WHERE maxRankAway.[label] = T.[label]
            ) maxrankaway
      OUTER APPLY
            (
                  SELECT *
                  FROM maxRankDraw
                  WHERE maxRankDraw.[label] = T.[label]
            ) maxRankDraw
      OUTER APPLY
            (
                  SELECT *
                  FROM maxRankHome
                  WHERE maxRankHome.[label] = T.[label]
            ) maxRankHome


returns results


      Label      sumwin      sumdraw      sumaway      maxaway      maxhome      maxdraw
      A      2      3      0      NULL      2      3
      B      0      0      3      3      NULL      NULL
      C      0      0      3      3      NULL      NULL",1.0
g55c5v1,irth91,Thx.... is good :),2.0
g55ewee,irth91,"i also solved on mysql using procedures, thank you sincerely for the help",1.0
g52mmkb,irth91,"You need to look into using window functions to solve ""gaps and islands"" problems.",2.0
g52q5mf,irth91,"I don't know what to try anymore, the goal would be to make a materialized view to speed up the queries, but I can't get everything in one query. On MySQL I solved easily with variables, but having to create a view and above all having to use a superior DBMS (MSSQL 2012), I got stuck",1.0
g54levh,irth91,"To solve your current (home win/draw/away win) you can use window functions like 

    last_value(home win) over (partition label order by datetime)

But to solve the same thing for your home/draw/away streaks, it's a bit more complex. 

First, you'll need to add a sequential integer column like GameNumber. Make sure it increments chronologically such that game 3 doesn't come after game 4. 

Then you use the row_number() function partitioned over label, ordered by game number and filter the set to only include HomeWins (or draws or away wins) = 1. You take that row number and subtract it from the game number and you get a value that doesn't mean much on its own, but it represents your ""island""... or your streak. So that island might get a value like ""6"" and every row where the value is ""6"" is a part of that winning streak. 

You can then add a windowed aggregate like count(*) over (partition by label, island order by gamenumber) and that will give you a running total of wins in the streak. 

Sound a bit convoluted? It is. There are other ways to solve islands problems as well but that's the one I use.

Look up Bert Wagner's video on Gaps and Islands if you need a different approach. Yours is on the easier side from what he shows because you can order the games chronologically and get a sequence number without worrying about date ranges overlapping. 

Good luck.",1.0
g54pizt,irth91,"this problem is driving me crazy!
I'm in trouble, can someone help me?

Table example

    CREATE TABLE resultMatch (
    ID int,
    dataComplete dateTime,
    TypeMatch varchar(255),
    risHome int(2),
    risAway int(2),
    ResultMatch varchar(3)
    );

    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""0000-00-00 00:00:00"", 61111702, ""B"", 0, 0, ""D"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 05:30:00"", 61120827, ""A"", 3, 0, ""W"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 06:00:00"", 61119317, ""A"", 1, 1, ""D"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 06:30:00"", 61120829, ""B"", 1, 0, ""W"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 07:00:00"", 60907427, ""C"", 1, 4, ""L"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 07:35:00"", 60882247, ""A"", 0, 0, ""D"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 09:30:00"", 61120833, ""B"", 1, 0, ""W"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 09:45:00"", 61120831, ""C"", 1, 3, ""L"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 09:50:00"", 60882249, ""A"", 1, 1, ""D"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 10:00:00"", 61112203, ""B"", 2, 2, ""D"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 12:45:00"", 61112205, ""A"", 2, 1, ""W"");
    INSERT INTO resultMatch (dataComplete, ID, TypeMatch, risHome, risAway, ) VALUES (""2017-01-01 13:00:00"", 61112669, ""B"", 0, 1, ""L"");

**the result I can't get is this**

    +-----------+-------+-------+-------+--------+--------+--------+
    | TypeMatch | Cur W | Cur D | Cur D | Max W  | Max D  | Max L  |
    +-----------+-------+-------+-------+--------+--------+--------+
    | A         | 1     | 0     | 0     | 1      | 3      | 0      |
    +-----------+-------+-------+-------+--------+--------+--------+
    | B         | 0     | 0     | 1     | 2      | 1      | 1      |
    +-----------+-------+-------+-------+--------+--------+--------+
    | C         | 0     | 2     | 0     | 0      | 2      | 0      |
    +-----------+-------+-------+-------+--------+--------+--------+",1.0
g53qmsu,irth91,I've had a medical research problem for six months that I've been mulling a solution.  This is exactly what I've been looking for.,1.0
g54h9pg,irth91,"Islands Solution #3 is the flavor I usually use, where you add a chronologically sequential number on your table, calculate a row number partitioned by whatever your appropriate ""island"" of records would be, then subtract that row number from the row's sequence number. 

When you figure out what that looks like for you, it's often pretty simple, in a hindsight 20/20 sort of way. 

https://www.red-gate.com/simple-talk/sql/t-sql-programming/the-sql-of-gaps-and-islands-in-sequences/",2.0
g54i7ig,irth91,"could you write an example using the data I wrote?

unfortunately I still do not have the optimal solution",1.0
g54qn1r,irth91,"Not without being in front of a computer with access to Sql server, and about an hour to burn. You may just try your row-by-row approach on this set if you don't know window functions yet.",1.0
g51tbo7,irrisy,"I dont know what this person is saying...

""Second weakness is that you can only contain 2 keys in a relationship table""

Well... here is my relationship table that holds any number of defined relationships between any number of tables.

https://youtu.be/R6OCMpPs70U",6.0
g54dpzr,irrisy,Only weakness is when it’s too normalize there are too many join to dimension it’s very annoying,1.0
g51r6mk,irrg9h,You need to automate the grabbing and inserting/updating of the db  from the file you receive. You also need to do some validation on the file.python is a good option as a language but there isnt anything off the shelf if that's what you are asking. You can probably find a college student to do it for a few hundred.,1.0
g51rsqk,irrg9h,"I'm not asking for anything specific, just asking how others would do it. I was thinking that I could push the data to a staging table then from there, I would check it to make sure that all is well with the data then push merge it to the main db. For deletions, mark files for deletion vs actually deleting them for a few cycles so if it's an error, I can catch it prior to it damaging my system if I perform ops on it.

What do you think of something like that approach?",1.0
g54fc9m,irrg9h,"Personally I almost always use staging tables to load in raw data to a db, that way you can do any data validation when loading staging to final, not to mention it is much easier to troubleshoot standard stored procs for data validation than to do it in whatever ETL tool you choose to use for the data load from CSV.",1.0
g51new6,irrdfb,"SQL is a declarative language, in that you tell the processor what you want, and it figures out how to do it.  So in this regards it's much easier to ""learn"" than something like say, C# or Java.  

I teach intro to SQL at conferences related to the market niche I work in.  I always tell people that if you can write down what you want to accomplish in plain English, some poking around the database and the Internet will get you the rest of the way there.

Example:

*Show me all invoices for Customer 1234 for Fiscal Year 2019.*  


1. Look through database tables for something that looks like Customer.  
2. Look through database tables for something that looks like Invoices.
3. Determine the key fields and look up How to Join two Tables Together in Google.
4. Lookup how to convert date fields to just the year into Google.
5. Done

The above is overly simplistic. But it shows my methodology.  I'm a consultant so often times I'm working in databases I'm not familiar with, or into unique problems that I need to solve.  Write out the problem in plain English, start with a basic SELECT \* FROM &lt;Table&gt; query and build each discrete requirement from there.",10.0
g5gajev,irrdfb,"I would suggest you to look at the Problem Solution at the Discussion forum; the problem may require new Function that you have not heard of which will be impossible to figure it out yourself. So look at the solution, try to understand the logic and the new functions that are being used, come back and try to solve it by yourself after a few days or a week",1.0
g51ac01,irqlup,"Try using a not exists in the where statement to get the newest record. 

Select ....
From table1 a
Where not exists (SELECT 1
                                FROM table1 b
                                WHERE a.pk = b.pk
                                 AND a.date &lt; b.date)",2.0
g549yd0,irqlup,I want to get the most recent prior to the pipeline snapshot date. I think this would give me the one afte,1.0
g4zpl6s,irm4rg,"Maybe think of something that you could personally use and build on in the future. Like a recipes database, with ingredients and the recipes themselves. Just come up with a sketch and build from there to where you want to take it. Once populated you can interface said recipes into a report in SSRS or Crystal Reports. If you want to share it, put it on GitHub and then put the link on your CV/LinkedIn etc.

If you just want exercises, I've got SQL Practice Problems by Sylvia Moestl Vasilik, 57 problems ranging from beginner to advanced and the book itself is fairly cheap.

Edit: I put recipes as an example, but you can do whatever you're comfortable and familiar with so you know a rough structure and how things tie in together if that makes sense.",2.0
g52lb4f,irm4rg,Thanks for ur kindness :),1.0
g54oa7m,irm4rg,"Happy to help!

Sorry I forgot to answer the first part of your question, but what's worked for me was passion for the field and willingness to learn. I've shown this by working towards certifications. I got 98-364, working on three more, scared out of my wits because I don't do exams well, but hey, potential employers want to see you're pushing. You can word your dedication strongly in your cover letter, it helps show you're not just looking for your next paycheck.

While looking for work in your field you may want to consider an entryway job like a system admin or something that will touch database work. It's really down to luck, and where you are. Unfortunately the pandemic has kind of squashed opportunities for entry level professionals, you may get an interview but someone can and will undercut you. Words from a recruiter I've spoken to recently. That and I heard recruiters reposting old positions that are no longer available. It's messy out there, but nothing's stopping you from directly sending your CV out to various companies with a short and sweet message as to why you want to work with them and what would you bring to their team.


Hope this helps!",2.0
g57r375,irm4rg,Really scary @posting old positions? Why somebody would do that? :/,2.0
g57yrsv,irm4rg,"Recruiters do this so they look busy and popular, almost like companies wouldn't be aware that barely anyone is hiring anyway. A job I applied for back in March has been reposted twice a week on average ever since, and they work with the NHS. Go figure.",2.0
g57zj7w,irm4rg,"Thanks for sharing ur experience, u know, this covid19 pandemic it's really caotic. It's very hard to understand what do next :/",2.0
g586l8c,irm4rg,"Chin up and take things one at a time. Best you can do is keep applying, all the while keep learning new stuff. And point that out when you get contacted, that you used the pandemic lockdown as an opportunity to learn more/deepen your knowledge. Don't get discouraged if you can't find anything just yet, it's just the climate, it's out of your (or anyone's) control.

Now, depending on what you want to do in the long run, whether it's querying or administering a database, check out Brent Ozar, he specialises in query tuning but more on the DBA side. Still very useful if you're aiming for the querying side. He's got a course he's doing for free this month on index tuning [right here!](https://www.brentozar.com/archive/2020/09/sqlbits-attendees-here-are-your-prerequisites-for-mastering-index-tuning/) if that's of any interest to yourself or anyone you may know is interested ;)",2.0
g587dkz,irm4rg,"Thanks still learning ahaha, doing hackerrank as much as I can. I want to be able to query well and effectively. Don't know in which side ahaha it will depend on the opportunities I believe :)",2.0
g4zuwnp,irlxky,"&gt; the minimum in order of left to right of (val1, val2, val3)

Huh? I think I understand what you mean, but the phrasing is a bit confusing.

&gt; I know one way to do this is to add a `ROW_NUMBER()` from a `PARTITION BY (val1,val2,val3) ORDER BY (val1 desc, val2 desc, val3 desc)`

Isn't your logic wrong? You need to sort ascending, not descending, to get a minimum at row number 1, and you made it sound like you need to partition by ID not by the values themselves.

    SELECT *
    FROM (
        SELECT ID
            ,value
            ,val1
            ,val2
            ,val3
            ,ROW_NUMBER() OVER (PARTITION BY ID ORDER BY val1 ASC, val2 ASC, val3 ASC) AS rn
        FROM UnnamedTable
    ) s
    WHERE s.rn = 1

&gt; but this is a huge table and this seems pretty inefficient

It feels inefficient, but in my experience with SQL Server it's usually the highest performing method *by far*. Notice that the task is embarrassingly parallel since each partition is distinct from the others. If there are a lot of partitions, then it's a ton of parallel tasks. And it's easy for the query engine to see that. That means it's often almost I/O bound, and no matter what you do you already can't get around needing to read every record of this table without a WHERE clause limiting it in some way.

You do need to decide if you care about ties. That is, if you should use `RANK()` or `DENSE_RANK()` instead of `ROW_NUMBER()`. My instinct would be to add the primary key to the sort order to the order if you decide you don't care about ties but you need the output to be deterministic for the value of `value`.",9.0
g4zyrde,irlxky,"I'm not sure I totally understand you but it seems you want to select the row that has the minimum sum of the three values, no?",2.0
g525y9n,irlxky,"Typically Postgres' `distinct on ()` performs better than using window functions:

```sql
select distinct on (id) *
from the_table
order by id, val1 desc, val2 desc, val3 desc;
```",2.0
g53wicz,irlxky,"Hey, this is amazing. I never knew about `DISTINCT ON` and it's exactly what I need.",1.0
g52ekkm,irlxky,"`SELECT`

`ID,`

`SUBSTRING(MIN(lpad(least(val1, val2, val3)::varchar, 10, '0')||lpad(value::varchar,10, '0')||lpad(val1::varchar, 10, '0')||lpad(val2::varchar,10, '0')||lpad(val3::varchar, 10, '0') from 11 for 10)::NUMERIC AS value`

`SUBSTRING(MIN(lpad(least(val1, val2, val3)::varchar, 10, '0')||lpad(value::varchar,10, '0')||lpad(val1::varchar, 10, '0')||lpad(val2::varchar,10, '0')||lpad(val3::varchar, 10, '0') from 21 for 10)::INTEGER AS val1,`

`SUBSTRING(MIN(lpad(least(val1, val2, val3)::varchar, 10, '0')||lpad(value::varchar,10, '0')||lpad(val1::varchar, 10, '0')||lpad(val2::varchar,10, '0')||lpad(val3::varchar, 10, '0') from 31 for 10)::INTEGER AS val2,`

`SUBSTRING(MIN(lpad(least(val1, val2, val3)::varchar, 10, '0')||lpad(value::varchar,10, '0')||lpad(val1::varchar, 10, '0')||lpad(val2::varchar,10, '0')||lpad(val3::varchar, 10, '0') from 41 for 10)::INTEGER AS val3`

`FROM table`

`GROUP BY ID;`",1.0
g53f7sk,irlxky,"SELECT id, val1+val2+val3 as new_val_col
FROM table_name
GROUP BY id,
ORDER BY new_val_col
LIMIT 1;

Pls check if this works.
This must be faster.",1.0
g4y544d,irf9hk,"I have to look up a pivot example every time I use one. 

https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-ver15
    
    SELECT 'AverageCost' AS Cost_Sorted_By_Production_Days,   
    [0], [1], [2], [3], [4]  
    FROM  
    (SELECT DaysToManufacture, StandardCost   
        FROM Production.Product) AS SourceTable  
    PIVOT  
    (  
    AVG(StandardCost)  
    FOR DaysToManufacture IN ([0], [1], [2], [3], [4])  
    ) AS PivotTable;  
    

You're missing an aggregator (SUM(), MAX(), MIN(), AVG()) inside the pivot()",2.0
g4yzs42,irf9hk,"Or you can use case statement which helped me in converting rows to columns
Like 

Count (case Statement) as 'Produxt_a'",2.0
g4y53yi,irf9hk,"You could select the VendorID and VendorName, then just left join the program counts to itself.  


    select distinct a.VendorID, a.VendorName, b.Program_Count as Program_1_Count, c.Program_Count as Program_2_Count
    from Table a
    
    left outer join Table b on a.VendorID = b.VendorID and b.Program = 'Program_1'
    
    left outer join Table c on a.VendorID = c.VendorID and c.Program = 'Program_2'",1.0
g4y55c3,irf9hk,"Missing the aggregate for Product_Count and you need to use brackets instead for the columns:
    
    select *
    into #temp
    from
    (
    select 'A' as VendorID,	'VendorA' as VendorName, 'Program_1' as Program, '123' as Product_Count
    union select 'A' as VendorID,	'VendorA' as VendorName, 'Program_2' as Program, '234' as Product_Count
    union select 'B' as VendorID,	'VendorB' as VendorName, 'Program_2' as Program, '147' as Product_Count
    union select 'C' as VendorID,	'VendorC' as VendorName, 'Program_2' as Program, '258' as Product_Count
    union select 'C' as VendorID,	'VendorC' as VendorName, 'Program_1' as Program, '369' as Product_Count
    ) a
    
    select * from #temp
    
    
    select VendorID, [Program_1], [Program_2] 
    from 
    (
    	select VendorID, VendorName, Program, Product_Count 
    	from #temp
    ) as SourceTable 
    PIVOT 
    (
    	max(Product_Count) for 
        Program IN ([Program_1], [Program_2]) 
    ) as PivotTable",1.0
g4x96y1,irbau3,"1. From [https://www.w3schools.com/sql/sql\_any\_all.asp](https://www.w3schools.com/sql/sql_any_all.asp):

ALL means all values must hold true.

i.e. the first part is saying show me where the area&gt;= all the areas in that continent. There is only going to be 1 area that is equal or greater than all the areas in that continent (i.e. would be the topmost one).

2.  It is a nested query. Even though it performs it in batch, you can think of it row-by row to work out what is going on: 'For every row in world x, select all areas from world y where the continent is the same.' Essentially what it is doing is reaching out of scope from the current row in world to refer to ALL areas for that continent by doing a self join. 

ie. by joining on continent only, essentially the inner query is returning all areas for that continent. Then the ALL kicks in saying our area must be greater than all of those areas.

If we didn't have continent in join clause, the same query would return the greatest area in the entire world.",2.0
g4xqznw,irbau3,"&gt; in the WHERE clause in the 2nd select, can you explain this in as much detail as possible? 

it's a **correlated subquery**

each row in the outer query is *correlated* to only those rows in the subquery for the same continent",1.0
g4xr7ag,irbau3,"by the way, instead of using ALL, another way to write this query is

    SELECT continent
         , name
         , area 
      FROM world x 
     WHERE NOT EXISTS
           ( SELECT area 
               FROM world y 
              WHERE y.continent = x.continent
                AND y.area &gt; x.area )",1.0
g4x6ody,irb3hk,"This:

&gt;select (x/y) as z from mydata group by ID

Is a syntax error in most RDMS that I know...

You need to only include columns that are either in the group by, or enclosed in aggregates.

Surely this doesn't actually run?",8.0
g4x8tb2,irb3hk,"As I recall, MySQL 5.7 allows you to include unaggregated fields in a statement with 'goup by'.",3.0
g4xcotz,irb3hk,"provided you turn off ONLY_FULL_GROUP_BY, which is on by default starting with 5.7.5",1.0
g4xf0ks,irb3hk,That's the SAS quirk. SAS forces this code to run with a warning. I think that's why it flew under the radar.,1.0
g4xuydu,irb3hk,"PROC SQL is only barely SQL.  From the doc: ""When you use a GROUP BY clause without an aggregate function, PROC SQL treats the GROUP BY clause as if it were an ORDER BY clause and displays a message in the log that informs you that this has happened.""

http://documentation.sas.com/?docsetId=sqlproc&amp;docsetTarget=n0tf6s2l1rfv5ln1o04ojc4rotu1.htm&amp;docsetVersion=9.4&amp;locale=en#p1glsp7fzgs7i5n1sqquuwkflljb",10.0
g4xval6,irb3hk,Thanks! The documentation is helpful.,3.0
g4ybj3e,irb3hk,"&gt; where GROUP BY is used without an aggregate function

Can't speak too much without seeing the actual code and following the logic, but is this being used as a proxy for distinct to remove duplicate records?",5.0
g4ycjfi,irb3hk,i was going to say exactly the same thing. most likely what the original person was trying to do or they backed out an aggregate/testing for duplicates and forgot to remove the group. either way its effectively a syntax error depending on the platform or a distinct.,2.0
g4yjlob,irb3hk,"You might want to cross post in r/SAS... PROC SQL adheres to an older ANSI standard and doesn’t include window functions, plus a bunch of other minor differences exist.

Are you assuming that the author of this code knew what they were doing, or does it seem like it was sheer luck it didn’t totally bomb?",2.0
g4yjqa7,irb3hk,I think professionally I have to assume sheer luck. XD,2.0
g4wqkic,ir72wj,"Why do you have 7 columns? I would expect only a single foreign key between `schedule` and `class`, something like: 

```sql
create table class
(
  class_id   integer primary key,
  class_name text not null
);

create table schedule
(
  schedule_id integer primary key, 
  room_nr     integer not null, 
  weekday     integer not null check (weekday between 1 and 7),
  start_at    time not null, 
  end_at      time not null, 
  class_id    integer not null references class
);
```",1.0
g4y1t7v,ir72wj,"I was going to link a user to a schedule and in the that schedule, have their classes for all seven periods, and each period to a class. That way to get a user's schedule it would be easier. Is there a better way to do this?",1.0
g4y2bgk,ir72wj,"I didn't know what exactly that ""schedule"" table is supposed to be, so I assumed it's scheduling classes during a week (e.g. English, Mondays between 10:00 and 12:00) 

So basically your schedule table should have a column `period`, and then you'd have 7 rows in that table for the 7 periods (similar `weekday` in my example)",1.0
g4whj26,ir65nz,"Well you’ve come to the right place. No need to pay. Just tell us what you need help with, what you tried and we can help you.",27.0
g4wkgkq,ir65nz,Just paste the questions here and get ready to be dazzled!,12.0
g4xc6pp,ir65nz,"Make a post with the exercise you're having problems with, along with whatever portion of the answer you have, thus far - even if the query doesn't work, show us what you've got, along with an explanation of your logic. Then explain where you're stuck, and we'll get you on the right track.",7.0
g4wt7kv,ir65nz,Everyone loves a good problem to solve,4.0
g4wtri7,ir65nz,As long as you post a complete question (with DDL/DML) then I’d be happy to help.,1.0
g4wwjjz,ir65nz,I can help you. DM.,1.0
g4x6n6r,ir65nz,Yeah just post it here. Happy to practise.,1.0
g4ycfum,ir65nz,I will do it for free,1.0
g4yjk33,ir65nz,I can help you. DM me,1.0
g4wu7jw,ir65nz,"If you don't come right with all of the free help, have a look at unemployed professors website.

But I would go with the free help from everyone and learn how to do it.",-1.0
g6j0x2p,ir65nz,"that website is quite expensive. I can help with most reasonable cost, or even free in some cases that do not involve writing long papers. If you use the website, consider talking to me too. Thank you.",1.0
g4vqstd,ir2dfh,Hey everyone! Blog author here. I'll answer any questions that you may have!,1.0
g4xgd2p,ir2dfh,"I see in the documentation for Dolt that database renames aren't supported, is there any plan to implement this? 

This is rather vital for large scale data architectures when doing manual restores.",1.0
g5ceuvy,ir2dfh,"I apologize for the late reply! I was relying on Reddit notifications and I must have missed it.

Currently, whenever you initialize a dolt directory (using `dolt init` just like with git) and run `dolt sql-server` inside of it to start a MySQL-compatible server, it defaults the database name to the directory.

If the server is running in `--multi-db-dir` mode [(docs here)](https://www.dolthub.com/docs/reference/cli/#dolt-sql-server_options), then each subdirectory is its own database, and the name of the database matches that of the subdirectory. To rename a database in this instance would be to rename the directory. As a result, we currently don't have any functionality built into dolt to support renaming the database.

If this is a use case that you're interested in, then feel free to [submit an issue](https://github.com/liquidata-inc/dolt/issues) (or I can do it in your place). As well, you can [join us on Discord](https://discord.com/invite/RFwfYpu) and we can all discuss it with you! This would probably be the fastest way to get an implementation, as we tend to prioritize the issues and features that our most active users find and desire :)",1.0
g4vgbzk,ir06jl,"Yep. In SSMS, click Tools -&gt; Options .. Query Results -&gt; SQL Server -&gt; General.

Click the “Play Windows default beep...” checkbox.",15.0
g4vhdt2,ir06jl,Thank you!!!!!!!!!,4.0
g4x0973,ir06jl,How can we enable this for MySQL workbench ?,2.0
g4wz1sj,ir06jl,"I turned this on a couple of weeks ago, and have zero regrets.
I do wish you could change the sound file though!",1.0
g4vyy11,iqzgb3,"If I was the interviewer, which I am sometimes, I wouldn’t care about your certifications. Just means you studied enough to pass a test. I want to see how you can apply your skills to solve problems.",23.0
g4w4bb3,iqzgb3,I always get tests on the last interview.  That’s when they make the actual decision.  I have few token MS certs but nobody cares.,5.0
g4wuuq6,iqzgb3,You're probably right but don't you think that getting a certification can ease the process of getting an interview ?,2.0
g4wponc,iqzgb3,What do you ask?,1.0
g4wq0r0,iqzgb3,"Besides basic things like explaining the types of joins, I want someone to be able to have a conversation with me about something like performance optimization and trade offs. How would you design a table or ETL workflow to solve a business problem? I don’t give a shit if you know some SQL Server or Oracle proprietary thing that isn’t relevant anywhere else. When I ask SQL questions I am watching how fast you can write a query and how you write it. I can tell if someone is guessing or intuitively gets it.",3.0
g4xdgxf,iqzgb3,"To a non-technical hiring Manager, it *can* improve the chances of your resume winding up in the pool of resumes to review. But for me, certifications have absolutely no influence on the hiring decision. 

It shows commitment to see something through entirely which is a good quality to have but beyond that, it's really just something people like to slap on their email signature. It is no measure of ones ability.",4.0
g4wcrks,iqzgb3,I am trying to change careers and I was thinking of writing some SQL code on a gitbub account instead of trying to get a certification.,3.0
g4vx9lx,iqzgb3,Anyone if any free certifications are being provided like one provided by Oracle couple of months before which I missed.,1.0
g4xzt1j,iqzgb3,"None.  Been doing SQL a LONG time, have never been asked about a certification.  Been asked questions/tested on my knowledge of the various elements and concepts a lot, though.",1.0
g4uthct,iqwd5e,"I think STUFF() with FOR XML should get you where you need to go.

&amp;#x200B;

Here is a stackoverflow explanation of how it works.  [https://stackoverflow.com/questions/31211506/how-stuff-and-for-xml-path-work-in-sql-server](https://stackoverflow.com/questions/31211506/how-stuff-and-for-xml-path-work-in-sql-server)",1.0
g4wyjep,iqujyr,"But I want to really understand it, and reach a point where I can understand joins between multiple tables on different conditions. To visualize it in my mind, and also be able to execute complex select statements.

I think [this tutorial](https://www.toptal.com/database/sql-indexes-explained-pt-2) gives you a good start. It is all about visualization and understanding how queries work under the surface.

I figured solving problems would be the best way to do it. Like, ""here are these tables, i want this specific data out of it"".

That is the right approach. Walkthrough exercises in the tutorial, then solve specific problems. Learn to read execution plans. If you work on the OLTP area,  in most cases, you will see nested loops join explained in the tutorial.",1.0
g4tyhu7,iqroym,"Two things come to mind:  


* it's a big plus when hiring someone when the candidate can discuss a project they created / worked on. Having something interesting on your list of accomplishments goes a long way to being viewed as a good prospective project contributor. It helps you be able to talk about the challenges in that project, and how you overcame them.
* one of the big differences between a small personal project and a larger team project is testing and maintainability. When it's a 1-person project, you can have the whole project in your head and make good global decisions easily. When it's a larger project, you will not be an expert in all parts of it, and will rely on having structured tests when you want to modify another part of it. Writing tests gives you the freedom to modify and refactor code and know that it still meets expectations. So being able to show that your personal project has well-laid-out unit tests is a way to prove that you're ready to work in a larger collaborative environment.",31.0
g4vzy42,iqroym,"Commenting to stress TESTS! I’m interviewing data engineer candidates right now, and a ridiculous number fail out because they don’t even include tests as a TODO item in their exercises. If you include tests, you are demonstrating the kind of collaborative and thoughtful practice that will make you a great investment, even if we have to train you up on some other stuff. Write tests, and if you can’t (because it’s a take-home and you ran out of time, say) write that tests are a known TODO.",10.0
g4ucwit,iqroym,"i work for a large US based telecom, and my day to day is SQL and Python.   I started out as a call center rep, taking calls.  9 months later i found myself training new hires, then 4 months after that i was put back on the phones. after 6 months of that i was able to get a ""promotion"" to QA (listening to calls and scoring).   

This was the stepping stone for me.  I really enjoy logistics, so i took my small team and i make some systems to keep us on track.  i was using things like sharepoint and infopath.  i had touch on access briefly to where i could understand the super basic concepts.  I didnt really like access so i ended up making some infopath/sharepoint workflows for the team, and started to get into excel.  that was 2004-2005.  

a position opened up for a reporting analyst.  it was remote to the point where i was still in the call center, but my boss was out of state.  I started using excel more and more. I eventually took up reporting the survey metrics for the nation. 

then one day all of my data wouldnt fit into a single excel sheet. i took up learning ms sql so i could store the data, and use excel to make connections to the DB in order to supply the pivot table report.  

things just progressed from there. MS SQL, SSIS, SSAS. Then teradata, vertica, oracle.  

About 2 years ago now, the company decided it would no longer pay for ms sql licenses so i needed to get out.  By this point i was doing a lot of ETL work and less reporting.  my main toolset was SSIS to move data around the different environments.  

we were told to move away from licensed products and to open source.  Luckily i was able to keep my MS SQL box, but i started picking up python.  my paired programmer and i started tooling around with python, and man, did it ever blow up.   Its all we use now.

Python controls everything we do, and it is our daily use tool. We use it for our ETL, to monitor our services on our servers, move files, create logging for what we do.  

I am on a small team, we have 10 people, and 3 of us are the hard core developers.  the others are a mix of project managers and analysts that use the datasets we create and maintain.  

It is a really good and fun (to me at least) job.   I love it and it is my career.   

so when you ask what you can do with python and SQL knowledge i would answer most simply.   

ETL
analysis
front end tools (flask and django) if you like that sort of thing.  
and anything else you can figure out to make everyone's life simpler on a computer.  especially in a corporate environment.


edit:
to add on to that, im now making really good money for where i live (deep south, very close to where hurricane laura just hit). Im completely remote and have been for almost 10 years now.  I can work from wherever i have an internet connection as i have built up the trust levels between my team and management.  all of my coworkers are remote. a couple in texas, a few in atlanta, on in montana, some more scattered around california.  i am able to make double the average pay for my area and never leave the house until i want to.",18.0
g4ux3tx,iqroym,"This is my exact career as well. Minus the QA part but worked very close with them. I was on the billing team for my MSO. I started their use of scripting languages as well. 

For anyone wanting a career... Starting off in a call center and showing some drive is a really good place to start.",2.0
g4uxzq4,iqroym,But you have to admit.  If you get stuck on the phones it will end you...   taking calls is draining.   However it is usually super easy to move up if you show any kind innitiative.,1.0
g4uybr2,iqroym,Yes. If you are not out in two years it is time to move on.  It means that either you are not showing enough initiative or they are not looking to actually promote anyone. The bad thing about where I worked was that they were not willing to pay but you got the all-important experience.,1.0
g4wnf2f,iqroym,"Great story, thanks for sharing.  It's amazing how we end up where we end up in our careers.  I kinda fell ino excel/sql as well and then it all blew up from there.  Been learning and using python over the last year, really love it.",1.0
g4xhecy,iqroym,We've had the same career. 🙌,1.0
g4u9ovb,iqroym,"A question of code podcast is really good for people specifically learning and making the transition into a dev role, one of the hosts has recently changed from a career in teaching to back end coder so maybe useful for you.",3.0
g4u9bjy,iqroym,"Not sure where you're located (assuming USA, but these same concepts should be applicable in any market I imagine), but regardless of location you'll need to check out local programmer jobs nearest you. Those will be the easiest to get, unless you're in a place like NYC or Seattle then your search will be tougher as places like that attract talent from international places.

Find a posting locally that hopefully lines up with your experience and go for it. If there's not one that lines up with your experience, take note of what skills they're asking for and take that as what you need to learn. Find some material online and go through some on each. Create something with it, if possible.

Try for that job posting once you're comfortable with some basic questions being asked of you. In parallel, get a hold of recruiters in your area. Let them do some leg work for you for jobs you may not have known were out there. I personally prefer bigger sized recruitment companies, but even those are mostly local. There are some national IT recruiting brands worthy of reaching out to like Tek Systems or Robert Half Technology.

You're not going to get the first job most likely, but with this approach you'll atleast get the interview and learn something from the questions they ask you.",2.0
g4ufi9p,iqroym,Lots of stuff uses those two. Data Engineering is fun and lucrative.,2.0
g4u71tf,iqroym,"I don't know what kind of job you are looking at -  


since you said the front end design, it sounds like the backend which would be sql/database developer. You should know at least one programming language and connect with database to fetch the data and use them in the right place.  


data analyst: try to know how to generate reports, clean up data (\~50% or so of the work lol), know how to change from business talks to technical talk  


database admin: know how to deploy databases, backup, optimize/tune the databases/queries",1.0
g4uc14e,iqroym,"Bi developper

BI analyst",1.0
g4x0t2l,iqroym,"My recommendation is to build a portfolio.  You could get involved in open source projects and make contributions first.  Then once you have a portfolio, you can take it to an employer.",1.0
g4ua34w,iqroym,"I'd recommend going into plumbing. At least when you're a plumber, you know a big part of your job is helping other people handle shit they don't want to deal with.",0.0
g4ttdca,iqqruq,"No changes to data → no chances to forget to update result → no reason to calculate on fly.

You first case is about histrory, you won't change it, but you'll read it often, so store this data and don't bother.

The second one is about future and is always a subject to change. If your match just predicts when a game'll happen, then calculate it. And then when both players agree with some date, store it, cause now it's their decision. Or if it never happens at all, you'll just have datetime of game in game's table, and actually there is no reason to store shedule table after that at all. If it's still needed for history reasons, then you should store that dates (maybe use `ifnull` in the view to either return existing value, or calculate on fly), cause calculation algorithm may be changed later, but you must have same values for old data.",1.0
g4twdb1,iqqruq,"That was pretty much my assumption. Especially since we will be working with a LOT of aggregated statistics from the first example. The second example is comparing stored data to see if there was a scheduled match for when that game was completed, but you're right, there are a lot of use cases to update this result (ie. a match wasn't schedule for a certain time period, but later we decide to create one retroactively. Or multiple matches within a time period and we could need logic to determine which one is the correct match, so that one being calculated makes sense. The second use case I was pretty certain needed to be calculated dynamically, but it was the first I was most uncertain on so thanks for the help!",1.0
g4tfw1n,iqpinm,"So, this is my first technical post using Hashnode's platform. Here I discuss a bit of the basic concepts of SQL, hope you enjoy! :)",1.0
g4t295d,iqmpeo,"It'll return a single row with a single value, 1. If you can get that result back then you've shown that you can connect to the DB and that it can return a result, so it's at least running.

As for what table it queries, I don't think that it queries any. You can write that query equivalently as 

    SELECT 1 FROM DUAL; 

(like Oracle) but if you read the MySQL documentation this is purely for compatibility and DUAL is not required.",8.0
g4thmpa,iqmpeo,"This query tests if the database is alive because it asks the database, ""Hey database, give me one row with the value of 1."" The database responds: ""Here you go, here's 1."" Database is alive.  
  
Of course you could ask it something more complex, like a count from some table, but the point is it's supposed to be as simple and quick as possible, and you don't actually care about looking at any ""real"" data.  
  
So when you configure a database driver for your application, and/or perhaps a connection pool, it will run a query like this, or expect you to tell it which query to run, which would usually be just this.  
  
A connection pool could actually be running this query to ensure that the connections it maintains don't expire, by the way, more than it's checking if the database server is alive.",6.0
g4u0c2x,iqmpeo,One could think that disconnecting and reconnecting would be easier but it is much more time and cpu consuming thing to do with all the authentication and connection establishing.  So if that query fails the connection pool can kill it and start a new one instead.,2.0
g4te8co,iqmpeo,"It helps to use EXPLAIN as it tells you no tables read.

 MySQL  localhost:33060+ ssl  test  SQL &gt; EXPLAIN SELECT 1\\G

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* 1. row \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

id: 1

  select\_type: SIMPLE

table: NULL

   partitions: NULL

type: NULL

possible\_keys: NULL

key: NULL

key\_len: NULL

ref: NULL

rows: NULL

filtered: NULL

Extra: **No tables used**

1 row in set, 1 warning (0.0005 sec)

Note (code 1003): /\* select#1 \*/ select 1 AS \`1\`",5.0
g4u0h2b,iqmpeo,"Thanks for that, was wondering what the plan would show, but not enough to go and install MySQL to check.",1.0
g4t9dyt,iqmpeo,"If ""select 1"" works or not depends on the DBMS you're using. It works on e.g. MSSQL but it doesn't work on Oracle Database. There you have to specify ""select 1 from dual"" as stated by other commenter.",3.0
g4tea6e,iqmpeo,I don't think this tells you anything about the DB.  It doesn't address anything but the interface.  It's about the same as PRINT 1 or console.log(1) or console.writeline(1). It's returning what you give it.,0.0
g4tkc8n,iqmpeo,"It sends a request to the database, the database responds. It tells you that the database is responsive.

Unless you have a clever system which realises the triviality of the request and thus bypasses actually sending it.",4.0
g4sozun,iqkau3,https://stackoverflow.com/questions/707335/t-sql-cast-versus-convert,3.0
g4sqaqo,iqkau3,"That pretty much says most of what you need to know. I prefer to use CONVERT always because my code doesn't need to be run anywhere except SQL Server, sometimes CONVERT does stuff CAST can't, I don't really like the syntax for CAST, and there's no point in mixing usage just to be ANSI Standard sometimes.",1.0
g4svh8l,iqk5vs,"Know how to create SQL functions, Stored Procedures, Views, how you might go about exporting the data to a BI tool",1.0
g4tnkse,iqita0,Could you do this in SQL? Yeah. Should you do this in SQL? Probably not. This sounds like logic that should be baked into your application layer.,2.0
g4tnv2m,iqita0,"Unfortunately I don’t have that flexibility on the app side, my options are SQL or Java; and I don’t know enough Java to get the job done.",1.0
g4tqx1n,iqita0,"Here's a potential technique. I'd worry about race conditions/collisions depending on throughput of this method. If you enforce a PK constraint on the #NewSKU table (in this example) you might get failures if the procedure returns the same value twice. Not sure how likely this is to occur though. Additionally this is a little brute force-y since its generating a large pool of possible valid SKUs. You could keep track of the last issued key by Location and then increment from there instead of starting at one.

    CREATE TABLE #StorageLocation (
    	LocationId varchar(7) PRIMARY KEY
    )
    
    CREATE TABLE #LegacySKU (
    	SKU varchar(14) PRIMARY KEY
    )
    
    CREATE TABLE #NewSKU (
    	SKUID int IDENTITY(1,1) PRIMARY KEY,
    	SKU varchar(14)
    )
    
    /* Not used, but useful for reference */
    INSERT INTO #StorageLocation
    VALUES
    	('000001'),
    	('0000001'),
    	('000002'),
    	('0000002');
    
    INSERT INTO #LegacySKU
    VALUES
    	('00000100000001'),
    	('00000010000001'),
    	('00000200000001'),
    	('00000020000001'),
    	('00000100000003'),
    	('00000010000003'),
    	('00000200000003'),
    	('00000020000003');
    GO
    
    CREATE OR ALTER PROCEDURE #GetSKU
    	@LocationId varchar(7)
    AS
    BEGIN
    
    	WITH tally AS (
    		/* Tally Table 
    		   Source: https://www.sqlservercentral.com/blogs/tally-tables-in-t-sql
    		*/
    		SELECT
    			ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS n
    		FROM       (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n)
    		CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n)
    	),
    	possible_skus AS (
    		SELECT
    			@LocationId + RIGHT('00000000' + CAST(n AS varchar(14)),14 - LEN(@LocationId)) AS SKU
    		FROM tally
    	)
    	SELECT TOP 1 SKU
    	FROM possible_skus
    	WHERE SKU NOT IN (
    		SELECT SKU
    		FROM #LegacySKU
    	)
    	AND SKU NOT IN (
    		SELECT SKU
    		FROM #NewSKU
    	)
    
    END
    GO
    
    INSERT INTO #NewSKU (SKU)
    EXEC #GetSKU @LocationId = '000001';
    
    INSERT INTO #NewSKU (SKU)
    EXEC #GetSKU @LocationId = '0000001';
    
    INSERT INTO #NewSKU (SKU)
    EXEC #GetSKU @LocationId = '000002';
    
    INSERT INTO #NewSKU (SKU)
    EXEC #GetSKU @LocationId = '0000002';
    GO 3
    
    /*
    	Should return 12 rows, 4 ending in 2, 4 ending in 4, 4 ending in 5
    */
    SELECT * FROM #NewSKU
    ORDER BY SKUID;
    
    DROP TABLE IF EXISTS #StorageLocation;
    DROP TABLE IF EXISTS #LegacySKU;
    DROP TABLE IF EXISTS #NewSKU;
    DROP PROCEDURE IF EXISTS #GetSku;",2.0
g4ttzio,iqita0,"Fantastic, thanks for that. I'll give it a try.


Just out of curiosity, what are your thoughts on pre-generating all possible SKU numbers as a table + the Legacy list, and then just have a boolean indicator to denote if a SKU had been used or not?


I did some napkin math and it seems like the possible numbers (given the way the Storage Locations are configured) + the legacy IDs only tops out around 250k +/- 50k.


There is almost no way in the next 50 years that this customer would ever break through that ceiling, so from a resources perspective; would it be better to do it on the fly (like in your example) or just pre-generate them, grab one that hasn't been used, update it's flag and call it good?",1.0
g4tubh8,iqita0,Definitely could go either way! Storage is generally cheaper than CPU so it'd probably be better to pregen a table of the possible values like you outlined.,2.0
g4tui8j,iqita0,"okay, well I really appreciate your help; it's always good to have options. 


Those tally tables are cool, that's the second time I've seen them referenced today, but I had never heard of it before.",1.0
g58xm1t,iqita0,"Hey again, still working on this. I ended up not creating the master table, and used a modified version of what you suggested here. But now I am struggling a little bit with turning it into an actual Stored Procedure. Any advice on how to do that?",1.0
g4pjv3y,iq6rur,"`HAVING`, not `WHERE`",2.0
g4pxioz,iq6rur,"I'm not following.

Should I post the entire query?",1.0
g4qkuf1,iq6rur,"that's a highly unusual move (letting us see what's causing the problem, rather than having to guess) but i would risk it

apologies for sarcasm, it is meant in a friendly way",1.0
g4qwee9,iq6rur,"hahahaha my bad, I posted the entire query in the op.

thanks man",1.0
g4r4glr,iq6rur,"according to your posted image, the error message is

    Unknown column 'Available' in 'where clause'

maybe my eyes are failing me, but i don't see any of your WHERE clauses mentioning `Available`

by the way, the WHERE clause at the end has 3 conditions, but one of them is stated twice

furthermore, by requiring those join columns to be NOT NULL, you're negating the purpose of a LEFT JOIN -- just throw away those conditions and make them INNER JOINs",1.0
g4rnbq8,iq6rur,"Oh yes, sorry, I translated from Spanish so I could show it here in English. You can find it in the query as DISPONIBLE.

I'm sorry I'm a newbie, what should I do in the ""where""? I tried made them INNER JOIN and didn't work and I also disabled them with ""--"" but still getting same error.",1.0
g4rs1bs,iq6rur,"    SELECT ...
         , stock.Fisico            AS Fisico
         , reserva.Pendiente       AS Reservado
         , ventas.Bajas            AS Bajas
         , stock.Fisico - 
           reserva.Pendiente       AS Disponible
      FROM api_articulo articulo
    INNER 
      JOIN api_marca am 
        ON am.id = articulo.marca_id 
    INNER 
      JOIN api_iva iva 
        ON iva.id = articulo.iva_id 
    INNER
      JOIN ( SELECT ...
           ) stock 
        ON stock.articulo_id = articulo.id
    INNER
      JOIN ( SELECT ...
           ) reserva 
        ON reserva.articulo_id = articulo.id
    INNER
      JOIN ( SELECT ...
           ) ventas 
        ON ventas.articulo_id = articulo.id 
     WHERE stock.Fisico &lt;&gt; reserva.Pendiente",1.0
g4qu4gr,iq6rur,You should at least post the query or explain better if you want this answered,1.0
g4qwfhn,iq6rur,"my bad, I just added the query in the op! ty!",1.0
g4pg899,iq48vz,What’s making them grow out of control? It sounds like if you just shrink them they’ll only grow back. The classic cause is a long running transaction that should have committed a long time ago.,2.0
g4qfibq,iq48vz,"Ya, I have no idea - not experienced with SQL in the slightest and would not have the knowhow to pinpoint this (or the time to learn how outside of my regular job)  As Chaos said, best bet is to hire a consultant - would love to, but thats not up to me (and the business has been trying to hire - already hired 2 who just decided not to show up to work)",1.0
g4pqtp2,iq48vz,"Best solution:

Hire a ms sql consultant to look at your problems. Pro: He can solve this without dataloss and provide help in preventing the problem. Con: That will cost you a lot of money.

Worst solution:
Stop the application that connects to the database. Do not check if there are any other database and/or active connections to the sql server. Just reboot that machine.  After it comes back online, make a full and a log backup of the database on the passive node. After that shrink the database to the smallest size. Start the application back up again. Pro: fast and easy. Cons: Possible dataloss, downtime, lots of disk i/o, the problem might come back. Application might now throw errors. AG group might not come online anymore (yeah, now you are in trouble).

Finding, understanding and solving the problem is not always easy. Start with looking for active connections. And if you find long running ones, do not just kill them. You might have to, but whatever that session was doing is now lost. It could be problems with your passive node too. Look at the other sql server and check the logs. Or your backups might have been failing and it is as simple as fixing that and then shrink the log file to a reasonable size (not smallest) and do it during low activity times.",1.0
g4pbvs3,iq47mu,"You are on the right track: 
You need 2 things to succeed:

Help column:
iif(fl_to=a,0,1) as Flag

When done use the count function in conjunction with the over (partition by tr_id, flag order by tr_id, fl_date.

So smt like this:


Select max(maxflights) from(
Select Maxflights=count(*) over (partition by tr_id, flag order by tr_id, fl_date) from tbl) innerquery",1.0
g4rekdb,iq47mu,"I fully understand your answer, thanks! However a noob question: how do I append the help column to the existing table tbl? Which commands?",1.0
g4t7tlo,iq47mu,"I did find how to append the help column and I do get what you mean: you want to code the above as 0-&gt;1-&gt;1-&gt;1-&gt;0-&gt;1, then count over the partitions and then take the maximum which leads to max(3,1)=3 in this case. However the code does not partition wel as it just counts all the 1's. Do you have an idea of how it might work?",1.0
g4oghj9,iq3fjm,"I think beginner stuff would be all about querying data and the basics of building a database. So they will know what Tables and Views are, how to make them and when to use them.

Intermediate is moving more into the how the system works rather than just using it as a tool. Indexes would be here (their types and functions). From a query perspective its more advanced functions such as (UN)PIVOT, CTEs or sub-queries and recursive functions. Additionally I would expect a continuation on database design theory (OLAP v OLTP for example) and performance improvement.

Advanced is all about getting a solid understanding of the how and why behind SQL and applying that throughout. You're going down more DBA route here. Maybe also the use of performance tools and the other tools in the suite (SSIS &amp; SSRS for example)?

I consider myself to be between Intermediate and Advanced. I have a lot of design knowledge and have dug into functions to get what I need from the system (sometimes writing my own). However my knowledge on how to administer a DB is lacking and I have glossed over some of the how &amp; why of SQL.",41.0
g4oxrqz,iq3fjm,"&gt;I consider myself to be between Intermediate and Advanced. I have a lot of design knowledge and have dug into functions to get what I need from the system

I totally feel the same. I've designed DBs, migrated them, created tons of stores procs, views, functions, etc. But I still feel like I cannot call myself advanced. I one time had to optimize a big materialized view, and a senior dev showed me how using a NULL EXISTS in the where clause was more computationally efficient than the sub queries I had. That shit  blew my mind. And man he was right. Query times went from 2 mins to 15 secs.",12.0
g4r22gt,iq3fjm,[deleted],14.0
g4r429k,iq3fjm,I found your response inspiring. I’m looking into learning SQL and was wondering what you considered a solid teaching tool?,1.0
g4r9oop,iq3fjm,[deleted],13.0
g4s2ks1,iq3fjm,"Adding it here for any other replies, but:

MS SQL definitely runs on Windows 10, and you can find SQL Server 2017 and 2019 installations using a Developer license, for free, no cost, no expiration, through Microsoft.

And if you still don’t feel like paying for Windows 10, SQL Server 2019 runs on Linux!

You can’t run SSMS on Linux though, as far as I know... but you can use Visual Studio or Azure Data Studio.",3.0
g4rjyb5,iq3fjm,This is incredible information and I’m hoping to land a junior position soon. Thank you for your time!,1.0
g4p5ftr,iq3fjm,"I don’t think it’s the ol, imposter syndrome, but it’s always incredible how more senior folks can blow your mind like that. I have a few people on my team right now that are just animals, but for different things.",3.0
g4rtrpi,iq3fjm,"One guy on my team can pick apart my code in review and make it about 3 times as efficient just from a glance. 


I've been here for ages and it still blows my mind every time.",6.0
g5612yo,iq3fjm,"Do you have any suggestions for how to learn how to do these sorts of optimizations? I feel like so much is dependent on your specific SQL dialect and how your DB is set up that I haven't found a good universal guide like ""here's the obvious way to write a query, but here's the more performant version"".",1.0
g4ov9m3,iq3fjm,"""SQL skills"" is a bit of a trigger phrase for me because it's very much overloaded, since SQL work ranges from developing a database (that is, writing the database server itself) through designing a database (using a DBMS someone else wrote to implement a data system), through writing queries and doing DBA work. People tend to lump these things together without carefully understanding what job they're looking for (or what candidate they want to hire) or how to best build a data team or ...

Anyway! Since you asked *specifically* about querying, I can leave out all the rest:

A beginner:

* Knows the tool. Can setup and use a command-line tool, a GUI tool, knows a couple of each at least. Can diagnose connection problems. Understands how to save, load, manipulate files.
* Knows the tool: understands and can diagnose errors about queries the tool might give -- doesn't say ""I don't know what 'Syntax error in your SQL statement' means"" and instead just fixes it themselves.
* Can write queries and understands all join types, sub-selects, GROUP BY, and ORDER BY clauses
* Understands how to test queries for correctness
* Understands data types and casting
* Familiar with data representation
* Understands constraints, default values, and auto-increment sequences
* Very familiar with available built-in functions (for strings, aggregation, date math, etc ...)
* These skills apply to at least one DBMS and tool set.
* You've asked only about querying, but certainly someone who's a beginner at writing queries can read and understand (if not write) a data model. They can find their way through the database and look at constraints and understand which relations exist and what they mean.

Intermediate:

* Appropriate use of transactions
* Able to implement error handling
* Understands DBMS query execution model: parsing, compilation, optimization, caching, concurrency control
* Starting to understand DBMS implementation-specific features: remote queries, I/O, recompiled, parameter management, ...
* Solid ideas about when the database should do the work and when the client application should do the work (wrt to sorting, representation, formatting, aggregation, etc)
* Understands locking, isolation levels, and concurrency control
* Appropriately applies more structural query models -- views, CTEs, pivot, stored procedures, UDFs, ...
* Working with windowed functions
* Starting to show competency with multiple DBMSes
* Better at understanding models, including complicated relationships. Some ideas about when one relationship model might be better than another.

Advanced:

* Mastery of skills in more than one DMBS product
* Understands query plans or EXPLAIN output
* Understands non-traditional RDBMS constructs and SQL application (column stores, streaming or distributed stores, ...) and their applications
* knows how to diagnose and correct query performance issues
* Able to identify and correct indexing problems with appropriate indexes and types
* Capable of identifying and remedying concurrency issues
* Knows when the model is the problem rather than the query (or indexes or ...) and can work to help fix it. Not *strictly* query-related, but I think it's inextricable. 

But, I'm wondering: why are you asking? Are you a recruiter? Maybe a non-technical hiring manager? Maybe, if we know what you're after, then we can give a more relevant answer.

EDIT: added some notes about model concepts.",28.0
g4p6pto,iq3fjm,Good answer.,3.0
g4p7w1w,iq3fjm,Thanks &lt;3  Just edited it to add some notes about modeling ideas.,2.0
g4rifhp,iq3fjm,Good content.,2.0
g4ti8nx,iq3fjm,"What do you mean by 'write a database server'? It sounds like implementing one's own version of SQL, but surely no one does that in this day and age.",1.0
g4tp136,iq3fjm,"&gt;  sounds like implementing one's own version of SQL, but surely no one does that in this day and age.

Of course they do, and I'm living proof. When I worked at Microsoft, I was on the SQL Server team for five years, where I spent time on the query optimizer.  When I worked at Oracle, I implemented a key-value store with transactional semantics which is (was?) used internally in the cloud infrastructure.

There are lots of data stores: MySQL, Maria, Snowflake, memcache, redis, Percona, CouchBase, MongoDB, Hadoop, HBase, Postgres, SQL-Lite, BigTable, Oracle DB, Cassandra, SQL Server, Ingres, Access, Paradox, DB2, Riak, NonStop SQL, Drill,  Libre-Base, ...  What is SAP's internal RDBMS called? And SAP bought Sybase, right? 

Then, just at AWS, there's Aurora, RDS, Redshift, DynamoDB, Keyspace, Neptune, TimeStreamer, and QLDB. (Did I get them all?) Azure has as Cosmos and a couple others. And there's the graph-based systems, like Filament and Horton and graphd and ...

There's lots of data stores, and lots of people work on them. You don't think they write themselves, do you?

Sure, lots of these aren't a ""version of SQL"", but they are data stores, and lots of people work on them. (It only takes a few minutes to download and set up Maria DB; even faster if you just get it set up and running in your cloud. But it takes millions of man-days of effort to write this kind of software. Maybe the disparity distorts perception?)  Working on this kind of project often involves knowing SQL, and knowing it from the implementation and definition side, which is a couple layers past the allowed ""Advanced"" level in the context of this question.

On top of it, that kind of work (even if not a SQL-queried data store) involves understanding very fundamental things about data storage: performance, I/O subsystems, scaling, scheduling, concurrency, CAP theorem, transaction theory,  fault tolerance, distributed systems, code (hopefully, provable!) correctness, and so on.",2.0
g4urn6l,iq3fjm,"Of course you are right. My mind jumped directly to a business using databases, but plenty of people are rooted in the advancement of core technology. 

Nice write up!",3.0
g4ou1x7,iq3fjm,"intermediate imho is pivot, windowed functions like row_number, cte and join on self, cursors, cross apply. basically everything that allows you inter-row calculations on already normalized and aggregated data using the basics. when i teach sql for beginners, these are the functions that open the gates to a whole new level of solving problems with sql as a data analyst, effectively turning your business knowhow into repeatable calculations instead of doing it by fiat in excel. 

everything over these i think is expert level, you either trying to do something very niche so you have to learn the functions you need while figuring it out just this slice (or be lucky to have someone available who done exactly this), or requires so much routine, experience and dedication to learn the steeper sql into a working knowledge is that you pretty much have to be in a role dedicated to nothing but SQL development, no double duty as sysadmin, BI, programmer etc. 

imho everyone with solid excel skills can learn up to the advanced part and usually it solves most corporate data related business problems outside of actual tools integration and db development.",7.0
g4p4fn4,iq3fjm,"Here is what I consider.

&amp;#x200B;

**Beginner**

DDL:

* create / drop databases
* create / drop tables with index, primary keys and foreign keys
* create / drop views

DML:

* select with simple joins between tables and view
* insert by values and by query
* update and delete with simple where clause

Admin:

* installing, starting and stopping a server
* creating, deleting user and assigning permission to database as a whole

**Intermediate**

DDL:

* create / drop functions
* create / drop stored procedures
* create / drop triggers
* create / drop check constraints
* create / drop tables with virtual columns

DML:

* correlated subqueries
* CTE
* lateral join (cross / outer apply)
* (un)pivot
* update with joins
* window functions
* using transactions

Admin:

* setting fine-grained permissions to tables, views and columns
* backing up and restoring databases
* finding performance bottlenecks

Anything over the above are expert level.",5.0
g4phds5,iq3fjm,"Beginner:

Query a table or two, view, stored procedure, function and know the difference. Understand what a primary/foreign key is.

Intermediate:

Comfortable with all multi table joins, subqueries, cte's, temp tables, pivot tables, isolation levels

Advanced:

Performance and query tuning for speeding up slow queries, understanding execution plans, b+ trees, understanding and building indexes.",3.0
g4pbxsl,iq3fjm,"With a lot of technology stacks that we develop with, I think mastery comes in two places.

1. Technical mastery of the tools
2. Conceptual mastery to apply the tools

For 1, that's knowledge of the features and capabilities of the tool set you're working with. 

For 2, that's figuring out how to apply the tool to solutions based on the requirements one is provided. Do you know the tool well enough to know how you can abstract, automate, and improve your development workflow so that you can build robust solutions with minimal technical debt?

(1) can be at an intermediate or even lower level with a tool but if they know how to create a solution using (2) then they don't necessarily need to be an expert in that tool. If they're an expert in both, then they can call themselves an Architect and charge people tons of money to hire out their firm or speak at conferences.",2.0
g4pfwme,iq3fjm,"* Beginner - Nothing to do with creating tables, updating or deleting data.  A beginner is someone who is skilled at selecting and aggregating data from tables resulting in correct information.  Knowledge of how to join, WHERE clauses, GROUP BY (with simple aggregations like SUM and AVG) are required, because without JOINS, there is no point to even write SQL.  In this phase, the developer is working with discrete statements submitted to the RDBMS.
* Intermediate - The above skills plus knowledge of how to create tables, insert data, update data, and delete data.  More advanced data manipulation syntax is utilized (CTEs, temp tables, control flow (IF statements, stored procedures, etc.).  Basic knowledge about indexes is required.  In this phase, the developer is working with batches that have statements integrated together (for example, a stored procedure does a bunch of data manipulation in steps before returning a result.
* Advanced - The above skills plus applied knowledge of modeling life in schemas and normal forms, in addition to skills in tuning the environment.  I'm mostly a SQL Server guy, so examples here would be knowing when to and when not to define a primary key as a clustered index, setting MAXDOP, FILLFACTOR optimization, proper data type selection. etc.",2.0
g4pk7qd,iq3fjm,"On a related note, it seems like most SQL training out there gets you through the beginner phase. I’m not aware of tutorials on how to use CTEs, stored procedures, window functions, variables, etc so that you’re not a danger to yourself or those around you....",2.0
g4qc876,iq3fjm,"This sub really needs a sticky answer to this question, it comes up every few days",2.0
g4oy323,iq3fjm,Advanced to me is anyone who actually knows how to use a cross join. I still have no god damned clue how that thing works.,2.0
g4pd1u9,iq3fjm,"It's usually how I build a numbers table:

    with num as (
    select 0 n
    union select 1
    union select 2
    union select 3
    union select 4
    union select 5
    union select 6
    union select 7
    union select 8
    union select 9
    )
    
    select hundreds.n * 100 + tens.n * 10 + ones.n * 1 number
    from num ones
    	cross join num tens
    	cross join num hundreds
    order by number

Everything gets joined to everything else.",7.0
g4pmjzt,iq3fjm,You just blew my mind. Wow... Impressive!!!!,3.0
g4pwgtl,iq3fjm,"It's useful when you want to make a record of every possible combination of some things from different tables as well

    declare @numdays int = 100;

    with num as (
    select 0 n
    union select 1
    union select 2
    union select 3
    union select 4
    union select 5
    union select 6
    union select 7
    union select 8
    union select 9
    ),

	dates as (

    select DATEADD(dd,-1 * (hundreds.n * 100 + tens.n * 10 + ones.n * 1), FORMAT( getdate(), 'dd/MM/yyyy', 'en-US' )) dateof
    from num ones
    	cross join num tens
    	cross join num hundreds
	where hundreds.n * 100 + tens.n * 10 + ones.n * 1 &lt; @numdays
    ),

	colors as (

	select 'red' color
	union select 'orange'
	union select 'yellow'
	union select 'green'
	union select 'blue'
	union select 'indigo'
	union select 'violet'
	),

	headstails as (

	select 'heads' heads_or_tails
	union select 'tails'
	)

	select dateof, color, heads_or_tails
	from dates
		cross join colors
		cross join headstails
	order by 1,2,3

This gives me every possible combination of heads/tails with the colors of the rainbow for the last 100 days.

It's useful when I need to report on a set of dimensions and need a value for every row even when no aggregates are found for a particular combination. Anyone doing time series analysis using an rdbms will have faced a similar problem at some point in their career I'm sure.",3.0
g4q2qnk,iq3fjm,"Wow this is amazing thank you so so much. I will never forget this, you just made sense of something I have not been able to. You're the best!!!!!!",1.0
g4q3570,iq3fjm,No problem! Happy to help.,1.0
g4qwmhd,iq3fjm,I saw this in an Itzak Ben-Gan text. Great tool for building any range of numbers quickly.,2.0
g4qzt3x,iq3fjm,His books got me through the early years of my career. Funny enough I don't recall seeing this trick from him but I very well could have plagiarized it.,1.0
g4r4rsi,iq3fjm,"Not suggesting plagiarism, he could easily have gotten it from someone else. He's worked with some brilliant people.

It's truly a remarkable way to generate huge number tables without the storage cost.

Thank you for sharing it!",1.0
g4r5xor,iq3fjm,"&gt;Not suggesting plagiarism, he could easily have gotten it from someone else.

Didn't mean to imply that you were! Just acknowledging there's a possibility I might have gotten that bit from Itzik Ben-Gan if others were curious and I my memory may be failing me at the moment. 

&gt;It's truly a remarkable way to generate huge number tables without the storage cost.

It might be a little verbose for some but it's easy enough create a table valued function and then generate it on the fly.

&gt;Thank you for sharing it!

My pleasure!",2.0
g4ozpjn,iq3fjm,"I know you're probably joking, but a CROSS JOIN is a simple [Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product).",3.0
g4p1iiu,iq3fjm,"I am not joking lol, I just am scared as hell of cross joins. I once had an issue resolved by using a cross join, which came as a recommendation from a co-worker. It worked, and the result made no sense to me and seemed like magic. Thank you for this!!!",2.0
g4rhuq8,iq3fjm,"Cross joins are (relatively) easy. But also easy to blow up the server. 

Cross **apply**s will bake your noodle :)",1.0
g4pprd1,iq3fjm,"The below is off the top of my head, I'd probably add and rearrange stuff if I actually spent time pondering it.

**DBA**

Beginner - setting up database

Intermediate - authentication, roles, privileges, 

Advanced - load balancing, replication, backup, restore

**DML**

Beginner - simple select (from, where), insert, update, delete. 

Intermediate - joins, group by, having, distinct, limit, offset

Advanced - views, with, recursive queries, hierarchical queries, window function, analytic function

**DDL**

Beginner - Create table, create columns. Data types.

Intermediate - understanding normalization; creating indices, primary keys, constraints, etc.

Advanced - creating function and triggers.",1.0
g4ri5za,iq3fjm,"I might be gatekeeping here, but IMHO you can’t even call yourself a beginner DBA if you don’t have a handle on the basics of backup and restore. Having working backups (and knowing they work, and how to restore them) is fundamental - without that, there’s not much point to anything else.",1.0
g4pzbty,iq3fjm,For analytics Intermediate is window functions and CTEs and I’m not advanced yet so idk,1.0
g4q0mxs,iq3fjm,"Advanced: Build a data warehouse and deal with the day to day issues for a year.
You're going to need most of the SQL language and some best practices for that.",1.0
g4qhb5r,iq3fjm,Making heavy use of Dynamic SQL always seemed advanced to me.  It can be pretty powerful inside a stored procedure.  Good thread.,1.0
g4rhz50,iq3fjm,"* Beginner: Selects, most joins, group by, ctes, subqueries, unions, #tables, etc.
* Intermediate: Loops, dynamic SQL, indexes, table design, cross apply, cross join, outer apply, open query, merges, updates, inserts, pivots, full outer joins, where exists, intersect/except, etc.
* Advanced: Functions, execution plans, statistics, logging, error handling, cursors, triggers, right joins, etc.

Anything on this list can move up or down into a different category depending on how complex the data is, and exactly what you're trying to do. For example, you could read a simple execution plan for a simple query, and that isn't advanced, it's beginner. Or write a simple function that is an outer apply that isn't advanced, it's intermediate. 

On the other hand you can do some dynamic SQL that is advanced. 

Just depends.",1.0
g4ro7ys,iq3fjm,intermediate prob using sub queries and case when statement with agg functions. Advanced would be window function with complex logic?,1.0
g4s6lt6,iq3fjm,"Beginner... can connect to a database, run a few select statements, perform some simple joins, maybe create some tables and indexes.

Intermediate... completely administer and collect/query any data they want on the system and fully understand performance characteristics and tuning.

Advanced... ""here, I wrote this entire SQL compliant database to custom fit your needs""

(The last one was a bit of a joke... but only a bit.  It speaks to knowing not just how to ""use"" the database engine but how it ""works"" internally.)",1.0
g4sj8hg,iq3fjm,"I totally disagree with some of the answers here. Same I would disagree with most of the answers of the same question in other programming languages, because I think the answers would be similar.

I don’t grade knowledge of the language by level of programatic depth you have, I grade by your ability to solve a problem using the language. I know sooooo many people that know all the bells and whistles of SQL but given a difficult problem come running for help.

IMO beginner is knowing the languages and the ability to pass simplistic test on the language.  Intermediate is the know how to take a difficult question to answer without guidance. Advanced is the design of the query to take a difficult question to answer in the best way possible

Edit: I’ve interviewed dozens of people on SQL",1.0
g4xa48u,iq3fjm,"And sooo, I'll go on a different tact to most people here.

Beginner: Can work out how to use google to get the answers you want to query or to create what you need from the database.

Intermediate: Starts to think about performance (such as no expressions in join clauses, etc.). Also becomes aware of some risks and how to safeguard (such as dynamic SQL/ sql injections) etc.

Code structure is also of good form.

Advanced: Excellent code structure -clear. Very good understanding of the nuances of performance, regularly reads and understands execution plans. Also understands advanced concepts like rolling partitions etc.

&amp;#x200B;

Re DBA things such as 'Resource governance', 'database backups' 'log shipping', ;a-z about access' etc - you dont need those skills if you are not managing the server yourself, but you definitely need them if you are managing the server.",1.0
g4o3g11,iq2kno,"    SELECT ...
      FROM ...
     WHERE ...
    GROUP
        BY ...
    HAVING SUM(something) &gt; 0",3.0
g4o75rp,iq2kno,"I might adjust the HAVING to &lt;&gt; 0 (use case dependent, but a strict reading of OPs question is he’s looking to eliminate zeroes, not just return things greater than zero), but otherwise this will do the trick.",6.0
g4sgb28,iq2kno,http://sqlfiddle.com/#!9/5a55a/1,1.0
g4oiy99,iq2kno,My answer is for sql server but it may translate. It would help to see the query and some sample data. depending on where in the query this data is coming from you could pick a few different routes. You could use a MySQL equivalent to the CASE expression if you wan to eliminate the zeros in the SELECT statement or like u/r3pr0b8 said you could use the having clause.,0.0
g4om1rk,iq2kno,Add dummy data to the question,1.0
g4oqd3b,iq2kno,"not trying to be rude but I would spend more time trying to format that sample data than figuring out the answer to your question. I think it would be easiest for you to post the query \*formatted\* as follows and replace any sensitive data with whatever you like.

    select
        ID
        ,Payee
        ,Rate
        ,Activity
    from Some_Table
    where Some_Condition_Is_Met

then post a small sample data set as follows: (this is what I think your sample was trying to say..)

|ID|Payee|Rate|Actvity|
|:-|:-|:-|:-|
|1|BCBS|.12|9/8/2020|
|2|Medicare|.10|9/9/2020|
|3|BCBS|.14|9/10/2020|

Formatting your questions in such a way will help you to in the future get answers quicker and if you ever migrate over to stackoverflow they will be thankful as well.",2.0
g4rft60,iq2kno,Please reformat your post so the data is readable.,0.0
g4oi90u,iq2kno,The having will work but so might just saying where rate &lt;&gt; 0. I think doing it in the where statement might be less resource intensive than in the having.,-1.0
g4p9vgs,iq2kno,"&gt; The having will work but so might just saying where rate &lt;&gt; 0.

Tood has 3 transactions, each of which isn't 0, adding up to 26

Biff has 3 transactions, each of which isn't 0, but one of them reverses the other two, so the total for Biff is 0

OP wanted no **sum** to equal 0

""less resource intensive"" is not the same thing as ""gives the wrong answer""",4.0
g4om5za,iq1w7i,"It looks like in MySQL you could use the EXTRACT statement. Here is a link to the docs for it in case you need to do some tweaking on it. [https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function\_extract](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_extract) this is for v 8.0... not sure what the current version is

    EXTRACT (MONTH FROM COLUMN_WITH_DATE);",2.0
g4obnhp,iq1w7i,"Not sure what type of SQL you’re using but in standard sql I would use the parse_date() function to convert the string to an date type (as well as your other table if needed).

You also have the option of using a regex_replace or a substr to change the underscore to a hyphen.

*Edit - I initially read the question as best way to join.

If you’re trying to group a column that has dates as strings but are simply inconsistent you can use either string method I mentioned earlier to clean up your data.",1.0
g4oh40u,iq1w7i,`parse_date()` is not part of the SQL standard,2.0
g4pl36b,iq1w7i,"&gt; in standard sql I would use the parse_date() function

is this really standard sql?  can you give a link please?",2.0
g4tbuda,iq1w7i,https://cloud.google.com/bigquery/docs/reference/standard-sql/date_functions#parse_date,1.0
g4ti7iu,iq1w7i,"thanks, but i don't buy that Google Bigquery is really standard SQL",1.0
g4ochn8,iq1w7i,"I am using Mysql. or did you mean something else?

The date column is consistent. My current query returns one ratio for the entire set. I want a set of rows per date(daily, weekly, monthly). So I want to make three new queries with this query.",1.0
g4nn9bh,iq0mo6,"You can use subqueries.

    SELECT C.Id,
    	(SELECT COUNT(1) FROM CreativeLinkTracking WHERE CreativeId = c.Id) AS Clicks,
    	(SELECT COUNT(1) FROM CreativeImpressionTracking WHERE CreativeId = c.Id) AS Impressions
    FROM Creative c

EDIT: removed the ""GROUP BY c.Id""",5.0
g4nqr36,iq0mo6,"Wow, thanks. That was extremely obvious. I knew I was just being dumb.",3.0
g4nrle5,iq0mo6,"No problem.

&gt; I knew I was just being dumb

Nah, that is called overthinking, we've all been there.",3.0
g4nyg61,iq0mo6,Just be careful with sub queries. As long as you're never going to work with huge amounts of data they're fine,2.0
g4o3ag4,iq0mo6,"Yeah, if the data becomes too much I’ll probably switch to filling aggregation tables instead. But this is only for the admin interface with exactly one user. And considering that it’s a replacement for a google product, even slow will feel lightning fast ;)",1.0
g4o8lbv,iq0mo6,"Sounds 100% reasonable. You mostly a sql person, or a back end dev who got shoved into data stuff?",1.0
g4ogcgn,iq0mo6,The latter ;),1.0
g4oh01h,iq0mo6,What programming languages do you know?,1.0
g4olcaa,iq0mo6,"C#, TS/JS and Java are what I work with, in order of both amount of usage and preference ;)",1.0
g4oour7,iq0mo6,"How many years have you been working in C#? I ask because we have a senior level position open that we'll be posting soon. It's 100% remote so you'd just work from home (we provide all the hardware). 

Last thing would be that it does require that you're somewhere in the US.

And before you think this is spam or something. The last 3 people we've hired were from me posting our job openings on reddit :). The company is alpinetestingsolutions.com, but don't look at the job postings, those are old ones. I think we'll have the new ones posted sometime next week",1.0
g4oszls,iq0mo6,Full-time at an SMB (I’m actually our whole IT department minus system administration) for 8 years now. But I’m afraid I already work fully remote and I’m in Germany ;),1.0
_,iq0mo6,,
g4obb9r,iq0mo6,"It would still be fast over large amounts of data, if an index were placed on CreativeId.  It appears to be a FK column in each table, so it makes sense for each to have an index.",1.0
g4nq0xe,iq0mo6,Agreed.. Or CTEs,2.0
g4nsvbi,iq0mo6,CTEs in this case will produce the exact same execution plan. And the query is small enough that it doesn't really improve the readability,2.0
g4s4bb4,iq0mo6,"Yup, as long as the poster learns that they exist.",1.0
g4pjpug,iq0mo6,"i don't have MS SQL to test on, but this looks wrong

i think you would have to either include `Clicks` and `Impressions` in the GROUP BY clause, or else remove the GROUP BY clause because it actually isn't needed, since the two subqueries are **correlated**",2.0
g4pkbgb,iq0mo6,"Oh yeah, the group by isn't needed.

I ended copying his query and forgot to remove the end :P",1.0
g4o7hr7,iq0mo6,"If link tracking and impression tracking have a unique id, you can use distinct in your counts and use those unique ids instead of the timestamp.",1.0
g4nums9,ipz0k0,You either need to distribute the mdf/ldf and create a new instance of localdb on the users machine. When you have the instance attach the mdf/ldf OR create a new instance and deploy the code to it.,1.0
g4nuw3m,ipz0k0,That being said if localdb is a good enough database you would probably do better with compact sql or sqllite as they are much more used to this type of deployment scenario. Localdb is for local dev before pushing changes to an actual sql server.,1.0
g4o7tog,ipz0k0,is there a good tutorial on transferring my local db over to one of these?,1.0
g4m45mc,ipu1rm,"Make sure your MySQL database is running. Then do this link  
[https://popsql.com/docs/connection-guides/connecting-to-mysql](https://popsql.com/docs/connection-guides/connecting-to-mysql)",2.0
g4m4o5n,ipu1rm,The UI sometimes glitch like that try restarting/changing connections etc,2.0
g4quwdf,ipmzxr,Is it working as expected?,1.0
g4qvf5a,ipmzxr,"Well, yeah. But I was looking for optimization tips.",1.0
g4s5c9v,ipmzxr,"Insert on conflict do nothing, returning ID


To make this work, you would need a unique index on ip field, but that should be the natural key anyway it would seem.",1.0
g4skubi,ipmzxr,"I have a unique key. However the problem with on conflict with this particular case is the serial is an incrementing integer. And even if the value is presents it gets incremented. Which leaves gaps in my ids.
So I returned to the query in the post above.
With a uuid as a primary key, there won’t be problems like this with on conflict.",1.0
g4tgwvt,ipmzxr,Gaps in IDs are expected with any high concurrency environment. I thought the goal was optimization.,1.0
g4th7q2,ipmzxr,"As far as I can tell, your solution would also fail with concurrent activity. At the time of the not exists select, the row may not be there, but at the time the insert is committed, the conflicting row may be there from another transaction committing.",1.0
g4tj8ay,ipmzxr,"Optimization is definitely important. So is this a better approach then?

Anything you would change about it?

    INSERT INTO globals.ips (ip) VALUES ('1.1.1.1'::inet)
    ON CONFLICT (ip) DO UPDATE SET ip = EXCLUDED.ip
    RETURNING global_ip_id AS id;",1.0
g4zmdkf,ipmzxr,"Any reason to update a field to the same value it already has? Why not ""do nothing""? I assume probably it doesn't execute the returning clause in that event? I haven't needed that use case so I am not sure.

If needed, I might union all the results of the returning, plus just a select on the table.

Else, you may get updates that are no-op. Not sure if Postgres optimizes that away to skip actually creating a new row version.",1.0
g4yw4ci,ipmzxr,"I've been testing this, and it is not as fast as the other one. We are talking in MS, but  it's going to be run every (multiple times per front-end request as it fetches the content needed) single time and it shouldn't be taxing.

Any suggestions?",1.0
g4z10vi,ipmzxr,"I am trying something like this now:

    WITH v AS (
        SELECT %s::inet AS ip
    ), i AS (
        INSERT INTO security.ips (ip)
        SELECT ip FROM v
        ON CONFLICT (ip) DO NOTHING
        RETURNING sec_ip_uuid as uuid, ip
    )
    SELECT COALESCE(i.uuid, p.sec_ip_uuid) AS uuid
    FROM v
        LEFT JOIN i ON i.ip = v.ip
        LEFT JOIN security.ips p ON p.ip = v.ip; 

It appears to be faster (when the IP is already in the system). Like the original one.",1.0
g4koqh1,iplj7c,You can't use PySpark?  Then with VSCode install the Spark &amp; Hive Extension: https://marketplace.visualstudio.com/items?itemName=mshdinsight.azure-hdinsight,1.0
g4kttia,iplj7c,"Thanks for taking the time to answer.  I think I may be just missing some simple things.

I had played around with that extension earlier.  It looks like it supports: HDInsight cluster, SQL Server Big Data Cluster's, and Livy Endpoints.  I do not believe I have one of those.   If I do, I couldn't find a place to enable the SSL trusted certificate anyways?  I tried all 3 and couldn't get it to work.

Here is what I have set given as details to set up:

* Host: [000.00.000.0](https://000.00.000.0)
* Port: 00000
* Database/User/Password
* Spark Server Type: SparkThriftServer
* Thrift Transport: SASL

Then I have a .pem file to link to in the ""SSL Options"" trusted certificates.",1.0
g4kuhhr,ipkgye,"No *good* way.  I would suggest running a query with your WHERE clause and looking at the output before running the update to gauge just how much is going to get changed.  

Or run your update in a transaction and look at the numbers of rows modified BEFORE you commit the transaction.

To record the before &amp; after you could set up triggers to record the OLD and NEW values.",1.0
g4lcxwr,ipkgye,"My recommendation would be to insert a new column into this table if you can and call it something like 'update\_datetime' and just insert the date time of rows you insert or change. I only do this to tables that get frequent updates that I have a hard time keeping track of, but where I will also want to know the change history. 

Maybe something like this could be helpful?",1.0
g4lst6z,ipkgye,"Oh, you just gave me an idea ... I'm thinking I create a whole new table for the results and just put the regex results into that table, without touching my original table. Something like INSERT INTO ... can I do that with a regex on the field?",1.0
g4w2ebc,ipkgye,"I honestly don’t know, sorry. SQL is notoriously inflexible with regex but it’s worth a try I suppose.",1.0
g4k9j4f,ipjiy2,"start with the FROM clause

if there are two or more tables involved, examine how they are joined, and figure out the primary and foreign keys, to learn which are the ""one"" tables and which are the ""many"" tables in each one-to-many relationship",3.0
g4ki1vy,ipjiy2,"One piece at a time. Just like a paragraph, you don't read the entire thing at once. 

Like the other guy said you start at the FROM to see what your base is, then what's being joined to it and how that will affect it.

Break the query into pieces. Remove the joins and just execute the base FROM. Look at the results and be sure you understand why you're getting them. Then move to the next Join and run the Joined table or SubQuery by itself, understand it, then add the join to the base, and run it, understand it, rinse, repeat. Once you understand the joins, the WHERE, GROUP BY, HAVING and ORDER BY clauses should just make sense. If not, break that down and add each piece one by one, run it, understand it one piece at a time.

As you're doing this consider how important it is to leave comments for other devs so they don't have to reverse engineer your work.",1.0
g4ksm0u,ipjiy2,"Not sure what you mean by big. If you mean lots of data from a single table maybe look at using some filters in the where clause.

If OTOH you mean big as in a  lot of fields and tables/files then formatting is your first step. I use Notepad++ and either the Poor Mans T-SQL Formatting tool or SQLinForm. These will put each field on a separate line as well as arranging the files in the FROM and JOINs in an easier to read format. I personally like to space out all the aliases to a column on the right hand side so it is easy to spot the various aliases. 

I've been asked to tune queries and one of the most complex queries I had to unravel had all of the JOIN logic in the WHERE clause making it impossible to easily see how all the files were joined. In the end I had to use indention to keep track of the file joins as they had joined A to B, B to C, C to D and then A to E, E to B, B to C etc. (The second B to C was unnecessary...). 

Once I had it formatted where I could read it and as already mentioned starting with the FROM clause tuning was easy from there.",1.0
g4luhhr,ipjiy2,"I've seen some big queries that look like a mess and sometimes that's due to formatting making it incredibly hard to read.  While I think other posters have made some great points, you can also run the query through something that will format it like Poor Man's in Notepad++.  That may help you make some logical deductions as to what the query is doing.",1.0
g4ljzon,ipgpbg,I wish there was an Oracle version of you.,2.0
g4pic5t,ipgpbg,In case you haven’t run into Tanel Poder’s work I suggest googling him. He’s shared a ton of utility scripts over the years and has recorded a fair few hours worth of troubleshooting sessions. There’s obviously a load of other Oracle experts but Tanel immediately comes to mind when it comes to this sort of approach.,2.0
g72fjzw,ipgpbg,"Thanks! Unfortunately, I have zero interest in Oracle.",1.0
g4jph0y,ipdn80,"[https://www.guru99.com/relational-algebra-dbms.html](https://www.guru99.com/relational-algebra-dbms.html)  


do your best and show us the result if you think it is the answer.",1.0
g4j3uwi,ipc9gu,"you need a **self join**

search on that term and you should find some useful examples",1.0
g4ip6y6,ip8ax4,Put very simply.,6.0
g4kly8v,ip8ax4,"The tiny bottom portion is like ""The rest of the owl"" in ""How to draw an owl"" lol.",6.0
g4lq4jg,ip8ax4,this lol like 95% of what SQL is not included,1.0
g4lwt2p,ip8ax4,"Pretty much, yeah.

CRUD is about as obvious as it sounds. The meats and bones of what can be done with T-SQL is beyond my current experience for sure. I mainly deal with figuring out formulas/""algorithms"" ( It sounds fancier than it is ) to workout how to apply the company's legacy reporting logic to a new software DB and maintain formality to a degree where payroll is involved. It's a clusterfuck and a half and it's 1000+ lines of T-SQL in a stored procedure that drives it all since 2007.

At the end of the day it pays well and i'd rather do that than debug code with generic errors that point to nothing at all besides my incompetence. Server errors i can deal with. T-SQL feels like a technical language that i can in the most basic terms, translate to ""corporate english"". And that's what's been paying me since 2014.",1.0
g4l8ayk,ip8ax4,Great! Putting SQL on my resume now.,3.0
g4jxl6c,ip8ax4,"Wait, I know all this. Does this mean I'm not a beginner?",4.0
g4lhsfy,ip8ax4,You should probably start applying for senior roles :P /s,4.0
g4mh26o,ip8ax4,"Genuinely, if you know everything here without looking it up and can use it in a simple practical environment... then you’re probably not a beginner in terms of “you’re still learning the basics”

That doesn’t mean you’re an expert or anything, obviously, but you have some idea what you’re doing and are no longer a beginner in most people’s eyes

Although obviously it’s subjective where the line is drawn",1.0
g4j1te5,ip8ax4,Would INSERT INTO be what I’d use to do a delta load of new data into an existing table? I want to practice creating a Stored Procedure to do updates as necessary but am unfamiliar on how to do so.,2.0
g4jc5u9,ip8ax4,"Yes. 

UPDATE to change existing lines, INSERT INTO for brand new ones.",4.0
g4jc9n9,ip8ax4,Thank you! I’ll revisit with new encouragement,1.0
g4jcbuh,ip8ax4,If you get stuck on anything feel free to drop me a message!,1.0
g4rj3xy,ip8ax4,"i think i figured it out!

i have a python script that goes and gets all the new data, then puts it into a df and sends it to my staging table in SQL.   


then i have a stored procedure based on a query i made, it takes that staging table and does some stuff and puts it in a temp table. this temp table is then INSERT INTO my production table. 

&amp;#x200B;

the stored procedure is running now, but hasnt thrown any errors yet so it looks good. thanks for the encouragement!",2.0
g4rj857,ip8ax4,I'm glad to hear it's all working man! Top stuff!,1.0
g4jchfb,ip8ax4,"Thank you, I really appreciate it! I probably will tbh once I organise my thoughts into something coherent. We have data scientists at work I can ask but sometimes i feel like I’m exhausting them with questions.",1.0
g4j3cf9,ip8ax4,This is great! Textbooks and YouTube tuts can be very daunting for newbies,2.0
g4mgrq4,ip8ax4,"After 15 years of using SQL it still pisses me off that I can’t write

    INSERT INTO &lt;table&gt;
    VALUES — Or some other keyword to avoid confusion
        &lt;column&gt; = @SomeValue,
        &lt;other_column&gt; = 1,
        &lt;other_column&gt; = SELECT foreign_key FROM &lt;other_table&gt; WHERE &lt;foo&gt; = bar;",1.0
g4ik0ge,ip7jkj,"I'm new to Postgres but if I was passing up values to a query I would do it this way.

    UPDATE t
    	SET name = v.name
    FROM my.table t 
    INNER JOIN (
    	SELECT
    		id
    		,name
    	FROM (
    		VALUES  (1, 'one'), 
    				(2, 'two'), 
    				(3, 'three')
    	) AS t (id,name)
    ) v ON t.id = v.id

I have no idea how to write this in python because I simply struggle with it's lack of syntax.

In similar languages it may look like this.


    var MyValues = []
    
    for(var i in idNameTable){
    	
    	MyValues.push(""(""+keyValuePairTable[1].id + "",'"" + keyValuePairTable[1].name + ""')"");
    	
    }
    
    var query = '
    	UPDATE
    		SET name = v.name
    	FROM my.table t 
    	INNER JOIN (
    		SELECT
    			id
    			,name
    		FROM (
    			VALUES  (' + MyValues.Join(',') + ')
    		) AS t (id,name)
    	) v ON t.id = v.id
    '

You can send a large amount of values to your inner select and do one single transaction.

Case statements are very good for some updates but they can be unwieldy when you have very specific updates like what you are doing.",2.0
g4im21d,ip7jkj,"Thanks for the reply! I'm more of a python person, but I was hoping to optimize some of my scripts.  I feel like python makes logical sense to me, but I struggle with intermediate SQL...I can see from your example there are a lot of commands I need to understand more clearly. 

In python, I can just pass an SQL statement as a text string. I use pandas to organize/edit data then when I'm ready I push any updated rows back into the database.

To clarify, it looks like you're using the UID, value pairs  to join to the rows in the database, then setting the new value?",3.0
g4irh1c,ip7jkj,"Yeah you are basically constructing a table using the VALUES ().

You can pass a sql query up for each row in python. That works until your datasets get large. 30k calls to sql has couple TCP packets and latency per call and unless you are doing pooling, its serial. A 30k dataset could take an hour. 

If you batch process where every 1k rows and send a SQL statement up using VALUES() you are batch processing. 

Rather than taking an hour, it takes 30 seconds.",2.0
g4jebng,ip7jkj,"My hero u/Thriven

Yeah when I have 50k+ rows with unique values, my current code can take a while. I knew there had to be a better method but not being super familiar with intermediate SQL I didn't know which direction to go.  Thanks again!",2.0
g4kbzrq,ip7jkj,"After posting that I realized I may have over complicated it.

Systems which have massive bulk update/insert statements tend to keep things simple even at the cost of the size of statements.

They would probably do something more like this

    var query = ""UPDATE a SET name = '""+ name+""' WHERE id = '+ id + ';""

The output would look like this

    UPDATE a SET name = 'Bob' WHERE id = 123;
    UPDATE a SET name = 'Joe' WHERE id = 312;
    UPDATE a SET name = 'Tom' WHERE id = 412;
    UPDATE a SET name = 'Sue' WHERE id = 654;
    UPDATE a SET name = 'Jim' WHERE id = 981;
    UPDATE a SET name = 'Tim' WHERE id = 127;

The extra characters do add up when you are inserting/updating gigs of data but it's the cost of keeping it simple and error proof.",2.0
g4ln9xb,ip7jkj,"Ahh well in that case it's not much different from what I have set up now, except I don't chain the update statements together. For each unique value in the column I'm updating, I do a separate commit.  When my column is only a few unique values, like a category, there's no issue. But if I'm passing a column of many unique values, it can take a while.

I'm guessing the biggest thing slowing my updates diwn is that I could theoretically do a separate commit for each row.  I had no idea I could chain them together like this",1.0
g4vf0ru,ip7jkj,"Thanks again! I had to make small changes to maybe 5 lines of code, and now I can update 100k rows in like 10 seconds, when it used to take anywhere from 10 seconds to an hour. 

SQL syntax clearly makes very little sense to me. It seems like everything is so flexible, but I'm only aware of the most straightforward methods and commands",1.0
g4id3jf,ip7jkj,"As I read more, I'm learning about the concept of [HOT  Updates](https://www.dbrnd.com/2016/12/postgresql-increase-the-speed-of-update-query-using-hot-update-heap-only-tuple-mvcc-fill-factor-vacuum-fragmentation/). If I'm understanding that article correctly, I should stick with the method I'm using now because that chunks the updates into smaller bits to save memory(?)",1.0
g4kkk88,ip7jkj,"Oh wow man. Thank you for this article lol. I had no idea about mvcc.

I would say you probably can ignore HOT Updates. Normal updates are fine. If you have space concerns, I would run the VACUUM command after to clean up deleted rows.

https://youtu.be/GtQueJe6xRQ?t=791",1.0
g4hxtgr,ip51pn,Looks like you have syntax highlighting on everything.,7.0
g4i8ogv,ip51pn,There’s a setting buried in your options to turn this off. Search “highlighting” in VS Code settings,7.0
g4if8m4,ip51pn,"It was in the extension settings, thanks!",3.0
g4i24fl,ip51pn,"do you have any peripherals plugged in that could act as a mouse? Touchpad, controller, etc.",0.0
g4i2h7b,ip51pn,"Yes, I have a mouse plugged in. But disconnecting didn't do anything. This is on a laptop, so there is a touchpad as well.",1.0
g4ie9g1,ip34qb,"My first recommendation for more hands on sql training is something like this:

https://www.amazon.com/SQL-Practice-Problems-learn-doing-ebook/dp/B01N41VQFO

It's great at walking you through and getting you ready to answer different business questions. Never too early for kaggle, but I would actually prefer leetcode for learning the more advanced SQL topics of CTE's and managing subqueries. As for Oracle, can't help you there, I prefere SQL Server.",3.0
g4lrukx,ip12cy,"That sounds like a great idea. Employers like people who enjoy their work. Coworkers like it too. I know a lot of BI analysts who do that. 

I think you should start with a schema diagram. You can easily do it. I never create tables without drawing it out first. Without actually seeing how it's organized, you're bound to make mistakes and it's much harder to drop and recreate a whole bunch of tables than to erase a picture.",1.0
g4mciem,ip12cy,"Thank you for your comment and input! I found another dataset that I have more interest in which I feel like will make the learning experience more engaging. I hope to translate that into my career by working in an industry that I like. 

I appreciate your advice about creating a schema first. It really helped and made things more intuitive.",1.0
g4s0k1h,ip12cy,"By the way just a general tip on coding interviews. You aren't going to be able to answer all the questions. Like I don't offhand know how to do recursive ctes. Hasn't been a part of my life in many years and I've forgotten. But as long as you know the sort of thing you should be doing, the actual syntax doesn't matter so much. You can always Google it. I don't know if this helps. I know people who worry about it.",1.0
g4usee3,ip12cy,"I appreciate the tip, it's helpful! Now I have a bit more of an idea on what to expect. Do you know if the technical questions are often more on the basic side for BI analyst positions or similar roles?",1.0
g4vybw7,ip12cy,"Yeah probably basic. You should understand how joins work. Possibly data warehousing. I don't know so much about that because while I do a lot of BI work, we don't have any plans to hire someone to do only that. By the way if you're interested in data display theory I'd really recommend Stephen Few. He has a great blog on it. And Edward Tufte. He's a classic.",1.0
g4wautx,ip12cy,"You piqued my interest, I will definitely check them out!",1.0
g4h3vnj,ip0jj5,"&gt; This doesn't work, because there are two columns

so don't use two columns

    SELECT *
      FROM othertable
     WHERE id IN
           ( SELECT id
               FROM table
             GROUP 
                 BY id
             HAVING COUNT(*) &gt; 1
           )",6.0
g4hbetc,ip0jj5,"Yeah, I tried that bit:

         ( SELECT id
             FROM table
           GROUP 
               BY id
           HAVING COUNT(*) &gt; 1
         )

But you know what I bet I did wrong?  I tried the clause:

    HAVING COUNT(DISTINCT ID) &gt; 1

I think that won't work.  There was my problem.  Thanks, man, you've solved my problem!",1.0
g4h3k2o,ip0jj5," You can join on the query

    SELECT ot.*, t.ID_count 
    FROM other_table AS ot 
    INNER JOIN 
    (     
        SELECT ID, COUNT(*) AS ID_count     
        FROM table     
        GROUP BY ID     
        HAVING COUNT(*) &gt; 1 
    ) AS t 
    ON ot.ID = t.ID",4.0
g4itm01,ip0jj5,"You're right in the fact that the query with the subquery could be getting confused because you have two ID tables, and it doesn't know which to go with. The easiest way to solve this is by using table aliases.

        SELECT OT.* FROM OuterTable AS OT
        WHERE OT.ID IN
             (SELECT T.ID, COUNT(T.*)
              FROM TABLE AS T
              GROUP BY T.ID
              HAVING COUNT(T.*) &gt; 1
             )

By using table aliases, you explicitly declare which columns are being pulled from which table, leaving no room for SQL Server to get confused and start assuming thing.

Hope this helps!! :) Any other questions about SQL queries / querying you can DM me!",1.0
g4gvs5q,iozfyk,"    SELECT version
      FROM scores 
     WHERE user_name = $user_name
    ORDER
        BY date DESC LIMIT 1",1.0
g4gydol,iozfyk,Thank you! May I ask how you'd print out that value or assign it to a variable?,1.0
g4gztv3,iozfyk,"looks like you're using php

i don't do php, sorry",1.0
g4h581j,iozfyk,No worries. I'll figure it out!,1.0
g4hxrlq,iozfyk,"It's been years since I've used php but make sure you're using PDO for your queries, otherwise you may be vulnerable to sql injection attacks.",1.0
g4gtwdn,iozchz,"You'll have a problem with party requiring a row on member and member requiring a row in party. With this design it's a populating problem. So can't create a new party because there is no host, can't create a host because party doesn't exist yet. So you have a circular dependency. 

Not going into specifics, with some DBs and nullable foreign keys you can.",1.0
g4gux6d,iozchz,Thank you for your answer but I'm a bit confused. Should I create a separate table for the head of the party to avoid this problem or how could it be resolved? I also noticed that I was ambiguous in the title I meant member.id not member_id.,1.0
g4gvrr8,iozchz,"It all depends on what you want to model. Is a host also a member of the party? If yes, should it be an attribute of the joining table (so add ""is_host"" boolean in party_member_list or something similar)? Or should it be such that everybody is a person and roles (host, 0..n guests) are mapped separately?

You have infinite options but some of them reflect better on your understanding of the problem. That understanding is the key to solving it.",1.0
g4gwc1z,iozchz,"Ok, I just realized by host you mean the head of the party. It's actually a political party I'm trying to model, so yes, the head is also a member.  I originally wanted a one-to-one relationship for party.head_of_the_party (containing the member's id) and member.id and a many to many relationship on top of that: party.id -&gt; party_member_link.party_id, party_member_link.member_id -&gt; member.id.",1.0
g4gxm09,iozchz,"Ah, now I get the strange wording around party (English isn't my first language). It was no fun and parties.

I think you should map it thru the linking table, maybe separating the regular members from the board members.",2.0
g4gvx8t,iozchz,"&gt;  Why can't I use head_of_the_party as a foreign key pointing to member_id?

because it should be referencing `member.id` instead",1.0
g4gwfg1,iozchz,"Yes, I was trying that just made a mistake in the title.",1.0
g4gzzos,iozchz,"and why didn't it work?

you gave no SQL, you gave no error message",1.0
g4h20y8,iozchz,"This wasn't really using SQL, it's just a DB design tool but it's okay, I figured it's due to circular dependency.",1.0
g4h431r,iozchz,"&gt;  I figured it's due to circular dependency.

actually it was /u/dmntx that figured it

the solution is to not have `party_id` in `member`",1.0
g4h704p,iozchz,"Thank you but I already solved it by creating a separate table to connect the head to members. For me it was easier this way because I'll have to write the records for demonstrational purposes and I don't need to deal with an additional table whenever I add members.

And as for the circular depency, I figured it out in the meantime.",1.0
g4hac06,iozchz,"&gt; I already solved it by creating a separate table
&gt; ...
&gt; and I don't need to deal with an additional table

oh, okay",1.0
g4hk2ps,iozchz,What I mean is that every party has one head so I only need to input 2 times once for every party and I won't need to input twice every time I add a member.,1.0
g4gy75g,ioywt5,"I think you're certainly on the right track.

I would say the best thing to NOT do is lie on your resume. You can mention you have SQL experience and even possibly list what you know (aggregation, sub queries, JOINS, etc.). Be honest in your interview and explain what you know and how you will be able to apply it to your job.

Side note: I think having SQL + a Data Viz software like Tableau/PowerBI are foundational skills in becoming a DA. It might be a good idea to make some visualizations in PowerBI with public data sets to show in your portfolio. This will give a good idea to your interviewer on the things you're able to create. Bonus points if you can explain it in a way the whole room can understand and make them feel engaged.

Edit: To add a bit more to the SQL experience, you could download a public database from Kaggle, load up MSSQL on your local machine, and then show different ways of cleaning the data, organizing it, and answering common business questions. Their is endless amounts of data at your fingertips, and even if you don't have business experience you can still show knowledge and value.",19.0
g4h95ca,ioywt5,"is there any guide or turotial for steps by steps instealling SQL server and how to use it?

like I know how to write SQL since i learn from a EDX tutorial but I ahve now idea how to setup a SQL server ot how it works in real live (beside the EDX interactive input box)",2.0
g4h9dy7,ioywt5,"Google search six week sql and that sight, which is free I think, goes through it. It is also fairly good about covering topics.",1.0
g4gy0qd,ioywt5,"Some things I didn't know until I got a job using SSMS daily:

Using stored procedures. (To me understanding these is super important. It can help validate and keep data up to date. Also it lends to automating data feeds making everyones job easier)

In conjunction with stored procs knowing how to create views to so you dont have to rewrite queries. This can be handy for people who dont have server side read acess. You can make a base user and create executable excel spreadsheets that update upon opening the file or query upon change in a cell. Fairly useful. 

From there you should be familiar with giving permissions to users and creating different accounts for different purposes. This is good for limiting who can access what, especially if your data is linked to Powerbi or powerapps. 

Naturally, there is a lot more you could know, but i feel like tagging those three under sql skills will help.",5.0
g4h0nbo,ioywt5,How is the stored procedures different than creating a view?,1.0
g4h9scr,ioywt5,"A view is concerned with the results of a query, in a way it is closer to a relational table whose storage characteristic is that it is either virtual or materialized, and can be recomputed. A stored procedure is very different in that it is stored code. It is not exactly concerned with storing or outputting a result; it is closer to creating a function in a traditional programming language, e.g. supporting inputs from its user,  variables, but this depends on language used (it does not have to be written in SQL) - in fact, there are many interesting stored procedure languages available with some of them, for example,  even supporting loop constructs (which traditional SQL does not support). You can do a lot of interesting things with these like validating input of the stored procedure, behaving differently based on user executing it, sending an email, hooking it to a trigger, etc. Furthermore (this varies from DBMS to DBMS and what kind of problem you are dealing with) but each of them have a different impact on performance.

In conclusion, although both stored procedures and views are database items/tools, they are very different and are not mutually exclusive to one another (they can be used together). Although both can store code, they have a different purpose and usually, the type of code is very very different from one another.",2.0
g4h8hn4,ioywt5,"Well a view is just a pre set query. Or that is the way I understand it. It just executes a query the same everytime. Whereas a stored proc can execute a set of steps on timer. So it may be used to pull in data with specific conditions from another database or something. You can set them up to trigger at specific time intervals as well. So if you want to check another table every day or twoce a day you can do that. You can also trigger store procs from PowerAutomate and then populate other tables based on conditions which then a view is linked to. 

In my experience stored procs are way more powerful than views. 

Caveat : I consider myself to be between beginner and an intermediate user so if Im wrong feel free to weigh in and correct me.",1.0
g4ha6hf,ioywt5,"Ok basically a script that can be scheduled, etc",1.0
g4hwm13,ioywt5,"A view is syntactical sugar. It’s an alias for a single query. 

A stored procedure allows for multiple complex statements.",1.0
g4hu8f2,ioywt5,"I’m no DBA, but this excel with an executable is exactly what I am after. Is there terminology for researching this? or any other tricks you know for this to work from a security standpoint? On network, etc?",1.0
g4hwduf,ioywt5,"&gt;ow for this to work from a security standpoint?

Just google search executing sql query on google. Within excel under the data tab you will have the option to choose from other sources. From there you can select/connect to your server. Definitely research it to tweak your needs like executing upon open. I specifically set specific cells to amend my 'where' clause so I can pull in the query by adjusting the cell. You can get really creative.",2.0
g4hke98,ioywt5,"There are a lot of questions by starters in SQL how to generate a statement you can trumy to answer those most of them are not rocket science.

So you can try your solution and follow up on the solution in the answers. This will give you a practical way of learning and how to solve problems.

But essentially it is by doing doing doing.",1.0
g4hn3ic,ioywt5,Thanks for all the help! I’m grateful!,1.0
g4ifzl8,ioywt5,I read this article today.  Maybe it will help you especially the second tip. [https://www.stratascratch.com/blog/top-five-data-science-interview-preparation-tips/](https://www.stratascratch.com/blog/top-five-data-science-interview-preparation-tips/),1.0
g4fzqo3,iotxb1,"I'm not sure how someone would accomplish this while keeping their full time job. I thought about this scenario before. Several things that came to mind. Granted this would be dependent on any contract/statement of work agreed to by both parties.

1. SQL is a very narrow skill to have as a freelancer, You'd be hard pressed to find an individual only requiring database work. Usually you'd find someone wanting a full stack developer to deliver an application. This would leave you with businesses looking for someone for work that doesn't warrant an actual hire but there's enough that they cannot perform the work themselves so a short term contract.
2. You'd have to find a business that is okay with you only being available outside of your full time job. If they have something urgent come up that you cannot address immediately, that can be a problem.
3. Your technical skills would need to be top notch. This is why they'd be inquiring about a service you provide.  
4. You'd need to promote yourself in someway to get the attention as well as compete with others. This would include consulting firms as well which is where businesses might go to first instead of independent contractors.

I think something that could possibly work is hooking up with someone who would get a project for an application or website and they could throw the database work your way. 

The other option is to just to get something setup at home to mess around with if you would want to do it just for fun. No money but it might lead to a career opportunity if you have some fresh SQL experiences vs losing it over time.",11.0
g4g0w6q,iotxb1,"Just like any other professional trade, you actually have to **hustle** \-&gt; networking and direct word of mouth or outreach to people whom you are in a position to help.  Upwork, Toptal, other freelance sites, also exist if you prefer the leads to come to you (but that's not exactly a ""hustle"" - focus on learning to sell your services for greatest ROI on your time).  


By the way, unless there is some overarching reason for a BA not to be using SQL in your current role (other proprietary tools, limited database access, etc), you can spin up your own local datamart with minimum effort in order to stay in shape and likely improve your efficiency, esp. if you are using Excel very heavily or having to manage data moving to and from different platforms.",6.0
g4gm9wr,iotxb1,"This is also a big reason I try to remain on good terms with management I work with and go for a friendly ""hello"" every now and then.

If they get into a bind and know you can do the work for their products, it might lead to some side work. They already know and can vouch for your abilities and understand your current engagements so can be willing to work around that.

This has worked out fine twice for me, and everyone was happy and open to things in the future.

Edit: obviously check with your current employer to make sure everything is good to go.",2.0
g4g7cn9,iotxb1,"There probably is a market for one-off report writers (or report fixers) but you'd need to have a reputation to start with. I've been asked to look at reports that weren't running (or they were showing incorrect figures) but those requests always came from/via people I knew and people who knew I could be trusted. A lot of this kind of work requires access to potentially sensitive data. Even test systems can have information on it that would be embarrassing/costly if it was disclosed. Additionally it helps if you're familiar with the schema involved if it's something complex. Sure, sometimes you can spot something obvious immediately but more often you need to understand the data behind it. 

Anyway, I've delivered SQL training (to beginner groups). I'm not sure I'd describe it as a hustle, but I've been paid for it. Training groups of people is actually exhausting work, but it can be rewarding. Depends very much on the individuals, the setting, why they're there, etc. SQL is actually hard to teach in isolation (since it's a bit abstract) but I'm generally surprised by how positive (most) people have been. 

If you just want to keep your skills sharp then do a personal project. Doing a full database design from scratch is a good experience if only because you see how wrong your initial assumptions are and how you have to adapt as you go along.",3.0
g4g2i6k,iotxb1,"I see someone in this subreddit posting to look for SQL guys and gals to get them to side-hustle for him every now and then.

Other than that, since SQL is such a widely used skilled, it would be difficult.  I can't ever picture myself dishing out work to someone for a few hours per week here and there.  It would have to be through a consulting company if I were. 

Are you at a large company?  There could be ways to suggest and find ways to integrate your SQL skills.  Additionally, try contributing to some open source solutions on github if you want to keep your skills sharp.",3.0
g4gyz4c,iotxb1,"Become a side drug dealer. Unlike those noobs on the street, you build yourself an elaborate data pipeline in SQL detailing your sales per product type alongside some PowerBI reporting and dimension drill downs and such.",2.0
g4gz2z8,iotxb1,"SQL on its own typically isn't enough to provide actual value to potential clients. Think about what kind of problems your potential clients face and then come up with a product or service to solve those problems.

It's very rare that a client will trust you enough to say, hey motortonian, here's the keys to my data and a vpn token. Start analyzing all my data.

You should take a look at your network of potential clients and see what kind of problems they have in common that may require a database. Come up with a small proof of concept product to show them and then see if they're willing to toss you a few bucks a month to maintain and license that product.

Maybe you have a lot of friends that own restaurants and use DoorDash as a delivery service.

What you can do is create a standalone process that connects to the doordash api with data that your client will want to analyze and then store that into a sqlite database on premise or maybe on another db in the cloud. Then create a report or 2 that gives them better analysis than what's been provided by doordash.

Then once you've proven your value, you can do bigger and better projects for the client. Maybe analysis that can correlate their internal data with delivery orders and find optimal inventory levels for specific things like to go boxes and other delivery items.",2.0
g4gob6m,iotxb1,"&gt; so I'm curious if anyone has ideas for a SQL-related side hustle

Teach your way into a side hustle. Start small: colleagues, friends, meetups, etc.    


&gt; Do you have a side hustle that uses SQL in some capacity? If so what do you do?

I like teaching schema design and SQL, preferably in a small, friendly setting. So I conduct workshops and classes. It's not much of a hustle — but it makes some money and it keeps improving my professional network.",1.0
g4xbh4x,iotxb1,"When all else fails, write a comedic amazon e-book titled: 'Attempting to create a SQL-related side hussle - and failing at it!' and advertise the \*\*\*\* out of it.",1.0
g4ffegp,ioqceq,"Yes, it’s the column names.  A “zero” at the beginning of the expression is interpreted as an attempt to begin a hex/Binary/varbinary value, so wrap your WHERE clause columns (and SELECT columns if you select it explicitly) in square brackets or rename the columns to something legible.

Look how the error says “Na” instead of “00Na”.  It means it’s ignoring the “00” since it thinks you’re trying a hex/varbinary value like “0x06”

If you wrap the where clause columns in square brackets youll be telling SSMS and sql server that it’s a column name, not a value.
Remember to do this for both where clauses.

    WHERE [00N...]",6.0
g4fgr15,ioqceq,"Thanks!   
But now I am just getting an error that the column name doesn't exist 

`SELECT * FROM table`   
`WHERE [00Na000000BHFSKEA5] LIKE '%acp.edu'`  
`OR [00Na000000BHFSKEA5] LIKE '%`[`act.org`](https://act.org)`'`

ERROR:   
`1) [Code: 207, SQL State: 42S22]  Invalid column name ‘00Na000000BHFSKEA5’.`  
`2) [Code: 207, SQL State: 42S22]  Invalid column name ‘00Na000000BHFSKEA5’.`",1.0
g4fgz4j,ioqceq,"Then the column doesn’t exist in the table you’re querying.

1) There may be two different versions of the table.    One version that had the column, another that is in the dB/schema you’re pointing to. Are you fully qualifying the table: Database.schema.table ?  Are you using the right database?

2) you’re misspelling the column name (which seems easy to do given its name).  Drag it into the query from the object explorer, or just use the object explorer to select top 1000 from the table, and it will populate all of the columns into the select statement and you can copy/paste from there.",4.0
g4fcgba,ioqceq,"Not sure of it's just reddit formatting, but it looks like you're using quotes instead of apostrophe to wrap the text.",5.0
g4g6t7e,ioqceq,"Definitely reddit formatting, kinda weird.

    WHERE column = ‘Testing a string’",1.0
g4egq4y,ioki15,"Assuming it's all in the context of one connection/session (and it must be, if you're using CTEs), you can use temp tables in place of the CTEs. And performance will probably get better, if you do it right.

If you have `select` permission, you can create temp tables. If your DBAs won't allow you to create temp tables, I don't know what to tell you. They're pushing everyone down a bad path by blocking access to a very useful tool.

&gt;This is due to bank regulations and try to keep data “flowing” without getting out of compliance

I'm not aware of any _actual_ regulations that say you can't have staging/intermediate tables in your production environment. If it's a rule your IT department has decided on from their ivory tower, they may be doing the company a disservice.

I can understand not allowing processes to create persisted tables on the fly (it would require granting `ddladmin` access, which most DBAs won't do for individual users or application-level accounts). But if you have a job that's running daily that needs scratch space, persisted tables (which will be empty anytime the process isn't running) are a valid approach.",5.0
g4fqqkf,ioki15,I wish I could do temps or create real staging tables where I can append and truncate as needed.  Someone in IT with no data experience (I’m assuming) decided this compliance process the beginning of the year and it’s killing me.  I appreciate the feedback and I might try to use giant sub queries instead of Temps/CTEs.,1.0
g4frsv9,ioki15,"Subqueries will most likely yield the same results as CTEs.

If you can't use true temp tables (`#tablename`) in your code, that environment is FUBAR and the wrong people are making technical decisions with incomplete/invalid ""knowledge"" or assumptions. This ""compliance"" demand is ultimately going to cost the company dearly, and protect them from nothing.

Edit: How are they checking that you're using temp tables in the first place? If you slipped one into your code, would they even notice?",3.0
g4gasux,ioki15,Well I was just following stupid rules.  I’ll be trying what you said and hope people don’t see in the peer review.  Thanks again!,1.0
g4f59yx,iogtjj,You are unlikely to need to show the project in an interview. More common is for an interviewer to get you to describe and discuss it. If you can do that confidently it's evidence that you really worked on the project yourself.,2.0
g4f5l0r,iogtjj,Thanks for the tip! I was hoping that was the case but most of my previous interviews have largely been behavioral,1.0
g4doefq,iofo18,SSRS report with parameters may be a better fit for what you’re wanting to do. It’s included with SQL Server.,3.0
g4djecc,iofo18,"You're going to want to look into the Data Connectors within Excel....as far as the actual spreadsheet values being able to interact with the SQL code from the connector....I don't think so or haven't seen anything that would do that.

Edit: Every time I've thought of this I haven't been able to find a way to put in variables that would tie to the spreadsheet from SQL connector code window.",1.0
g4dpr1m,iofo18,"I've seen this don with the parameters in Excel for a product called F9, a financial report writer.  It then queries the SQL database.  But I don't know what's under the hood to make this work.",1.0
g4dp0by,iofo18,"Excel's Get Data function also known as power query can run sql queries with parameters, it's a bit of a fuss to set up the parameters and you will need to get familiar with the M language.
Each single parameter needs to be queried into PQ and stored as a list, the M code in advanced editor can be edited to reference the parameter name.
Pros: once setup is easy to adjust as needed
Cons: worksheet values need to be refreshed into PQ once before refreshing the sql query, else it will pick up the last loaded parameters.",1.0
g4en4m9,iofo18,"How many variables do they need to input?  This can be done relatively easily using power query or with VBA. 

Power query - 

What I generally do is create a table in excel with the potential answers as a defined/named range, then create another one row table which will use indirect() to reference that named/range.  This will become your input parameter.  You would ‘get data from table’ in the single row table, drill down to the value, call it xxxxx_parameter,  close and load as connection only.

Connect to the DB including your ‘where in’ clause, and in the Advanced Editor, insert the parameter into the query string. The query will effectively use whatever is in that single table cell. 

A couple downsides do this- excel privacy settings do not like queries from different sources, so you can either use a staging table, which I have not been able to figure out, or ignore privacy settings in get data -&gt; data source settings.

With VBA, you could create a connection to the DB and do something similar as above, except write a ‘dynamic query’ and set a variable as the value in the single row table to be used in the query.   This can work well, but I personally stay away from VBA If at all possible.",1.0
g4etfhg,iofo18,i always use the old school odbc connection to get this stuff to work properly. you can set parameters directly to cells when you edit the connections,1.0
g4dsme1,iofo18,"So, I've done this. Yes you're probably in the wrong place for this as your issue isn't really with SQL however the way I did this probably does make this an SQL problem.

Firstly, asking the user to enter the ids next to each other column by column is probably not the best, perhaps you're better off creating a user form that will allow the user to select multiple ids and then your user form would save that info on a hidden sheet in a table.

Secondly, your where clause is likely to need to be of the form

    WHERE column IN ('value1','value2')

If you want the user to enter many ids.

To achieve the above, I would simply pass all the ids into a single parameter of a stored procedure and have them all pipe delimited through vba. Then your stored procedure can either user string_split or you can loop through the string, removing a piped section at a time until you form a temptable that represents your table of ids as hinted firstly above. You can pass parameters in like this for any number of fields you want to filter on.

Your SQL script would then use the temptable in your where clause for the values.

    WHERE id IN (SELECT column1 FROM temptable)

You'd then just need your vba script to connect to the database and execute the stored procedure such as

    ""EXEC storedprocedure @id = "" &amp; idparameter

Where idparameter is your pipe delimited (or other delimited) ids such as ""1|2|3"" and storedprocedure is the name of your stored procedure.

There's plenty of resources online on how to connect to SQL Server via vba and the rest is just loops and getting the data you need to pass to it in a suitable place such that you can easily loop through it.",0.0
g4djk14,iofcxj,My guess is using PARTITION BY,2.0
g4dmxk6,iofcxj,"That definitely works, use partition by with a CTE and just do a WHERE RowNum &lt;=5",1.0
g4dw6jm,iofcxj,You can also try Top 5 apply,1.0
g4fkd1i,iofcxj,"You're looking for a [windowing function](https://docs.oracle.com/cd/E17952_01/mysql-8.0-en/window-functions-usage.html). Basically, a window function is applied to an ordered grouping. In this case you want the `rank()` function which assigns a rank to rows in the group in the order that you specify. 

Here is an [example](https://www.oracletutorial.com/oracle-analytic-functions/oracle-rank/) of the rank function usage.",1.0
g59v6st,iofcxj,Rank looks very promising. Thank you.,1.0
g4dsi9b,iof9zb,"For the record, you are not creating these counties, you are merely reporting them, :)  But you'll need to use row_number(), partition by() and rank().

NOTE - I have not tested this but it ought to be close.


    select state_name, county_name, pop2010, 
     row_number() over 
    (partition by country_name order by pop2010 desc) as 
        country_rank 
              state_name, county_name, pop2010
              from counties_query) ranks
              where country_rank = 5;

If anyone else can do this better I am interested.",1.0
g4d3h75,iocbio,Learn SQL... by using SQL!,6.0
g4cw8ni,iocbio,How about no?,2.0
g4e7b0v,iocbio,[But why?](https://images.app.goo.gl/eAtDRAWuLKwQvEoY9),1.0
g4eo95l,iocbio,Cuz a lot of people know excel!,1.0
g4cx8ov,ioc94x,"[w3schools](https://www.w3schools.com/sql/default.asp) Is the best cheatsheet I know of. Covers a lot of the major DDL, DML, and database concepts in a quick and concise way. It also provides examples for each concept.",4.0
g4czjus,ioc94x,Do you mean this? - [https://www.w3schools.com/sql/sql\_ref\_keywords.asp](https://www.w3schools.com/sql/sql_ref_keywords.asp),1.0
g4d02m2,ioc94x,I meant the entire left menu bar. Are you just looking for definitions?,3.0
g4d25i3,ioc94x,I mean those poster size one-page cheatsheet style.,2.0
g4dvk0r,ioc94x,"3 pages here for the basics but heavily depends on you being able to understand and apply this to what you're using it for. They have PDFs at the top and text at the bottom to copy into your own format. I've never needed to use a cheat sheet for SQL before, but it looks okay to me.

https://www.sqltutorial.org/sql-cheat-sheet/

Note there are plenty of things not included like windows functions, temp tables, CTEs, pivot/unpivot, etc. A lot of people need to look up the syntax for some of those things before writing anyway. If you need to have any of those concepts on here, then I'd start to question what your goal is.",2.0
g4d1a6l,ioc94x,"Here are some SQL concepts in an actual printable cheat sheet format:

 file:///C:/Users/shrey/Downloads/sql-cheat-sheet.pdf 

 [https://intellipaat.com/mediaFiles/2019/02/SQL-Commands-Cheat-Sheet.png](https://intellipaat.com/mediaFiles/2019/02/SQL-Commands-Cheat-Sheet.png)",0.0
g4d3fkx,ioc94x,"&gt; w3schools Is the best cheatsheet

downvote for this too facile answer",-1.0
g4gxrq7,ioc94x,I have found one here but its not one-page - [https://towardsdatascience.com/sql-query-cheatsheet-for-postgres-96eba6e9e419](https://towardsdatascience.com/sql-query-cheatsheet-for-postgres-96eba6e9e419),1.0
g4d3lj4,ioc94x,"this is not a cheatsheet *per se* but it's close --

http://troels.arvin.dk/db/rdbms/",2.0
g4d44z9,ioc94x,Downvote for linking an entire textbook instead of a cheatsheet,1.0
g4d8tkp,ioc94x,Upvote for something that I have been looking for! Really great page!,1.0
g4ctpcv,iob9e7,"This is some ugly code, but it works.  The text parsing part to get the month could probably be simplified or may need to be altered depending on how consistent the delimiters are.

The first step unpivots to get price and tax represented in the same field.  Then, we parse the month from the filename.  Finally, we pivot to generate a field for each month (this assumes you'll have more than just July and October, of course).

    DECLARE @Table table (filename varchar(25), price money, tax money);
    
    INSERT INTO @Table
    VALUES 
    	('abcd_01.07.2020_abcd', 5, 1),
    	('abcd_01.10.2020_abcd', 10, 2)
    	;
    
    ;WITH Unpvt AS (
    SELECT *
    FROM @Table
    CROSS APPLY (VALUES
    	('price', price),
    	('tax', tax)
    	) as ca(Metric, Value)
    )
    , Pvt AS (
    SELECT Metric
    	, Value
    	, MONTH(CAST(RIGHT(TextDate, 4) + SUBSTRING(TextDate, 3, 2) + LEFT(TextDate, 2)  as date)) AS Month
    FROM Unpvt
    CROSS APPLY (VALUES (REPLACE(SUBSTRING(filename, CHARINDEX('_', filename) + 1, 10), '.', ''))) AS td(TextDate)
    )
    SELECT Metric
    	, [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]
    FROM Pvt
    PIVOT (
    	SUM(Value) 
    	FOR Month IN ([1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12])
    	) p",5.0
g4dl4ld,iob9e7,As soon as you said it was gonna be ugly I knew it was gonna use the PIVOT operator.,3.0
g4cxjam,iob9e7,Thank you very much :),2.0
g4crvoi,iob9e7,"It's possible, using a combination of string parsing on the `filename` field and the `PIVOT` operator.

If you're frequently operating on substrings of that `filename` field, I would recommend storing those as independent fields or even persisted computed columns so that you aren't constantly re-parsing them.",3.0
g4ct9ky,iob9e7,"Thanks for your answer. 

Do you have an idea for the Pivot-code? Maybe an example :)",1.0
g4crwpk,iob9e7,"I agree, this does seem possible.",0.0
g4dnndr,iob9e7,"unpivot price and tax to a single column, then pivot  on the datepart out of the filename",2.0
g4e5keg,iob9e7,"Is pivot useful at all in sql? I finally learned my way through 1 example to use in a report I wrote, but it was cumbersome and difficult to remember how you'd set up the syntax and everything. It seemed like I could do the same thing with a simple select using max(), min(), sum() etc. Is that really all the pivot function is anyway - basically a stored procedure to do that? Doesn't seem any simpler or more efficient. It's there any point to learning how to use the actual pivot function instead of just doing it the direct way yourself?",1.0
g4diyon,iob9e7,You straight up posting homework here or what?,2.0
g4dnw52,iob9e7,that or just simplified his problem to the basics to make it easier to grasp how the solution would work,3.0
g4bq2d1,io6qd2,"I would really recommend just going ahead and creating the tables based off your ERD. From there you can begin normalization and making changes. 

I could be wrong but, I don’t even know if you can truly normalize based off an ERD or at least i never have",8.0
g4bqd00,io6qd2,"You might be right because I haven't found any resources on how to normalize off an ERD. Our faculty showed us only an example, and even that isn't properly documented (i.e. what was done, why it was done).",4.0
g4bqhbg,io6qd2,"That’s what i’m thinking. My entire database class in college we never normalized an ER diagram, only the tables. 

Try giving that a shot and see if it works out",6.0
g4bqoiu,io6qd2,Thank you for guiding me. I'll get on to it.,3.0
g4bqqmu,io6qd2,No problem! If you would like the free textbook that we used in my class I would be happy to send it over to you,4.0
g4bsho3,io6qd2,"Thank you for the offer! If you just leave the name of the book here, I'll find the pdf.",3.0
g4brcuw,io6qd2,"Maybe you get confused between both? ER diagram is ... the diagram to show the tables and their relationships (hence the name 'entity-relationship') and the tables are what is in the database. The tutorials are correct, you want to normalize the tables. Once you finish it, make the ER diagram of those results.  


For example, I had a big table of authors and books. After normalization, I would have two tables - authors and books and an associative entity 'wrote' between authors and books. You can break them down more if a big table have 'genre (or subject)' and 'publisher'.",6.0
g4bsfqt,io6qd2,"Nope. I understand what you are saying. But what I said in the post was correct.  Our faculty showed us only an example, where she normalized from an ERD and that isn't properly documented (i.e. what was done, why it was done). [Here are a couple of pictures of my slides](https://imgur.com/a/Hhk7wsi)",0.0
g4d5gyz,io6qd2,"The ERD here was likely used to be more readable and fit everything on one slide. 
The process itself is carried out in tables as per of the process for normalisation is making sure data isn’t repeated. 
However the ERD is useful for understanding how the everything relates to everything else at a top down level",2.0
g4eo22l,io6qd2,You're not understanding. She actually did it in class. She normalized this ERD herself.,1.0
g4bwa0v,io6qd2,"Check out the Wikipedia entry on database normalization. Follow the instructions up to 3NF. If you do thay, you should be good to go. You might have to make assumptions in which case, ask your prof. For the business requirements. Eg. You might have phone_num as a field. If so, you might need to ask if you should assume that a user may only have one phone number",2.0
g4bzzbf,io6qd2,"To normalize a table you need to understand the data, ask yourself question like does it make sense to put those two information together ? If not then separate them.",2.0
g4bcuyf,io3np1,I would recommend an interactive course like Kahn academy,1.0
g4br8be,io3np1,I'm reading Practical SQL from No Starch Press and so far so good.,1.0
g4cuh06,io3np1,https://youtu.be/gjUNNHlMwWA maybe you can start with this,1.0
g4ek365,io3np1,W3Schools is the best option to learn the basics. And I also recommend StrataScratch to practice. They'll provide you interactive exercises with real problems.,1.0
g4bbjq4,io3fu5,"I haven't looked at the data, but your query looks correct. Are you sure you're using the right column as input (`geom`)?

If you don't divide by the constant they specify in the book you should get the answer in square kilometres.

Arizona should be about 295km^2, so your answer doesn't seem too far off. However Arizona should only be the 6th largest state: https://beef2live.com/story-ranking-states-area-89-118259

Also, try doing the explicit cast they are using to the `geometry` type, and then cast back to `numeric` after, just to rule out there's no issue there",2.0
g4bexa0,io3fu5,"Ok, it was all correct. I made a quick search on Google to find Fips and turns out 02 is Alaska, not Arizona. Everything correct, sorry.",1.0
g4bblj6,io3fu5,Go back a few pages where they talk about using ST_Area with geometry vs geography,2.0
g4bepn2,io3fu5,"If you cast geom as geog it's the same, Arizona is the largest.",1.0
g4bez26,io3fu5,"Sorry, all correct, 02 was for Alaska, my bad.",1.0
g4bbw2v,io3fu5,Does your table maybe have a column named `area`?,1.0
g4bqelb,io3593,"There is probably a SQL plug-in for VS so start with googling “reading and writing to SQL via visual studio”. Google is your friend and don’t feel ashamed of using it to help you learn. 90% of “coding”, and in your case learning something new is googling :) also YouTube if you’re a visual learner.",2.0
g4bte7e,io3593,"Perfect
Thanks for the point out

Will definitely do that",1.0
g4agzr1,inyjym,"Obviously one of the joins isnt finding a matching value on one if the tables. Change the inner joins to left joins one at a time and run the query. When you get a result that has null values for th ed table in the left join, thats where your problem is.",4.0
g4ah2fs,inyjym,How do I fix it once I find out which table it is?,1.0
g4ah67i,inyjym,That will tell uh ou which table doesnt have matching data. It's not a SQL problem it's a data entry problem,2.0
g4aha9m,inyjym,"Okay, so the only way to fix it would be to view the data (which we can't do for this assignment). Does the string of tables I am joining appear to be the only way to do it given the question?",1.0
g4b87hy,inyjym,Your can't view the data for this assignment? That doesn't make sense. You're viewing the data when those results come back.,3.0
g4capxe,inyjym,we just can't view all the tables in their spreadsheet,1.0
g4anap8,inyjym,"you are asked for 

&gt; Produce a listing of stores that received helmets 

you are joining to sales instead

&gt; INNER JOIN Sales.SalesOrderHeader soh

you should be looking into the ""Purchasing"" area of your ERD (purchaseOrderHeader)",3.0
g4angej,inyjym,"That could also make sense, but then where would Territory Name be coming from?",1.0
g4ao181,inyjym,"I dont know - what were you told about this?

from the ERD it would seem a Store is always tied to a particular SalesPerson (non-null FK) and a SalesPerson could be linked to a territory (not always tho).",2.0
g4ahh9f,inyjym,If you run a select * from on one of the tables are you saying it returns 0 rows? If the tables are empty you will never get a result from selecting with or without an inner join.,2.0
g4ahkd8,inyjym,Should I try running SELECT * from each table and see which one comes back empty then?,1.0
g4ahm9u,inyjym,Run that on the tables you are trying to join.,2.0
g4ak3au,inyjym,"Unfortunately it looks like the SalesPerson and Customer tables are causing the issues, which are the two that would be usable",1.0
g4ajshx,inyjym,I’d replace saleperson with customer and add productsubcategory tables and of course your where conditions,2.0
g4ak1oq,inyjym,"Unfortunately it appears both the Customer and SalesPerson tables might be empty, causing the issue",1.0
g49sdel,inufa5,The best way is to create a temporary table or regular table... a view would work too... but if you can't do that hard coding the values is the only way,3.0
g49tziu,inufa5,"Since you can't create your own table, the ""with a view"" suggestion could be like this:

    with lookup_values as
     ( select 'A2933F91HF3' as id,'2015-02-13' as time_x
      union all
       select 'B34UP134133' as id,'2017-08-11' as time_x
      union all /* ... */
     )
    select a.column1, a.column2 /*, ... */
    from large_table a
     inner join lookup_values b on (a.id = b.id and a.time_x = b.time_x) ;",2.0
g49vxrp,inufa5,"You can simplify this a bit using a `VALUES` clause:

```sql
with lookup_values (id, time_x) as (
  values 
    ('A2933F91HF3', date '2015-02-13'),
    ('B34UP134133', date '2017-08-11'),
    ...
)
select ..
  join lookup_values ...;
```",1.0
g49w2aw,inufa5,"Good to know, I wasn't familiar with that PostgreSQL syntax.",1.0
g4b4ea2,inufa5,That's actually standard ANSI SQL,1.0
g4ezdq3,inufa5,Really? I know it's not valid syntax in Oracle.,1.0
g4ggg2t,inufa5,"I didn't say it's valid in Oracle, just that it's part of the SQL standard",2.0
g4a15hu,inufa5,"Turns out temporary tables are always an option regardless of permissions. Who knew? So, is creating a temporary table, doing a ton of inserts, and doing a JOIN still definitely faster than just a long WHERE IN clause?",1.0
g4a297d,inufa5,"Each situation is different. I don't know the general answer for PostgreSQL, but my guess would avoiding the inserts is the faster way. Instead use the with clause.

The best way to determine the answer to your question is to try several ways and see which one takes less time.",1.0
g49sb4z,inufa5,"Assuming that ID and Time_X can be queried out, you could use a WHERE EXISTS clause.",1.0
g49sun1,inufa5,"Thanks, this looks promising--can you help me out a bit as to how I might set up the equivalent of my `IN` clause using `WHERE EXISTS`?",1.0
g49urym,inufa5,"Sorry mate, it's my bed time now! Try looking at some examples specific to your RDBMS. It's a useful function, and can perform better than an IN clause with larger datasets.",0.0
g49ycwz,inufa5,"you still gotta code all those values into the sql, though",1.0
g492qcc,inq2o9,"It sounds like you want to use dynamic SQL such as:

    declare @var nvarchar(max) = (select thing from table)
    declare @sql nvarchar(max) = '
        select things
        from othertable
        where thing = ' + @var + ''
    exec (@sql)

The syntax there is off, and dynamic SQL gets a little weird with using the ' symbol as it relates to injecting a variable, but you should be able to find some examples online.",2.0
g4959iv,inq2o9,"My first idea was so, but In such case I would have to declare number of `@var`  parameters equal to number of columns in Table 2 and then join those parameters to pick up correct expression. In the meantime, I have came across  `sp_executesql` that may allow me to insert entire function from Table 1 to dynamic SQL",1.0
g4964xr,inq2o9,"Totally doable, just get the names of the columns from the sys table, and then write your join out dynamically.

You can do something where the first row in the columns @table has a prefix of ON X.ID = Y.ID and all following rows have a prefix of AND X.ID = Y.ID.",2.0
g498v11,inq2o9,"I can see wider field how to overcome this problem , thanks to you!",1.0
g49oj6k,inq2o9,"Dynamic SQL tends to be frowned upon but there are a lot of valid use cases for it. You can dynamically generate your entire select, and your join conditions.

It's a pain to work on, and you'll often find yourself typing `select @sql`then copying the text out of the results window to see what it looks like, but it is doable and works like a charm some of the time.",2.0
g4glhss,inq2o9,"The easiest way to write dynamic SQL is to write out normal SQL, then find and replace ' with ' + '''' + ' then wrap the whole thing in single quotes. Works every time.

Also PRINT @SQL has a better output format, so it's useful if you don't have some SQL formatter to hand.",1.0
g4gp3mq,inq2o9,"That doesn't really work well when you're trying to dynamically write a select statement or join conditions. I mean you aren't wrong but you end up writing:

    select ' + @columns + '
    from table
    join table
        ' + @join + '
    where things",1.0
g49fn2i,inpa4v,"https://youtu.be/SwtlCVyxqHk?t=127

Umm what?

So you think all clustered index (in case of in_row_data allocation even for data pages) pages are co-located for same index values for every table?

This is either the greatest discovery in MS Sql internals ever or complete BS.

Any source for this insight?",5.0
g49ntdw,inpa4v,"ELI5 explanation. No, not on disk sectors. Beats the GAM IAM explanations, and its quite a bit easier to mentally ingest for newer individuals. 

What I have ""known"" for many years stems from my understanding of some amazing articles by Paul Randal (a SQL Server DBCC dev) on IAM chains and mixed/uniform extents. College years, so somewhere in the 07-09 range. The same explanation has been repeated to me a few times over a decades time, so I use it.

If you have information/alternate eli5 explanations, do share.",0.0
g4ahfbn,inpa4v,"So, as usual, BS and evasive maneuvers using obscure terms (how would GAM -that's global allocation map - even begin to play into this, I wonder) 

You can read practically everywhere (google it) that clustered index storage in MS SQL is very similar to b-trees, with leaf pages (or allocation units ) holding the whole row of data (IN_ROW_DATA). 

( a couple links quickly googled https://www.sqlshack.com/designing-effective-sql-server-clustered-indexes/ , https://www.sqlservertutorial.net/sql-server-indexes/sql-server-clustered-indexes/ )

You might have heard of ORACLE's CLUSTERS - which is a completely different concept and it does co-locate different table data and you somehow applied that to SQL Server.

Maybe I'm being overly harsh, but based on our prior exchanges I came to expect 80% banalities and bullshit from you. Your most recent series have not shaken my impression.

ps. given that the most upvoted content on this sub are yearly, quarterly and monthly reposts of memes that were stupid to begin with, your content is still about 1000% better and I would suck at doing any kind of Youtube - so just try to be better (more accurate) in what you ""know"", ok?",11.0
g4b4pli,inpa4v,"Yo, you're smart enough to test this out and see if he's right, aren't you?

My man ASE is a fucking wizard. If he says its true... I want a god damn expert to *prove* it to me he's wrong. I don't want a book, or an article. I want statistics. 

And, if he is wrong, and you can prove it, I will bet you a steak dinner that he's the kind of man that will thank you.

I'm not smart enough to prove this. You are. So do it, or otherwise reevaluate this comment you made:

&gt;ps. given that the most upvoted content on this sub are yearly, quarterly and monthly reposts of memes that were stupid to begin with, your content is still about 1000% better and I would suck at doing any kind of Youtube - so just try to be better (more accurate) in what you ""know"", ok?

Where is your content stacking up here if all you're doing is saying he's wrong without coming correct? I say all this with love, too, because I've been talking to you for awhile here, and you've helped me immensely, so I respect you, but I respect him, too.",2.0
g4aye2p,inpa4v,"&gt;You might have heard of ORACLE's CLUSTERS - which is a completely different concept and it does co-locate different table data and you somehow applied that to SQL Server.

100% what I thought MSSQL was doing, kind of:  


What I assumed, was the IAM was always the root page. ([https://www.sqlskills.com/blogs/paul/inside-the-storage-engine-gam-sgam-pfs-and-other-allocation-maps/](https://www.sqlskills.com/blogs/paul/inside-the-storage-engine-gam-sgam-pfs-and-other-allocation-maps/))

IAM chain-&gt; IAM page -&gt;heap/cluster -&gt; intermediaries -&gt;leaf  


Combining the oracle table cluster ideology with the IAM trees makes perfect sense in my head. 

What I really assumed was:

IAM chain-&gt; IAM page-&gt;Heap-&gt; data pages  
IAM chain-&gt; IAM page-&gt;A Clustered Value (no matter the source, like a table cluster)-&gt; intermediaries-&gt;data pages 

So I just spent the last 45 minutes in DBCC PAGE/IND looking for evidence of that crossover.

Nope. Index partitioned, no crossover. 8 years, I accepted this as fact. Good on you for being a schmuck. No sarcasm, I respect that. Im over here Jenny McCarthy'ing this shit...  


&gt;...banalities...

Ill upgrade that gold I gave you, to a platinum if you can find my data model anywhere else. I might even be willing to drop that $100 one on a post you create roasting the shit out of me. 30% unoriginal at best (""necessary content""). Pure SQL Reddit bot (oauth2 via execute external scripts), Decision forests, data-driven query builders. Banality my ass.",-1.0
g4b668w,inpa4v,"&gt;  Good on you for being a schmuck. No sarcasm, I respect that.

WTF? Are you bipolar within one paragraph or you just don't know what 'schmuck' means?

&gt;  if you can find my data model anywhere else. . I might even be willing to drop that $100 one on a post you create roasting the shit out of me.

Keep it. Why would I care? Told you last time that you should check out Vault 2.0.

And I've argued with you before and about as entertaining as pissing into the wind.

That aside, I consider it proven that someone who dont know 100% of their shtuff can be very productive and deliver decent solutions also. This also has nothing to do with banality but I guess knowing meanings of words is not your strong suit.

Be well.",3.0
g4f2iho,inpa4v,"You say he is wrong, but he seems to be delivering results in his video that conform to his belief. I'm not really interested in the name calling, but can you demonstrably prove he is wrong? 

For example, this could be a *glitch* in the way the engine is designed, and it could be patched any day now.",1.0
g49zpfd,inpa4v,003 has no sound after the intro. Thanks for putting it together and up!,2.0
g4a45bl,inpa4v,thanks for that... looks like i rendered in mono. rerendering now.,1.0
g493gcw,inpa4v,Bump.,-1.0
g4b5bd3,inpa4v,"Great videos! However, these days you can use online optimizers that will do everything for you, Like EverSQL.com",0.0
g4b5j6d,inpa4v,"Huh, every comment you have made has that url in it.",0.0
g48unoy,inomfm,"SQL is very useful but it's only one part of a larger ecosystem. People that just do SQL do exist at larger companies. Most people learn programming languages, and server infrastructure to compliment the database back end. Knowing the other pieces let you design better databases as you understand the full use case.",3.0
g48wdb3,inomfm,"The question would be, do you know what you don't know?

With many skill sets, especially the technical ones, you'll progress from ""what's this"" to ""I've got this down pat.""  This is the ""beginner"" phase.  Joins, complex ones, temp tables, maybe indexes.  You still write queries that can be slow.

When you know what you don't know, that's when you're actually getting somewhere.  You know what you're doing might behave badly, but you also know how to look for it and could probably Google your way through it.  You know what a query plan is and look at it to see if this query is fast, or just looks fast because the data is small.  You can write better indexes than clippy, and without its help.  You know when it's safe to self join, and when it's asking for trouble (and how, with a little Google, to fix it).  That'd be intermediate.  At this point you understand that what you know is far from everything.

A pro knows to do all the above without Google (but may still look because it's a lot of detail), can read a query plan in seconds, tweak the query to behave well without using hints.  You can still vomit out complex queries when you're six pints into happy hour (though syntax may suffer at this point), and write complex stored procedures that are immune to parameter sniffing.  Dynamic sql is an old friend with a love/hate relationship, and you know how queries can go badly before ever hitting execute.  Probably a lot more stuff I don't know because I'm not a pro.",4.0
g48y2ke,inomfm,"In my experience a lot of people ""know sql"" if they can select from a single table. 

But until you can grab any information from 5+ tables formatted correctly, run updates use cte's stored procedures, deletes. Where people can rely on you to pull information correctly. Id say you're in ""good"" at sql territory. 

Eventually you'll get into optimizations for speeding up your queries. Id say thats advanced.",4.0
g494d4l,inomfm,"Complicated joins, window functions, pivot/aggregation, CTEs, and stored procedures imo.",2.0
g49hmaq,inomfm,About 5 to 7 years...,2.0
g4asr52,inomfm,"People who can solve problems are good. People who 'know' SQL tend to invent them instead, in the long run.",2.0
g4bgqbm,inomfm,Swagger.,2.0
g48rhdk,inomfm,experience,3.0
g4a53bg,inomfm,"The amount of time spent on a whiteboard vs coding. Like they say: ""weeks of programming saves you hours of planning"".


Goals for a higher level of understanding: be able to rewrite your code using different syntax so you can choose the best solution; instead of 'this' solution.",1.0
g48ks2k,inmsut,"I have to run so this will be a short answer. When you first open the program, click the wrench next to ""MySQL Connections.""",2.0
g48lhwz,inmsut,"They're in the mysql database, in the 'user' table. The 
GRANT command adds the entries. The host is the host name that mysql is running on or you can use the IP address. The user table has a host entry. That's the host your python programming is running on according to DNS.",2.0
g48m0rv,inmsut,"Host: the network info of the machine running the server

DB: you can specify the specific database within the RDBMS, though you don’t have to if you’re ok with typing ‘database.tablename’ every time

User: the username you use to login. You can do this as root, but I wouldn’t recommend it. If you have sudo access to the host machine, you can run ‘sudo mysql’ and it’ll run a shell as root, and you can follow various online tutorials to create a user from there. For example, if you only need to read tables, you could do

    create user immarealcat identified by ‘123isnotasecurePassword!’;
    grant select on *.* to immarealcat;

Password: you specify this when you make the user. You can also update it later with SQL commands similar to the above.

Edit: if you have any other way to get in, you can find all the users by querying information\_schema. I’ll be back on it a minute to write an example.

edit 2: to see all users:

    select * from information_schema.USER_PRIVILEGES;",2.0
g48uh8p,inmsut,"Hi, thank you so much for explaining! I understood everything clearly :)",2.0
g48m4m7,inmsut,"Host refers to which server the MySQL data is being served from, either by IP or by network name.  If it's being served from your local machine then this would be 'localhost' instead.",2.0
g4834y0,inksxx,You could start a job as a support technician for a web development company who uses SQL databases then work your way up to be a developer,12.0
g48drzh,inksxx,SQL Developers in US healthcare often make more than $120k with pretty solid benefits. It's challenging work with interesting data.,7.0
g48o2d0,inksxx,"For whatever it's worth, I've worked for a few big health systems and just want to temper expectations a little bit: 

1. That's more than most pure SQL developers make (at least at my current employer).  Senior level people or managers can certainly make that much, but  that's certainly not the average.  

2.  It's hard to get far doing only SQL.  Generally you want to be versed in a variety of ETL tools and BI tools.  You'll want to have a solid understanding of how database engines work and actually execute your queries.  Having one or more general purpose languages in your tool kit is also becoming more and more important these days, as is a familiarity with a variety of database models.

3.  Healthcare experience is highly valued.  It's harder to get hired to senior roles without previous healthcare experience  (I think my employer is moving towards not hiring people without healthcare experience).  The flipside of that is if you can get into an entry level role, an employer will want to help grow you as a developer.",8.0
g48pdv4,inksxx,This is more in-line with what I was thinking. The ecosystem is bigger than just the language.,1.0
g48zymv,inksxx,Thank you for correcting me. I agree with everything you said and shouldn't have implied that the average SQL developer pay is even north of 100k. The word often is definitely the wrong word.,1.0
g48fj54,inksxx,with SQL as their only skillset?,5.0
g48gkhd,inksxx,Industry or clinical knowledge likely involved too.,5.0
g48ivye,inksxx,"Yep. Generally speaking healthcare data is the hardest to work with and takes years to be proficient at, so companies are highly incentivised to keep good developers.",2.0
g48qsg4,inksxx,Yes... with SQL as their MASTERY skillset. Many companies hire individuals that have different masteries vs individuals that are decent in a few areas when building the core team. The jack-of-all-trades individuals are good for support roles/backfill.,1.0
g491ms5,inksxx,"True. The jack of all trades is the modern 'full stack developer', but some companies may still hire 'masters' for certain areas.",1.0
g485uhc,inksxx,"Yea, you can get pretty far with just sql. You will get to points where you realize you need a scripting language to finish a solution to a problem, but sql has a lot of stuff that allows you to skip learning other languages for a while (but you still have to put the effort in to learn something new in sql).

It analyst, data analyst, data conversion, data specialist, sql developer. All these rolls exist, and you can occasionally get away with only knowing only sql. Ive gone to several interviews for these kinds of roles (most of them before i knew sql) where they only asked sql questions.",12.0
g489hlo,inksxx,Do you happen to know whether a job as a data analyst in a news outlet requires a degree? Data analysis in journalism appears to be huge.,5.0
g48bdhh,inksxx,No idea. My experience has been with investments and ERP.,6.0
g48h6fb,inksxx,"No. You would need a journalism degree to get a journalism job or start from the bottom in a newsroom.

Data analysts don't work for news outlets.",5.0
g48fcfa,inksxx,Recently?,1.0
g48fw9s,inksxx,"Yep. Started out in 2013, then changed jobs in 2017. Both times, I've had interviews where SQL was the only language tested. I do have a bachelors, but I've worked with people who only had an associates degree in the same position.

A big part of IT that I don't see people talk about much are personal/planning skills. Being able to work with others and communicate. In medium/large companies, not many people hold all the keys. You usually have to know who to go to get things done, like networking/security and web/app development stuff. Shouldn't go in thinking you're gonna be the only IT person for the company.",1.0
g48e7rn,inksxx,DBA is the way to go.,5.0
g48gm1i,inksxx,Data Engineer.,3.0
g48kcyj,inksxx,"I never would have thought that the SQL Server train would be going this long.  How many times have you heard that programming is going away.  Machines will program themselves.  Self-serve BI.  The only thing that has happened is that most every environment has become far more complicated.  We're now stretching databases between on-prem, data centers and the cloud.  Azure not enough?  OK, the database now stretches across AWS too.  Add Tableau, PowerBI, data warehouses, Snowflake, complex integrations (SSIS), reporting, etc.  I have made an entire career out of the database realm - I NEVER would have believed that this was possible when I started.",3.0
g48gfch,inksxx,"SQL is just a tool.

That's asking if one can get a job just knowing how to use a hammer and sure you might be able to get a job pounding nails all day but it's much better for your career to learn about carpentry as a whole.

You can learn the basics of SQL very quickly but you should supplement that with topics such as data modelling, data visualization, ETL, scripting and automation. Not to mention learning about an organization's business processes.

Understand how the tools you have and knowledge you gain can be used to help solve people's problems.",5.0
g48h4fv,inksxx,"If you want to use just SQL, good luck. You'll need to be extremely good and be lucky enough to live near a decent sized pool of SQL only jobs.

As part of a moderately larger skill set, but SQL is a big part of your work, that's a much more viable career path. Exactly what else you need depends on the specific job, but Python is good choice. Luckily, once you get good at using on general purpose language, it's not hard to pick up another one if you need it. 

I'm seeing DBA brought up on here, and that definitely uses SQL, but you'll want to do some research on what DBA's actually do and decide if that sounds like what you're looking for.",1.0
g483k8q,inksxx,You can't really make a career out of just SQL. It's just a tool and like most tools has a very specific use. That's like saying you've learned how to use a hammer and want to make a career out of it. A good carpenter must know how to use many tools. It's the same for a person working in technology. Ask yourself what you want to BE and then learn to use the tool**s** appropriate for that role.,-1.0
g483cnp,inidvx,"There's this. 

 [https://www.vim.org/scripts/script.php?script\_id=5081](https://www.vim.org/scripts/script.php?script_id=5081) 

Not updated in a while and I haven't used it myself. I would have thought there might be more demand for this kind of thing, but this is the only one I saw.",2.0
g483mi5,inidvx,"Ha, thanks, that's actually a lot like what I'm looking for! 

And yes, totally agree that this seems like something that would have a lot of developers developing and using it. 

There's even terminal-based browsers so I wonder why this isn't a thing.",1.0
g49bydx,inidvx,"[psql](https://www.postgresql.org/docs/current/app-psql.html)  is the default command line client that comes with every Postgres installation. 

You can ""pimp"" it using [pspg](https://github.com/okbob/pspg)",1.0
g47t1a6,inidvx,"Postgres usually comes with ‘psql‘ as a CLI client. If you installed a local postgres server from the usual apt repository, you should have it on your path already.",0.0
g480all,inidvx,"That I already know - but \`psql\` isn't really that nice to look at or use - it's a bit too simple to actually be of use to me. What I'm looking for is something that has tabs and vim-style typing.

&amp;#x200B;

Maybe no terminal-based SQL IDE exists, but it's worth asking :)",0.0
g48kj6g,inidvx,"psql doesn't have to use the line based editor, it can use vim if your EDITOR I'd you want and access it with \e from psql. But I'm guessing that will still be pretty bare bones for you.  

Additionally there a number of vim plugins that would let you write sql in vim in a more ide style, like dbext or it's slightly more modern cousin dadbod.",1.0
g47xf42,inidvx,"This has been posted on Reddit, it's a good intro to psql

[https://knowledgepill.it/posts/postgresql-basics-guide/](https://knowledgepill.it/posts/postgresql-basics-guide/)",0.0
g480cgv,inidvx,"I'm way past the basics, using SQL daily at my job - I'm just looking for a terminal based IDE that has more features than \`psql\` but isn't quite as resource intensive as these fully-fledged desktop Java resource hogs.",0.0
g489dci,inidvx,"You could try looking at Emacs + postgres, sure someone will have.",1.0
g49c9pl,inidvx,"&gt; but isn't quite as resource intensive as these fully-fledged desktop Java resource hogs.

Not sure which ones you mean. I use a Java based SQL (GUI) client which I don't consider to be a ""resource hog"" at all.",1.0
g4j2678,inidvx,"Well guess what - this post isn't about your PC/Mac and your experience - it's about mine. I have 3-4 VS Code (Chromium essentially) windows &amp; a browser (Firefox) open while running a handful of Docker services, which adds up to quite a bit on my 8GB RAM Dell XPS 13. 

&amp;#x200B;

Add Datagrip or DBeaver to the mix and typescript linting in my pre-push hook when I push to a remote git repo and my PC freezes. 

&amp;#x200B;

So of course I'm looking to minimize system resource usage wherever I can. This isn't about your use-case. The question wasn't ""hey does this take resources on your computer"" because I don't care about your computer, why would I? The question is about whether someone knows and uses any good terminal IDEs.",1.0
g47lol8,inhkh8,"SQL, C++ and java are three completely different languages...

You should probably end up learning at least the basics of all three at different times.",3.0
g47m9sn,inhkh8,"To my knowledge , SQL is Fourth-generation languages and they are dependent on the third-generation languages like C++ and JAVA. So SQL is like an add-on for other languages to help them make websites and manage the data on them",-9.0
g47nwgb,inhkh8,"I think the comparison you're trying to make is that C++ and Java get turned into some target language (assembly or Java byte code). The flavor of SQL you use will probably be running C or C++ (or whatever) code under the hood to parse the SQL statements and build the abstract syntax tree, but just as you don't need to know assembly to program in C, you don't need to know what's going on under the hood of your SQL statements in order to make them run. 

  
Take the SQL class. I assure you it's not beneath you. Knowing your normal forms is mandatory, the materials will have nothing to do with your C++ programming, and having a good handle on databases is both really interesting and extremely useful.",5.0
g47tl7c,inhkh8,"Thanks a lot for your time and thanks for explaining to me. I was worried because when our Dr came to the first lecture he kept connecting what we will be taking with Java concepts like abstract, Interface and others.",1.0
g48upjn,inhkh8,"No problem, I think it's great that you're asking questions on Reddit.

A relational database system, as a standalone program, has nothing to do with your Java code, so I'm confused about why your professor is relating it to Java programming concepts. You can use the APIs in a Java library like jodbc in order to execute SQL statements from your Java (this is probably what your professor is referring to), but otherwise, your general programming in Java is quite separate from the SQL itself. 

Put another way, you can run a SQL database without using any Java whatsoever. In fact, there are plenty of data analyst-type folks who have entire careers where they only use SQL, who don't use a general purpose programming language (Java, C++, Python).",2.0
g47mbis,inhkh8,"Er... no, not really.",7.0
g47m32n,inhkh8,"Java is more or less C++ without pointers. Sql is a pretty valuable for working with databases. You're likely sabotaging yourself if you actively avoid it.

You will likely work with databases at some point in your career.",3.0
g47slc2,inhkh8,"SQL is completly different skill set than C++ or JAVA.

last two are programming languages and SQL is a querying language.

I guess at some point or another in your life you will have to work with a DB - and they speak SQL.",2.0
g48loxu,inhkh8,"Most high-level programming languages have constructs embedded within them to allow for programs to do SQL calls.  It might just be a function, you might have to embedded a DLL or something, but the capability will be there somehow.

SQL is a language all its own--a query language to be used for pulling and manipulating data in RDBMS tables.  It can do many things, but is also very powerful when combined within a program of another language.

A couple of examples:  early in my career, I wrote COBOL/CICS programs which used DB2 SQL to pull data from our database tables.  Then for a long time I used SQL within Peoplesoft (4GL) programs.",1.0
g48z8ki,inhkh8,"Do learn some sql.  Yes, you can just use an ORM and never see the data, but the queries may not scale well (or at all).

SQL is not a programming language.  In a programming language you define the steps to get the output.  In sql you define the output and the engine figures out the rest.

Common programming best practices can cause the sql engine to make planning mistakes, resulting in poor performance.  If you at least understand the difference in SQL, you'll be in a much better position to deal with it when it inevitably comes up, or at the very least not look like an arse when someone else starts talking about normalization.",1.0
g47ehkd,infptn,"Select app_type, sum(bytes_received) + sum(bytes_sent) as total_bytes group by app_type order by sum(bytes_received) + sum(bytes_sent) desc

To break that down:

We are grouping by app.  You knew that one.

Sum, per app, all sent bytes.  Repeat that for rcv bytes, add the two together, and give the new column a name.

For the sort, it's important to be aware that mssql doesn't recognize a column alias in the same statement, so you have to repeat the sum+sum bit.  (Or use a CTE.)  Don't worry, the engine is smart enough that it'll only do the sums once.",4.0
g49keos,infptn,"Thanks you so much.
That is a really helpful and sorry I did not see the notification that you posted.

Works perfect.",1.0
g46zx4z,incm5l,"Basics like different types of joins, unions, aggregates, window functions, case statements etc. will hold you in good stead anywhere. Doesn’t matter marketing or not.",5.0
g474b5q,incm5l,Thanks! I'll brush up on those,2.0
g471z1s,incm5l,"I don't use much aside from select, join, and where to pull reports. I'd suggest getting really good at regex.",2.0
g473z7a,incm5l,Regex? What's that? I've never heard of that before,2.0
g4742c8,incm5l,"Info here: [https://en.wikipedia.org/wiki/Regular\_expression](https://en.wikipedia.org/wiki/Regular_expression)  


Practice here: [https://regexone.com/lesson/introduction\_abcs](https://regexone.com/lesson/introduction_abcs)",1.0
g477lie,incm5l,"If you're using it for marketing you might want to familiarize yourself with KPI's and Data Visualization.  A lot of queries you create can be reused weekly, monthly, and quarterly for reporting and analysis.",2.0
g47f4vv,incm5l,"Window functions as well with others mentioned above. Also, regex is a must!!",1.0
g46bvol,in94bv,You'll need a HAVING clause that can filter your aggregated results.,2.0
g46myt1,in94bv,"Got it,tahnks!",1.0
g46grvs,in94bv,"You are 99% of the way there. The problem is that you haven't added any code to let SQL know that it should only return the cities with greater than the average. So it's returning all the cities. 

You need to include that condition in your code, look up the HAVING function, that's what you want to use. Good luck.",1.0
g46mz4f,in94bv,Thanks!!,1.0
g464h5i,in7zjb,"Sql server can handle volumes of data that make a million rows at 1GB look like a pittance, because it is.  Sooner or later, you'll need to work with it, and exporting to excel is just causing more pain for yourself.

SQL is designed to handle that kind of data, and to do it efficiently.  (Sometimes it even manages it!)   bLearn to leverage it and it will save you a ton of time and effort down the road.

I manage a fleet of servers that are typically around 50GB live and 150GB data warehouse.  The most valuable data is in a table that reaches millions of rows and weighs in an about half of the total data warehouse payload.

Excel would choke on that.  I've seen people try it, the result is hilarious (They stubbornly wait for it to fail).  One supporting index and that same analytics task runs fast enough in-engine that it doesn't even register 1 second on the wall.

There's tremendous value in SQL, especially for analytics.",10.0
g46krg6,in7zjb,"&gt;Excel would choke on that.  I've seen people try it, the result is hilarious (They stubbornly wait for it to fail).  One supporting index and that same analytics task runs fast enough in-engine that it doesn't even register 1 second on the wall.

Exactly this. Anyone doing analytics in Excel who hasn't yet run into a brick wall of scalability is either very new or has worked in a very strange environment to have not been exposed to large datasets. If that person then branches out into consulting it will not be long before they have a really humiliating moment in front of a client.",5.0
g45zl05,in7zjb,"If you are a deeply experienced analyst, basic SQL will not be troublesome for you to learn. You could spend a weekend looking into it and be suitably equipped for basic operations. If you're brought in as a senior analyst on a job and they have to go fetch an intern to run a simple query to put the data onto your local machine, it's going to be a really bad look.",5.0
g45wjfy,in7zjb,"Most businesses use sql, so if you don't know it you wont be able to pull your own data. But plenty of consultancies only use excel, they're just consultancies rather than analytics.",4.0
g45ypnq,in7zjb,"Good point. My terminology may be inaccurate. The business would indeed be a consultancy with data analytics being the primary tool to help guide the recommendations. I also forgot to mention that my intention is to have the clients provide the data, so I wouldn't need to pull data from their systems. They would send me the raw data and I would slice and dice and send back reports and recommendations.

Thank you for your reply. I believe you may have confirmed my suspicion that Excel and Access may suffice for my purposes.",1.0
g478lpp,in7zjb,"If a business has the skills to pull their data from their source systems to your specifications, they've just demonstrated they've already got 80% of the way to doing the analysis themselves. I assume you're also bringing some domain-specific knowledge or insight to add value? Because if I hired a consultant to do data analysis and I found out it was all Access and SQL, I'd be a bit concerned about whether I was getting value for money and question why we weren't doing it in-house.",2.0
g461azj,in7zjb,"So I do healthcare analytics for a consulting firm where I use Sql, access, excel, and bit of PBI. With the data sets you work with you’ve mentioned access is better than excel for certain things like large claims manipulation.  

Some teams within my firm only use excel and struggle to run anything close to a 1 gb in size but are adamant it’s the norm. I feel it’s sort of like that: Whatever you’re used to using always seems like the best answer but having used SQL, I think you’ll benefit a lot and won’t regret it. 

Not bashing on access. It’s great since it is already available and can handle big data sets better than excel but now I mainly stick with Excel and SQL. 

You could try PBI, which is Pretty slick and better than access imo. Plus since you have extensive experience with Access I’m sure picking up SQL won’t be hard. You just need to learn syntax (Which access provides to some degree) since you already know how you want to structure your data set and understand data manipulation. 

Just my two cents. Sorry if my ideas seem disjointed. I’m out right now but was excited to discuss this since I literally had the same the same dilemma a few years ago.",3.0
g475ubp,in7zjb,"Op I am saying this with the utmost respect possible.  Please don’t take offense to this:

I manage a database / data analytics department.  If I was looking to hire an analytics consultant and I saw all they had was access and excel, I would not let them anywhere near my data.

Please please go ahead and dive head first into sql.  You’re going to need to be able to hit the ground running within their systems and get them progress on day 1.  Barely anyone will take the time to send you raw datasets and have you build reports.  They will expect you to use their tools and their systems. 
You’ll need to show your work, and show that you can handle very large datasets. This requires an RDBMS other than Access.

We are here to help with any questions you may have.  We only wish you the most success",1.0
g48qndg,in7zjb,"I used to work in a very successful small business that provided data analytics and reports to our customers. We regarded ourselves as a 'boutique' business because we served a very niche need in the industry. Our clients ranged from Fortune 5 to other small businesses.

Our tool was an Access database. Our clients sent us data or we downloaded it from their secured sites. We did our thing and provided reports back. We also exhanged excel files with our customers.

Interestingly, the DBAs weren't the decision-makers when deciding to hire my former employer. Those choices were made either much higher up the ladder by the customer or by the executive who needed to consume our analytical information. 

Most of our customers are compelled to hire external for legal or regulatory reasons. They don't care about our tool or what technology we use; to them that is our proprietary. They just care that we are a good steward of their data while it is in our hands.

My former employer did however, spend a shit-ton insane amount of money to beef up IT, security, and get all of the fancy certifications one could get in the industry for it. After all, we were asking other businesses to throw their data over the firewall to us.",1.0
g48rosv,in7zjb,"I appreciate your anecdote, but there’s a reason Access and Excel aren’t really used in any Big Data discussions.  My organization has millions of transactions per day.  Access chokes on about 1 hours worth of data for me.

If you’re looking to be a consultant, you need to be at the forefront of innovation when it comes to your subject.  The consultant should guide the client to approach the problem with the best possible solution overall. Scalable, manageable, efficient, and powerful solutions should be offered.  Access and Excel don’t meet those criteria.  

Access is a niche product that serves its purpose.  That purpose is not Data and Analytics. If it were, Microsoft would be pushing it like they do Azure Synapse, data warehouse, Data Factory, Power BI/PowerQuery (which Evolved from Excel) etc.

At the very least, learning dedicated Data Analysis and reporting tools like Tableau, Power Bi, Qlik, etc will put you miles ahead of the competition.",2.0
g4ak5op,in7zjb,"SQL is not so hard to think whether you should learn it or not. Just start learning it. It would not take more than a week. I'd also recommend the resources like w3schools, stratascratch or datacamp.",1.0
g45sa5h,in7mr8,You can use sql and (R or Python) in combination. Learning both is worthwhile.,7.0
g4635gn,in7mr8,"SQL is the language of the (relational) databases. You often don't choose to use it but have to as that's how you communicate with the database. Your DB Admin or your RAM will roast you alive if you try to query all the data from a database all the time because you wanted minimal exposure to SQL. In addition, SQL is pretty simple and some things are easier to do with it than with Pandas/R. 

You don't want to use Excel to anything complicated. Analysis done in Excel is slow to reproduce and pain to maintain. It is not straight forward how a logic of an Excel file goes: there is one column of formulas here, a mega long and unreadable formula there and an ocean of formulas just for joining a table.

I haven't done much with R so I'm not really qualified on discussing that but I do my work with Python &amp; SQL and I do fine. Pandas for data manipulation, SQLAlchemy for interfacing the DB, Matplotlib/Seaborn for visualization and Scipy for some complicated calculations. Then Dash/Bokeh if interactive dashboards are needed. 

To be honest, 2 weeks of Python is nothing in terms of getting fluent in it. I think it took me 9 months to be fluent with it. Probably like 1-2 years to write decent quality code. After 3 years of programming daily I still have much to learn.",5.0
g462g8y,in7mr8,"Shortest recommendation: SQL then Excel then R.

So trying to answer which one ""best"" is a tough answer ... but there's no clear cut ""winner"", but all 4 are going to be useful skills for a long time.  So let me give you a perhaps more practical answer:  

As a job hunter / employee, you likely aren't going to get to choose to use exclusively one of them unless you are the only data person, which gets kinda lonely.  You will be more marketable if you can do 3 of the 4 at least, and be productive more quickly if you have some familiarity to start from.

For which to do *next*, matter of preference, here are some thoughts...

Excel is amazing, and it's an awesome tool if you are going to work for someplace small that really has no idea how to ""data"" yet.  You will be a god if you can show them some Excel macros.

On the other end of the spectrum, if you are going to be accessing enormous sets of data, you almost guaranteed to need to know how to do joins etc and extract some useful subset of the data using SQL even if that's just to get that filtered dataset to R\\Python\\Excel 

Then R and Python and tremendously powerful tools to do cool analysis and visualizations not available in Excel (and SQL doesn't do visuals).   I said R last because R and Python have tremendous overlap in functionality, and you've already done some R.

SQL is also different construct of ""programming"" then Python, so that'll be a good perspective to learn (and why I said that before Excel)",3.0
g46vz09,in7mr8,"So in terms of learning it would be: 

Excel -&gt; SQL -&gt; Python/R

Asking because i want to teach myself these languages, I've started with SQL after already knowing basic excel (vlook up, pivot, index match etc).",2.0
g46y0f9,in7mr8,"I don't think you need to become an expert in SQL before you start learning python or r. IMO, you can get familiar with SQL while simultaneously learning the basics of python or R. There will probably be a larger learning curve to python or r than SQL, so you won't be able to utilize your SQL knowledge within a python/r code until you understand the basics of the language.

I personally like python because you can do a lot with it and it's a language that makes sense to me.",3.0
g46wa1u,in7mr8,"You need to learn all 3, to varying degrees of expertise.

SQL - Absolutely essential. Data lives in databases. Underpins most infrastructure you’ll have to interact with.

R/Python - Makes it easy to do statistical analysis, create visuals, create pipelines, do data mining, scraping, whatever.

Excel - Everyone uses it so you need some facility with it. I’m not a huge fan, but it has its uses. I recommend it for financial modeling, simple proofs of concept, creating simple lists, quick and dirty filtering during data exploration. It’s also a decent reporting tool when you do all the real work in the DB and query it from Excel.

In industry SQL will be by far the most essential skill. SQL encompasses a lot more than just the language, and I’d include data modeling, ETL, data warehousing, etc. under the aegis of SQL. This will be a hard requirement for many jobs.

Python/R is essential for the cool stuff. Most places won’t make this a hard requirement, but it’s a very nice to have as you can do all kinds of creative stuff with it.

Excel depends on the industry, but it’s everywhere. If you work in anything related to finance, expect to develop a “familiarity breeds contempt” attitude. Be familiar with it, but being an Excel whiz isn’t going to be required in any job you would actually want to have.",1.0
g46xodl,in7mr8,"I love SQL and spend a lot of my work time using it, but I don't consider it a great data *analysis* tool.  I consider it a great data storage, data retrieval and data aggregation tool.  SQL isn't good for computation.  It isn't designed for doing complex statistical work, it's designed to optimize data retrieval.  I consider SQL knowledge an indispensable tool for collecting data.  There aren't any native data visualization tools with SQL, so whatever you do with it will have to be exported to some other platform to share your insights.

Excel is great for business analysis.  You can run a lot of computation through Excel including just about any business and financial math you're likely to encounter.  Excel has okay data visualization tools.  Excel is also ubiquitous -- everyone has it, it's familiar so it is easy to share.  If you learn to write VBA, you can become a rock star Business Analyst.

I've spent less time with R than SQL or Excel, but I am very fond of it.  It is the best tool for doing sophisticated analysis, especially for more scientific, machine learning and rigorous academic work. People argue over whether R or Python is better for Data Science, Excel isn't even a distant third in that race.  There are R libraries for just about every imaginable academic project.  R has amazing data visualization tools, absolutely top class.  It's free, but it isn't as widely known as SQL or Excel so it's harder to share or collaborate.",1.0
g462v6b,in6stx,"Django has its own entity relationship manager so you would be better to create your models in Django and let it create the database and tables, queries, etc for you, this is one Djangos strengths.

Reversing the database to django models will be hard. If you are determined to do a database first approach you may be better to use something other than django for the front end. 

I offer my advice as someone who is knowledgeable about the sql side and often think of things from the database side of things first and only recently started learning Django with no background in development. I am usually one of the first people to cringe  at ERMs but in the case of django and a smaller project it will be easier to let Django take care of things.",2.0
g48ybef,in6stx,"Yeah I'd stick with ingredients and measurements, the ingredients would just be an ID and a Description, things like butter, sugar, flour etc. The measurements table would be the same, with the descriptions being grams, ounces, cups etc. I'd have a recipe header table which contains a recipeid and a text description and maybe a link to a photo, I'd have a recipe ingredients table which would be recipeid, ingredientid, ingredientamount, measurementid, so you'd be able to say a cake has 100 grams of flour and 3 units of egg for example. Finally the recipe steps table would be recipeid, stepnumber, stepdescription. Hope that makes sense!",1.0
g47h1bd,in4wtc,CTE (particularly recursive) and Window functions would be big I think.,2.0
g47hqvm,in4wtc,"Thank you, I googled both of these constructs, and I couldn't find any references to whether or not these are part of the SQL standard ...?

Are CTE and Window functions supported by all the (major) db vendors ...?",2.0
g47l4hx,in4wtc,"I don't know if they're standard, but they are important.  I have reports that would be horrific to run without them.

A CTE is basically a sub query.  A Recursive CTE references itself.  For example, you have id and parentid in a table.  It is a type of self join that goes all the way down the rabbit hole and find ALL children of the selected object, even if it's sixteen layers deep.  (There's usuay a limit, I think the default is 100 layers in SSMS.)

A window function is just telling sql to peek at the next row or the previous row (with sorting criteria).  It lets self joins looking for a chronological event run against massive tables without wrecks ng the cache and slamming the disks (LEAD and LAG in in mssql).",1.0
g47m4q3,in4wtc,"&gt; A window function is just telling sql to peek at the next row

I could find information about PostgreSQL implementing Window functions, making me believe (probably) most other db vendors will support it too.

It's important for my project that I'm able to create generic syntax that's supported by all (major) db vendors, since the entire idea with the project, is to create a generic SQL engine. But I do expose the ability to directly inject your own custom SQL, for the _""edge cases""_ ...

My guess is that if you're doing complex reporting, such as recursive sub queries, and you're not willing to use _""materialized path""_, this is probably something only supported by MS SQL, and not really _""standard""_ per se ... :/

Thank you, I'll have a look at what I can do in these regards :)",2.0
g4447b3,imyr6p,"If your only goal is learning sql, you don’t need to go buy a pc. If you’re looking to learn ms sql (Microsoft SQL Server), you will need some way to run windows software, which doesn’t mean buying a new computer either.   

Instead of listing all the different ways you could run the different flavors of sql,  what flavor of sql are you looking to learn, if you have already decided.

Regarding performance, the system requirements for running a sql database are completely dependent on the data you’re accessing and storing. Remember, there are many databases inside your phone, tablet, and most other electronics that need to remember any data, each with WILDLY different system requirements.",2.0
g44tyh4,imyr6p,"&gt; If you’re looking to learn ms sql (Microsoft SQL Server), you will need some way to run windows software

Not true. SQL Server 2017+ has Docker images that work on macOS and it's available w/o Docker for a couple Linux distros.

https://www.red-gate.com/simple-talk/sysadmin/containerization/creating-your-first-sql-server-docker-container-in-macos/",1.0
g447fci,imyr6p,You can always run a MS SQL server on azure or AWS if you need one,2.0
g44ndwa,imyr6p,Look into running windows on a VM or using a cloud based windows instance over RDP.,2.0
g449t78,imyr6p,If you want to get a pc just for sql.  You can literally get the shittiest one possible and it’ll work fine. But you should be able to run most types of sql on Mac except maybe ms sql server. I’d honestly ask the instructor if you hade his contact info,1.0
g44tuu4,imyr6p,"&gt; except maybe ms sql server

SQL Server has worked in Linux &amp; macOS since 2017 thanks to a port and Docker containers.

https://www.red-gate.com/simple-talk/sysadmin/containerization/creating-your-first-sql-server-docker-container-in-macos/",2.0
g45back,imyr6p,I’ve always been a pc guy so I wasn’t sure either way thanks for the heads up!,1.0
g45xf6o,imyr6p,Just buy an instance on azure.,1.0
g468fc7,imyr6p,"If your sole purpose is to learn sql you can use sql lite on your Mac.

https://www.sqlite.org/download.html

There are maç packages for it and it is very lightweight.

Just install it I believe it comes with an example database as well.

There are maybe a couple of syntax differences in syntax but nothing to major.",1.0
g49ckdp,imyr6p,SQLite is most probably already available on a Mac,1.0
g49fsyg,imyr6p,Could be I don't use a kiddy laptop 😜,1.0
g49cicf,imyr6p,[Postgres](https://github.com/okbob/pspg) runs quite well even on low end hardware. It probably also is probably the DBMS that is the closest to the SQL standard.,1.0
g42idaz,imq2ol,"You've outgrown your current role. Good luck on this new opportunity, I hope you get it and it works out well!

Regarding your workload, yea in some cases you can present something and say like, ""Hey, here's something we could be doing..."" and that might work... but in general the business should be requesting data sets or analyses to support their needs and often that's all that is going to come down the pipeline to you. In many cases it's the opposite problem... people requesting things they 'absolutely need' and then they only look at them 2 or 3 times and forget about them... or a product manager going out of their way to offer new things, which the execs then reply, ""uuuh... sure"" and then you're forced to build something nobody was asking for. So I'd say don't sweat it, use your downtime productively, and hopefully you get a new role... if that doesn't pan out, then explain the situation to your manager in the most positive way you can and see if you can get additional responsibilities, or learn to add things to your workflow like CICD/DevOps or something -- stuff that may not add functionality for your customers, but which will add to your toolset that you want to use in future roles.",20.0
g42584i,imq2ol,"Usually a 3.9/5 with no actual complaints just means they doesn’t want to give you a raise or promotion and they knows a 4 or higher on reviews will make you want one.

If neither you nor your boss know of anything else that needs to be reported anything else will just distract from what matters. 

 Sounds like a standard data analyst job. It may be different at big tech companies though.",27.0
g42jttc,imq2ol,"It's entirely possible that your boss doesn't understand what you do or how you do it.

Sometimes management comes from people who've done analytics for a while, and they know what it's about. Sometimes they come from the business side of whichever industry you're in, and have used the analytics and know how the data relates to the world, but not really be up to speed on how to actually do the analytics.

In the 2nd case you can sometimes find yourself with the autonomy to implement new ideas, but the flipside is you often won't get credit exactly because the situation is not understood.",11.0
g42toa2,imq2ol,"Data analyst responsibilities vary so wildly it starts to come down to what you actually want to do.

Some jobs will want you to churn out reporting, which, as you and most others tend to, ends up being automated to some extent and you end up with a lot of down time. There are jobs where you are in a big enough company and the work is there for this to be quite a variable and interesting role. Personally, this is a large part of what I do, creating ETL processes and some actual report implementation here and there. 

Another responsibility is insight in some roles, what does the data actually say? Conducting your own analysis to answer a question which can solve a problem or explain something. 

Something to highlight though is sometimes you need to hunt for work, look for people who are manually creating things, go and speak to people and put yourself out there. Automate their stuff and get yourself known. That's how you get to those higher ratings (sometimes*). Demonstrate your worth and don't fade into the background. Build your own reputation and don't wait to be handed tasks, always think of the bigger picture. 

(*) if you are having a meaningful conversation this sort of stuff should be brought up under things you can improve and this isn't an attack, this is honestly how I would look at things if I were in their shoes, a lot of the time you need to look for opportunities to go above what is given to you to do as a 'task'. Sounds like the person doing your review doesn't understand your role though as they should really give you a reason why you don't have a 5. There is a lot to be gained from being managed well by someone and moving roles sounds like a good opportunity to find that better management, make sure you ask these sorts of questions at your interview. ""what will my personal development be like"", ""will I have regular 1-2-1s, reviews?"". I had crap managment for 5 years and in 8 months of good management I have developed so much it is terrifying. Move jobs. 

Good luck with your interview, tech companies tend to have more opportunities and things to get your teeth into.",6.0
g42ps3j,imq2ol,"Hi there. I hope you can receive my messages. I'm currently working at a bank and trying to make my transition to DA role. 

Is there any advice or any information that I should aware of?

At the moment, I'm trying to learn SQL and a bit of PowerBI. Do you think I should learn anything else besides those two?

Appreciate if you can give me any advice. Cheers.",4.0
g43y6mi,imq2ol,I’d do python instead of power bi unless the company your trying to work at already uses power bi,2.0
g43zold,imq2ol,"Hi there. Thanks so much for the feedback.

To the best of my knowledge, I thought that Python and Power BI are two different things. Python is a language while power BI is like a data visualization tool you use to visualize the data you extract from the database.

So is there any reason you'd think that I should learn python instead of power bi.

Again, I'm truly appreciate any feedback or comment.",2.0
g440m3y,imq2ol,"Yeah your completely right about the differences between the two.

The main reason I’d do python over power bi is because unless other people in the company want to use power bi it will be kind of useless. If the company is using/wants to use Power Bi it’s great though. Plus it’ll teach you to use power query in excel which is really useful.

Python has this really neat library called pandas that makes it really easy to read a sql query (literally just pd.read_sql_query() )do an analysis and then send the output to an excel file (.to_excel() )And since most companies already use excel so it’ll be familiar to anyone you give you’re analysis. And if you’re working with accountants they’re gonna want to work with it in excel no matter what most of the time.",2.0
g446ndw,imq2ol,"you can do these things easily with powerbi as well btw. But powerbi also typically executes queries that run inside their database/warehouse/whatever, so if there's rules that forbid him to download the dataset to his computer or the dataset is simply too large to take out of the database before analyzing, it makes more sense for him to just submit the queries through a tool like powerbi.

That being said, he should probably just use what most of the other people at his job use and what'll make him most productive there. If that's python, then that'll be the best to use, most likely.",2.0
g447dqx,imq2ol,"I had never used powerbi to export a dataset but that’d be pretty cool especially since a non technical person could easily add a few filters and get the data set they want.

I definitely agree that if they’re using powerbi learning it is the move. And if they have any tools they use other than powerbi and python I’d learn those first (unless it’s access). But if they aren’t using any of them I’d still recommend python for any analyst.",1.0
g4576jq,imq2ol,"I see. Absolutely awesome to have some kind of guidance at the beginning. 

Thank you so much.",2.0
g45gzwu,imq2ol,No prob man dm me if you have any more questions.,1.0
g43bgku,imq2ol,"Hugely depends on the company.

In some companies an analyst is sort of a demi-god that is a hardcore SQL developer who has tons of 'power' within the company relative to asserting requirements, and providing answers to the C suite executives.

You are basically the right hand of the top level executives, and your name is enough to get you into any meeting.",4.0
g44mcbm,imq2ol,I'm kinda that however it feels like there's a ceiling above me,1.0
g451jhs,imq2ol,find another job. this is what my life was like until i became an etl developer. most data analyst roles are a dead end but they are good jobs for people fresh out of school to get some professional work experience,2.0
g4cnrjg,imq2ol,Thank you all for the responses. They were all very helpful. Onto trying to prepare for my interview for the next couple days.,2.0
g42eumm,imq2ol,This was was last job to a tee. Sounds like you’re being underutilized and they don’t have room for you to grow. It happens. Continue to teach yourself new technologies and apply for jobs and you’ll find something. Good luck and use recruiters if you aren’t already.,2.0
g442xnu,imq2ol,"Sounds pretty standard. We have a similar scenario, reports that we've built and automated that we now just tweak and maintain. 

We also have quite a lot of ad hoc requests too though which keep us going. Depending on the size of the company though you might not get that many, it also depends on the culture of the company, some places just aren't that data focused. 

In your position I think I'd be doing one of three things

1. Try to improve the current reports - either make them prettier, or easier to use, or more user friendly or including more information.
2. Look for people who don't currently use data in their workflow and find out if there's a way you can help them (by building new reports or a one off project)
3. Look for a new job somewhere busier",1.0
g42epqw,imq2ol,[deleted],-4.0
g42ihdn,imq2ol,It's tagged as Discussion; I think the topic is appropriate for someone looking for guidance on their career path.,4.0
g4173wv,immzus,"When you join on a key you define a condition for entries to get into the result. When you use several keys you establish more conditions, so it is normal, that fewer entries suit them and therefore get into the result.",1.0
g4198mq,immzus,"So take 2 tables

Each has 10 rows

The cartesian product is 10x10   = 100 rows

Now filter those 100 rows by your join predicate

The remaining rows are the result of the join

Now imagine both tables had a primary key, from 1..10

You join on those keys. Each row in each table has an exact match with the other.
The result has 10 rows.

Now imagine the rows in the first table have 1..10, the second 5..15

Only 5 rows match so an inner join gives you 5 rows. 

See, there are no rules except the ones you  write into your join predicate.",1.0
g41ccyt,immzus,"for example.. say I have two tables. A policy ""pol123"" that exists in both tables.  From table1 I need to get the account\_id from table 2.  Just for the sake of agrument, I have to do a left join from table1 to table2.  I know its obvious I'd join on \`POLICY\_ID\`.

But can i also join on \`ACCOUNT\_ID\`? why or why not?

`table1`

|POLICY\_ID|ACCOUNT\_ID|
|:-|:-|
|pol123|blank|

&amp;#x200B;

`table2`

|POLICY\_ID|ACCOUNT\_ID||
|:-|:-|:-|
|pol123|45354||",1.0
g41dq88,immzus,"Yes you absolutely can

Whether it is useful or not depends on your database model",1.0
g41u5h7,immzus,"To rephrase u/phunkygeeza's answer, most of the time you should be joining on a reference from one table (or a dataset) to a key or a grain of another.

In normal circumstances this would be a join using a foreign key to a primary key. 

Keys can have more than one column in them (and they don't necessarily have ""_id"" in the name either).

Any time you are NOT doing the above, you might end up with ""duplicates"".",1.0
g42l5pb,immzus,"Yes, it *potentially* will narrow the results.  

You can think of joins as additional predicates in the WHERE clause -- the more you specify, the more you potentially limit results.",1.0
g43eh8q,immzus,"because when you do a join you're actually saying

Select * from table t
Join othertable ot on ot.id = t.id

give me all the rows from table and all the rows from othertable that have a matching id.

If I don't have a match, the row isn't returned.  If I have more than one match, than duplicate rows (on the table side) are returned along with their othertable data.",1.0
g40zmyn,iml28g,"&gt;How do the YouTube's server finish the comparison work in less than one second, and response with information of the video.

Because they're not querying a table(s) with billions of records with a traditional relational database system, they're using a caching server (https://memcached.org/) to accomplish these types of lookups which are VERY fast and VERY efficient at what they do.",6.0
g419yws,iml28g,"Edit: assuming you're asking about RDBMS, there are many other engines that may work differently

The basics are the same whatever size the database. You find rows by looking up values in indexes.

Larger databases tend to make large tables into ""shards"" or partitions. First the lookup is directed to the right partition, that might be say 1m out of 1b rows. Then the index is used to find the row you are looking for.

This is very over simplified but the point is that large databases really aren't that different.",2.0
g42cbn1,iml28g,"Youtube uses some sort of RNN.

You are not receiving the 1 billion results. Nor are you actually searching for them.


Thought experiment:

What if.. you had a table for every word known to man. And in those tables are video ids.

Instead of the search being a where clause, its more of a from clause. Now let's split the search into an intersect (like a union)...

Search :TV Repair DIY

Select from tv

Intersect 

Select from repair

Intersect 

Select from diy



Even then you are looking through millions of records.

Eventually the RNN picks up on the fact that TV Repair DIY, or just TV Repair is a common phrase. So it decides to create a new shard. TV Repair.


The search now becomes:

Is there a shard for tv repair diy? No.

Tv repair? Yes

Tv diy?  Yes

Select * from Tv repair 

Intersect 

Select * from tv diy


Eventually.. a shard is created for YOU.


This is what you like, based on your time on YouTube, based on the stuff you have googled, based on your shopping lists and web browser cache.

Edit: now think about each of those records in each table... those are actually pages. Maybe 20 videos. Easy to render a block.",2.0
g40u40a,iml28g,"Easy the video has a numeric ID. A large table properly indexed isn't an issue. The largest interger MySQL can use is  2\^63. Or  9,223,372,036,854,775,808. I don't think youtube has that may videos. Not sure how they do it. But I would break them up into tables. So each video would have an id number and a group number.",0.0
g40u2lr,iml28g,Indexes are your friend.,-2.0
g41021u,imks60,"No. Users should never be able to directly execute queries anyway and segregating databases like this will be a pain backup wise. There are scenarios where this approach is handy, but this is not it. Kiss (the acronym) :)

Create a new users table and put the users in there. And be sure to store the passwords securely. Eg not plain text. Google that (hint: salt+hashes)",2.0
g40cjex,imj7y0,"if you're confused:

he meant to type `select * from foo` but accidentally typed `select 8 from foo` cuz it's the same key

I do this all the time too",7.0
g44cq9q,imj7y0,I does happen sometimes,1.0
g47mjsk,imj7y0,"I usually type reserved words capitalized, so I am usually holding the shift key when I am hitting the 8 on the keyboard. So, it does not usually happen to me.

E.g.

    SELECT * FROM foo;",1.0
g41s4ol,imj7dp,Maybe Apex SQL. https://solutioncenter.apexsql.com/hands-free-no-coding-sql-server-database-replication-of-a-reporting-database/,1.0
g407ngr,imiqs8,"I have, and I actually prefer it versus trying to convince them of your skill level. 

I recommend starting a hackerrank account ahead of time and doing some challenges on your own first to scratch the rust off if needed. Depending on the job requirements, the complexity of what they're looking for can vary, but BI roles typically require a lot of grouping and aggregation, if I had to think of something to focus on. Whatever the case, just know the job test will probably be timed (I had 30 minutes to solve 5 challenges).

Good luck!",20.0
g41cuzc,imiqs8,Thanks! Yeah I have 60 min. I was worried it’s going to be more advanced like window functions or something that I haven’t done in a long time,5.0
g41mm30,imiqs8,"Windowing functions are just miniature aggregations within your overall query, nothing to worry about there even with a little brushing up.",5.0
g422dxl,imiqs8,"Those aren't the useful window functions, IMX. It's the analytic and ranking window functions that are really useful. ROW_NUMBER(), RANK(), DENSE_RANK(), LEAD(), LAG(), and FIRST_VALUE().",3.0
g40csgx,imiqs8,They're usually relatively easy. I think they're just used as a filter by HR to catch any awful programmers that can't produce functional code. I had one for python the other day that asked me to build a function that returned all the prime numbers in an array. I got full points even though my naive solution was redundant and inefficient. HR only looked at the points I earned. Your results may vary but that was my experience.,9.0
g413qoh,imiqs8,"Seems to be a trend. I interviewed for an aerospace last year and they put human resource machine (the steam game) in the interview process. Bewildering.

hackerrank sql isn't anything special, except in how they accept 'correct' submissions. know your ASC vs DSC order bys and functions. Most of the questions are variations on a theme but are essentially selects with increasingly obtuse WHERE clauses.",6.0
g419sgm,imiqs8,"I hadn't heard of this before, but I'm amused that in the SQL section, almost every question has a 98% success rate....so like what is being ranked here other than ""this person somehow submitted an answer that isn't in the bottom 2%""?",7.0
g41bgki,imiqs8,Probably just checking that you know what you claim you know on a basic level?,3.0
g41f7ui,imiqs8,Not had an interview use them but have been revising my sql as I have been on a long break from it recently. It’s okay but some questions can be very poorly worded. The discussion tab also lets you read other submissions and work out different approaches (although it’s why there’s a 99% pass rate on the problems),2.0
g3zhc2z,imgdnf,I think you need this: [https://www.w3schools.com/sql/sql\_case.asp](https://www.w3schools.com/sql/sql_case.asp),2.0
g3zj7sf,imgdnf,Agree. Case .... when .... statement should do the trick. Just add multiple clauses within each statement.,0.0
g409ibb,imgdnf,"not sure this is the right direction...

how does a CASE statement look across multiple rows to see if there's more than one row?",1.0
g3zvdsq,imgdnf,"&gt; I am trying for a query to show me the case.caseid where a client.clientid has more than one case.caseid 

that doesn't make sense... if there is more than one `case.caseid`, which one do you want to show?

&gt; and has case.status = new.

also doesn't make sense... should all of the multiple `case.caseid`s have `case.status = new`, or only one of them?",1.0
g3zz62s,imgdnf,There is only one case.caseid per client id where the status is new. I want to show that case id if the client has more than one case id,1.0
g408nmj,imgdnf,"    SELECT mult.clientid 
         , mult.case_count
         , case.caseid AS new_case
      FROM ( SELECT clientid
                  , COUNT(*) AS case_count
               FROM case
             GROUP
                 BY clientid
             HAVING COUNT(*) &gt; 1
           ) AS mult
    INNER
      JOIN case
        ON case.clientid = mult.clientid       
       AND case.status = 'new'",1.0
g3zepwz,img20i,In your second query you've written AND where you should have written WHERE,10.0
g40aoxg,img20i,That should be the auto reply by a bot for all questions with a left join problem. Most likely this will be the answer.,2.0
g3zf2bf,img20i,Yes! You are right.,1.0
g40lb7l,img20i,"Even with WHERE i think it wont work as the null is not real values in table, but the absence of value matched, you need to use something like WITH cte",-1.0
g40t5y9,img20i,"NULL is indeed not a real value, and SQL interpret Null as unknown. But i dont see why you cant exclude null values in the where clause. 

... WHERE xxx IS NOT NULL is just like asking SQL give me all records the are not unknown",2.0
g4tbkmv,img20i,"These Nulls is the representation of the missing values in the results of the joined tables but not values in the tables, answer this please, is the WHERE CONDITION applies on the result of the query or the values of the tables?!, correct me if i am wrong",1.0
g3zfpmc,img20i,"Using Henry as an example, you're asking the engine to join to rows in Orders where CustomerId is both 2 and NULL - it's an impossible condition to fulfil.

What I think you meant is:

`SELECT` [`c.Name`](https://c.Name) `AS Customer`  
`FROM Customers c`  
`LEFT JOIN Orders o ON` [`c.Id`](https://c.Id) `= o.CustomerId`  
`WHERE o.CustomerId IS NULL`

You could express the same thing using [NOT EXISTS](https://dev.mysql.com/doc/refman/8.0/en/exists-and-not-exists-subqueries.html).",3.0
g41cyd0,img20i,"    | Col1 | Col2 | Col3 |
    | :--- | :--- | :--- |
    | 1 | Joe | null |
    | 2 | Henry | null |
    | 3 | Sam | null |
    | 4 | Max | null |

Becomes:

| Col1 | Col2 | Col3 |
| :--- | :--- | :--- |
| 1 | Joe | null |
| 2 | Henry | null |
| 3 | Sam | null |
| 4 | Max | null |",2.0
g404qnu,img20i,"Here's another method, retaining the option to pull all records, including and/or filtered on order counts:

    -- CTE: Customer Table (temp data for this exercise)
    ;WITH Customer (Id, Name) 
    AS (SELECT 1, 'Joe'
    UNION SELECT 2, 'Henry'
    UNION SELECT 3, 'Sam'
    UNION SELECT 4, 'Max'
    )
    
    -- CTE: Orders Table (temp data for this exercise)
    , Orders (Id, CustomerId)
    AS (SELECT 1, 3
    UNION SELECT 2, 1
    )
    
    -- YOUR QUERY: Counting Orders that exist in left join, grouped by Customer
    SELECT Customer.Name
    ,  OrderCount = COUNT(Orders.Id)
    FROM Customer
    LEFT JOIN  Orders
    ON Customer.Id = Orders.CustomerId
    GROUP BY Customer.Name
    
    -- OPTIONAL: Filter on how many orders exist (remove to show all)
    HAVING COUNT(Orders.Id) = 0",1.0
g3ze4sy,imfvvd,"I just don't think it accepts evaluations like that - it's expecting a constant, which (in the context of the runtime evaluation of this query) a variable is. The actual reason will be something to do with how query evaluation is implemented.

Not big into MySQL so I dunno if the below are the correct docs, but they confirm that's the case for LIMIT so I imagine the same applies to OFFSET and it's been omitted from the docs/I've missed it.

[https://dev.mysql.com/doc/refman/8.0/en/select.html](https://dev.mysql.com/doc/refman/8.0/en/select.html)",1.0
g406sfv,imfvvd,"    select distinct salary, ranking from 
        (select salary, dense_rank over (partition by salary order by salary desc) as ranking from employee)
    where ranking = @N",1.0
g40gb2j,imfapu,The fake data part sounds good,1.0
g412eox,imfapu,There is currently a breaking bug with the interface so that is removed but it should be fixed and out near the beginning of the week.,1.0
g3zsua6,imfapu,This looks so cool! Definitely going to check it out.,0.0
g3zczhn,imdnff,I did that. Can you post more specific question?,1.0
g3ytqap,imd8qf,Sqlbolt,2.0
g3ytxhw,imd8qf,It cant load my CSV file,1.0
g3yu5l6,imd8qf,U have a CSV?..then u will need SQL developer.use the import data option.,1.0
g3yv5eh,imd8qf,Where to get that?,0.0
g409i31,imd8qf,"https://www.microsoft.com/en-us/sql-server/sql-server-downloads

It's free. If you are looking to practice mssql in an environment you'd find at a job... 100% recommend this route.",1.0
g40eylq,imd8qf,"Thanks, I will try it.",1.0
g3z22jo,imd8qf,Try ms access in lieu of other big databases.,1.0
g3zw1oh,imd8qf,"Sql server express is free, powerful, easy to install, uses full sql, and isn't deprecated. A much better choice.",1.0
g3ynwsp,imbzer,"When you do your insert do insert into explicitly state the 11 columns eg insert into tablename (column1, column2, etc...) up to column 11 then in the select only select the 11 columns you want then the 12th column should use your default value

Syntax should be

INSERT INTO [tablename] (columns separated by ,) SELECT columns separated by , FROM [tablename] WHERE if required",2.0
g3yr8t9,imbzer,"Before I saw this reply i found its possible to use

     INSERT INTO [tablenameWith12colums] SELECT *,0  FROM [tablenameWith11Columns]  

and it works just fine. But i believe your solution should work as well :D",1.0
g3z36dz,imbzer,Yeah in the way you have done it you are overriding the default value with the 0 value which in this case is fine as the default value is 0 however if you had any other value in your select it would overwrite the default value best practice if you want the default value is to not add the column in the update statement to be on the safe side pleased you have managed to achieve what you needed 👍,1.0
g3ygu44,im9fl0,You're going to have a real hard time with SQL. Learn to think in code.,2.0
g3ykedn,im9fl0,"I can't help you visualize it, but I guess it takes a lot of time to execute, so you can rewrite it using EverSQL",2.0
g3xqooz,im70ma,With the design or the implementation?,3.0
g3xrwzb,im70ma,i messaged you,-2.0
g3xsamm,im70ma,I’m not doing your assignment for you,13.0
g455dbx,im70ma,Lool,1.0
g3xu63w,im70ma,i dont want you too lol,0.0
g3xt2qk,im70ma,Just explain to us what you need to do and maybe we can help.,2.0
g3xu8ig,im70ma,"so i need to create tables for different aspects of a business, i just dont know where to start and which tables should be related",1.0
g3xuq75,im70ma,"I always start with the biggest one, or the biggest object. If you were making a database about a grocery store, you may want a table for customers, and a table for products.

Then you can make a table that includes information from those two tables...like sales. The sales table could list the customer, date and time. Then you can have a forth table called purchases...that table could list every item that has been purchased, and which SALE it was apart of.

So which business are you going to do ?",2.0
g3zg6ym,im70ma,[https://gyazo.com/524f6b2fb9d5d461f3581b0d506990cd](https://gyazo.com/524f6b2fb9d5d461f3581b0d506990cd),1.0
g3zgkrh,im70ma,"this is the flowchart ive made so far 

[https://gyazo.com/ecfbc8b979c1513827f7cf7fb8904bd9](https://gyazo.com/ecfbc8b979c1513827f7cf7fb8904bd9)",1.0
g40aeb0,im70ma,"Makes sense to me, now you need to reference primary and foreign keys",1.0
g40b1sg,im70ma,"so for customer table the primary key will be their phone number

for the order table the primary key will be their order number

for the employee table the primary key will be their employee number

for the product table the primary key will be the product ID number

does this make sense?",1.0
g46y050,im70ma,I would give each customer a number other than their phone number. Just a number that increments by 1 for each new customer. What if customers shared a phone? Then it would not work as a primary id,1.0
g3y326y,im70ma,"Group your business domain into objects - customers, orders, products, etc. Then define the relationships. An order can have many products, but an order can belong to only one customer for example. Draw it out and design your tables based on business rules.",1.0
g3xpidi,im6uob,/u/AbstractSqlEngineer has a very indepth tutorial on how to make stuff. ive been writing sql for while and im learning a ton of stuff fromm his vids.,6.0
g3xz7wb,im6uob,Thank you for the shoutout!,6.0
g3xpvpy,im6uob,Thanks for the info. Will def take a look at it!,2.0
g3y1uqx,im6uob,He is the biggest boss we've seen this far.,2.0
g3xygdw,im6uob,"To make a proper model, look up the topic 'entity relationship diagram'. MySql workbench have the built in. I once used this one before [https://www.lucidchart.com/pages/er-diagrams](https://www.lucidchart.com/pages/er-diagrams)  


I believe it's database (also any DBMS name) developer that is designing the databases.",3.0
g3ycr0q,im6uob,"Why would you separate credentials and user info table? Also, the top level comments table.",3.0
g3znrhx,im6uob,"I split user info even farther

Emails are modeled as contact. While usernames are just names. Passwords are hashed data that can be encrypted and salted. 

This allows you to store more emails/phone numbers, link several usernames to one person (across several applications), and store SSNs (encrypted) without changing the table columns.

Abstraction allows the business say ""we are doing this now"" and you dont have to do anything about it except say ""already planned for that, let's test it"". 

Need I talk about the reputation you gain from always being prepared? The lack of time from feature to release?",3.0
g4390l5,im6uob,"Well, the name of the table is ""credentials"". Usually most of the systems let you register and login only with one email and they use your email and password to authenticate you. So there is usually no point to link multiple emails to one name. It only adds unnecessary complexity to queries.",1.0
g3y6r36,im6uob,"Actually, yes, good effort!  Get that sucka into a free SQL server and start toying with it.",2.0
g3y9iss,im6uob,For the comment and reply IDs is there a common way to come up with unique keys? All I did there was concat the primary keys of the corresponding table to create the unique reply and comment IDs. For the user ID I’m just incrementing. Is this the norm or is there a better way to do it?,2.0
g3ycc0f,im6uob,"Don't worry about that, any contemporary database will handle primary and foreign keys natively as long as you CREATE TABLE in a way that will facilitate and have the proper settings on the database.  Keep learning and reading.  If at all possible, try to take CPT-242 and CPT-202 at a local technical college (many are offering fully online + free tuition) - personally helped me a TON even after I was already programming on-and-off for years!  Literally everyone should take CPT-242 in particular **regardless of career** if they are in any business setting in my humble opinion - it is THAT insightful.",1.0
g3zimh8,im6uob,"No need to separate user info and credentials (from a modelling perspective).

No need to separate top level and reply comments (same applies). They're all comments, and we can capture their nesting with a column. Just key them to a parent comment (nullable to allow for top level comments) and you get n levels of nesting.

&amp;#x200B;

&gt; Lastly, what are the names of people with roles that just create data structures? 

Data Modeller, I think.",1.0
g3zp10y,im6uob,"If you create a Name structure (let's talk small and say, name and nametype), you can house pablo Picasso's full name. First name, 24 middle names and a last name, and also store a reddit user name, your apps username, twitter user name, etc.

Like the name structure, you can create an Event, Contact, Text, Numeric, Hashed, and Flag structures too. I call these shared attributes. Because Post and Person share these attributes. 

A post can have text, and an event type that represents when it was created, so can a person. A DOB event, a marriage event, a second marriage event.. etc etc. An event is just naming a date. 

With contact (email, phone, etc) you can house secondary emails and use heuristic engines to match and merge people.

Text, numeric, flag... those can store your post Name, post Content, upvotes, isComment, isPost,  isText, isPicture type info.

With person and post, and these shared attributes... youare almost ready to create reddit.

Edit: also with this type of setup... you are not adding columns when you want to add features. Its just rows of data. And if you can abstract your code (difficult at first, it gets super easy after a while) you can create new features by just adding rows to a table, and possibly get away without adding any more sql.

Elricsims.com/system/   check subjects for the current subjects I use.

Domain shape: https://youtu.be/Hg7pxhjL7Ls

Shared attribute: https://youtu.be/KmESfvWnPKE

Couple Reddit bot vids in pure sql: https://youtu.be/dCILNJat8Iw",1.0
g42an3i,im6uob,"If you're looking for an alternative written guide to SQL, check out ""Practical SQL"" from No Starch Press: https://nostarch.com/practicalSQL

It has a bent toward data journalism, but a good section on table design. I find all the No Starch books make the subject matter easy to grok.

Good luck!",1.0
g3wteb1,im1gh9,"It's a bit difficult to follow your explanation. It would be easier if you could clearly state the table(s) you have, only including the minimal set of relevant columns. Then maybe provide some sample data that lives in the table(s). Then maybe show us what the final output should like.",1.0
g3wwidn,im1gh9,"That's honestly kind of what I've done.

The final query has a lot of complicated joins to about 10 different tables (to get to items I need, our database is massive) that I felt would bloat my post up with irrelevant info. The ask of my client is more complicated than just pulling these codes, he wants denials by an insurance, etc. but again that's not really relevant to the ask.

&amp;nbsp;

Here's an example table of the items above that you would see:

ENCNTR_ID | ACCT_NBR | CPT
-|-|-
123 | **A468** | **12345**
124 | A468 | 70000
125 | **A468** | **90003**
126 | D427 | 39999
127 | E528 | **12345**
128 | Z438 | **12345**
129 | Z438 | 50001
130 | G123 | **90001**

&amp;nbsp;

I've bolded the matches to make them easier to see (cuz numbers suck to look at). In this case only the A468 ACCT_NBR should pull in with what I want it to do, even though E528, Z438, and G123 all have a 'matching' CPT, they don't have BOTH the 12345 AND a 90000-90010. I can't think of a way to reliably get SQL to do that, because while the ACCT_NBR shows these, I can't use ACCT_NBR to join because as you can see it isn't unique in the table. But *because* ENCNTR_ID is unique in the table, I *also* can't use that to reliably check for whether both requirements are on the account, because both requirements are on the same data column. Meaning they'll show up on separate rows, which would also be a separate encounter ID.

* This issue is hard to word properly, I hope it's coming through.

My idea would be two subqueries that pull in the ACCT_NBR, one looks at the 12345 CPT, and the other looks at the others, and then INNER JOIN them together on the ACCT_NBR - where there's no match, the account didn't have both requirements. I just don't know some of the deeper mechanics of SQL and whether this will break something or add duplicates or create corrupted values or something.",1.0
g3xcklv,im1gh9,"I'm not going to guarantee speed of execution, but one of the logical ways to accomplish this is

    select a.acct_nbr from my_table a
     where exists (select null from my_table b
                    where b.acct_nbr = a.acct_nbr
                           and b.cpt = 12345)
      and exists (select null from my_table c
                    where c.acct_nbr = a.acct_nbr
                           and c.cpt = between 90000 and 90010) ;

The above query should return all ""account number\[s\] that ha\[ve\] multiple CPTs, one of which has to be 12345, and *at least one more* that is any value between 90000 and 90010. """,1.0
g3xdt2k,im1gh9,"My apologies if some of this syntax doesn't work for Oracle (this is T-SQL), but I'm sure it's similar.  All you need to do is get the account numbers with both criteria via INTERSECT and then join back in.   I'm sure there are many other ways to do it, but this is very simple.

    
    
    
    DROP TABLE IF EXISTS #Claims;
    GO
    
    CREATE TABLE #Claims (
    ENCNTR_ID int, ACCT_NBR varchar(4), CPT varchar(5)
    );
    INSERT INTO #Claims (ENCNTR_ID, ACCT_NBR, CPT)
    VALUES 
    	(123, 'A468', '12345'),
    	(124, 'A468', '70000'),
    	(125, 'A468', '90003'),
    	(126, 'D427', '39999'),
    	(127, 'E528', '12345'),
    	(128, 'Z438', '12345'),
    	(129, 'Z438', '50001'),
    	(130, 'G123', '90001');
    
    --90000-90010 and 12345
    
    --SELECT * FROM #Claims;
    
    ;WITH Both AS (
    SELECT ACCT_NBR
    FROM #Claims
    WHERE CPT = '12345'
    INTERSECT
    SELECT ACCT_NBR
    FROM #Claims
    WHERE CPT LIKE '900[01][0-9]'
    )
    SELECT c.ENCNTR_ID
    	, c.ACCT_NBR
    	, c.CPT
    	, CASE WHEN b.ACCT_NBR IS NOT NULL THEN 1 ELSE 0 END AS BothCodesFlag
    FROM #Claims c
    LEFT JOIN Both b ON b.ACCT_NBR = c.ACCT_NBR;",1.0
g3wtjqi,im1gh9,"    SELECT acct_nbr
         , encntr_id
      FROM [table]
     WHERE cpt IN (12345, 90000, 90001, ... , 90010)
    GROUP
        BY acct_nbr
         , encntr_id
    HAVING COUNT(CASE WHEN cpt = 12345
                      THEN 'ok1'
                      ELSE NULL END) &gt; 0
       AND COUNT(CASE WHEN cpt IN (90000, 90001, ... , 90010)
                      THEN 'ok2'
                      ELSE NULL END) &gt; 0",0.0
g3wus3a,im1gh9,"Would this be a subquery?

If not a subquery, the final product has something like 20 items in the select statement, and I'm not sure how that interacts with a GROUP BY / HAVING (especially with a nested COUNT with another nested CASE WHEN function, haha).

GROUP BY w/ a math function needs *every* item in the SELECT statement also included in the GROUP BY, correct?

Regardless, this is a really cool set of nested functions that I never would have thought to use. Thank you!",1.0
g3wvpuy,im1gh9,"&gt; Would this be a subquery?

i guess it would be, yes

this query gives you the `acct_nbr, encntr_id` combinations that meet both conditions specified in the HAVING clauses

if you need to see additional row-level details, then join this back to your table

    SELECT t.acct_nbr
         , t.encntr_id 
         , t.cpt
         , t.othercolumns
      FROM (
           /* my query goes here */
           ) AS d
    INNER
      JOIN [table] AS t
        ON t.acct_nbr  = d.acct_nbr  
       AND t.encntr_id = d.encntr_id",1.0
g3wz4n7,im1gh9,"SQL can do so many things that I never realized. I don't think I've ever subqueried in a FROM statement before, I've always done ""WITH"" to subquery. But it makes sense.

Oh wait...your subquery is grouping account and encounter together, right? Encounter_ID is wholly unique on the table, there are no duplicated Encounter_IDs. Each CPT would be a separate row with a separate Enc_id but not a separate account number. So I don't think I can include the encounter ID, right?

Can I just group by the account number? So, like, in layman's terms: ""Hey SQL, I want you to find every account that has both a 12345 and any of [the CPT range].""",1.0
g3x312u,im1gh9,"&gt; Can I just group by the account number? 

sure!

and then the join will be on only one column",1.0
g3wlzwi,ilywyu,"SQL is a programming language, Sequel is an application that writes the SQL code for you. If you don’t know the SQL language, you’re not ready for a job yet that lists it on the JD. Google around for some introductory courses and start practicing, then see if you can apply it at work.",6.0
g3wt75p,ilywyu,Why can't you just write SQL where you are now? Best way to learn is on the job,3.0
g3xh0md,ilywyu,"Think of SQL as a set of standards and constraints. Each particular database language (MySQL, PostgreSQL) implements SQL in mostly the same way, but each have their own expertise and ideosyncracies.",1.0
g3xie8q,ilywyu,Okay thank you!,1.0
g3xf6cu,ilywyu,"With respect, outside of specialized data viz suite expertise, you are not a data **engineer** if you are using drag-drop tools to create reports from existing data sets, rather you are a data warehouse end user / analyst.  You need to get out of your drag-drop comfort zone and start attacking your databases directly within the most pertinent query-writing medium - this alone will likely add 30%+ to your potential salary range from where you are right now.  Get to learning!",-1.0
g3xihdi,ilywyu,Well I think it's time to throw out my degree because I drag and drop a program that is a small part my full time job :(,2.0
g3xoxes,ilywyu,"Everyone starts somewhere man. :) you’re good, it was a good question to ask, never used the sequel tool myself but ages ago I loved MS sql server because it would generate sql and I learned from that how to get better and better at it.",2.0
g3xj620,ilywyu,"Nah, but it is time to get some thicker skin and realize that you came here to ask for help, not to talk back to those who try to provide it because they are not privy to information that you did not present.",-4.0
g3ynyp6,ilywyu,Talk back these nuts,0.0
g3unjdw,iluig4,"Is MATCHUP a keyword in PostreSQL I'm not familiar with?  Could you please show the result set you want?  

It looks to me like you just need another IN clause:

     SELECT   
         Text 
    FROM
           ShirtDB 
    WHERE
        Name IN ('Peter','Nathan','Simon')    
    AND    
    Color IN ('red','blue','green')",2.0
g3uoeor,iluig4,"Hey sorry, I meant to put in another line saying `MATCHUP` isn't actually a keyword but I wanted the functionality. I figured out how to do what I was asking and added the solution to my post. Thank you!",1.0
g3v15t3,iluig4,"I'm glad you got it working, but I still don't have the foggiest idea what you were trying to do.  What does it mean for a color to be applied to selected individuals?  What is your result set?  I have a strong feeling there are better solutions for your problem, but I don't understand your problem well enough to suggest them.",2.0
g3v7r8k,iluig4,"Sorry, I basically wanted to specify the color for each person, and select out that row.

It sounds like /u/truilus below gave an even better answer than my specified answer, using tuples.",1.0
g3vk3xg,iluig4,"Cool!  That is a neater solution, and one that I didn't know could be done.  That may well help me down the road.",2.0
g3v38h0,iluig4,"You can use tuples with the IN operator:

```sql
where (name, color) in ( ('Peter', 'red'), ('Nathan', 'blue'), ... )
```",2.0
g3v7s3v,iluig4,"Hey thanks, this is a much better solution. I'll update my post.",1.0
g3xenq4,iluig4,"&gt; I don't have the ability to create tables, so I can't create a table and do a join--do I have any recourse here?

http://sqlfiddle.com/",1.0
g3yymq4,iluig4,I prefer https://dbfiddle.uk SQLFiddle is extremely unreliable for me.,1.0
g3u2ju5,ils60c,"Formatted:

    SELECT DISTINCT page_id as recommended_page 
    FROM Likes 
    WHERE user_id IN 
        (    SELECT user2_id 
             FROM Friendship 
             WHERE user1_id=1 
         UNION 
             SELECT user1_id 
             FROM Friendship 
             WHERE user2_id=1
       ) 
        AND page_id NOT IN (SELECT page_id FROM likes WHERE user_id=1) 

When you say that you're swapping the UNION for OR IN, are you doing:

    SELECT DISTINCT page_id as recommended_page 
    FROM Likes 
    WHERE user_id IN 
        (
         SELECT user2_id 
         FROM Friendship 
         WHERE user1_id=1 
        )
        OR user_id IN
        ( 
         SELECT user1_id 
         FROM Friendship 
         WHERE user2_id=1
        ) 
        AND page_id NOT IN (SELECT page_id FROM likes WHERE user_id=1) 

If so, then the problem is that everything before and after the OR is evaluated separately. Add brackets around your two user_id INs so that they're evaluated together.",3.0
g3u2s2h,ils60c,Yes! I was doing exactly like your second query. Where should the brackets be?,1.0
g3u361x,ils60c,"At the start of the WHERE and at the end of the user_id IN OR user_id IN block. I've commented them below:

    SELECT DISTINCT page_id as recommended_page 
    FROM Likes 
    WHERE 
        ( --here
            user_id IN 
            (
             SELECT user2_id 
             FROM Friendship 
             WHERE user1_id=1 
            )
            OR user_id IN
            ( 
             SELECT user1_id 
             FROM Friendship 
             WHERE user2_id=1
            )
        ) --and here
        AND page_id NOT IN (SELECT page_id FROM likes WHERE user_id=1)",2.0
g3u2zzc,ils60c,"&gt;If so, then the problem is that everything before and after the OR is evaluated separately. Add brackets around your two user_id INs so that they're evaluated together.

I still dont understand how that is a problem? The user id should be in either of those?",1.0
g3u3nuv,ils60c,"Because without the additional brackets, this bit:

    WHERE user_id IN 
        (
         SELECT user2_id 
         FROM Friendship 
         WHERE user1_id=1 
        )

Gets evaluated separately to this bit:

        OR user_id IN
        ( 
         SELECT user1_id 
         FROM Friendship 
         WHERE user2_id=1
        ) 
        AND page_id NOT IN (SELECT page_id FROM likes WHERE user_id=1) 

Which means that that 

     AND page_id NOT IN ...

is only getting applied to the 2nd OR user_id IN.

Edit: Basically (A or B) and C is not the same as A or (B and C).",4.0
g3w9xe9,ils60c,"It's comparing the stuff on the ""left"" to the stuff on the ""right"":

&gt; THIS or THAT and THE OTHER

In this case it's checking for either ""THIS"" or ""THAT and the OTHER"".

By adding parentheses, you're forcing it to evaluate the groups the way you want it to:

&gt; (THIS or THAT) and THE OTHER",1.0
g3tvibt,ilq28j,"It sounds like you might want to set up a reporting database that mirrors your application database.

[This article](https://www.brentozar.com/archive/2019/06/4-ways-to-move-data-into-a-reporting-server/) gives a high level description of a few approaches.",21.0
g3tu750,ilq28j,Create a copy of the DB that restores from a backup. Then give them access to that new DB. Keeps you Prod environment safe.,11.0
g3txmz7,ilq28j,"I dont think itd be too crazy to do this along with limited prod access.

Give them full access in test, but allow reviewed/approved sql be created in prod as views or sprocs in a schema just for them. Set some requirements about runtime and coding standards for the review (though you might not have all the answers immediately). Puts a lot of responsibility on the reviewer and the business users might be impatient, but peer review is a thing.",4.0
g3tufe3,ilq28j,"I don't know much about Azure SQL (is it a fully managed service? Or kinda like RDS?) but what I'd likely do for other database solutions like postgres, is to create another database that's a read-only replica. Then the users can query this replica, and the data lag compared to the master instance will be pretty low.

I would not give users access to the prod database directly, there's too many potential risks IMO.

Either way, you'll definitely want to investigate how to lock the database down, so that your users can't do too much damage (even to the replica), so that probably means setting quotas, permissions etc.",8.0
g3uc2lz,ilq28j,"Most of these are saying to spin up a new DB, but it sounds like you won't get buy in for that. What I would recommend is making an AD group for read access to the necessary database. That way you can add and remove people as needed. When (not if) they start writing things that cause performance problems you'll then have the exact user's who need hands slapped VS. a new generic login that would cover it up.

If you don't use AD groups.... I'd go with a new general login.",6.0
g3x22in,ilq28j,"Don’t ever use generic accounts to be shared by users. It removes any chance at accountability for who’s doing what. If Bob in accounting is running a query that’s slowing things down for everyone else, you need to know it’s him so it can be addressed.",2.0
g3xc2lt,ilq28j,"I definitely don't want people to do that - I just interpreted the initial post as not having the resources or buy-in for building out another DB/ETL pipeline/backup+recovery strategy. It then comes down to AD group policy (highly preferred), single user management, or a single account. If it is truly for reports and not ad-hoc SQL then the single account would be a ""report"" account. Then hand slapping when it is used for ad-hoc development. You can still get that granularity by seeing the source of the SPID, but hopefully a group policy can solve the short term problem so that extra step isn't needed.",1.0
g3uint1,ilq28j,"Paying for extra processing power won't be enough when analysts start writing bad queries, and they WILL write bad queries.  Cache contention, memory pressure, raw IO throughput.  It hurts when someone does something bad, and depending on the job killing it can actually be worse.

Unfortunately this is the bane of the DBA - data analysts that don't know SQL as well as they think they do.  (""If you don't know how to use the query plan, there's a good chance you'll bog it."")

First, CYA.  On-Prem has the DAC - Dedicated Admin Console.  I don't know what the Azure equivalent is, but you need to set it up and not tell ANY of the analysts about is.  Keep it a secret - only people who will have to clean up the mess they make will even know about it, and it will ONLY ever be used to clean up messes.

A second server periodically updated from backups would be ideal, but it's also a bit of overhead and sooner or later someone will want realtime data.

u/Bakuwoman makes a great suggestion - use an AD group.  If the server slows down you can check who's doing what, punt them, and go after them with a ruler. (Use a plastic one though - the metal ones are more effective but they can also draw blood, which is generally frowned upon by management.)  I find it's also a lot easier to manage the permissions this way.

If possible, consider taking whatever people are doing in PBI, converting it into views (or stored procedures if necessary) and tuning the crap out of that.  Only expose the views, not the tables.  We have to do something similar with Izenda because when you give people access to build their own joins sooner or later they'll bog the server.  It's not a technology problem, or even really a people problem.  SQL's simplicity is deceptive.",6.0
g3ujt8f,ilq28j,I don't believe they even HAVE SQL knowledge. I believe they are planning to download prod table to their local pbi desktop lol. Or at best direct query. This Will not be pretty. Thanks a ton,2.0
g3uk3ei,ilq28j,"In that case, push them towards download, and if they complain it takes too long they can build queries after they do a bootcamp and Brent Ozar's beginner tuning classes.  :)",2.0
g3ulsf5,ilq28j,"But, doesn't the download slow down the server as well. If they are doing a select * from a big Transaction table",1.0
g3umho3,ilq28j,It can get a lot worse with a join.  You're starting to see why DBAs don't like reporting analysts in their stuff.  :),1.0
g3umqoy,ilq28j,"We have ETL pipelines for all our old projects. Since this is a new one, nothing is defined",1.0
g3umvgw,ilq28j,"Bogging a data warehouse generally doesn't mess things up that much... :)  If you can get them to keep the reports in there, it'll be a lot safer as they can only stall reports.",2.0
g3und1f,ilq28j,"Also, this does not seem like they want a DWH DB. My boss is just saying once this goes live, they will be building their own reports and we will no longer be involved. That is why they are asking for dB tables access 😂😂",2.0
g3uxrlb,ilq28j,"Dangit!

:(

Make sure they have read-only access and get the DAC (or equivalent) going.  You need to make sure you can un-stick the server, and if their session is readonly it shouldn't need to do a lengthy rollback.

Consider learning the basics of SQL tuning if you haven't already.  I started here and it was a massive boon:

https://www.brentozar.com/training/think-like-sql-server-engine/

Sooner or later someone will come up with a report that management likes and the engine hates.  Someone will have to make the engine like it.

Then maybe get a nice shiny 30cm ruler, placed prominently on your desk.  The plastic ones that bend a little are really good for this - you can get a nice sharp sting with no lasting effect beyond a few minutes.  (Boredom leads to interesting discoveries...)  When you're giving them access, tell them that ruler is how you're going to notify them that they've written a query that's impacting the application, and they'll know by the sting and redness on the back of their hands, usually shortly after their session with the server drops.  It'll have 'em good and paranoid if you DO ever have to boot a transaction.

Put a metal-and-cork one beside it, tell them that it's for repeat offenders, and which side they get will depend on your mood.",3.0
g3v0xpi,ilq28j,"Yes, Brent's classes are great. I took this and fundamentals of index tuning.",2.0
g3up3wu,ilq28j,Can i get some of whatever your boss is smoking? haha,2.0
g3ymmw9,ilq28j,Remember also that downloading tables to a local computer has a lot of GDPR/compliance implications. That's generally a big no-no,1.0
g3v3pr2,ilq28j,"Try building AAS model on top of Sql tables. You can modify the columns easily there, can set up relationships and aggregations will be done. On top of AAS model, you can setup the reports.
For real time data too. You can set up ADF pipelines to refresh data in SQL tables.",3.0
g3v7hy5,ilq28j,I like this idea. AAS doesn't have any cap on number refreshes per day no?,1.0
g3v7wvp,ilq28j,"Nope, not on the refreshes.",1.0
g3uta8t,ilq28j,"I'm more relaxed than a lot of other people here are. Yes, of course if you can get some sort of replication system going then you should use that but that isn't necessarily straight-forward depending on how much data you've got and the time available. 

Do you have a dev/test system? One option might be that they can write queries on that, and then when they're finished you can create views of their queries which are the only objects they have access to. If they're using these queries to drive reports then that should be fine. 

Either way it goes without saying that any access should be read-only and you should be able to audit who is running what and where (+1 to the AD group suggestion to make this easier to manage it). Whoever owns this system needs to be aware of the risks involved and they are the one to approve it or not. If the system does run slow what is the consequences? Do you lose millions of dollars? Do people die? Or is it just slightly annoying for some people? Ultimately, this is the organisation's system and they should be able to access it in the way they want, so long as they understand the risks. 

I've worked with dozens of business systems which had direct query access for a couple of power users and yes, we almost always eventually had the odd instance where queries would cause slowness in the system. It wasn't the end of the world though and we just handled those cases as they arose. In one case we had to remove someone's access because they simply wouldn't listen to feedback, but they soon left for unrelated reasons. There's an assumption that business users don't know SQL and that might be true in your organisation but it's not universally true.

edit: The other thing the system owner needs to understand is that if these users have access to the base tables that bypasses any permissions within the application (at least in terms of reading data).",2.0
g3u3yg8,ilq28j,"We have a similar situation and we have a second set of prod servers that are refreshed on the half hour via tlog files. I'm not sure how that'd work on azure as we host our own stuff but I'd imagine there are people out there that have done it.

It should be a hard no on prod access to more than a few users. Like a, I'll find a new job if you force me to give them prod access, no.

The minute one of them writes a left join on two massive tables and brings production down, it'll be your fault and you'll be the one fixing it.",1.0
g3uu5bj,ilq28j,What the other guy said. Create a reporting database.,1.0
g3uwbon,ilq28j,"DSS copy for everything besides last 24 hours, otherwise teach them to query using WITH(NOLOCK) table hint. It can result in a dirty read but will mitigate risk of application issues from long running querys",1.0
g3vhtfm,ilq28j,this is what happens when the ad hoc querying tool is not good enough. They ask for going directly to DB tables.,1.0
g3wdqxm,ilq28j,"Yes a dataset or fire up an Analysis Services instance. Decide on your refresh frequency though, if those users need the very latest data it won't be the best way forward.",1.0
g3u3i2j,ilq28j,"You should never hit an application database with a reporting tool. Spin up an additional database. Why do the need real time access to data? Typically business users can deal with a day lag in data. 

If they *hav*e  to have real time / near real time data, then you could use SQL Servers native replication functionality, or use another NRT replication tool.",1.0
g3tintk,ilky0q,You will need to look into the Postgres (server side) logfile for more details. There is nothing in the query that warrants an error like that.,2.0
g3tkgxs,ilky0q,"With Postgres I would probably write the query like this:

```sql
select t.id, t.recorddate, t.temperature
from (
  SELECT t1.*, 
         t1.temperature &gt; lag(t1.temperature) over (order by recorddate) as is_bigger_than_previous
  FROM weather t1
) t
where is_bigger_than_previous
```",2.0
g3v4rsa,ilky0q,"Truilus, thanks for taking the time to answer. 

Regarding your first point, I am a bit novice in most of this stuff and I think I might have some problems with the configuration or not sure how to really access the information, when I check where the logs are I get:

* show log\_destination &gt; stderr
* show logging\_collector &gt; on
* show log\_directory &gt; log
* show data\_directory &gt; /Library/PostgreSQL/12/data
* show log\_filename &gt; postgresql-%Y-%m-%d\_%H%M%S.log

I have been trying to access the logs in the data folder but I have not been able to access them even after getting sudo rights in my local computer. I have not been able to check what went wrong.

**Nevertheless, your suggestion using ""lag"" works perfectly.** 

Thanks",1.0
g3v8st1,ilky0q,"If you can connect with the Postgres superuser (usually `postgres`)  you can use: 

```
select pg_current_logfile();
```

To get the (relative) path to the currently used log file.

Given the settings you have shown, it should be at 

```none
/Library/PostgreSQL/12/data/log/postgresql-2020-09-.....log
```",2.0
g3vas1t,ilky0q,"Thanks, I accessed it but there is a lot of info I really do not understand, possibly it has to this with this message:

     Termination Reason:    Namespace CODESIGNING, Code 0x2

but don't worry, this already is over my head. I will continue with other exercises trying to find solutions like your ""lag"" option!",1.0
g3t13db,ilk81u,"Raghu Ramakrishnan's *Database Management Systems.* Some more recent developments will be missing, but the fundamentals are sound. I had to read this a few years ago, and ended up meeting Ramakrishnan during a pitch session at Microsoft last year. Nice guy.",1.0
g3rxv76,ildqaw,"This is basic, but horribly complicated by poor column names and really badly written instructions.",46.0
g3s2wfv,ildqaw,"Right... I can definitely see how someone learning could get confused by this.

Half the battle in any sort of operational analytics team is trying to figure out WTF people are actually asking for...",24.0
g3tmcx7,ildqaw,"Sadly seems like the issue in many fields of work, deciphering what someone needs is significantly harder than actually doing the task",1.0
g3tt40x,ildqaw,never be afraid to ask questions early and often.￼,1.0
g3u329x,ildqaw,"In our field (SQL development, in this case) particularly, this highlights the importance of the BA/PM roles - whether it be a dedicated BA, or a manager/director that sits in a psuedo-BA/PM role.

The ability to efficiently and effectively translate and intermediate between developer and end user is critical.",1.0
g3sieh5,ildqaw,"Why does that COUNT() specify a field? Why use NTEXT - it seems to have been deprecated for years, so no learner should be exposed to it. Was NSTRING ever a datatype? Why would you represent a schema with an Excel screenshot? I think they want the results grouped by date, but it's hard to tell. 🤔

Stop using this material if possible. If this is representative, it's terrible.",0.0
g3rujni,ildqaw,Only thing to add is I would consider casting the date time as date. If the datetime really is a datetime then the group by will return a row for every distinct date and time in the data. This doesn’t meet the brief which asks you to return a count “for each created date”.,14.0
g3s00mt,ildqaw,"Yes, I think it’s easy to gloss over this in the instructions, but doing a CAST(CreateDttm as date) in the SELECT and GROUP BY is necessary to get a clean, summarized result. Not sure what the data looks like, but in my experience it’s very unlikely that any single timestamp would have an amount of records significant enough that someone would want a report grouped by it. In this case, it probably makes a lot more sense to group by dates instead of date times.",11.0
g3ren1m,ildqaw,"It tells you to bring back the following data items: createdDttm and count(communicationTypeFK) - so these will be in your select statement. Next is your from statement. The table name is 'Communication'. Last is your condition, which they say you must use 6 as the communicationTypeFK. However, it also says ""for each created date"" so you will need a group by statement. Like so:

&amp;#x200B;

select createdDttm, count(communicationTypeFK)

from communication

where communicationTypeFK = 6

group by createdDttm",52.0
g3rtugw,ildqaw,Cane here to contribute but the above comment is a perfect explanation,6.0
g3tldj5,ildqaw,"or is it? see the response from u/MisterSifter84

datetime vs date",3.0
g3rv6zm,ildqaw,That’s it...? Was overthinking I think,8.0
g3sqtkz,ildqaw,I agree this is the solution,1.0
g3u2mz9,ildqaw,This is the solution. Dont forget the semicolon at the end haha.,1.0
g3rtb5g,ildqaw,"i think the task is so basic and spelled out so overcomplicated you are overthinking it...took me like 3 reads to get to ""that's it?""",12.0
g3rdwmo,ildqaw,"It is asking you to write a select statement. First, it is describing the need as a business requirement. Second, it is describing some important data element details that translate what the data elements mean for the requirement, so you can construct the correct query.",5.0
g3renok,ildqaw,The script  is pretty much given out in the last 4 lines,5.0
g3shvs3,ildqaw,This is a horribly written question complicated by crazy nomenclature. If this is for an interview you might want to consider another company.,4.0
g3wrouf,ildqaw,I thought this too,1.0
g3wryet,ildqaw,But then so are most user stories/requirements I have to work with! Perhaps it’s designed to test requirements analysis too! 😂,1.0
g3tdg9y,ildqaw,"Wait..what is this from, it seems like a practice test or some sort of learning module. How could i also see these questions and practice my sql?",1.0
g40uwfd,ildqaw,"It's an annoying question. I think the above answer, casting the dttm to date, is the most reasonable first iteration before you have to go and meet the executive who will inevitably be crying that you haven't put his dummy in straight the way he likes it. Then you'll just have to go from there.",1.0
g3rz68u,ildqaw,[deleted],0.0
g3tm99e,ildqaw,"Average, have been using it for ~1 year at work, I was definitely just overthinking this question",2.0
g3r1pab,ila3hv,"If you go to the Visual Studio installed, does your installation report that it has errors? If so, run a repair and that problem should go away",1.0
g3r3et8,ila3hv,I'm not sure how to check the installation for a report ?,1.0
g3r3l0u,ila3hv,"If you open up Visual Studio Installer, if there is an error with the installation then it will have a warning triangle next to it

It is likely it is something like this as SSMS uses the Visual Studio shell",1.0
g3r4a57,ila3hv,"I've installed VS already

The error comes after the VS installation and after the SQL installation.

It's during the SSMS installation.

Are you recommending I re launch the VS installation ?",1.0
g3pzqjr,il7hy5,"There are more ways to interpret what do you mean by ""doesn't work"", but I think one of the things would be understanding of NULL and how comparison with it works.

Specifically,

      NULL in (1,2,3)

Will return unknown/null.


        1 not in (2,3,NULL)

will return unknown/null.

To avoid these situations, test for null separately and make sure nulls are not in your in/not in lists (i.e. filter nulls with a where clause in the subquery)",2.0
g3q09w9,il7hy5,"You were right. I check for null and now it works.

SELECT Building_name from Buildings where Building_name NOT IN (Select Building from employees where Building is not null)",2.0
g3q16vd,il7hy5,"The output of the IN operator is a boolean value for the outer query so it'll return true/false for whatever you are comparing.

The sub query here contains some NULL values for Building which is interpreted as unknown for each record in the outer query. Instead of throwing an error or a warning of ""Sub query contains null values"" or what not, it just does not return any data.

I am not a fan of the IN operator with a sub query for this reason. It's fine for a known set of records such as hard coded values but if there is ever the possibility for a NULL to sneak in, It's not a good implementation for anything you'd think of putting into production use.

EDIT : If you add WHERE Building IS NOT NULL to your sub query, You'll get what you are looking for.",2.0
g3q1ezk,il7hy5,Yes thats exactly whats happening. Thanks for the explanation. Thats why I do these exercises cause you find something new every day.,1.0
g3q059f,il7hy5,"What are your results, and what are your expected results?",1.0
g3q0iiz,il7hy5,"I found the solution. Apparently NULL in the subquery was producing unexpected results.

The following works

SELECT Building_name from Buildings where Building_name NOT IN (Select Building from employees where Building is not null)",2.0
g3q0np6,il7hy5,"Do the *Building\_name* field in the *Buildings* table and the *Building* field in the *Employees* table contain the same type of data? Or does that *Building* field in *Employees* have something else in it, like maybe a Building ID or something",1.0
g3q1j5f,il7hy5,They are same type. I found the solution btw. The problem was NULL in the subquery.,1.0
g3set2h,il7hy5,"For NOT IN, I like to do coalesce(field, 'dummy') not in ('a', 'b', 'c'), or if you're checking a numeric field that you know is always positive for instance coalesce(field, -1) not in (1,2,3). As long as you turn your nulls into something that won't exist in your data, you're safe with the null handling when you check against a NOT IN list. You could also do   field not in ('a', 'b', 'c') or field is null   but you have to be careful to put it all in parentheses or you could get very unexpected results. I prefer to avoid using an OR when a method like coalesce(field, 'dummy') works the same and doesn't risk mistakes like that",1.0
g3pp8ag,il69xs,"Most correct term is Object.

Dataset can be used too.",5.0
g3r4knw,il69xs,"Dataset is the best answer imho.

I might have a little bias because tblRefDataSet is the name I use.

And tblRefDataPoint for.. columns. TblRefProcess for functions/jobs/procedures etc etc


If you are looking for other abstracted names, here are some subjects I have in my model.


http://elricsims.com/system/subjects",2.0
g3qfeld,il69xs,"&gt; Object

Thanks, yeah that makes sense, I might use something like `schema_object` for more general stuff like triggers/contrains etc too - would those still fit in the definition of ""object"" in SQL?

&gt; Dataset

Yeah that makes sense for the stuff that returns rows, thanks.",1.0
g3q17nw,il69xs,"just curious, why do you need to distinguish between a table and a view?

as far as i can tell, they behave identically in SELECTs

if you really want to know which it is, you could always query INFORMATION_SCHEMA to find out",2.0
g3qh6us,il69xs,"&gt; just curious, why do you need to distinguish between a table and a view?

It's a very detailed definitions system, every database, schema, table, view, column, index, trigger, constraint etc is defined in code and gets a UUID, all these definitions actually get inserted into a bunch of tables about themselves too.  Then it does code generation and other stuff from there.  

Not even limited to database stuff, it does all sorts of other things too.

Tables and views are very different in most ways within this system of ***definitions***, even though they're kinda the same for ***usage***.  Much like INFORMATION_SCHEMA needs to know the difference, so does this definition system.

Obviously to generate code for a table or view, you need to know if it's a table or view.  And INFORMATION_SCHEMA is gunna be empty anyway before the tables etc have been created, and my system stores a lot more extra metadata beyond the scope of the actual schema alone.",1.0
g3qjsew,il69xs,whoa,1.0
g41cpu0,il69xs,"The best distinction is between ""return types"" when addressed, so ""rowset provider"" could be a table, view, table valued function, stored procedure that returns 1 set.

Other than that you are likely to get a scalar result, a tuple or void.

Object is possibly too base but ""table like object"" is something I've seen around

If your utility is building databases then the DDL for tables and views is very different. Functions and stored procedures wildly so. You object types are best organised around those structures.

There are many OO models for database objects avaliable and it is worth seeing how other developers have approached it.

The https://en.wikipedia.org/wiki/Common_warehouse_metamodel
is a large collection of metadata object definitions that cover pretty much any data object you could imagine",1.0
g6nu8nx,il69xs,"One I just came up with: **tabulation**

I like it because:

1. It starts with ""tab"", so it still ""feels"" table-like (without much conscious thought), but broader.  Being a longer word than ""table"" gives a vibe that it encompasses more.
1. It's a real word, and it basically matches that meaning.
1. It's a rarely used word - so I search entire codebases without other stuff in the search results
1. The ""-ulation"" part kinda sounds like ""a process of turning something into a table"" - which is a good description of what VIEWs (and functions that output rows) are... while still sounding inclusive enough of regular tables.",1.0
g3ppim6,il5zjf,You want the keyword pivot. Think of cte as a temp table or subquery.,2.0
g3y0etu,il5zjf,"Pivot and unpivot are easy enough. Using a data driven approach to create the categories is tricky. It’s doable though. The only solution I’ve found is creating SQL dynamically and running it through sp_execute_sql. It’s ugly and not easily maintainable, but it works well enough.

That said, why do the pivot in your data? What is your view driving downstream?",1.0
g3ythud,il5zjf,"This fake example of mine is maybe too simplified... I am starting with event-level data and need to have person-level data to feed into a regression model. 

What I do now is a series of left joins, like
Create view flat_source_v as 
Select personID, Q1_Food, Q1_Phone, Q1_Gas, Q2_Food, Q2_Phone, Q2_Gas 
From (
Select personID, case when category = “Food” then amount end as Q1_Food,
Etc
From source
Where date between jan1 and apr30) as q1
Left join
(similar) as q2
On q1.personID = q2.personID;

And that’s totally fine but if there’s a way to make it data driven based on the categories I have, that’d be great. Also, in practice there’s more than just “amount” to be pivoted, do PIVOT might not actually be the way to go...",1.0
g3pau2v,il1ivd,This might be interesting: [The SQL Murder Mystery](https://mystery.knightlab.com/),7.0
g3q7get,il1ivd,Looks good but I prefer books since those test machines are hell on Earth. Jet Brains Academy is a good example.,1.0
g3rh8sg,il1ivd,"lol I went to look into it. I was like, damn I spend the whole day today to find a bug in our oracle DB and I dont hve time for this shit now!",1.0
g3p9zfr,il1ivd,"I'm *certain* I've listened to a podcast about a very similar book that used the data from a NASA probe, but I can't find it now.",5.0
g3qtw9d,il1ivd,I think it's this one [https://bigmachine.io/products/a-curious-moon/](https://bigmachine.io/products/a-curious-moon/),3.0
g3r5sfg,il1ivd,"Yes, that's the one!",1.0
g3qk1ce,il1ivd,Take a look at this one [https://bigmachine.io/products/a-curious-moon/](https://bigmachine.io/products/a-curious-moon/),2.0
g3sgxsh,il1ivd,This is really good. I learned from it and it was a fun story.,1.0
g3xecgg,il1ivd,Some cool datasets:  https://pds.nasa.gov/,1.0
g3ogct1,ikyn08,You’ll get more responses to actual questions.,6.0
g3o7g3m,ikxqbd,"Query the INFORMATION_SCHEMA. Know it, love it. Even better than asking your coworkers “which customer table should I use?”.

Select * from myserver.mydatabase.INFORMATION_SCHEMA.COLUMNS

You can also use sys objects and there are other types for the information schema, but I always use columns and just group as needed if I want only tables, etc. You can use self joins on it to do all kinds of cool stuff when you don’t have an ERD and foreign keys (you never will have them).",3.0
g3ojofn,ikxqbd,"Another way is as follows:

Type the name of the table in the query window.

    someschema.sometable

Then highlight it and press Alt+F1. It will give you the details you need.",3.0
g3os57w,ikxqbd,"This is one of my fav things to use. I copy and paste the column list then use multi-line edit to prepend commas for my column list. 

Yes, I'm a filthy comma prepender because I'm lazy. Sue me.",1.0
g3pbfbf,ikxqbd,"... or you can drag the word ""columns"" from the object explorer onto the query window. You can do this with an individual column name, the word ""columns"" or the table name.",1.0
g3od8ba,ikxqbd,"If you want the list of columns within a script you're writing, you can right-click on the table in Object Explorer and then go Script Table as --&gt; Create To --&gt; New Query Editor Window (Clipboard &amp; File are the other options) to get the script for creating the table, which happens to include a complete listing of all the columns, their data types, etc.., which you can then edit down to just what you need.",2.0
g3o6stt,ikxqbd,I think you can see them if you right click on the table and go to Properties...it may be called table properties...,1.0
g3o779d,ikxqbd,"I looked at Table Properties and it only has info about the table itself, not any field info.",1.0
g3o7nf4,ikxqbd,"Try this script....only replace ‘Your table name’ with the name of the table (last line) 

SELECT 
    c.name 'Column Name',
    t.Name 'Data type',
    c.max_length 'Max Length',
    c.precision ,
    c.scale ,
    c.is_nullable,
    ISNULL(i.is_primary_key, 0) 'Primary Key'
FROM    
    sys.columns c
INNER JOIN 
    sys.types t ON c.user_type_id = t.user_type_id
LEFT OUTER JOIN 
    sys.index_columns ic ON ic.object_id = c.object_id AND ic.column_id = c.column_id
LEFT OUTER JOIN 
    sys.indexes i ON ic.object_id = i.object_id AND ic.index_id = i.index_id
WHERE
    c.object_id = OBJECT_ID('YourTableName')",1.0
g3om6jz,ikxqbd,DESC &lt;Table Name&gt;,1.0
g3pbhnn,ikxqbd,"Doesn't work on SQL Server, which is what this question is tagged as.",1.0
g41g6au,ikxqbd,"sp_help is useful here

even better, sp_describe_first_result_set will do the same for the result of any query

But for your specific question for ssms try the right click menu in tbe object explorer, script table as create, insert or update. Each returns the columns and types in the right template format",1.0
g4bxysq,ikxqbd,"SCRIPT TABLE AS CREATE from the gui in SSMS:

OR



	SELECT	tab.name AS table_name

	    ,   col.name AS column_name

	    ,   typ.name AS type_name

	    ,    REPLACE(CAST(col.max_length  AS VARCHAR(30)),'-1','MAX') AS column_length

	FROM sys.objects tab

	INNER JOIN sys.columns col

	ON col.object_id=tab.object_id

	INNER JOIN sys.systypes typ

	ON typ.xusertype=col.user_type_id

	WHERE tab.type='U'

	ORDER BY tab.name,col.column_id",1.0
g3o34h7,ikw56n,"It's hard to answer this without the complete statement that the newcategory column fits into; the ""it"" that you're able to run currently. Is it just a select statement against a single table? multiple tables that are joined? Because the exists feel like they're trying to do too much all at once when it might be faster (&amp; more understandable, imo) to first have a statement that handles the x in-a-certain-range** + how many episodes are in that range, and THEN have a second statement that acts on the results to finally get newcategory. Can you post what you have right now that works?",1.0
g3nhoa6,iku76l,"&gt; select * where X = 'Y' or having count(concat(T,U))=1

you can't have an OR that combines WHERE and HAVING

HAVING should, ideally, only be used with GROUP BY

and how come there's no FROM clause?",2.0
g3nk73x,iku76l,"The original table is fed in via google sheets. The “Excel” formula is QUERY(data, “query”, headers), and data is the table.",1.0
g3nhvh6,iku76l,"Ok going off MySQL here, I think if you have having with a where you would need to use a group by. Another thing it might be is the concat and count that would always be 1 unless I'm missing something? concat puts two stings together and count simply counts how many strings. Are you trying to say T+U=1?",2.0
g3nop23,iku76l,"For the second criterion, I’m trying to get only rows that have unique combinations of T and U. That’s why I was hoping to use COUNT to determine how many there are of each such combinations, and use that constraint to achieve my goal.",1.0
g3o7akf,iku76l,List them all. I have SQL statements that are 4 or 5 pages long.,2.0
g3oh3nq,iku76l,That still leaves the fact that I don’t want any non-unique combinations.,1.0
g3ny20p,iku76l,Try where x=y or (t=whatever and u=whatever),1.0
g3o72et,iku76l,T and U have a very large variety of values and I don’t want any values where any particular set of T and U values to repeat.,1.0
g3ntn27,iku45t,"So this is slightly ridiculous, but if you don’t really care which agency name appears, try wrapping it in a max or min function (&amp; don’t forget to remove it from the group by clause!). 
I can’t find verification that it works in MySQL, but I’ve used it in SQL Server, on occasion.",2.0
g3qaslg,iku45t,"I also tried this and it worked as well, initially when I tried we had the group by at the end so it was still pulling back both but removing solved the issue!",2.0
g3nj24k,iku45t,"I think this should do what you are trying to do, Group the Agencies using the agencykey, then count how many policies below to that agency key. Then select a CommonName from a subquery that links to the agencykey (totally random which one you get if they have more than 1 common name associated with agency key) then order by the count of the policy

&amp;#x200B;

Select Distinct  AgencyKey, count(policyid),

(select  CommonName  from yourtable tempName where tempname.AgencyKey

= yourTable.AgencyKey) as CommonName

from yourTable

group by AgencyKey

order by count(policyid)",1.0
g3qajbt,iku45t,"&gt;t how many policies below to that agency key. Then select a CommonName from a subquery that links to the agencykey (totally random which one you get if they have more than 1 common name associated with agency key) then order by the

This worked!! Thank you so much :)",2.0
g3r6gwi,iku45t,No worries,1.0
g3nnogo,iku3rh,This feels like a basic question and you might be better suited to provide some mock data in your description using the table formatting feature.,2.0
g3npch7,iku3rh,"Guessing you also have sales data for the hot dogs, including a time stamp for each sale? You’ll need that. I don’t know the specific Postgres syntax, but [this link](https://stackoverflow.com/questions/1964544/timestamp-difference-in-hours-for-postgresql) seems to describe a similar question. And [here’s documentation](https://www.postgresql.org/docs/9.1/functions-datetime.html) for the `extract` function, which looks like the function you’d need to use. 

But regardless of the specific syntax, the idea will be to get how many hours are between the sale time stamp and the start of the game. You can then `group by` that calculation (so long as the results are integers) and get the sums of hot dogs and dollars sold. The one thing that might require testing is to see that the rounding works like you want it to. E.g. whether or not you want a sale one minute before the start of the game to be grouped with a sale 59 minutes before the start of the game.",1.0
g3mz9wj,iks2hd,"The hard part isn't the SQL query itself, the real skill is understanding where the data you want is. After a time with the DB you'll get familiar with the tables, where to select and do the JOINs to make your analysis.

If you have a solid grasp of data analysis and able to do some SELECT / JOIN / GROUP BY / WHERE you'll be more than fine.",54.0
g3n4q7e,iks2hd,"Seconded. One of the first things I did at a new job was to dump all of my schemas, tables, and column names into excel so that I could sort and filter to the information I might want to see, then build tables to pull that info back. It gave me a good starting point when starting a query from scratch.",22.0
g3n6niw,iks2hd,This is very helpful. Thank you!,4.0
g3n6stx,iks2hd,"Exactly this.

The world's leading expert on SQL is useless if they don't know the schema and where the data they're looking for is located or how it's related.",13.0
g3n8cwa,iks2hd,"If you already have domain knowledge (i.e. you know sales and are working with sales data), conquering how your new DB structures their data is the next challenge.  It sounds like you were pretty honest about your skills.  I'm sure you'll be fine.

When I started my current position, I ran a lot of INFORMATION_SCHEMA queries to create lists of columns and tables. I also browsed around to look at the queries my colleagues were running.  This gave me good insight into heavily used tables and how they were joined.  I also got to see how much SQL game they had.",4.0
g3o5xzr,iks2hd,I still do a lot of INFORMATION_SCHEMA queries because we have so many systems and tables. Those are probably the most useful queries that I run!,3.0
g3n6pic,iks2hd,Thank you! I appreciate the advice.,2.0
g3n5hgq,iks2hd,What is the big 6?,19.0
g3n6wbz,iks2hd,"Sorry, that's just what the instructor called it in our course. It's: SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY.",8.0
g3n8zfk,iks2hd,For future reference:  Nobody calls it that.  Seriously.  Been doing SQL since it was a brand new thing.  First time I've ever heard the term used like that.,50.0
g3nv2z7,iks2hd,Professors love shit like that. I had so many that taught me their own little methods and sayings as if they were standard.,6.0
g3odses,iks2hd,I had a teacher who said try...catch statements are like playing baseball: you try high and catch down low.,2.0
g3nv9jx,iks2hd,"Also for reference: while my parent comment is correct about people knowing what you’re talking about, those are def the most important clauses to understand. Swap HAVING and ORDER BY to get what’s likely to be the most important in order.",3.0
g3qqa2f,iks2hd,"I'd disagree with you there. The only order these should be listed is in the order of execution:  

FROM   
WHERE  
GROUP BY  
HAVING  
SELECT  
ORDER  
LIMIT",2.0
g3nbybe,iks2hd,[deleted],7.0
g3o6adl,iks2hd,Dem CTEs though.,5.0
g3nvkgf,iks2hd,"To add just a bit, temp tables function pretty differently per DBMS, just be aware that what you learn about SQL Server might not be transferable to Oracle, for an example.",3.0
g3o6972,iks2hd,Dafuq? Never heard anyone call it that. I think your instructor just thinks he’s cute. 😄,2.0
g3qrrsf,iks2hd,"All query types are significant relative to the context of their use! You’ve moved on from that lesson now, enjoy.",2.0
g3nbxwy,iks2hd,"The entire industry of data analysis is fueled on imposter's syndrome, and full of people 'learning as they go' - the latter is redubbed 'innovating new technologies'.

No, I think for you to clearly state you're ""learning at a low-level"" absolves you of any dishonesty, and at this point I would not negotiate against yourself any further. If they want to offer you a better role or submit you in a different role, do not get in your own way. You'll learn much more on the job than you can on your own, so you're going to be stepping into some unchartered territory no matter how prepared you are.

Also, FYI contractor/vendor companies aren't as invested in you as they come off. Don't feel like you need to be totally honest - your goal is to get a foot in the door at an actual company, those intermediary agencies are just a tool, and they will use you like currency and spit you out after regardless.",17.0
g3o2ywt,iks2hd,"* ~~Get job.~~
* Get paid to learn how to do job while on job.
* Listen to problems and learn new things to solve problems.
* Impress people by being low key about learning new tricks and improving.
* Tell funny story years later about not feeling qualified.


You already did the hardest part.  The rest is just going to work every day and doing the things.",9.0
g3o7tr8,iks2hd,Thank you!,2.0
g3pv7m9,iks2hd,"| Impress people by being low key about learning new tricks and improving.

Excellent advice. I've been really disturbed by co-workers proudly announcing the latest thing they learned when I thought their ""new"" trick was fundamental.",2.0
g3njbhh,iks2hd,"Here are some things I would expect someone wanting to get paid for their SQL knowledge should know:

* the difference between join, inner join, outer join, left join etc
* what NULL is and what you can and can't do with it
* what a subquery is
* what a common table expression (CTE) is
* primary and foreign keys
* the basics of writing stored procedures, passing in parameters, using variables
* the basics of indexing, what is a clustered index, a non-clustered index, a covering index etc
*  what a temp table is and when to use it
* how to work with dates
* how to convert data from one datatype to another
* how to insert data from one table to another
* how to update or delete rows in one table based on the values in another table
* ideally knowing the basics of execution plans would be good, knowing what things like table scan, index scan, index seek, key lookup mean

I wouldn't expect them to know everything about what's on this list, but enough to say ""oh yeah, I know that you have to do this and that, be aware of these pitfalls, I've got some notes in my folder so I know where to get more information"" is fine. Just as long as they don't look completely blank if I ask them about anything on this list.",9.0
g3o7px7,iks2hd,Thanks!,1.0
g3ogf7h,iks2hd,I don’t think most analysts insert and delete data,1.0
g3n6yaa,iks2hd,"I took my first job in SQL with as much if not less experience than you, and I did just fine. Just be ready and willing to work nights to learn more, and be absolutely up front with them about where you're at in your journey. 

I basically taught myself SQL using this forum, and Google, and it wasn't particularly hard but I did come from a background in programming/networking/hardware.",4.0
g3nbd71,iks2hd,"Just roll with it, man. You’ll learn, Im sure of it.",5.0
g3nkukn,iks2hd,Best way to learn any kind of programming is on the job. Just go with it. Good luck!,4.0
g3n2nqb,iks2hd,"It really depends on what they're expecting out of the position, so I'd just be honest with them and ask that. 

If you've been doing analysis in excel for a while and are learning sql, then I think you'd be a good fit if they're looking for someone on the lower end of the DBA pay scale, that they could train. 

Just be honest with them and see what they say.",3.0
g3n6z35,iks2hd,Thanks! I will. I appreciate your help.,1.0
g3n7e9g,iks2hd,"One more question. What would you consider the low-end of the pay scale for this type of role as an analyst? I make a crappy salary right now because I work for a medical school as an administrator/analyst in healthcare, so anything is a step up. Is 70K too much for a low-level analyst?",1.0
g3n9k2r,iks2hd,"Given you have some excel and basic sql knowledge I don't think 70 is too high. I think it's probably on the high end of what I'd consider for an entry-ish level person, but it's not out of line. 

It also depends on your area. I'm in small town midwest so that type of role would likely be in the 60s range, but on the coasts it could be 80 or 90",2.0
g3npbr3,iks2hd,"Man, I wish. Maybe I just work for a shit company though. Given your CFB fandom I bet I'm not more than a few hours from you and I'm getting paid $55k, 2+ years out of school with an MIS degree, spending most of my day doing systems support and ad-hoc reporting but also acting as subject matter expert representing business users. I've gotten max raises every time the opportunity came up, just haven't gotten that promotion from entry level to intermediate.",1.0
g3nskrx,iks2hd,"IT Secret: fastest way to get a pay raise is to quit your job. 

There's tons of companies that need help with their data. Talk to a recruiter and see what your options are before you spend years staying at the same place getting under paid.",4.0
g3o6qi9,iks2hd,"Where? 55k is very low even if you live in bumfuckville. Most bumblefuck towns don’t even have any businesses with the sophistication to need a dedicated data analyst type, so you can’t be in that small a town.",1.0
g3o8m50,iks2hd,"It's not a small company or town by any means. We have 8k employees in the US, and probably another 50% as many full time contractors. Last I checked I think the balance sheet is in the $300MM range.

It is a low cost of living area though. My 800 sq ft apartment is around $700/mo, and when I was commuting to the office it was very consistently a 20 minute drive. It's close to family for me so it works out.

Honestly though I think it's a weird middle ground where the city is big enough that people grew up around here and want to stay, but there aren't a ton of big employers. If you're in this line of work (not data analytics, I mean their actual core focus) and want to live in this area, this place will always have openings.",1.0
g3ojb8f,iks2hd,"Personally I’d just take as much resume experience as possible and then leave that town for good for a new job and place.

If you’re unmarried with no kids, this is the best time you’ll ever have to move to greener pastures. I’d go for someplace like Charlotte, NC. You can get paid quite well in this field out there but the cost of living is still very manageable. If living close to extended family is important, you can always go back once you’re able to compete for a more senior position, and with a bigger bankroll.

I’m married with kids and I’m looking very seriously at doing the same thing  a few years down the road. If I didn’t have a family, I’d already be there.",1.0
g3pmj77,iks2hd,"Yea if you're in small town midwest it's easy to get stuck in the 50-60 range because everyone out here just gets paid less. I got out of that (by still living in the MW) by finding a job that's 100% work from home. So I basically live in one of the lowest COL areas in the country, but make average, or a little above average salary for my position. 

If you are open to it, I'd recommend to get your resume together and start looking for jobs that are 100% remote. It's a game changer. 

Also, in IT, unless you luck out to work for an awesome company, you generally have to move around a bit to get promoted or big raises",1.0
g3o76yl,iks2hd,Thank you my friend! I’m in Denver btw. Really appreciate all of your advice in this thread.,1.0
g3pm9i6,iks2hd,70 is absolutely reasonable in that area. Good luck!,1.0
g3n9105,iks2hd,This is highly dependent on where you are located and the associated cost of living. Pay is also determined by your level of work experience and expertise (not just SQL).,1.0
g3no5xh,iks2hd,"I'm a manager of analysts in a health care setting, 70k seems about right for me.",1.0
g3o7ed9,iks2hd,Perfect. Thank you. I work as a financial analyst for an academic medical center. The role I’m interviewing for is a revenue cycle consulting firm,1.0
g3n6ba5,iks2hd,What’s the big 6?,3.0
g3n71ua,iks2hd,"&gt;Sorry, that's just what the instructor called it in our course. It's: SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY.",1.0
g3nks27,iks2hd,Hahaha.,2.0
g3n3c7k,iks2hd,"If you have experience with Excel, especially with pivot tables, that is a good foundation for learning SQL. As the other person said, learning the language and syntax is half the battle, the other half is familiarizing yourself with your database environment. It's one thing to learn online using the DB of a fictitious company but the challenge is going to be applying those skills to the databases your company uses. I'd also recommend kudvenkat's channel on YouTube, it's a great resource.",2.0
g3n785t,iks2hd,"Agreed. If you are an expert (vlookups, macros), or master level (arrays) Excel user then SQL will feel like a long lost friend. It is just so much more robust, and you don't have to worry about things like formulas getting miscopied, or breaking, etc. 

It all just becomes code, and a lot of the code has the same syntax as Excel such as SUM(), or adding two things together, etc. 

You can basically Google your solution just by comparing your search to Excel. If you can think of how you might do something in Excel then you're about 80% of the way there to having a SQL solution.

I think a lot of new users to SQL get lost in the syntax and are under the impression that advanced SQL developers have memorized it. For example... if you put a gun to my head I probably couldn't figure out how to get the 1st of the month from a datetime stamp. I mean I might be able to hack something together, bash it until it works, but I don't memorize shit like that. I just Google it whenever I need it",3.0
g3n74y4,iks2hd,Thank you! I will check out the youtube channel as well. Big help!,1.0
g3nbswy,iks2hd,"Same, I'm an analyst who just interviewed to be a database admin. I'm not that. I didn't apply for that. A recruiter thought I was a good fit. 

&amp;#x200B;

And they offered to hire me, so I'm studying like crazy to learn.",2.0
g3o6yje,iks2hd,Just go for it! At worst all they can do is fire you and then you’re just back where you started anyway. 😀,2.0
g3o7567,iks2hd,"True, but you'll bet my resumes will say something like contracted for such and such for this long as this.",1.0
g3ojhsb,iks2hd,"Damned right you will! Then you’ll spin it into a higher paying position than you’d have otherwise. That’s the game.

You’ll probably be fine and successfully fake it til you make it though, especially if they’re willing to make you a DBA with no prior experience.",2.0
g3ojlaw,iks2hd,"Hooyah! In truth, my resume sounds really awesome for what it means.",1.0
g3o7ryx,iks2hd,Good to know I’m not alone. Thanks!,2.0
g3o5sul,iks2hd,"You’ll be fine. SQL isn’t hard. Understanding why and how the F**K the developers implemented particular business logic is hard. 

Finding stuff you need, understanding what it means, understanding business requirements and translating that to product will be 90% of your time.

Do some online courses and experiment with data that you know to build your skills.

Also, get to truly know and love your INFORMATION_SCHEMA and system objects. This is how I find 90% of objects what I need.

Google, YouTube, Reddit, and Stack Overflow will teach you the rest.",2.0
g3o7vll,iks2hd,Thank you!,1.0
g3o5x8b,iks2hd,"Analytics manager here. I tend to agree with others who say you're fine. Just be honest about what you know, fall back on your Excel skills as needed to prove you know how to think about solving problems using data, but also be careful to show that you are genuinely interested in learning new things (not just relying on your current skill set).

For the most part when I'm hiring, I could care less about existing familiarity with any particular language. Anyone with the right aptitude can Google syntax. I'm looking for brains, curiosity, and enthusiasm.

Just please do not tell them about all of the joins you can do (""I know about inner joins, left joins, right joins. . . .""). I'm not sure why so many people have felt the need to tell me this in an interview, but it's a little like applying to be a journalist and explaining, ""I can write interrogative, declarative, imperative, and exclamatory sentences."" It makes me question your grasp of the language.",2.0
g3o7zdz,iks2hd,This is really helpful. Thanks!,1.0
g3o78wx,iks2hd,"I started my current job with next to no SQL experience. I had learned fairly basic stuff for personal projects. I interviewed, thought I'd bombed, but landed the job. I have been at my current position for close to two months. I've learned a ton on the job, and my knowledge far exceeds that of when I'd started. It may feel like you're over your head for awhile but things will slowly start to click.",2.0
g3o8egt,iks2hd,Thank you!,1.0
g3ombsq,iks2hd,"no, this is how most of us get started.",2.0
g3onxhw,iks2hd," What i'd recommend to help you practice to use the w3schools editor/database and formulate your own questions to practice. For example some questions you may have is:

1. Who had the highest orders?
2. What is the single largest purchase ($$)
3. Can I find the name of the above?

You can often build upon your initial queries. This really helps out when you transition into a business analyst, or talking to stakeholders in general.  


In the real world, the main problem I face is that the data I receive is not as clean as a lot of these practice databases. You have to do your due dilligence to ask questions like ""are these keys actually join together"", ""can this join cause me to duplicate"", ""if I have 1million rows, how do i test if i even have duplicates or erronenous data?""",2.0
g3osrwl,iks2hd,"Learning sql isnt the hard part. Its building and designing the database structure that takes all the hard work. Its easy to male a flat structure, but what about adding multiple related tables. Primary keys and foreign keys can get messy so its up to you to structure it well. 

I could be wrong but with the actual language, i feel the most difficult part of SQL queries are the Joins",2.0
g3owjsc,iks2hd,"You're basically where I started 8 or 9 years ago. Don't worry at all.  I'm now a senior developer and in that time more than doubled my salary. All that through just learning on the job and looking for problems to solve. Congratulations on starting this journey! 

I don't know about everyone else but I have always suffered from imposter syndrome even when I'm being told how great of a job I'm doing. So if you're feeling it try to remember your not alone and can be successful regardless.


I suggest you subscribe to some SQL blogs. When you have a problem search for multiple solutions because the to result isn't necessarily the best. Get to understand normalisation. Then once you've got the basics down start looking at data warehouse designs (like the Kimball methodology). 


A few more tips long term.
 Learn to do things efficiently. Two different queries can return the same results but one might run in an hour and another in seconds. 
Look for how you can save time and help others save time. For instance if lots of people query one particular set of tables for 10 columns 90% of the time then it's probably worth making a view so no one has to type out that code.

 In my experience analytics jobs can be highly pressured for time. The boss promised his boss or client that you could do something in a day that really needs a week. Making things take less time to run and reducing the time it takes to write queries can help reduce some of that pressure. If you share it with others you'll be appreciated. 
I've made my career by being more efficient than my peers, sharing that knowledge with anyone who would listen and taking the time to make it easier for others to be efficient even if they don't really have the time to learn.

Good luck! I'm excited for you.",2.0
g3sslck,iks2hd,Thank you!,1.0
g3ozpwm,iks2hd,"Most of your learning will be on the job.

Right now you need good work ethics and a methodical approach to problem solving.

Learn how to test your own work and provide evidence of the results

Learn how to write clean, well formatted and highly readable code

Learn where and when comments are appropriate

Learn how to protect your work with git and work effectively with the right tools

Learn how to be the best communicator you possibly can

Learn when meetings are appropriate vs when you need focus time. Be the best at time management. You can even make a database to help you with this.",2.0
g3qnep4,iks2hd,"If you focus on what others do and what the future should be, you will never focus on the present moment, so you will never enjoy doing anything, including using SQL.",2.0
g3n995p,iks2hd,I learned SQL in 3 days. After that I was writing stored procedures.,2.0
g457be1,iks2hd,You kidding? I'm scared of stored procedures!,1.0
g3thstd,iks2hd,"One of the best and worst things about SQL is that it's easy to get started. It's human readable by comparison to a language like C++. This means that people can have a go with very limited technical expertise. 

As a result, there are lots of people working with data who are in over their head - and often they don't even know it! There are also lots of highly skilled people suffering when reading code that experienced analysts have written.

Learn the basics of computer science. Understand data types and think carefully about which you choose. Know about standard (ANSI) SQL and try to write that where possible. Consider your colleagues and your future self when writing any code that is not simply run-once ad hoc analysis. You will go a lot further.

Think get a lot more interesting when you start trying to automate things, query with huge datasets or work in a transactional environment.

SQL is very easy to do. But can be very difficult to do well.",1.0
g3n164c,iks1tt,"    SELECT DATEFROMPARTS(
             CAST(SUBSTRING(CAST integercolumn AS CHAR(6)),5,2) AS INT) + 2000
           , CAST(SUBSTRING(CAST integercolumn AS CHAR(6)),1,2) AS INT)
           , CAST(SUBSTRING(CAST integercolumn AS CHAR(6)),3,2) AS INT) 
           )  AS mydate",0.0
g3n19fi,iks1tt,"I think this may be kinda sloppy but works if your field is really formatted as MMDDYY:

&amp;#x200B;

    DECLARE @data as INT
    SET @data = 123020
    
    SELECT DATEFROMPARTS(
    2000 + SUBSTRING(CAST(@data as varchar(6)),5,2),
    SUBSTRING(CAST(@data as varchar(6)),1,2),
    SUBSTRING(CAST(@data as varchar(6)),3,2)
    )

EDIT: It will work only from year 2000 to 2999",0.0
g3n4xjm,iks1tt,"This one handles the elusive leading zero. Also, another approach using the STUFF function.

    declare @MyDate int = 090120;
    select @MyDate as MyDate,
      right(concat('0', @MyDate), 8) as MyDate2,
      stuff(stuff(right(concat('0', @MyDate), 8), 3, 0, '-'), 6, 0, '-20') as MyDate3,
      try_cast(stuff(stuff(right(concat('0', @MyDate), 8), 3, 0, '-'), 6, 0, '-20') as date) as MyDate4;",0.0
g3ni2na,ikryg1,do you know about the `ON DUPLICATE KEY UPDATE` option of `INSERT`?,1.0
g3nphm4,ikryg1,I do not...please go on,1.0
g3nr5ei,ikryg1,"you write the INSERT statement as normal

then you provide values to ~update~ the row if it already exists, based on a duplicate key

for the exact syntax, see the manual",1.0
g3mjfof,ikom1x,"Identifying missing records can be a tricky thing... especially the unknown unknowns kind, like what if an employee hasn't submitted ANY documents? Or, if there's a required document no one has submitted? 

This is a good example of where a cross join is useful. It can generate all possible/required records, which can then be compared against the records you actually have. 

A list of the needed docs and a list of all employees are needed for the cross join, which represents all required submissions. Then, left join onto *that* the docs that *have been* submitted. Anything in the cross join that doesn't have a matching record from the list of current submissions will be who/what still needs to be submitted. 

Below is a sample SQL version. The temp tables are just stand-ins for whatever makes more sense in your context.
 
    ;with RequiredSubmissions as(
        select distinct e.EmployeeName, d.DocumentType
        from #docs d cross join #employees e
    )
    select rs.*
    from RequiredSubmissions rs 
        left join #CurrentSubmissions cs 
        on rs.EmployeeName = cs.EmployeeName 
            and rs.DocumentType = cs.DocumentType
    where cs.DocumentType is null",2.0
g3n404d,ikom1x,Thank you!,2.0
g3njz40,ikom1x,I too had the same solution. Is there a way to do this without the cross join step,1.0
g3mnkx5,ikom1x,"Expanding on the above solution:

    DROP TABLE IF EXISTS #table_1;
    CREATE TABLE #table_1
    (Employee_Name  VARCHAR(32), 
     Document_Type  VARCHAR(32), 
     Date_Submitted DATE
    );
    INSERT INTO #TABLE_1
    VALUES
    ('Fred Miller', 
     'Driver''s License', 
     '2-2-2020'
    ),
    ('Joe Smith', 
     'Fishing License', 
     '1-1-2020'
    ),
    ('Joe Smith', 
     'Proof Of Insurance', 
     '1-1-2020'
    ),
    ('Joe Smith', 
     'Social Security Card', 
     '1-1-2020'
    ),
    ('Joe Smith', 
     'Driver''s License', 
     '1-1-2020'
    );
    WITH REQUIREDSUBMISSIONS
         AS (SELECT DISTINCT 
                    E.EMPLOYEE_NAME, 
                    D.DOCUMENT_TYPE
             FROM
             (
                 SELECT DISTINCT 
                        EMPLOYEE_NAME
                 FROM #TABLE_1
             ) E
             CROSS JOIN
             (
                 SELECT DISTINCT 
                        DOCUMENT_TYPE
                 FROM #TABLE_1
             ) D)
         SELECT RS.*
         FROM REQUIREDSUBMISSIONS RS
              LEFT JOIN #TABLE_1 CS ON RS.EMPLOYEE_NAME = CS.EMPLOYEE_NAME
                                       AND RS.DOCUMENT_TYPE = CS.DOCUMENT_TYPE
         WHERE CS.DOCUMENT_TYPE IS NULL;",2.0
g3n44b9,ikom1x,Thank you!,1.0
g3o7adw,ikom1x,"Cross join in a CTE to create every required combo. Left join the actual data on the same attributes, look for those nulls.",2.0
g3mg27z,ikom1x,[deleted],1.0
g3mgno5,ikom1x,"I currently don't have a table of required document types, but I can build one",1.0
g3lr6yz,ikmqyc,"Why are you subselecting everything and then joining all the files together in the main query?  

I imagine that you can delete all this:

INNER JOIN entries ON users.user\_id = entries.user\_id

INNER JOIN games ON entries.game\_id = games.game\_id

INNER JOIN payments ON users.user\_id = payments.user\_id",1.0
g3lrl30,ikmqyc,"I used those joins for these:

MAX(payments.payment\_date) AS Last\_Paid\_Entry\_Date, SUM(payments.amount) AS Net\_Deposit\_Value

they are in different tables than ""users""",1.0
g3ls27q,ikmqyc,I deleted the first two and kept the third one -  INNER JOIN payments ON users.user\_id = payments.user\_id,1.0
g3lsnkc,ikmqyc,Did that fix it?,1.0
g3ltj2c,ikmqyc,"No sir.  thanks though. 

I think each of the subquerys for some reason dont have a relation to the [users.id](https://users.id). if I run just one of the subquerys, I get the same exact value that is being duplicated. 

For example for the first subquery, SELECT SUM(entries.entry\_fee)

FROM entries

JOIN users ON entries.user\_id = users.user\_id

JOIN games ON entries.game\_id = games.game\_id

WHERE entry\_date BETWEEN '2018-01-01' AND '2019-01-01' AND sport = 'NFL'  


&amp;#x200B;

&amp;#x200B;

  
I have to change it to  this to get the results I want:

&amp;#x200B;

SELECT SUM(entries.entry\_fee), users.user\_id

FROM entries

JOIN users ON entries.user\_id = users.user\_id

JOIN games ON entries.game\_id = games.game\_id

WHERE entry\_date BETWEEN '2018-01-01' AND '2019-01-01' AND sport = 'NFL'

GROUP BY users.user\_id

ORDER BY users.user\_id;

&amp;#x200B;

thanks!",1.0
g3lx0a7,ikmqyc,"well, if you don't have a relation ship between two tables you will get duplicate rows in your results so that's probably it.

You can just do all those selects in one query, can you not?

Sort of like this (I didn't really validate it, just moved things around)

SELECT users.user\_id, [users.email](https://users.email),SUM(entries.entry\_fee) as Total\_Entry\_Fee\_NFL\_2018,

SUM(entries.entry\_fee) AS Total\_Entry\_Fee\_Others\_2018,

COUNT(entries.winnings) AS Total\_Winning\_Entries,

(SUM(mobile\_entry) / COUNT(mobile\_entry))AS Percentage\_Of\_Mobile\_Entries,

MAX(payments.payment\_date) AS Last\_Paid\_Entry\_Date, SUM(payments.amount) AS Net\_Deposit\_Value

FROM users

INNER JOIN entries ON users.user\_id = entries.user\_id

INNER JOIN games ON entries.game\_id = games.game\_id

INNER JOIN payments ON users.user\_id = payments.user\_id

WHERE entry\_date BETWEEN '2018-01-01' AND '2019-01-01' AND sport = 'NFL'

and entries.entry\_date BETWEEN '2018-01-01' AND '2019-01-01' AND sport &lt;&gt; 'NFL'

GROUP BY users.user\_Id, [users.email](https://users.email);",1.0
g3ly9yw,ikmqyc,"&gt;SELECT users.user\_id, users.email,SUM(entries.entry\_fee) as Total\_Entry\_Fee\_NFL\_2018,  
&gt;  
&gt;SUM(entries.entry\_fee) AS Total\_Entry\_Fee\_Others\_2018,  
&gt;  
&gt;COUNT(entries.winnings) AS Total\_Winning\_Entries,  
&gt;  
&gt;(SUM(mobile\_entry) / COUNT(mobile\_entry))AS Percentage\_Of\_Mobile\_Entries,  
&gt;  
&gt;MAX(payments.payment\_date) AS Last\_Paid\_Entry\_Date, SUM(payments.amount) AS Net\_Deposit\_Value  
&gt;  
&gt;FROM users  
&gt;  
&gt;INNER JOIN entries ON users.user\_id = entries.user\_id  
&gt;  
&gt;INNER JOIN games ON entries.game\_id = games.game\_id  
&gt;  
&gt;INNER JOIN payments ON users.user\_id = payments.user\_id  
&gt;  
&gt;WHERE entry\_date BETWEEN '2018-01-01' AND '2019-01-01' AND sport = 'NFL'  
&gt;  
&gt;and entries.entry\_date BETWEEN '2018-01-01' AND '2019-01-01' AND sport &lt;&gt; 'NFL'  
&gt;  
&gt;GROUP BY users.user\_Id, users.email;

Hey thanks for that I appreciate it. I don't think that works because its putting too many different constraints on the same users. Nothing appears.  If that makes sense?",1.0
g3m173b,ikmqyc,try some left joins to see if you're missing a link in there.,1.0
g3ltmiq,ikmqyc,"&gt;  Each of the subquery columns are all duplicates.

why do you expect something else? Your subqueries are not correlated (do not depend on the main query) so they calculate the same value for the every result set row:

    ( SELECT SUM(entries.entry_fee)
    FROM entries
    JOIN users ON entries.user_id = users.user_id
    JOIN games ON entries.game_id = games.game_id
    WHERE entry_date BETWEEN '2018-01-01' AND '2019-01-01' 
    AND sport = 'NFL'
    ) AS Total_Entry_Fee_NFL_2018,


I would guess you are trying to get these formulas to fire per user - so dont join/select from ""user"" table in subqueries and use the outer query users.user_id",1.0
g3lve31,ikmqyc,"Hey thanks so much. What you are saying makes sense But I don't know how to actually apply that. I am trying to get it per user. 

 ""so dont join/select from ""user"" table in subqueries and use the outer query users.user\_id""

what does this look like?",1.0
g3m0tjx,ikmqyc,as if there is no 'join users' or 'from users' in the text of the subquery.,1.0
g3mnwgz,ikmqyc,"try this --

    SELECT users.user_id
         , users.email
         , SUM(CASE WHEN games.sport = 'NFL'
                    THEN entries.entry_fee
                    ELSE NULL END)  AS Total_Entry_Fee_NFL_2018
         , SUM(CASE WHEN games.sport &lt;&gt; 'NFL'
                    THEN entries.entry_fee
                    ELSE NULL END)  AS Total_Entry_Fee_Others_2018
         , COUNT(entries.winnings)  AS Total_Winning_Entries       
         , 100.0 * SUM(entries.mobile_entry) 
                 / COUNT(entries.mobile_entry) AS Percentage_Of_Mobile_Entries
         , p.Last_Paid_Entry_Date
         , p.Net_Deposit_Value        
      FROM entries
    INNER
      JOIN users 
        ON users.user_id = entries.user_id 
    INNER
      JOIN games 
        ON games.game_id = entries.game_id 
    INNER
      JOIN ( SELECT user_id
                  , MAX(payment_date) AS Last_Paid_Entry_Date
                  , SUM(amount)       AS Net_Deposit_Value    
               FROM payments
             GROUP
                 BY user_id ) AS p     
        ON p.user_id = entries.user_id
     WHERE entries.entry_date &gt;= '2018-01-01' 
       AND entries.entry_date  &lt; '2019-01-01' 
    GROUP 
        BY users.user_id
         , users.email",1.0
g3or4fy,ikmqyc,"Hey this is perfect except for COUNT(entries.winnings)  AS Total_Winning_Entries  is also using the where clause:

 WHERE entries.entry_date &gt;= '2018-01-01' 
   AND entries.entry_date  &lt; '2019-01-01'

how do I make it so that one uses all date? Thanks!!",1.0
g3q27qq,ikmqyc,"    SELECT q.*
         , p.Last_Paid_Entry_Date
         , p.Net_Deposit_Value     
         , w.Total_Winning_Entries           
      FROM ( SELECT users.user_id
                  , users.email
                  , SUM(CASE WHEN games.sport = 'NFL'
                             THEN entries.entry_fee
                             ELSE NULL END)  AS Total_Entry_Fee_NFL_2018
                  , SUM(CASE WHEN games.sport &lt;&gt; 'NFL'
                             THEN entries.entry_fee
                             ELSE NULL END)  AS Total_Entry_Fee_Others_2018
                  , 100.0 * SUM(entries.mobile_entry) 
                          / COUNT(entries.mobile_entry) 
                                             AS Percentage_Of_Mobile_Entries
               FROM entries
             INNER
               JOIN users 
                 ON users.user_id = entries.user_id 
             INNER
               JOIN games 
                 ON games.game_id = entries.game_id                   
              WHERE entries.entry_date &gt;= '2018-01-01' 
                AND entries.entry_date  &lt; '2019-01-01' 
             GROUP 
                 BY users.user_id
                  , users.email       
            ) AS q             
    INNER
      JOIN ( SELECT user_id
                  , MAX(payment_date) AS Last_Paid_Entry_Date
                  , SUM(amount)       AS Net_Deposit_Value    
               FROM payments
             GROUP
                 BY user_id ) AS p     
        ON p.user_id = entries.user_id
    INNER
      JOIN ( SELECT user_id
                  , COUNT(winnings)  AS Total_Winning_Entries     
               FROM entries
             GROUP
                 BY user_id ) AS w     
        ON w.user_id = entries.user_id 

did you happen to notice how i changed your BETWEEN condition to a pair of inequalities?   ask me if you don't see why",1.0
g3ullhr,ikmqyc,"Hey man really appreciate this. fixed it a little and it worked. Yes sir, the inequalities are better because the BETWEEN clause is inclusive of 2019-01-01 00:00:00, which is not the year 2018!! thanks man",1.0
g3uqv9k,ikmqyc,"&gt; fixed it a little

please share!",1.0
g3nv3ni,ikm16k,"A query for an analytics database is either  (A) highly selective, ie processes a small set of rows or (B) low selective, i.e. processes a large number of rows.  
For (A), rocksdb works very well be default. It is extremely fast for point lookups and short range scans. For (B), you need to build a columnar storage as a layer on top of RocksDB. 

This is the approach we have taken at Rockset to make RocksDB be super-useful for analytics applications. Rockset uses RocksDB to build an index on every column of your record. Rockset builds an inverted index (like Elastic), a column store (like Redshift) and a row store (like Postgres) using RocksDB as the underlying storage engine.",3.0
g3njsao,ikm16k,"I'm at Rockset and a few of our founders help build RocksDB when they were at Facebook, more specifically, Dhruba Borthakur. We use RocksDB and RocksDB-Cloud  as an underlying base to do real-time data.  I think these blogs will help on how we use RocksDB: 

[https://rockset.com/blog/how-we-use-rocksdb-at-rockset/](https://rockset.com/blog/how-we-use-rocksdb-at-rockset/)

[https://rockset.com/blog/remote-compactions-in-rocksdb-cloud/](https://rockset.com/blog/remote-compactions-in-rocksdb-cloud/)

If you have more specific questions on RocksDB happy to answer them. I hope these blogs help.",2.0
g3lyope,ikm16k,"RocksDB is a key-value storage engine, good for certain high-volume OLTP workloads, but not really ideal for OLAP work.  Pulling small amounts of data quickly, not pulling massive amounts of somewhat filtered data.

Is OLAP the primary use of the data store, or are you trying to find something you can do OLAP on that can also support the (more important) needs of the OLTP workload?

If it's the first one, I'd trying to figure out how to restructure the data and put it in the MariaDB ColumnStore Engine, which is very good for OLAP work generally.

If it's the second scenario, that's tough, and really depends on the volume of data and type of OLAP work that is intended, does it need to run against the live data, or could an extract be an option.  Since it's key-value stored, are there only certain key-value pairs that are of analytic interest?  That might simplify the problem.  And also understanding how much data we are talking about... GB/TB/PB ?

Just some things to think about... I am not a RocksDB user, so the above is FWIW...",1.0
g3m3ia2,ikm16k,What if I used the tried and tested method of adding summary tables for some of the reporting workload?,1.0
g3m4mjq,ikm16k,"Yeah those are the kind of options that might work.  You may want to dump \*all\* the data instead of just summary tables, because analytic focused DB's are really good at making summary tables on the fly, and then you'd have the most flexibility to slice and dice the data as you may need.

But as always ""it depends""... can you do full copy every day?  Is that fast enough?  Is it time-series data so that copy can be incremental?  Or do you need a full copy?

Often analytic DBs keep each full copy so you can compare snapshot changes over time.  Obviously that means they can get HUGE, so now you're talking more buying more storage probably.  Is that kind of cost a limiting factor for your scenario?  

But within the same DB, if you build summary tables, I'd just use a different storage engine than a key-value store.  If they are small, you could just go with the default and call it good.",1.0
g3maazp,ikm16k,"&gt; key-value store

MyRocks is more of a column-family store",1.0
g3lm5v9,iklyqm,"    SET @balance = 1000 ;
    
    SELECT 0.5    * @balance AS step1
         , 0.3333 * ( @balance -
                      0.5 * @balance ) AS step2
         , 0.5    * ( @balance -    
                      0.5 * @balance - 
                      0.3333 * ( @balance -
                                 0.5 * @balance ) ) AS step3
         , @balance - 
           0.5    * @balance -      
           0.3333 * ( @balance -
                      0.5 * @balance ) -
           0.5    * ( @balance -     
                      0.5 * @balance -
                      0.3333 * ( @balance -
                                 0.5 * @balance ) ) AS step4           
                                                    
                                                    
    step1  step2   step3    step4
    -----  -----   -----    -----
    500.0  166.65  166.675  166.675

tested with MySQL

the SQL should be fine for Oracle (not sure about setting Oracle user parameters, though)",5.0
g3lq7uf,iklyqm,"Sorry, I wasn't clear in my original post.  The 4 steps were just an example.  I could have anywhere from 1 to 12 steps and don't want to have to write out for each.",3.0
g3lsaix,iklyqm,"yeah, you really should've mentioned that

so pffffft... there goes the time i spent testing it

you're gonna want a **[recursive CTE](https://docs.oracle.com/cd/E17952_01/mysql-8.0-en/with.html#common-table-expressions-recursive)** but i'm not going to do it for you",5.0
g3lsgnp,iklyqm,Sorry about that.  I really do appreciate you taking the time though.  I was thinking I would have to go recursive but was hoping something a little more simple.  I've never done recursive stuff before.  Looks like I get to learn something new today.  Thanks!,3.0
g3mfagj,iklyqm,"I agre, your description very much look like a recursive problem.

There may be a math based solution with analytical functions, but it depends on some details like rounding and variability of your problem.",4.0
g3mz645,iklyqm,"As u/Ximlab mentioned, this is doable using analytical functions.

Give it a shot. If it doesn't work, I can take a look at it later today.",2.0
g3n7cm7,iklyqm,"This looks like a simple window function solution:

```
select ps.step, 
       ps.percent, 
       sum(b.balance - (b.balance * ps.percent)) over (order by ps.step)
from balance b
  join payout_step ps on ps.payout_type = b.payout_type -- or whatever
order by ps.step;  
```

[Online example](https://dbfiddle.uk/?rdbms=postgres_12&amp;fiddle=68f05438119ae2f283492e9eab03de7d)",1.0
g3u8sb4,iklyqm,"Thanks for your help but unfortunately that didn't work.  While the first payment was correct the subsequent payments were wrong.  Your solution gave 1166.70 amounts where I was looking for 166.66.  I ended up just doing a recursive function which isn't as elegant as your approach but is working.  Thanks again.

I'll post the function as a separate comment if interested.",1.0
g3u96d0,iklyqm,"For anyone interested or who ends up on this post with a similar problem I ended up just writing a recursive function to handle this problem.  Code below.

    CREATE OR REPLACE FUNCTION get_current_balance(DSID IN NUMBER, STARTDAY IN DATE, STEP IN NUMBER, BALANCE IN NUMBER)
        RETURN NUMBER
        IS CURRENTBALANCE NUMBER(11, 2);
    BEGIN
        SELECT
            CASE
                WHEN STEP = 1 OR SCHEDULEDDAY &lt;= STARTDAY
                    THEN BALANCE
                ELSE
                    GET_CURRENT_BALANCE(DISTRIBUTIONSCHEDULEID, STARTDAY, PAYMENTNUMBER, BALANCE - (BALANCE * PERCENT))
            END CURRENTBALANCE
            INTO CURRENTBALANCE
        FROM RT2.PAYMENTDETAIL
        WHERE DISTRIBUTIONSCHEDULEID = DSID
            AND PAYMENTNUMBER = CASE WHEN STEP = 1 THEN 1 ELSE STEP - 1 END;
        RETURN(CURRENTBALANCE);
    END;",1.0
g3mkjg7,iklw92,There might be some mismapping going on in the virtual field table. See https://docs.microsoft.com/en-us/dynamics-nav/field-virtual-table for more info.,2.0
g3l41ww,ikjzyq,"Ok I think I managed to solve case a 

Just need help with case b",2.0
g3l42yb,ikjzyq,"*Ok I think I*

*Managed to solve case a Just*

*Need help with case b*

\- newguy1111111111

---

^(I detect haikus. And sometimes, successfully.) ^[Learn&amp;#32;more&amp;#32;about&amp;#32;me.](https://www.reddit.com/r/haikusbot/)

^(Opt out of replies: ""haikusbot opt out"" | Delete my comment: ""haikusbot delete"")",3.0
g3nkdgq,ikjzyq,"From the information you've given we don't know the range of dates in the Borrow table, so we have no way of knowing how many dates are before 1/1/2017 and how many dates are after it.",1.0
g3krkb7,ikh94m,"How do you know that it's not the templates that have been altered?

If your database or filesystem has been compromised, other parts of your system may be compromised as well. The safest thing to do is to wipe &amp; reinstall the whole server and restore your backups from a known safe state.",8.0
g3kt6p7,ikh94m,Okay cool. I thought about wiping and reinstalling the whole server &amp; backups but then I managed to remove some scripts in the header.php &amp; index.php files and issue halted for a while then came back after 3-4 days. so thought I'd check the database too...Thanks anyway,1.0
g3kw6dn,ikh94m,"&gt;  and issue halted for a while then came back after 3-4 days

Which means your server has been compromised. Wipe, change all possible linked passwords, then restore from a known clean backup.",6.0
g3kw92z,ikh94m,okay thanks,1.0
g3l2e4l,ikh94m,"I would also note it would be good to take the time to upgrade to the latest versions of things too. Depending on the way the server was compromised. If it's a flaw with an old version of OS, PHP, Wordpress (or any combo of the 3) simply restoring to a previous point may leave you vulnerable to whatever the attack was. I'm speaking from experience.",2.0
g3l262t,ikh94m,"Classic hacked WordPress site. Youll need to decide on if you're spending hours combing through the site and cleaning it or paying a service like Sucuri to do it. My old job we had to clean WordPress by hand and it could take an hour or more. Eventually we learned on Sucuri to do it for us instead. Time depends how thorough you are. You definitely need to update all your plugins, theme, and the core wp install. But that alone won't clean the infection.

Good luck sir!",2.0
g3kikyg,ikgu5q,"For accounting I use SQL to query the data that is collected by the ERP and sits in the ERPs tables or in some form of data warehouse.

Typically I use it to build my own highly specialised reports that the ERP can’t provide me in a single or few reports.

I don’t use SQL much but that’s a practical example for you in a non-IT based function.",7.0
g3nbmt0,ikgu5q,Perfect example.,1.0
g3ltunc,ikgu5q,"I use SQL extensively but it's definitely not my main duty.  The Warehouse Management System (WMS) is basically a huge database so for me, SQL is a means to get ad hoc data to answer a specific question (""Tell me how many times X happened last month""), or to create recurring reports in a separate BI reporting / alerting system (Cognos) or to get big hunks of data to analyze.  

Any real time reporting the warehouse workers do is probably an SQL that I wrote to extract a small set of specific information (do I have anything stored in the wrong area of the warehouse (ie fridge item not in the fridge), is all my work done (something received that isn't put away), where are my open orders that need to be completed, how much picking is left to do, did all my trucks leave on time, how much receiving did we do today, how much replenishment will we have to do tonight).  And on and on and on.",4.0
g3kpuub,ikgu5q,"""Like will they have databases and tables set up for me, am I doing that myself, or it depends? "" - It depends entirely on your role, the company and how the company view your role as well.

I would expect that you would walk into an existing database system, if not, that's something else entirely which would involve knowing about data structures and architecture which is a job all in itself.

In my previous role I mainly queried existing tables to use for reporting and analysis, this data would come from various CRM, sales and advertising systems. In my current role I am more technically focused and will spend time writing stored procedures to bring new data into our database (from APIs etc) or to fix data issues that are reported (a calculation might be incorrect for example). 

It is all very much job specific and over time you will start to connect the dots together about what sorts of skills you would need for what role.  What sort of roles are you looking at out of interest?",3.0
g3l0otr,ikgu5q,Wow your new role sounds interesting! How do you bring data into a database is that involving another program?! I’ve been mainly looking for business or data analyst roles. I’m very interested in data analytics and I’m trying to learn more about data science as well. I’m starting to learn Python and I have a decent understanding of Tableau and Excel. Any tips on the job search would be much appreciated as well! Thank you!,1.0
g3l67ju,ikgu5q,"Depends on the source but generally we use Azure Data Factory to build a pipeline that can do various tasks for example: a task to find a file (from a storage container), then take the data and place it into a staging table in the database, then trigger a stored procedure to further transform the data into a destination table. 

If you are looking at data analytics jobs then the database will already exist and it would be a matter of learning what data you have, how it relates to the business and where you can connect tables to gather good insight. 

In terms of your job search technical skills are great but having soft skills are equally as important. Big things that will help you out will be learning the domain that the company works in, making sure you really understand what the business wants to achieve. Being able to communicate insight not just data, finding out reasons why for everything and maybe offer potential things to try as well based on your research. In better terms, ""how can the business apply this to help us achieve our goals?"". 

Some 'data analyst' roles can end up becoming spending 100% of your time churning out reporting for people without doing any of the insight side of the role and it has happened to me on a couple of occasions. Luckily, I prefer that side of the job which is more technical and involved automating processes which is why I ended up changing roles. 

Sounds like you are learning all of the right stuff, try your hand at Power BI as well, you can download it for free. A lot of businesses are tied in with Microsoft and use it.",2.0
g3nolpr,ikgu5q,Thank you so much for your advice! I appreciate all the help I can get as I look for a full-time role,1.0
g3mptw9,ikgu5q,"How is the set up? Like do they set up my own computer connected automatically to the data bases? Or do I need to master git and learn how servers work to know how to connect to these data tables?

Like imagine literally that all you know how to do is SQL queries. What beyond that should you or would you recommend we know?",1.0
g3l4nq7,ikgu5q,"Yes, you can expect most companies to have a pre-existing database and architecture you will be using.  More than likely, it will be database(s) and architecture(s).  

An example from my first SQL gig, an analyst role would be something like:

'The manager of customer service states that his reports are wrong.   They are showing an incorrect number of total calls/day.  It appears that the total is including the wrong field, the value for 'missed calls' in the total.   Please investigate and correct.'   Resolution would involve talking to the manager and his staff, getting examples, and then looking into the underlying SQL.   If it were relevant, I would rewrite the query to be correct.   It would be released in a normal deployment cycle, and I would follow up with everyone to make sure it worked.   

As others have pointed out, an analyst role is about 90% working with pre-existing systems, usually an ERP, a CRM, and operational software.",3.0
g3lsu06,ikgu5q,"A lot of applications feed directly into a SQL database or at least some type of database.

I use it daily or multiple times a day to find information in the dataset that is not easily accessible from the application.",3.0
g3ndmzb,ikgu5q,"I use it daily for writing programs our ERP doesn’t have. For example, I am able to pull all of our back order data out of our software, have a spreadsheet push/replace the data, and email customers that their items are on back order.",2.0
g3og153,ikgu5q,"Really depends. Sometimes your job will be ""he cloudybandit, can you get be some data on the XYZ product? Just the user numbers, engagement, etc for this year"" which, depending on the complexity of it all could be fast or could take a while, in addition to data cleaning etc.

Sometimes it will be to develop the database structure for a new product coming out because you're the only SQL person there. 

Many times you'll be connecting to the cloud (e.g. AWS), and pulling database from a server located there.",1.0
g3omkwx,ikgu5q,"i work at a big bank youve prob heard of. our trading platform(s) all create lots of data since lots of money is exchanging hands everyday between other banks, big corporations and hedge funds..and all that data needs to be stored somewhere. 

i also worked in call centers in the past...all of those calls and the data from the inquirys from customers get logged into a database.  the business measures all that stuff to make sure everyone is doing what they're supposed tooo",1.0
g3jp7e4,ikbyqg,"Table 1

| Ticket\_ID | Ticket\_Category | Start\_Date | End\_Date |
|:-|:-|:-|:-|
|123|a4| 1/1/2020 | 8/21/2020 |
|453|a5| 2/1/2020 | 12/31/2020 |
|567|a4| 10/14/2019 | 8/31/2020 |",1.0
g3jpf2e,ikbyqg,"Table2

| TIcket\_ID | Ticket\_Modify\_Date | Old\_Val | New\_Val |
|:-|:-|:-|:-|
|123| 5/5/2020 |Null|U1|
|123| 6/10/2020 |U1|U2|
|123| 7/5/2020 |U2|U3|
|453| 4/5/2020 |Null|U1|
|453| 8/8/2020 |U1|Null|
|453| 10/1/2020 |Null|U2|
|567| 11/1/2019 |Null|U3|
|||||",1.0
g3jpu43,ikbyqg,"Table3

|Ticket\_ID| Ticket\_Category |Year|Jan, Feb, Mar, Apr, May, Jun, July, Aig, Sep, Oct, Nov, Dec|
|:-|:-|:-|:-|
|123|a4|2020|Null,Null,Null,U1,U2, U3,U3,U3,U3,U3,U3,U3|
|453|a5|2020|Null,Null,Null,U1,U1,U1,U1,Null,Null,Null,Null.Null|
|567|a4|2019|Null,Null, Null, Null, Null, Null, Null, Null, Null, Null, U3,U3|
|567|a4|2020|U3,U3, U3, U3, U3, U3, U3, Null,Null, Null, Null, Null, |",1.0
g3k04zs,ikbyqg,"Am I to assume that the aggregate value in each ticket for each month is the 'U' value at the end of the month? Is that correct?

Edit: what if a ticket is modified more that once a month? Do you only want the last 'U' value or what? :)

Great tables btw, thanks for including them.",2.0
g3kmr76,ikbyqg,Thanks for the help! And you are right. I'm only interested in the last U value If the ticket is modified multiple times in a month.,1.0
g3ji09m,ikb3a7,"http://mystery.knightlab.com 

https://www.w3schools.com/sql/",13.0
g3jj5uj,ikb3a7,"Didn't knew the first one, very creative. Thanks",3.0
g3k625b,ikb3a7,The first one seems interesting.,1.0
g3k37gr,ikb3a7,Try to see if there are any old retired ornery SQL masters living in the same apartment complex as you who need to have their car waxed.,6.0
g3jr0b9,ikb3a7,"Sqlbolt, sqlzoo",3.0
g3jwphj,ikb3a7,pgexercises,3.0
g3k0qv3,ikb3a7,"for practice, also try hackerrank :)

if i remember correctly some of the questions were similar to what you would expect in an interview.",3.0
g3jk0g9,ikb3a7,I recommend the SQLDome,2.0
g3m275d,ikb3a7,Could you provide an url? Thx!,1.0
g3jpi18,ikb3a7,Check out datacamp and strata scratch. They provide interactive exercises to practice.,2.0
g3jymsb,ikb3a7,Strata scratch. It’s a Postgres backend.,1.0
g3jzq1k,ikb3a7,"What i'd recommend to help you practice to use the w3schools editor/database and formulate your own questions to practice. For example some questions you may have is:  
1. Who had the highest orders?

2. What is the single largest purchase ($$)

3. Can I find the name of the above?

You can often build upon your initial queries. This really helps out when you transition into a business analyst, or talking to stakeholders in general.",1.0
g3k4cyk,ikb3a7,"I just learned by doing. Create a database and learn how to create tables, columns, indices, triggers, functions, etc. Learn how to add/update/delete data. Learn how to select that data. Keep making up more and more complex relationships.",1.0
g3khwas,ikb3a7,"How about a free SQL course (link open til Wednesday night NY time). It's got heaps of practice on some of the basics and tries to bring everything to life to help you remember. Why don't you give it a try?

Given you're using Postgre (mine uses mySQL) you might like to focus on several of the lectures that are purely conceptual, with some really fun and interesting analogies to help you remember the concepts.

If you're interested, here's how to sign up in 3 quick steps:

1. Follow this link and watch the promo to check that the course is right for you: [https://www.udemy.com/course/sql-for-real-world-data-analysis/?referralCode=B1619CE91415B0D2C586](https://www.udemy.com/course/sql-for-real-world-data-analysis/?referralCode=B1619CE91415B0D2C586)
2. Enter password “maxSQL”
3. Dive in!

Otherwise, SQLzoo has some good practice questions too.

Cheers",1.0
g3kjv9x,ikb3a7,"I recommend codewars.com. It has exercises on various languages, including SQL.",1.0
g3mukeo,ikb3a7,"I recommend 

https://pgexercises.com/",1.0
g3j89fv,ik9nd8,"Use python to iterate over each line and each word, inserting into a table along the way.

www.geeksforgeeks.org/python-program-to-read-file-word-by-word/amp/

At that point you should be able to use any relational DB.

Not to sure if there is any easier way to get the contents into big query, but I know there are good python libraries for that as well.",5.0
g3jc12l,ik9nd8,"Solved! Thank you!

I'm sure there's an easier way to do it too; I can't seem to get it into BigQuery but SQLite seems to handle it no problem.",2.0
g3jczql,ik9nd8,Awesome!  Python is incredibly flexible and pretty straightforward.  You might even be able to do this without a database just using Pandas.  But your project sounds like a lot of fun!  Good luck!,1.0
g3jdt1e,ik9nd8,"Thanks! I tried using Pandas and I'm sure Python is easier - I'm trying to get an analyst job using SQL, gotta find fun projects for practice, you know?",2.0
g3j7jir,ik9nd8,"While conceptually you could create a table in a relational database from a text file by cramming everything into one column delimited by spaces and then run additional procedures to split out the punctuation, I am near-certain that there is a superior way to get this done with Python.  Perhaps try asking over there?  Also - what is the context around this project?",2.0
g3j81kh,ik9nd8,"Thanks! I'll give them a shot.

Context: US Political campaign speeches, how many different words are used by candidate, are some words used more frequently than others, length of the words used in their speeches",2.0
g3jm6j8,ik9nd8,"You’ve already solved the issue, but for future reading check out “collections.Counter” in Python. 

https://docs.python.org/3/library/collections.html#collections.Counter",2.0
g3ivo3p,ik70kw,OurSQL,4.0
g3j226i,ik70kw,"keep it simple and clear:  


SQL = language  


MySQL = database management system (DBMS)",5.0
g3jfpxx,ik70kw,SQL Server (Microsoft) or Oracle are the most common databases in the non-tech F500 world. I would lean toward both of them before the other ones. All the F500 companies ive worked @ use SQL server &amp; Oracle for mission critical stuff.,3.0
g3jveaw,ik70kw,Same.,2.0
g3iw6lh,ik70kw,I actually use both on a daily basis for the last 5 years or so. I would say if you want to go for a data analyst then practice with mssql/tsql first as alot of back office/erp systems are mssql based. Mysql is used in my case in web apps on linux servers. But as others have noted both are just implementations of sql study the concepts first and really the only difference is in syntax and some advanced features.,2.0
g3iqnrb,ik70kw,"Isn't MySQL just a flavor of regular old SQL?

Learn SQL.  That way you'll have the basic covered, whether you end up working in Oracle, MS SQL Server, MySQL, Postgres, or something else.",3.0
g3jfk4i,ik70kw,its an ansi compliant database but the syntax between all the db's differs slightly.,2.0
g3k4pvr,ik70kw,MySQL is probably the least ANSI compliant database out there,2.0
g3k8cm2,ik70kw,"nah, that'd be MS Access",3.0
g3kjhsc,ik70kw,GROUP_CONCAT is not ANSI SQL.,1.0
g3kore7,ik70kw,thank you for your valuable insight,1.0
g3kpxe3,ik70kw,Just thought it relevant based on the topic and your flair.,0.0
g3irqn8,ik70kw,"&gt; Does it really Matter ?

yes it does

what you seem to be asking is, whether there's a difference betwwen MySQL and Microsoft SQL Server -- yes there is

SQL on its own is a standard language, you could google it

SQL Server is Microsoft's attempt at SQL

MySQL was MySQL's attempt at SQL, but MySQL is now owned by Oracle

does this help?",1.0
g3jh9g6,ik70kw,"SQL is a standard used across multiple DB systems which each put their own touch on it, but the core is the same.

MySQL is one of those offerings.

From a data analysis perspective you're mostly just gonna wanna worry about learning the SQL language, as that will carry across most products and then once you feel comfortable with that you can explore learning details of different flavors.",1.0
g3in4ka,ik6oig,Probably integer division instead of floating point.,2.0
g3is345,ik6oig,"okay, let's start here --

    count(1)*10 / sum(TT.totalusers_per_test)*10 as pct_test

those 10s are going to cancel each other out

what you want is this --

    100.0 * COUNT(*) / SUM(TT.totalusers_per_test) AS pct_test",1.0
g3k9nfz,ik6oig,"thank you, this works!",1.0
g3ik4bv,ik64xd,"Let me give you a simple example to inspire you.

I have two tables:

Employee, has two columns, emp\_id and emp\_name

Salary, has two columns, emp\_id and yearly\_compensation

Suppose I want to see the name and salary for all employees. I have to join the two tables.

    select emp.emp_id, emp.emp_name, sal.yearly_compensation
    from employee emp inner join salary sal on (emp.emp_id = sal.emp_id) ;

In the example above, emp and sal are table ""aliases"" that are declared in the ""from"" statement to avoid typing.

I can apply functions in the select statement. I want to increase each salary by 10% (multiply by 1.1)

    select emp.emp_id, emp.emp_name, sal.yearly_compensation * 1.1 as raise_10pct
    from employee emp inner join salary sal on (emp.emp_id = sal.emp_id) ;

I want to create a new table with the employee name and new salary.

Create a table with emp\_id, emp\_name, raise\_10pct (look up the create table statement for your RDBMS). Something like

    create table new_salary
    (emp_id number, emp_name varchar2 (100),
     new_yearly_compensation number) ;

Populate my new table

    insert into new_salary (emp_id, emp_name, new_yearly_compensation)
    select emp.emp_id, emp.emp_name, sal.yearly_compensation * 1.1 as raise_10pct
    from employee emp inner join salary sal on (emp.emp_id = sal.emp_id) ;
    commit ;",3.0
g3il1oc,ik64xd,"Thanks a lot for the whole explanation man, you are a God!

I only have one question: in your example you manually typed 1.1 but in my particular case I have different amounts of VAT for each item so I can't just type 10%, I need to use the price of each product and multiply it for the VAT (this would be in another table) and apply it in a new column, how can I do that?",1.0
g3imp4m,ik64xd,"Let me fix my example. I have a third table that has ""employee raises"" because not all employees get the same raise.

emp\_raise has emp\_id, raise\_pct

If, for employee 20 raise\_pct will be 0.1 (=10% raise) then table emp\_raise would have a row with emp\_id = 20, raise\_pct = 0.1

and for employee 20 if the salary is 50K the new salary will be 50K \* (1 + raise\_pct) = 50K \* 1.1 = 55K.

Show new salaries for employees. The query below assumes that each employee has a corresponding row in salary and also a corresponding row in emp\_raise

    select emp.emp_id, emp.emp_name,
      sal.yearly_compensation * (1 + rs.raise_pct)
        as new_yearly_compensation
    from employee emp
      inner join salary sal on (emp.emp_id = sal.emp_id)
      inner join emp_raise rs on (emp.emp_id = rs.emp_id) ;",2.0
g3inxvl,ik64xd,"care to fix it one more time so I can get it better? hahaha

in my case, I don't have a column with the % in the same table, it's in another table as I show in my first post.

You know what I mean?

In any case, I think that maybe I should ""take"" the % into a new column in the table I'm creating, then multiply it by the cost and put the result in a new column.",1.0
g3ipabv,ik64xd,Look closely at my second example. I multiply the salary (which is in one table) by the raise percent (which is in another table.),2.0
g3irstj,ik64xd,"My maaaan! Thank you so much! One thing, why did I got just two results for the whole ""api\_articulo"" table?

[https://imgur.com/a/t1Ik7Qk](https://imgur.com/a/t1Ik7Qk)

/EDIT: Solved! Thank you so much! I owe you big time. Thanks for making me think about it instead of just doing it for me. You are awesome! :)",1.0
g3itae4,ik64xd,"Back to my example. I take the employees, look up their salary in a second table, look up the raises in a third table. What about people that don't get raises? I could have decided to put them in the third table with a raise of 0 %, or put them in the third table with a raise value of NULL, or just not put them in the third table at all.

Let's take the third case: in the table with raises I only put people that got a raise. But I want to show the ""new salaries"" for everybody, even people that didn't get a raise (though in that case the new salary wil be the same as the old salary.)

If I do a straight ""inner join"" I will only get back the employees that are in BOTH tables in the join. But I want all the employees to be returned, even if they don't have anything in the raise table. It will look like this

    select emp.emp_id, emp.emp_name,
      sal.yearly_compensation * (1 + coalesce (rs.raise_pct, 0))
        as new_yearly_compensation
    from employee emp
      inner join salary sal on (emp.emp_id = sal.emp_id)
      LEFT OUTER join emp_raise rs on (emp.emp_id = rs.emp_id) ;

The query above assumes:

* All employees are in the salary table
* Some (not all) employees are in the emp\_raise table, and if they are not in the emp\_raise table they are getting a 0% raise. The LEFT OUTER join will return all employees with NULL for raise\_pct if there is no row in the emp\_raise table.

Coalesce function : [https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#function\_coalesce](https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#function_coalesce)

Different types of joins: [https://oracle-base.com/articles/misc/sql-for-beginners-joins](https://oracle-base.com/articles/misc/sql-for-beginners-joins)",1.0
g3it10k,ik64xd,"One more thing, and I won't bother you again.

If I wanna add more fields below ""select"" and AFTER the one I used to calculate the cost price + VAT, I'm getting an error that says:

    SQL Error [1064] [42000]: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'articulo.aging,
    from api_articulo articulo
        inner join api_iva iva on (art' at line 8

This is how I'm doing it:

    select
        articulo.id,
        articulo.name as NAME,
        articulo.partnumber as PARTNUMBER,
        articulo.upc as GTIN,
        articulo.images,
        preciocosto * (1 + iva.tarifa) as COST
        articulo.aging,
    from api_articulo articulo
        inner join api_iva iva on (articulo.iva_id = iva.id)",1.0
g3itmpr,ik64xd,"&gt;select  
articulo.id,  
articulo.name as NAME,  
articulo.partnumber as PARTNUMBER,  
articulo.upc as GTIN,  
articulo.images,  
preciocosto \* (1 + iva.tarifa) as COST  
articulo.aging**, &lt;--- you've got a comma here**  
from api\_articulo articulo  
inner join api\_iva iva on (articulo.iva\_id = iva.id)

In a select statement, the comma means ""there's another column following this one""",1.0
g3ifrnw,ik4vyx,"I think I understand what you are trying to achieve. If I am interpreting you right, you will need to self-join this table on the Identifier and evaluate whether the OutputTime is greater than the succeeding ID for each product, like this:

    select t1.*
    from MyTable t1
      inner join MyTable t2
        on t1.Identifier = t2.Identifier
    where t1.ID &lt; t2.ID
      and t1.OutputTime &gt; t2.OutputTime
      and t1.Product &lt;&gt; t2.Product
    order by t1.ID;",3.0
g3ivw6n,ik4vyx,"Thank yo so much. I think I had almost the same query but used wrong alias name.

 t1.Identifier = t1.Identifier 

instead of

 t1.Identifier = t2.Identifier",2.0
g3ibh90,ik4vyx,What you wrote is a bit confusing. Could you post your target output?,2.0
g3j3kla,ik4vyx,"People will recommend self joins for this.  Be careful with them as they scale badly.

Consider using a window function.  Look up examples of LEAD and LAG.

I can't do the syntax off the top of my head...

I've had queries drop from 12 minutes to 15 seconds by replacing a self join with a window function.",2.0
g3jw6kt,ik4vyx,"Self joins are wasteful.

ANY ALL SOME probably the best solution, LEAD LAG very close second. 


Select * from table t1

Where outputtime &gt; all (

Select outputtime 

From table t2

Where t1.rowid&lt; t2.rowid

And t1.outputtime&gt;t2.outputtime

)",2.0
g3hvuj8,ik2e57,"Although it's not the DB implied by the flair, SQLite's documentation on WITH is excellent, and has a section called ""Controlling Depth-First Versus Breadth-First Search Of a Tree Using ORDER BY"": [https://sqlite.org/lang\_with.html](https://sqlite.org/lang_with.html)",4.0
g3i35qn,ik2e57,"Thanks I check it out this weekend, I not going to have time through out the day",2.0
g3i481p,ik2e57,Why not use an index?,3.0
g3i6m6j,ik2e57,"This really appears to be a case of a developer thinking they are smarter than the database engine, lol. I am sure your suggestion of an index is all they really need.",5.0
g3ihhax,ik2e57,"At first I was thinking row store vs column store, but I am not 100% sure anymore.",1.0
g3i8eg6,ik2e57,"If one has a tree hierarchy already defined then the type of search used (DFS vs BFS) will output different values.

And the resulting output may be of value in specific use cases.

Maybe the way some document is being formed?",2.0
g3i9ray,ik2e57,"Possibly. If you're not using an index. But if you are using an index you don't have to do that.

The point of a DFS/BFS is to find things fast. But an index does it without you having to implement a search algorithm. Finding things fast is already a standard feature of the DBMS.",1.0
g3ih0vy,ik2e57,"Sorry what you mean by index, I really new to SQL.
I not really searching, I trying to translate content into xml. The xml follow a depth first structure. Thanks for any help.",1.0
g3ii0wr,ik2e57,"You're accustomed to just rummaging around a pile of data to find a needle in a haystack. Databases don't work that way.

You don't ""search""  for things in a database the same way you do elsewhere. When you write a query, you tell the database ""look on these tables for rows matching these criteria, and give me this fields from them.""

You _usually_ want to be searching (and joining tables) on indexed fields. Tables and indexes are stored as balanced trees (there are exceptions, but since you're new you don't need to worry about this here) and the database engine uses those trees to efficiently locate and return the data - it walks down the tree to the leaf node where that data first appears, and moves on from there.

Do not attempt to ""outsmart"" the engine. Experienced people *sometimes* win at that game (I'm making a distinction between ""outsmarting"" and ""knowing how the internals work and playing to those strengths""), newbies 99% of the time lose - and that 1% win is luck.",1.0
g3in996,ik2e57,Okay got it. Thank you,1.0
g3ijv5y,ik2e57,"Indexes are created with the CREATE INDEX ... keywords. They are used to make searches faster. It's a standard feature of relational databases. But you only add indexes after you identify that there is a performance problem. 

You don't need to implement any search algorithm. 

An xml document is a tree structure. As such there is a root node and child nodes. You likely won't need to store a depth index. It's implied from the parent of a node. But you will need to store the parent id of a node. So you know to which parent it belongs.",1.0
g3in57k,ik2e57,Okay thank you,1.0
g3iir72,ik2e57,"If you have hierarchical data, you may need one or more tables. Or a single table with a self-referencing foreign key constraint for a parent/child relationship between pieces of data.

It's really hard to say what you need to do without knowing anything about the data you're storing.",1.0
g3hr6qp,ijy2in,"The business logic is hard for me to follow, so I'm just looking at your sample data and your two-level query.

I can probably show you how to use a recursive CTE to solve this, but I'm not sure how you plan on connecting from row i to row i+1. In your two-level example, it looks like you're connecting the two levels via the first level's endId = to the second level's endId, but I don't see how that makes sense given that endId is the same for all rows in the hierarchy.",2.0
g3hva8d,ijy2in,"Well - yes. How to output a level 1 to n I don't know. 

The table is a result of another recursive query (which I can not alter) - so I know that endID is always correct (or so I assume). The business side of that first query is simply to always return the current endId - regardless of which Id is given as input.

What I hope to achieve is something else. Analyse which kind of businesses have the most merges, when they happen, etc. 

So I might forget about endId and only use, id and new\_id. That would probable be the best learning experience - and I wold also 'control' if the other query is correct :-)

I guess output could be like this

    EndId, merge_level_one, number of ids
    EndId, merge_level_two, number of ids
    EndID, merge level...n, number of ids",1.0
g3inh62,ijy2in,"Please include your sample data as text in the question, not an image.

As sql create table and insert statements would be ideal.",2.0
g3kjzci,ijy2in,"    CREATE TABLE ""users"" (
    	""key""	INTEGER NOT NULL UNIQUE,
    	""co_id""	INTEGER NOT NULL UNIQUE,
    	""co_new_id""	INTEGER,
    	PRIMARY KEY(""key"" AUTOINCREMENT)
    )
    
    INSERT INTO users (co_id ,co_new_id)
    VALUES 
       (1,NULL),
       (2,3),
       (3,10),
       (4, NULL),
       (5,NULL),
       (6,3),
       (7,8),
       (8,NULL),
       (9,3),
       (10,11),
       (11,NULL)
       ;
       

I've tried my luck with variants of this query:

&amp;#x200B;

      
    with RECURSIVE ach (co_id, new, x) as (
    select u1.co_id, u1.co_new_id as new, 1 as x
    from users u1
    --join users u2
    --on u1.co_new_id = u2.co_id
    where (u1.co_id = 3 OR u1.co_new_id =3)
    union 
    select u3.co_id, u3.co_new_id new, x+1 as x
    from users u3
    join ach
    on ach.new = u3.co_id
    )
    select * from ach
    order by new, x

The results are something like this:

&amp;#x200B;

    co_id   new     x
    11		3
    11		4
    2	3	1
    6	3	1
    9	3	1
    3	10	1
    3	10	2
    10	11	2
    10	11	3
    
    
    

The inteded output should be like this:

    co_id	        new	x
    2		3	1
    6		3	1
    9		3	1
    3		10	2
    10		11	3
    11		NULL	?
    

Showing that co\_id 2,6 and 9 merged to co\_id =3 (level 1)

Company 3 changed to company 10 (level 2)

company 10 became company 11 (level 3)

company 11 is the final form (level 4 or 1)",1.0
g3gr0bi,ijy2in,"lmgtfy:

https://sqlite.org/lang_with.html",-2.0
g3hhozn,ijy2in,"A better way would be to work with a Graph Database! This kind of query pattern runs pretty well with graphs! Keep in mind it is very different than sql!
Take a look at TinkerPop with Gremlin or Neo4j with Cypher for example!",-4.0
g3gft19,ijw9o5,"with literally no details we can't even make a guess as to what your issues are.

Provide the following info and you may get a useful response:

- is the python code running on the same machine as the SQL server? Is your SQL shell which you say is instantaneous running on the same machine? If not, what's the bandwidth between the two machines?
- how many rows are transferred from the SQL server to the client?
- what SQL system are you using?
- what SQL library in python are you using?
- what is the query you're sending?
- what is the schema of the table(s) you're querying?
- what indexes are on those tables?
- how large are these tables?",8.0
g3gh59d,ijw9o5,"Sorry about giving less details. Please find my answers.
1) python code runs on digital ocean server whereas sql is in Azure. Not sure about the bandwidth.
2) I’m running 14 Select queries and each query is just 1 row. 14 queries for 164 sites. So 14*164.
Not sure about question 3
4) pyodbc
5) select statement
6) just a flat table
7) no indices
8) 14*164 rows

I know one thing for sure with regards to indices. Indices are auto added in ms azure so in that case if I insert records using python do I need to add again?

Thanks for your time!",5.0
g3ghng8,ijw9o5,"So you're running 14*164=2296 queries and it takes a total of 10 seconds to run all of these queries? And you get back 2296 rows? I would consider that very fast over a remote connection. If you need it to be faster, you should probably combine those queries into one query.",4.0
g3gi6j4,ijw9o5,Sorry it’s just 164 queries with 14 upserts and it took 10 seconds but when I ran it after sometime it worked fine,3.0
g3gk4yz,ijw9o5,"164 queries in 10 seconds is still not bad, is this too slow for you? Are all these queries to the same database server?",3.0
g3gkj2f,ijw9o5,Yes same database server. It wasn’t slow but I had run it many times before and it used to be fast. As I said after sometime it worked.,3.0
g3gluox,ijw9o5,"I'm pretty sure the answer is that your python/pyodbc is just running from a remote server and has to establish a connection to the DB whereas when you are querying through azure it is ""local"" so connection is already established.",4.0
g3gxv6o,ijw9o5,"Yeah this sounds like the issue to me, too, with the usual disclaimer of we don't have the entire picture. To /u/KrishnaKA2810 - is your Python program running from remote, and is it establishing a new connection with each query? Or are you using a connection pool?",2.0
g3h3m3g,ijw9o5,"mwdb is correct.  if you have a connection pool then im thinking the 10 seconds is simply connection time.   if you are connecting and closing between each statement then that will increase time exponentially because you are connecting to a remote host every time.   

if your python looks something like this.

    db = pyodbc.connect('blah blah')
    cursor = db.cursor() 
    cursor.execute('sql statement')
    cursor.close()

    cursor = db.cursor() 
    cursor.execute('sql statement2')
    cursor.close()

    cursor = db.cursor() 
    cursor.execute('sql statement3')
    cursor.close()


then you may want to pool your execute statements to one single connection. as long as the timeout isnt something stupid low it should run fine on the single connection.

so change it to the following.

    db = pyodbc.connect('blah blah')
    cursor = db.cursor() 
    cursor.execute('sql statement')
    cursor.execute('sql statement2')
    cursor.execute('sql statement3')
    cursor.close()",1.0
g3gga4g,ijw9o5,"Lacking any details, it could be any one of (or combination) of things:

* Database is configured with auto-close on, resulting in ""spin up"" time
* Network latency
* Slow I/O as you read data from disk to load into cache for the first time
* Python library loading &amp; compilation
* Query plan parsing &amp; compilation on first run (at which point it's in the plan cache and reused, hopefully)
* You have a bad plan in cache (for the parameters chosen) being used from Python, but getting a new plan when you run via SSMS
* While running the query from Python, something else was running which caused resource contention on the server, and it's not running when you use SSMS
* The slow response has nothing at all to do with SQL/SQL Server, but exclusively your PC or your Python code
* Other?",3.0
g3gjbjr,ijw9o5,Thanks. Please see my comments I’ve posted details.,0.0
g3gjn89,ijw9o5,"it seems like you might be better off running one query and keeping those data points local in the python code, then uploading after the code runs. running 14 queries seems like a bit much for one line each.",1.0
g3gjwgq,ijw9o5,Sorry can you explain what that means? I’m not sure if I understood it.,1.0
g3gkc22,ijw9o5,"If you are running 14 queries in the same table, it might be better just to run one query, and save each row to local variables or objects. Then you don't have to call the database each time.",1.0
g3gknxc,ijw9o5,Sorry it’s not 14 queries. It’s 164 queries for 14 upserts.,1.0
g3gl1b2,ijw9o5,All of the 164 queries are run one by one.,1.0
g3frdd3,ijrk55,"It's per database in express.  You can always get the developer edition, which is also free and doesn't have size limitations.",1.0
g3fsu3x,ijrk55,But we can't use for production uses right? What if OP was using it as a backend for his Web app or something,1.0
g3ft3rw,ijrk55,Developer edition is meant to be used as a test and dev server... nothing is preventing you from deploying to express for production.,2.0
g3ig0ys,ijrk55,can you connect to a dev server from a remote computer? i have my current sql server express server running on an old PC and i do alot of the sql development on my laptop over my home network,1.0
g3igp9e,ijrk55,"Yea I do that at home too. 

I have a computer, that is dubbed the monster, that has 2017 dev on it.

I connect on my laptop/or remote into the monster from my MacBook, and my wife connects to it via bootcamp on her iMac. Same time, same sql user. Sometimes I'm win auth and she's using a sql user. 

But yea. Its the full sql version that you can't use in production.",1.0
g3ih3rj,ijrk55,"cool, thanks. what does being in production mean though? like MSFT can see if its hosted on a legit server or being accessed by multiple people? i mean for what its worth my sql server express db is in production now as it scrapes the web all day and night ha..",1.0
g3iiimg,ijrk55,"A production environment is defined as an environment that is accessed by end users of an application (such as an Internet Website) and that is used for more than Acceptance Testing of that application or Feedback.


I can't find the msdn link for this.

Edit: if you are scraping data to figure out if you can build an app, that's not production.",1.0
g3imffx,ijrk55,thanks. yea its just for my personal consumption. i write alot of sql at work and do alot of ETL stuff but im not so much on the dba/architect side if you havent noticed. always making reports/munging data so im learning a bunch on my own,2.0
g3iizky,ijrk55,"In a standard environment, you have dev servers, staging, qa (sometimes swapped with staging) , and production (multiple servers for failover).

The developers edition is meant to save the company a ton of money off the first 2, sometimes all 3, stages of development.",1.0
g3i3bnt,ijrk55,"Correct. 10GB per database, not per SQL Server instance.. So if you have 10 databases, that's a total possible 100GB spread evenly over 10 databases.

Having said that, there is also the \~1.5GB maximum memory limitation on the buffer pool and the CPU is limited to the lesser of 1 socket or 4 cores. 

If your doing compute heavy ETL, I'd be more worried about the ram/processor bottlenecks than I would be the storage, as I'm guessing you're using the express instance as staging/workspace and then eventually offloading it to another database server/data warehouse.",1.0
g3if70s,ijrk55,its just a personal web scraper so i think ill be okay for now. i suppose in the future i will migrate to postgre if i build something that does more etl and requires lots of data'r,1.0
g3ecde2,ijiypk,"u/ORCH1D and u/Antagnostic

So 3 things:

Thanks for the help.

Orchid's reply isn't even showing up on the thread, had to manually check his profile to see the last reply: 

https://prnt.sc/u8l8hu 

https://prnt.sc/u8l9jr

Finally, since I AM trying to learn and not just look for the answers, some questions

    Why is it pulling from advisor **a** and not just advisor? EDIT WHILE STILL TYPING THIS WHOLE THING UP: As I typed and was rechecking code I think this is basically renaming the advisor table internally so that the user does not need to type in too much things like ""advisor.i_ID""?

    The s.[name] and i.[name], are these s.id and i.id from the advisor table being replaced in terms of attribute names or what?


Again, many thanks!",2.0
g3eh96y,ijiypk,"u/SteamV Weird that my reply isn't showing up! Glad you checked my profile. Here's a quick explanation of what's going on:

&gt; advisor \*\*a\*\* and not just advisor? EDIT WHILE STILL TYPING THIS WHOLE THING UP: As I typed and was rechecking code I think this is basically renaming the advisor table internally so that the user does not need to type in too much things like ""advisor.i\_ID""? 

\^\^\^ Yes! That's basically it. The ""a"" is just an alias for the advisor table.  So you can type a.i\_ID instead of advisor.i\_ID.

 

&gt;The s.\[name\] and i.\[name\], are these s.id and i.id from the advisor table being replaced in terms of attribute names or what? 

Not quite. Think of it this way... look at the two JOIN statements I wrote after the SELECT : 

    LEFT JOIN student s on s.ID = a.s_ID 
    LEFT JOIN instructor i on i.ID = a.i_ID

These two JOINS are linking the advisor (a) table to the student (s) table and the instructor (i) table by linking the s\_ID field on advisor to the ID field on student and the i\_ID field on advisor to the ID field on instructor.  I recommend researching a bit more on JOINS to fully understand the concept. Anyways, once you've made these links you can now pull ANY field from student or instructor table and it will match the ID from advisor as long as you start with the alias and then the field name. Try adding s.dept\_name to your query. This will also pull each STUDENT'S department name into the table as well. 

 

Sorry if this explanation sucks. My 2 year old is mashing teh keyboard. Look up JOINS!",2.0
g3elmrp,ijiypk,"This has been a blessing, I thank your candor!",3.0
g3es3hx,ijiypk,Happy to help - JOINs are a very big part of SQL (especially LEFT JOINs). If you spend a bit of time learning the basics of them you will be in a good place. Good luck and keep posting if you have questions!,2.0
g3e2yuz,ijiypk,What table is the advisor’s name stored on? I don’t see it in the link. On mobile tho.,1.0
g3e4tih,ijiypk,"Can you not see any of the tables?

here are some screengrabs:

Advisor table: https://prnt.sc/u8klmw

Student: https://prnt.sc/u8klrh

instructor: https://prnt.sc/u8klvc",1.0
g3e57hq,ijiypk,Yeah so which table has advisor_name? The advisor table has two id fields and neither appear to be holding name info. I see name fields for student and instructor but your question is asking about pulling the advisor’s name.,1.0
g3e5pe8,ijiypk,"There is no advisor name in the list, or at least ctrl+f did not show any.

Since there is no advisor name I'm under the impression that the advisor ID and name are the same as instructor ID and name.",1.0
g3e6e6y,ijiypk,"    CREATE TABLE advisor 	
    (
        s_ID			varchar(5), 	 
        i_ID			varchar(5), 	 
        primary key (s_ID), 	 
        foreign key (i_ID) references instructor (ID) 		on delete set null, 	 
        foreign key (s_ID) references student (ID) 		on delete cascade 	
    )

The foreign key agrees.",1.0
g3e7j04,ijiypk,but isn't that the instructor id from the instructor table and student id from the student table?,1.0
g3e6g69,ijiypk,"Cool. Your question is hard to understand because there is no way to know that the advisor name is the name field from the instructor table. Anyways, here's how to get the names of both STUDENT and INSTRUCTOR in the same table:

    SELECT s.[Name] as student_name, i.[Name] as advisor_name FROM advisor a
    LEFT JOIN student s on s.ID = a.s_ID
    LEFT JOIN instructor i on i.ID = a.i_ID

This code uses two JOIN statements to link the advisor table IDs back to the student and instructor tables, from there you can grab their names.

EDIT: Updated code to change fields to student\_name and advisor\_name",1.0
g3e9w3r,ijiypk,it looks like advisor is a junction table for m:n relationship between students and instructors. do your joins accordingly,1.0
g3e4cg8,ijiypk,It sounds like they are looking for usage of aliases. Try giving the *_3 tables_* in your *_2 joins_* an alias. Then alias the name column from each table to have the necessary appropriate labels as specified.,1.0
g3e7c2p,ijiypk,"select s_ID, i_ID from advisor

left join instructor on instructor.name=advisor.i_ID

left join student on student.ID=advisor.i_ID

&amp;nbsp;


I've gotten to here, how do I replace the ID's with their respective names?",1.0
g3ebhpm,ijiypk,"add ""[student.name](https://student.name) as student\_name, [advisor.name](https://advisor.name) as advisor\_name"" (without quotes) to your select clause.",1.0
g3e6ink,ijhjr6,i dont think mkdir takes a parameter in DOS,1.0
g3eml5k,ijhjr6,"Windows doesn't have mkdir. That's Linux.

Windows is MD.

Another clue this is a linux command is the minus P. That's a Linux mkdir argument to create the parents if needed.

MD already behaves like that so there's no need for an argument like that.",1.0
g3en31n,ijhjr6,Watched the video. he's using *nix via ssh.,1.0
g3dkcyv,ijgncg,"In your case the role is: The Fall Guy.

I am sympathetic though. Really sounds like you got given a steaming bed pan.

Is there one thing that you could focus on fixing first that would get some visible benefit in the short term?",24.0
g3e7bnc,ijgncg,"Haha, exactly what I came here to say.",2.0
g3duz26,ijgncg,"Working on something similar for a client right now. So far we've dropped the run time from 5.5hrs down to 2hrs with a simple query refactor and are continuing to look for performance tuning opportunities.

* What DBMS platform are you working with? SQL Server, Oracle, etc
* What ETL platform are you working with? SSIS, Informatica, Talend, etc
* Where are your most severe bottlenecks? Source, transformation, target? Tackle each one separately.
* Refactor ETL for incremental versus full load
* Analyze indexes to ensure they're supporting the ETL
* Tune DML for efficiency
* Once all of the above is done, analyze the server resource usage to determine if you have the horsepower for what you're wanting to achieve
   * Memory
   * Storage
   * Compute
   * Network",11.0
g3dzah7,ijgncg,"I feel bad for you.

BI is full of entryists these days and a lot of them are in offshore teams.

The cheapest way to train a team is pick a bad customer project and let them cut their teeth on it.

The role of  ""technical owner"" is to polish the resulting turd.

It's not a terrible job and you can use it for the same purpose: train yourself how it should have been done.

All the techniques for ETL and making Data Marts have been around for plenty of time. 

Extract the requirment from the old project and start rebuilding.

You should hopefully emerge as a hero, not a million dollar waste of space.",7.0
g3dym83,ijgncg,"It's quite a vague term and your explanation as to how you became a technical owner sounds familiar. Often where a project/solution is delivered by an external party,  when it comes back in-house someone becomes a technical owner as the external party want nothing to do with it and management need to be able to say that it is 'in hand' internally.

People will probably come to you now if there's an issue, even though you may little about it.

TL;DR politics",2.0
g3e1h3c,ijgncg,"As much as it sucks, this is also an opportunity - work on a plan and divide things into tactical and strategic.  It doesn't say what your SQL experience level is, but it sounds like some performance tuning on the data loads is the immediate concern, so see if you can identify the bottleneck and some way to band aid it and buy you some more time to get into a more thorough analysis.",2.0
g3f4l8t,ijgncg,"1. Identify and document baseline performance
2. Identify and prioritize the various ETL processes that feed into the board by their criticality to the business
3. Develop plans based on #2 to begin addressing issues
4. Execute

I've had many of these types of projects.  Sometimes the only way to move forward is to roll up your sleeves, grab a baseball bat, and start kicking ass.  You can do this.  Take a deep breath.  Go kick ass.",1.0
g3ilta0,ijgncg,"guess the first question is what you yourself want to get out of it: do you want to use this as a learning opportunity to make the most out of figuring out the problem, even if the end result will never work? or you want a serious attempt at fixing it and make a name for yourself? or investigate the root cause and proove that there is no salvaging this and whoever greenlit this at the top must go under the bus?

either way the easiest place to start is relevancy and frequency. in my experience most dashboards are out of touch with the decisions that can be realistically made from them, and just display data for the sake of displaying it. if you can't make a value added decision based on it, cut it. if you can't make a difference in the same week, don't reftesh it hourly. if the chart is always the same showing steady numbers, cut it. unless it's there to pacify the customers/bosses that nothing ever changes, in that case aggregate the fuck out of it.... you get the idea.

at the first try, see if you can reduce the functionality and displayed data to something more streamlined and actionable using common sense, and make that work to have at least something...then you can go back to unfucking the original vision, or sell them on the idea that less is more.",1.0
g3d0e33,ije18t,"Do you perchance have an index on that date field?  When you do WHERE date &gt;= '2020-08-01' and there's an index, it can use the index.

When you apply a function to that date column, the index can no longer be leveraged, because it's the outcome of the function that has to be applied. This means that it's applying that function to each and every row.

Using information from [this Stack Overflow post](https://stackoverflow.com/questions/1520789/how-can-i-select-the-first-day-of-a-month-in-sql) to get a formula to get the first of the month, your where clause then becomes

    WHERE date &gt;=  DATEADD(month, DATEDIFF(month, 0, @mydate), 0)

The trick is you want to avoid applying a function to the column within the table within a WHERE clause.  Performance tanks, as you've discovered.",69.0
g3dkf89,ije18t,Holy shit! I just ran it in less than a second! Strangers on the internet giving their time to help others give me hope for humanity. You guys are all awesome.,35.0
g3dlf0s,ije18t,"If you want more of this kind of thing, since it appears you're living in a SQL Server world, take a look at [SQL Saturday](https://www.sqlsaturday.com/).  Free one-day seminars where you can attend sessions on a variety of topics. It's a great way to learn, and with this nifty pandemic, a lot of them have gone online, which means you can attend events which aren't anywhere near your local geography.

You'll learn a lot of nifty techniques that will help you with things like this.",15.0
g3ghov0,ije18t,"I'm not sure what's going on with SQL Saturday right now. The currently-posted events are primarily virtual right now, but there's almost nothing posted for North America at the moment - only 4 events through February. Which isn't to say someone in NA couldn't attend an overseas event - but then you've got timezones to deal with.

I was hoping to do Montreal in November, but it's in-person and I can't even get over the border under current rules.",2.0
g3fdzqz,ije18t,"lol, Indexing is yo friend son.",3.0
g3d7m9n,ije18t,"&gt; The trick is you want to **avoid applying a function** to the column within the table within a WHERE clause.

that's the answer

besides the ""date difference in months past the zero date"" method, which is a bit of a hack, another way to do it is

    WHERE datecolumn &gt;= CONVERT(DATE,GETDATE()) 
                         - INTERVAL DAY(GETDATE()) - 1 DAY

ordinarily you would use `CURRENT_DATE` instead of `CONVERT(DATE,GETDATE())` but i'm not sure if MS SQL supports that particular sql standard

and if date interval arithmetic doesn't work either, then...

    WHERE datecolumn &gt;= DATEADD(DAY
                              , 1 - DAY(GETDATE())
                              , CONVERT(DATE,GETDATE()) )",12.0
g3ff498,ije18t,TIL thanks,2.0
g3d7oi2,ije18t,"In addition to /u/redneckrockuhtree's excellent answer, you want to do some research into *making a query SARGable* - SARG stands for Search Argument and it's basically writing the query in such a way that SQL server can run it in a set-based way rather than a row-by-row based way.",11.0
g3d8jho,ije18t,Have you tried maybe storing the result of your date function in a variable then use the variable in the query?,4.0
g3f6qm5,ije18t,Non-SARGable predicate. Yeah. That's gonna be a table scan.,3.0
g3d9kj2,ije18t,"Same answer every time: For any reporting period function, simply make a dates table. Index it correctly. Now the optimiser understands what you're trying to do and will give you a good plan every time 

Bonus: Now you can get multiple reporting periods in the same query and compare them in your BI viz",2.0
g3dlegn,ije18t,Quick question on the back of this. For a quick and dirty analysis would it be better to make a temp dates table and join to it than to include the dates you need in the select query?,2.0
g3dlmuj,ije18t,"If you have an optimiser estimation issue then yes, any table will do. SQLS is usually set for auto stats so that is a bit of a proviso.",2.0
g3dlnkf,ije18t,"&gt;  Now the optimiser understands what you're trying to do and will give you a good plan every time 

[Not necessarily](https://www.brentozar.com/archive/2020/08/date-tables-are-great-for-users-but-not-so-great-for-performance/)",2.0
g3dn8i1,ije18t,"Fair cop, I have seen this happen. You can usually optimise your way out of it. Functions can be a bit of a dead end for trying to optimise your way around the problem.",1.0
g3d08fp,ije18t,Can you post explain plans?,2.0
g3dni2a,ije18t,"I think you were downvoted since there was sufficient information there. _BUT_ I think this is the best answer; always run EXPLAIN and see what the query plan is, and more importantly, differs by.",2.0
g3dpuip,ije18t,"Likely the case - I definitely agree it can be solved otherwise, but I prefer to start with explains as a vehicle to show the “why” behind the answer.",2.0
g3g0c3x,ije18t,"Have you tried EverSQL.com? It will automatically optimize and rewrite your query, for free.

Specifically for your query, the database optimizer is not using an index, when your column is inside a function.",1.0
g3g7dfr,ije18t,"Seems someone answered, but if you need a fast query with a dynamic function field, you would make a virtual table to precompute that value, index that field, join that table and query.

It's a core principle of map reduce.",1.0
g3d26nk,ije18t,"A simple way to optimize queries is to perform any calculation/aggregation/function in 1 step and only grab what you need. For example - 

SELECT mydatemodified = EOMONTH([mydate], -1))
, Id
INTO #TEMP 
FROM sourcetable

Then you would select from your #temp table join to your source table (if needed), like this:

SELECT * 
FROM #TEMP
Mydatemodified =&gt; DATEADD(MONTH, -1, getdate()) 

If it's still slow, You can add an index to the temp table, and /or you can select mydate into #TEMP, and then perform the EOM function there. 

For me, this is the quickest way to realize performance gains on existing code/tables",1.0
g3ct295,ijd838,"always start with a **logical data model** -- paper and pencil is sufficient for most cases

""how many of these for each one of those?""

""are these things different things or versions of the same thing?"":

""what identifies a unique instance of one of these things?""

then i map out the table names, primary keys, and foreign keys

after that, i can start adding each table's various descriptive attributes",12.0
g3dxq5z,ijd838,ER - model (Entity Relationship) step by step :),1.0
g3fsclz,ijd838,"My tables fall into 4 categories. 90% of my tables have the same structure/columns in pattern A. 7%, same thing, pattern B. 2% pattern C, 1% pattern D. You could say 5/6 designs if you include Date (multi calendar support) and Geography, but those never change.


However, I tell sql that I need to model a new subject and it creates all of my tables/views/functions/procedures for that subject.


So my planning is really... has this subject already been abstracted/defined? Or is it truly a new subject?

If new subject: add new row, press button, ~100 objects created.",1.0
g3g0pmr,ijd838,I create a table with everything I need to store and then apply what's called normalisation. It is a nice and easy method to help you organize your tables,1.0
g3bo63g,ij1cfy,Definitely get a technical or finance degree/diploma. SQL jobs need you to develop domain/business expertise in whatever company you work for. For example let’s say you work for a big grocery chain. You need to understand the grocery business to be effective at your job.,4.0
g3bp7q8,ij1cfy,hmm i see ill take that into consideration thank you,2.0
g3cd54q,ij1cfy,"Checkout the course SQL for data science offered by UC DAVIS Coursera. This might help you, and practice on hacker rank and refer Doc's from w3schools. 😁",2.0
g3d5m4m,ij1cfy,noted thank you!,1.0
g3bfxoq,ij1cfy,"I imagine you could get hired with SQL and little or no experience for an entry level analyst job.   The pay will vary a lot by region, but the $30/hr that others mentioned sounds reasonable to me.   

I would caution against this route, as without any education (or substantial industry experience) this would probably be a dead end job.   You'd write SQL for reports and...that's about it.   With experience and/or a more complete education, your first job could be a stepping stone into virtually any other role within IT.  

I do think this approach might be okay for a student job or to see if you really enjoy SQL enough for 40 hours/week.",2.0
g3bp4sv,ij1cfy,sounds very reasonable ill take that into consideration thanks,2.0
g3flg08,ij1cfy,"Yes, it's possible to get a job with SQL without experience and degree. Learn it and start applying for entry level jobs. There are some great platforms to learn SQL I can recommend you; W3schools, Datacamp, Stratascratch, and Leetcode. These platforms can get you exercises from beginning to advance and can help you prep for the interviews too.",1.0
g3fmfd8,ij1cfy,appreciate it brother you gave me some much needed hope,2.0
g3alqap,ij1cfy,"Well, without a degree yes, without experience no. Experience here can be anything, and for example I got a lot of experience working for free and building a portfolio.

It isn't easy, but it becomes easier as you get older.

For me the starting salary was 60K/year with full benefits and a yearly bonus that I think was about 10% of my salary.

In terms of other languages to learn it will depend on what you do with SQL, for example if you want to get into statistics and data science (harder without a degree because of the math) then you might prefer to learn R, however Python is very flexible for data science and also allows a great deal of ETL.

You could also learn Java or C#.",1.0
g3bfjz0,ij1cfy,"Yes, no degree required, you have to have experience so you can show your can actually write SQL. Conversely a Degree would open the door.

Agreed, my first pay was contracting for around $30/hour no benefits.",1.0
g3bpaof,ij1cfy,"I feel as though sql and python are the way to go, btw would getting a 2 year associates at a community college help a little bit?",1.0
g3btx9c,ij1cfy,"Do you want to be a DBA? If not, get a math degree.",1.0
g3bu607,ij1cfy,i honestly dont know man whatever makes the most money,0.0
g3buo68,ij1cfy,"DBA route takes less thinking. Probably going to get a lot of hate saying that, but it's a much more linear field. ""Easy way,"" to make six figures a year.",2.0
g3c2lj1,ij1cfy,"Haha. Great Joke. There is a reason why we have 8 zillions of Java developer, and very few DBAs. Most developers are very bad at SQL and datamodeling. But the future is converged databases/Cloud/AI/Machine Learning.",1.0
g3dzydb,ij1cfy,"Oh, I didn't mean it like that... I meant comparing the DBA route to the data science route and all the statistics you'll need to leverage.",2.0
g3bxmlr,ij1cfy,sounds good thank you for all your help,0.0
g3bxzed,ij1cfy,"I'm currently working to get my associates. I landed a DevOps Analyst job with no experience professionally with software development or SQL. I am more than likely making on the low end of what someone in this position would usually make, mostly do to this company being relatively small. I'm gaining a ton of knowledge though. I'd suggest trying to learn the basics, and then trying to land an entry level job. Pursuing a degree definitely wouldn't hurt. I'm going for a software development associates.",1.0
g3by47d,ij1cfy,"kuddos, sounds good brother",1.0
g3a9kjo,iiyvmy,"This is a video of mine on the date dimension in my model.

At around 6:04 I make sql server execute python to pull data from holidata.net?.. a holiday site.


Maybe something similar will help.  You can use a job (assuming mssql because of powerbi)


https://youtu.be/t-aayfZJJ8c


Edit: I've also worked with oauth2 inside of sql via python like the example above. I've queried fitbit (not on my yt channel), and have a reddit bot in pure sql (exec external python like the date dimension vid).. thats been scraping this subreddit for about a year now.",1.0
g3aqb3v,iiyvmy,There are a few saas products that can automatically pull from common API sources and push into a db(fivetran &amp; matillion to name a couple). There’s a low technical barrier to entry to use them. Otherwise it’s probably going to be a python script.,1.0
g3ays48,iiyvmy,"My first guess would be automate a python script that uses API requests, does some kind of transformations, and loads the data in the db, then PBI easily queries that. I'm not familiar with how you'd handle any duplicate data other than using timestamps or some other unique identifier you may or may not have through the APIs.",1.0
g3ojx96,iiyvmy,Duplicate handling can be done by addressing a stored procedure via Python instead of directly inserting to a table. Let the database server do the work.,1.0
g3xk2bj,iiyvmy,"SPs are harder to work with (you won't have version control, harder to edit the proc)",1.0
g3xylxj,iiyvmy,Sounds like you’re set on reinventing the wheel. Have fun.,1.0
g45ydtl,iiyvmy,"It's not really reinventing the wheel, more like replacing the horse and buggy with an automobile.",1.0
g46qfns,iiyvmy,You’re so right. Why do we even need databases right? Let’s all do NoSQL.,0.0
g46r78f,iiyvmy,"No, NoSQL would not be efficient at all for many use cases.",1.0
g46rt1e,iiyvmy,You still looking for that Python based solution for this?,1.0
g46s4ih,iiyvmy,When was I looking for one? I suggested they could make an ETL type of process using Python. This would avoid the pitfalls of SPs.,1.0
g46ske2,iiyvmy,"Your complaints about procs aren’t accurate. It’s also the right tool for the job in the case of enforcing requirements on the data. I look for the right tool to get the job done with available resources. Sometimes that’s SQL, sometimes it’s Python. Sometimes it’s something else completely. Your opposition to procs seems based in doctrine not facts.",1.0
_,iiyvmy,,
g3bnp8c,iiyvmy,"I have used Microsoft Power Automate to run several different ETL (extraction translation load) pipelines. REST APIs are easier, but I have done a SOAP API. It is pretty easy to setup custom connectors with oauth or other authentication methods. 

I would recommend looking for something pre-made if you aren't a developer. Although Power Automate appears 'codeless', for most ETL use cases  you will need to know programming concepts to build a functioning pipeline.",1.0
g3bw9tk,iiyvmy,Check out Streamsets. Incredibly powerful open source ETL that can easily do what you’re describing.,1.0
g4fe0tu,iiyvmy,Thank you guys very much for the information. I'd never heard of ETL so that's something I'll figure out.,1.0
g3a7c95,iiyvmy,Saving this for later.,0.0
g3aw534,iixo0p,"Is the information you need in the [Availability Group dashboard](https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/use-the-always-on-dashboard-sql-server-management-studio?view=sql-server-ver15)? There are some dmvs that feed that dashboard that might help.

https://sqlperformance.com/2015/08/monitoring/availability-group-replica-sync",1.0
g3b4h18,iixo0p,"Good thought, and interesting link. I have it saved for next time (if there is one) because the last of the big DBs finished ""reverting"" about 3 hours ago.

Thanks!",2.0
g39i7br,iiwquu,"edited: [https://www.techonthenet.com/sql\_server/like.php](https://www.techonthenet.com/sql_server/like.php)

WHERE cust\_contact LIKE '\[JM\]'

It is looking for characters to match on \[JM\].

w3schools (great resource for all SQL related questions/ coding languages in general) - [https://www.w3schools.com/sql/sql\_like.asp](https://www.w3schools.com/sql/sql_like.asp)

&amp;#x200B;

I would also recommend using Data Camp as a means to learn SQL. You can get 2 months free by signing up to to use the benefits from Visual Studio.

Data Camp - [https://datacamp.com/](https://datacamp.com/)

Visual Studio - [https://docs.microsoft.com/en-us/visualstudio/subscriptions/vs-datacamp](https://docs.microsoft.com/en-us/visualstudio/subscriptions/vs-datacamp)

&amp;#x200B;

This way you can practice and learn (for free) for 2 months and if you want to continue learning all you need to do is register a new email (for another 2 free months).",16.0
g3bkv6a,iiwquu,"Your solution isn't right though as some other comments way lower stated the actual solution. You'll either have to use a LIKE OR LIKE, or a REGEXP to implement similar functionality as what the original example actually does.

LIKE '[JM]%' actually works out to be, at least in SQL SERVER, all names that start with J OR M not JM.",3.0
g39o1im,iiwquu,Seconding w3schools!,3.0
g3a8hzo,iiwquu,"see, that's the problem with w3schools -- they clearly show the square brackets being used in MS Access and MS SQL Server, but then they show you **[squadoosh](https://www.urbandictionary.com/define.php?term=squadoosh)** for MySQL

never use w3schools for syntax -- introductory training, okay, but not for detailed syntax",3.0
g3aw0bt,iiwquu,Agreed.,1.0
g3e3ojn,iiwquu,"Since bizarrely no one seems to have given you the actual answer for MySQL.

https://dev.mysql.com/doc/refman/8.0/en/regexp.html#function_regexp-like

    SELECT cust_contact
    FROM customers
    WHERE REGEXP_LIKE(cust_contact, '^[JM]') =1
    ORDER BY cust_name
As an aside, seems weird that its showing and matching contact and ordering by name, but whatever...",2.0
g3e464w,iiwquu,Thanks. This works and got the desired effect. Time to write this down in the book &lt;3,1.0
g3b94tm,iiwquu,I highly recommend becoming more comfortable reading the official docs and also use help sites like stack overflow and w3schools. Will be way faster for you to resolve minor issues instead of posting on forums that will take much longer.,1.0
g3b3ybl,iiwquu,drop the brackets and just use %JM% to return everything with JM in it. I think it's case insensitve also.,1.0
g3b4rdv,iiwquu,"That isn't what the brackets do in OPs example and it seems like others don't realize that either.

The brackets in this case implement pattern matching so that '[JM]%' actually finds any name that starts with a J or an M not JM.",3.0
g3bcpw3,iiwquu,"Yep, need to use an OR condition or REGEXP

    SELECT cust_contact
    FROM customers
    WHERE cust_contact LIKE 'J%'
    OR cust_contact LIKE 'M%'
    ORDER BY cust_name;",3.0
g3bgvav,iiwquu,"```sql
Select cust_contact
From customers
Where cust_contact regexp '^[JM]'
Order by cust_contact ASC;
```",3.0
g39apt5,iive8g,"i think you should have two tables for books -- `books` for the title and other descriptors (one row per book), and `book_copies` (each copy of a book tracked separately, multiple rows per `book_id`, composite PK of `book_id` plus `copy_no`)

then `loan` would have a FK to `book_copies` -- `book_id` plus `copy_no`

patron should not have a `loan_id` column, it doesn't need it, and besides, that would limit each patron to only one loan

you don't really need an `available` column either, as you can tell whether a particular book is available by comparing all `loan` rows for book copies with book copies registered in `book_copies`",1.0
g39d27l,iive8g,"expanding on that last point, here's the query to see if a book is available (i.e. if it has at least one copy not loaned out)

    SELECT CASE WHEN COUNT(*) &gt; COUNT(l.book_id)
                THEN 'yes'
                ELSE 'no'  END  AS available
      FROM book_copies AS c
    LEFT OUTER
      JOIN loan AS l
        ON l.book_id = c.book_id
       AND l.copy_no = c.copy_no
     WHERE c.book_id = 42937",1.0
g39ju5t,iive8g,I appreciate the insight as I want to improve my data modeling skills. This was just a practice database to run through concepts though.,1.0
g39avyf,iive8g,I would think that the patron and book IDs you would define as variables to be passed from the UI.,1.0
g39ens5,iive8g,You can have it inside a stored procedure and it is already refactored.,1.0
g3aeftk,iive8g,"I'm from a SQL Server background more than MySQL so apologies if any of this is inaccurate...

you can certainly put it in a stored procedure, pass data in as parameters and use them in your queries. So if you pass in the book title and author, you'd use that to look up the book ID, then use the ID to update the table as in your first query. One thing I'd add in to it as well is to wrap everything in a transaction, so it either all succeeds or it all fails - i.e. if something goes wrong with updating the patron's status then the first two queries are unwound and no data is unchanged.

running several related queries like this is exactly what stored procedures are for. I'm not sure what you mean by ""more complex queries"", it's often best to keep things simple as it means the code will be easier for others to understand and follow, and even for you to understand when you come back to it a few months later!",1.0
g3apyg4,iive8g,"Okay, I'll implement it with a transaction for practice. For ""complex queries"" that was the name of the lesson. I wasn't sure if anything more involved would be standard practice. Thanks",1.0
g39jt07,iiqm95,"What I think you should do is that you partition your tables. For example data for each year and month and then you can freeze your monthly data afterwards. If you need some statistics such as averages you could record /n/ and sum(prize) for example and use that for fast average calculations where you only need to access rows for the months where the precalculated stats don't exist.

If you need statistics such as standard deviation you should also calculate the fixed sum of squares. See  https://en.wikipedia.org/wiki/Standard_deviation for specifics. Quick sample standard deviation is 

Sd= (sum(x²) - avg(x)²)/(n-1) = (sum(x²) - sum(x)/n)/(n-1)",3.0
g39vsdc,iiqm95,"What type of time granularity? Is it adhoc? Hour/day/week/month/year?

I've approached this from a couole angles, all of them bounce (save to a new table) data.

Massive historical processing

Fork -&gt; lookup if range exists / process and save if it doesnt.


The table structure remained the same.

The easiest way to describe a limited version of my tables:

PK: headerFK: TypeId : startdateid : enddateid : measuretypeid1 : measure1 : measuretypeid2: measure2... 100

The Type M:N Family. Family M:N Class, Class M:N Realm.

I use the family to tell me the slice. Ex: DAY , WEEK, MONTH, QUARTER, YEAR, CUSTOM.

Each family has a Type of ALL, ACTIVE ONLY, INACTIVE ONLY, and other clauses i might need.

I use the column groups 2-100 (generically named for a reason), to store other necessary information for the range. 1 always stores exactly what the RCFT explains. Maybe 2 would be previous period and 3 would be next period, 4 would be the period before 2 and 5 would be the period after 3, 6 would be periods 2 and 4, while 7 would be 3 and 5. Etc.

The measureTypeid, same RCFT structure, but that id lets me know if its a sum, min, max, stdev, incident  risk, cumulative incidence.  W/e

There are some indexes to better physically store data,  type, header, and start for the cluster.

So if you had a date range for a month aggregate, and wanted to know the sum/avg/min/max. Its a simple query.

Select measure1, measure9, measure23, measure24
Where type in (family Month) startdateid and enddateid.

It may not look simple, but when you are dynamically creating these querys because the query construction is also data driven... you are essentially calling a procedure like..


Exec dbo.prc_Aggregates '1/1/2018', '1/1/2020', 'MONTH', 'ALL', 'SUM,AVG,MIN,MAX', 'JSON', @Results OUTPUT


And because its data driven, the non clustered covering indexes are created using the data that exists.",3.0
g36y1in,iihi2h,"I like to take predominant concepts and then go about distinguishing them. While doing so you inevitably hit new vocab that you can lookup as you go. This provides a pretty nice landscape on which to start your building. Broad concepts to start with might be different types of schemas and data storage varieties. Here’s a list to start with: data warehouse, transactional database models, Star schema, snowflake schema, fact-dimension models, unstructured data (such as document stores)",10.0
g374bc4,iihi2h,Hi this was very helpful. Im looking started with the idea of normalization and expanded from there. do you have any recommendations for resources to learn this?,2.0
g36zj1s,iihi2h,"I think a great place to start for this kind of knowledge would be understanding normalization.

https://en.m.wikipedia.org/wiki/Database_normalization

This page is pretty conprehensive.

Happy to answer questions if you reply or DM.",7.0
g373oup,iihi2h,"Hi, I have already gone through this given article and have studied normalization.",0.0
g371m58,iihi2h,"The design of the DB is generally done by DEVs and DBAs  help with tuning the model. It should be done by db architects, but that's costly and only done in big projects.

Focus on learning how to write proper SQL and its tuning. To give an example there was a credit card processing company i worked for that had some 1970s code which did a select before doing an update on a row  (which is dumb because updates ""have"" a select in them); that code in particular executed thousands of times per minute on a massive as400 file, and instead of storing a value showing that the select had succeeded and the value existed,  they kept doing it several times in the same code. 

Now that select took a few secs, multiplied by the number of times it was done in the code, multiplied by the 1000s of operations per sec... it starts adding up quickly. 

They also had  several tables storing the same info which was used  by several different pieces of code done by different teams... that was a fun re-architecture job.",3.0
g374max,iihi2h,thank you.Woah that story really paints things into light. I agree that focusing on writing good SQL. However I think that writing good SQL requires some knowledge of databases. Which is why I decided to take a detour towards learning more databases.,1.0
g373rpa,iihi2h,"Instead of reposting my youtube playlist / links. Lets chat!

Whats bothering you about what you are reading?

Why don't you feel efficient?

What do you know, that you don't know?

What made you come to the conclusion, regarding 2 areas of SQL?",4.0
g379s0o,iihi2h,"While Learning SQL I noticed that I was learning the language and how to execute actions, but when actually applying SQL I was encountering problems of how best to design the database that I would be dealing with. Should I know this the execution to see that the database was made was easy. So based on my experience it seemed that not only did I have to learn the language but also understand the thinking and design behind these. So I concluded that what I did not know was the the actual design aspect of databases. I don't feel efficient because I do not think that a solid foundation for all future dealings with databases is being laid. I simply don't understand databases or really the scope of it. Its a double blind situation, I need to know more in order to know what to learn, but to know more I need to learn first.",3.0
g37d3xr,iihi2h,"In order to design a database effectively, you have to understand the data that goes inside it. If you're learning by using someone else's database you're at a bit of a disadvantage before you start, because you don't really understand why the queries you're using have been written the way they have.

A useful exercise is to try and design your own database. Pick a subject you understand (a friend of mine who did this recently chose video games) and then try to list all the different bits of data you'd want to store (game title, synopsis, review scores, which studio it came from, what platform(s) it's on, what genre(s) it is, and so on). Then see if you can categorise the data into different groups (i.e. your tables). Think about how each piece of data relates to other pieces of data in your database, for example a game will only have one synopsis but it might be developed by more than one studio or have more than one genre. A genre can apply to more than one game, a game might have a different age ratings depending on what country it's sold in, and so on. The answers to these questions will determine your database design. As you progress you'll see where the rules of normalisation come in, and you'll split some data into separate fields, and spin other data off into a table of its own.

Bear in mind that there's no single right answer to database design, and you can have two totally different designs that will still be able to store all the data you need and that you can query to get the data out again. So instead of thinking ""is this correct?"" you should concentrate more on ""does this work?""

You will probably find that you design a database that works perfectly well for several months and then you need to insert some new data that doesn't fit your existing model and you have to change it. That happens all the time, a database is never finished!",6.0
g37ft01,iihi2h,"Learning the synatx is a must. All of it. Even the syntax people don't use, like ANY ALL SOME, because you will see ""self join"" solutions that are really horrible when it comes to performance compared to any/all/some. SYNTAX is your tools. Your limits.

So thats your Access part of sql. How you access the data.

You wont find much on the scope of a database. The average article writer doesn't think like that. Some approach it from a MVC mindset, some on a ' per customer ' mindset. Some approach design for what the business needs... there is no fundamental  design in the real word. Just methods and quotes about normal form.

The scope of the database is the system itself. Why do you have a database in that system? Is it just a junk drawer of tables and procedures? Maybe you should have a dbSystem database that houses tables pertaining to your systems objects: tables, columns, database, procedures, etc. A dbData database that stores your primary data. The scope of the database directly relates to what the dbms needs. What SQL needs... all data is the same, all companies store the same data, so every design should be the same... but its not. 


What I think you are looking for, is data modeling. Database design could be a 5 minute youtube video.

Data modeling, however. That's a dissertation. 

What columns need to belong in every table? Why?

Whats the purpose of a hair color field in a person table? Is it not just an attribute? What is the difference between height and hair color? Well, height is a measurement, it has a unit of measure, hair color does not.. but hair color refers to another subject area... color. What frequency length is Red? The frequency is a measure, it has a unit of measure.  Doesnt that mean that a Measure table can describe both an attribute of a person and a color? And the speed of a car, earths rotation, orbit around the center of a binary starsystem?  Do we really need measurement columns in 5 tables, or can we do it in one.. measurement table ... that all other subjects can use? 

How.. do we do it in 1 table... well we couldn't, we would want a way to Type that measurement. Possibly another classification table regarding the family of measurement... speed vs liquid measurement vs time vs currency... inches should not be in the same category as Fahrenheit. 

All of that is data modeling. Abstracting data as data. Being able to look past the column name. Being able to look at, and model, relationships between subjects, instead of adding another column to a table when you need it. No matter what data comes my way, car company, health data, reddit data, my model doesnt change. It doesnt need to change, it models data.

And you wont find that type of foundation in any article or Wikipage. Because nobody writes about it, and very few practice / understand it. You definitely wont find it in a single college lecture or udemy course. Thats why in the data industry there is such a huuuge skill gap. You could go from weakest team member to the lead in the data industry quicker than probably any other industry.



However, the simple fact that you are questioning what you are learning, and its efficacy, means you are doing great. 


Data is easy. You have data, it relates to other data. Foreign key, primary key. You can read any system out there with enough time poking around in it. Read random articles about data modeling. I like doing. Doing is learning for me, so grab some random data from r/datasets and model it at home. 

Getting to the 'my db builds its own objects and repairs itself, therefore I have no work stress' level... that takes discipline and determination. 


If you want to design this:

Http://www.elricsims.com/system

Go here:
 https://www.youtube.com/playlist?list=PLPI9hmrj2Vd_ntg2HACiHYeYl7iRvrgPb


Also, if you have any questions, you can always message me directly. Want me to review a model you made? Sure, I'm bored. I'm stuck at home in the middle of a pandemic posting on reddit.",4.0
g380koi,iihi2h,"You should take the online course Database System and application. It is available on many online platform like Udemy, MIT, EdX, NPTEL",1.0
g38vw16,iihi2h,I would recommend [this book](https://www.bookdepository.com/Database-Design-Relational-Theory-C-J-Date/9781484255391).,1.0
g39sxpq,iihi2h,"This helped me understand how relationships are formed it's a long video
https://youtu.be/ztHopE5Wnpc",1.0
g366cxo,iiedmj,"

    SELECT 
    Date
     , SUM ( CASE WHEN ID = 'ID1' THEN AMOUNT           END) AS ID1
    , SUM ( CASE WHEN ID = 'ID2' THEN AMOUNT END)    AS ID2
     , SUM ( CASE WHEN ID = 'ID3' THEN AMOUNT END)     AS ID3
     FROM
         Table 
    GROUP BY
          Date

This might work, please try it out and let me know",3.0
g366jtl,iiedmj,"Here's how you do it using `pivot`: [http://sqlfiddle.com/#!4/49635/2](http://sqlfiddle.com/#!4/49635/2)

Note the drawback is you need to hardcode all the values of `id`, or else use a dynamic query to generate the list of all values.",2.0
g36710v,iiedmj,"Hey, thank you so much; you make my exact example and provided the solution perfectly.

I think the part that was confusing me was the following:

    pivot (
      sum(amount)
      for id in ('ID1', 'ID2', 'ID3')
    )

the `sum(amount)` was throwing me off, since we're not actually summing over multiple items. Doesn't sql/oracle have some ""select one item"" option, which is faster?",1.0
g368rdl,iiedmj,"Yes, `sum()` is only used as sort of a dummy function here. If you have another row per combination of date and id, the amount will get added up, like so: [http://sqlfiddle.com/#!4/42336/1](http://sqlfiddle.com/#!4/42336/1)

So I think the question is how do you want it to behave in that situation? If the answer is this situation will never arise (perhaps due to the primary key), then really any aggregate function will do here, and they should all be blazing fast as they're only dealing with a single row.

~~But in general, you could probably use first() which just gets the first item according to order, and if order is not specified it just gets an arbitrary item:~~ [~~https://docs.oracle.com/database/121/SQLRF/functions074.htm#SQLRF00641~~](https://docs.oracle.com/database/121/SQLRF/functions074.htm#SQLRF00641)

~~However I just tried that on sqlfiddle and it didn't work, so perhaps it was introduced in Oracle 12c.~~

Hmm, actually, I think I misunderstood use of `first`, so please ignore the above part with the strikethrough. :)",1.0
g369st5,iiedmj,"Thanks for your thoughts. I have a followup: I'm realizing that I have (potentially) millions of `ID`s, but only hundreds of `Date`s, so it might be better to use `Date`s as columns and `ID`s as rows. I think this is the way to do it:

    SELECT * FROM (
        SELECT
          ID,
          Amount,
          Date
        FROM
          my_table
        WHERE ID IN (1,2,3)
    )
    pivot (
      SUM(Amount)
      FOR Date IN ('2020-08-01','2020-08-02','2020-08-03','2020-08-04','2020-08-05','2020-08-06')
    )
    ORDER BY ID;

A follow up is: do I have to do `SELECT * FROM` and then do a subquery? Can I just select immediately with the `WHERE` clause?

I'm asking because the query itself is going to be about twice as large since I have to put the `Date IN ...` twice.

Edit: nevermind, I'm selecting out IDs in the first query and Dates in the pivot part.",1.0
g369wxp,iiedmj,"Hey, how did you format your code in the comments? I've been trying it for the past 20 mins. Can you please tell",1.0
g36ddz8,iiedmj,"The easiest way is to install the Reddit extension. Forget what it's called, I'm in my car. I think it's something like RES. Otherwise, put four spaces before each line of code the create the code block.

Edit: it's called Reddit Enhancement Suite, it's a chrome extension. I makes formatting posts quite a bit easier.",2.0
g36goeb,iiedmj,"Thank you. I was trying the four spaces thing, but it was not working consistently.",1.0
g36imwc,iiedmj,"I'm on my computer now so I can respond a bit more fully.

In the future, if you want to know how someone did something on reddit, you can click the `source` link just below their post, which will show you the plain text of exactly what they entered.

You can do inline keywords (fixed-width font) by surrounding text with the backtick character \`, located next to your `1`. So typing \`this is fixed width\` shows up as `this is fixed width`.

If you want a block of text, use four indents. Make sure to put two newlines prior to the block, otherwise it won't show up, so in your reddit form it should look this:

&gt; This is a normal paragraph with text, followed by a newline

&gt; `&lt;blank line&gt;`

&gt;     four spaces before this line
&gt;     four spaces before this line too
&gt;    only three spaces here; see it how it got messed up?",2.0
g37zujw,iiedmj,This has been very helpful. Thank you,2.0
g3650i5,iiedmj,[deleted],0.0
g365s2j,iiedmj,"Sorry, what exactly do you mean? I have a `Date` column there.",1.0
g348vnd,ii2fgi,"    SELECT DISTINCT t.id
    FROM table t
    WHERE t.toolname IN ('a', 'b', 'c')
        AND NOT EXISTS 
        (SELECT 1 FROM table t1 WHERE t.id = t1.id AND t1.toolname IN ('d', 'e'))

If you're concerned with them being used on the same date, add 

    AND t.date = t1.date 

to the NOT EXISTS.",3.0
g3309v8,ihvtne,"I have a youtube series for building a fully featured end to end enterprise dbms.

The database file design and index design for the physical model.

A very consistent table design for subjects (instead of business data) and dbms functionality. (Row based security, self cleaning, time dependencies) 

And database design for purpose.


Not for beginners. 

Here's the playlist.

SQL Master Data Management - Complete Tutorial: https://www.youtube.com/playlist?list=PLPI9hmrj2Vd_ntg2HACiHYeYl7iRvrgPb


Edit: I don't go into relational algebra. If you have experience in architecture or design... you'll find some amazing patterns in here.  I'd put it in the DKNF normalization category, but its really in a league of its own.",34.0
g33bdoc,ihvtne,What do I need to learn to not be a beginner and understand your videos? Currently reading through the O'Reilly SQL book.,3.0
g33cgog,ihvtne,"I guess its not so much as... not for beginners... but more, I'm going to test your ability to abstract patterns. 

People are used to making tables with the data they see. A person table has a first name, last name, address, phone number. This is far from that. Names are their own subject, lots of things besides people use names. 

Flags are a subject. Lots of things use flags. Numerics, addresses, etc.

Some people are used to making a junction table for every relationship they have... but I only use 1 per db. You only need 1 relationship table to hold all your relationships.

I shouldn't say, not for beginners.  I should say... only for people who really want to test their skills and resolve.",6.0
g3319x8,ihvtne,Oh you weren't kidding. 50 video so far is pretty good. Usually you see comments say they got tutorial and its some half-assed made 3-minute vague video. Good stuff I added it to my watchlist for later tonight.,4.0
g331t5y,ihvtne,"You will not find that in this series lol. I hate those. 

These are data patterns you wont find in in your BS or MS degrees, and design patterns that will model anything... because it doesnt care about the data you are modeling. Data is just data, no company is 'special' (except faang).

First attempt at video tutorials on this were about 30-60 mins a vid.. i got them down to 15-20. Some patterns take a while.",6.0
g332m7r,ihvtne,What’s your day job?,5.0
g33d2ae,ihvtne,"Data architect is my title. I'm in the healthcare world. 

Medical equations, questionnaires,  ehr/emr. Complex data-driven decision trees for medical analysis. 

I am responsible for the design of*, and how we access, data. The SOPs for all data we store.",13.0
g347hnp,ihvtne,"What would you suggest is a good sauce for data modelling fundamentals?
I have been working with TSQL for more than 2 years now. But, I don't have a background in creating tables / modelling /normalization",1.0
g34y4vt,ihvtne,"Same series / play list link. 

Any series that goes over normal form 1-3... well you would basically have to unlearn that, for this model.

The problem with most videos on normalization is that there is no real structure. They may or may not go over what transitive properties are...and if they did, the commonly practiced idea is that a field (like first name) may actually be a property of a person. So they would say its OK to create a table with some of these values as columns. But in fact, the subject is Name, not person, because a person can have a crap ton of names and parts/food/etc can have names. I've never seen a video that dug deeper into actual normalization. Just another interpretation of a couple sentences.

If I found a video/series and got you saying ""the key, the whole key, nothing but the key; so help me codd""... and then you ran into my series... it would be counter intuitive.  Some basic video would tell you no duplicates in a table, where in my model the concept of a duplicate is irrelevant because a single record stretches across 5 tables AND this is a multi-tenant system. So my company could own a set of records in the Language table, and yours can too, but mine would be in a different order because I'm in the US. And you might be in a Scandinavian country, so you would want different languages on top for a better user experience.

I think 2 years of tsql programming is enough to jump into this series. The coding part isn't difficult... its the ideas/concepts and the mindset that sre challenging.",3.0
g35cjx5,ihvtne,I found Stanford's courses on edx pretty useful.,2.0
g35dyhy,ihvtne,Link?,1.0
g35mjph,ihvtne,I highly recommend this,1.0
g35sytc,ihvtne,"I recently asked a very similar question [here](https://www.reddit.com/r/SQL/comments/ih743y/need_ms_sql_server_dba_crash_course/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf)

Personally, I’m going through  [Udemy: SQL &amp; Database Design A-Z™: Learn MS SQL](https://www.udemy.com/course/sqldatabases/) \- SuperDataScience Team

And found anther [beginner Database Design YouTube course ](https://youtu.be/ztHopE5Wnpc) that’s very useful. 

Good luck, have fun learning!",2.0
g35vvbt,ihvtne,"&gt;Personally, I’m going through  
&gt;  
&gt;Udemy: SQL &amp; Database Design A-Z™: Learn MS SQL  
&gt;  
&gt;\- SuperDataScience Team

I have actually linked it in OP, lol. Cool, I will get it on next sale.

&gt;  And found anther [beginner Database Design YouTube course ](https://youtu.be/ztHopE5Wnpc) that’s very useful.

Indeed it looks useful. Thank you.",1.0
g35wpww,ihvtne,"Here’s a link to some [Udemy Groupons](https://www.groupon.com/coupons/udemy) to get courses for 9.99 today. 

The A-Z course is good but I would recommend doing the course in Postgre OR SQL Server not both. The course has duplicate instruction on each interface. You really only need to do one to understand concepts. 

Also another good, quick course to pick up is [SQL Database MasterClass](https://www.udemy.com/share/102p52CEsZeFpaRnQ=/)",1.0
g3608tj,ihvtne,"&gt;  Here’s a link to some [Udemy Groupons](https://www.groupon.com/coupons/udemy) to get courses for 9.99 today.

It's okay, we just had a huge Udemy sale end in Australia today. Tough I think they run it every month.

&gt;The A-Z course is good but I would recommend doing the course in Postgre OR SQL Server not both. The course has duplicate instruction on each interface. You really only need to do one to understand concepts.

I would like to learn both Postgres and SQL Server though. But I get what you mean.

&gt;  Also another good, quick course to pick up is [SQL Database MasterClass](https://www.udemy.com/share/102p52CEsZeFpaRnQ=/)

This course seems to be a bit advanced.",1.0
g34xqgk,ihvtne,"It sounds like you're interested in two areas that are frequently treated separately, database development (design) and data modeling.

The development aspects: creating objects like tables, schemas, views, and normalization are covered in fundamentals books. I like Itzhik Ben-Gan's T-SQL Fundamentals.  Despite its title there's a lot of breadth here.  He has a much larger book, *T-SQL Querying* that covers indexing and query optimization. Excellent resources.

I can't help you much on data modeling, but for that I would look at resources that use keywords like ""Business intelligence"" or even ""data science"" with SQL.",0.0
g34ydym,ihvtne,"&gt;It sounds like you're interested in two areas that are frequently treated separately, database development (design) and data modeling.

I wonder what gave you the impression that I am looking for Data Modelling as well? I am only looking for Database Design, and for that you need to establish Entity-Relationship models first.",1.0
g3347kd,iht0w0,Awesome thanks,2.0
g3ic1d9,iht0w0,Doesn’t work? Showing $12.99.,1.0
g3q7ue0,iht0w0,"Sorry about that, the coupon has now expired. Be on the lookout for future ones though...",1.0
g32jqya,ihszk7,"The equivalent of TRY-CATCH error handling in PostgreSQL is a block of code in this way:

[ &lt;&lt;label&gt;&gt; ]
[ DECLARE
    declarations ]
BEGIN
    statements
EXCEPTION
    WHEN condition [ OR condition ... ] THEN
        handler_statements
    [ WHEN condition [ OR condition ... ] THEN
          handler_statements
      ... ]
END;

Have a look at Postgres docs about Trapping errors

If you want to use it in your functions, keep in mind it can only be used inside PL/pgSQL functions.",1.0
g32e4wr,ihqgx7,"A couple of quick pointers:

The foreign keys are the ID values in a table that refer back to the primary key in another table. So in the Sales table, the CustomerID and CarID are both foreign keys. You should mark these as being INT fields, as they'll be the same data type as the primary key they refer to. Similarly, you've got MakeID in CarDescription as nvarchar(50), it should be INT so it's the same datatype as MakeID in the MakeModel table.

All the labels should be consistent, you've got dbo on some tables but not on others, and some fields are missing data types.",1.0
g3161br,ihkcxt,"Thanks both for your answers :).

I suppose I will use this joins when I do the insert or update of the values, right? So I will add the code with the joins to this stored procedure?

    INSERT INTO Maintenance
        (
        [Completed], [Completion_date], [Zone], [Likelihood], [Impact], [Risk_comments]
        )
    VALUES
        (
        @Completed_param, @Completion_date_param, @Zone_param, @Likelihood_param, @Impact_param, @Risk_comments_param
        )
    END

PS: In the main table the Impact\_ID and Likelihood\_ID can be empty",2.0
g30nicf,ihkcxt,"It sounds like you may want this. That ""extra"" table is a mapping between your Impact and Likelihood tables, so you would use that to join them together and then multiply the two values in your `SELECT` statement. I made these `left joins` assuming that the `Impact` table has all of the values that you want to retain. Read more about joins [here](https://www.w3schools.com/sql/sql_join.asp).

```
SELECT a.Impact_id
  , a.Impact
  , a.Impact_score
  , c.Likelihood_ID
  , c.Likelihood
  , c.Likelihood_Score
  , a.Impact_Score * c.Likelihood_Score as Risk
FROM Impact_Table AS A
LEFT JOIN Extra_Table AS as B
  ON A.Impact_ID = B.Impact_ID
LEFT JOIN Likelihood_Table AS C
  ON B.Likelihood_ID = C.Likelihood_ID
;
```",1.0
g30nyqv,ihkcxt,"The way to reference values in other tables in SQL is with joins. There are several types, I recommend going to YouTube and watching a few videos on it first. It’s not difficult but it can be challenging to grasp if you’re not used to set based logic.

So the other table will hold a list of events and you’ll directly populate the IDs for impact and likelihood?

Write a select statement from that data table and left join to the risk tables. Join on the IDs of the risk tables. In your select statement add your equation for the risk score and name it RISK_SCORE (or whatever).",1.0
g348icw,ihkcxt,"Finally and thanks to your comments I finally got it working in the stored procedure. This is what I did

Insert procedure:

    INSERT INTO Global
    	(
    	[Completed], [Completion_date], [Zone], [Likelihood], [Impact], [Risk_comments], [Score]
    	)
    VALUES
    	(
    	@Completed_param, @Completion_date_param, @Zone_param, @Likelihood_param, @Impact_param, @Risk_Comments_param, 
            (
    	SELECT I.Impact_Score * L.Likelihood_Score
    	FROM Impact AS I 
    	JOIN Likelihood AS L
    	    ON I.Impact_ID = @Impact_param AND L.Likelihood_ID = @Likelihood_param	
    	)
    	)

Modify procedure:

    UPDATE Global
    	SET
    	    [Completed] = @Completed_param
    	    ,[Completion_date] = @Completion_date_param		
    	    ,[Feedback_date] = @Feedback_date_param
                ,[Zone] = @Zone_param
                ,[Likelihood] = @Likelihood_param
                ,[Impact] = @Impact_param
                ,[Risk_comments] = @Risk_comments_param
                ,[Risk] =
                    (
                    SELECT I.Impact_Score * L.Likelihood_Score
                    FROM Impact AS I 
                    JOIN Likelihood AS L
                        ON I.Impact_ID = @Impact_param AND L.Likelihood_ID = @Likelihood_param	
                    )
            WHERE [ID] = @ID_param

Regards",1.0
g38rcyp,ihkcxt,"Glad you got it working! 

From a ""best practices"" and readability standpoint, I would suggest being explicit about what kind of \`join\` you're using in your query. You're specifying \`join\`, which by default, is typically an \`inner join\` meaning only values that exist in \`Impact\` and \`Likelihood\` will be returned.",1.0
g3kffj3,ihkcxt,"Hi, 

Thanks for the comment; I changed `join` by `inner join`",1.0
g30jbni,ihjg3h,Yes.,1.0
g30wgfq,ihjg3h,"Depends...

I don't mind if the ERD is constructed using only Primary and Foreign Keys to save space....as long as there is a way for me to see the full table details later if I am interested.

[https://dbeaver.io/](https://dbeaver.io/) can generate the ERD for you if the database already exists.

It has an option to generate using the following:

* all fields
* all keys
* primary keys

I sometimes just need to see how tables relate to each-other. All keys would do just fine for me then. At that moment I am not interested in the extra columns.

When have been handed an ERD diagram to implement, 100% I need all the columns AND their constraints.",1.0
g31qboc,ihjg3h,It is quite common to have ERDs at different levels of abstraction. https://www.visual-paradigm.com/guide/data-modeling/what-is-entity-relationship-diagram/,1.0
g31wenb,ihhotv,You should read about how they compile oracle.   It's kind of insane.,2.0
g321bqw,ihhotv,"Sounds like a good read but oracle is a company not a specific product so I can't find anything, link?",0.0
g32263u,ihhotv,"Hmm... I would say the name oracle is synonymous with their DBMS more than anything else but am thinking compile is overused keyword as you compile things in the database system as an oracle developer.

Here's the story: [https://news.ycombinator.com/item?id=18442941](https://news.ycombinator.com/item?id=18442941)",4.0
g30py36,ihhotv,How much and where did you avail that training?,2.0
g348pti,ihhotv,"Company sponsor, directly through Oracle so I have no idea but would estimate about £6.5-£7k for everything.",1.0
g317ylh,ihhotv,Nerds are usually bad at that stuff so it kinda means it will be a grade A education,1.0
g30fp4z,ihh1v0,Use pandas.,2.0
g5jlttd,ihh1v0,Just to let you know that i coded a garbage app that does it the way i described it above. nerver managed to make pandas work for this particular use case...,1.0
g2zyt1u,ihfep4,I am not understanding what you are trying to do here. You have a between operator in your case statement and I am not sure what it is supposed to do. Are you setting IH invoice date to a certain range based on the current dates? Do you want IH invoice date to actually have the value “between date and date“?,1.0
g303gst,ihfep4,"We use a fiscal calendar and the start and end dates of our months are different from a normal calendar. So, I am trying to write a where statement saying if today's date is withing fiscal month 9 then run for the fiscal month 8 range. Then nest this logic out for the rest of the year so that I can have a program run and export the files. 

Yes, I am trying to have the IH invoice date select a date range to run for based off of the current date. Typically how I would set it if I were to run it for a range would be:

Where 
IH_Invoice_date between '2020-07-20' and '2020-08-21' 

However, I am trying to automate this and need to get the additional logic so that it can run for our fiscal months. 

Thanks - Alex",1.0
g2zyzcf,ihfep4,"You need to read about date functions (to convert a string into a date) and date literals.

A date literal would be DATE '2020-08-31'

SELECT \* FROM MYTABLE WHERE MYDATECOLUMN &gt; DATE '2020-08-31' ;

[Teradata DateTime literals](https://docs.teradata.com/reader/S0Fw2AVH8ff3MDA0wDOHlQ/XbMSHicspRH~BFPuh77KuQ)",1.0
g304fre,ihfep4,"Thanks, I read up on literals but I'm still not sure on how to build out the if then logic I discussed in my previous comment",1.0
g4pfdiq,ihfep4,"Hi [Nerd3001](https://www.reddit.com/user/Nerd3001/)!

I have asked our Teradata users for help.

Hope you find the following link useful:  [https://techsupport.teradata.com/community?id=community\_question&amp;sys\_id=b66c7217dbfe5850e921ebcad39619a6](https://techsupport.teradata.com/community?id=community_question&amp;sys_id=b66c7217dbfe5850e921ebcad39619a6)",1.0
g30qe8q,ihdonp,"Oracle is expensive as fuck and is generally used in very large companies

PostgreSQL is more often used in startups

What kind of organization do you want to work for?",1.0
g311m0t,ihdonp,I honestly don’t know. I would be open to either large or small Corps but mostly I’d be fine with whatever will keep me relevant and in-demand longer haha,1.0
g328w62,ihdonp,"Oracle is not expensive. Those day are in the past. If you want to try your SQL skills and much more. 

Oracle Cloud Free Tier.

No time limit
Always free
Two Databases 20G storage
Two 1GB RAM Linux VMs 
100GB block storage
10GB object storage
10 GB archive storage
10 Mbps load balancer
10 TB/Month Outbound data transfer
Free tools like Oracle Application Express (APEX) and Oracle SQL Developer.

If you want a local/on prem install. Install Oracle XE. Free of charge, have almost all the functionallity as the Enterprise Edition.",2.0
g329fem,ihdonp,Thank you,1.0
g329p1w,ihdonp,Anytime. Good Luck,1.0
g2zx3zc,ihdonp,As a side note: the database is called PostgreSQL or Postgre**s**,1.0
g2ysi15,ih9rrr,"You could create a table of IDs that simy doesn't skip, put it on the outer side of a join to use in your sort.  You'll need to convert the nulls into zeros before adding it all up, but that's pretty trivial.

Or perhaps you could also pull in the windowed ID along with the value and conditionally add previous value based on the difference between ID and previous ID.

I've only built a few window functions and not had to deal with skips (the skips are specifically what I was looking for).",2.0
g2yszom,ih9rrr,"&gt;Or perhaps you could also pull in the windowed ID along with the value and conditionally add previous value based on the difference between ID and previous ID.

I'm not sure exactly what you mean by this, but it seems like it might be something that would work. I don't want to add in a bunch of NULL/0 rows in the data.",1.0
g2zb7yn,ih9rrr,"OK, now that I have a proper computer in front of me...

You already know how to use a window function to pull stuff from the previous row, right?

Value, ValueBackOne, ValueBackTwo, ValueBackThree

Repeat for your ID.

ID, IDBackOne, IDBackTwo, IDBackThree.

Now when you add the values, put a condition on it.

    Total = Value + 
       (CASE WHEN IDBackOne   &gt;= (ID - 3) THEN ValueBackOne   ELSE 0) +
       (CASE WHEN IDBackTwo   &gt;= (ID - 3) THEN ValueBackTwo   ELSE 0) +
       (CASE WHEN IDBackThree &gt;= (ID - 3) THEN ValueBackThree ELSE 0)

By doing this, if one or more IDs is too old, it'll get zeroed out and not counted in your total.  It should also neatly handle any NULLs that creep into your data, since ""$null &gt;= -2"" still does not evaluate to $true.

The way I've written it is not really single statement.  Use a CTE, or in-line the Window functions right inside the CASE statements.  Either will work, and should perform the same (though a temp table will need a little extra memory).  I recommend a CTE for readability as in-line will make those CASE statements stupid long.",2.0
g2zcgd4,ih9rrr,"Yeah, I use lots of CTEs. So basically populating the values I need into separate columns in advance. I think that would work!

I've done that before for other issues, but never thought about it for this one.",2.0
g2yv6hn,ih9rrr,Walking doggo.  I'll explain better a little later.,1.0
g2zgnuj,ih9rrr,"Well one way is a sub-query as follows. It will be really bad in performance though.

    SELECT t.id, t.value, t.customer
        , RequiredValue = ISNULL((SELECT SUM(t2.Value)
                                  FROM dbo.table t2
                                  WHERE t2.Customer = t.Customer
                                    AND t2.id BETWEEN t.id - 3 AND t.id
                                 ), 0) 
    FROM dbo.table t",2.0
g2znaww,ih9rrr,"Actually, this is a pretty decent solution. I was about to post another idea, but I just tested yours vs mine and yours is 97% faster. I'm saving yours in my toolkit. Thanks.",1.0
g2ytxrv,ih9rrr,why not simply use a join?,1.0
g2z9e91,ih9rrr,"Self joins don't scale very well.  (I've seen people use a self join to find the next chronological event - it gets very bad very fast, and by the time there's a million rows, a query that should take 15 seconds needs a whopping 12 minutes.)",1.0
g34zgbq,ih9rrr,"I'm not sure the solution you posted aligns with the description. For example the second to last row for id = 8 has a solution of 1 but for ids 7,6 and 5 the total is 2. I'm also not sure having multiple rows with the same id is correct.

Regardless of those details my solution is also another approach. I'm using a tally table to create all possible ids and then left joining the actually existing ids. This lets me use a window function to sum over ""missing"" rows.


    DROP TABLE IF EXISTS #table -- Only works on 2016+
    
    CREATE TABLE #table (
        id int,
        [value] int,
        customer int
    )
    
    INSERT INTO #table
    VALUES
        (1,1,1),
        (2,0,1),
        (4,1,1),
        (5,0,1),
        (6,1,1),
        (3,1,2),
        (4,0,2),
        (5,1,2),
        (8,0,2),
        (9,1,2);
    
    WITH tally AS (
        /* Tally Table 
           Source: https://www.sqlservercentral.com/blogs/tally-tables-in-t-sql
        */
        SELECT
            ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS n,
            0 AS v
        FROM       (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n)
        CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n)
    ),
    tally_join AS (
        SELECT
            #table.*,
            SUM(COALESCE([value],v,0)) OVER (ORDER BY n ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING) AS sum_ids_lt_three_back
        FROM tally
        LEFT JOIN #table
        ON tally.n = #table.id
    )
    SELECT *
    FROM tally_join
    WHERE id IS NOT NULL
    ORDER BY id
    
    DROP TABLE IF EXISTS #table -- Only works on 2016+",1.0
g36fgkt,ih9rrr,"The id column isn't a primary key, I could've made that more clear. The primary key for the table isn't relevant in this case.

Customer 2 only has ids 3,4,5,8,9; they don't have values for 7 or 6.",1.0
g2ym9uu,ih8lbr,"So you could probably do 

    SELECT 
        city,
        SUM(total1) as Total1,
        SUM(total2) as total2 
    FROM (
    SELECT c.receiver_city     AS city,
           count(c.order_no) AS total1,
           COUNT(NULL)       AS total2
    FROM core c
    GROUP BY city
    
    UNION
    
    SELECT c.sender_city       AS city,
           COUNT(NULL)         AS total1,
           count(c.order_no)   AS total2
    FROM core c
    GROUP BY city)
    GROUP BY 1
    ORDER BY city",3.0
g2zpm9k,ih8lbr,"The way to handle this depends on how to treat the cases you haven't mentioned. What do you want to happen for each of these cities: 

    city         | total1  | total2 
    ---------------------------- 
    detroit      | 2018    | 1147
    detroit      | null    | 1023
    
    portland     | 2914    | 7823
    
    washington   | 4401    | null
    
    seattle      | null    | null
    seattle      | 4201    | null
    
    chicago      | null    | null

At the very least you'll want to build queries to check if any of those cases ever happen / show that they don't.",2.0
g2zdqjc,ih8lbr,"    WITH 
        receiver AS(
            SELECT c.receiver_city AS city,
                   count(c.order_no) AS total_receiver
            FROM core c
            GROUP BY city
        )
       ,sender AS(
            SELECT c.sender_city AS city,
                   count(c.order_no) AS total_sender
            FROM core c
            GROUP BY city
        )
    
    SELECT COALESCE(r.city,s.city) AS city
           ,r.total_receiver AS total1
           ,s.total_sender AS total2
    FROM receiver AS r
    FULL OUTER JOIN sender AS s
        ON r.city = s.city
    ORDER BY city

I think this might work",1.0
g2zw0ci,ih8lbr,"I think the easiest way is to unpivot the data, then aggregate:

```sql 
select t.city, 
       count(t.order_no) filter (where t.type = 'receiver') as total1,
       count(t.order_no) filter (where t.type = 'sender') as total2
from core c
  cross join lateral (
     values (c.receiver_city, c.order_no, 'receiver'), 
            (c.sender_city, c.order_no, 'sender')
  ) as t(city, order_no, type)
group by t.city;
```

This is more or less an ""inline union"" but also identifies each row if it's taken from the receiver_city column or sender_city.",1.0
g2ybsn3,ih7ebh,"I believe the first is considered a correlated subquery. The second is a non-equi-join.

The honest difference is that most people will probably avoid correlated subqueries, even if they understand how to use them well, at all costs. In terms of performance, it's a tossup that varies by situation but, again, people will go with what they're more comfortable with anyway.

edit: just grammar for clarity",2.0
g2yejzh,ih7ebh,"Thanks for your response !
So which one would you say you prefer ? :p",1.0
g2yf9sr,ih7ebh,"The second is more clear to analyze, both immediately if you need to troubleshoot and verify and for anyone coming after (including yourself six months down the road) who needs to modify or reuse.",3.0
g3068av,ih7ebh,"One thing i've noticed is that it seems impossible to duplicate rows (multiple matching values) with a correlated subquery. If there we do, we obtain the error message

&gt;Msg 512, Level 16, State 1, Line 2 Subquery returned more than 1 value. This is not permitted when the subquery follows =, !=, &lt;, &lt;= , &gt;, &gt;= or when the subquery is used as an expression.

Is that one key difference ?",1.0
g376kg5,ih7ebh,"

code below.

in my random data test, the first query is much less resource heavy than the second query. The GROUP BY is unnecessary. It causes sql to do more work.

edit: the second query also removed students who had a 0 count. whos didnt have anyone older than they were.

The first query, is in bad form. You should not write sql like that.

That being said... this query has the exact same execution plan as your first query, down to the agg stream placement.


SELECT s.sno, s.sname, s.age, x.count

FROM student s
OUTER APPLY
(
SELECT COUNT(1) xcount

FROM student s2

WHERE s1.age&gt;s2.age

)x


Sample code i used in my test.


      declare @students table (id bigint identity(1,1), names nvarchar(100), age bigint)
      
      declare @countmin bigint = 1
      declare @countmax bigint = 100
      
      while @countmin &lt;= @countmax
      begin
      insert into @students (names, age)
      values(
      concat(char( ROUND(RAND()*(256-1)+1,0,1)),char( ROUND(RAND()*(256-1)+1,0,1)),char( ROUND(RAND()*(256-1)+1,0,1)))
      ,ROUND(RAND()*(20-8)+8,0,1)
      )
      set @countmin = @countmin +1
      end
      
      select *, (select count(1) from @students s2 where s1.age&gt;s2.age)
      from @students s1
      
      select s1.names, s1.age, count(1) 
      from @students s1
      inner join @students s2 on s1.age&gt;s2.age
      group by s1.names, s1.age
      
      select s1.names, s1.age, x.counts
      from @students s1
      outer apply 
      (
      select count(1) counts
      from @students s2
      where s1.age&gt;s2.age
      ) x",2.0
g2yd3zb,ih743y,"Entry level DBAs wouldn't generally get involved in query optimization, normalization, etc. Not to say you shouldn't learn those things but that may not be as beneficial as knowing how to install and configure SQL, security, maintenance and job scheduling and backup/restore. 

With that being said my suggestion would be [SQLSkills Accidental and Junior DBA course](https://www.sqlskills.com/sql-server-training/accidental-and-junior-dbas/). I taken their advanced courses and they are fantastic. They may also have some related courses  on Pluralsight.",9.0
g2yhb75,ih743y,"This looks awesome but is the course you’re referring to the immersion event or one of the online classes. The link drives to the immersion course that costs $3,300",3.0
g2ykc0k,ih743y,"Yes they are expensive courses, but well worth it especially if your employer will pay for it, and while probably not offered before your interview. I do highly recommend their courses. Here is a list of the [courses](https://www.sqlskills.com/sql-server-training/online-training/) offered through pluralsight which I think is $30/month. 

You may also want to download the developers edition of sql to play around with or follow along with.",3.0
g2yneia,ih743y,"Thanks for the list man I appreciate that! And I have the SSMS developed edition downloaded and am playing around in SSIS. It’s a neat little package. 

Reminds of visual job flow scheduler similar to an airflow + python setup to ETL, but inside the Microsoft warehouse.",2.0
g2yubtt,ih743y,"If you're at a more basic level of ""what do I use to query SQL Server"" then you might need to start here:

https://www.brentozar.com/archive/2020/07/how-to-get-started-with-sql-server-management-studio/

Note: SSMS is not the SQL Server engine, you might also need to [install the Developer edition](https://www.sqlservertutorial.net/install-sql-server/) on your computer if you have no SQL Server anywhere.


If you want a free introduction to SQL Server, this is another *great* course from Brent Ozar:

https://www.brentozar.com/training/think-like-sql-server-engine/


If you have a few dollars ($39), check out Brent Ozar's Fundamentals of DBA course. It's on sale this week:

https://www.brentozar.com/archive/2020/08/updated-fundamentals-of-database-administration-class-just-39/

And if none of this seems interesting to you... you may not really want a DBA job :D",9.0
g2z5yrb,ih743y,Perfect response here,4.0
g2ygc6m,ih743y,"I'm not a DBA myself but from what I understand it's more about maintaining the server and databases rather than being concerned with the actual data. So you'll be concerned with server availability, backups, restores, failovers, user accounts and permissions etc. You might get involved in query optimisation but I think only for queries that really have an adverse effect on the server. I'd look into learning about the data management views (dmv's) as well. Think of the DBA as the team mechanic rather than the racecar driver.",3.0
g2ynt0y,ih743y,"Per Microsoft,

* Plan and implement data platform resources
* Implement a secure environment
* Monitor and optimize operational resources
* Optimize query performance
* Perform automation of tasks
* Plan and implement a High Availability and Disaster Recovery (HADR) environment
* Perform administration by using T-SQL",2.0
g31wegf,ih743y,"DBA can be both mechanic and driver, and also one or the other.  You’re thinking of operational/production dba vs development dba.


https://www.brentozar.com/sql/picking-a-dba-career-path/

Speaking as someone who is both.  I’m in charge of mostly code development/tuning/deployment on the oracle side (less operational) and both operational/code on SQL Server.",1.0
g2yh29s,ih743y,"How is it that you've done your aggregations in Python but have still done `group by`s and windowing functions in SQL?

Check out [T-SQL's aggregate functions](https://docs.microsoft.com/en-us/sql/t-sql/functions/aggregate-functions-transact-sql?view=sql-server-ver15) (you'll sometimes use them in conjunction with windowing functions) to replace what you're doing in Python. It may be faster.

Anyway....you're not so much looking for a DBA class as you are a database theory and design class. There is definitely some overlap, but many DBAs don't get into schema design, and many database designers/developers don't get into database administration.",2.0
g2yi1mu,ih743y,"Initially I worked in SQL so I gained some skills but our database was pretty unreliable so I transitioned to a new data source that was easier to access in python. Got used to python and used to to query SQL from there on. Still write sql transformations from time to time. 

Good point, I think database theory and design is exactly what I’m looking for in fact. That sounds much more like the role. Thank for bringing that up. 

Do you have any classes you’d recommend?",1.0
g2zd6xh,ih743y,"There are some courses on EdX  also you might wanna check. 
https://www.edx.org/course/managing-sql-server-operations-0",2.0
g2zglbh,ih743y,"Since this is entry level, if you have a chance then ask what type of DBA they are looking for -- either missing proficiencies, balancing out the team, or if there is something very specific in mind for the role. Since it is ""entry"" it sounds like there's already atleast someone else in a DBA role who has certain responsibilities.

Typically DBAs lean either towards Systems (system administration, monitoring, storage, backups, etc) or Development (query/index tuning, optimization, data architecture, data cleansing, etc).

So by asking about this, you would show interest in the role, as well as informing yourself of where you might focus your attention.

Beyond that it sounds like you're on the right track. You already have a Brent Ozar link, but check this out, and maybe focus on the areas you think would be most valuable in the short term:

 [https://www.brentozar.com/archive/2019/07/welcome-to-the-dba-training-plan/](https://www.brentozar.com/archive/2019/07/welcome-to-the-dba-training-plan/)",2.0
g2yzj01,ih743y,Cool will review.,1.0
g2xcjv7,ih11tj,"Send it to the publisher, they'll add it to the errata if there is one published for this book and fix it in the reprint.  Stuff like this often slips through, like a misspelling in a novel.  Rarely do I read a first edition novel that I don't find at least one.",12.0
g2xemwi,ih11tj,Donald Knuth used to give individuals that found bugs in his published works a [check](https://en.wikipedia.org/wiki/Knuth_reward_check) for notifying him.,5.0
g2xfapd,ih11tj,"I have sent them an email, thank you

That is really surprising, especially since this problem is like 25 years old and was originally designed by Rozenshtein. However, the original solution seems to be wrong as well (see Edit)",1.0
g2xafo8,ih11tj,I think you're right.  Seems like it should be a rank instead of a dense_rank.,4.0
g2xcf4m,ih11tj,"Indeed, I think the RANK() function solves the issue",3.0
g2xx4xb,ih11tj,"There are a few things you can take from here.

Scalability and robustness of queries is important. This example is obviously a demonstration of a windowed function, but it falls apart very quickly. It is not a robust solution

The Scalability of your solution is also questionable. 1000 students with a Cartesian join will make 1,000,000 rows. Not ideal! For readability, its worth using a CROSS JOIN instead of having a comma separated list of tables:

     SELECT 
     FROM Table a
     CROSS JOIN Table b

Finally, this query shows how different data types affect your query. If the age was of type DECIMAL(5,2), and was calculated properly, the Windowed function would work very well indeed.",2.0
g2y3d14,ih11tj,"Hi, Thanks for your feedback

How is the data type DECIMAL(5,2) changing anything ?",1.0
g2y4nyq,ih11tj,"You'd end up with ages like 19.24, 18.76, 20.03 and so on, as I say, if they were calculated like this. Simply converting an INT to a DECIMAL wouldn't make a difference.",1.0
g2y6yre,ih11tj,It would simply reduce the likelihood of getting duplicate values but would not solve the issue would it ?,2.0
g2xo1xc,ih11tj,"The problem statement is flawed, and IMHO, Rozenshtein's rephrasing is hardly brilliant. Maybe I'm missing something, but the real world problem statement is actually ""find the three youngest students"", and we don't have enough data, because we only have years and not birthdates. So, to OP's point, this only works - and even then, hardly - because there is only one 18- and one 19-year-old in the set. The truth is, we don't know which of the 20-year olds are older than one another, so they must all be returned as though they were a simultaneously birthed litter. And because that's improbable, our result set most likely contains people that are older than four others.",1.0
g2xpdzj,ih11tj,"I don't think there is any issue or ambiguity with the problem phrasing :  You want to find students who are older than at most two other students. We don't care about months or days, we simply look at years since its the only data we have",1.0
g2xwqn7,ih11tj,"So... something more like this?

    select * from student s
    where (select count(*) from student where s.age &gt; age) &lt; 3

..or am I still missing something?",3.0
g2xyln0,ih11tj,"Yes ! That seems to be a very good solution, Thank you",1.0
g2wktjr,igx8ph,"    CASE WHEN CAST(start_time as time) &lt; '05:00:00' THEN 'Morning'
        WHEN CAST(start_time as time) &lt; '13:00:00' THEN 'Afternoon'
        WHEN CAST(start_time as time) &lt; '21:00:00' THEN 'Evening'
        ELSE 'whatever you call the time between 21:00 and 00:00'
        END Shift_Name",5.0
g2x4qd8,igx8ph,"Start with this, OP. You’re getting NULL values because you’re checking if the time is equal the time- which is almost never going to be the case.",2.0
g2wiicv,igx8ph,"I'm on mobile, so I'm not adhering perfectly to your column names... hopefully this gives a general outline though!

What I'd do is make a set of CTEs:

; With Shift_names as ([put the date and shift logic in here so a row looks like (2020-08-24, Bill, Afternoon)]),

--giving a row number for each of the above, that we can use below to group output
Row_alignments as (select *, output_row_number = ROW_NUMBER() OVER(PARTITION BY date, shift_name  ORDER BY worker_name) )

--aggregating... the MAX here will act ad COALESCE since only one entry is not NULL

SELECT date, weekday_name, 
MAX(case when shift_name ='morning' then worker_name end) as Morning_worker,
[Repeat last row twice with other shifts]
From row_alignments 
GROUP BY Date, Weekday_name, output_row



That's kindof a hacky solution, but it'll do in a pinch.  Let me know if that makes sense, or if I can clarify anything.",2.0
g2wirq1,igx8ph,"Change this query so that the result has the day, shift (morning/afternoon) in a single column, person's name as each of the columns .

Then query on that newly created table with your case statement. You do not have to actually create a table if you do not want to, just put the first query in brackets as the 'from' table. If you don't understand this, create a table first.",2.0
g2wh3q8,igx8ph,[deleted],1.0
g2whlcy,igx8ph,"Weird flex, but ok.",0.0
g2wilyy,igx8ph,"I was trying to help you, not “flex”. Figure it out yourself then.",0.0
g2wk3st,igx8ph,"Your answer wasn't helpful, it was snarky and rude. I'm not surprised you deleted your comment.",1.0
g2wk9vo,igx8ph,That was your perception not the intent. I deleted it because you can obviously figure it out yourself and don’t want help. Bye.,0.0
g2wmkhx,igx8ph,"Try 

With MorningCrew as (

SELECT convert(VARCHAR(11), start\_time, 103) 				AS DATE

	,DATENAME(weekday, start\_time) 							AS Week\_Day

	,CASE 

		WHEN CAST(start\_time AS TIME) = '05:00:00' \\\`\`

			THEN name

		END 												AS name

	,row\_number() over(partion by start\_time order by name) as NameSort

FROM \[somedb\].\[dbo\].\[tb\_some\_table\]

WHERE \[start\_time\] &gt;= dateadd(day, 2 - datepart(dw, getdate()), CONVERT(DATE, getdate()))

	AND \[start\_time\] &lt; dateadd(day, 9 - datepart(dw, getdate()), CONVERT(DATE, getdate())

)

, AfternoonCrew as (

SELECT convert(VARCHAR(11), start\_time, 103) 				AS DATE

	,DATENAME(weekday, start\_time) 							AS Week\_Day

	,CASE 

		WHEN CAST(start\_time AS TIME) = '12:59:59'

			THEN name

		END 												AS name

	,row\_number() over(partion by start\_time order by name) as NameSort

FROM \[somedb\].\[dbo\].\[tb\_some\_table\]

WHERE \[start\_time\] &gt;= dateadd(day, 2 - datepart(dw, getdate()), CONVERT(DATE, getdate()))

	AND \[start\_time\] &lt; dateadd(day, 9 - datepart(dw, getdate()), CONVERT(DATE, getdate())

)

,EveningCrew as (

SELECT convert(VARCHAR(11), start\_time, 103) 				AS DATE

	,DATENAME(weekday, start\_time) 							AS Week\_Day

	,CASE 

		WHEN CAST(start\_time AS TIME) = '20:59:59'

			THEN name

		END 												AS name

	,row\_number() over(partion by start\_time order by name) as NameSort

FROM \[somedb\].\[dbo\].\[tb\_some\_table\]

WHERE \[start\_time\] &gt;= dateadd(day, 2 - datepart(dw, getdate()), CONVERT(DATE, getdate()))

	AND \[start\_time\] &lt; dateadd(day, 9 - datepart(dw, getdate()), CONVERT(DATE, getdate())

)

SELECT   coalese([A.Date](https://A.Date),[B.DAte](https://B.DAte),[C.Date](https://C.Date)) as Date

		,coalese(A.Week\_Day,B.Week\_Day,C.Week\_Day) as Week\_Day

		,coalese(A.NameSort,B.NameSort,C.NameSort) as NameSort

		,coalesce([A.Name](https://A.Name),'') 								AS Morning

		,coalesce([B.Name](https://B.Name),'') 								AS Afternoon

		,coalesce([C.Name](https://C.Name),'') 								AS Evening

FROM MorningCrew A

FULL OUTER JOIN AfternoonCrew B

	ON [A.DATE](https://A.DATE) = [B.DATE](https://B.DATE)

	AND A.Week\_Day = B.Week\_Day

	AND A.NameSort = B.NameSort

FULL OUTER JOIN EveningCrew C

	ON [A.DATE](https://A.DATE) = [C.DATE](https://C.DATE)

	AND A.Week\_Day = C.Week\_Day

	AND A.NameSort = C.NameSort

order by 1,2,3;

&amp;#x200B;

I have yet to work in an organization with three shifts where all three shifts are staffed equally. Usually first shift is staffed highest then second and lastly third. I would expect the results to look more like

&amp;#x200B;

Date Weekday A B C

.

.

Date Weekday D E 

.

.

Date Weekday F

.

.

 

Good luck. Comment out the line with the Name Sort and drop the number 3 from the order by if you do.",1.0
g2vu44x,igsoie,"Import them into a single table. You will need to know how an import or ETL tool works like, Python, PowerShell, Import/Export wizard in SSMS, or SSIS.

If you already have them in separate tables for some reason, you can use UNION ALL to concatenate the result sets.",4.0
g2w82l1,igsoie,"7.2 million rows is not considered ""extremely large"" these days (for a relational database). I wouldn't even consider it ""large"", rather ""medium sized"" actually.",5.0
g2w8cvn,igsoie,For only 12 files and a one off it’s probably not even worth using SSIS as you’d have to install it and configure the environment. Just use the flat file import wizard and dump it into a single table. 7.2 million records isn’t even a speed bump for SQL.,2.0
g2z65iq,igsoie,Also create appropriate indexes. Beginers guide [here](https://www.toptal.com/database/sql-indexes-explained-pt-2),2.0
g2zowto,igsoie,"What import wizard do you recommend? I use one on Azure Data Studio, but it only allows me to add 1 flat file for 1 table.",1.0
g2zyofd,igsoie,"I use SSMS not Azure Data Studio. If you already use ADS just use that, I’m sure it has a few import facilities.

I would honestly just be super lazy since this is a one off. No need to spend time making it easily repeatable or learning other tools if you’re already using ADS. At least for now.

Import each file into its own staging table, just call it STG_JAN, STG_FEB etc. Create a table with the same structure and data types. This will be your target for the collated data. Then write SELECT * INTO statements to move it from your staging tables to your real target table. It’s repetitive but very easy. You can run them all at once.

There are many other ways to do this but I think you’d rather spend time on your analysis than wrestling with moving data.

If you had many more files or you wanted to change the time periods on the fly and fetch directly from the source etc., I’d recommend a different route. This is quick and dirty though and will get you started.",1.0
g30vv3d,igsoie,I’ve tried INTO and it works. Thank you so much!,1.0
g30wrby,igsoie,No problem. Glad it worked!,1.0
g2uj8d4,igm0g6,"SQL Server Express Edition is free for all usage. It has some resource and size constraints. 

SQL Server Developer Edition is free for the type of usage you describe. It has no resource or size constraints.",3.0
g2yccyb,igm0g6,"I've bought a course for preparation for the MTA exam of DB fundamentals, and I need SQL server and SQL server management studio, will any of those work?",1.0
g2yefga,igm0g6,"Either one should work. If you hit the Express limits, grab Developer",2.0
g2yig93,igm0g6,"Thanks man, life saver",1.0
g2ujg92,igm0g6,Thanks very much. Google didn't turn up that option.,0.0
g2ut90s,igm0g6,You should use sql dev edition. Its the same as enterprise but its free as long you don't use it for production,2.0
g2uu31x,igm0g6,I have enterprise on my work laptop. I just want it for practice using transact sql.,0.0
g2uinq0,igm0g6,"[SQL Server 2019 Express is a free edition of SQL Server, ideal for development and production for desktop, web, and small server applications.](https://www.microsoft.com/en-ca/sql-server/sql-server-downloads)",3.0
g2uk0c7,igm0g6,Thanks,1.0
g2un8zd,igm0g6,"Please don't use express.  You're doing testing, practice, or development, so download and use developer edition.  It's free and doesn't have any CPU, RAM, or database size constraints.",6.0
g2uu719,igm0g6,Cheers I have installed Developer.,2.0
g2umb3l,igm0g6,"For yourself, SQL Express will likely work well.  The features it's missing shouldn't impact you, especially if you're trying to learn the language.

If you start getting into some of the more advanced concepts of SQL, like HA and DR, you may need Enterprise.  You can use the Developer version for this, but be aware that MS very narrowly defines what is allowed here - there cannot be ANY production data at all on it.  Not even a replica.  Best to not even install it on any work asset if you can help it.",-1.0
g2umuaq,igm0g6,"&gt; there cannot be ANY production data at all on it

This is not the case, at least under the current license terms. You can have production data on a dev edition instance but you can't use the instance for production workloads - software development and QA testing only.

&gt;Best to not even install it on any work asset if you can help it.

No way! Developer Edition was made for your developers! Spending production licensing money on dev/QA instances is throwing a lot of money away.",3.0
g2un99m,igm0g6,"For a dev shop, yes, that's what it is meant for. For your average person learning, or if your company is not a dev shop, it might be better to err on the side of caution.  Auditors are very harsh because they get a percentage of extracted licensing fees.

Is a clone of prod for dev purposes allowed?  I've never seen anything concrete on that.",0.0
g2ua6f8,igkcmr,"You don't add another ""else"" you add another ""when"".

 CASE WHEN pl\_from.whos\_place = 'VAN' THEN pl\_from.phy\_svc\_grp\_d 

WHEN pl\_from.whos\_place = 'MLR' THEN pl\_from.phy\_svc\_grp\_d

ELSE '02' END AS 'team\_id

&amp;#x200B;

If this is what I think you mean then alternatively you could use an IN statement instead of = and do one WHEN like this:

 CASE WHEN pl\_from.whos\_place IN ('VAN','MLR') THEN pl\_from.phy\_svc\_grp\_d ELSE '02' END AS 'team\_id'",25.0
g2wkc6r,igkcmr,"first off, thank you so much to break it down for me i really appreciate it!
the value of 'MLR' is similar to that of '02'
those two values are from pl_from.place_id
where as whos_place is more of a category (van, warehouse, store, etc)

so the (else '02') part is where i also need to put MLR

Hope that makes it more clear.",1.0
g2w5nte,igkcmr,"As others gave you the right answer, I'm sharing a [post](https://www.reddit.com/r/SQL/comments/g7660z/ive_been_writing_sql_queries_for_4_years_now_and/) I made few months ago on this topic which taught me a lot.",2.0
g2wdshz,igkcmr,Thank you so much!,2.0
g2vkx2n,igkcmr,https://lmgtfy.com/?q=case+when+sql,-6.0
g2u5lqg,igj4ru,"Is there a clustered index on this table? If no, the table is stored in a heap, and it can't use a non covering (an index that does not include all required fields in a query) non clustered index. It would require a look up in the heap, but the heap isn't ordered, so you know the row id, but that's pointless as you still need to look at every single row to find that row id.",3.0
g2vewjt,igj4ru,"You can tell it to use the index anyways and compare the query plans. See for yourself why it doesn’t want to use the index.

    SELECT city
    FROM Dimension.City WITH (INDEX(ix_indexpractice))
    WHERE [State Province] = ‘Florida’

You’ll probably find that forcing it to use the index and then to do all those key lookups ends up being more costly in the end.",3.0
g2u71tb,igj4ru,"SQL Server tends to try to avoid doing key lookups whenever possible, and since you're selecting a column that is not part of the index it would have to do a seek on the index to get the clustered index key for that row (and you do have a clustered index right?) and then do a key lookup on the clustered to pull the city for that key. Key lookups will force a foreach loop operation, which SQL will also try to avoid (but it can at least run them in parallel). You've got two options - add a FORCESEEK hint and see how performance is with the key lookup (sometimes it's worth it, sometimes SQL was right and a scan is actually faster) or bite the bullet and add the include so that you stay entirely within the nonclustered index.",2.0
g2vobhw,igj4ru,"There are two indexes on that table: a clustered index of [City Key] and a non-clustered index of ([WWI City ID],[Valid From],[Valid To]). When you created index ix_indexpractice, you created an index on [State Province] and included [City]. When you do this, all instances of [State Provice] are indexed in ascending order. Additionally, the value of [City] is also copied to the index. So, this index looks like this:

|State Province|City|City Key|
|:--|:--|:--|
|Alabama|Abanda|31978|
|Alabama|Abanda|45281|
|Alabama|Abanda|78824|
|...|...|...|
|Wyoming|Yoder|32789|
|Wyoming|Yoder|42725|
|Wyoming|Yoder|80434|

Notice the query engine is able to get everything is needs (including the [City]) from the index just by performing 4,024 seeks for the [State Province] alone. There is no further need to do any additional lookups. However, when you removed this covering non-clustered index, the query engine is not going to behave the same way. If you replace the index with just this index:

|State Province|City Key|
|:--|:--|
|Alabama|31978|
|Alabama|45281|
|Alabama|78824|
|...|...|
|Wyoming|32789|
|Wyoming|42725|
|Wyoming|80434|


the query engine is going to make a decision. It will decide if it is more cost effective to lookup all 4,024 [State Provices], then go to each [City Key] on the clustered index, and find the [City] in the same way we did the first time with the covering index. Or, is it just more cost effective to scan the entire table (the entire clustured index) just once. The engine decided to discard your created index and just scan the existing clustered index.

|City Key|WWI City ID|City|State Province|...|
|:--|:--|:--|:--|:--|
|0|0|Unknown|N/A|...|
|1|5450|Carrollton|New York|...|
|2|5451|Carrollton|Virginia|...|
|...|...|...|...|...|
|116292|17658|Kiahsville|West Virginia|...|
|116293|17705|Kimball|West Virginia|...|
|116294|17728|Kincaid|West Virginia|...|

* Your first query (with no index, just doing a table scan) was estimated to cost 2.69935. 
* Your second query (with the index that included [City]) was estimated to cost 0.0254219.
* Your third query (with the index that did NOT include [City]) would have cost 8.11212.

You can dig this discarded query plan out of the trash by running this hint:

    SELECT city
    FROM Dimension.City WITH (INDEX(ix_indexpractice))
    WHERE [State Province] = 'Florida'

As you can see from the query plan, it just cost too much effort to try and join to [City].",2.0
g2z8juv,igj4ru,"This is a great answer. To understand basics, you (OP) can do exercises from the [tutorial](https://www.toptal.com/database/sql-indexes-explained-pt-2). It is not Sql Server specific, but it helps you understand how indexes work.",1.0
g2u1r16,igj4ru,"What happens when you use the second index and use a [State Province] that returns fewer rows than Florida (if possible)? There's a cutoff SQL Server has where after estimating its going to return ~5000 rows it just does a scan instead of a seek and that might be what you are running into. Also, doing a seek with a key lookup is normally undesirable for a number of rows because you'll end up reading the same page multiple times (depending on how the data is ordered in the clustered index).",2.0
g2u39x2,igj4ru,"If I do the  \[State Province\]  with the maximum amount of rows (Texas at 7186 rows) or if I try one with less (Alabama at 3875 rows) it still does an index scan.

&amp;#x200B;

Are these simply not enough rows for the query optimiser to justify using an index in this situation? Now that you mention it the plan doesn't have a key lookup so it must be thinking that it's just faster to scan the full table, could that be it?",2.0
g2u3l2o,igj4ru,"Definitely could be, what data type is the city field and how wide is the table?",2.0
g2u4yof,igj4ru,"The city field is NVARCHAR(50), there's only 14 columns in the table.

&amp;#x200B;

To fudge it a bit since it's just a practice DB I've inserted 1744425 dummy rows with a  \[State Province\] of Florida2  but when I run a select for that it still does an index scan (the total table size is now 1860720 rows).

&amp;#x200B;

I tried a SELECT using Tennessee (1150 rows) and it does do an Index Seek and a clustered Key Lookup.

&amp;#x200B;

So it's just the query optimiser finding that sweet spot where the Seek and Key Lookup combination is faster than a full scan but ironically that would only work for less rows because the more rows you return the more expensive a key lookup is?",2.0
g2uf6j3,igj4ru,"Yeah, that's about it, although it might not always equate to ""faster"". With its limited knowledge of the contents of the clustered and non clustered indexes, it's designing plans that will work, then its executing the plan it calculates will be best. Sometimes that's faster. Sometimes that's less resource intensive. Sometimes it just says ""this isn't complicated or resource intensive enough to bother doing anything other than the simples thing: just use the clustered index"".",1.0
g2v8ezy,igj4ru,Can you post the indexes and a query plan ?,1.0
g2w318y,igj4ru,"The issue is that for the noninclude index, it would have to use key lookup which is quite costly.   
At some point, it just decides (based on estimates) that doing a full index scan is faster/better then doing 4024 key lookups.

When you take a look at the plan, whats the estimated row count?",1.0
g2u033s,igj3n8,"Hello u/cumfartsandhearts - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g2ucrb6,igj3n8,"    WHERE myDateTime &gt;= @passedInDate
      AND myDateTime  &lt; @passedInDate + INTERVAL 1 DAY

this has the advantage of utilizing an index on `myDateTime` (if one exists)",1.0
g2ueji9,igj3n8,"Oh man, thank you.",1.0
g2v6irq,igj3n8,Or just cast the date time as date.,1.0
g2wzgob,igj3n8,CAST isn't [SARGable](https://www.brentozar.com/blitzcache/non-sargable-predicates/).,1.0
g2v5vlf,igdn4c,"I think you're already on the right track with your lookup tables.  

Create table venue (venueid smallint or whatever is needed, venuedesc varchar(100))

Create table balcony (venueid, balconyid,  balconydesc)

Create table section (venueid, balconyid, sectionid section description)

And so on. That's probably how I would approach it. You could even create additional tables to avoid putting any redundant strings in for the description.",5.0
g2uweu4,igdn4c,"The venue geometry kind of doesn't matter to the database. If you have to reserve 12C of Venue 37, that's all the information you need.

It might matter to a booking system if it wants to handle groups, and that's a thing you can handle in a different way if / when you get to that. But reservations still only need to care about just ""reserve 12C -&gt; 12F of Venue 37 please"".",2.0
g2v4cds,igdn4c,So maybe you're saying I'm going the wrong direction in thinking this through. I tend to think venue-&gt;level-&gt;section-&gt;row-&gt;seat when I should be centering things around seat and work my way up ?,1.0
g2vle69,igdn4c,"I'm suggesting that you're making considerations for things you don't even need to at this point.

There's no need for the reservations database structure to match the physical layout of a venue. Even if you have subsequent use cases like ""I need to report on ticket sales over time broken down by venue and audience section"", *that's* the part where you can join in other tables with all the information about theater layout.",2.0
g2x1kek,igdn4c,"this is right i think, and good general advice. You probably would be better off ""centering things around seat"". A venue has x seats, each needs a unique id and that is it. 
If it has row/col/etc, those can all be added in separate columns, as meta data.

I think its a matter of choice whether you put all seats in one table, but it depends on how you like to write queries and what kinds of queries you need to run. 

the unstated part of all the replies though in this thread, is that ""doing it the easy way"", whatever they are essentially proposing, is usually helloworlding into long term problems quickly. you can set up a db quickly or ""the right way"" easily, and then have a very difficult time addressing actual business or user needs. people need to extract value from these things. getting comprehensive calendar-data back from single queries for n-seats across x-venues over y-days is not simple, and there is no answer that isn't going to be constrained by all the other choices you have to make like db-vendor, provisioning services, client/state data controls, and so on. 

without letting something like this distract you, it would be good to write down 10 or 15 actual english questions as specific and as general as you can think of that you or your client need to address.

- How many seats are open now at x venue, All venues, venues that are handicapped accessible, venues that serve alcohol venues for under 18?

- Who is playing where when?

- When is my band playing?

- Is there a cheap seat available tomorrow in the balcony?

- Have i made my decision for christ?

- Can I add remove venues easily? 

- Can i change venue information such as opening hours easily?

It's worth building a test db with these in mind so that you can show along the way what kinds of queries exactly you can and cant answer ""easily"". 

Good luck. I think you are on the right track.",1.0
g2vb3c1,igdn4c,"Generally the ""parent"" table will be the one side of a one to many, and venue would be on top.  

This is not a hard rule though.  When it comes to lookups your DB engine will do things in the order that it thinks will be easiest, so from that standpoint it really doesn't matter.  If you flip the hierarchy the query plan won't change.

Instead, consider if it's possible to have, for example, a section with no seats, or seats with no section, and if it matters that one or the other needs a dummy record to match to.  (For example, seats with no section could be put into the ""other"" or ""misc"" section, or a section could be given a seat called ""not a seat.""  They have very different implications in your application.)",1.0
g2uuoo0,igdn4c,"unless there are customer requirements that state otherwise, it might be easier to store each venue's seats in its own table. 

Is there an advantage to keeping all the seats all together you are thinking of?",1.0
g2vbet8,igdn4c,"Biggest advantage to keeping all venues in together is scalability.  You don't have to touch your code to add a new venue.

If you have enough venues for the sheer volume of seats to be an issue, then you may want to consider getting professional help.  (A data architect, not a shrink.)",2.0
g2wcujq,igdn4c,"maybe, but if there are hundreds of venues what are the chances that all of them have all the exact same metadata? Each venue could have unique fields, or the seating numbering could be completely different and incompatible. One place could have row numbers, and another row letters. One place could not have numbers at all because its family seating with no placeholders. Why should all these ""seats"" be together?

It seems harder to write queries for the big seat table than to write separate subqueries for each venue if they have differences in structure.",0.0
g2wvina,igdn4c,"It's actually not difficult at all to write a query to pull a subset of data based on another table.  This is how things usually happen, and good ""normalization"" of the data can make this super easy.  Once you're joining 3 tables, joining 30 is just repetition.

However, changing the code after the fact to add a new venue down the line requires going back to the code or using dynamic SQL.  (Careful with Dynamic SQL - it's powerful, but can also degrade performance if over used.)

Your ""show seats"" query could, consistently, look like:

    select
        v.name as Venue,
        l.name as Level,
        sec.name as Section,
        r.name as row,
        s.name as seat
    from venue v
    join level l on l.venueid = v.id
    join section sec on sec.levelid = l.id
    join row r on r.sectionid = sec.id
    join seat s on s.rowid = row.id
    where v.name like @SelectedVenue
        AND l.name like @SelectedLevel
        AND sec.name like @SelectedSection
        AND r.name like @SelectiondRow
        AND s.name like @SelectedSeat

(I'm not sure if you can use ""row"" as a table name of if it's reserved - might need square brackets or a different name).

Just pass a wildcard ('%' for MSSQL) to parameters you don't want to specify.  For example, specifying SelectedVenue and filling the rest with % will give you all seats in that venue.  From there you could join to the reservations table and add a condition for ""not fully reserved"" along with the event ID and you have your ""what's available"" query.

If you have a venue that ONLY has seats, and no sections, you just define each intermediate level as only having 1 thing in it.  Venue = BarWithAStage, 1 Level called ""Main"", 1 section called ""Tables + Standing"", 1 row called ""No Rows"", and then all the ""seats.""  The application (or even the person using it) can simply ignore the specific seat numbers, simply using it to ensure there's no oversubscribing on the capacity.

Alternatively, there could be only 1 seat with a max occupancy column.  This would be 1 for reserved seating, with a seat for each chair.  A table reservation would reflect the seating capacity of the table (if it can be shared between parties) with the capacity set to, say, 4, while a seat called ""standing room"" with a capacity of, well, whatever capacity is left over after counting the chairs.",2.0
g2wf2sd,igdn4c,"This is exactly the scenario I'm facing but with less than 10 potential venues.

I was thinking that doing a query for a non-existent seat or row would just return a null value (or whatever).",1.0
g2uvtss,igdn4c,Not that  I know of. My only thinking was that it would be easier to find a seat by venue and event. What were you thinking?,1.0
g2sy36b,igd9g5,"    WITH Dist_GeoCode 
    AS (
    SELECT DISTINCT 
           Vendor
         , LEFT(zipcode,3)) AS GeoCode
      FROM t1
    ) 
    SELECT Vendor
         , MIN(GeoCode) + '-' +
           MAX(GeoCode)         AS GeoRange
      FROM Dist_GeoCode
    GROUP
        BY Vendor",1.0
g2t50yt,igd9g5,"That seems to only return one line per Vendor, but in some cases there shoudl eb more than one line - in my example above, ABC should have 3 lines, and DEF should have 3 lines, but this would only return 1 record for ABC (123-129) and DEF (902-909). Is there a way to expand on this to get each individual group of GeoCodes based on sequence? I.e. 123, 124, 125 be grouped into 123-125, while 127 and 129 be separate (127-127 and 129-129)?",2.0
g2tde3l,igd9g5,"yeah, there is a way, but i'm not going to attempt it

these are called ""runs"" and they're covered in Chapter 24 ""Regions, Runs, Gaps, Sequences, and Series"" in Joe Celko's book *SQL For Smarties* (3rd edition)",2.0
g2uf6hh,igd9g5,"I think you're trying to do too much with one query.

A double cursor would fix this issue and check for the sequential zip logic to create the proper range when a sequence is broken.



Outer cursor for distinct vendors

Inner cursor for zip codes ordered by your left 3 ascending. 


If the start range + 1 = the zipLeft3 or end range + 1 = zipleft3 then zipleft3 is your new end range. Each inner cursor pass will adjust end range. 

122-123, then 122-124.. but when 126 appears you hit an else.

Else the range is completed, update record, reset start range to zipleft3. Which allows the above logic to start again for the next range.

126 becomes the new start range, if logic repeats for the next sequence. 126-127.. etc etc.",1.0
g2uv4e3,igd9g5,"Thanks for the info, I'll have to dig into this. Based on what you've said, this seems like the only real way to accomplish what I need.",1.0
g2uwbn9,igd9g5,"If you havent used cursors before/  want the code itself, let me know.  I'll use T1 and write it up.",1.0
g2v9brt,igd9g5,I could definitely use any guidance you can offer. Until your post I had never even heard of cursors!,1.0
g2vkxf6,igd9g5,"ugh what a mess, 1 sec


     DECLARE @T1 TABLE ( Vendor NVARCHAR(10), ZipCode NVARCHAR(10))
     
     INSERT INTO @T1 (Vendor, ZipCode)
     VALUES
     ('ABC','12345')
     ,('ABC','12445')
     ,('ABC','12555')
     ,('ABC','12755')
     ,('ABC','12999')
     ,('DEF','90210')
     ,('DEF','90211')
     ,('DEF','90411')
     ,('DEF','90511')
     ,('DEF','90666')
     ,('DEF','90777')
     ,('DEF','90990')
     ,('DEF','90991')
     ,('DEF','90992')
     ,('GHI','90991')
     ,('GHI','91092')
     ,('GHI','91192')
     ,('JKL','90991')
     ,('JKL','90992')
     ,('JKL','90992')
     
     DECLARE @Vendor            NVARCHAR(10)
     DECLARE @Zip            NVARCHAR(10)
     DECLARE @Zip_3            NVARCHAR(10)
     
     DECLARE @Final_Zip            NVARCHAR(10)
     DECLARE @Final_StartRange    NVARCHAR(10)
     DECLARE @Final_EndRange        NVARCHAR(10)
     DECLARE @Current_StartRange    NVARCHAR(10)
     DECLARE @Current_EndRange    NVARCHAR(10)
     DECLARE @UpdateFlag        BIT     = 0
     
     DECLARE curVendor CURSOR LOCAL STATIC FORWARD_ONLY READ_ONLY
     FOR
         SELECT DISTINCT Vendor
         FROM @T1
     
     OPEN curVendor
     FETCH NEXT FROM curVendor INTO @Vendor
     
     WHILE @@FETCH_STATUS = 0
         BEGIN
     -- New Vendor - clear variables
             SELECT @Current_StartRange    = ''
             SELECT @Current_EndRange    = ''
             SELECT @UpdateFlag            = 0
     
     
     -- loop through zip codes for specific vendor
             DECLARE curVendorsZip CURSOR LOCAL STATIC FORWARD_ONLY READ_ONLY
             FOR
                 SELECT ZipCode, LEFT(Zipcode,3) 
                 FROM @T1
                 WHERE Vendor = @Vendor
                 ORDER BY  LEFT(Zipcode,3) ASC
     
             OPEN curVendorsZip
             FETCH NEXT FROM curVendorsZip INTO @Zip, @Zip_3
     
             WHILE @@FETCH_STATUS = 0
                 BEGIN
     -- reset update flag                
                     SELECT @UpdateFlag            = 0
     -- if start range is empty, set it.
                     IF @Current_StartRange = ''
                         BEGIN
                             SELECT @Current_StartRange = @Zip_3
                         END
     -- else this isnt a new iteration, we already processed a record.
                     ELSE
                         BEGIN
     -- if the start range is the same as the incoming variable, we can skip it... else, check the value
                             IF @Current_StartRange &lt;&gt; @Zip_3
                                 BEGIN
     -- if the next zip is +1 from startRange set the EndRange as Zip_3
                                     IF CAST(@Current_StartRange AS BIGINT) + 1 = CAST(@Zip_3 as BIGINT)
                                         BEGIN
                                             SELECT @Current_EndRange = @Zip_3
                                         END
                                     ELSE
                                         BEGIN
     -- if not, lets see if we already set the endRange, is the EndRange + 1 the new Zip
                                             IF CAST(@Current_EndRange AS BIGINT) + 1 = CAST(@Zip_3 as BIGINT)
                                                 BEGIN
     -- if it is, just update the endRange
                                                     SELECT @Current_EndRange = @Zip_3                                                        
                                                 END
     -- else.. we have a problem!
                                             ELSE
                                                 BEGIN
                                                     SELECT @UpdateFlag = 1
                                                 END
                                         END
                                 END
                         END
     
     -- if the updateFlag = 1.. lets update
                     IF @UpdateFlag = 1
                         BEGIN
                             PRINT CONCAT(@Vendor, ' | ', @Current_StartRange,'-', IIF(@Current_EndRange='',@Current_StartRange,@Current_EndRange))
     -- because we updated.. lets set the new StartRange to the Zip_3
                             SELECT @Current_StartRange    = @Zip_3
                             SELECT @Current_EndRange    = ''
                         END
                         
                     FETCH NEXT FROM curVendorsZip INTO @Zip, @Zip_3
     
                 END
             CLOSE curVendorsZip
             DEALLOCATE curVendorsZip
     
     -- is there still data because we ran out of records for the vendor?
                     
             PRINT CONCAT(@Vendor, ' | ', @Current_StartRange,'-', IIF(@Current_EndRange='',@Current_StartRange,@Current_EndRange))
     
     
             FETCH NEXT FROM curVendor INTO @Vendor
     
         END
     CLOSE curVendor
     DEALLOCATE curVendor",1.0
g2vla3y,igd9g5,"kk sorry, yay reddit formatting.

i added a couple more cases to the data.

ending on a same number, ending perfectly on a sequence. 

abc and def had their own cases. ending on a gap and ending on a triple.",1.0
g2vltwg,igd9g5,"Also.. dont overuse cursors. 

There is a balance, and it takes time in the physical/development optimization field to find it. 

Cursors are not pure evil (they actually reduce latches and waits due to quick access vs large joins), and cursors are not spaghetti's gift to humans (a join is comparing records row by row, so sometimes cursors are unnecessary) .",1.0
g2t55op,igcnkb,"Is the table partitioned?  If not, I would recommend doing so, for a number of reasons - makes maintenance, including imposing a retention, quite a bit easier.

When properly partitioned, the table's indexes are ""aligned"", which results in the ability to target the indexes across a specific partition.  Much smaller chunks of work.",11.0
g2tdwl6,igcnkb,"Came here to say the same. If your data is changing primarily by addition rather than by modifying existing data, partitioning could really solve your issue.

Are the tables and indexes really changing so much that a rebuild is necessary that frequently?  What kind of fragmentation percentages are you seeing?",4.0
g2vi5mk,igcnkb,"Not OP and not a DBA but I have a few tables where I work that are 10+ years old and approaching 250m records, increasing in size exponentially.

Is it possible to add partitions to an existing table without losing data? What kind of performance improvements could we expect? This table sees 10-20 scans (not seeks) per second and is one of our leading causes of timeouts.",1.0
g33li4e,igcnkb,"You can't partition an existing table. You have to create a new partitioned table, then move the data over.

But partitioning is primarily a _management_ feature, not a performance feature. It's possible to partition the table and get _worse_ performance.

250M records isn't that much, compared to many databases. Indexing and properly-sized hardware can go a long way.",1.0
g2thu0p,igcnkb,"Sorry I'm somewhat lost in this, if you are referring if the tables change constantly than no, only the data inside the tables change, but than again there is a bunch of stored procedures that I'm not sure what they're doing, they might be creating temp tables than dropping them who knows",-1.0
g2twnu3,igcnkb,"He's asking if your tables are being modified very often; INSERTS, UPDATES, DELETES, etc.

If a table doesn't change, then the indexes on the table don't need maintenance, there are no changes to introduce fragmentation.

The other question would be, does older data, the data you spoke about archiving, ever change? 

If you partition your table (and indexes) based on say, business date, then, as /u/bricked_machine noted, you can target just those partitions of the index to be rebuilt. 

i.e. instead of rebuilding the entire index for all x years of data you have in a table, you can just rebuild it just for the months that have data that is changing.

Edit: Take /u/alinroc's recommendation first and try Ola's stuff before you try anything more complicated.",2.0
g2szaov,igcnkb,"What would happen if you just left it alone? (Hint it really should be fine unless you are doing  exciting data movement). If you can’t be tempted away from all that heavy work then just cut out the stats updating, rebuilding indexes already does that for you.",9.0
g2t4r78,igcnkb,The problem is that we have have to make sure the rebuild finishes at a better pace since whenever it runs it affects our online customer's from processing payments,4.0
g2t7s83,igcnkb,"What are you rebuilding? Just indexes? I suggest you don’t do that at all, it’s mostly a waste of effort these days unless you’re nuking and repopulating you’re tables",5.0
g2t96f3,igcnkb,I've noticed a performance hit when not running a rebuild for several weeks,4.0
g2u1g7z,igcnkb,"Then identify a better way of handling it. You’ve found out the hard way that rebuilding everything just doesn’t scale. The first step is to identify where the time is going when it’s slow, if it’s a query or several queries then it’s easy to determine where the time is going there by fetching the real query plan and following where the time comes from. Obviously from there it could be a number of (easily solved) problems but let’s pretend the time is going reading the entirety of an index because at some point the DB thought that was the best way to execute a query, but really it should have been using a targeted seek on the index and driving from another table. Perhaps it’s doing this because there are unrepresentative statistics but that’s less and less likely if you’re running an up to date DB version. Maybe (and most likely in my experience), the indexes were always only just getting by and want redesigning to actually enable decent performance - the most common cause here is an index with not enough columns or columns in a bad order. An example of a bad order would be (date_col, filter_col), if I query the table on date_col between someDate and someotherDate and filter_col = 5, then I’m going to read the entire index that covers the date_col filter. If my index was ordered filter_col first, then I would only read the chunk of the index that covers both filters - I read much less data. The DB will know this and will be more likely to use my index because it can use it super effectively, rather than basically having to read it all. I’ve only mentioned this example because it’s what I see pretty much 90% of the time when someone comes to me and says they’ve tried everything already. There are plenty of possible explanations but they’re almost always super easy to solve, you just need to see the problem to solve it.",8.0
g2vchd7,igcnkb,This is a really great comment.,2.0
g2stxn4,igcnkb,"check out an auto indexing tool, such as tigertoolbox. I also deal with large databases, you can control run times, exclude indexes, all kinds of great features. It was also created by a few microsoft devs. I've been running it in production for a few years now and am satisfied with it.   [https://github.com/microsoft/tigertoolbox](https://github.com/microsoft/tigertoolbox)",5.0
g2tgszp,igcnkb,"You've hit the nail on the head with your question about archiving historical data.  Look into the concepts of Data Warehousing (DW) and ETL.  In short, the ETL process would:

1. **E**xtract data from live.  All data that hasn't already been through this process.  DON'T USE TIMESTAMPS!  Use a marker of some sort (see step 4).
2. **T**ransform the data, usually stripping out data not worth keeping, changing data types, consolidating, and maybe even changing the relationship!.
3. **L**oad the data into a Data Warehouse.
4. Verify the data aligns and mark rows that have been successfully crossed over.
5. Purge records from live that have been marked and are over a certain age (Live retention).
6. Optionally purge from DW if data gets really old (DW retention).

Note that the Data Warehouse is usually a wholly separate database, and does not necessarily have to be in the same SQL server (for example if you're putting it on a box with high compute and cheap storage).  The process itself should ALSO be run separately from any application services.  For example, AppService and AppDWService.

Many records will exist in both locations.  The easiest way to ensure the data can be correlated easily is to store the DW Primary Key for that row in a new column in the Live table (obviously this column doesn't need to be copied).

Run steps 1-4 regularly (hourly, or 15m, for example), and run steps 5/6 less often (daily or even weekly).

Revoke access to the Live database for any analysts in your organization.  Get them onto the Data Warehouse.

Then consider moving your reporting over to the Data Warehouse too.

&amp;#x200B;

Until you're able to do that, here's some measures that may help with performance:

Is this a single table, or is it many tables?  If you can at least chop it up it would help.  Do the biggest tables over the weekend, spread the rest out through the week.  (If performance doesn't drop too fast for this to be viable.)

Don't do ""full stats"" unless you know it matters.  Normally SQL does a random sampling and ends up with pretty decent stats.  If your indexes have a fairly consistent spread of data, sampling will give you ""good enough"" stats.  If you have a lot of outliers, and those outliers are killing performance, that's when you do full rebuilds.

Also avoid rebuilding on tables that don't need it.  If you're rebuilding ALL of them but only 2-3 moderate sized tables need it, you're wasting a lot of IO rebuilding healthy statistics.

Consider having the poorly performing queries looked at, a tuning pass might help, and maybe some expensive operations (like sorts) can be pushed to the application layer.  Sometimes the query that takes forever is NOT the problem - I've found issues where it's a super simple query being run thousands of times per second, bogging the whole server.",4.0
g2tjf2x,igcnkb,"Excellent information, we would need to DW some of the data like you mentioned with a marker, I guess I would need a DBA to help us ""mark"" the data with some queries to know what exactly we can extract from live. We will probably have to outsource this work as I'm not a DBA and my knowledge of SQL is limited in Data Transformation",1.0
g2tlcud,igcnkb,"It's not a simple task at all, and unless you're willing to ""reset"" (rename the database and spin up a new empty one), it needs developer time.  Preferably one that understands Database Architecture (though those get expensive fast, and a solid developer can usually make do).

I only touch the ETL processes when I'm trying to coerce them into doing what I want them to do.  I don't want to go anywhere near the code that does it!",1.0
g2tnq6r,igcnkb,"lol true, I need  a DBA knowledgable enough that can give us options on the best approaches on what to do",1.0
g2t1n4q,igcnkb,It sounds like you are doing a full scan on your statistics update. Have you considered switching to a sample percent?,3.0
g2t4urs,igcnkb,"I am running full stats, you suggests looking at my largest table's and just doing the ""smaller"" ones first on the stats part?",1.0
g2tepct,igcnkb,"Also instead of Rebuilding every index, whatever may be the fragmentation %, takes lot of time. For low fragmented index you should Reorganize the index. If you are using MS SQL Server DB, you can use Ola Hallengren's solution for DB maintenance. It takes care of all things.",6.0
g2u635f,igcnkb,Came here to say this,1.0
g2te5vw,igcnkb,"Update statistics has options in which it does not do full scan of the tables, instead it uses %sample rows, which can be given as parameter, and calculate the statistics of the table. It is much faster and in most cases it is very good approximation.",2.0
g2theym,igcnkb,"&gt;I see, this is what my maintenance plan is doing right now:

 

I'm doing a maintenance plan that does this:

Rebuild Index Task

DB: ######

Object: Tables and Views

Original Amount of free space

Than it goes to a Update Statistics Task

DB: #####

All existing statistics

Scan Type: Full Scan",1.0
g2ti9c8,igcnkb,"Yes, now you can modify it such that if the fragmentation is more than 30%(say), then it should Reorganize instead of rebuilding. And you should use sampling instead of full scan for update statistics.
Changing these two will reduce maintenance time by significant amount.",2.0
g33lo52,igcnkb,"&gt; now you can modify it such that if the fragmentation is more than 30%(say), then it should Reorganize instead of rebuilding.

Or you can install Ola Hallengren's solution and not have to fiddle with making a maintenance plan do these same things.",1.0
g2v07rn,igcnkb,"Get away from maintenance plans, they're inefficient. Ola hallengren scripts 100%. It does smart reindexing so you set two thresholds. If it's above the highest threshold (90%} it rebuilds the index. If it's below that but above the second threshold (50%) it rebuilds the index. If it's below 50% it does nothing. 

It also does statistics and gives you the option to only update statistics that have been modified. Likely doing the full scan on everything isn't necessary. Usually less than half of the statistics REALLY need updated weekly. 

I've seen maintenance plan reindexing try to rebuild an index on a 500 GB table even though it was 5% fragmented. I wouldn't be surprised if that's part of your problem.",2.0
g2wizk4,igcnkb,"i second that.

also consider taking the scripts to an agent job, schedule the job at night or whenever it is possible to run, and set a timeframe where sql server can work through all affected index. get a list of affected index and loop through this list, before starting with the next index check the time so you can abort if you hit your time limit.",1.0
g2tg2z6,igcnkb,"What method are you using to do your maintenance? Maintenance Plan or other?

Are you doing a brute-force rebuild every week, or only the tables that really need it? Have you considered multi-threading your maintenance job (if you're hosting 1TB of data, you probably have more than 2 cores)? Ola Hallengren's Maintenance Solution lets you do this, I don't think Minion Reindex does yet (it does multi-thread checkdb however).

If you're doing _both_ a weekly rebuild of your indexes and stats update, *stop*. Rebuilding the index also rebuilds the statistics, so you're doing double the work.

&gt;Any DBAs want to chime in on what I could do to archive historical data and make the DB smaller? That's probably a tough question to answer not knowing my DB, but in theory, is there a query I could run that could somehow archive data and insert it into another DB where records older than xxx date?

Is there a single query? No. You know your data and environment, you'll have to write those data movement queries yourself. Foreign keys can make this a bit of a chore. But you also have to take into consideration the business requirements - what data can you have in ""near-line"" storage vs. the live database?",3.0
g2thaqm,igcnkb,"I'm doing a maintenance plan that does this:

&amp;#x200B;

Rebuild Index Task

DB: ######

Object: Tables and Views

Original Amount of free space

&amp;#x200B;

Than it goes to a Update Statistics Task

DB: #####

All existing statistics

Scan Type: Full Scan",1.0
g2tmpsy,igcnkb,"Yeah you’re losing a lot of time there to a brute-force index rebuild. And the subsequent stats update isn’t gaining anything at all, just eating cycles. 

Go get Ola Hallengen’s Maintenance Solution. Https://Ola.hallengren.com/. Install it, including the Agent jobs. Then schedule the index &amp; stats jobs and disable the maintenance plan jobs. 

Ola’s jobs will more intelligently look at the churn on the tables and only do rebuilds/reorgs where needed, and you can tune the thresholds if the defaults aren’t doing well enough for you.",6.0
g2u6gmf,igcnkb,Ola jobs are legit. Implemented them at a previous role and whenever I pop in there on that team my ola solution is still there untouched. Even using them to do the backup and t-log backups. Legit even adds new databases on the instance to the backup maintenance automatically. Lol. Amazing stuff.,1.0
g4sci6o,igcnkb,Is there an equivalent of https://Ola.hallengren.com/ but for postgresql?,1.0
g4sjrv0,igcnkb,No clue. I barely use Postgres.,1.0
g2tk6sn,igcnkb,"On top of everyone else's comments, you don't mention if this system is transactional or reporting.  If you can afford to archive historical data, it's likely not feeding into a transactional system, which makes me think your entire architecture is wrong.

I'm not saying you can't put a 1TB reporting database in MSSQL, plenty of people do, but it wouldn't be my first choice and there are a lot of configurations both hardware and software that you'd want in place first.  Number one of which would be columnar storage.",2.0
g2tmjvy,igcnkb,Its both which increases the complexity of this project. I agree maybe MS SQL was not the best approach but that is how this application was setup before taking ownership of this architecture.,2.0
g2tna1i,igcnkb,"The first step should be to take the data that is used by the application (live data or operational data) and make that highly available and fault-tolerant. At the same time move it away from the analytical side, so that a rogue BI query doesn't impact the performance of the application. Then you can work on making the analytical database faster.",1.0
g2svdhu,igcnkb,"I think you might be hitting the point where you should look into big data solutions, maybe Azure SQL Database would suit you. I used to be a DBA for a similar sized db that was rebuilt every day during the night. This was however on a separate hardware and a MPP DB solution.",1.0
g2svnuo,igcnkb,Has the limitations of AzureDB increased? I thought they were around 1TB,1.0
g2t04oq,igcnkb,"https://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/resource-limits

It says it's up to 8TB depending on the use type and number of cores.",1.0
g2vsihs,igcnkb,"What happens if you don't rebuild everything every week? Could you split your tables into two groups and rebuild on alternate weeks?

What sort of storage are you using? I've found some of the traditional ""best practices"" don't have the same impact in the age of SSD and nvme storage. If you're on fast low latency storage maybe see what happens to performance if you go a few weeks with no rebuilds.",1.0
g2yc6nv,igcnkb,"I tried this and it did cause a performance hit, in which I had to resume the rebuild. I could split it, I would just need to locate the top tables and see what I could do with the rest",1.0
g33hgux,igcnkb,"Can you do online rebuilds? If so, could they overlap with system load, maybe helped by setting a lower MAXDOP?",1.0
g2vswhl,igcnkb,"What’s the underlying storage : spinning disk or SSD? 
Many of the previous comments are great, but if your data is on SSD then fragmentation has no delay effect so rebuilding indexes will only reduce their size. On modern systems I do daily stats updates which on 1-2TB databases takes 20 min or less usually, and Hallengren dynamic rebuild/reorganize at 40/60% thresholds. 

If spinning disk, and if you can, try to move some highly used indexes to their own file groups  on super fast storage - SSD or NVME. Multiple controllers etc - not sure how close you are to the hardware but most storage can be made much faster to a point. 
RAM : what’s your page life expectancy like - in the hundreds or many thousands?  If under a few thousand, consider adding more RAM.",1.0
g2ybzwa,igcnkb,"spinning disks, I wouldn't have enough RAM to move them to. I would like to have anything other than regular disks but at this time is all we have :(",1.0
g2ydiui,igcnkb,"For everyone recommending Ola Hallengren's Maintenance Solution, I'm just a hesitant to do this in production and something ""goes wrong"". I will do some research on it,want to make sure there is no repercussions of implementing",1.0
g33m77o,igcnkb,"&gt; I'm just a hesitant to do this in production and something ""goes wrong""

Tens of thousands of DBAs use Ola Hallengren's stuff every day. Many of the consultants I know who do ""DBA as a service"" work for companies without full-time DBAs install it almost immediately on their clients' systems. I've even heard Microsoft folks recommending Ola's maintenance solution.

Which also means that if you call a consultant to help you out of a jam, there's a good chance they'll see the backup &amp; index maintenance jobs sitting there and know where to start - it's giving them a head-start on things.

If you need to change something in how the jobs run, you don't have to change a maintenance plan, you just add/change some parameters in an Agent Job step.

The code's out on Github and Ola's website if you want to comb through it before installing. It's out in the open, no secrets hiding in it.

But you are right to be hesitant to just do something in production as your first time. So install it in your non-production environments first, get a feel for how it all works, and then move on to production. Just make sure you disable those maintenance plan jobs once it's installed, or you'll make even more trouble for yourself.",2.0
g2spw7x,igb8sh,are you asking about SQL Server specifically or?,1.0
g2symfi,igb8sh,I mean yeah let’s say in a sql cluster. Is it correct to say always on is active active and the other active passive?,1.0
g2syzfj,igb8sh,I mean SQL Server as in the specific microsoft product,1.0
g2sz55y,igb8sh,Yes! For me sql server,1.0
g2szojy,igb8sh,"I don't know much about SQL Server as I've never used it, but https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-ver15 might help. 

I think active-active is what microsoft calls ""always on"", but I could be wrong.",1.0
g2udbvj,igb8sh,It is I’m just confused how it would work on the storage side of things,1.0
g2t60c2,igb8sh,"a traditional cluster is a number of instances/servers exposing a singular interface to a user. Usual intent is that the failure of one of the nodes in the cluster is transparent to the user. Sometimes it is done as a method of load balancing as well. Usually it is implied that some physical resource sharing exists.

always on availability group is a software solution for SQL Server that supports high availability or read scalability. It is built on top of windows failover clustering - a technology that allows independent servers to share certain resources, to present a singular end point (interface) and to establish a quorum/voting system. No actual physical resources might be shared in this case.",1.0
g2shlne,igags4,"You can't do an equality test against `NULL` because `NULL` has no value. You need to make that `extra IS NOT NULL`. You might also need to check for an empty string, `extra IS NOT NULL and trim(extra) &lt;&gt; ''`

&gt;There is no primary key for this table, it may have duplicate rows.

Unless you have a good reason for this, you really ought to fix it.",9.0
g2shwx4,igags4,"&gt;Unless you have a good reason for this, you really ought to fix it.

Its not my personal database. Its an online problem and I pasted the description.",2.0
g2thhay,igags4,"You can also do `ISNULL(extra, '') = ''` which will check if it is NULL OR '' all at once",1.0
g2tmy5t,igags4,Possibly at the expense of SARGability,1.0
g2shl4x,igags4,I think it’s because **extra!=null** should be  **extra is not null**.,3.0
g2shx3a,igags4,Thanks,2.0
g2su01a,igags4,"You got your answer, but to further explain, Null != Null

Since Null has no value, it doesn’t equal anything and any equality test will fail. As others said, is Null is the solution. Also often can use x != x and this will show something is null, it doesn’t even equal itself",1.0
g2sfnf2,igaa4j," One of the key scenarios that is supports is the ability to assess and migrate your databases from on prem into Azure.  Utilizing the Data Migration Assistant (DMA) to discover and assess your on prem SQL databases with a view to migrating them either as a SQL database hosted inside a virtual machine in Azure, Azure SQL Database or Azure SQL Database Managed Instance.",1.0
g2tuu3s,ig57yd,"Can you give a more specific definition of what you are trying to achieve? I'd recommend starting by checking out DAMA, [I've written about it](https://jonshaulis.com/index.php/2019/09/24/what-is-dama/) if you wanted a quick synopsis. This is a tomb of information that has a lot of fluff, but also a lot of great ideas and concepts. More than that, it offers resources on these specific ideas and allows you to dive deeper down the rabbit holes. It is largely based around getting certified, but there is a wealth of information to be found in it.

If you have some specifics you can give us, I can give you a better-tailored answer.",2.0
g2u7oh9,ig57yd,I was looking for some resources which could help me understand the business side of Data Management end-to-end.,1.0
g2u89zw,ig57yd,"The DAMA book won't give you that, but it's a tome of knowledge I'd still recommend here. [I'd check out this book though](https://www.amazon.com/Navigating-Labyrinth-Executive-Guide-Management/dp/1634623754/ref=pd_bxgy_img_2/146-6145462-2539639?_encoding=UTF8&amp;pd_rd_i=1634623754&amp;pd_rd_r=70ca2f57-a90f-461f-8c3f-5de8afc66feb&amp;pd_rd_w=DCiVZ&amp;pd_rd_wg=yJrbs&amp;pf_rd_p=ce6c479b-ef53-49a6-845b-bbbf35c28dd3&amp;pf_rd_r=QSE7KXQE0MX2AT1KTBZJ&amp;psc=1&amp;refRID=QSE7KXQE0MX2AT1KTBZJ#customerReviews), I think it's the starting point of your journey.",1.0
g2u8h54,ig57yd,"Thank you so much. 

Can I DM you in the future if I need further help?",1.0
g2u9x8g,ig57yd,"Sure thing, I'm always happy to help.",1.0
g2sf807,ig57yd,"I joined an Enterprise Architecture team that lasted about three months trying to find the right answer. Unfortunately our director went on leave a couple weeks in, so we were all just googling and trying to find the best way. We lost funding and went to different teams. 

I recently saw a job with those requirements, I just moved on.",1.0
g2tv9mb,ig57yd,"Enterprise Architecture is extremely expensive and conducting every activity related to data is far too costly. It's best to identify your mission and vision, then perform an evaluation. The assessment should help indicate the areas of need that can be improved so you can focus. I see too many projects start out ambitiously and fail slowly. It's always best to take your highest ranked items which you can leverage quick easy wins to snowball into bigger projects. Many items are more process-based than technology, so if the culture isn't there, it's not happening.",2.0
g2shfdw,ig2p0s,Beginner isn’t what you’re looking for if you’ve already covered sub queries.,1.0
g2sj3t3,ig2p0s,That’s what I was thinking as well. Where do you think I should go from here in terms of getting more advanced? Any courses/textbooks you recommend?,1.0
g2snz6r,ig2p0s,Take a look at data camps courses. If you sign up for a MS VScode account you can get two months free on data camp (you can keep making new emails after the 2 months runs out).,2.0
g2r6793,ig1zhc,only 13 crimes reported in Chicago...,1.0
g2r6ga3,ig1zhc,Well I’m manually inserting the data myself. I just wanted to plug and chug like 25-50 so I can practice sql queries with it. Plus my laptop isn’t very good. I don’t have much memory to like download the csv. I’m trying to save up for a better laptop.,2.0
g2r7d6j,ig1zhc,"i know i am messing with ya, have fun with SQL. I really like the book learn sql in a month of lunches books. i had Sql reporting thrown at me at my last job and just had to learn it. I got by with what i needed to do, i didn't do anything crazy advanced. I am now trying to develop an app for our on site police team so they can log incidents and stuff so i am getting very familiar with SQL and the .Net frameworks",2.0
g2r9zl8,ig1zhc,Oh haha. I really am enjoying myself. I’m just working on getting comfortable with it. And nice man! At least you got the experience. I’m assuming the app would be specific to your area? Good luck with it tho.,2.0
g2rb5pg,ig1zhc,"Yeah it's my first real experience building something of this nature, I usually just build android games or something simple. I have a github for it, but it might not be updated if you were interested. Basically i am doing a lot of copy and paste (as long as i know what the code means) and a ton of rework. I am learning a ton from it, which is always the goal.",2.0
g2re0j3,ig1zhc,"Hell, I can still check it out. I wanna say I’m still a newbie to this coding but I gotta say I’ve been loving it. The more involved I get, the more I can understand and the more I want to learn. I love it.",2.0
g2rtcux,ig1zhc,"[https://github.com/millionandbell/Security-Database](https://github.com/millionandbell/Security-Database)

Here is my current build, i actually just updated it. I have a lot of work to do and a lot of cleaning up to do before it is even close to ready for production. I am also still learning GitHub so everything might not be perfect on there. If you have any questions you can PM me.",2.0
g2s7gic,ig1zhc,Awesome! I’ll check it out! And will do.,2.0
g2qfj4o,ifwlo9,"A question that open ended is just probing to see if you actually do have any real experience with query performance tuning. I think your response is fine, but if you're concerned with coherence and succinctness, I'd pad the beginning with some variation of: ""It really depends, but assuming I've already gotten familiar with the data and what the query is trying to accomplish, I'd look at X and Y, then try A, B and C.""

Really, the culprit of a slow running query can only be one of three things: the query code itself, the database structure, or the hardware (server capacity, bandwidth, etc).

As long as you check all the boxes and mention the obvious things like refactoring (the code), checking for covering indexes (the database), and making sure the server isn't overloaded (the hardware), you can offer to expand upon any of these in greater detail.",61.0
g2rc54m,ifwlo9,"This is such a good answer. When I was still an operations DBA and we didn't have a performance team, I helped out during so many interviews and there were so many canned answers where I could tell they'd done enough research to answer, but not enough to know if they'd actually understood and were just bad at interviewing / thinking on their feet or if they really didn't get it. An answer like this and I'd be pretty sure you had a good understanding if you answered the follow up questions well.

To OP: Side note, the answer will also vary based on the position you're interviewing for. I expect far more from a DBA than a SQL Dev or Business Analyst. For DBA, I'd hope you'd mention looking at the server waits, jobs running, DMVs, need for archival, which Anti-patterns you'd look for, etc. Whereas for a dev, I hope you have at least some understanding of the app behavior and execution plans.",7.0
g2qfg0d,ifwlo9,"Your response has fixes, but no troubleshooting.

How do you know if you need to switch to temp tables or add an index?  You wouldn't try one unless you had a reason to.  Maybe lead with how you decide what the problem might be, then drop a few quick samples (like your response), and then finish with how you'd fix it.

For example:

I like to start with the query plan for the entire script, to see where it's spending a lot of time.  I look for things like X, Y, and Z.

From here I can make an educated guess at the problem.  Sometimes it's A, sometimes it's the opposite of A!  Sometimes it's B, or M, or even a BM!  (OK, leave the poo jokes out, as accurate as the description can get.)

And then finish up with specifically HOW I would test the fix before proposing it.",14.0
g2q8mp6,ifwlo9,"Why do you feel that way? What you said sounds perfectly valid to me.

In a job interview situation theres also no harm to just ask if they want to go more in-depth into something specific.",7.0
g2qahzw,ifwlo9,"And if thats the case, then I'm happy. I think the optimisation of SQL scripts is probably a more indepth discussion than a typical interview scenario would allow for. 

Perhaps that is something I should be aware of, and structure my answer in a way which shows I understand the fundamentals, but when asked I can go in to more detail.",6.0
g2qi3wp,ifwlo9,"a.)  ""I write a python script in 15 minutes that does it faster!""

b.) ""I can't, only the DBA can.""

c.) Request IT add use-the-index-luke.com to the internet whitelist

d.) Ask how do YOU improve the performance of an SQL script, and pull out a clipboard for taking notes.

e.) Ask ourselves if we're really about improving ourselves and the code was a metaphor all along

f.) Realize that most interview questions are just that, questions, asked by human beings probably forced at gunpoint to come up with questions during the interview, and that how we answer is less important than how we read the room and put at ease concerns anyone might have with our credentials, capabilities, or awkward lunches in the future.",16.0
g2rorcm,ifwlo9,I like your style.,3.0
g2qj8rb,ifwlo9,"What others haven't mentioned is the whole identifying-what-the-problem-really-is thing.

Although this is a SQL interview, and they likely want the answer about improving the performance of SQL queries, step 1 is always to identify where the slowness is. Is the query actually taking a long time? Or is this an n+1 situation? Do you know what a query plan is, and how to identify which parts of it are indicative of a problem? Profile, profile, profile.

c.f. Rob Pike's 5 Rules of Programming: [https://users.ece.utexas.edu/\~adnan/pike.html](https://users.ece.utexas.edu/~adnan/pike.html)",3.0
g2rl3pa,ifwlo9,"I usually laugh and first say that I just go online to ask for help to get a consensus of opinions from other experts.

Then I talk about how there is no real way to do it, it's just a process where you start ripping the guts of the query out. The first thing I personally do is rewrite the entire query into my preferred formatting style. This doesn't take too long, and seems pedantic, but it lets me scan over every single line of code, which generally highlights several interesting sections where their might be room for improvement. Then I slice the thing up, and start running tests to see what works better using all the things you just talked about (#tables, etc.) Then I put it all back together again and see if it works better.

I liken this process, and many other processes in SQL to just hitting things with a hammer until they do what I want, and I go on to say that it is possible the query cannot be optimized, and that changes may need to be made to either the environment infrastructure (i.e. server speed,) or upsteam in some other application layer/database where a new index needs to be created, or data needs to be stored in a different way. Sometimes it might be the best it can be, but you won't know until you just rip it apart and try new things, and how do I learn about new things? I come online to forums to ask other experts... so I tie the entire explanation back to the light joke I made at the start and finish by saying something like, ""In the past I've done a lot of work on that and &lt;insert specific example, from specific job on your resume&gt;, but what I primarily did there was &lt;insert specific thing that relates to the job you are trying to get&gt;. Do you have a lot of queries that need to be improved in your existing environment? ""

The intent here is to get them to talk about the thing you just mentioned. It's sort of a subtle interviewing technique where you throw out a topic at the very end of your answers where you want the conversation to go. The general idea here is that you are bringing up topics they might want to question you about *before they do*, and in doing so you are able to control your answers and prepare for them in advance. It creates a very disarming sort of conversation because you're ""helping"" them get to all the things they want to ask, and all the while you're asking them pointed questions about the job you're interviewing which combined together demonstrates both proficiency, and the probability that you can handle the job.

I like to try and have an interviewer walk away thinking that I was the easiest person that they ever interviewed. I brought up topics before they did, and answered questions before they asked them. It was like one of those romantic serendipitous first meetings where you're completing each others statements, and making little 'inside' jokes about how ""everyone has a lot of queries that need to be improved, trust me, your environment can't be as bad as &lt;insert another specific example / job you had&gt; and tell them how you went on to institute best practices, and rewrite their jobs until they were operating efficiently.""

Be personable but don't be shy. If the job isn't about that, then try changing the topic to talk about something the job is about after you get done making your little jokes. You want the conversation to feel smooth, but you also want to give off a slight impression that they need you more than you need them, and that you aren't afraid to walk away if the job isn't a good fit. This is particularly hard to do, and you don't want to overtly make them feel that way, but you do want them to think that you are particularly interested in finding out if this job is a good fit or not. The intent here is that you want them to *think* you're a good fit for the job, but to be slightly concerned that *you might not think the job is a good fit.* Not that the company isn't a good fit, or that the interview isn't a good fit... but like the actual job, which you're spending so much time in the interview trying to ask questions about. Because why else are you asking so many pointed, educated, and relevant questions? No one else did that... why are you? Is there something wrong with us? You've been so polite, and this interview has been so magical... do you have a concern? What can we do to make you think this is a good fit? 

That's the way you want them to feel, and then if it really is a good fit be up front and tell them you are very excited to work there, that you are very confident you can do the job and solve their problems, and specifically ask them what they if any challenges there are you to be successful in the role you're applying for. Just when they were worried you might be concerned you tell them that you love them. 

One last thing: Once you ask a question learn to shut the fuck up and let them answer it, let them spend 5 minutes bullshitting you with a bunch of generic crap, and then when they finish, repeat the cycle above. Address something specific they said, tie it to a specific job you've had, and then use it as a springboard to ask your next question.

edit: Always tell them you've interviewed at a few other places and are still in the process, seeking final interviews, etc. You'd not only be amazed at how quickly some companies will extend an offer, but you'll also be amazed at how quickly they'll schedule followup interviews if they like you.",4.0
g2qianj,ifwlo9,"If you get a broad ass question like that, I think your answer is fine. The only other thing I can think of off hand is looking for crap left joins. But, that falls in line with using a temp table where appropriate, which you mentioned.

I'd ask if they had a specific example or script that we could talk about in detail.

I just fixed something the other day that was causing deadlocks, because a repetitively harmless looking query, was doing page locks instead of row locks. The fix was counter intuitive, but worked.

Generally speaking, when we interview for SQL guys, we usually just say can you tell us about a time you're proud of where you needed to debug/optimize something and what did you do. Gives you an opportunity to talk about some scenario where you knew the data really well, and give details on what you did",2.0
g2r7nry,ifwlo9,"Some good answers here already, so I'll just add some icing to the cake.  

One thing would be to clarify whether they're talking about an external script versus a stored procedure. If it's the former, you can improve performance by saving it as a stored procedure, which is almost always faster on account of query plan caching. I want to say there are script-specific enhancements you could do, but can't recall any off the top of my head.  

Another option is to point out that Step 0 is to identify poorly performing code in the first place, and offer to circle back to that topic after addressing the immediate question. From there, you can talk about using monitoring tools, querying the DB's archival performance measurements, setting up alerts, and what sort of schedule to do those things on.",2.0
g2sfbrv,ifwlo9,"When you as a question like that you are trying to determine how a candidate's thought process works. What steps would they take to solve a problem? Do they take logical, orthogonal steps or do they just add indexes &amp; call it done?  Do they look at the underlying data or just the query?  Do they know sometimes the underlying platform can cause issues.",2.0
g2sgcmt,ifwlo9,"First we need to check the query execution plan of the script. Then we have to check how to minimize the cost for individual steps/tasks. Based on where it is costing more, we can then think of creating indexes, temp tables, updating statistics, rebuilding/reorganzing existing indexes, etc.
We can also check the wait types and log for any hardware or network issues. 
The answer to this question really depends on why the performance is slow, then we can take any action based on it.",2.0
g2q5rq5,ifwlo9,"Hello u/mikeyd85 - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g2rah0i,ifwlo9,"""Ask someone who knows about Sql""",1.0
g2rn9rs,ifwlo9,I put my where condition back in on my date dimension table that for some reason spans from 1900 to 3000. Not sure why did that but dammit I'm sticking to it.,1.0
g2rr65r,ifwlo9,"If i was interviewing... this would be a gold star answer:

Using statistics io and time, exec plans, and my extensive knowledge of sql syntax... I would try several surgical approaches on problematic sections of queries, choosing the most optimal path given the situation.



There exists people who write off some syntax because they heard it was bad. And those that default to temp tables when the physical layer is poorly structured (lack of dbfiles and groups for nonclus/columnar/blob indexes, pk identity as a clustered index, etc). Being too specific in your answer may drum up past experiences for the one conducting the interview. Best to keep it vague, yet show you know exactly what to do.",1.0
g2sbybt,ifwlo9,I put my witch doctor mask on and start chanting at the keyboard.,1.0
g2swsrq,ifwlo9,If it’s pulling from a juicy view that takes forever to run I just throw that baby into its own table lol,1.0
g2t6f51,ifwlo9,good answers,1.0
g2uqoyh,ifwlo9,I fixed the performanceof a query the other day by removing a join from a 500 row on  a million row table.,1.0
g32b7ks,ifwlo9,"Ask. Which database, OS and hardware you are running on. Otherwise you cant really answer the question.",1.0
g2qekvo,ifwlo9,Index everything baby 😎,-1.0
g2q1l9k,ifv37u,"A `timestamptz` column will automatically be converted to the ""local time zone"" (defined by the session's timezone). there is nothing you need to do. 

The `at time zone` operator is intended to convert between the two timestamp types (`timestamp` ==&gt; `timestamptz` or `timestamptz` ==&gt; `timestamp`) in a consistent way.

In fact, if you want your `timestamp_utc` to display in UTC, you need to apply `at time zone` for **that** column. 

```sql
select timestamp_utc at time zone 'UTC' as timestamp_utc, 
       timestamp_utc as timestamp_local
from ...
```

If you want the output of a `timestamp` column to include the offset, you could use `to_char()`

```sql
select to_char(timestamp_utc at time zone 'UTC',  'yyyy-mm-dd hh24:mi:ssOF') as timestamp_utc, 
       timestamp_utc as timestamp_local
from ...
```",1.0
g2q6jj1,ifv37u,"&gt;In fact, if you want your timestamp\_utc to display in UTC, you need to apply at time zone for **that** column.

That's actually not the behavior I'm observing. I just did a `select *` from a table with multiple `timestamptz` columns, and in both Redshift and Postgres (11.2) they're displayed in my client (Postico) with the time zone, ie \`2020-08-24 09:23:45+00'

Didn't know about the `ssOF` formatter for `to_char` though; I'll try that! Thanks!

\[EDIT\] Just tried `to_char(timestamp_utc at time zone timezone,  'yyyy-mm-dd hh24:mi:ssOF') as timestamp_local` and the output had the timezone as +00 instead of -07. Which actually makes sense now that I think about it, because after the `at time zone` the data is in `timestamp` format with no timezone, so the `ssOF` formatter doesn't know which timezone to use.",1.0
g2qisq1,ifv37u,"&gt; and in both Redshift and Postgres (11.2) they're displayed in my client (Postico) with the time zone, ie `2020-08-24 09:23:45+00'

Then your client's time zone is set to UTC",1.0
g300yaj,ifv37u,"I'm trying to create a new table, so I'm less concerned about how the data is displayed in the client and more concerned about how it's being stored in the db.",1.0
g2pro83,ifsy0w,"you need to end case expressions with ""END""

e.g. 
 

      case when trans-code = x then trans-date END",4.0
g2pv1sc,ifsy0w,"Your case statment is wrong. Two ways to write a case statement

    case country_code
      when 'CH' then 'Switzerland'
      when 'FR' then 'FRANCE'
      when 'AL' then 'ALBANIA'
      else 'Some other country'
    end

In the example above, we are evaluating distinct not null values of a single column (The else part is not required, but if none of the conditions are met, then the result will be NULL)

Another way

    case
     when country_code is null then 'No Country'
     when country_code in ('GQ', 'GNQ') then 'Equatorial Guinea'
     when country_code = 'TW' and destination_country != 'CHINA' then 'TAIWAN'
     when phone_country_code = 672 then 'ANTARCTICA'
     else 'Something else'
    end

In the example above, we are evaluating different conditions based on a variety of columns",2.0
g2ppop1,ifsy0w,Can you post up a sample of the table(s) you are querying?,1.0
g2qp8f1,ifsy0w,"as /u/ichp has pointed out, each case statement is missing an ""end"", however you need to make sure that each part of the datediff statement actually has a date to compare against, in other words what will the trans-date be if it's neither x nor y?

What I'd probably do is create variables for the opened date and payment date,initialise them to null, populate them with separate select queries, and only run the datediff if they're both populated. This gives you the option to inform the user if either date isn't found.",1.0
g2p7uhh,ifqw8u,"&gt; Is the a ordering the columns matter in GROUP BY clause?

no",8.0
g2p76zw,ifqw8u,"So I've never actually come across this but did a quick check using some data.  
I had a product\_id, sum(quantity), category, description

&amp;#x200B;

all results come back as random, but the sum never changes. across all three different ones.

  
If you aggregations are changing then likely there is an error in your code that we may be able to help with.",2.0
g2p80dz,ifqw8u,"I don't think the order is matter. It only looks at the unique combination such as

info says: year 1,2,3 and subject: cs, math, db  


i use group by year, subject ( or subject, year)

it will show 9 combinations. cs 1, cs 2, cs 3, math 1, math 2, math 3, db 1, db 2, db 3",1.0
g2sqcwc,ifqw8u,"That is quite a nice question, and something I've also wondered when I first started learning about it.

Not necessarily the 'sort order' .. but is grouping first by a, then b, equivalent to b, then a?

So in other words, if you are grouping by year and month, does it matter if you group by year, then month.  Or month, then year.

Always good to use actual an actual query and see the result set to get a better understanding.",1.0
g2pjemm,ifqw8u,"No, it will not.  The GROUP BY clause defines ""These are the bits of data that can make a row unique - if they're all the SAME, then aggregate.""

It won't even affect the output sort order in any predictable way (if at all - I've never seen it tested).",1.0
g2ptyyz,ifqw8u,"group by and order by are separate clauses. You may notice in a particular RDBMS that when you GROUP BY a set of columns, the output rows also appear ORDERED BY the same set of rows, but that is an implementation ""artifact' and should never be depended on. If you want to order rows, always include the ORDER BY clause.",2.0
g2pu9a6,ifqw8u,The presentation order only depends on what we actually put in the order by clause,1.0
g2p60k3,ifqunh,"if you want to play with 'live database'. localhost is fine in your case.   


MySQL, MS SQL, and/or PostgreSQL

  
to practice the SQL queries:  


[w3schools.com](https://w3schools.com/)

[sqlzoo.com](https://sqlzoo.com/)

[hackerrank.com](https://hackerrank.com/)",3.0
g2pynpd,ifqunh,"**Learning SQL**

A common question for newcomers is how to learn SQL. Please view the [wiki](https://www.reddit.com/r/SQL/wiki/index) for a list of online resources, or [review existing posts about learning SQL](https://www.reddit.com/r/SQL/search/?q=learn&amp;restrict_sr=1).",3.0
g2uv3x7,ifqunh,Of the sites listed in the wiki are there any clear standouts?,1.0
g2uvgg3,ifqunh,[http://sol.gfxile.net/g3/](http://sol.gfxile.net/g3/),1.0
g2re4b5,ifqunh,"Learn the basics on W3Schools. They provide easy to understand and interactive exercises. After this, check out strata scratch site. There you can test your skills and learn some advanced and real topics.",2.0
g2pd4nz,ifqunh,"We have a tool that may interest you:

[https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/](https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/)",1.0
g31xzzf,ifqunh,I'm starting with SQL bootcamp from Udemy and supplementing with W3School and SQLZoo,1.0
g2pf5zt,ifqunh,"New York City visitor: excuse me officer, can you tell me how to get to Carnegie Hall?

officer: sure... practise, practise, practise",1.0
g2p4il1,ifqagz,"consider the topic called entity-relationship model (ER diagram or ERM)  


Also: hint: you want to self-join the table itself.",1.0
g2pe4ru,ifqagz,One `EMPLOYEE` table should suffice.,1.0
g32bt91,ifqagz,If you really want to understand modeling.Skip the ORM. Design the tables write the SQL queries you need.,1.0
g2ojss8,ifn6xi,"i would normally write `WHERE Key NOT IN...` but your way is fine too

looks like the problem is an unindexed Key column in List1

try this instead --

    SELECT *
      FROM Test2
     WHERE NOT EXISTS
          ( SELECT Key
              FROM List1
             WHERE Key = Test2.Key )",6.0
g2okqoi,ifn6xi,"I gave it a run and it looks like Access will again commit suicide on me, haha. 

&amp;#x200B;

Is there a possibility that the \* over concrete selections slows the process down? This might be totally irrelevant, but I assume less Columns means less work to do?",2.0
g2pcpxa,ifn6xi,"Another reason that using exists is better is that if the column contains nulls then the filtering won't work as one would expect anyway.

http://www.sqlbadpractices.com/using-not-in-operator-with-null-values/

Are you able to post the SQL as it is when you use exists rather than posting pseudocode?",3.0
g2pjio0,ifn6xi,I'll give this a shot tomorrow. Thanks!,1.0
g2ooldq,ifn6xi,"not irrelevant at all, good catch

what happens when you run it with only the columns you need?",2.0
g2op3bb,ifn6xi,"Gave it a try right away, but no differing result.

What drives me crazy is how

    SELECT Key
    FROM Test2 
    WHERE Key IN (Select Key 
    From  List1)

works like a charm, haha. Gives me all the ""duplicats"" I am not asking for. So it must have something to do with how the NOT is approached.... I'll give you a feedback when I figured it out!",2.0
g2oppie,ifn6xi,how do you create an index on a column in Access?  i actually forget,2.0
g2ot4df,ifn6xi,"Uff, don't ask a Noob such questions, haha. I allways did it in the table editor, but I have no clue if thats the best way.",1.0
g2pak5u,ifn6xi,"It's CREATE INDEX *indexname* ON *tablename* (*fieldname*)

So to create an index on ""Key"" in List1 it would be something like:

CREATE INDEX ix_List1_Key ON List1 (Key);",1.0
g2pjjhp,ifn6xi,I'll give this a shot tomorrow. Thanks!,1.0
g2p5cvf,ifn6xi,"Have you tried this? 
    
    SELECT *
    FROM TEST2 T
    LEFT JOIN LIST1 L ON T.KEY = L.KEY 
    WHERE L.KEY IS NULL 
I tend to do exclusive joins instead of using `NOT IN`",4.0
g2rxn31,ifn6xi,"Yo, thanks a lot! This works like in a charm and in no time! 

&gt;SELECT \*  
FROM TEST2 T  
LEFT JOIN LIST1 L ON T.KEY = L.KEY   
WHERE L.KEY IS NULL 

The T you put after the table-name works like a ""Test2 = T"" so you can write ""T.Key"" instead of ""Test2.Key""?  Like: **From** *the* **table** *named* **T.** 

Thanks for the Input! The first steps are allway a lil iffy, haha.",2.0
g2suwxp,ifn6xi,"Yeah no problem. `NOT IN` is the same thing as writing `!=` (not equal to) a bunch of times. For example 

    ...WHERE L.KEY NOT IN (1, 2, 3, 4, 5) 

is interpreted as 

    ...WHERE L.KEY != 1 AND L.KEY != 2 AND L.KEY != 3 AND L.KEY 
    != 4 AND L.KEY != 5

This can cause performance issues with large tables.

And yes, the `T` and `L` are called aliases, you're exactly right that they're just there to represent the tables so we can write `T.Key` instead of `Test2.Key`


Here's a helpful diagram posted on this subreddit a while ago.

https://i.redd.it/dyqnzpuddxk21.png",1.0
g2pkhu4,ifn6xi,I'll give this a shot tomorrow. Thanks!,1.0
g2q88j5,ifn6xi,"This is what I'd do as well, assuming it works in Access. Also make sure you have an index on the Key column.",1.0
g2rxu6q,ifn6xi,"Thanks for the Input! 

&amp;#x200B;

What helped me was: 

a) turn the queries to tables so I can index the ""Key"" columns (minor contributor?)

b) Use: 

&gt;SELECT \*  
FROM TEST2 T  
LEFT JOIN LIST1 L ON T.KEY = L.KEY   
WHERE L.KEY IS NULL 

Thanks for the Input!

&amp;#x200B;

(this is a copy and paste text to keep you updated. Thanks for engaging in helping me! You dont have to reply.)",1.0
g2qicj9,ifn6xi,I agree as well. I typically will left join and say where the key is null.,1.0
g2vht1c,ifn6xi,This,1.0
g2olo8t,ifn6xi,Is it an Access database or are you using Access as a front end for something else?,1.0
g2omjbm,ifn6xi,The Data I work with was important from Excel Sheets into Access. I therefor assume it realy is an Access Database.,1.0
g2ov73y,ifn6xi,"For the sub query try 

SELECT key FROM List1 WHERE key is not null",1.0
g2oxabv,ifn6xi,"Access commited suicide on me again. RIP.

Thing is, I am borderline sure the Code works fine, it's just the amount of Data that my PC can't manage somehow. Is that even a realistic possibility?",1.0
g2ozamr,ifn6xi,If the column Key is nullable in either dataset then IN/NOT IN becomes an expensive operation.  Rewriting as an EXISTS offers better performance and is logically what you're looking for.,1.0
g2p06fw,ifn6xi,"Oh, this sounds like a good idea. So like:

    Select *
    From MainTable
    Where Exists (MainTable.key = Searchtable.key)

Okay, you dont have to reply if you dont want to, I gonna have to look this up either way. Thanks for the input!",1.0
g2peijt,ifn6xi,"From what I remember access will die if a query hits 2gb. 

You need to find a way to make the request smaller.

Does select * from test2 run?

If so, create a second table with a lost of keys that you DO want. Join onto that table. In your where clause put where tableb is not null.",1.0
g2pjhpu,ifn6xi,I'll give this a shot tomorrow. Thanks!,1.0
g2rxww3,ifn6xi,"Thanks for the Input! 

&amp;#x200B;

What helped me was: 

a) turn the queries to tables so I can index the ""Key"" columns (minor contributor?)

b) Use: 

&gt;SELECT \*  
FROM TEST2 T  
LEFT JOIN LIST1 L ON T.KEY = L.KEY   
WHERE L.KEY IS NULL 

Thanks for the Input!

&amp;#x200B;

(this is a copy and paste text to keep you updated. Thanks for engaging in helping me! You dont have to reply.)",1.0
g2ozq3l,ifn6xi,"I think the Syntax is OK, but I don't work in Access any more unless I really have to...  That syntax will work in SQL Server and the two generally take the same code (wildcards and how you mark datetime in a query are the only ones I ever ran into).

How wide are those tables?  30k rows is nothing if they're two columns wide, but if they have 10KB XML strings in them it's a whole different ball of wax.

Does access let you put Indexes on tables?  If so, stick one on Key (in both tables, though it will likely only choose one).  At 30k rows it starts to matter a little, and it's possible your tables are just too big.

&amp;#x200B;

Of course, you could much more easily just go

Select table2.\* from table2 RIGHT OUTER JOIN table1 PM table2.key = table1.key WHERE table1.key IS NULL

(If this is school work, this is likely the answer the prof is looking for - determining if you understand the difference between an outer join and an inner join.)

Edit:  Whoops, Table1 should be the one fully included, switched the outer join direction and which table we're checking for a null.",1.0
g2p29vo,ifn6xi,"&gt;I think the Syntax is OK, but I don't work in Access any more unless I  really have to...  That syntax will work in SQL Server and the two  generally take the same code (wildcards and how you mark datetime in a  query are the only ones I ever ran into).

Thanks for the input. I didn't actualy choose to work with access, it's just the plattform I have acces too and that my colleagues (sparly) use. Is SQL-Server similar/superior as an alternative?

&amp;#x200B;

&gt;How wide are those tables?  30k rows is nothing if they're two columns  wide, but if they have 10KB XML strings in them it's a whole different  ball of wax.

Oh, they do indeed contain various strings in like 10-12 columns. I can't exactly tell you what they are saved as or how big each dataset it. (I guess I just gotta look at the table and rightclick somewhere?)

&amp;#x200B;

&gt;Does access let you put Indexes on tables?  If so, stick one on Key (in  both tables, though it will likely only choose one).  At 30k rows it  starts to matter a little, and it's possible your tables are just too  big.

Can i stick Indexes to querys? While in my testversion I use tables, in my work-version those 30k rows are products of querys that allready sort stuff out. And the ""key"" is a product of linking date, ref-numbers and shit. So the key is a string itself, not unlike what you would build an Index-formula arround in Excel.

&amp;#x200B;

&gt;Of course, you could much more easily just go  
&gt;  
&gt;Select table2.\* from table2 RIGHT OUTER JOIN table1 PM table2.key = table1.key WHERE table1.key IS NULL  
&gt;  
&gt;(If  this is school work, this is likely the answer the prof is looking for -  determining if you understand the difference between an outer join and  an inner join.)

Haha, nah it aint homework. I try to work up some stuff formerly (poorly) managed in Excel. We get 2 forms of daily reports with 2k rows each as .csv that, in the end, I wanna have import to access (via makro I guess) so they can be processed as soon as they come in. I allready managed to break them down in seperate queries to what information we actually need and bring them together in one query to see the notable differences.

Now I need to know if these .csv we get contain answers to all the remarks we made in a prior 2k row .csv we send them. So each case gets a ""key"" (Date &amp; ""\_"" &amp; Number) and that's what I wanna face up against. (Thinking about it; can't I just force access to inlcude even those ""Keys"" that do not find a counterpart in the list I compare them to?)

I'm sure I make it uneccesary complicated by using SQL, but personally I like ""coding"" over using the intended interface. I feel like this helps you understanding the process a little better in the end. (Plus it's more fun that way,lel)",2.0
g2p68wm,ifn6xi,"Yes.  SQL is far superior.  Far more costly too if you can't squeak by on Express.  Tuning queries on SQL Server is manageable (and you could easily use a Stored Procedure to save the intermediate data into tables for the express purpose of sticking an index on them, while at the same time even going so far as to reduce Table2 to ONLY the Key column (if you don't need anything else in it).

No, you can't index a query.  You can, however, save the results to a temp table and put an index on THAT.  Not sure how messy that is in Access though...  Generally when you have a query that combines queries, unless you've explicitly used temp tables or a TSV (don't use a TSV if you can help it), the engine will process it as a single query.

IF your query is far more complex than your sample (table1 and table2 are entire query sets), then things can get messy.  I've observed that Access handles complex queries very, very poorly, doing things that a proper RDBMS (like MS SQL) can see through (usually...).  I've had to ""step"" queries through as many as a dozen Views in Access to get performance manageable.  (Until I learned to get it to pass the raw SQL query to the server with a bit of dynamic SQL trickery - then things got a lot easier - but that doesn't help you here.)

What you're doing with your data is a great case for using SQL.  You're just...  Pushing the Access engine a little too hard.  If your CSVs are originally coming from a SQL data source and you have read access to it, you may wish to consider querying it directly.

Try the outer join is null method.  It should reshape the query plan and might be more usable.  I've never tried to tune in Access though, so I can't help much beyond that.",2.0
g2pkaeu,ifn6xi,"Thanks a lot for this very helpfull input!

I will definetly check out SQL now and might drop access for the better, as I am not required to use it, as long as I can present the relevant data as an excel-sheet in the end, haha. 

Man, it's also good to hear that I might not be completly wrong with how I attempt to use Access asa Database-Tool, haha.

Thanks for this inside!

Edit: Oh, and sadly I have 0 chance to get access to the original database. Allthough I might be able to cut a corner on our side of the process! Good idea!",1.0
g2pmkmy,ifn6xi,"There are ways to automate importing data into SQL from the CSVs.

You could write a script or program that will pull all the CSVs from some location (share, FTP server, web API, etc...), upload it to staging tables (Extract), run the SQL code (or call a stored procedure) to process the data (Transform), and save the output into the final table (Load).  OH look, there's that ETL acronym.  A buzzword to add to your brag sheet.  ;)  (Don't forget that after loading data into the actual table to also clean out your staging tables, if they aren't just temp tables, and remove or otherwise mark the source files so you don't process them again.)",1.0
g2q0lkl,ifn6xi,"Thanks for the input mate! I'll be able to autmate by writing a macro in VBA, right? Thats something I'm decently capable of... I hope nothing changes to much from importing/exporting excel stuff...

I'll keep these buzzwords in my head in case I wanna annoy one of my german colleagues with some overly complicated english terms. ""Oh sorry, it's just the language I'm am used to, when automating your job away... lel""",2.0
g2qfr73,ifn6xi,"Yup.  VBA should work as well, you just need to figure out what commands you'll be using to talk to the database.",1.0
g2rxw6j,ifn6xi,"Thanks for the Input! 

&amp;#x200B;

What helped me was: 

a) turn the queries to tables so I can index the ""Key"" columns (minor contributor?)

b) Use: 

&gt;SELECT \*  
FROM TEST2 T  
LEFT JOIN LIST1 L ON T.KEY = L.KEY   
WHERE L.KEY IS NULL 

Thanks for the Input!

&amp;#x200B;

(this is a copy and paste text to keep you updated. Thanks for engaging in helping me! You dont have to reply.)",1.0
g2odtfi,iflz9d,"&gt;Or is there a way to create the table and populate it in the first instance?

Yes, that's what SELECT INTO is for, it creates the new table and populates it with the selected rows.

You can select the columns you need from the existing tables. For your new columns, could you handle the conditions you have in mind with CASE?",4.0
g2ojge9,iflz9d,In standard ANSI SQL that would be `create table new_table as select ...`,5.0
g2oe8vu,iflz9d,"Excellent, thank you. I'll try it out.

Some columns will need to populated based on text within the columns, ie: I need to count the number of ""Customers"" and then the number of ""Drivers"". While other columns I need to populated telling me the timestamp of when a message is sent. Can I chain all these together in a big CREATE TABLE query?",1.0
g2ofb5e,iflz9d,"no, create your table cleanly first -- that way you can determine exact datatypes, nullability, default values, etc.

then populate it with 

    INSERT 
      INTO table 
         ( column1
         , column2 
         , ... ) 
    SELECT column1
         , column2 
         , ... 
      FROM table
    INNER
      JOIN table
        ON ... 

the thing about SELECT INTO is that you don't have the same control",4.0
g2oszzp,iflz9d,"Okay that's an interesting way to do it. I get the sense that the task I need to complete is based on creating one big query to create everything at once however? 

If I create the table separately, is this not scalable? Sorry I'm no expert at all and was just curious. Thanks for your response.",1.0
g2ouaaf,iflz9d,"Why would you think it's not scalable? when the table is created, it's completely empty. If you need to populate it more than once then truncate it first or drop and recreate it. The only time it wouldn't be scalable is if you decide you need more columns, but if you're changing your query to give you more columns you might as well change your create table statement at the same time.

Also, why do you think you need to do everything in one big query?",2.0
g2p0kaa,iflz9d,"&gt;Yes, that's what SELECT INTO is for, it creates the new table and populates it with the selected rows.

Only in SQL Server, which OP didn't state he/she is using.

Edit: Not sure why pointing out a probably irrelevant solution deserves downvotes, but K.",0.0
g2oquhk,iflz9d,"I'm curious, why do you need a table? Would you not be better served by a view based on the query?",3.0
g2pauko,iflz9d,"You should specify what RDBMS you're using.  The specific Syntax varies a bit.

MS SQL, Oracle, MySQL, PostGres, etc...",2.0
g2sdef5,iflz9d,I'm using PostGres! :),1.0
g2pakbt,iflz9d,"&gt; also with new data that's based on wherever a condition is fulfilled 

Is there a reason you can't use a view?  If you need it to be an actual table, you'll need to look at a trigger on the source tables.  This is generally something best handled in the application though, as triggers can impact performance.  (Great for strapping the feature on to an existing database for an application you can't modify, but do watch performance.)

If it doesn't have to be real time, what you are describing is basically an ETL or Data Warehouse process.  Consider looking into those concepts.  If you're updating on a schedule, your INSERT INTO (as others have already suggested) will include an outer join to the destination table filtering on the destination table returning a null (which does mean you need to keep it joinable to the source data).  This also has a performance impact, and depending on your data can actually be worse than a trigger, but at the same time it is also scheduled and can be written to run in batches when the database is idle.

&amp;#x200B;

Creating the table the first time the logic runs is simple enough:  You just check if the table exists, and create it if it doesn't.  (This only helps if you'll be doing this to many database.)  This is generally done in your application code or in an upgrade script, and would be followed by the code to populate it.",1.0
g2ocb55,ifkp76,"Looks like you're missing a right bracket after

    - SUM(pls_fy2009_pupld09a.visits)

so it's messing up the calculation of the percentage change.

Edit: and the corresponding left bracket between ROUND and CAST.",3.0
g2pib9p,ifkp76,"You're right! Thanks! 

ROUND ((()))

Is it normal to have a hard time seeing this? I had to manually substract all of them to realize my mistake, feeling really retarded...",1.0
g2q0hgg,ifkp76,"Use a SQL Formatter/linter -- it can help a lot with complex queries like this. For example, Notepad++ has a plugin called Poor Man's T-Sql Formatter. I am sure there are many others for other editors, etc.",1.0
g2ofa2q,ifkp76," 

1. /\* I don't get that since between 2009 and 2014 libraries changed in number so
2.  \* are we ignoring all the libraries that existed in 2009? Because I get different
3.  \* number of visits if I select them without JOIN...
4. \*/

&amp;#x200B;

Not sure if this was also a question but by joining 2009 you filter out anything from 2014 that didn't exist in 2009. And vice versa you get no results for anything that existed in 2009 but no longer in 2014. If you want to compare both tables you should union them.",2.0
g2pgs63,ifkp76,Union them?,1.0
g2qhjw6,ifkp76,"Yes building 1 set first and the filter over it much easier then a full outer join.
But people prefer differnt things.
Something like this. Must have some errors but it's just for the idea.

    with cte as (
    SELECT
         stabr
        ,visits
        ,'2014' as Year
    FROM pls_fy2014_pupld14a 
    
    union all
    
    SELECT
         stabr
        ,visits
        ,'2019' as Year
    FROM pls_fy2009_pupld09a 
    )
    select stabr
          ,sum(case when year = 2014 then visits else 0  end ) as Visits2014
          ,sum(case when year = 2019 then visits else 0  end ) as Visits2019
          ,ROUND((cast(sum(case when year = 2014 then visits else 0  end ) AS decimal(10,1)) - 
              sum(case when year = 2019 then visits else 0  end )) / 
                  sum(case when year = 2019 then visits else 0  end ) * 100, 2)
    from cte
    where visits &gt; 0
    group by stabr",2.0
g2xc7z7,ifkp76,"Full Outer Join solved the problem, I'm happy with that but thanks for your well worked answer I find hard to read and understand right now, I hope an upvote will compensate your effort.",1.0
g2omrey,ifkp76,"Your WHERE clause is going to eliminate any row that didn't have corresponding data in the other year.

Example, fictional library ""Library of America"" was established in 2014 so will be NULL in 2009. `WHERE pls2009_visits &gt;= 0` will eliminate that row even though there is 2014 data.

You want all rows from each table right? If so, you want a FULL OUTER JOIN.",2.0
g2pgmqy,ifkp76,"Yes! That's it, thanks! Damn, I always focused on Java, Python etc. and this SQL world is far deeper and complex than I thought...",1.0
g2ljx2h,if5u07,ETL tool.,22.0
g2m1nqy,if5u07,Can you give some names of such tools?,2.0
g2m7pb4,if5u07,"I use SAP BODS all the time, but there is Informatica as well n few others.",5.0
g2r8pqf,if5u07,As much as I hate to admit it this is a valuable skillset if you are dealing with SAP!,1.0
g2mmig1,if5u07,"Pentaho, SSIS, Informatica",1.0
g2lnhfj,if5u07,"Modeling.

Throw in some modeling.

Then a little bit more modeling.

Then add a few cups of optimization. Speed vs availability vs flexibility. Physical layer, code layer, model layer, development layer.

ETL is nice to have. I'd say 90% of issues I've seen in the data engineering world have been resolved through data modeling or physical layer optimizations. Maybe some code refactoring.


Edit:

http://elricsims.com/system

Modeling...",9.0
g2nr95m,if5u07,"What do you mean by modeling? 

To me it's a pretty vague word, how many abstractions must there be before it's a model?",2.0
g2nxuyc,if5u07,"Being a subject matter expert on the data management system and data as data, instead of attempting to be an SME on the data the business generates.

Name fields do not belong next to the personId. Name is a subject in itself. More than just people have names, and there are different classifications of names, more than just a type (nickname, full name, middle name, reddit, twitter). More than just a family of names (current, previous). And more than just a class of names (legal, user generated). But assuredly these are Person - name realms, which may be similar or contain different records under the Organization- Name realm, or the Part - Name Realm.

At the most basic level; a 'domain model' (Person, Part, Food, Group, Recipe) could be modeled using 4 degrees, R-&gt;C-&gt;F-&gt;T-&gt;B. Where, in hierarchical order, a Base table FK to a Type, a Type FK to a Family, Family to Class, Class to Realm would allow you to model any domain like concept. 

For a 'shared attribute' like Name, Flag, Numeric. A Simple R-&gt;C-&gt;F-&gt;T-&gt;X setup would suffice. RCFT meaning the same, where as X would represent n^NumberOfDomains for optimization purposes. You could just make another base, but now you are modeling for the dbms.

Although we will have a (P-People, N-Name, F-Flag, G-Group)

PR-&gt;PC-&gt;PF-&gt;PT-&gt;P 

And a 

GR-&gt;GC-&gt;GF-&gt;GT-&gt;G model

The NR-&gt;NC-&gt;NF-&gt;NT (and F) model forks at its base. So a PF GF PN and GN tables exist. But other than the Pid or Gid, the N tables look the same, the F tables look the same. 

But we can store pablo Picasso's full name, which is already something most systems cannot do.

Theoretically, we just need numbers to represent the relationship, so let's program for the DBMS. Every tabke will have a Security Flag to flag the row as have security, one for Active, one for Trusted, one for Planned (on off via dates), and even a column to represent 'cleaning' in case a row should be redirected to a different row if the previous row is wrong, or depricated. Think misspelled US State names being redirected to their real value, while keeping the misspelled value for lineage and other metrics. We also want a number that represents the table (same for every row, never found in another table, SEQUENCE mssql syntax)

Because these models are defined, because these tables are 95% identical, these objects can be automatically created. Along with their CRUD views/procedures, etc.

By modeling data, as data, you have effectively modeled all data in a consistent pattern, domain/shared/reference (no X ref for shared) / dimension (date and geo only) are now all modeled.

Edit: so the system can create any object you need for any new subjects of data (language, gender, race)

Now we can move to relationships.

RCFT Rel model. No need for multiple relationship tables, we just need the PK and table number for parent and child, and can use the RCFT to determine if its a direct, descendant, ascendant, etc relationship.  Now we dont need to run recursive CTEs in live code (optimized access). We just need to rerun the recursion when a relationship is changed. Relationships are a subject.

Next step would be to convert most of your complex mathematics into decision trees. RCFTDecision and RCFDecisionParameterType.... 

Modeling. When you don't have to hand create any more tables, because you've modeled data as data and for the DBMS... you won't need to abstract anymore.

Edit 2: https://youtu.be/TmUrH8C9vus",1.0
g2lxk4z,if5u07,Almost every single Data Engineer position I've seen included python skills prominently.,4.0
g2m200s,if5u07,Yes I am learning Python.,1.0
g2lltky,if5u07,"Data Normalization, Be familiar with 1NF, 2NF, 3NF, BCNF, 4NF and 5NF.",4.0
g2lllzh,if5u07,If it’s FAANG I would also prep MPP/Distributed querying.,3.0
g2m1ygl,if5u07,Can you please expand on MPP/Distributed querying?,1.0
g2mbijb,if5u07,"Redshift, snowflake, spark, hive etc",2.0
g2nl6w4,if5u07,Thank you for posting this. I was also looking for direction towards data engineering,1.0
g2l9sv8,if300x,"This is a perfect situation for a CTE:

        WITH SalaryRank
          AS
           (
              SELECT employee_id,
                     salary,
                     DENSE_RANK() OVER (PARTITION BY salary ORDER BY salary DESC) AS salary_rank
                FROM employee
            )
         -- this returns all the employee details of whomever has the 2nd highest salary
      SELECT e.*
        FROM employee e INNER JOIN SalaryRank SR
          ON e.employee_id = SR.employee_id
       WHERE SR.salary_rank = 2",5.0
g2la3hp,if300x,"You can do a lot more too.  Replace that last sql statement with:

    -- this returns all employees with the top 10 salaries (desc)
     SELECT e.*
        FROM employee e INNER JOIN SalaryRank SR
          ON e.employee_id = SR.employee_id
       WHERE SR.salary_rank &lt;= 10",1.0
g2layae,if300x,"&gt; this returns the top 10 employees by salary (desc)

no, not really

suppose seventeen people have the top salary

it returns a lot more than 10 employees

if you were to restate this as ""returns the employees earning the top 10 salaries (desc)"" then that would be correct",2.0
g2lg8gz,if300x,"Edited for accuracy, well except for testing the code :)",1.0
g2lax7c,if300x,"    -- or this would return the % difference between each salary
    SELECT e.*,
           (sr1.salary - sr2.salary)/sr1.salary AS percent_above_next_salary
      FROM employee e INNER JOIN SalaryRank sr1
        ON e.employee_id = sr1.employee_id LEFT OUTER JOIN SalaryRank sr2
        ON sr1.salary_rank = sr2.salary_rank + 1
    
    /*
    i'm too lazy to put this into an analyzer and test it, but you should get the idea.  Good luck!
    */",1.0
g2ldt58,if300x,I am not looking for new solutions. I already have an alternative solution. I wanna know what mistake I am making in my query.,1.0
g2lfnjx,if300x,"The mistake is trying to do it in a subquery!  Start rolling with CTE's, it will make your life easier. :)",1.0
g2lfvur,if300x,That doesnt answer my question at all. sorry. I can also make an alternative query using MAX but that is not what I want. I wanna learn from my mistakes.,0.0
g2lp828,if300x,[deleted],1.0
g2lqwwo,if300x,"&gt;To get the nth record without having to rewrite your query each time, you'd need to use window functions, CTEs, dynamic SQL, or a combination of those concepts.

You are wrong, I found my mistakes and updated my post.",1.0
g2m1yju,if300x,"&gt; Both of these ways dont work.

why do you think so? looked workable to me, out of curiosity i did sqlfiddle and they do work.

http://sqlfiddle.com/#!9/c77d6/7",1.0
g2m2f7z,if300x,See my edit,1.0
g2kykdb,if1oaa,Nobody is going to want to scroll thru the possibility of thousands of comments on different questions.  Maybe go to a DISCORD chat channel for that.,2.0
g2lb5tx,if1oaa,"yes!!!

https://www.reddit.com/r/SQL/comments/iewiiq/assistance_with_sql/",1.0
g2kyul6,if1oaa,If I post a new query everyday people like you would be the first to complain,0.0
g2kzevp,if1oaa,People like you don’t deserve an answer.,0.0
g2kzgx6,if1oaa,Thanks for confirming what I said. You are a childish idiot. Goodbye,0.0
g2rfft4,if1oaa,"&gt;Thanks for confirming what I said. You are a childish idiot. Goodbye

exactly how  a spoilt brat would react if they don't get their way.",1.0
g2rxabv,if1oaa,"I dont know why you are attacking me for absolutely no reason. That guy was being mean to me and that's why I reacted to him this way. While I didn't do or say anything to you. I think you should find something useful to do with your time instead of attacking random strangers on the internet. 

And I have no interest in continuing my conversation with you so I won't reply even if you attack me again",1.0
g2l0d20,if1oaa,"Reflect on your first comment to me and you’ll see you should be saying that to yourself.
Bye Felicia.",0.0
g2kwlks,if1oaa,"&gt; I dont want to make a separate thread every time I want people to critique my query.

if each time it's a different query, then yes, you do
 
 

&gt; Can we make a daily general discussion thread? 

please... no

all of /r/sql is for general sql discussion",1.0
g2lh6oa,if1hdj,"I think that this is a great opportunity to learn a valuable technology skill that will last your whole career:  How To Lose Gracefully.

It sounds like your teammates minds are decided.   Rather than convince them, I would spin that around and ask 'How can I adapt to MongoDB?'   In your professional career you will very often have to adapt to technological decisions made by other people.    

Now, I'm not picking sides or saying 'you're wrong.'  Rather, it is very important to be able to pick your battles, and if you have already lost, don't fight it.   Smile, get to work, learn something.   You'll win the next one.",3.0
g2l9cj9,if1hdj,"First of all I would never call anybody on my team a script kiddy. 

The world has enough gatekeeping in the tech industry. They dont need another egotistical person contributing to **that** hot mess. Step one is check your attitude and be polite.

Second, you're right that SQL has better normalization than mongodb. (or most other document based dbs) the main benefit of the better normalization is to do faster queries and reduce corruption.

But, I think most teams are better off using mongoDB despite that.

I've been in several startups and ive seen this train crash several times. 

The reality is 90% of coders arents going to learn a lot of stuff. Unless you want to do the entire project by yourself, you'll learn to play nice with their stack. This is true in any company. 

This becomes an even more pressing issue when you get into very, very large projects.

Also, if you build the architecture correctly it wont matter when it comes to scale. Any typical application using mongo db will start to have trouble with scale just like every other db, but you can scale horizontally with that stack just like any other stack.

I think most applications and teams go through a typical lifecycle like this where some members of the team hate how its being built. The applications that succeed arent ""built differently"" they're just fixed and maintained or maybe even migrated to a different stack.

I once worked for a company who migrated from mySQL to mongo to postgres within ~ 8 months. This is not uncommon in the tech industry. Get used to it. 

Any given application is more than just it's stack.

This whole phase of picking bad technologies early on is normal in the application lifecycle.

Lastly, in my experience if you call all your classmates ""script kiddies"" you'll feel all the worse when some of them get jobs out of school faster than you. And if they heard you call them ""script kiddies"" they're not going to help you land a job either. You're screwing yourself with this attitude.",6.0
g2lbvwd,if1hdj,"Yeah you're right, I shouldn't be so sour.

But I don't think they really don't know much about databases. I shouldn't be judging but someone else knows somewhat more or less than someone else. But in my case I think I really know more. But put that aside, they are storing the form as html, this I can't take. I spent 1 month learning about SQL and it's techniques and the way they've agreed to do things is quite stupid. 

Note : By Full HTML  I mean like this
&lt;form&gt;&lt;input type=....&lt;/form&gt;, I cannot accept this.

&gt;I've been in several startups and ive seen this train crash several times. 

Can you elaborate your this point ?",1.0
g2lwxmn,if1hdj,"The db is almost always the first point of failure in any project. Choosing mongo or postgres or mysql is not going to change that.

When i say ""seen the train crash before "" I mean if you made the db in mysql there is no garauntee that would solve your scaling problems.

Just recently i was consulting for a company with a postgreSQL database who was considering moving to mongoDB because their postgreSQL db was regularly brought to it's knees with its computational load. mysql does not mean your scaling problems are solved. 

At best your mysql database will be slightly more robust, but will be the first point of failure either way. 

This assumes your team codes everything ideally which they almost certainly are not. A mysql database can run worse than any mongodb if implemented poorly.

Considering you have a batch of junior coders, you are probably better off with the technology that can be implemented faster. Quick iterations will teach you everything about the needs of the project.

As far as the raw html storage.. . You should accept this as a temporary solution with known fixes in the future. There are ways to store html fragments in a db safely, but it sounds like your team needs to figure that out.

Sometimes people can't be convinced until the solution becomes necessary. Even if it does not fail, we can probably expect them to make improvements over time.",2.0
g2ml7xu,if1hdj,"I'm an experienced, highly priced consultant with 20 years experience in the IT industry. I'd say that OP is over engineering and the others under engineering: What matters today is how fast can you get reliable, robust and maintainable solutions out there in the market. I've seen projects where developers dive head first into a technology which should give them a huge performance increase but will take three times the time to implement. Quite fast it's over the point where it's cheaper to add more computing power to the server(s) or cloud than to keep on paying triple development costs.

Additionally I was at a customer where they moved away from MySQL and into NoSQL world and they had one really good reason: the schema was ever changing and the cost of updating the old records to new schema was taking longer and longer as new data was added to the database.",2.0
g2kz17k,if1hdj,"First of all, more power to you.

Secondly, it will probably be hard to convince them as they seemed to have their minds set and have no desire to learn SQL. The argument of it not scaling may also be difficult, because the way Mongo is structured, you can have a sharded replica set and scale by adding another server to the cluster.

What they are missing is that the database is part of a system. Other parts of the business will eventually want to look at the results of that form and SQL is a very popular language. There are product teams in companies with no engineer in them and they rely on SQL. On top of that, if you say that the engineers will code all the interactions with the database, then the engineers become the bottlenecks to development and companies don't like that. 

Also, if they want to have a job waiting for them when they come out of school, SQL will look good on their CVs.",2.0
g2luqnj,if1hdj,"There are already plenty of comments pointing out how this is an opportunity to learn how to loose gracefully when working with a team, so I won't get too into that.

Let's just take a step back and look at what it is you were asked to do and what it involves. You have users and those users create forms. Great, for simplicity's sake, two objects that are affectively self-contained with one relationship. Here is where you go ""wrong"".

&gt; Now, I implemented it perfectly in mysql ( used proper techniques like multi-table design, normalised tables etc. ) . I am having 14 tables in total ( user, form_info, questions, placeholders,form_aesthetic, input_type,single_line_answers, radio_btn_answers ... etc. ) have broken down most of it to make it as normal as possible.

I would always hesitate to say you did anything ""perfectly"", but that's a different issue.

You had the right approach, and I'm sure your design was going in the right direction for building a well architected survey platform; the problem is that it is probably overkill and goes way into the weeds on topics and tech that your team is not proficient in.

I'm not sure what kind of time you guys have, but I have built an enterprise grade survey platform from scratch before and I can tell you that four undergrads are not going to be able to do it in the time I've ever been given for a school project.

While I strongly dislike NoSQL DBs for actual app DBs, using it for this project sounds reasonable. What you should be doing is pushing back on some of the design issues you see like lack of schema validation and such.

&gt; they are using Node.js which itself is shit when compared with Java and Go

Here you make it clear that you are just as bad as your teammates in terms of understanding the tech options brought to the table.

Computationally, Java is a bit faster than Node, but when it comes to IO performance, [Node beats out Java](https://www.tandemseven.com/blog/performance-java-vs-node/#:~:text=When%20it%20comes%20to%20real,Node%20is%20the%20clear%20winner). For that reason, Node is actually a great choice for building a survey platform.

Go would be a great one as well, but we have to look at the circumstances. 

* Your whole team only knows node.
* The UI will be JS, so why not use the same on the backend?

&gt; It won't be right but I am beginning to think them as script kiddies and novice.

Some of the things you have described don't sound like good design choices, but don't say this. You guys are still undergrads. If you rag on your peers about not knowing as much as you then eventually everyone will hate working with you. Instead put your effort towards learning how to work with people like that. Personally, I find teaching others to be the best way to learn.",1.0
g2kkqm4,ieyoup,"The wiki link is working for me, as are the links inside the wiki (well, the three random links I clicked on). When you click on the wiki link what happens? Does it not take you to https://www.reddit.com/r/SQL/wiki/index ?",1.0
g2kp70x,ieyoup,[Wiki page](https://imgur.com/a/Lpy5JaE),1.0
g2kpdt3,ieyoup,it might be your app.... have you tried it through the reddit website?,2.0
g2kuk16,ieyoup,I tried the website just now and it works there!,1.0
g2k566g,iext3q,"There’s a hierarchy data type in SQL Server, but that’s about all anyone can add without some more context. Maybe you can take a step back and describe what you’re actually building, why you need the OID hierarchy, etc?",3.0
g2k7sot,iext3q,"The final database will contain a list of skills associated with jobs. Every skill for every job. Organized in a hierarchy. For example you might have Professional/Programming/Javascript as a skill. Or Industrial/Welding/Arc/Underwater. Each level is assigned an integer, and the result is an object identifier composed of multiple integers separated by a decimal (0.53.48.2546.5 for example). As you can imagine the final database will be fairly large. The database will be used across languages, so the text representation isn't ideal for searching as it will be multilingual. I know that each record needs to store its child and parent ID,  and it appears that a Recursive CTE query is the way to search. But I just wanted to find what the best practice experience is from others who've worked with similar hierarchical datasets.",1.0
g2o4vh6,iext3q,"I've always gone with the materialized path approach for storing hierarchies. Most of my experience is in SQL Server so we have hierarchyid datatype available, but there should be something fairly equivalent in other RDBMS'. The largest entities I've worked with were thousands of lines\_of\_business--&gt;products--&gt;items--&gt;add-ons and really didn't run into much issue with performance. The materialized path approach is very easily read and understood by humans, but with the right data structure can also be efficient for the DB engine to work with also.",2.0
g2ov9cc,iext3q,"Thanks, I'll check that out",1.0
g2k3ga7,iext3q,"Relational dbs do not do trees well, by design.  One row per oid you need to store things about at a guess",2.0
g2js0mm,iew3dk,This function will return the first non NULL value in the list that you pass in (sort of like an ISNULL() function but with the ability to pass in more than argument),2.0
g2jsgto,iew3dk,"Could I coalesce a single value, or must there be more than 1 argument in the list? Can you provide a basic coalesce( , ) for example please",1.0
g2jtar3,iew3dk,"Doing it on one value wouldn't really yield much.

For example say I wanted someone's preferred name, if that's NULL, I would want their legal name, and if that is also NULL I would want to return 'No Name'.

We could use COALESCE(PreferredName, LegalName, 'No Name')",3.0
g2js5qb,iew3dk,I use it to replace null values - it works great,2.0
g2k8g63,iew3dk,"Like they said - it's a generalized isnull() function. So there's really no point to isnull() at all except it takes 1 less keystroke to type out, in the special case where you only want 2 possible values.
Similarly in oracle nvl() is like isnull(), but at least it's shorter to type so you'd save 3 more keystrokes.

For what it's worth, just use coalesce all the time instead. And coalescing 1 single value is the same as doing nothing so you shouldn't bother. If that 1 value is null, coalescing it alone would still result in null.",2.0
g2lh5h4,iew3dk,"I use it a lot in outer joins, especially with aggregations.  You definitely need at least two arguments.  Here's an example of a data table left outer joined to some fact table, using coalesce to substitute 0 in case there are no facts on that date.

(edited, made a mistake!)

        WITH DateList
          AS
           (
              SELECT date_value
                FROM date_table
           )
      SELECT DL.date_value,
             COALESCE(ft.num_value,0) AS total_sold_or_whatever
        FROM DateList DL LEFT OUTER JOIN fact_table ft
          ON DL.date_value = ft.date_value",2.0
g2jfc6f,ietf9i,"I took this course and was extremely impressed. Itzik ben-gan also has several sql books.

https://training.solidq.com/course/advanced-t-sql-querying-programming-and-tuning-for-sql-server/",12.0
g2jh5fv,ietf9i,Thanks! I will check this out. Do you feel like this properly prepared you for working on professional databases?,3.0
g2jssob,ietf9i,I worked as a sql developer for 3 years before i took it. We met in a physical location and a lot of people looked lost. Everyone there were dba's/full stack developers.,4.0
g2jymoo,ietf9i,"It’s crazy how you post this because I’m currently starting Jose portillos course on sql: zero to hero, and was wondering what my next route should be or whether I should start python?",5.0
g2k42gy,ietf9i,I first started with SQL and have just started “Python for everyone” on Coursera &amp; loving it.,1.0
g2jgwk9,ietf9i,Do you have a lot of Udemy courses?,3.0
g2jh6yn,ietf9i,"Not quite sure what you mean, but the only Udemy course I have purchased is Jose Portill’s Complete SQL course",4.0
g2jhlju,ietf9i,"I am sorry if my comment came across as uncalled for. Regarding your post, I've got MySQL beginner to expert from Udemy and it is really great.",4.0
g2jm3de,ietf9i,"No worries! Yeah, Udemy is a really reliable source and I learned a lot from it",4.0
g2jqhki,ietf9i,"Can you recommend any SQL classes from Udemy?

Edit: Intermediate classes I mean",2.0
g2k1ivj,ietf9i,"Hi I just started this course today so I can't help but I had a weird question about the program itself:

When I downloaded it, I ended up with some extra apps that I don't recognize such as ""Reload Configuration"", ""Stack Builder"", ""SQL Shell"", and a folder of ""(other)"" stuff for PostgreSQL12

Did you end up with these same things? Are they useful?",3.0
g2k4cq2,ietf9i,"Read ""SQL for Smarties"" by Celko and ""SQL Antipatterns"" by Karwin",3.0
g2jjp2r,ietf9i,I'd just do hackerrank,3.0
g2k4bjj,ietf9i,I have never seen anything on hackerrank actually be good,4.0
g2j0gdb,ieqibz,Data profiling is a technical approach (analysis on how data is filled) where data discovery is more a business approach (what is the sense of data and what insight can we find out within it),5.0
g2ir9xi,ieqibz,"""What do you mean you have another database chock-full of sensitive customer information that you didn't tell the compliance team about?""",4.0
g2isnsy,ieqibz,"In my experience ""data profiling"" is a review of statistics and metadata, while ""data discovery"" is the initial stage of understanding a data set and requires examining values. Data discovery will often include data profiling but not vice versa.

Not sure if there are more precise definitions but that's how I think of it and invoice for it.",3.0
g2ivpc6,ieqibz,nice,-1.0
g2ix051,ieqab2,"**Brief Overview:**

***TRY block:*** Run these statements and check if there is an error in the execution. If there are no errors, exit TRY block and do not go into CATCH block.

***CATCH block:*** Enter CATCH block only if there is an error in the TRY block. In case of error, run the statements in the CATCH block. By nature, once the execution goes to the CATCH block it will not automatically undo the statements in the TRY block. (that part needs to be handled using transactions).

**Current Problem:** 

Errors in CATCH block are only thrown if there is a call to a statement like THROW or RAISERROR. If these statements are not there the job step should result in success. Otherwise, the job step will result in failure.

In your project, please check if the CATCH block is throwing any error or not.

Another thing to note is this: in your Parent-child proc style of execution flow, if there are dependencies between successive procs, it is better to have the execution fail and rollback the changes in the statement rather than process bad data due to existing failures upstream. But that is up to the requirement and not stringent rule to be followed.",3.0
g2ixxfl,ieqab2,"There are no dependencies that would warrant what you're describing, the children prepare and curate data which is then added to our analytics database. I did mention to someone else that my BEGIN/COMMIT TRAN might be nested outside of both the TRY/CATCH.

The idea here would be that the parent runs &amp; commits, even if a child fails, and it just throws an error, logs the error, and in our logging process we can see that the error count is &gt; 0, and email ourselves. Then we'd go fix the child, and manually run it.",1.0
g2j0fyf,ieqab2,"In that case, the only change required is to remove the RAISERROR or THROW statements in the CATCH block. Ensure that there is a dbmail in the CATCH block to send the error out to recipients and so on.",2.0
g2j5vf6,ieqab2,When I get back to my work computer I'll post some actual code for an example. Not sure I'm following you or not.,2.0
g2ix8nx,ieqab2,"So essentially with a try/catch block the idea is that when the try fails (for example it tries to do something that it cannot accomplish) whatever code is in the Catch block then executes, this is especially true if you are utilizing a try/catch block with a transaction (where the commit exists within the try and the rollback exists within the catch) in which if you executed code, if the piece of code containing the commit were to not be able to execute, you would cause a rollback of everything that happened within the try.",2.0
g2ixpif,ieqab2,I might have my BEGIN TRAN / COMMIT TRAN nested outside of the entire TRY/CATCH block...,1.0
g2j05re,ieqab2,I have a process in which I use the try catch blocks outside of each procedure which works decently.,1.0
g2j3bfe,ieqab2,"So just to be clear, each of your calls is within it's own TRY...CATCH statement, right?

e.g. within your master sproc :

    BEGIN TRY 
         EXECUTE dbo.sp_Whatever1;
    END TRY
    BEGIN CATCH
         SELECT ERROR_MESSAGE()
    END CATCH
    BEGIN TRY 
         EXECUTE dbo.sp_Whatever2;
    END TRY
    BEGIN CATCH
         SELECT ERROR_MESSAGE()
    END CATCH;

And beyond what others have recommended, you're confident of whatever you're running in your CATCH statement?",2.0
g2j5ti2,ieqab2,"No, more like this:

Parent object:

    BEGIN TRAN
    BEGIN TRY
        EXEC A
        EXEC B
    END TRY

    BEGIN CATCH
        INSERT INTO LOG.Table
        SELECT ERROR_MESSAGE()
    END CATCH
    COMMIT TRAN

Then each child has a similar setup, and some of them might have multiple steps (BEGIN/END) within the TRY block.",2.0
g2j98lh,ieqab2,"Begin tran

Begin try

Commit tran

End try

Begin catch

Select @error = error_message()

Rollback tran

Insert @error

End catch",6.0
g2jclqp,ieqab2,"I love you so much. Like you have made me so much better at what I do, and I appreciate the hell out of it.",3.0
g2ju95s,ieqab2,"I've made more mistakes, therefore I have more answers lol.

I'd say its good practice to name your transactions and based on what your are saying, maybe have transactions in your children and just a try catch at the top.

In your parent you might have to use 

if trancount&gt;0

 rollback

In the catch. Hanging transactions can be a pain.",1.0
g2jxhm8,ieqab2,What do you mean name my transactions?,2.0
g2jxuwu,ieqab2,"Begin tran t44 

Begin tran t55

Commit tran t44

Rollback tran t55


You can give them aliases.",1.0
g2kcinc,ieqab2,How does this relate to my my catches and logging table schema?,1.0
g2lk8dk,ieqab2,"Rollback transaction, without a name/savpoint, will rollback all transactions to the first outer most begin transaction. Nested transactions will fail the entire parent tree.

So if you want a child to fail and the parent to keep going, you gotta specify what you are rolling back/committing.

Edit: error logging (insert/update) in a child, is wrapped in the parent's transaction. Therefore, rolled back.",1.0
g2wupm1,ieqab2,"So I have to remove the commits and try from the children, and build them into the parent package basically?",1.0
_,ieqab2,,
g2lsfmb,ieqab2,"I think you've arrived at the solution now, but yes, this won't work. Unless you commit in between the transaction will rollback for EXEC A and EXEC B. Personally, I put each individual sproc call in it's own TRY...CATCH... because you can only get one error message so if they both fail you will only get one error logged.",1.0
g2j9udx,ieqab2,"I had this exact same issue with my procs. I am familiar (or so I thought) with how try catch works. It logs most errors, but when it primary keys get violated for some reason it doesn't catch it.",1.0
g2mzcgt,ieqab2,[deleted],1.0
g2mzezf,ieqab2,Wasn't even aware of such a setting.,1.0
g2j6qsm,ieq4i1,"For this specific example, in SQL Server you could do this like below, i'm not sure about Oracle syntax but it might give you an idea:

    WITH AsciiValues AS 
    (
        SELECT 97 AS AsciiValue -- 'a'
    
        UNION ALL
    
        SELECT AsciiValue + 1
        FROM AsciiValues
        WHERE AsciiValue &lt; 122 -- 'z'
    )
    INSERT INTO alphabet_table (constant_int, letter, constant_varchar)
    SELECT 1, CHAR(AsciiValue), 'Triangle' -- CHAR function converts numeric ASCII value to character
    FROM AsciiValues;",2.0
g2jb9xg,ieq4i1,"it comes in emails, word docs, chats and ect. I build forms with their desired wording.

So there is no consistent input or source table

quite tedious ):",1.0
g2jbhay,ieq4i1,"apologies, the data is never so consistent or incremental, the above was just an example.

it wont be a,b,c it will be ratings, questions, answers, or anything else the customer thinks of.

But I see how that would work if the data was sequential and consistent",1.0
g2jfe0n,ieq4i1,In that case then you will probably need to either use PL/SQL or put the values into a CSV file and read from that,1.0
g2jp100,ieq4i1,The same principal could probably still apply. Insert all of the values into a temp table or table variable with column1 being a sequence of numbers and column2 being your values to iterate through. Iterate through column1 similar to the above example and actually select the second column.,1.0
g2ya7is,ieq4i1,"If you don't already have a table set up with the changing value, you can use a table expansion.

    INSERT INTO ...
    SELECT
      constant,
      t.column_value,
      constant
    FROM TABLE(sys.odcivarchar2list('a', 'b', ...)) t
    ;",2.0
g2ir2st,ieq4i1,"Where can we find the data that you want to insert into column 2?  

If this data is available in a source table you can use a cross join to fill  col1 and 3 with constants. 

CROSS JOIN (SELECT INTVAL AS COL1, CHARVAL AS COL3. ) t",1.0
g2qaacr,ieq4i1,"I ended up doing it the long way, but sublime text made it not terrible. Thank you for the suggestions!",1.0
g2ip72q,ieq4i1,"This might not be the answer you're looking for, but I would use Excel to do this quickly.

In the first column put the letters you want to insert (eg. a in A1, b in A2), then in the second column put:

=""insert into alphabet_table values (1, '""&amp;A1&amp;""', 'triangle');"" 

And copy that all the way down to z, then copy paste into sql developer, f5, commit.

That way you can easily insert multiple values, change them as necessary etc.

Sorry if you were looking for a shortcut using sql, I'm not aware of anything but someone else may be able to help.",1.0
g2ipyz7,ieq4i1,"That's a good suggestion, but yeah its not quite what I need. Wont go into detail but I have a particular customer who send things a particular way, sql would be best. But thank you!",2.0
g2ixm0d,iepmwa,"First rule about joins: There are no rules about joins. Ignore all subsequent rules if you need to. [When you know how to use them, you won't be taking my orders](https://www.youtube.com/watch?v=CWLb2F0MEoY). Joins are complex, and while you can come up with some general rules, they cannot be represented so simplistically.

Second rule about joins: Never use OR in a JOIN. -- As a quick add on here. I remember once where I had a data source that was 500K rows, and another that was about 50K rows, and I used about (4) or (5) OR's in my JOIN. The first run took about 19.5 hrs to run on a dedicated server. I took the logic to two guys with advanced math degrees that calculated the total possible theoretical set the engine had to process before giving me the answers I wanted and it was something like 12.5 TRILLION rows. We rewrote it such that there were (4) or (5) LEFT JOINS instead of using any OR's, and then we created a sub-step that used CASE logic to pick the value from the new columns. The rewrite took minutes to run compared to the original expression.

Third rule about joins: If at all possible, do not use CASE logic, dateadd, or any other strange sort of function in a join.

Fourth rule about joins: They are processed vertically. SO FROM/INNER segments the data between those two tables, then a LEFT afterwards will tag data on. It is easier if you settle on a proper FROM table first. For example, have a bunch of data by day but you might be missing a day? Don't start with FROM Table, start with FROM DatesTable and then LEFT JOIN to the table. An INNER JOIN following this will process vertically once the set between A and B has been established, *predicated on the condition you join with C.

Fifth rule about joins: LEFT JOIN becomes an INNER JOIN if you add something in the WHERE condition.

Sixth rule about joins: There is a subtle difference between adding a condition such as `ON B.ID = 1` compared to `WHERE B.ID = 1`

Seventh rule about joins: Generally speaking the more of them you have, across multiple tables/databases/servers, the worse your performance will be, and often times you can take a complex query which many joins and improve the performance greatly by segmenting the data in chunks, so do the main joins and dump the data into a #table, index the #table, and then process the next set of joins such that you are incrementally building the final data set and forcing the engine to process the logic in blocks.

Eighth rule about joins: LEFT JOIN can produce an ""anti-join"" such that `WHERE B.ID IS NULL` will give you everything in A that is not in B.

Ninth rule about joins: It is generally going to suck for performance if you JOIN a bunch of views that are complex. You can get much better performance by selecting the view into an #table, indexing it, and then joining. Bonus points if you can segment the view first. See the seventh rule for context.

Tenth rule about joins: Use indexes, and learn how to leverage them in your code. For example, if you want a date between two days, you may find much better performance by saying something such as: `where date &gt; date2 and date &lt;= date3`. Many things such as dateadd(), or other conversions will not leverage a join. I'm not giving any specific examples here, just cautioning you to try and keep joins as simple as possible. If you have a join you're going to do often, you might want to add a column to a table even if it is redundant. For example, joining date to datetime can be tricky, so I often will have a date value next to a datetime value specifically for the purpose of joins, OR, I might strip time stamps out completely and make sure all of my tables are DATE. See second and third rules for more context.

Eleventh rule about joins: If possible, try to have datatypes the same across all tables. INT joins to INT, not BIGINT, etc. This isn't necessarily going to fuck anything up... but it's a best practice, and it helps you think through things. For example, why have one column in one table varchar(55) and in another table have it nvarchar(100)? There might be actual reasons for why you need to do this, and if that's the case see the first rule.

That's about all I can think about off the top of my head. I have been working with SQL for a long time now, and I still routinely have to troubleshoot joins to figure out why they aren't behaving as expected. Generally I just ""bash"" the problem with a ""hammer"" until I figure out why something isn't working... just comment one section out, try to rewrite the section, look at the raw data and find 2 specific examples that should be joining (or not joining) and looking to see what the actual values are so I can mentally understand why the code isn't behaving as expected.

A good example here is that I have a fairly complex function, that hits two fairly complex views, and when I try to use them all together I would have needed to join on a `dateadd(dd, 1, a.datefield) = b.date and a.value &gt; 0` (see the first rule), but it just wasn't working. The intent here was to pick up another value that the function was calculating on the fly, and I was using an #table with indexes. No matter what I tried, it just wasn't working. It was, I believe, because of how the function itself was designed... so the solution here was to add the value in question to the #table and rewrite it such that `dateadd(dd, 1, a.datefield) = b.date and b.value &gt; 0` and it worked like a charm. 

Could I have theoretically figured out a way to solve the problem without adding the value to my #table? Maybe. But we came up with the idea of adding it to the #table, everyone agreed it would solve the problem, it took about 30 minutes of dev work to do, and then the query ran as expected, and efficiently. So while I am naturally curious about why, and I might dig deeper when I have more free time... but the problem was solved another way, the solution worked, and it worked well.

edit: A few more came to mind.

Twelfth rule on joins: If all else fails and you absolutely cannot figure out why a JOIN isn't working... it's probably got something to do with *NULL*. Learning how to JOIN on *NULL* is worth your time, and looks weird as fuck. See the first, second, and third rules for added context.

Thirteenth rule on joins: OUTER APPLY is your friend and often can replace a JOIN. CROSS APPLY/CROSS JOIN are also very useful to learn, as are FULL OUTER JOINS. RIGHT JOINS are basically useless, and can always be expressed as a LEFT JOIN -- HOWEVER, you may find yourself one day in a situation where you have a very complex piece of code, and adding a RIGHT JOIN on at the end will make life much easier than rewriting everything. Actually using a RIGHT JOIN for the first time , and having a valid reason for doing so, was to me a bit of a career achievement. I've only done in twice, and if you see the fourth rule and start with a good FROM table you really shouldn't ever need one. The two times I needed them were because I didn't realize until after weeks of development that my FROM was wrong, and should have been something better. For this reason a RIGHT JOIN is my all time favorite, simply because it exists to bail your ass out of a bad decision you made in the past.

Fourteenth rule on joins: Do NOT write a LEFT JOIN if it functions as an INNER JOIN. This is lazy coding and will serve to confuse future developers. I hear a lot of people say to only use LEFT JOIN as much as possible regardless of how it functions, and to them I say, ""fuck you."" Others below speak about how not to put conditions in the WHERE that belong in the ON. It's just lazy fucking coding. Stop doing it.",26.0
g2jibp8,iepmwa,"Nice stuff! Absolutely on point about never using OR.

Some other rules:

NEVER express join logic in your where clause. Always in the join expression.

NEVER filter on any columns from a left outer join'ed table other than to filter out NULLs.  (I.e., if you do ""where outertable.x &gt; 12"" -- that's now an inner join since x is not allowed to be null.)

NEVER try to solve the above by adding "" or x is null"" or wrapping x is an isnull() expression in your where clause - move that critiera to the outer join clause.

Finally and most importantly: NEVER use RIGHT OUTER JOIN.   (Always restructure your sql if you've gotten to the point where that seems like a good idea -- it's not!)",4.0
g2jmtwt,iepmwa,Sometimes you must say OR IS NULL. See the first rule.,2.0
g2la5i5,iepmwa,"&gt;  It is easier if you settle on a proper FROM table first

yes!!  

this was my point earlier about finding the ""driving"" table

https://www.reddit.com/r/SQL/comments/iepmwa/rules_about_joins/g2iwehj?utm_source=share&amp;utm_medium=web2x&amp;context=3",2.0
g2j2bld,iepmwa,"&gt; Sixth rule about joins: There is a subtle difference between adding a condition such as ON B.ID = 1 compared to WHERE B.ID = 1

Can you explain what the subtle difference is please?",1.0
g2j61sj,iepmwa,"It's really hard to explain...

WHERE will segment your data *after* a join, but doing `ON = n` will segment your data *during* the join. 

They can often lead to identical datasets, but they can often lead to small variations that might not be what you want. Someone smarter than me can probably provide an actual example. I don't run into it often, but I have on occasion. It is just one of those things I have learned to test, and if I'm struggling to write a certain type of join I might move things up or down to see what if any difference exists.",1.0
g2jj7vb,iepmwa,"No difference on inner joins.  It's simply better style and readability to always express join logic in join clauses and filter logic in where clauses but technically/logically they can be in either place.

But big difference on left outer joins -- logically the results differ.

Very old blog post here: https://weblogs.sqlteam.com/jeffs/2007/05/14/criteria-on-outer-joined-tables/",2.0
g2jmyw0,iepmwa,I only meant to express that there is an actual difference between using a value in the ON vs the WHERE to make the reader alert to the possibility.,1.0
g2jnw2k,iepmwa,"Right, and I am clarifying that -- there is only a difference on outer joins, not inner joins.",1.0
g2jtmr5,iepmwa,"Not sure I'm following or not, bud. I've been out having a few drinks with the girlfriend so I'm not sure if I need to.",1.0
g2j824u,iepmwa,"Thought of an example.

Say you have a table (A) with ten ID's and you write code such as:

    FROM Table A
    LEFT JOIN Table B
        ON A.ID = B.ID

Now in this example you would expect to keep all (10) records from (A) and only have values for those that share an ID with B.

    FROM Table A
    LEFT JOIN Table B
        ON A.ID = B.ID
    WHERE B.ID = 1

That will now give you only (1) record assuming that it exists in (B), or you'll get nothing back if it doesn't, however consider this:

    FROM Table A
    LEFT JOIN Table B
        ON A.ID = B.ID
        AND B.ID = 1

Now you will retain all (10) records from (A), but only get the value from (B) for that single ID.

What I said about before &amp; after might not have been correct. It was as assumption, but someone smarter than me can probably give you more detail. 

High level.. those two things are not the same, and their behaviors are different for certain datasets. In a simple example you might be wondering why anyone would ever do this, or care, but in a complex query using complex data you might run into this one day in the wild. It isn't something that I encounter often, but I have seen it enough to know to always pay close attention to those nuances if a query is not behaving properly.

edit: B.ID not A.ID",1.0
g2jl0yc,iepmwa,"My best understanding on this is think of the ON A.ID = 1 as a WHERE statement for Table B. It will only join ID’s from A that are equal to 1 so it’s as good as writing

LEFT JOIN
(SEL * FROM TABLE B WHERE ID = 1)

BUT the ON A.ID = 1 has more to offer. Let’s say you are using the row_number function on table A and you want to only join on the first row (row number 1) for your data and you end up using a HAVING filter then you could possibly want to write ON A.ID = B.ID AND A.row_number = 1

But I’m very sure this is inefficient use of row_number and the join so someone reading this is probably burning on the inside reading that I have done this.",1.0
g2l9cqk,iepmwa,"    FROM Table A
    LEFT JOIN Table B
        ON A.ID = B.ID
    WHERE A.ID = 1

&gt; That will now give you only (1) record assuming that it exists in (B), or you'll get nothing back if it doesn't

could you have another look at this please

i'm pretty sure it will return 1 row in both cases",1.0
g2lsuta,iepmwa,I might be wrong but I believe the ON will return all 10.,1.0
g2lujnt,iepmwa,"yeah, that's definitely wrong",1.0
g2lt4ui,iepmwa,Might have meant B.ID = 1.,1.0
g2lukib,iepmwa,;o),1.0
g2nlvs1,iepmwa,"I guess to tack on a little context, it just depends on the data you're working with and the condition in the JOIN. This really relates to the twelfth rule. For example if you think of the concept of `&gt; 1` for a certain type of field, depending on where you place the code between the WHERE and the ON you can absolutely get different sets of data if there are NULL's involved in the field you're applying the concept. See the fifth rule about a LEFT becoming an INNER.",1.0
g2j35t7,iepmwa,"&gt; so the solution here was to add the value in question to the #table

I've experienced a similar problem to this - we wanted to apply a user-defined function to a subset of the records and no matter what we did with the query it was always applying the function to every row and then filtering the results afterwards. This made the query run unacceptably slowly. No matter what we tried and how we rewrote the query we couldn't get it to work the way we wanted until we decided to filter the records into a temporary table and then apply the function to that. The original query was giving the correct results, it was just taking way too long to do it.

We also had to put very detailed comments in the code to explain why we'd written it the way we had and to please not think that ""this code would be more efficient if it was just one query and didn't use a temporary table"" - it just goes to show you can't always trust sql server to come up with the best query plan, and sometimes you have to guide it in the right direction.",1.0
g2j6auu,iepmwa,"A person will always be smarter than an engine. Generally when I rewrite a query to optimize it I start by just testing, making little modifications seeing if there is anything promising or not.

If a query takes 2 hrs to run but I can get 80% of the data into a #table in 4 minutes... that's a good start. I don't really consider this a 'scientific approach,' per se, but rather using brute force to come up with multiple different queries that all should yield identical results and then just literally testing and comparing them. I like the process to hitting SQL with a hammer until it does what I want.",1.0
g2l9un8,iepmwa,"&gt; A person will always be smarter than an engine

nonsense, and i am prepared to die on this hill

how many devs have you seen using `FORCE INDEX` (MySQL) or `USE(index)` (SQL Server) or similar

then they feel so proud of themselves they start using it all the time!

a pox on their houses",2.0
g2na68e,iepmwa,"OK, fair point. Haha, wishing a pox on their house in a pandemic year says a lot. Maybe *some* people aren't smarter than the engine.",1.0
g2jdi7z,iepmwa,Isn't it a left join becomes an inner join when adding a where clause for the right table. Works fine with where clauses in the left table.,1.0
g2jg0yp,iepmwa,"No.

Think of that as half way between a true left and a true inner, but still mathematically an inner.",1.0
g2jj8aa,iepmwa,How? It will return every row I expect from the left side and if the right side matches it will fill in columns. Feel like I'm missing something.,1.0
g2jmpdb,iepmwa,The where will exclude null.,1.0
g2jnis6,iepmwa,"Maybe I need to lookup the mathematical definition. It always returns the entire left side I specify in the where clause. Then it adds any matching records from the right side. Maybe I'm looking at my expectations instead of mathematical definitions. 

That's why I don't teach mathematical definitions. Not much practical value for most people. That, and apparently, I don't understand them. :)",1.0
g2jtk9u,iepmwa,"SQL is a mathematical language. If it omits even a single record then it is an INNER an not a LEFT. In the case we are discussing a NULL record in the source table would be ignored, therefore it is an INNER if you want to look at it in terms of a Venn diagram.",1.0
g2jxsab,iepmwa,"Are you saying the following where clause would not return null records? I always thought the left side of the join would return all rows specified in the where clause unless there is a where clause in the right side.

  where col = value or col is null",1.0
g2kcl78,iepmwa,Or null would keep it a true LEFT but become unnecessary if the condition was used in the ON.,1.0
g2l5263,iepmwa,I searched for articles or documentation that supports your original claim that using a where clause always turns a left join into an inner join. Can't find anything.. Do you have any references? I'd love to understand this.,1.0
_,iepmwa,,
g2iqh15,iepmwa,"When doing a left join to fill in information, first check the number of records on your main table, the write the join, but don't add the columns to the select yet. Run that query and check that you're getting the same number of records. Only after that check proceed to add the extra fields from table B. 

If your table B has duplicates, your result will duplicate records.",6.0
g2j0dik,iepmwa,"This is good advice. I never trust a left join. 

Depending on filter conditions, left joins can act like inner joins. 

If possible, filter the tables before using the left join using a temp table or a with clause ( aka Common Table Expression (CTE)). 

And always verify the left join is doing what you think it is.",1.0
g2iwehj,iepmwa,"&gt; What are some of the rules about joins that you have from experience I may not be able to easily pick up from SQL references?

two tips

first, if you have a restriction, like writing a query to return results for a particular person or time period or whatever, then that condition determines what your first table in the joins should be

    SELECT ...
      FROM firsttable
     WHERE firsttable.foo = 23

this condition identifies what's ""driving"" the query

this is also the way the optimizer will likely execute it

then you add your other tables, joining them one at a time...

    SELECT ...
      FROM firsttable
    INNER
      JOIN secondtable
        ON secondtable.bar = firsttable.bar
    LEFT OUTER
      JOIN thirdtable
        ON thirdtable.qux = secondtable.qux
     WHERE firsttable.foo = 23

and there's the second tip -- in your ON conditions, put the joining table's column under the joining table, and the joined table's column on the right, so it sticks out to allow easy reference to an earlier-mentioned table

makes it so much easier to read and understand",4.0
g2jgcbf,iepmwa,"I never noticed how it ""sticks out"" to put the source join on the end instead of the inside. I've always done it the other way because it was how I was taught, but you're making sense.",1.0
g2ip1fr,iepmwa,"Not so much a rule as a time/sanity saver: If a join won’t work the way that you want because of things like aggregates not being allowed in joins, stage the data in a temp table and perform your aggregations/transformations there (or a subquery but temp tables are just so easy) and then join to the temp table instead of the original table. Quick and easy ETL to keep you from pulling your hair out.",5.0
g2loani,iepmwa,"Don't use Venn Diagrams to explain them. 

https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/",2.0
g2n85tx,iepmwa,"My own .02 is that I use Venn's to explain / describe them exclusively when trying to understand what is going on. I think in terms of Venn's relative to a JOIN, and a Venn diagram is the quintessential way of visualizing set theory, and relationships between objects.

I understand it isn't for everyone, but if you come from a mathematics background then they are a super easy way to explain JOINs. If you don't, I can understand how they would be confusing and there are probably easier ways to visualize what is occuring.

My point is that I don't like the idea of ""not using them"" and find it rather specious. A few people say that, not everyone.

The blog talks about how joins are Cartesian, which is true, but generally speaking a Cartesian JOIN is referring to something like a CROSS JOIN, not an INNER or a LEFT.

For a true Cartesian product you cannot really express a JOIN using a Venn diagram, or if you did it would look like a FULL OUTER, but for all other types of JOINs you can absolutely use a Venn.",1.0
g2q88u2,iepmwa,"Thanks for this. As someone relatively new to this, the Venn diagrams helped immensely for just getting a grasp of the differences",1.0
g2il15v,iepmwa,[deleted],-1.0
g2kl7ni,iepmwa,What do you mean,1.0
g2i8bs1,ieobo6,"&gt; According to my understanding the group by shhould reduce the total number of rows. 

what GROUP BY actually does is produce **aggregate rows**

you get one aggregate row for each *unique combination of values* in the GROUP BY columns

&gt; Plus I dont understand why there are two group by columns.

because you want aggregates for each combination of actor and director

you have to have them both in there, because you're looking for collaborations

p.s. there are no aggregations in the SELECT list, but there is one in the HAVING clause",3.0
g2iah9h,ieobo6,"Given this table : {""headers"":{""ActorDirector"":[""actor_id"",""director_id"",""timestamp""]},""rows"":{""ActorDirector"":[[1,1,0],[1,1,1],[1,1,2],[1,2,3],[1,2,4],[2,1,5],[2,1,6]]}}

If I exclude the having COUNT* clause I get this output

{""headers"": [""actor_id"", ""director_id""], ""values"": [[1, 1], [1, 2], [2, 1]]}

Now how does adding the Count(*) work in this case because I see only one row per combination. Do you get what I am saying?",1.0
g2ih8u3,ieobo6,"Because the count is in the HAVING Clause, it is just limiting which rows are displayed.  In this case, if that combination of Actor/Director only collaborated 1 or 2 times, then that combination of Actor/Director will not be included in the output.

Edit: to see this more clearly, you can add the Count(*) as a field in your select list, then you c an run the query with and without the Having clause to see this effect",2.0
g2iu8rj,ieobo6,↑ this,2.0
g2i9k2c,ieobo6,Adding count() (just to solidify what you’re seeing) to the select list would show how many times each actor:director has collaborated. The having clause says “only where it’s more than twice.” Sometimes additional context helps me understand what a query is doing...,1.0
g2idjlv,ieo1rz,"MySQL, MS SQL, and/or PostgreSQL  


[w3schools.com](https://w3schools.com)  


[sqlzoo.com](https://sqlzoo.com)  


[hackerrank.com](https://hackerrank.com)",4.0
g2j8jhf,ieo1rz,"My wife is just starting SQl, no prior programming experience. So I started a series for someone like her. Homework and practice exercises. 

Still super new playlist, working on the next 3 vids and practice problems.



https://www.youtube.com/playlist?list=PLPI9hmrj2Vd_X9e72itiMZes5c5FH69ZP&amp;feature=share",4.0
g2i95g2,iel4y6,"Don’t use views for that. Any optimization won’t be detected in your case.

Here is my tip of the day on learning SQL — learn as little as possible to do what you want, and no more.

Don’t actually study things like views until you are faced with a situation that requires views.  Views are used to optimize long-running retrievals, so unless your Select takes more than 5 seconds to execute, don’t bother.",-10.0
g2icpiq,iel4y6,"If you don't learn you don't know that you need it. Thats the worst tip I've ever read. Basically everything in programming can be achieved with ""if else"" statements, hence don't learn enything else.",6.0
g2ihm80,iel4y6,"I would disagree with this in general. There are a lot of reasons to use views that have nothing to do with optimization, and frankly a view is a horrible choice for something that takes long to run.

Views are great for security. You can give users access to the views, and not the tables. You can also use schema binding to make it much harder for someone to modify a table. This can have a lot of benefit for a lot of things, especially cloud migrations. For example in my current DB we have a schema named GCP, and every table that is being synced to our Google cloud has a view that is simply a select from table, where we have each column specified instead of using a star, and then our ETL points to the view and not the table. Regular users cannot even see the GCP schema objects.

Another reason to use views is if you have most of the data you need in a table, you don't need to transform it at all, but you want to denormalize the data to make it easier for a user to interact with. A simple example would be a table that has an ID in it that relates to some kind of 'status' such that 1 = active, 2 = closed, etc. 

Now a user could simply join to your IDType table to get those values, and its a very straight join... but why have your users do that when you can simply create a view that does that, or self joins to itself to get some kind of previous value, or aggregates, etc? Just create a view.

Back to the top point: The only time you don't use a view is when it is too complex or inefficient. If you have a table with 10 billion rows and you wanted to aggregate it by day... you might be best suited to dump the aggregates out into a new table, design a sproc that runs daily to update that new table, then create a view of the new table. You wouldn't want to point the complex aggregate query to the table as a view and just let users go wild on it.",5.0
g2iijfs,iel4y6,"I agree.  I force people to use views at my job for the main reason that its less work for me.  If I can create and maintain one view for multiple purposes, it saves time and money.  Not to mention adjusting a backend view is 100x easier than releasing an entire application.",2.0
g2j5n2m,iel4y6,that is the opposite of good advice and that is also the opposite use case of a view? the fuck are you even talking about?,1.0
g2h20ij,ieihej,"You can try something like this. (Note: not an expert in SQLite so please test in DEV before applying to production data).

&amp;#x200B;

    UPDATE AgLibraryFolder
    SET pathFromRoot = substr(pathFromRoot, 1, 7)||substr(pathFromRoot,8,4)||'/'||substr(pathFromRoot,8,10)",3.0
g2kpz4o,ieihej,"Thank you for your help, together with DharmaPolice's answer I was able to update the lrcat file as needed.

:-)",1.0
g2its61,ieihej,"You've been given what looks like the correct answer so the only thing I'd add is with things like this, I like to do a harmless test query first, something like :

    SELECT 
         pathFromRoot, 
         'Photos/'||SUBSTR(pathFromRoot,8,4)||SUBSTR(pathFromRoot,-11,11) as new_name 
    FROM AgLibraryFolder;

This will show you the value of your string / replace thing so you can see if it's sensible or not. 

If it looks right, then you can put it into an UPDATE as already mentioned. I also like to put in a WHERE clause so it only affects records that you've not already fixed. So in your case, the unfixed records are all a set length (17 characters?) so you could add in to your update 

    WHERE  LENGTH(pathFromRoot) = 17

This does three things. 

1. It means your update won't break the one record you fixed manually. 
2. If you run it accidentally a second time it won't break everything. 
3. You can deliberately run it again because the second time, 0 rows should be affected. If not, then something is wrong.

(String length is a very crude way of filtering records but it may suffice in this case)

Always backup your database before doing an update like this.",1.0
g2kpwim,ieihej,"Thank you very much - especially for the ""dry run"" version with the select and new\_name.

I tried it and initially it was not working. The output was missing a ""/"", that i added:

    SELECT
    pathFromRoot,       'Photos/'||SUBSTR(pathFromRoot,8,4)||'/'||SUBSTR(pathFromRoot,-11,11) as new_name  FROM AgLibraryFolder;

This gives me the correct new\_name. The WHERE is required, too, as there are some rows in that table that do not follow the ""Photos/YYYY-MM-DD"" scheme, namely the top level directory ""Photos/"" and a few others.

I had to use 18 as the LENGTH though, 17 did get me no results. So at this point, the SELECT is working and with new\_name it also shows me a valid new name.

&amp;#x200B;

The UPDATE was a bit tricky for me ... initially I tried copy pasting the UPDATE from above but that threw an error. Adding the WHERE to the UPDATE, and adding it in the correct place, did the trick.

&amp;#x200B;

Thank you for the help!",2.0
g2gih43,ieg4i3,"Unless you specify the order of the output in the query, the order the results come out will be indeterminate. 

If you need your results in a specific order, you need to specify that in your query. 

If you have duplicate records and want to update ‘random’ ones, you’ll probably want to select the set of records and use random() to generate some random numbers. 

See https://stackoverflow.com/questions/4329396/mysql-select-10-random-rows-from-600k-rows-fast as a starting point.",2.0
g2giqvv,ieg4i3,Can you provide a table output that illustrates the different outcome you're looking to produce here?,1.0
g2gnx8o,ieg4i3,[deleted],1.0
g2gojn5,ieg4i3,"table: players  
player\_id | name | country | ......  
|       1      | Test1  | Belgium| ......  
|       2      | Test1  | Canada | ......

I'm using this to INSERT:

    INSERT INTO user_items (name, card_img, rating, price, position, club, club_img, country, country_img, card_type, walkout_img, chance, user_id) 
    SELECT DISTINCT name, card_img, rating, price, position, club, club_img, country, country_img, card_type, walkout_img, chance, id
    FROM players, users GROUP BY name
    ORDER BY -LOG(1+RAND())*chance
    LIMIT 6",1.0
g2h6dj2,ieg4i3,"&gt; Hi can someone tell me if it’s possible to make SELECT DISTINCT shuffle the distinct value(s) if two or more rows only have one of the same column values? 

perhaps you could show an example

DISTINCT operates on *all columns in the SELECT list*

so now two rows are identical, thus ""picking one at random"" doesn't make sense because you'll omit a valid row that's different from the one you're keeping

perhaps you're thinking of GROUP BY on some (but not all) columns?",1.0
g2hhoos,ieg4i3,"table: players  
player\_id | name | country | ......  
|       1      | Test1  | Belgium| ......  
|       2      | Test1  | Canada | ......

I'm using this to INSERT (now):

    INSERT INTO user_items (name, card_img, rating, price, position, club, club_img, country, country_img, card_type, walkout_img, chance, user_id) 
    SELECT name, card_img, rating, price, position, club, club_img, country, country_img, card_type, walkout_img, chance, id
    FROM players, users GROUP BY name
    ORDER BY -LOG(1+RAND())*chance
    LIMIT 6",1.0
g2hi8a5,ieg4i3,GROUP BY name —— picks the top row and never returns another row that has the same value.,1.0
g2hjkku,ieg4i3,"&gt; picks the top row

no, it *creates* an *aggregate* row, picking *arbitrary* values for the non-aggregated columns

please read https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html

your SELECT query is invalid, and will only work if you override the ONLY_FULL_GROUP_BY server setting",2.0
g2hjtnm,ieg4i3,Thanks for your help. I will dig deep into it.,1.0
g2e90d4,ieakzm,One comma too many,3.0
g2e98qk,ieakzm,oh my word.,1.0
g2e9a7p,ieakzm,You absolute legend. Could I ask why the error doesn't come up for line 4?,1.0
g2egeab,ieakzm,"Line 5 is where it encounters the error. The comma is still valid in the position, but it means it's expecting another expression in line 5. That's the error.",3.0
g2esvur,ieakzm,thankyou!,1.0
g2dsaie,ie7b33,"No you didn't change anything with that statement.

Be sure with whatever database client you are using that auto-commit is off. This way if you do accidentally run a statement that makes changes, you can rollback.",3.0
g2eh5lf,ie7b33,That makes sense. Thanks for clearing that up!!,1.0
g2eaiqv,ie7b33,"Also, you should have an edit log (ask your admin) for every user, and can always rollback.",1.0
g2eh7bz,ie7b33,"Good to know, I appreciate it!",1.0
g2fqn3o,ie7b33,My bet would be that you din't select the query itself but just blank space. Which results in that message as well. Happends to me quite often.,1.0
g2exba2,ie3xcd,Port 3306 is default for MySQL,1.0
g2d6y1d,ie2pba,"If you have some general corporate office skills, that is decent writing, Business or Accounting and are better than average at Excel, you should look into Analyst-type roles.  

Every analyst role I've ever seen expects ""good"" Excel skills and most refer to SQL as a nice-to-have skill.  Business/Data/Process Analyst roles are a great place to grow your knowledge without either the expectation of high volume data-entry drudgery or  becoming instantly overwhelmed with talents you don't have, e.g. DBA, SQL Developer.",34.0
g2d7qjz,ie2pba,This seems like great feedback. Thank you.,7.0
g2dzrtf,ie2pba,"Yeah,  I'd add in some VBA there too. I started my current job as a Data Analyst in a company I was working in customer service for with a tiny bit of sql and vba and I've learnt heaps just from doing it. I think the online examples can only give you so much experience compared to being let loose on a database with 100 different tables.",3.0
g2dzxf5,ie2pba,What's VBA?,4.0
g2e0o8p,ie2pba,"Visual Basic. It's the language to write macros in for Excel(and the rest of the Microsoft office Suite I think). So if you're producing the same report each week, you can just automate it. I think SQL is great to start with and then work with VBA later because VBA is a bit more complex but you need to get practice in the naming scheme of directory.table.column because there's a similar scheme in VBA. I found with VBA I just sort of googled different commands as I needed them, or used the macro recorder and followed along b with what was being recorded.",4.0
g2e2kas,ie2pba,I took a course in high school on this but I'd have to re-learn it but I'm willing.,2.0
g2j8yzl,ie2pba,"VBA express is a good resource when you're looking for help. I learned quite a bit by also going on there and trying to solve other people's problems. I'd write 20 lines of code and someone would come on and day that my code works but here's 2 lines that does that.  

It's useful but it's easy to get into the habit of using it when the built in functions of Excel will do some stuff faster.

I'd also suggest doing one of the free courses on power bi if you want to get into analyst roles. It's being used more and more.",2.0
g2dlv67,ie2pba,"Like everyone else here, report writing as an data analyst/report writer/report developer will likely be the most common entry level job to land. I got my first job as a Jr DBA since I had interned for the company before and had some IT background thanks to school, however a lot of companies require more advanced knowledge of SQL for most Jr DBA positions.",6.0
g2e0xv2,ie2pba,"You could look at getting into Data Engineering, which applies data centric skills with a focus on software engineering as well. This would require some more computer science specific skills, perhaps some knowledge of software development and systems engineering. It is more IT focused than data analyst work - however I think would pay more, and depending on your interests I would argue more interesting than fucking around in excel.

In addition to relation data work using SQL there is usually work to do with other kinds of data such as raw text, JSON data and others.

Might be worth exploring!",5.0
g2djuki,ie2pba,"I second the comments mentioning report writing, it can easily later translate into working on Data Warehouses or even designing and creating database objects for applications depending on what you enjoy.",3.0
g2eaonc,ie2pba,"I'd suggest that you look at some entry level jobs you'd apply to (that include SQL as a necessary/desirable skill). 

LinkedIn is a good place to start (no need to create an account, you can search for jobs from the landing page). I think what you'll find is that SQL is one component of many that a database analyst would have. For example – good problem solving, good communication (e.g. when talking to non-database people at your job), and prudent programming skills are important too.

That aside, I second the comments that are already here. SQL is good to have, but other languages are good to have too – just depends on what kind of job you'd get into and really want down the line.

Good luck out there.",3.0
g2em76a,ie2pba,I'd love to learn it all if I could.,1.0
g2d342a,ie2pba,"I'd say a report writer for a company.  Usually writing custom queries from data input into an ERP or CRM system.  Possibly a Jr. DBA, but you'd probably need a bit more under you than just some SQL query experience.",4.0
g2d3bqq,ie2pba,Thanks. I need a career change so I started learning SQL on Udemy.com.,3.0
g2d3m1o,ie2pba,I had a friend recently change careers by learning SQL and SAS.  He's making more money and much happier in life now.  Good luck on your journey.,12.0
g2d3ofq,ie2pba,What is SAS?,3.0
g2d4sah,ie2pba,"It's a software to help visualize data.

[https://www.sas.com/en\_us/home.html](https://www.sas.com/en_us/home.html)",3.0
g2d6f5n,ie2pba,It's way more than that. It's statistical analytics software.,6.0
g2ddr9w,ie2pba,Came here to say this. Good for data crunching but it’s an expensive software. Can also learn Python or R for the analytics and those are free :),7.0
g2d4ukr,ie2pba,I've got so much to learn 😅,2.0
g2dhysw,ie2pba,"The good news is that no company expects you to know everything right away for an entry-level analyst role. Start by learning SQL, because even if a company uses SAS/Python/whatever, you can generally write SQL inside of one of those programs.",7.0
g2dk39v,ie2pba,"If it is an entry level position they tend to assume you know nothing except the very basics. It's more about selling yourself, and to convince them to invest in you.",5.0
g2dpk5z,ie2pba,I've always done well at learning on the fly.,2.0
g2hnjev,ie2pba,Then you will succeed! Just keep your head down and work hard. Good luck.,2.0
g2daz0d,ie2pba,You might look into power BI and tableau as well. They serve similar roles.,5.0
g2nyc24,ie2pba,There are a lot of entry level jobs you can find on LinkedIn that require SQL as one the necessary skills. Create an account there and start applying. I also recommend one platform Strata scratch to prepare yourself for the interview. This is worth a check.,2.0
g2ol4xi,ie2pba,Can you tell me more about Strata?,2.0
g2cm4c6,ie17qg,"use an INNER JOIN to returnb only cars with certificates

alternatively, use a LEFT OUTER JOIN and flag each car with whether or not it has a certificate using `CASE WHEN cert IS NULL THEN 'N' ELSE 'Y' END`",3.0
g2ddrwk,ie17qg,"You can use `intersect`

```sql
select car
from my_cars
intersect
select car
from cars_with_certificate
```",1.0
g2hn3si,ie17qg,thanks :),1.0
g2cltvk,ie089g,"    SELECT [Date]
         , [Value]
         , TestDate
         , MaxTestDate
         , ( SELECT MAX(MaxTestDate)
               FROM yertable )       AS LastTestDate
         , ....                      AS DaysBetween
      FROM yertable",3.0
g2cp2jo,ie089g,"Create a separate CTE with just Select Max(Date) FROM &lt;DESIRED TABLE&gt; and do a cross join with any table where you want this fixed value. 

FYI cross join : from  T1 join T1 on 1=1",3.0
g2cpapb,ie089g,Also you can Coalesce the already created colum with To_date'desireddate',1.0
g2d72by,ie089g,"Find the max date in a CTE, then join it back to your results with an INNER JOIN ON 1 (which will always be true, so it will be appended to every result row)

Like this

    WITH max_date_cte AS (
     SELECT MAX(TestDate) AS MaxTestDate
    FROM yourTable
    )
    SELECT Date, Value, TestDate, MaxTestDate, DATEDIFF('day', Date, MaxTestDate) AS DaysBetween
    FROM yourTable
    INNER JOIN max_date_cte
    ON 1;",2.0
g2cdfnm,ie089g,"' when ""MaxTestDate"" is null then ""MaxTestDate"" ' is wrong because 'then MaxTestDate' will be still null. How do you determine the lastTestDate when MaxTestDate is null?",1.0
g2cf9nn,ie089g,"I think a `COALESCE` combined with a window function (like `LAG(MaxTestDate) OVER (ORDER BY ""Date"")` should be what you need.",1.0
g2j5wf9,ie089g,"I think this would work best with respect to performance.

    DECLARE @maxTestDate DATE = (SELECT MAX(TestDate) FROM dbo.TableName);
    
    UPDATE dbo.TableName
    SET MaxTestDate = @maxTestDate
        , DaysBetween = DATEDIFF(DAY, [Date], @maxTestDate)",1.0
g2cq1vz,idzrgt,"At my work, for pagination we use offset and limit, for filtering we use where.",2.0
g2d4p82,idzrgt,using offset and limit is considered bad btw,1.0
g2du6lw,idzrgt,"For those who would like elaboration:

https://www.citusdata.com/blog/2016/03/30/five-ways-to-paginate/

TLDR: Limit/Offset can be quite inefficient especially at larger offsets, but is fairly flexible as far as where you can apply it and generally works fine. There are other options, but they also have their limitations.",2.0
g2cxo6i,idzrgt,"Well yeah that's in the SQL, but I'm talking about dynmically doing this stuff

For example if the url has the following query params:

exmplae.com/route?fields=id,title,desc&amp;sort=createdAt&amp;page=1&amp;limit=10

Then when I use the above class with mongoDB and mongoose I'll be getting 10 posts in the first page, sorted by the time of creation and I'll only get the posts' id, title and description with no extra data I don't need... 

And I wanna build a similar solution for MySQL but can't figure out the best way to implement it...",0.0
g2d62xs,idzrgt,"Maybe I'm misunderstanding the question... there are really only a couple ways to do it (that I'm aware of):

`SELECT * FROM table WHERE condition='value' LIMIT 10, 5`

will deliver 5 rows of data starting with row number 10. 

The other way is to keep track of the maximum id you've shown, but that makes sorting an issue.",1.0
g2da2jz,idzrgt,"You misunderstood the question, let me explain, I want to be able for example to provide via query param the following url query: [example.com/route?fields=id,title](https://example.com/route?fields=id,title) and somehow make MySQL select only the id and title values and only provide them to the response..

And now in the same dynamic way in the example above I want to be able to also sort (for example by date DESC or ASC, or by like count etc...), filter (for exmaple only get posts with X amount of likes, or with X amount of comments etc..) and paginate... the above class does all that using mongoose (library for mongoDB) and it's just insain that it just works like that because a lot of people add graphQL for this sort of functionality and in a single class you basically allow your project to not have graphQL at all... (of course graphQL as much more functionality and is not only used for filtering and selecting, but you get the idea)",0.0
g2dk87p,idzrgt,"Looks like the code you posted just builds the query, it's just mongo uses JavaScript as the query language so it might look more magical than it is.

If you are dealing with a single table it should be pretty similar code to build SQL table by writing some code like:
""Select "" then append result of Function iterating over fields provided in the API call and verifying they are valid then append "" from table order by "" then result of function verifying that provided field name is valid then add the limit/offset (again after verifying that values are sane).

If you use orm let it build the query.

If search is also required you will have to make sure that you add search values via prepared statements to prevent SQL injection.

If you have many tables that could be a bit more complicated and you should probably find some orm/query builder for your language.",1.0
g2dmtv6,idzrgt,Yeah that's what I was thinking of doing but as you said there could be security issues with that approach in MySQL (sql injection etc...),1.0
g2c9z9x,idz4vz,"    SELECT ...
      FROM yertable
     WHERE FLOOR( ( ( ID - 1 ) % 8 ) / 4 ) = 1",2.0
g2caql0,idz4vz,"Do you mean that the IDs increment 5 numbers each time? Or that every 5 rows share an ID?

I think you already realise, but it's not very clear what the problem is. Some example rows would probably be very helpful if you're allowed to share them.",1.0
g2e6q86,idz4vz,If you don’t know what Google is idk if SQL is for you.,-1.0
g2besjq,idv461,"What database product is used by Shopify? If that is also PostgreSQL you could maybe use logical replication. 

Another option to replicate changes is to use something like [Debezium](https://debezium.io/)  which collects changes a the source databases (uses logical decoding for PostgreSQL) and then publishes those changes to other database using Kafka.",1.0
g2bffvg,idv461,Does it matter which database Shopify uses? Since I believe I can only interact with it through its Shopify Admin API,1.0
g2c0ao6,idv3pi,"If you just need the final total, you'd just need to use a `SUM`:

    SELECT SUM(itemPrice*itemQuantity) AS finalPrice FROM products;

If you want the individual prices __as well as__ the total price, you could use a window function as suggested by u/samuja:

```
SELECT 
    itemPrice*itemQuantity AS finalPrice,
    SUM(itemPrice * itemQuantity) OVER () AS totalPrice
FROM products;
```

which gives you the following:

|  finalPrice  | totalPrice |
| -------------|------------ |
|       14.00  |      66.87 |
|       49.90  |      66.87 |
|        2.97  |      66.87 |

working fiddle [here](https://dbfiddle.uk/?rdbms=mysql_8.0&amp;fiddle=3c7a635d30952adbdec50ac7be13e819).",5.0
g2cgsud,idv3pi,Thank you so much,1.0
g2be1m6,idv3pi,Look for sum over,2.0
g2bwp4n,idv3pi,"If I got it right, is this what u try to run:

SELECT SUM(s.FinalPrice) 

FROM (SELECT X*Y AS FinalPrice FROM Z) s;",2.0
g2bo4kk,idv3pi,Would you mind providing an example of the input and output you're expecting from the query? It would help just to clarify. A toy example,1.0
g2c7m4b,idv3pi,"If I understand you right, I'm pretty sure you can just

    select sum(itemPrice*itemQuantity) as finalPrice from products;

if you want to nest them, consider using a CTE, since they're easier to look at later:

    with CTE_NAME as (
    select
    itemPrice*itemQuantity as finalPrice
    from products)
    select
    sum(finalPrice) as sum_finalPrice
    from CTE_NAME;",1.0
g2blznl,idsvaw,Where is the path to the photos? Is it a network share?,1.0
g2boqep,idsvaw,"Photos are uploaded directly to the program when creating a new profile, and judging by the size of the database (over 4GB) they are somehow saved directly to that database. I don't know too much about SQL, so I'm not sure if that's even possible.",1.0
g2bpxea,idsvaw,"If the images aren't stored within the DB, then it could be as simple as the pc running the app not having access to the file-share on which the images reside. You'll need to inspect the app code to see what the save/retrieve image function does, and where it saves it to. With the error message ""Path Not Found"" however, I'd put money on it being a network/file share that the new PC's aren't getting access to.",1.0
g2bthmb,idsvaw,"This definitely helps, I'm going to check a few things out. Thanks for your assistance. I'll report back if I spot anything.",1.0
g2cwows,idsvaw,"You can save photos or anything really by just converting the file to a byte array, not sure what level of programming your at but that’s how photos are stored in a database. Then when the application needs it it just converts it back into the file type it needs from the byte array.",1.0
g2acqp3,idocqw,"I would be more than happy to contribute. I've been training SQL Devs and DBAs for years. I have a lot to offer, but I just get so tired of answering it over and over again. I thought there had to already be a stickied resource, but, if not, yeah I'm all in.",26.0
g2af3vg,idocqw,"We do have the sidebar section Learning SQL, maybe we just need that stickied?

I don't feel that I really know the underlying cause that these questions always come up -- is that just the way it is, people aren't going to search or check a sidebar? Or would a sticky help because it's front and center?",6.0
g2afj2x,idocqw,"To be fair, after looking at the initial sub rules, I hadn't gone looking for it again. The only ways to really handle it are to allow it, make it against the sub rules and remove them, or just collectively agree to stop answering. Stickying it might help, but it often feels like fresh out of school or ""I just got this job, but I don't know what I'm doing!"" type posts and that kind of questioner probably isn't looking at sub rules and sidebars...

I'll have to look at the actual resource when I'm less fired, but maybe it just needs to be more comprehensive and/or user friendly?",2.0
g2dv2xu,idocqw,"&gt; and that kind of questioner probably isn't looking at sub rules and sidebars...

Yeah I agree, which is why I think a sticky would help by increasing visibility to that resource by putting it at the top of the posts instead of at the sidebar where a lot of people won't know to look.",2.0
g2dwgjo,idocqw,"Would probably encourage some of us to put more effort into it, too, knowing it would actually be used regularly to re-answer the same question. I'm sure I speak for a lot of us when I say I like automating things haha.",1.0
g2bm7ij,idocqw,"Maybe an automod comment if keywords appear in the post? (Get Started, Where to Begin, Newbie, etc.) Also give a user a way to comment and call that automod post if it isn't triggered off the OP. 

List some key resources/reading material. I think an auto comment would have a much higher chance of being read than the rules or a stickied resource.",2.0
g2drexf,idocqw,"This might be possible, but would be more work to set up and maintain so I think the cost v benefit is lower than starting out w/ just a sticky.",1.0
g2brq4l,idocqw,"&gt; We do have the sidebar section Learning SQL, maybe we just need that stickied?

Ya really the sticky is going to be way more visible.  The sidebar learning section is so out of the way on the desktop and slight chance in hell a mobile user will see it (who ever clicks on About in mobile)

This is a common problem in many subs",2.0
g2dr6hc,idocqw,"Yeah, I've seen various efforts to deal with these things... I think the visibility of a sticky would help and wouldn't be too cumbersome, compared to things like 'holding' posts until approved by mods, identifying posts by using keywords, etc. This seems like lower effort and would hopefully have some success.",1.0
g2an7pd,idocqw,"As a SQL noob, I'd love to see something like this implemented. I didn't want to be that guy that posted the same question or a close variant so I browsed through countless threads in an attempt to find my answers and went down an SQL rabbit hole where I quickly lost my bearings.",13.0
g2atsx7,idocqw,"Yep I second this, a stickied resource guide would be great",3.0
g2avdgp,idocqw,I'm a SQL beginner and I'd really appreciate something like that. Maybe adding it to the subreddit's Wiki will be a better option?,5.0
g2drki4,idocqw,My current thinking is maybe a sticky to alert new users that we even have a wiki -- remember a lot of people are new to Reddit as a whole too... and then maybe some work to fill out the wiki a bit.,2.0
g2bnvmr,idocqw,"Interested to see how often these questions actually pop up but one option is consolidate. 

We can have a couple days of the week set out for beginners just starting? An ask anything Wednesday, Friday? Then as other have mentioned it can be included in the rules, so that they use the appropriate forum. 

To be fair, SQL I feel is growing. As data has become more mainstream people are having to use SQL to access data, so it may be good to get some structure in preparation for the sub's growth.",2.0
g2drc3g,idocqw,"My impression is that a lot of the posts are from people who are just coming into r/SQL for the first time, or are relatively new and haven't actually read the sidebar and wiki. In those situations, many people wouldn't be aware of such a convention. Plus, if it's *not* one of these dedicated days, that shouldn't prevent people from asking questions.",1.0
g2b8g7n,idnhx8,"Are your PKs IDENTITYs? (i.e. will they auto-increment?)

And do you need to add an entry to SECONDARY if there isn't one for the OBJECTID already?

How you handle the PK value will differ depending on question 1, but if it's an IDENTITY then you can just:

    INSERT INTO SECONDARY (OBJECTID, [RANK], COMMENT)
    SELECT OBJECTID, MAX([RANK])+1, ""Your comment goes here""
    FROM SECONDARY
    GROUP BY OBJECTID

(If you want a different comment for each OBJECTID then put them into a table and join them in.)",2.0
g2c32ge,idnhx8,"I'm sorry, I double-check the tables and it's actually slightly different than what I described.

Strictly speaking the columns on the secondary table are:

    OBJID ( PK, FK )
    RANK ( PK )
    UID ( UNIQUE )
    COMMENTS

They don't auto-increment.  Between the OBJECTID and the RANK they form a unique PK, but I imagine incrementing is handled in the business logic.

I won't need a different comment each time.  Same one, a few thousand times.

I'm not sure what I'm looking at in this SQL.  At no point is the PRIMARY table referenced.  Where am I finding, in this SQL, the list of the OBJECTID's from the PRIMARY?

Thanks for your help, by the way. :)",1.0
g2c94rj,idnhx8,"&gt;At no point is the PRIMARY table referenced. Where am I finding, in this SQL, the list of the OBJECTID's from the PRIMARY?

Do you really need it, though? You have OBJID in SECONDARY and that's an FK to OBJECTID in PRIMARY. It's possible that there are OBJECTIDs in PRIMARY that are not in SECONDARY; if you need to newly add them to SECONDARY then I'd do it in a separate statement.

&gt;They don't auto-increment. Between the OBJECTID and the RANK they form a unique PK, but I imagine incrementing is handled in the business logic.

OBJID and RANK are fine, it's the other unique ID that makes things more difficult.

So going back to what I suggested originally, this will give us OBJECTID and the max RANK+1 for each OBJECTID, which will together will form new PKs for our new rows. The comment is just the text in quotes; they should have been single quotes in what I had above, that's my mistake.

    SELECT OBJECTID, MAX([RANK])+1, 'Your comment goes here'
    FROM SECONDARY
    GROUP BY OBJECTID

But we need to get the new UID as well and it needs to be based on the current MAX(UID) so that it stays unique. So let's expand on what we have above:

    DECLARE @maxUID int;
    SELECT @maxUID = MAX(UID) FROM SECONDARY;

    INSERT INTO SECONDARY (OBJECTID, RANK, UID, COMMENT)
    SELECT OBJECTID, MAXRANK, @maxUID+ROW_NUMBER() OVER (ORDER BY OBJECTID) AS newUID, 'Your comment text goes here' 
    FROM
        (SELECT OBJECTID, MAX([RANK])+1 AS MAXRANK
        FROM SECONDARY
        GROUP BY OBJECTID) a

So we use the subquery to calculate the max rank for each OBJECTID and then add 1 to it. OBJECTID will stay the same in the new rows, so that's fine, and we've just calculated our new RANK. Then we need to get a sequence of new UIDs, so let's use ROW_NUMBER to create a sequence of incrementing numbers from 1 to whatever and then add our current MAX UID to it.

There's probably a better way to write this, perhaps someone else will suggest something.",2.0
g2clptr,idnhx8,"By the way: I appreciate your help, and I'm sorry my efforts to sanitize this data is making this more confusing than it needs to be. :D",2.0
g2dbz3q,idnhx8,"You're welcome, glad to help.",1.0
g2cllqg,idnhx8,"&gt; Do you really need it, though? You have OBJID in SECONDARY and that's an FK to OBJECTID in PRIMARY. It's possible that there are OBJECTIDs in PRIMARY that are not in SECONDARY; if you need to newly add them to SECONDARY then I'd do it in a separate statement.

I do need it.  I need to add a comment to every record in the PRIMARY that has a certain TITLE.  Anything that looks like 'ABC123%'.  (Obviously, I'm sanitizing all this information.  It's company stuff.)

I can obtain the list of OBJECTID's with a query that looks like that:

SELECT OBJECTID FROM PRIMARY WHERE TITLE LIKE 'ABC123%'

That'll give me a few thousand records, then, for each OBJECTID (they'll be distinct &amp; unique) I want to do the INSERT into SECONDARY.

As near as I can tell, there are no restrictions on UID beyond it being unique.  The rows are all distinct but I've checked and they're not continuous.  They are SORT OF sequential, but there are lots of gaps.  I'm pretty sure the business logic is just picking the next number and moving on with it's life, so if I just do the same I'll be OK.",1.0
g2dbxn7,idnhx8,"Okay, so then your INSERT becomes:

    DECLARE @maxUID int;
    SELECT @maxUID = MAX(UID) FROM SECONDARY;

    INSERT INTO SECONDARY (OBJECTID, RANK, UID, COMMENT)
    SELECT OBJECTID, MAXRANK, @maxUID+ROW_NUMBER() OVER (ORDER BY OBJECTID) AS newUID, 'Your comment text goes here' 
    FROM
        (SELECT s.OBJECTID, MAX([s.RANK])+1 AS MAXRANK
        FROM SECONDARY s JOIN PRIMARY p
            ON s.OBJECTID = p.OBJECTID
        WHERE p. TITLE LIKE 'ABC123%'
        GROUP BY OBJECTID) a

If you need to also add in OBJECTIDs that exist in PRIMARY, but don't already exist in SECONDARY, then you *also* run:

    DECLARE @maxUID int;
    SELECT @maxUID = MAX(UID) FROM SECONDARY;

    INSERT INTO SECONDARY (OBJECTID, RANK, UID, COMMENT)
    SELECT OBJECTID, 0, @maxUID+ROW_NUMBER() OVER (ORDER BY OBJECTID), 'Comment here'
    FROM PRIMARY p
    WHERE p. TITLE LIKE 'ABC123%'
        AND NOT EXISTS (SELECT 1 FROM SECONDARY s WHERE s.OBJECTID = p.OBJECTID)

You could probably incorporate these into a single statement.",1.0
g2do5hr,idnhx8,"Thanks man, it worked.  I had to make a minor modification to the most inner section of the first SQL, where I grouped by s.OBJECTID, because OBJECTID is ambiguous and p.OBJECTID is not exactly the same as s.OBJECTID -- so SQL Server complains about with the:

&gt; *...invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause*

error.  But aside from that it worked like a dream!",2.0
g2dosg6,idnhx8,"Ah yeah, I see where the error would have been, my mistake.

Glad to hear that it helped.",1.0
g2dqu0n,idnhx8,"Wait, no, I'm sorry I lied.

I ALMOST worked.  In the interest of safety and understanding what I was doing I ran the innermost SELECT, then the next SELECT, then finally the INSERT.

Gave me this error:

&gt; *Cannot insert explicit value for identity column in table 'SECONDARY' when IDENTITY_INSERT is set to OFF.*

Checking the columns, it seems that UID is an IDENTITY column.

I **think** this makes my life easier.  Well, it would have, had I noticed that earlier.  And it would've made YOUR LIFE easier, at lot more than mine!

Can I get away with not inserting anything into that column?  So I can just go with:

    --DON'T NEED THIS ANY MORE...
    --DECLARE @maxUID int;
    --SELECT @maxUID = MAX(UID) FROM SECONDARY;

    INSERT INTO SECONDARY (OBJECTID, RANK, COMMENT)
      SELECT OBJECTID, MAXRANK, 'Your comment text goes here' 
      FROM
      (
        SELECT s.OBJECTID, MAX([s.RANK])+1 AS MAXRANK
        FROM SECONDARY s JOIN PRIMARY p
        ON s.OBJECTID = p.OBJECTID
        WHERE p. TITLE LIKE 'ABC123%'
        GROUP BY s.OBJECTID
      ) a",2.0
g2gdm9d,idnhx8,"Yep, if UID is an IDENTITY then that makes things much easier, you don't need to calculate the next valid UID, SQL Server will just auto-increment it.

What you've given above looks fine, but you don't even need the subquery anymore, you can collapse it into:

    INSERT INTO SECONDARY (OBJECTID, RANK, COMMENT)
    SELECT s.OBJECTID, MAX([s.RANK])+1 AS MAXRANK, 'Comment text'
    FROM SECONDARY s 
        INNER JOIN PRIMARY p
            ON s.OBJECTID = p.OBJECTID
    WHERE p. TITLE LIKE 'ABC123%'
    GROUP BY s.OBJECTID",2.0
g2jnqi6,idnhx8,"Yup that's what I ended up using.  You got me there, thanks again.",2.0
g2j0hge,idnhx8,"To start with I am considering the situation where you have a different comment corresponding to the Primary Table in a separate table or view. Something like below.

    DROP TABLE IF EXISTS #CommentTable;
    
    CREATE TABLE #CommentTable 
        ( OBJECTID INT
        , Comment NVARCHAR(MAX)
        );
    
    INSERT INTO #CommentTable
    VALUES
     (101, 'Comment 1')
    ,(101, 'Comment 2')
    ,(103, 'Comment 3');

If it is the same comment for all records, it can be something like this.

    DROP TABLE IF EXISTS #CommentTable;
    
    SELECT ObjectID, Comment = CAST('Universal Comment' AS NVARCHAR(MAX))
    INTO #CommentTable
    FROM dbo.Primary;

Finally, coming to the insert in both situations.

    DECLARE @maxUID INT = COALESCE((SELECT MAX([UID]) FROM dbo.Secondary), 0);
    
    INSERT INTO Secondary ([UID], ObjectID, [Rank], Comment)
    SELECT
          [UID] = @maxUID + ROW_NUMBER() OVER (ORDER BY ct.ObjectID ASC, ct.Comment ASC)
        , ObjectID = ct.ObjectID
        , [Rank] = COALESCE(LAST_VALUE(s.[Rank]) OVER (PARTITION BY s.ObjectID ORDER BY s.[Rank] ASC), 0) + ROW_NUMBER() OVER (PARTITION BY ct.ObjectID ORDER BY ct.Comment ASC)
        , Comment = ct.comment
    FROM #CommentTable ct
    INNER JOIN dbo.Primary p ON ct.ObjectID = p.ObjectID -- prevents any record not in primary
    LEFT OUTER JOIN dbo.Secondary s ON ct.ObjectID = s.ObjectID
                                   AND ct.Comment = s.Comment
    WHERE s.UID IS NULL -- this will remove the chance of dupes",2.0
g2jntqq,idnhx8,"That's not quite the way /u/Saeveo and I got it licked (see above) but that'll also work.  I'll keep that in mind, thanks!",1.0
g29b21l,idj3i1,"I would love to hear any feedback you have about this video. Thanks!

 [https://www.youtube.com/watch?v=9l-aaOURKYY](https://www.youtube.com/watch?v=9l-aaOURKYY)",1.0
g29obee,idhmt5,'sysadmin' means they can do anything on the server.,2.0
g2a5gp6,idhmt5,"I know, the sql admins are the only ones with that role. the developer users group is set to public.",-1.0
g29t08m,idhmt5,Change their server role to public. Sysadmin is overwriting their database access and allowing them to do anything. Create databases/alter schemas delete databases.,1.0
g2a5j0n,idhmt5,"I know, the dev group has public",-1.0
g2stlsv,idhmt5,"Usually, in a logical approach, if the DB is not large and you have the space, create a TestDB from it and give it a try with test users, see what you can't and can do, after you find out what permissions you need, implement it on PROD",1.0
g298i1k,idgrwy, [https://www.sqlservertutorial.net/sql-server-basics/](https://www.sqlservertutorial.net/sql-server-basics/)  This  website takes you from the very beginning up to an advanced level. Enjoy!,10.0
g298yp6,idgrwy,"It sounds amazing 🤩. Thank you, bro.",1.0
g2a3q0t,idgrwy,The STREETS!,6.0
g2arnwd,idgrwy,Is it YouTube Channel lool?,1.0
g29agff,idgrwy,https://www.w3schools.com/ is a helpful resource for SQL and lots of other languages.,5.0
g2ari4n,idgrwy,"Thank you, sir! ❤️",1.0
g2a5nev,idgrwy,I love w3schools,4.0
g2a5loe,idgrwy,"I'm currently learning using sql bolt, it's pretty good",3.0
g2a9uld,idgrwy,This is a very good resource for learning sql. You learn while practicing at the same time.,2.0
g2arqeo,idgrwy,"Okay, I will look for it.",1.0
g2at02h,idgrwy,"* https://www.postgresqltutorial.com
* https://pgexercises.com/
* https://sqlzoo.net/wiki/SQL_Tutorial",3.0
g31a06v,idgrwy,"Here's a complete course on the fundamentals of SQL, that I'm offering for free on Udemy for the next three days:

[https://www.udemy.com/course/sql-basics-crash-course-with-sql-server/?couponCode=FREE-SQL-TRAINING](https://www.udemy.com/course/sql-basics-crash-course-with-sql-server/?couponCode=FREE-SQL-TRAINING)

As a self-taught SQL programmer, I’ve learned the hard way that all the books, videos, and articles in the world won’t help without LOTS of structured practice, i.e. coding exercises. So I've included nearly 100 coding exercises in the course, covering every fundamental aspect of SQL programming.

And I walk through every subject in bite-sized videos ( averaging less than 10 minutes in length). Every concept is built up step by step, with no hand-waving at intermediate topics.

I hope you enjoy...and of course, reviews/feedback are greatly appreciated:)",3.0
g2a5hwu,idgrwy,"khan academy as a start.
And followed by standord courses on edx.",2.0
g2arp19,idgrwy,Thans for your attention ❤️,1.0
g2a9iz4,idgrwy,"Free Code Camp on YouTube has a few SQL videos that are hours long, and start from the basics to more advanced concepts without any ads. Edx also offers a SQL course, and its free, but if you want to get a certificate from it it’s $50.",2.0
g2artvz,idgrwy,"Actually, main thing is knowledge, not the certificate, I think.",1.0
g2a9r1u,idgrwy,"[leetcode.com](https://leetcode.com) doesn't have specific training, but they will give you test problems that you can figure out. They'll tell you if you get the right or wrong answer, and each question has a forum so you can see how other people solved it, if you're messing up somewhere, etc.",2.0
g2as14g,idgrwy,Thank youu🤩,2.0
g2abrpq,idgrwy,"Chances are that your local technical college is doing some form of free or significantly subsidized tuition right now.  If so, then get into their DBA certification program.  You will want to take a Databases class before you actually get into SQL as there is no substitute to being informed on fundamentals and rules that are pervasive across this wonderful world.",2.0
g2ailvz,idgrwy,"I personally enjoyed reading “Sams Teach Yourself SQL in 10 Minutes.” 

After you get the basics down, try online exercises with [pgexercises](https://pgexercises.com/) and more advanced topics like [window functions](https://www.windowfunctions.com/).",2.0
g2b807e,idgrwy,Check out W3Schools and Strata Scratch.,2.0
g2bx2tm,idgrwy,"Try Hackerrank's easy tasks if you're a beginner. Later when you become more skilled, move to the medium and hard ones.",2.0
g4ioqjz,idgrwy,[https://www.youtube.com/watch?v=gjUNNHlMwWA](https://www.youtube.com/watch?v=gjUNNHlMwWA) Maybe start with some filters?,2.0
g294ikh,idgrwy,"We have a tool that may be cool for you:

[https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/](https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/)",1.0
g294qon,idgrwy,"Thank you, Sir!",1.0
g292l7f,idgqxx,"&gt; THEN (SELECT master\_sn\_region.region FROM master\_sn\_region) 

This line is probably returning multiple records, which can't happen. It needs to be a single value. 

What is returned when you run the select query above?",2.0
g29qd5c,idgqxx,"That makes sense, but here's the thing:

&amp;#x200B;

We have this line for results where there's more than one region. 

    CASE  WHEN ((SELECT COUNT (*) FROM sn_change_region WHERE sn_change_region.change_number = sn_change.number) &gt; 1)  THEN 'Multiple'

We have this line for results where there's only one region. 

    WHEN ((SELECT COUNT (*) FROM sn_change_region WHERE sn_change_region.change_number = sn_change.number) = 1)  THEN (SELECT master_sn_region.region FROM master_sn_region)

&gt;I understand it's not working because the **master\_sn\_region.region** sometimes returns multiple values, which is why we have the multiple clause before. Can't I ask it to return the first row or value (which in theory should be the only one?)",1.0
g29sv9f,idgqxx,You are checking against sn_change_region then selecting from master_sn_region. Is that a 1:1 relationship?,1.0
g2931nt,idgou5,"&gt;HAVING orders =(SELECT MAX(orders) FROM View1)

This is constraining your resultset to the single record with the highest order count. Just remove this and it should return the id, code and max order for each id &amp; code. No need to constrain it more.",1.0
g29bs0s,idgou5,The most effecient way would be to use an explicit cursor  to display the output,0.0
g28qz6z,idf627,"Go for it, itll at least give you a bigger picture if you don't end up liking it. 

You'll likely find yourself working with backups/restores and storage solutions for databases more then you are now. 

Not every dba does the same thing though, so id read the job description and make sure it looks interesting to you.",2.0
g297gnc,idf627,"Depending on the company size, your job role may involve PowerShell. Larger companies will try and automate backups etc using it

You can do some interesting stuff with it and it would also make your life a lot easier at a smaller firm as well👍

Edit: Might be worth looking at PowerBI if you're interested in 'data', i've found ""Data Scientist"" roles to be some of the most fulfilling i've ever done.",1.0
g299id4,idf627,I currently do some light PowerBI work from time to time. It's definitely an interesting tool!,2.0
g29aexk,idf627,"Yeh great tool, i'd highly recommend PowerShell for the more DBA focused roles and good luck with career progression!",1.0
g294tdf,idexo2,"It sounds like you have a `TIMESTAMP WITH TIME ZONE` column, which is a different data type than `TIMESTAMP`. Change the column data type to `TIMESTAMP`.",3.0
g27wf01,idbtyq,"For added clarity:

- I am not a DBA, I am querying read only tables

- I have advanced experience with SQL so can write complex queries

- I've tried TABLESAMPLE but can't seem to get past the local tables error issue and fear it will still do the same thing as TOP PERCENT and not capture all users",1.0
g27y40q,idbtyq,"Put all of your records that meet your criteria (date range) into a temp table, then do the newid() in the temple table.

If you still have millions of rows in the  table, I'd use a Modelo or right () on a tableid to limit the record count down. The Modelo or right should have a randomly selected number for the match.",1.0
g27z4pu,idbe0d,"When you added  \* 24 to the statement, SQL server follows the orders of operations in math. So what really happened was not (Date1-Date2)\*24, but actually Date1-(Date2\*24). I got an arithmetic error, but I think -954255.283 is BC -0712-07-14 06:47:31.

You can try the TIME data type if it fits your need. SELECT CAST(CAST('2013-08-05 17:23:33' AS DATETIME)-CAST('2013-08-05 09:00:30' AS DATETIME) AS TIME(0))",2.0
g27xk2u,idbe0d,"What the...? I really have no idea what you're trying to do, but if you really want 8.384 then you need to add another set of brackets around the whole thing.
    
    SELECT (CONVERT(float, CONVERT(DATETIME,'2013-08-05 17:23:33'))
    -
    (SELECT CONVERT(float, CONVERT(DATETIME,'2013-08-05 09:00:30'))))*24    

But really, no. 

Don't use floats. Don't do date arithmetic like this. Use DATEDIFF. And the brackets are confusing matters.

edit :     

        SELECT DATEDIFF(MI,'2013-08-05 09:00:30','2013-08-05 17:23:33')

Will give the number of minutes between those two date times. You can make it more precise and use seconds if you want.",1.0
g27xyf9,idbe0d,Thank you for a straight forward answer. Do you know why floats give such wonky outputs?,1.0
g27zj6i,idbe0d,"Well in this case, the wonky outputs was simply due to the order of operations. You were basically doing :

    41489.7246875 - (41489.3753472222 * 24) 
    	or put another way
    41489.7246875 - 995745.008333333",2.0
g27y4qp,idbe0d,"I would avoid the use of FLOAT as a datatype in SQL Server in almost every context. 

This blog is kind of patronising but gives some more explanation : https://blog.greglow.com/2018/01/15/sql-newbie-mistake-1-using-float-instead-decimal/

edit: See the DATEDIFF edit in my original post.

edit 2 : This is a better longer article : https://www.red-gate.com/hub/product-learning/sql-prompt/the-dangers-of-using-float-or-real-datatypes",1.0
g27t519,idbe0d,"&gt; I'm trying to do some math on dates as floating number. 

patient: doc, it hurts when i do this

doctor: well, don't do that then",1.0
g27wt6u,idbe0d,Ugh here we go. So let me waste my time asking this implied follow up question. wHaT wOuLD bE A beTTeR AlTeRNaTIve SOluTiOn?,-1.0
g27zfad,idbe0d,"&gt; wHaT wOuLD bE A beTTeR AlTeRNaTIve SOluTiOn?

you never stated the problem, just asked for help with FLOAT arithmetic

you expect us to understand that  0.3493402777749 is right but -954255.28364583326 is wrong

are you just trying to do DATEDIFFs?",1.0
g27bmun,id8n0e," Azure Security Center protects SQL servers hosted on either Azure VMs, Azure Arc and on-premises. Safeena shares how ASC protects SQL running on-premises and how to leverage ASC threat protection for SQL in this type of scenario.",1.0
g27f41q,id7u04,"&gt; 'CartItem' should have auto generated fields called 'cartId' and 'productId' linking 'Cart' and 'Product'?

i think you need to dig into this assertion - who/what ""should have auto generated fields"". And if you dont see the column in the table, your code wont be seeing it either, so make sure whatever does the generation works properly and maybe verify the generated schema for sanity after.",1.0
g26xbkq,id5ft9,"I don't see why you would need to do ""lots of queries"" for that. You didn't show us your table setup nor the way you are sending.

So considering a many-to-many table like this: 

```sql
create table user_grouping 
(
  user_id int not null, 
  group_id int not null, 
  some_attribute text, 
  primary key (user_id, group_id)
);
```
You can update everything for a single user with a single statement

```sql
with new_mapping (user_id, group_id, attr) as (
  values 
     -- everything for a single user
     -- deleted things do not show up here
    (1,100, 'Bla'), (1, 45, 'Foo'), (1, 42, 'Bar')
), remove_old as ( 
  delete from user_grouping  g
  where not exists (select * 
                    from new_mapping np
                    where np.user_id = g.user_id
                      and np.group_id = g.group_id)
)
insert into user_grouping (user_id, group_id, some_attribute)
select user_id, group_id, attr
from new_mapping
on conflict (user_id, group_id) do update
  set some_attribute = excluded.some_attribute;

```

The `values` list contains the complete new mapping. Anything that was deleted is **not** listed there. The DELETE statement in the `remove_old` CTE will take care of that part. 


This would even work for multiple users.

-----

&gt; *I would love to reduce the number of transactions made to the database.*

What you have described should all happen in one **single** transaction. Not multiple transactions.",2.0
g2f74iy,id5ft9,"Thank you for this advice, I think I am getting there. Question, however- is it possible to modify this query to create the `user_grouping` if it does not exist yet? Or would I have to handle that separately?",1.0
g27g051,id5ft9,"&gt; remove_old as ( delete from

out of curiosity - are you guessing or will this part really work without being ""called"" in the ""executable"" part (i dont have an instance to verify atm)?

My belief that Postgres has been the bastion of sanity is melting away...",0.0
g27iypr,id5ft9,"I know it will be executed because it's [documented in the manual](https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-MODIFYING) 

&gt; Data-modifying statements in WITH are executed exactly once, and always to completion, independently of whether the primary query reads all (or indeed any) of their output.",1.0
g26urcs,id5ft9,"Hello u/NewsworthyEvent - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g2678z1,id1rdo,[Cross Join](https://www.sqlservertutorial.net/sql-server-basics/sql-server-cross-join/),10.0
g2706x5,id1rdo,That’s the one! Thank you very much.,1.0
g2674cj,id1rdo,You’re likely looking for a Cartesian join. Also depends on what database system you’re operating on.,2.0
g26pd02,id1rdo,"Like others mentioned, your example/result set is a Cartesian product of the tables dates and names. 

select dates.date,names.name from dates  cross join names;",1.0
g26ij2b,id1rdo,JOIN Table2 ON 1=1,-1.0
g27isp3,id0riu,"TRUNCATE TABLE ext\_sys\_table; 

should be after the declares",2.0
g27z6qi,id0riu,"That works. Wow. So all my declares have to be at the top. That's lisp levels of difference to what I am used to.

Thank you",2.0
g29u5b5,id0riu,"So by ""it works"" it means it built and created the procedure but the procedure ultimately did nothing when run.

This is it's current state

    DELIMITER $$
    DROP PROCEDURE IF EXISTS ext_sys_table_populate $$
    CREATE PROCEDURE ext_sys_table_populate (
    	IN `@table_schema` VARCHAR(256)
    )
    BEGIN
    	/* Variables */
        DECLARE curr_table_name VARCHAR(256);
        DECLARE finished TINYINT;
        DECLARE statement VARCHAR(4000);
    	DECLARE sys_table_cursor CURSOR FOR 
    	SELECT `table_name`
    	FROM information_schema.tables
    	WHERE `table_schema` = @table_schema
    	  AND `table_name` like 'sys\_%';
    	DECLARE CONTINUE HANDLER 
    	FOR NOT FOUND SET finished = 1;
    
    	/* Instructions */    
        TRUNCATE TABLE ext_sys_table;
        SET finished = 0;
    	OPEN sys_table_cursor;
    
    	getTables: LOOP
    		FETCH sys_table_cursor INTO curr_table_name;
    		IF finished = 1 THEN 
    			LEAVE getTables;
    		END IF;
    
    		PREPARE statement FROM '
    		INSERT INTO ext_sys_table (
    			source_table,
    			id, 
    			asset_guid, 
    			value, 
    			start_date, 
    			end_date, 
    			timestamp	
    		)
    		SELECT 
    			\'?\',
    			id, 
    			asset_guid, 
    			value, 
    			start_date, 
    			end_date, 
    			timestamp	
    		FROM `?`';
    
    	EXECUTE statement 
        USING @curr_table_name,@curr_table_name;
        
        SELECT @curr_table_name;
        
    	END LOOP getTables;
    	CLOSE sys_table_cursor;
    END;$$
    DELIMITER ;

When I run 

    CALL ext_sys_table_populate ('mysql_database_name');

it executes, says no rows affected (which may be just the way it's reported in mysql) but I see nothing populated in ext_sys_table.

This syntax is so fragile and it seems to return nothing from inside the stored procedure, I'm not sure how to go about debugging it.",1.0
g29uv8c,id0riu,"&gt; SELECT 
&gt;             \'?\',
&gt;             id, 
&gt;             asset_guid, 
&gt;             value, 
&gt;             start_date, 
&gt;             end_date, 
&gt;             timestamp   
&gt;         FROM `?`';

Change to: 
SELECT 
            concat(""\'"",?,""\'""),
            id, 
            asset_guid, 
            value, 
            start_date, 
            end_date, 
            timestamp   
        FROM ?';

Its not something you are doing wrong necessarily.. its something small, possibly with string quotes.",1.0
g29va42,id0riu,Thanks I will give it a shot and update you shortly. Any recommendations on getting any type of debugging for stored procedures? I have commented out everything and put select @variable and it returned nothing.,1.0
g29x9w9,id0riu,"Its hard to debug. I think there is some paid something or other that I ran into once. 

But basically, you just break it up into components and test them.

For example, you can make that 1 SP into 2 SPs and test each part to see where it goes wrong.",1.0
g2a2q37,id0riu,"This compiles but it doesn't insert any records.

The select statement does not return anything. It doesn't even seem to be running.

Here is the code

    DELIMITER $$
    DROP PROCEDURE IF EXISTS ext_sys_table_populate $$
    CREATE PROCEDURE ext_sys_table_populate (
    	IN `@table_schema` VARCHAR(256)
    )
    BEGIN
    	/* Variables */
        DECLARE curr_table_name VARCHAR(256);
        DECLARE finished TINYINT;
        DECLARE statement VARCHAR(4000);
    	DECLARE sys_table_cursor CURSOR FOR 
    	SELECT `table_name`
    	FROM information_schema.tables
    	WHERE `table_schema` = @table_schema
    	  AND `table_name` like 'sys\_%';
    	DECLARE CONTINUE HANDLER 
    	FOR NOT FOUND SET finished = 1;
    
    	/* Instructions */    
        TRUNCATE TABLE ext_sys_table;
        SET finished = 0;
    	OPEN sys_table_cursor;
    
    	getTables: LOOP
    		FETCH sys_table_cursor INTO curr_table_name;
    		IF finished = 1 THEN 
    			LEAVE getTables;
    		END IF;
            
            SET @curr_table_name = curr_table_name; -- using clause will only except an @ prefix variable
    
    		PREPARE statement FROM '
    		INSERT INTO ext_sys_table (
    			source_table,
    			id, 
    			asset_guid, 
    			value, 
    			start_date, 
    			end_date, 
    			timestamp	
    		)
    		SELECT 
    			CONCAT(""\'"",?,""\'""),
    			id, 
    			asset_guid, 
    			value, 
    			start_date, 
    			end_date, 
    			timestamp	
    		FROM `?`';
    
    	EXECUTE statement 
        USING @curr_table_name,@curr_table_name;
        
        SELECT @curr_table_name;
        
    	END LOOP getTables;
    	CLOSE sys_table_cursor;
    END;$$
    DELIMITER ;",1.0
g2b1ajk,id0riu,"&gt; FROM `?`';

should be FROM ?';",1.0
g286mk6,iczmx0,"Create a dates table, insert all dates you will ever need, than write a query from dates table left joining your query.",1.0
g25ef21,icxged,Can you post the queries?,1.0
g26ps6a,icxged,"Perhaps when you include 2010, you are basically adding another condition/filter which is additional work.",1.0
g26tsaz,icxged,"The probable reason is that oracle opted to use the index on dates when you put more restrictive conditions based on date. That index could be a good choice even for the first query but oracle sometimes doesn’t choose the optimal one. To fix it, I would try first to update statistics. If it doesn’t work, you can use a hint to force the index.",0.0
g256tdz,icw58x,"Why don't you just get rid of your cte and replace it with the header table joined to the customfields table and say

    where pnumber in
    (select pnumber
    from customfields
    group by pnumber
    having count(*)&gt;1)

My phone won't show me your inner join properly so can't take that into consideration if it was more significant.",5.0
g259qw8,icw58x,I got this to work. Thanks!,2.0
g25a4ru,icw58x,"Don't know how I managed to reply to your comment on the other person's comment but glad you sorted it, was it just a matter of adding your other where clauses like and ID in etc...?",1.0
g25d9i5,icw58x,Yeah just kind of combining the two.,1.0
g257o7s,icw58x,&gt;This doesn't seem to work. Gives me every pnumber no matter what.,1.0
g258hgl,icw58x,"Yes, the only way you can achieve your goals is with a sub query.. Above solution works perfect for your requirement",1.0
g2595a6,icw58x,IDK doesn't seem to work or take the date span into account. I'll try it again. Keep in mind - i don't want totals - I want every record. Thanks.,1.0
g2560zn,icw58x,Having clause,1.0
g2563k9,icw58x,Can you expand a bit please?,1.0
g256eye,icw58x,"Sure, it’s like a Where for Aggregates. Look it up. 

I read it really quickly but I think you just need Having Count(column)&gt;1",1.0
g257qri,icw58x,"So you can't expand on what it is you're saying? I'm sorry, I don't follow?",1.0
g259g3e,icw58x,[deleted],1.0
g259vro,icw58x,"Eh, you're being too vague for me to follow. Thanks for the response though!",1.0
g257fuy,icw58x,"Hold up, this is the same question you just asked that I answered then you deleted. If you do a count(\*) OVER ( PARTITION BY PNumber ) in your CTE, and then filter in your query on cte.Num &gt; 1, you're saying ""if this PNumber exists more than once in the above CTE, include it, otherwise don't. Meaning you will eliminate all records that don't have duplicate PNumbers. You're partitioning by PNumber, so count will just give you the count of rows that have the same pnumber. There should be no way it's eliminating the first one. 

&amp;#x200B;

Do you have an example dataset, what you're getting, and what you're expecting?",1.0
g2583gy,icw58x,"My question wasn't clear.

I have

|ID|Value|
|:-|:-|
|1|44|
|2|44|
|3|44|

I am getting

|ID|Value|
|:-|:-|
|2|44|
|3|44|

I am doing

`,Count(1) OVER (PARTITION BY PNumber) AS [Num]`

&amp;#x200B;

keep in mind - i want to get everything in a set date span. Count returns the number for every instance. So sure my query will give me all the records in my time span but the count will reflect records from all time. So there mayb be 4 records in my time span but 15 total for all time so the count show 15.",1.0
g259stz,icw58x,"You're not doing a count over in your example in your original post, you're doing row_number over which is what eliminates the first row of each one when you say cte.num &gt; 1.",2.0
g25a53j,icw58x,"No, I am doing a count over - on my end. The thread has evolved past OP nd new things are being tried upon suggestion. Such as count over. It didn't provide results I needed. Thanks for the response and your time!",1.0
g24utd8,icu5pe,Does the service account for sql server have access to your user specific folder?,1.0
g24wq7b,icu5pe,Tried in another lication on D:\ and it was working okay,1.0
g24x8t9,icu5pe,By default service accounts shouldn’t have write access to the users\desktop\ folders.  You would probably have to grant that on the target folder yourself.   Besides that putting databases on the system drive is a good way to fill up said drive if you are doing anything of scale,1.0
g24z23r,icu5pe,"Yeah, But I didn't add a secondary drive only have one NVME.
But how do I grant the write access?",1.0
g2502gg,icu5pe,How do I check that?,1.0
g24z9ix,icu5pe,Right click on folder -&gt; security-&gt; add sql service acct to write acl,1.0
g24zrra,icu5pe,What?!,1.0
g250voi,icu5pe,https://lmgtfy.com/?q=how+do+i+change+write+access+on+a+windows+folder,2.0
g24mmzl,ict0v8,"If there is a hardwareID column then it is likely a foreign key to another table that has the strings.    SQL mgmt studio should be able to generate an ERD (entity relationship diagram) that will show these table to table relationships.   Once you find the “hardware” table being referenced you should have your mapping. 

Unless by chance, the integer in question is a string resource Id for a string that is compiled into the front end program.  Then you would need to decompile or access the code for the front end.",1.0
g24pqa6,ict0v8,Lol the table with the strings was like right in front of me and I am just an idiot. Thank you,1.0
g24nzpk,ict0on,"*without* cross join? whatever for?

you *need* a cross join of dates x employees, then a left join to attendance",2.0
g24of2c,ict0on,"Yeah that's what. I got this as a test, ans did it exactly like you said - cross join, left join, and then filtering out the nulls and thus getting the absent people.

But they want it done without a cross join, idk how that could be done. Tried something, but it's not working",1.0
g24rbfj,ict0on,do you happen to have sample tables?  i'd like to test something but i'm too lazy to write the CREATE TABLE and INSERT statements [off your screenshot](http://idownvotedbecau.se/imageofcode),1.0
g24rnfo,ict0on,"I made the above sample tables while trying my code, here you go- 


https://pastebin.com/yqZJFdSH


Sorry about the image lol, snipping and pasting to imgur is just too convenient lol",1.0
g24uaek,ict0on,"thank you

i'll get back to this after the Raptors game",1.0
g25gvss,ict0on,"trickier than i thought

will have to sleep on it

do you have a numbers table?  (google: [you need a numbers table](https://www.google.com/search?q=you+need+a+numbers+table))

if not, here's the one i'm using

    CREATE TABLE numbers (
    n INTEGER NOT NULL PRIMARY KEY
    );
    
    INSERT INTO numbers VALUES
      (  0 ),(  1 ),(  2 ),(  3 ),(  4 ),(  5 ),(  6 ),(  7 ),(  8 ),(  9 ) 
    , ( 10 ),( 11 ),( 12 ),( 13 ),( 14 ),( 15 ),( 16 ),( 17 ),( 18 ),( 19 )
    , ( 20 ),( 21 ),( 22 ),( 23 ),( 24 ),( 25 ),( 26 ),( 27 ),( 28 ),( 29 ) 
    , ( 30 )
    ;
    SELECT DATE('2020-05-01') + INTERVAL n DAY AS date
      FROM numbers
     WHERE n BETWEEN 0 AND 30",1.0
g28cpta,ict0on,"i am stuck, honestly, and i have 30+ years of SQL experience

please, please do me a favour and post the solution which does not involve a cross join",1.0
g28qzu0,ict0on,Agreed! I beat my head against the keyboard for far too long yesterday trying to think of a way to do this without a cross join. But I couldn't think of a way to do it.,1.0
g28xfkp,ict0on,"I told the person I was stuck and don't think it is possible without a cross join, he said it was. I'm gonna ask him, will update if I get an answer",1.0
g2w8piv,ict0on,"This is a retarded ass constraint. I don’t understand the purpose of not using a cross join.


I’m sure it’s possible to do without a cross join with a bunch of hackish methods, but I don’t know why you would do it except as some kind of academic exercise.

It’s the best tool for the job. I can use the handle of a screw driver to hammer in nails, but it’s not exactly effective when I can just as easily grab the hammer.",1.0
g28waeo,ict0on,"Is there something similar to a cross-apply in MySQL? I was able to get this to work in SQL Server like such. Not sure how/if it could be converted to LATERAL...it's a MySQL 8.0 feature.

    select 
        date, 
        manager_id, 
        count(*) as Number_Of_Absent_Employees
    from Employee e 
    cross apply (
        select date 
        from Attendance ia 
        where not exists(
            select top 1 1 
            from attendance iia 
            where iia.id = e.id 
            and iia.date = ia.date
        )
    ) a
    group by manager_id, date
    order by date

Not as syntactically elegant looking as a cross join...but hey, it works! 😊",1.0
g2wu4c5,ict0on,"&gt; Is there something similar to a cross-apply in MySQL?

nope, just CROSS JOIN",1.0
g3hw67h,ict0on,Well?! Don't leave us hanging OP! How'd your professor do it?!,1.0
g3qb5yg,ict0on,"I'm really sorry man, this was for an interview, and I forgot to ask the interviewer how to do this. Didn't make it through, so would be awkward to ask now :/

But I'm pretty sure it's to be done using window functions, as a big chunk of the interview was focused on that",1.0
g3qbtbs,ict0on,No worries! So sorry to hear about that. It's never a fun convo to have.,2.0
g24wr03,icrorp,"Fixed! Solution was updating the select to  SELECT IFNULL(sum(amount), 0) as total",1.0
g24b7u4,icpk4m,"We have a cool tool that may help:

[https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/](https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/)",1.0
g23y2il,icp97e,"I use Yed chart but see they have something called yed file which might interest you: 

https://yed.yworks.com/support/qa/16063/connect-to-sql-server?show=16071#a16071",1.0
g23y95v,icp97e,"Cool, I'll check it out! Thanks :)

Edit: Unfortunately, it doesn't allow me to make a chart and export it as SQL code like Lucid does. Nice try though!",1.0
g24mr36,icp97e,"Is lucid chart actually good for sql code? I'm actually curious. I signed up for a free trial so i could make some tables and export them as sql code, and all the code was wrong. I ended up just re-writing the code anyway. My alternative / upgrade is sql developer + tableau. But i'm not sure if that will serve your needs.",1.0
g23k4d0,icn260,"    SELECT COALESCE(one.Period,two.Period) AS Period
         , one.TotalDemos
         , two.TotalTrials
         , 100.0 * one.TotalDemos / two.TotalTrials AS percentage
      FROM ( SELECT ""Call Date"" AS Period
                  , COUNT(*) AS TotalDemos
               FROM ""customer_calls""
              WHERE ""Call Type"" = 'Demo'
             GROUP 
                 BY Period ) AS one
    FULL OUTER
      JOIN ( SELECT ""modified"" AS Period
                  , COUNT(*) AS TotalTrials
               FROM ""users"" 
              WHERE ""customertype"" = 0 
             GROUP 
                 BY Period ) AS two
        ON one.Period = two.Period",2.0
g26dlvw,icn260,"Hey! This worked like a charm for day wise ratio. 

However, what I forgot to mention was that my date values are by day (date time) whereas I want the aggregated values by month. I'm not exactly sure how we would use coalesce with this constraint to get the values. Any ideas?",1.0
g26humn,icn260,"Also, as an additional question, suppose I wanted to add another where condition in sub query 'two' which is based on 'one' like -

...TotalTrials FROM ""users"" WHERE ""customertype"" = 0 AND ""Call Type"" = 'Demo'GROUP BY Period)

That doesn't seem to work and gives an output as unknown alias one used in select query. Basically, in the second table I just want to count the users who customertype = 0 and also had a call type = demo based on data from the first table. Do you have any suggestions on how I could incorporate that here?",1.0
g23gdjf,icl72a,"I hope that works out better than the guy who thought ""null"" in a license plate was a good idea.",57.0
g23wre3,icl72a,What would happen with that one?,9.0
g23x5fr,icl72a, [https://www.wired.com/story/null-license-plate-landed-one-hacker-ticket-hell/](https://www.wired.com/story/null-license-plate-landed-one-hacker-ticket-hell/),27.0
g247v2x,icl72a,"So far I haven't had a problem with NULL in MA, but I also pay any tickets when I get them.",6.0
g24aemr,icl72a,Paying tickets when you get them is actually what screwed that guy over if you check it out,9.0
g252m8q,icl72a,Oops...guess I forgot the details of his story :),0.0
g25yx59,icl72a,"Heh, that's what came to mind when I saw this -- the 'Null' guy from California I think it was.  As a SQL DBA this is so funny!",3.0
g23ie3d,icl72a,Little Bobby Tables grew up and is driving now.,35.0
g23m096,icl72a,"I had the same on my Audi RS6, too: https://imgur.com/XusUFMR",11.0
g23syj3,icl72a,"What does Helmut have? Do you have him back? I need updates, apparently!",1.0
g23y7dt,icl72a,"Hahaha, he has California plates, but not vanity ones yet. He's still at Musicar, waiting on new tweeters from Morel: [https://www.flickr.com/photos/kennethaward/50166397808/](https://www.flickr.com/photos/kennethaward/50166397808/)

Hopefully getting him back at the end of this month.",2.0
g23xlxc,icl72a,C5 huh.  My wallet weeps for you.,1.0
g23y95b,icl72a,"I sold him a while back to make room in the garage for Helmut, my 2019 911 Targa. Both of our wallets weep on that one, heh.",2.0
g23ycmi,icl72a,Best post on this sub in a year imo.,4.0
g23lfus,icl72a,"If the LPR system messes this one up, it’s kind of on them.",3.0
g247cai,icl72a,"I took the pic around 2 years ago in Pittsburgh, PA.",1.0
g25wq49,icl72a,IS NOT NULL!!!!,1.0
g23ew0l,icl72a,These guys paid so much for this.,-3.0
g23gni0,icl72a,So much? They're only $79,6.0
g234qsl,ickhcd,"    LEFT JOIN (SELECT sum(total_views) AS total_views
                     ,sum(total_unique_views) AS total_unique_views
               FROM view_stats GROUP BY challenge_id) vs 
        ON challenges.challenge_id = vs.challenge_id

You're joining in a subquery (vs), but your subquery doesn't return the column (challenge_id) used in your join predicate. You would need:

    LEFT JOIN (SELECT challenge_id
                     ,sum(total_views) AS total_views
                     ,sum(total_unique_views) AS total_unique_views
               FROM view_stats GROUP BY challenge_id) vs 
        ON challenges.challenge_id = vs.challenge_id",2.0
g23543o,ickhcd,"Thank you so much! Been struggling since yesterday, it works now 🥳",1.0
g24knwe,ickhcd,They shouldn't be storing name next to hacker\_id in the  *Contests* table. Breaks 3NF. tsk tsk. :),1.0
g235dzz,ick0ne,"First of all **thank you,** I've been looking for a decent book to get started on sql after reading manga for sql (I highly recommend it for **complete beginners** who still can't wrap their heads around the 101 basics) 

In my opinion knowing how to query data atm is highly valuable but given the pace at which the labor market is changing and evolving, its going to become completely normal in no time. (And ofc, the degree of importance is going to depend on which sector you work in but nonetheless being comfortable with it will be indispensable)",3.0
g237vhk,ick0ne,I have a method to get good books: use amazon to see the most selled and better reviewed and then check the date if it's recent enough. It's not a perfect method but usually the hive mind is right in this concrete cases.,2.0
g23etn4,ick0ne,"cheers

 ill check it out",1.0
g239ibh,ick0ne,"&gt; What are the biggest insights you have ever made with SQL that made you feel like an Omniscient God?

funny you should ask today that because i just had an occasion in another thread to explain one principle that has helped me -- **sargability**

see https://www.reddit.com/r/mysql/comments/ic2qmy/query_that_selects_this_week_vs_last_week/g2388of",2.0
g23uuxx,ick0ne,"Are there versions of SQL that this is more or less applicable to with different execution plans? I do try to take this into account, it certainly battles my colleagues when I rearrange a query and it runs faster when the query logic seems the same!",1.0
g242az5,ick0ne,"sargability applies, i believe, to all databases which use indexes to improve performance",2.0
g23vxce,ick0ne,When I first used recursive CTEs to be able to make a view out of a query that ‘needed’ temp tables and had to be a proc,2.0
g24h593,ick0ne,Ahh the joys of recursive CTE's. I rewrote a process to code low level codes that the commercial application took over twelve hours down to fifteen minutes using recursive CTE's as well as a few other insights. Got a steak dinner out of the CIO for that one! :D,2.0
g295c10,ick0ne,"Alt+Shift in SSMS

Writing a recursive CTE and TVF that would navigate up and down our employee hierarchy so a personnel app could either query all subordinates or all managers of a particular employee.

When I took a query from 45+ seconds down to a few milliseconds after realizing they were joining an nvarchar field to a varchar field, and forcing a cast of the nvarchar to varchar, thus allowing an index seek on both tables.",2.0
g23tbmc,ick0ne,"[https://www.reddit.com/r/talesfromtechsupport/comments/94d98g/my\_phd\_gives\_me\_much\_better\_sql\_skills\_than/](https://www.reddit.com/r/talesfromtechsupport/comments/94d98g/my_phd_gives_me_much_better_sql_skills_than/)

When your SQL is called Mystic by the person who was convinced his PhD enabled him to write queries we mere mortals would never understand and you took a 20+ hour query and got it down to 20 seconds...

:)",1.0
g249xvq,ick0ne,"Interesting achievement.

It happened to me just a couple times since I'm a newbie, but sometimes I get to a state of mind where there's no rational-sequential thinking but some kind of bizarre intuition that performs the task without verbal inner chat. As if step-by-step algoritmic thinking was not really necessary to do anything... Some kind of jedi meditation it seems... Do you know what I mean? It's like brushing your teeth, you don't need to focus on each step, you do it automatically...

I wonder if this skill get better with expertise...",1.0
g24hcog,ick0ne,I find myself doing work in my sleep. I've woken up many a time with a different approach. Most were successful but a few were out and out duds. All were learning experiences though.,1.0
g282cpf,ick0ne,"I hate when I'm falling asleep, get a more ""holistic"" view of things but then forget about it because conciousness can't grasp such complex things... Only time when I can get to that point for more time while conscious is with shrooms but then you go back to normal state.",1.0
g2gftef,ick0ne,"Recently converted a report from using nothing but three views in to several CTE's. Each view was fairly complex and relied on views themselves.  The user ran a report against each view independently, exported the results to Excel then migrated it all together into one report. They then manipulated the report further from there. This process took the user about an hour and a half to do. Now they just go to a link and wait about 10 seconds and it's all done for them, no manipulation afterwards. One of the views I recreated in the new report as a CTE was also used separately to create 3 individual reports manualy and then saved to a network drive once a week. I turned that into a SSRS scheduled job that they no longer have to think about. The first report I described above the user had to do 1-2 times a week, the second that I made into a job they were doing once a week, for the last TEN YEARS. The end-user raves about it and loves having been relieved of this mindless,  but neccessary, task. I feel real good about this recent win. Oh, did I mention that this was originally reports ran in Access? Painfully slow. This request came after our recent WFH due to the pandemic. The endure could no longer get the reports to run in Access because they were so intense. The pandemic has increased my report writing skills exponentially. I'm not a pro but I'm getting better.",1.0
g22tr27,icikzy,"The best practice would be to fix the data model. This information should be in a single table (`user_id`, `event_type`, `occurred_at`), not scattered around multiple tables. 

If the set of ""events"" is (relatively) fixed, then maybe using a de-normalized model using a single table with one `timestamp` column for each ""event"" might make sense. That would reflect the structure you are trying to extract from the current model (and would surely be the most efficient solution). It has the drawback though, that you need to add a new column each time a new ""event"" needs to be registered.",5.0
g22uh2l,icikzy,"Thanks for the suggestions. Unfortunately, I don't think there is anything I can do about the existing data structure.

And, what if the data sources are inherently independent, or if the granularities are different, such as with one-to-many relationships vs. one-to-one? It seems like there could be reasonable justification for separate tables. In any case, even if such justification is lacking, what might be the best solution at the SQL/query level?

Odds are, I won't be able to influence the data structure so I have to think of solutions at the next level down.",1.0
g233e4v,icikzy,"I would first calculate the timespan you need to to setup a list of dates. From that you can create the list of dates using `generate_series()` and then use a left join as you have done:

```sql
with min_max as (
  select min(min_dt) as min_dt, max(max_dt) as max_dt
  from ( 
    select min(""timestamp""::date) as min_dt, max(""timestamp""::date) as max_dt
    from table1
    union all
    select min(""timestamp""::date) as min_dt, max(""timestamp""::date) as max_dt
    from table2
    union all
    select min(""timestamp""::date) as min_dt, max(""timestamp""::date) as max_dt
    from table3
    union all
    select min(""timestamp""::date) as min_dt, max(""timestamp""::date) as max_dt
    from table4
  ) t
), all_dates as (
  select g.dt::date as dt
  from min_max m
    cross join generate_series(m.min_dt, m.max_dt, interval '1 day') as g(dt)
), all_users as (
   -- this should be replaced with a select to the actual ""users"" table
   -- assuming that user_id is just a foreign key to some other table
   select user_id
   from table1
   union     
   select user_id
   from table2
   ...
), all_dates_and_users as (
   select ad.dt, u.user_id
   from all_dates ad
     cross join all_users u
)
select adu.user_id, adu.dt, 
       t1.metric as metric1, 
       t2.metric as metric2,
       t3.metric as metric3,
       t4.metric as metric4
from all_dates_and_users adu
  left join table1 t1 on t1.user_id = adu.user_id and t1.dt = adu.dt
  left join table2 t2 on t2.user_id = adu.user_id and t2.dt = adu.dt
  left join table3 t3 on t3.user_id = adu.user_id and t3.dt = adu.dt
  left join table4 t4 on t4.user_id = adu.user_id and t4.dt = adu.dt  
;
```
Performance won't be great for this unless you do have a very powerful machine. 

-----

There are potential things to improve this. If you don't need a dynamic range of dates, but e.g. want metrics from a specific range instead you can skip the whole min/max part. I also assume you _do_ have a ""users"" table (or something similar) where each user only appears once. 

If both assumptions are correct, you can simplify the above to the following:

```sql
select adu.user_id, adu.dt, 
       t1.metric as metric1, 
       t2.metric as metric2,
       t3.metric as metric3,
       t4.metric as metric4
from (
  select g.dt::date as dt, u.user_id
  from users u
    cross join generate_series(date '2020-07-01', date '2020-07-31', interval '1 day') as g(dt) 
) adu
  left join table1 t1 on t1.user_id = adu.user_id and t1.dt = adu.dt
  left join table2 t2 on t2.user_id = adu.user_id and t2.dt = adu.dt
  left join table3 t3 on t3.user_id = adu.user_id and t3.dt = adu.dt
  left join table4 t4 on t4.user_id = adu.user_id and t4.dt = adu.dt  
;
```

If you don't need 100% up-to-date data, you might want to think about create a materialized view, that is maybe refreshed twice a day or whatever is feasible regarding performance and accuracy of the data.",3.0
g26xxcy,icikzy,"Thanks, I appreciate that example. I think this is probably the solution I'm going to use. But, the drawback is that I will probably generate  significantly more rows than necessary via the `adu` CTE because not every `user_id` has metrics for a given date. The larger the date range, potentially the more extraneous rows I have where there aren't any metrics. Does that sound like a real concern?",1.0
g26zbit,icikzy,"&gt; because not every user_id has metrics for a given date

But I thought that was the whole point? Generate the same list of dates for every user?",1.0
g270ju4,icikzy,"Technically, it is valid because it ensures I do not miss any dates. However, if I have 15 dates cross joined to a user, and there are metrics only for 4 dates, then I have 11 extraneous dates.",1.0
g22zdk1,icikzy,"If you accumulate the data that your receive daily I suggest you partition the tables by date, either daily or monthly partitions. 

If you decide to go with joins you should index the columns that are included in joins in all the tables involved.",3.0
g231pwr,icikzy,"Thanks for the suggestion. Currently we do partition the tables by date, but nothing else. Typically, most of my joins involve a date column and some sort of key or ID.",1.0
g22zpap,icikzy,"EDIT: I am a sql server guy so some of that may not work for you but it gets you going in the right direction.

&amp;#x200B;

I too think that 'The best practice would be to fix the data model.' 90% of the time that is not possible. But with that being said you have to make the data work for you.  If you have the ability to create tables in your database I would take the following approach.

1. Create a table to dump the results into.
2. You will also need a table to track run times so that you are not reprocessing data over and over again.
3. Create a stored procedure to agg the data for you...

`CREATE TABLE dbo.MasterEvents (`
`[Activity_Date] Date`
`,[User_ID] VARCHAR(55)`
`,[Activity_1] bit default((0)) -- X however many activities`
`--...`
`,[Last_Modified]`
`, PRIMARY KEY([Activity_Date],[User_ID])`
`)`

`CREATE PROCEDURE dbo.usp_GetEngagementData (`

`DECLARE u/LastRunDateTime`

`SELECT u/LastRunDateTime = LastRunDateTime from etl.logtable`

`merge dbo.MasterEvents [target]`
`using (`
`SELECT CONVERT(DATE,[source].[timestamp]) AS [Activity_Date]`
`, [User_ID]`
`WHERE [timestamp] &gt;= @LastRunDateTime`
`) [source]`
`on [target].[Activity_Date] = [source].[Activity_Date]`
`and [target]. = [source].[User_ID]`
`WHEN MATCHED`
`THEN UPDATE`
`SET [Activity_1] = 1`
`WHEN NOT MATCHED`
`THEN INSERT([Activity_Date],[User_ID],[Activity_1])`
`VALUES([Activity_Date],[User_ID],1)`

`-- DO THIS FOR EACH OF YOUR METRICS`

`-- LOG IN YOUR ETL THAT THIS HAS COMPLETED SO THAT YOU DO NOT REPROCESS OLD INFO`

`)`",3.0
g232hem,icikzy,"Pardon my unfamiliarity with data engineering, but it sounds like you're suggesting an ETL that adds metrics from one table at a time? I think I still am going to run into the question of: how do I add metrics to the table when there are overlapping dates for some or none, but not necessarily all?",1.0
g232u46,icikzy,"The merge handles that. Updates a row if it exists, flipping the activity to true. If the user didn't already exist a row is inserted for that day with that column (activity) to true. If you set defaults on the columns the others will all automatically be false.",2.0
g232vf6,icikzy,https://www.postgresql.org/message-id/attachment/23520/sql-merge.html,1.0
g26y0gn,icikzy,I see. So merge is like a FULL JOIN in a way?,1.0
g28gwp3,icikzy,"Yes in a way it is like a full join, it is a way to portion out the work that is to be done. By limiting it to just the data that is in-between the master table and whatever source table and limiting the rows that you are looking at in the source by tracking each time processing runs should make it fairly efficient. 

If you want DM me and I can help further. Like I said I am a SQLServerSorcerer so some of my syntax will be off.",1.0
g25s1pb,icikzy,Were you able to figure this out?,1.0
g26y60p,icikzy,"I'd say not quite. I am still trying to assess my options. I'm not too familiar with data engineering, so your code is foreign to me. Do you have any resources in mind that I can read to get a better understanding? Nearly all of my SQL experience is for data retrieval and manipulation, for the purpose of data analytics.",1.0
g22jst5,icfxa4,"You describe a lot of stuff about working out and most of the functionality you describe has more to do with an application, even if it's just something that manages emails and doesn't have a GUI.

So, to be honest, it doesn't really matter which database engine you choose; they can all do what you need sufficiently, and they all have free versions you can use to learn. So you could even get by with a free tier of Azure/AWS database and it would be fine for this purpose.

You could 'force' some relationality into the data schema, however due to low complexity, you could easily get by with really just a list of exercises with a couple attributes and dimensions all in one table. Then maybe create a view via recursive CTE or something to create a pseudo-randomized 'calendar of exercises' that combines a date range with the kind of structured randomness you are looking for.

Most likely, your database isn't going to be pushing out emails/notifications/etc (though you could actually for instance make SQL Server send emails on a schedule using SQL Agent or SSRS) and you'll need some type of functionality to 1)schedule events and 2)relay the data for each event from the database.

For instance, if you want to use SQL Server:

Download and install SQL Server Developer (2017 or whatever, this 'runs' the database) and the latest version of SQL Server Management Studio (this allows you to control the database and develop on it). This may be ideal because you can also use SSRS, SSIS, Agent, sp\_sendmail, etc for free as long as it's not for commercial/production purposes. It is fully featured so will allow you to learn and practice any advanced features even without paying Enterprise licensing fees.

OR

Create an Azure/AWS account and create a free-tier database-as-a-service instance, install SQL Server Management Studio, and follow directions provided on Azure/AWS to learn to connect to your database. I'm certain these services also have free-tier BI tools that can probably alert/email you somehow. 

There's a million ways to accomplish the overall goal with any combination of different tools and techniques. In fact, if the point wasn't to learn SQL, I'd say you could build a way more lightweight version with something like Google Sheets combined with a couple other free tools. Hell for all I know Google Sheets has push notifications and would give you everything ;)

If this project motivates you, then you are already ahead of the pack as far as beginners go. It isn't intrinsically ideal for SQL because of a lack of naturally relational elements to it, but as long as you are aware of that it shouldn't hold you back. Once you've gotten a little experience you may desire something more deeply relational to help you move into more advanced territory.",3.0
g22ltgq,icfxa4,This does excite me! Thanks so much for your reply that give me some solid guided work to do 😁,1.0
g22it9e,icfxa4,"So, SQL is the means by which to interact with a relational database.  You are nowhere near ready for any SQL, rather you need to design your database.  I am not sure what your experience was so you may need to draw on your education (CPT-242 at your local technical college would be a great refresher).  Brush up on database design first and then start working out a schema while also identifying where the data will come from (creating as / where needed).

As far as software, I am a big fan of SQL Server / SSMS.  They are free up until you start using in a production environment or want some of the nicer options such as SSRS / SSIS (your project would benefit from SSRS to send the workouts but there are other ways).

Once you work out the database schema, you can start playing with the actual SQL.  The code to generate what you are looking for will not be that difficult and you can likely self-serve on it via google and specific questions on StackOverflow / here / other places (note - ""write all of my code"" is not a specific question).

As far as delivery, core SQL does not do this type of thing, at least not within my own sphere of knowledge.  SSRS within SQL Server would be an option but as mentioned above, it would not be available within the free edition.  Other SQL dialects may have better options, or you can see if you can work out a stored procedure / Windows task to communicate with Outlook / Google Calendar / SMS relays / etc.

Good luck!",2.0
g22l0u3,icfxa4,"Well my drawn brainstorm table as it stands is the 7 exercise groups as headers and 5 exercises per group underneath.

For querying I don't think this is the best way to structure it through. I have two ideas so far:

1) separate table per group, i.e.

""Legs"" Table
Squats
Lunges
Calf raise
Leg Press
Leg extension

2) have a Group and an Exercise field and contain all in a single table, i.e.

Group | Exercise

Legs | Squats

Legs | Lunges

Legs | Leg Press

Chest | Bench Press

Abs | Crunches

Etc.

Then could do like

SELECT exercise
Where Group equals Legs
Random ()
Limit 1
Union select
Where Group equals Chest

As you can see I haven't looked into do it properly but that's my idea",1.0
g22o4rk,icfxa4,"&gt;Well my drawn brainstorm table as it stands is the 7 exercise groups as headers and 5 exercises per group underneath.

Hard stop - that's a pivoted table, not a relational one.  Go take that database class. ;)",1.0
g22ouap,icfxa4,"No I know that, read the rest of my post :)",1.0
g24s9fz,icfxa4,"Fair.  I think you can get by with one table, but may be more efficient with something like Program (header-level) joined to Workout (transaction-level).  Your script would write lines to the Workout table with fields Date, Category, Order, Reps, Weight, Etc (multiple lines per day) and then your reporting tools would pivot from this table based on a filter by date.  You can also go back and append history such as actual reps / weight.  You may also want a third, fourth, fifth, etc table that contains common information that would otherwise be denormalized on Program table or hardcoded (BAD HABIT) as parameters within the query  (location, equipment, number of exercises, acceptable weight ranges, etc, etc).",1.0
g24y3xg,icfxa4,"Thanks for this! I would love to build this from a learning point of view eventually, but from a training point of view I want it very basic; just 7 exercises for me to do in a list. 
Reps, weight, tempo, etc. All of that is in my head and I don't want the overhead of having to report/track all of that info. I don't even want a log, because I haven't really found them useful in the past (for working out that is)",1.0
g22k57g,icfxa4,"It seems like a lot of information. I always push for the smallest deliverable user story. What's the minimum feature that would deliver value?

US1: Email workout at 9am 

US2: Randomize Workout

US3: Add workouts to category

US4: Remove workouts from category

US5: SMS workout at 9am

... 

I don't know if it's a rule but I've seen most User Stories as:
As a {persona}, I want {some goal} so that {some reason}.
ie. As a user of the app, I want to receive a workout email at 9am so that I know what muscle group I have to work that day.

Just my thoughts on it. It is a good size product with multiple features to learn with. Other people have answered some of the DB side, I'll just stick with this.",2.0
g22lkcx,icfxa4,"Thanks for your pressure to KISS 🙂

As for story format, there is no evidence that any particular format leads to better outcomes over another. So long as it accurately describes a problem on the user side to solve or a desired behaviour change it is fine. As long as it isn't a solution without context it's good!

I feel like overdoing detail is better because than can be trimmed back, where lack of detail is a breakdown of communication",1.0
g22lzns,icfxa4,"The easy part in this would be writing the code to execute your task if you're decent at programing ( I am not) 

You have the right idea i think, so start by just seeing how to setup a database of w/e flavor you chose and then plan it out however you know how to, as in identify your data ( Muscles, exercises, calendar...), categorize it ( Tables structure), format it ( Datatypes), index it.

At that point your program can go about creating a UI or delivery method that can either run queries or just select from Views(Something to lookup? You got a deep rabbit hole dude) preset in your database. Whether you want to receive your workout plan as a text message or an HTML table in an e-mail is not something you would use a SQL database for. You may be able to use things like SSIS since you can write scripts in there, but there's 1000 ways to skin a cat i guess.


Take Starting Strength 3x5 working; It consists of 2 exercise groups of 3 heavy sets of 5 reps ( Warm up not included) A and B , both consisting of 3 compound lifts and 1 or 2 isolated exercises. Workout A is Squats, bench, deadlift + isolated. Workout B is Squats, Overhead Press, Rows and a Complimentary or 2. You do the plan 3x a week, alternating A, rest day, B, Rest day, A, 2 rest days, then B, A, B etc.

 Consider the muscles involved for each lift and see how you could categorize them...Your ""Muscles"" table could be something like this

 ID | Name | Function | Group | Rested | OppositeMuscleID | LastWorkedDate

Function and Rested can be Bit columns like Push/Pull and Y/N, so just 0/1

OppositeMuscleID can be a foreign key index, referencing the ID column.

Group can be a numbered ID for a given Limb or part of your torso ( So like, 1 to 5 or 6? Arm, leg, chest, back, abs &amp; lower back ( Core) )

I'd then go about creating Exercise Tables refering to values from the Muscles table such as ID, Function, Group and OppositeMuscleID and use Stored Procedures to either create or update tables and/or views using conditions based on the values in your Muscles table, where you could check to see for example if you've had 1 or more rest days since a given muscle was worked out, then create your workout plan based on either isolated or compound lifts, send your notification and update the ""Rested"" column for newly worked out muscles with the timestamp and flipping the bit for others that have been rested so they're ready for the next run.


Hey you wanna build a workout app with me? Lol jk.",2.0
g227eqv,icez8j,"In general, SQL views are simply a stored query. You query the view which runs the query you defined it with. So there really isn't a concept of refreshing it, but yes, as the underlying data changes, so would the results of querying the view.  
  
Now I've never used Athena before, but a quick google led me to here (https://docs.aws.amazon.com/athena/latest/ug/views.html) which says ""A view in Amazon Athena is a logical, not a physical table. The query that defines a view runs each time the view is referenced in a query.""  
  
So it seems the concept is the same in Athena, and I would think when you upload the file to S3, you'd ""see"" that data when you subsequently query the Athena view. But it should be easy to test.",2.0
g227sn6,icez8j,"Yes - echoing the other commenter, your views will 'refresh', they are just a way of storing your query.",2.0
g22sjmr,icez8j,"Echo what everyone else here has said and add that if the table structure changes, you need to update your view to pick up the change. For example, if a column is added, you will need to alter your view to see the new column.",1.0
g21zpbk,icdp6h,"Alright, I am by no means a SQL power user so take with grain of salt.  However, what you are trying to do here with relational database scripting, **while eminently doable**, is far easier to accomplish with dedicated dashboarding and data pivoting/viz tools such as Power BI, Tableau and even Excel.  Nearly all of these tools will direct-connect to your SQL server to boot.  This will further enable your audience to self-serve by filtering, searching, drilling up/down, etc.  Unless you specifically need to write a dashboard to embed into an app, e-mail, etc, I would not go to this trouble.  Even if you don't have a feasible option to connect to a data viz suite, you will likely get better mileage with whatever is the equivalent to SSRS in PostgreSQL.",6.0
g220429,icdp6h,Yeah I’d definitely just get the dates in one column then setup the yoy in pandas if I was pulling this,2.0
g22g0d1,icdp6h,"this, I would use DAX MEASURE for this",2.0
g22ls73,icdp6h,"Yep, I agree that it's easier with certain other tools. However, since I don't use R or similar tools (I am mainly using Google Sheets, Tableau, and SQL), I feel like there is some inefficiency to go from queries to insights. Rather than transition data from SQL to these other tools, in this specific use case, I'd rather have it pivoted and readily accessible in the SQL client.

Also, I am using PrestoDB. I've used PostgreSQL in the past though. For my problem, it seems like the Postgres solution can directly apply to Presto.

Which tools do you use from A to Z for data manipulation and reporting?",1.0
g22o1bz,icdp6h,"Respectfully, ""since I don't use"" is never the right response. :) 

Transitioning data from your RDBMS to other tools is in almost all cases as simple as logging into your server from said tool.  Even Excel can do this.

I personally have had some great success aggregating data in SQL and reporting in Tableau and PowerBI, although I most commonly just work thru Excel in my current setting.  PowerBI is somewhat more convenient (esp. cost), esp. if you are already using Microsoft tools, but does have probably the highest learning curve (and inherent power) - you will want to take a class on it (edX has a great one that you can blow thru in a weekend).  Tableau is much more user-friendly although be prepared to google for help extensively at first if you are completely new, also suggest taking a class or at least watching some videos so you are not scrambling around for basic stuffs.  All that said, nearly any of these tools can get you rolling on interactive YoY / etc dashboards in the first hour or two.  

I will leave you with one final word of warning - refrain from building massive denormalized flat files in SQL for connecting to external tools - rather create a relational datamart, then pull these tables (or query which generates said tables) into your data viz app.  You can easily turn 20-50MB of data into gigabytes if you do not normalize into tables.  Example - you have 150 customers but their header-level info repeats across a million rows because you grabbed it all into your SELECT instead of just bringing the 150-line customer table with foreign key to it in your transactions, then joining the two within the data viz tool.",2.0
g22t32e,icdp6h,"I agree, you highlight the need for me to prioritize learning a few more skills :)

With Tableau or PowerBI, what would the typical workflow look like? Write a query in a SQL editor, confirm validity, and run that query directly in Tableau or PowerBI?

I just started learning Tableau recently, but I've spent more time ""doing"" rather than studying, so I am ignorant to a lot of the features.

And regarding your data mart point. I agree, but I actually am tapping into our company's data mart. Basically, I am doing some manipulation, aggregation, etc. to create a final result of a few thousand rows. What I am apparently spending too much time on is trying to output these rows in a particular format or structure, something that data viz tools seem best equipped to handle. However, I am doing this because I can actually eyeball the output myself and draw insights. I don't need to get the data into a viz tool *just yet*.

Ultimately, I just want to minimize the amount of lag time between getting the data and getting my answers.

What kind of workflow do you suggest for quick iteration like that?",1.0
g24092q,icdp6h,"Yes, that would be an example workflow.  As far as sending a query - yes I believe this is an option in nearly every mainstream software package.  The other option is to spin up your own relational tables and connect to them rather than sending queries (datamart approach) - you can keep the tables updated via scheduled stored procedures (MSSQL - am sure PostgreSQL has an equivalent).   This has been my own preferred method.  That said, most of my data needs are ""previous day / week / month"" - if I were wanting to ping real-time info I would most certainly be sending a query.

Main advantage of direct query is real-time data access and relative ease of use (esp. if queries are simple), main advantages of datamart are reduced real-time processing burden since data has already been calculated and staged, improved security and scalability.

Understood that you already have a company datamart but I would still consider creating your own.   Starting with the company datamart is fine - you're just moving the same data around.  Can be as simple as spinning up a blank database and creating table(s) there from your earmarked SELECT(s) as long as your IT does not frown on that (not sure why they would).  You can then connect these tables to other apps by logging into your environment from said app via ""get data"" menus.   I would suggest requesting a user that is only authorized for your specific database and cannot see any others for this purpose.",2.0
g26ydal,icdp6h,"Thanks for the info. Do you have any resources to share that can help me learn more about working with the relational tables?

It sounds like having my own datamart would provide more useful manifestations of our existing data. I would have be able to have simpler and quicker queries. Does that sound right?",2.0
g27qw19,icdp6h,"Precisely.  The main tools that I use to maintain my datamarts are DROP TABLE IF EXISTS,  WITH, CREATE TABLE and SELECT INTO.  May be worth to watch a few videos as well - I am not a daily SQL user and am not primed on best practices as a result.",1.0
g221b53,icdp6h,I would say window function using row_over(),2.0
g22lvva,icdp6h,Could you share an example please? And do you mean `row_number over()?`,1.0
g22no55,icdp6h,"Are you sure you are using Postgres? It neither has a `date_add()` function nor a `year()` function. 

In Postgres I would solve that by using `generate_series()` to create a calendar table ""on the fly"" and the doing a left join against that.",2.0
g22t5a5,icdp6h,"I am using PrestoDB, but it made me pick a flair and I figured that Postgres was close enough. I've also used it in the past. Perhaps I should've picked the Discussion flair?

I wish I could use `generate_series()` in Presto. There isn't such a function afaik.",1.0
g22qxe5,icdp6h,"Hey, not sure if this is the best way but here goes:

first I need to create temp table to replicate your data in the screenshot, then i get the start and end date of 2020 data. Use CTE to create reference table and join 2019 and 2020 data.

This is done using MSSQL:

    
    /* Create table */
    SELECT * 
    INTO #tmp
    FROM (
    	SELECT '2019-08-04' AS dt,	9 AS col1 UNION ALL
    	SELECT '2019-08-05',	12 UNION ALL
    	SELECT '2019-08-06',	12 UNION ALL
    	SELECT '2019-08-07',	8 UNION ALL
    	SELECT '2019-08-08',	20 UNION ALL	
    	SELECT '2019-08-09',	8 UNION ALL	
    	SELECT '2019-08-10',	5 UNION ALL	
    	SELECT '2019-08-11',	11 UNION ALL	
    	SELECT '2019-08-12',	11 UNION ALL
    	SELECT '2019-08-13',	9  UNION ALL
    	SELECT '2019-08-14',	7  UNION ALL
    	SELECT '2019-08-15',	8  UNION ALL
    	SELECT '2019-08-16',	6  UNION ALL
    	SELECT '2019-08-17',	4  UNION ALL
    	SELECT '2020-08-03',	1  UNION ALL
    	SELECT '2020-08-04',	1  UNION ALL
    	SELECT '2020-08-05',	3  UNION ALL
    	SELECT '2020-08-07',	2  UNION ALL
    	SELECT '2020-08-08',	3  UNION ALL
    	SELECT '2020-08-09',	3  UNION ALL
    	SELECT '2020-08-10',	18 UNION ALL
    	SELECT '2020-08-11',	18 UNION ALL
    	SELECT '2020-08-12',	10 UNION ALL
    	SELECT '2020-08-13',	19 UNION ALL
    	SELECT '2020-08-14',	16 UNION ALL
    	SELECT '2020-08-15',	5  UNION ALL
    	SELECT '2020-08-16',	4 
    ) tmp
    
    /* Set date range */
    DECLARE u/startDate DATE  = (SELECT MIN(dt) FROM #tmp WHERE YEAR(dt) = 2020)
    DECLARE u/endDate DATE = (SELECT MAX(dt) FROM #tmp WHERE YEAR(dt) = 2020)
    
    ;WITH dateRange AS (
    	SELECT u/startDate AS dt
    	UNION ALL
    	SELECT DATEADD(DAY, 1, dt)
    	FROM dateRange
    	WHERE DATEADD(DAY, 1, dt) &lt;= u/endDate
    )
    SELECT dr.dt
    	 , ISNULL(t20.col1, 0) AS data_2020
    	 , ISNULL(t19.col1, 0) AS data_2019
    FROM dateRange dr
    LEFT JOIN #tmp t20 ON dr.dt = t20.dt
    LEFT JOIN #tmp t19 ON dr.dt = DATEADD(YEAR, 1, t19.dt)",2.0
g22tm1w,icdp6h,"Thanks, I appreciate the example!

It looks like `dateRange` filled the gap where `2020-08-06` was missing.

My only fear is in terms of speed. My tables have another dimension to go along with date, which is basically an ID (`db_filename`).

So, my query will be more like:

    select dr.dt
    ,id
    from 
    etc.

t20 and t19 are actually large tables in my case.

I'd imagine the query would still finish, but it'd take a lot more time. I imagine this is just an inefficiency I can accept?",1.0
g22u1kq,icdp6h,"No problem!

Perhaps you can slice your table into the date range you wanted first before joining them?

For example

    ;WITH dateRange AS (
    	SELECT u/startDate AS dt
    	UNION ALL
    	SELECT DATEADD(DAY, 1, dt)
    	FROM dateRange
    	WHERE DATEADD(DAY, 1, dt) &lt;= u/endDate
    )
    , tmp2019 AS (
        SELECT * FROM #tmp WHERE YEAR(dt) = 2019 AND MONTH(dt) = 8
    )
    , tmp2020 AS (
        SELECT * FROM #tmp WHERE YEAR(dt) = 2020 AND MONTH(dt) = 8
    )
    SELECT dr.dt
    	 , ISNULL(t20.col1, 0) AS data_2020
    	 , ISNULL(t19.col1, 0) AS data_2019
    FROM dateRange dr
    LEFT JOIN tmp2020 t20 ON dr.dt = t20.dt
    LEFT JOIN tmp2019 t19 ON dr.dt = DATEADD(YEAR, 1, t19.dt)",1.0
g22uqcm,icdp6h,"Yes, that can definitely help, but I think I would still have that fundamental inefficiency.

If `dateRange` is say 30 rows, then 30 rows x 5m records = 150m records. Let's say  a date doesn't exist 50% of the time, then that's still 75m rows.

I know I am not able to do a `right join` at the end, so I imagine this is the only solution for this particular approach you shared.",1.0
g22vfe3,icdp6h,"I see where you are coming from.

I am not sure if string manipulation will have worse performance or not, but it eliminates date not found in your original table.

You can play around and see which one works better. Cheers

    /* Create table */
    IF OBJECT_ID('tempdb..#tmp') IS NOT NULL DROP TABLE #tmp
    
    SELECT * 
    INTO #tmp
    FROM (
    	SELECT '2019-08-04' AS dt,	9 AS col1 UNION ALL
    	SELECT '2019-08-05',	12 UNION ALL
    	SELECT '2019-08-06',	12 UNION ALL
    	SELECT '2019-08-07',	8 UNION ALL
    	SELECT '2019-08-08',	20 UNION ALL	
    	SELECT '2019-08-09',	8 UNION ALL	
    	SELECT '2019-08-10',	5 UNION ALL	
    	SELECT '2019-08-11',	11 UNION ALL	
    	SELECT '2019-08-12',	11 UNION ALL
    	--SELECT '2019-08-13',	9  UNION ALL
    	SELECT '2019-08-14',	7  UNION ALL
    	SELECT '2019-08-15',	8  UNION ALL
    	SELECT '2019-08-16',	6  UNION ALL
    	SELECT '2019-08-17',	4  UNION ALL
    	SELECT '2020-08-03',	1  UNION ALL
    	SELECT '2020-08-04',	1  UNION ALL
    	SELECT '2020-08-05',	3  UNION ALL
    	SELECT '2020-08-07',	2  UNION ALL
    	SELECT '2020-08-08',	3  UNION ALL
    	SELECT '2020-08-09',	3  UNION ALL
    	SELECT '2020-08-10',	18 UNION ALL
    	SELECT '2020-08-11',	18 UNION ALL
    	SELECT '2020-08-12',	10 UNION ALL
    	--SELECT '2020-08-13',	19 UNION ALL
    	SELECT '2020-08-14',	16 UNION ALL
    	SELECT '2020-08-15',	5  UNION ALL
    	SELECT '2020-08-16',	4 
    ) tmp
    
    ;WITH monthDay AS (
    	SELECT DISTINCT SUBSTRING(dt,6,5) AS dt
    	FROM #tmp
    )
    , availableDate AS (
    	SELECT CONCAT('2020-', dt) AS dt
    	FROM monthDay
    )
    , t2019 AS (
    	SELECT * FROM #tmp WHERE YEAR(dt) = 2019 AND MONTH(dt) = 8
    )
    , t2020 AS (
    	SELECT * FROM #tmp WHERE YEAR(dt) = 2020 AND MONTH(dt) = 8
    )
    SELECT dr.dt
    	 , ISNULL(t20.col1, 0) AS data_2020
    	 , ISNULL(t19.col1, 0) AS data_2019
    FROM availableDate dr
    LEFT JOIN t2020 t20 ON dr.dt = t20.dt
    LEFT JOIN t2019 t19 ON dr.dt = DATEADD(YEAR, 1, t19.dt)",2.0
g26yhe6,icdp6h,"Looks like I have some experimenting to do. Thanks for the help, I appreciate it!",1.0
g22sjbo,icdp6h,"Explain to me what you're really trying to solve for here? Getting the date you're missing is a simple thing if you use a date table and join to another query... 

When you say how do I prepare YoY analysis, what do you mean? How is this data going to be consumed?",1.0
g22txv4,icdp6h,"Essentially, I am trying to see YoY growth for various metrics based on a given set of dates and IDs (`db_filename`). I need things to be on an ID and date level, because I am going to create cohorts later and I need the granularity.

I want 1 date column, 1 cohort column (potentially multiple), and 1 column for each metric showing that YoY change.

This data will be consumed as a table with highlighting and potentially some other charts.",1.0
g22wqf1,icdp6h,"Consumed how? That isn't really answering my question from a data perspective... you mean through Excel? Consumed how?

The table you're describing sounds very small... so what is the problem? Just dump it out into a table and use it.",1.0
g231jqn,icdp6h,"Sorry for not quite answering your question.

Although the example data was small, here is my situation:

I have about 40m IDs in the database and there are multiple tables that track various activities for these IDs. For each day, there can be up to a million rows. I have CTEs for each of these tables, in the general format of: date, ID, metric A, metric B....metric Z. I am trying to unify all this data into one table, and compare YoY performance for each metric over a select set of days in 2019 and 2020 (my example timeframes).",1.0
g24p6ql,icdp6h,Can the past change?,1.0
g26yisc,icdp6h,"For some metrics, yes. But, for most, the data will always stay the same.",1.0
g28merv,icdp6h,So just store the data in a aggregate table? What's the problem? Run your job in chunks to get the old values that don't change to keep them on hand.,1.0
g220ai3,icdp6h,If you know any python I’d just pull the first table then setup the yoy in pandas because the sql for this is going to be tricky,0.0
g22ly96,icdp6h,"Sadly, I don't know python. I wish I did :/

The opportunity cost to learn it right now is a bit high for me, so it's more of a long-term goal of mine.",2.0
g21hdbx,icb9yu,"You can use CASE statements with the LIKE operator to map each unique item by a new name in another 'items' column. Then GROUP BY those new names from the new column generated by the CASE statement.

&amp;#x200B;

&gt;SELECT  
&gt;  
&gt;(CASE  
&gt;  
&gt;WHEN item LIKE 'Socks%' THEN 'Socks'  
&gt;  
&gt;WHEN item LIKE 'Shoes%' THEN 'Shoes'  
&gt;  
&gt;WHEN item LIKE 'Hats%' THEN 'Hats'  
&gt;  
&gt;END) as items,  
&gt;  
&gt;SUM(Value)  
&gt;  
&gt;From table  
&gt;  
&gt;GROUP BY items",4.0
g21n7mr,icb9yu,Much appreciated,3.0
g24zb8v,icb9yu,"This will not work. Remember that the group by will be evaluated prior to the SUM function, due to actual order of SQL Operation:

FROM  
JOIN  
WHERE  
GROUP BY  
HAVING  
SELECT  
ORDER BY  
LIMIT

The Items calculated column won't exist yet when the query engine gets to the group by. So you'd need to put the above case statement and value into a subquery, then sum from the subquery. Such as

    SELECT Items.ItemCategory, SUM(Items.Value)
    FROM
    (
        SELECT
            CASE
                WHEN item LIKE 'Socks%' THEN 'Socks'
                WHEN item LIKE 'Shoes%' THEN 'Shoes'
                WHEN item LIKE 'Hats%' THEN 'Hats'
            END as ItemCategory,
            Value
        From table
    ) AS Items
    GROUP BY I.items

Having said that, if you're wanting to have the ItemCategory be dynamic based on the item name, then perhaps the regex answer would work better.

But since you're learning. Always, ALWAYS remember the actual order order of execution, compared to how we write/read SQL.",2.0
g21io90,icb9yu,"If it’s 100% consistent that numbers are what you want to screen out of the “Socks2”-like values, you can use `regexp_match` on that column. So you would:

    SELECT 
    regexp_match(Item, ‘(\w+)\d’)[0] AS essential_item

As in [this discussion thread](https://stackoverflow.com/questions/41818279/postgresql-regular-expression-capture-group-in-select/41818778). Your dialect’s dealing with regexes may vary, so definitely consult on the syntax there. Generally, the parentheses in the regex define a group, so in the above example, I’m grouping letterlike symbols that occur before a digit. 

Regexp_match, if you make a group part of the regex pattern, returns an array of strings. The match group I’m interested in occurs first, so I’m taking the first such match by asking for the zeroeth item in the array that a grouped regexp_match returns. This will grab the texty part of the Item value and drop the numbers part. 

Did that help?",3.0
g21mnw3,icb9yu,"This is a bit different than what I was looking to do, but I think I may find use for this somewhere else. Appreciated!",1.0
g21n0ds,icb9yu,"I realize now that I didn’t add that I’d use the regexp_match in a CTE and then, from that

    SELECT 
    essential_item AS Allitem,
    SUM(Value) AS Sumvalue
    FROM cte_name 
    GROUP BY essential_item

Which would deliver the structure you asked for as output, as I understand it. Does that get you closer?",2.0
g21ny8t,icb9yu,"Honestly this just may be going over my head. Like I said, I'm only a few weeks into SQL but I will definitely take time to digest your explanation.",1.0
g21pq9z,icb9yu,"Ah, sorry, let me at least clarify [CTE: Common Table Expression](https://www.sqlshack.com/sql-server-common-table-expressions-cte/). It lets you create a table-like concept you can use as a stepping stone when you have complex transformations to do. I like them for keeping track of each step in a process, and I recommend them for newcomers to SQL because they give a lot of clarity (and get you some extra syntactical steps!)

So I’m proposing that you split your operation into two parts: creating a table where the item name is just the name part (no more Socks2, only Socks) and then that you sum all values with item name Socks. All together, that looks like:

    WITH cte AS (
        SELECT 
            regexp_match(Item, ‘(\w+)\d’)[0] AS item_name,
            Value
        FROM original_table)

    SELECT
        item_name AS Allitem,
        SUM(Value) AS Allvalues
    FROM cte
    GROUP BY item_name;

If you replace `original_table` with your table’s name, this should end up giving you the output you hoped :)

Good luck and please don’t be shy with questions.",2.0
g21ykgq,icb9yu,"I can’t tell if this is a school question or simplified business, but it stresses why well thought out data and normalization is important.  People above have explained how to answer your question, it’d be so much more efficient with a “category” column that you can group by.",2.0
g20y8te,ic7ypi,"with open(myfile, ""w"") as f:

    f.write(",2.0
g24tpni,ic7ypi,Thank you!,1.0
g212sng,ic7ypi,"Steps in your Python script:

1. Write new data to temp file.
2. If saving new data is successful, delete old file and replace with new file.
3. If not successful, notify that script failed and leave old file.",1.0
g24zvjs,ic7ypi,"Not sure why you're getting downvoted. This would be my approach. Stick it in a \_filename.csv, upon successful completion, delete orginal and rename \_filename. else if failed delete \_filename.csv and return error, preserving the last good csv.",2.0
g21icpc,ic7ses,You either need to group by all non aggregate columns you are pulling in or create a CTE or sub query getting your IDs and time values then joining to the sub query on ID,2.0
g21mk1l,ic7ses,"First, what DBMS are you using.  Oracle has Keep Dense_rank which I think would help you, but MS does not.  

Second, I am not sure how your formatting is messed up, but it is.  Did you put \^ at the front of your code?  On reddit, I think 4 spaces in front of each code line works.  Let me try:

    (SELECT A.AUTO_ID, MAX(A.EVM_LAST_EDITED), ...)
    (FROM evm_vp A)
    (LEFT JOIN TAT B)
    (ON UPPER(A.EVM_SYNC_ID) = REPLACE(REPLACE(B.tree_global_id, '{', ''), '}}', ''))
    (GROUP BY A.AUTO_ID)
    (ORDER BY A.AUTO_ID)

How's that look?  The code block feature also means you don't need \\ in front of \_.  

Finally, Do all the fields you want get updated at the same time?  In other words, are all the values correct in the same row, whichever row has the highest evm_last_edited?  If so, it's easier.  If some fields were edited earlier/later than others, it's a bit of a pain.  

Assuming SQL Server, and all fields are on a single row of data:

    Select * from (
    Select a.auto_id, a.circuitname, a.work_verification_status, a.evm_last_edited,
           row_number() over (partition by a.auto_id order by evm_last_edited desc) as RN
    FROM evm_vp A
    LEFT JOIN TAT B
    ON UPPER(A.EVM_SYNC_ID) = REPLACE(REPLACE(B.tree_global_id, '{', ''), '}}', '')
    ) foo
    where rn = 1",1.0
g213g92,ic616o,"The file path does not actually point to a real file somewhere. The sqlagent is running into a serious problem and reporting the line of code where this happens.

See if you can reinstall the agent. If this does not work, see if you can upgrade your server. If you are running on the latest CU, well then I am afraid you have to contact microsoft support.

Or build a new server and move your databases, account etc..",1.0
g2142du,ic616o,Thankfully we're a CU behind so hopefully that will fix things!,1.0
g214x4g,ic616o,"Good luck! Hopefully it is a bug that microsoft has already fixed. But honestly, the main reason I said that is simple. If you contact microsoft support they will ask you anyways. And depending on your contracts it can cost money. So best to do it first.",1.0
g21t7w0,ic5mde,"Try schema.table.column.  Do you have the synonym set up for cpp? Is cpp local or in a different schema?

Does the user you’re creating/running this code for have access to select from cpp?",1.0
g1zl3pn,ic1ylp,"Might want to cross post to a linux subreddit as this is a system question, not a SQL one.",10.0
g1zlp5m,ic1ylp,Done. Thank you.,3.0
g202udt,ic1ylp,Have you tried loading the lsb-release package?,4.0
g207qu6,ic1ylp,"Like the error says, try to install first the dependency needed (lsb-releases) and then install MySql, that should work

Edit:
Type in terminal:  sudo apt install lsb-releases",4.0
g1zvulx,ic1ylp,"Are you following a guide for a different distro by any change?

Programs and commands do differ between different distros. Has apt worked before? Try apt-get instead perhaps.",2.0
g20iad2,ic1ylp,This seems like the best advice here,2.0
g21jhn0,ic1ylp,Switch to postgres,1.0
g203bu6,ic1wbm,"'Hardening' means different things to different people.  if you mean protecting your data then make sure the grants on your schemas are allowing only you and those you want to have access.  

Now if you have admin privs then you need to go read up on the security chapter of the MySQL manual.",1.0
g2040wf,ic1wbm,"Gotcha. The data in the DB isn’t sensitive, I think mostly just the user privileges are what I need to secure. Can I do all of this through Workbench?",1.0
g206kxp,ic1wbm,Yes. Click on the name of the desired schema under the list of schemas (on the left side column) and then click on the wrench to see who has what privileges.,1.0
g1z23cx,ibyci2,"Usually if you're trying to generate your model from code you use the code first concept in entity framework. I'm not aware of anything that will create database files via a json structure, but that doesn't mean they don't exist. 

How big is your json? Unless it's crazy huge, I'd just manually create the DB once, then you can script it to deploy if you need it in more than one spot. I feel like you could easily spend more time trying to automate this than just doing it.

Like I said there may be tools out there I'm unaware of, but the only other way I could think to do this would be to create the classes in c# from your json, then write something to through them and read properties to generate the sql create statements.

Once again though, I feel like you'd spend less time just doing it by hand once, and scripting from there.",4.0
g21ts6v,ibyci2,"What language are you using?
As u/andrewsmd87 pointed out, the best way to convert a type from code to sql is to use an ORM like EntityFramework.
If you are working in c# though, i wouldn’t be using the json as a “table”.

If your data size isn’t very large, you can use something like MongoDB (no sql) to store json as-is. If using JS, check out Mongoose, which is an ORM for MongoDB.",1.0
g1xz7px,ibs9c7,"To do this, you need to use ""wildcard characters"" and the LIKE command:

https://www.sqlitetutorial.net/sqlite-like/",1.0
g236y5k,ibs9c7,"not sure if sqllite supports group by but..

select sum(total), date_trunc(month, date) month from xx 
group by date_trunc(month,date) ? 

probably getting year worth of months and their totals is fast as getting one month worth.

or 
select sum(total), date_trunc(month, date) month from xx 
where date between x and y 
group by date_trunc(month,date) ?",1.0
g1xuwqe,ibpm9l,"If you're wanting to just learn SQL, don't mess with cloud yet. Just one more thing to learn that isn't necessary for your primary goal. 

Cloud excluded, yes, SQL RDBMS's (Postgres, MySQL, MS SQL Server, Oracle) all run on a ""server"", be that a server in the traditional sense, or even just your laptop for dev/test purposes. They run as services on the server, so when it's on, they're on. The actual ""database"" is really just a collection of files that the RDBMS manages. But that varies greatly between the different vendors. The rdbms service is what takes your query, and executes it. It also manages the security of the database, creates query plans for getting data out of the DB, makes recommendations for indexing, makes sure all the rules are followed when inserting/updating the DB, etc. 

Perhaps  an ELI5 would work here,

You are seated in a restaurant, and a waitress comes to you. You simply say ""I want some spaghetti with bacon."" She goes on her way, passing off the request to the back of house, that gives an order to the chef to prepare the food, he puts you in a queue, looks up a recipe, cooks your food, let's the waitress know it's ready to deliver, waitress brings you the food. Turns out you don't like the food. You tell the waitress you want this meal removed from the menu. Who ever heard of spaghetti with bacon anyway?? Waitress goes and talks to management and security. Unbeknownst to her you are the owner of this restaurant. Anyway, After verifying you have the necessary authority to take something off the menu, she takes all the menus, splits them up between eight workers, and proceeds to remove the spaghetti with bacon from the menus. She then comes back to your table and let's you know she fulfilled your request. 

That is an RDBMS and SQL, as a declarative language. You say, in SQL Language, Select these columns from this table. Insert these records. Drop that table. And the RDBMS takes it from there compiling a query plan by looking at statistics, indexing, security, etc, puts your request in a queue to be processed, processes it, then delivers the result to you.",5.0
g1ypnzm,ibpm9l,"&gt; Cloud excluded, yes, SQL RDBMS's (Postgres, MySQL, MS SQL Server, Oracle) all run on a ""server"", 

Nitpicking, but: 

""Cloud databases"" run on a server as well - that server is just located in a huge (often distributed) datacenter (and might be virtualized) - but it's a server nonetheless. You can't really run a relational database without a ""server"" of some kind. 

The term ""serverless"" is complete nonsense in my opinion. Just because I can't control or have access to a ""server"", doesn't mean the software runs ""in thin air"".",1.0
g1ypywe,ibpm9l,"This is true. I should have worded that better. It's just, the definition of ""server"" gets more fuzzy when you do get to the cloud, especially with RDS and azure SQL database. So I was just trying to exclude that for simplicity's sake.",1.0
g224kln,ibpm9l,Kudos for throwing the number eight into your awesome explanation!,1.0
g1xf5oe,ibpm9l,"""Server"" from a SQL standpoint usually means a database instance.

For local host vs cloud, I'd recommend doing some reading on cloud in general (they'll waste no time telling you more than you wanted to know in that realm), especially PaaS and IaaS",1.0
g1z0ms1,ibpm9l,"| What are the pros and cons of hosting data in a local machine vs the cloud?

for learning SQL I'd go with hosting on a local machine, for the main reason is that it's then your own system and you can do anything you like with it. Cloud services cost money and may have some restrictions in terms of what you can and can't do with them, although these differences are irrelevant at this early stage. Bear in mind that different servers use slightly different flavours of SQL, so a query written for mysql might not run on a Microsoft SQL Server as-is.

Mysql is free, you can enter commands into it via a command prompt but you might find that a front-end such as mysql workbench would be easier to use. You can also use phpmyadmin but then you'd need a web server as well. Fortunately there are ""all in one"" packages you can download which give you everything you need (ampps is one example). For Microsoft SQL Server (MSSQL) you can download the developer edtion for free, and you'll need SSMS (SQL Server Management Studio) (also free) as your front-end where you'd type in commands. I'm not familiar with other database systems to tell you how to get started with them, though.",1.0
g1yf9nv,ibm5yf,"Understanding and interpreting the execution plan is the key. ""Use the Index"" mainly focuses on the part how to identify indexing problems in the plan. But the Postgres plan can reveal much more, e.g. wrong estimates, undersized memory settings, slow I/O. I **always** start with running `explain (analyze, buffers)`, hardly ever with a ""plain"" explain (without `analyze`). 

Most of the performance stuff you learn by doing, e.g. by re-writing a statement into an equivalent, but structurally different query. Sadly there is no real ""if you do this, then you'll get a faster query"" recipe. Many of the stuff that can be put into hard rules like that, is already coded into the optimizer. 

""Use The Index"" will give you the understanding on how the retrieval part works, you will need to apply the understanding of the inner workings to e.g. identify why an expression doesn't use an index. Or detect that the optimizer gets some estimates wrong and maybe a different approach to the query might help.

I very often see things that can be improved drastically by combining multiple queries into one. I recently stumbled over [this question](https://dba.stackexchange.com/questions/273699) where a series of one INSERT and 6 UPDATE statements could be turned into a single UPDATE statement. Temp tables are not a panacea to improve performance in Postgres (and are never needed to avoid locking problems, like e.g. in SQL Server)

Sometimes features like conditional aggregation can help to minimize the query and thus improve performance, as e.g. in [this question](https://dba.stackexchange.com/questions/273057) or [in this question](https://dba.stackexchange.com/questions/271756). 

Another example would be the calculation of a ""running total"" by using a co-related subquery, that could be calculated using window functions much more efficiently. 

I don't want to discourage you, but learning to be proficient in SQL tuning isn't something you can learn in a couple of weeks or even months. 

Some general hints would be:

* avoid `NOT IN` conditions. The equivalent `NOT EXISTS` is usually faster
* try to avoid co-related subqueries
* `OR` conditions are notoriously hard to optimize for relational databases. Sometimes re-writing multiple OR conditions into a sequence of UNION ALL queries makes things substantially faster
* `count(1)` is actually slower than `count(*)` in Postgres (and does not make [any difference](https://blog.jooq.org/2019/09/19/whats-faster-count-or-count1/) in other DBMS)

When you research things in the internet be careful about tuning recommendations you find for _other_ database products (especially MySQL). When it comes to that level, different database products behave very differently.",7.0
g1zfocg,ibm5yf,"I have found that with the right logic you can reverse the NOT IN to an IN in many cases. Usually around lists of values. I use a CTE against the list of values table to get the IN values by excluding the NOT IN values and then use that CTE in the final query. As you said avoid NOT IN if at all possible.

One thing many people do not realize is by default most databases assume that when you query something you may update records through the query and subsequently lock the records. There is usually a statement you can add to the query usually at the end that tells the database that this is a READ ONLY query. I've seen amazing performance gains just from that alone. In SQL Server add WITH NOLOCK to the table and on DB2 add FOR FETCH ONLY at the end of the statement. Not familiar with PostgresSQL, maybe someone else knows.",1.0
g1ziicm,ibm5yf,"&gt; by default most databases assume that when you query something you may update records through the query and subsequently lock the records. 

Postgres does not do this. Readers never block writers and writers never block readers. The ""nolock"" workaround is not necessary in Postgres to avoid SELECT statements acquiring any locks.  Oracle and MySQL with InnoDB behave in the same way.",1.0
g20bb1j,ibm5yf,Cool. Good to know. So is PostgresSQL two phase commit like Oracle or single commit?,1.0
g20ghmr,ibm5yf,"I don't understand. Two phase commit is only used for distributed transactions (e.g. between different types of databases) and has nothing to do with ""local"" locking of rows.",1.0
g23fib6,ibm5yf,"I am digressing. I was just curious about PostgresSQL. I work in DB2, SQL Server, and Oracle. By default Oracle is two phase meaning you have to commit every transaction whether it is one table and query or a group of queries. Nothing is stored until you explicitly commit the transaction. DB2 and SS are single phase meaning when you update etc it is done unless you turn it off.",1.0
g23ipjw,ibm5yf,"The correct term is ""autocommit"" not ""two phase commit"".

Postgres defaults to autocommit on (similar to SQL Server). But even if you turn off autocommit and use explicit transactions, readers never block writers and writers never block readers.",1.0
g21aviq,ibm5yf,"&gt;OR conditions are notoriously hard to optimize for relational databases. Sometimes re-writing multiple OR conditions into a sequence of UNION ALL queries makes things substantially faster

Very interesting, I'm currently having somewhat low performance and have been having a hard time figuring out why. The query uses multiple ""WHERE x IN (..)"" OR ""WHERE X IN ()"" .. etc OR's, with nested subselects, that have OR's in them.. might be the reason.",1.0
g24q328,ibm5yf,"Thank you so much, very invaluable. I will keep these in mind",1.0
g1yktmv,ibm5yf,"This guy knows his stuff.

Also, the mailing lists are invaluable and people are incredibly helpful.

I like to check that I have not modified the results of a query when doing optimization work with the EXCEPT keyword.

Old query except new query;
New query except old query;

If zero results for both of those, and the dataset is non trivial, it gives me some significant confidence that I have avoided unintended consequences from the query refactoring.",1.0
g1xyyiq,ibm5yf,"Just because you can't create or modify existing indexes doesn't mean that you can't benefit from index knowledge.  I'm sure Use the Index, Luke will help you.

If my employer asked me to hunt down inefficiencies in performance beyond indexes, I would look for two things:

1) Queries that try to do a lot of computation.  I believe this is particularly a problem in the BI world.  SQL is not a great calculator, it's a good retriever. 

2) Over reliance on Views.  Views can be great, but they are difficult to optimize.  This is particularly true if views are joined to multiple tables.  Look at the view's underlying query and see if the columns they're pulling in these queries really need to come from these views, i.e. is there some sorting or filtering that would be hard to reproduce.",6.0
g24qv17,ibm5yf,"Our work load usually comes from joining so many tables and retrieving hundreds of thousands of data rows, not many calculations are involved but I will be on the lookout for those. Maybe expensive calculations can be off-loaded to the backend. Thank you",1.0
g1woov6,ibm5yf,Remindme! in 3 days,3.0
g1xcizb,ibm5yf,"I will be messaging you in 3 days on [**2020-08-20 20:42:36 UTC**](http://www.wolframalpha.com/input/?i=2020-08-20%2020:42:36%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/SQL/comments/ibm5yf/what_are_some_good_learning_materials_for_query/g1woov6/?context=3)

[**2 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FSQL%2Fcomments%2Fibm5yf%2Fwhat_are_some_good_learning_materials_for_query%2Fg1woov6%2F%5D%0A%0ARemindMe%21%202020-08-20%2020%3A42%3A36%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20ibm5yf)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",2.0
g1xt38x,ibm5yf,"This isn't a direct answer to your question, but do you have the ability to grab data from the unoptimized databases and store them in a separate, optimized database that you have admin access to? you could have the long-running queries running as an automated task on a set  cadence and stored in an optimized, keyed/index db which the BI tool would hit. this obviously only works if you are reading data only and don't need it 100% realtime",2.0
g24r2b4,ibm5yf,I am afraid not. It would be a longshot in my situation :/,1.0
g1y7awa,ibm5yf,"Not related to your question sorry but what else do you have to use in your work? Like R language, C++, python, Excel?? Or is it just SQL only?",2.0
g24risj,ibm5yf,"For now only SQL and Excel. Rarely Python for things that can be automated and Javascript for simple web tools that can speed up my work process. 

I am not a db or data analysis person, this is a temporary thing and I will be slowly transitioning to dev roles where I will be using React, NodeJS, and occasionally Java.",1.0
g218dxn,ibm5yf,"Just a small add, I recommend  [https://explain.depesz.com/](https://explain.depesz.com/)

It's a site where you paste the results of your EXPLAIN ANALYZE {QUERY}, and it tries to highlight the problems in an easier/faster-to-understand way. It also aggregates some stats ( you can check a few in [https://explain.depesz.com/history](https://explain.depesz.com/history) )

Just my 2 cents, this site really helped me improve/understand things.",2.0
g24runt,ibm5yf,I have seen this site so many times but haven't tried it yet. I usually use DataGrip's Explain Analyse view. Does depesz provide more insight?,1.0
g1xw0z0,ibm5yf,"Break the queries down and leverage #tables, then create indexes on those #tables and replace sections of the code.

Try to look for places where you are doing unnecessary selects, or you have lots of complicated joins and then segment the process to take what you need, put it in a #table, then join to the rest of the stuff and keep going.

Just play with it and come up with different approaches until you find one that runs faster. Just bash that shit with a hammer.",2.0
g1yczjb,ibm5yf,"There is no such thing as `#tables` in Postgres. It's an invalid identifier in standard SQL and Postgres. In case you are referring to SQL Server's temp tables - temp tables are created differently in Postgres. 

But more importantly, more often than not, introducing temp tables to store intermediate results makes things slower, not faster.",2.0
g1z3god,ibm5yf,Yeah people say that about MS too but that isn't always the case.,1.0
g1wrz9t,ibm5yf,What DBMS are you using?,1.0
g1ws7o9,ibm5yf,"PostgreSQL. I flaired the post but yes, not very visible at first glance, I should have stated that",1.0
g1wxiwu,ibm5yf,D'oh! I missed that. My bad.,1.0
g1zfw21,ibm5yf,"You can find many free books and courses about writing effecient queries and query optimization [here](https://www.eversql.com/best-and-fastest-way-to-learn-sql/).

More information about how to index your database to optimize queries can be found [here](https://www.eversql.com/choosing-the-best-indexes-for-mysql-query-optimization/).

Also, you can learn from the automatic query optimization recommenadtions provided by EverSQL AI based query optimizer [here](https://www.eversql.com).

* In case you have a student account, you can get a free license from the GitHub education pack.",1.0
g1zhf5i,ibm5yf,"Just use EverSQL.com , it will optimize it for you automatically. It is as if the best DBA in the world is sitting next to you.",0.0
g1wjx47,ibl8pt,"With a Nested Loop, the outer input is executed only once while the inner input is executed once for each record returned by the outer input. When the algorithm decides a method of fetching rows for the inner input, ideally it will do so as an index seek operation, rather than a scan of the entire index or table. Seeks are significantly cheaper than scans, effort-wise.

The same holds true for the outer input. A seek is generally quicker than a scan. The algorithm mostly selects the table with the fewer rows as the outer and the table with the larger set as the inner to minimize the total number of loops. However, if the algorithm calculates that the absense of an index makes the query too expensive, it could reverse their order, or choose an entirely different method that doesn't involve loops.",7.0
g1x20m6,ibl8pt,Woah this totally makes sense. Thanks!,1.0
g1xwfnf,ibl8pt,"This might sound stupid, but say on the outside you have a clustered index such as ID, Date, and on the inside you have something such as Date, ID. Is that going to matter?",1.0
g203pg7,ibl8pt,"Thanks for the detailed explanation man. I just have one follow up question. If the optimiser opts to go with nested-loop algorithm, does it necessarily mean indexes on only one of the two tables will be used?",1.0
g1wff2k,ibl8pt,"the old cookbook analogy? you need to find the lime-mango seared tuna recipe, you'll need to look through 50% of the book, on average. You go to the index, look through 5 pages (even if you are bad at alphabet or if the recipe is listed under 'fruit-based sauces') and go to the recipe page directly.

so in your loop, instead of matching the current value (or another comparison) to a million of records of the other table, you just need to look up 10k or so.",1.0
g1wc7n7,ibkwce,"you'd only need it for a handful table/object/udf references though, I hope. Just switch to a different db and all try running the statement. unresolved references will be in the list of errors and you simply can click each one.

doing a find-and-replace in an editor like notepad++ (looking for ""from &lt;whitespace&gt;"" and ""join &lt;whitespace&gt;"") should work too.",1.0
g1x4hjk,ibkwce,"ALso check out the Redgate tools - SQL Prompt will be able to fully qualify and format your code. There's a trial download which would fix this in a few minutes, and then you can decide if it's worth purchasing. Couldn't live without myself.",1.0
g1w84it,ibjb6o,"&gt; Oracle screams at me saying that Im missing right parenthesis.

sorry, my Windows© CrystalBall® app isn't working just now, would you mind posting the query with the missing parenthesis",1.0
g1wa13q,ibjb6o,"dude is just looking for someone to write for him ""having sum(invoice_value) &gt; x"" for the query you already have written.",1.0
g1vfazz,ibg7sh,"what you want is something called a **pivot** query

not sure if Oracle has that syntax, so you can use 12 CASE expressions

    SELECT customer_id
         , SUM(CASE WHEN MONTH(date) = 1
                    THEN invoice_value
                    ELSE NULL END ) AS Jan2007_spent
         , SUM(CASE WHEN MONTH(date) = 2
                    THEN invoice_value
                    ELSE NULL END ) AS Feb2007_spent
         , ...
      FROM invoices
     WHERE YEAR(date) = 2007
    GROUP
        BY customer_id",1.0
g1w0trq,ibg7sh,Thank you a lot! I applied a pivot. Now I'm just having trouble with a conditional because I just wanna display values over X on that pivot,1.0
g1vkqfo,ibejrw,Anything except COBOL :P,15.0
g1w4544,ibejrw,"At 24, if they stick with it they'll be the last COBOL dev alive in a couple decades.

Edit: Since I don't know OP, changed he to they to be woke.",16.0
g1w4b0t,ibejrw,And will be able to basically write his own paycheck!,5.0
g1wka0l,ibejrw,Not worth the psychiatry bills to be honest. I have done some COBOL and wouldn't wish it on my worst enemy.,9.0
g1xtfaa,ibejrw,"FWIW, I started with COBOL and RPG, and never had an issue with them. *But* as soon as I started working with SQL I just fell in love. I have some C#/++/etc, experience, some Java experience, etc., but it just isn't something that particularly interests me. I was very good at those languages, but it just wasn't something I enjoyed doing with most of my time. What I do enjoy doing is working with those kind of developers as it relates to the data, and I think having experience/knowledge on those languages really fills a specific niche in the SQL world that is hard to find.",3.0
g1xtxwo,ibejrw,My experience was actually with DIBOL (DECs version of COBOL sortof). We had 1 million lines of code. We changed the GL number from 15 to 25 digits. 200k lines of code were affected.,2.0
g1xujbw,ibejrw,"&gt; DIBOL 

Never used it, but I spent a fair amount of time fucking around with FORTRAN.

And I never said COBOL was elegant. But it works.",3.0
g1x3588,ibejrw,"You realize most COBOL positions pay more than most SQL positions, right?",3.0
g1ybg6n,ibejrw,"You realise that sellings one's body pays more than most SQL positions, right?",2.0
g1z3ig4,ibejrw,"You haven't been to the third world, or Vegas, I take it.",1.0
g1vooc8,ibejrw,"I started out as a Data Analyst a while back and from there went DBA, to Data Architect to Data Engineer/Cloud Architect.     If you want to go the DBA-ish route you will be focussing more backend systems that prioritize code, optimizations, infrastructure management, scaling.  You may use SQL a lot in these roles but you will also need a lot of programming and scripting chops.  Understanding how queries are optimized, access patterns for data, and resource management are important. 

If you're going to focus on Data Analyst you will do a lot more SQL.   Analysis and massaging of data in SQL is a big component.  The other part is how you present the data and findings to people.  Leaning certain tools like Tableau, Looker, and other visualization platforms will be a big part of how you deliver your findings.",12.0
g1vpia1,ibejrw,"Some friends have said to me to focus on learning python for visualization of data, for a DBA do you think that would be a good effort, or should I direction my time somewhere else?",2.0
g1vrs3s,ibejrw,I think learning python is good for going the DBA route but I have found the process of building nice compelling visualizations is very time-consuming.  Visualization is part art and part storytelling in addition to getting the data prepped.  You could whip up some graphs with your python visualization knowledge as a DBA but I have found that DBA's have many other pressing concerns making them less effective as the person who writes great visualizations.  I've seen DBA/Data Engineers end up using python to handle more back end process automation instead of visualization and creating UI's for things.,5.0
g1xelcf,ibejrw,"Check with your employer and see if they'll pay (or help pay) for certifications or trainings. Microsoft has some good resources for database administration and they have different certifications you can work toward in different aspects of SQL Server (t-sql, database administration, etc) 

I've seen others mention hardware and that's a good call. Even as a database administrator, you're going to have to know at least basics in networking, server hardware, powershell, SQL integration and reporting services. In my experience, a DBA only administering the database is pretty rare.",3.0
g1ykya7,ibejrw,"Coming from only deving, I don't really know what to expect, I have a friend that works in Microsoft, maybe he can give me a hand. Also working at IBM maybe DB2 knowledge will be able to ask for. Idk",1.0
g1x333o,ibejrw,"If you want to be a DBA this won't be the best advice, but if you want to go into analytics or data science I would highly recommend it as a former COBOL guy.

https://www.reddit.com/r/SQL/comments/i05qpr/unemployed_looking_to_take_up_sql_recommendations/fzn9s48/",2.0
g1xh2rk,ibejrw,COBOL? You should be working on the Unemployment Benefit system right now as I hear they need COBOL devs,2.0
g1xtzo6,ibejrw,"I don't know how true it is now, but when I started working around Y2K something like 80% or more of the world's code was all in COBOL. It's everywhere. You'd be shocked how many modern companies still leverage it in some capacity or another, and the fact is that it's just too much of a hassle to replace.

The problem with COBOL isn't that it doesn't work, or that it needs replacing... the problem is finding COBOL developers.

If you know COBOL you can find a job anywhere overnight. There is simply no competition. And this is highly unlikely to change in my opinion before I retire, and I'm 38.

If you're young and know COBOL you can go anywhere, write your own ticket. It definitively shows that you are a properly educated programmer and that you can basically learn anything, and anything you add on top of it will simply make your resume become even more impressive to many companies who do have *some* COBOL needs, but predominantly are looking for someone who can do something else.

Basically any IT department that has an AS400 is going to call you if you have COBOL experience on your resume, even if the job you're applying for doesn't mention it. It is an immediate eyeopener that will get your resume pushed to the top of a list.",3.0
g1yl2tl,ibejrw,"Yes! People always crap on Cobol, but I'm really glad to have it under my belt, don't regret it one bit, but still am looking for a change",1.0
g1z3ebc,ibejrw,"SQL is a great career with lots of niches for you of you understand COBOL. Same game, different name.",2.0
g1vhj07,ibd0la,Great video!!,3.0
g1voezy,ibd0la,Thanks! I appreciate that!,3.0
g1vsuls,ibd0la,Love your scripts. Thanks for the video!,2.0
g1wyiyb,ibd0la,Thank you! I'm glad the scripts are useful.,2.0
g1z1oj4,ibd0la,Great videos. One question: You use a lot of nolock and recompile hints. Why do you do this?,2.0
g1z7ihw,ibd0la,I only do that to reduce the possibility of having any impact on the system you are running the queries against.,2.0
g1uwzsq,ibbrd8,Don't tell my boss.,12.0
g1xvj1c,ibbrd8,How would I find all the pre-filled tables?,2.0
g1y17uf,ibbrd8,"Just use this
```
SELECT name FROM sqlite_master WHERE type='table'
```",1.0
g1zj3by,ibbrd8,Thanks I just decided upon getting laid off to start studying SQL again after years have gone by and my only knowledge of the language was a college course I've long forgotten.,2.0
g1zqfvf,ibbrd8,Glad to hear,1.0
g1ugt4m,ibbpub,&gt;SQL Trace and SQL Server Profiler are deprecated. The Microsoft.SqlServer.Management.Trace namespace that contains the Microsoft SQL Server Trace and Replay objects are also deprecated.,8.0
g1u6lu4,ibbpub,"Profiler is deprecated, everyone should move to using Extended Events instead.

Here's some stuff to get you started:

https://www.sqlskills.com/blogs/jonathan/category/extended-events/",7.0
g1ugegw,ibbpub,"&gt;some


&gt;at least 12 blog posts per page for at least 5 pages

Uhuh... im gonna need glasses after this.",2.0
g1umldo,ibbpub,And that’s not even all the posts on XE from SQLSkills folks!,1.0
g1u8i2a,ibbpub,I use it a lot and I love it,1.0
g1uma1x,ibbpub,"You need to learn Extended Events. Profiler is deprecated and Microsoft isn’t putting any work into it anymore. XE exposes information that Profiler doesn’t now, and puts far less load on your server.",2.0
g1vfkwx,ibbpub,This is a quickstart document from Microsoft for Extended Events. [https://docs.microsoft.com/en-us/sql/relational-databases/extended-events/quick-start-extended-events-in-sql-server?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/relational-databases/extended-events/quick-start-extended-events-in-sql-server?view=sql-server-ver15),1.0
g1wjuro,ibbpub,Dave Bland had a great SQL Saturday lecture on extended events!,1.0
g1uylvq,ibbpub,Great post,0.0
g1u02au,ibbnat,"You can use the `bool_or()` aggregate function:

```sql
select id, bool_or(color in ('Black', 'Gray')) as is_black_or_gray
from the_table
group by id
order by id;
```

This returns a `boolean` column with true/false. If you want `'Y'` instead (I prefer `boolean` for these kind of columns) you can convert that using a CASE expression, e.g. 

```sql
case bool_or(color in ('Black', 'Gray')) 
  when true then 'Y'
  else 'N'
end as is_black_or_gray
```",2.0
g1w65cz,ibbnat,This is great! May I ask why you prefer Boolean? For efficiency?,1.0
g1w7oab,ibbnat,"Use the right data type for the right thing. 

Numbers (integer, numeric) for numbers, dates for dates, timestamps for date/time values and boolean for yes/no values.",2.0
g1u0piw,ibbnat,"    SELECT id
         , CASE WHEN COUNT(CASE WHEN color IN ('Black','Gray')
                                THEN 'ok' ELSE NULL END)
                   &gt; 0
                THEN 'Y'
                ELSE 'N' END AS is_black_or_gray
      FROM yourtable
    GROUP
        BY id",1.0
g1w6d3m,ibbnat,'Select count(null)' gives 0. I never really thought about it before but TIL. Thank you!,1.0
g1tuqgc,ibb465,Have you looked at the output? Are you sure each tables’ granularity is what you expect and the query isn’t severely duplicating them? Also how big are these tables?,3.0
g1twz04,ibb465,"Have you created appropriate indexes so that the query can execute without reading your tables in their entirety. Always put yourself in the position of your DB, how would you go about executing it? Check what plan it has come up with the EXPLAIN statement",2.0
g1twfio,ibb465,"How is it if you do a sub query instead of a join? This may help the optimiser know the ordering. 

Also is your select only retrieving the necessary data? Lowering fields and row counts can speed up the query",1.0
g1ucan7,ib9tkf,"A surrogate key is just the row ID and is not related to any data, just the id of the row when it was loaded into the table. for example if you loaded a set of data into the table and a row had an surrogate key ID of 15, you could reload the same data into the table and the row that had the id 15 would look completely different.   


You'll never be able to rely on the surrogate key in any production environment to look up a single row data IF the data is always changing... and it should be. If you need a reference look up then the natural key is your only static option IMO.",3.0
g1ukyb9,ib9tkf,"Thank you for your reply.

How would each table know which surrogate key to assign to the corresponding values?

There is client ABC with CustomerId 170 in table DimCustomer

How does the CustomerId get assigned to the correct customer in the fact order table?

Is this something happening already in the stored procedure? (I cannot access them to review to code sadly)",1.0
g1vj2vt,ib9tkf,"The dimension tables are populated (or updated) first, and any new records are allocated their own surrogate key. When the fact tables are populated, a search is done on the values in the dimension tables, and the relevant surrogate key is returned, which is then stored in the fact table. So in your example you'd search DimCustomer for client ABC and it would return the ID of 170. When doing the lookup make sure you're looking at the most recent ID for the data.",2.0
g1uorfu,ib9tkf,"I think you're focusing on the surrogate keys too much, what i'm trying to say is surrogate keys can be reused in the database, so relying purely on that will create problems for you when writing scripts based on that key.

What you should be more interested in is the factKey and the dimKey, essentially a primary key and foreign key scenario usually these are named accordingly i.e: Dim\_CustomerID and Fact\_CustomerID, further shortened to D\_and F\_

A fact table will be made up of multiple dimension tables, and it will be up to the Data Developer to determine the customerID in each of the dim tables and link them together in the table schema using standard PK/FK scenario. You can see this in SSMS by just right clicking the table and selecting design.

Often a lot of this is already written in an SSIS package. Have you got access to SSIS? How long have you been using SQL, cause i could potential just overwhelm you with jargon

Take a look at this free UDemy course:  [https://www.udemy.com/course/data-warehouse-for-absolute-beginners/](https://www.udemy.com/course/data-warehouse-for-absolute-beginners/)   


It'll give you the basics and help you understand a little bit more.",1.0
g1utfh0,ib9tkf,"I have been using SQL for about half a year now. Mostly writing queries, creating views etc..

As for now we don't use SSIS I think, atleast not as far as I am concerned.

We are using the data management in Dynamics 365 to export the entities to our data warehouse on Microsoft Azure.

I will look into SSIS. And the Udemy course.

Thank you very much for your thorough explanation",1.0
g1unfdw,ib9tkf,"One thing to add to /u/WhiteWulph's comment is that while an insertion-time row id is one way of implementing surrogate keys, there are [other ways to ensure key consistency across environments.](https://github.com/fishtown-analytics/dbt-utils/blob/dev/0.6.0/macros/sql/surrogate_key.sql) This is important when you want to be able to address the same record across both prod and test servers, for example.

As to how you actually generate surrogate keys, I really recommend *The Data Warehouse ETL Toolkit*; it does a great job of giving you a high-level implementation.

The birds-eye view is that when you're doing your batch load from source, you perform a lookup of your individual fields against the corresponding natural / business keys in the appropriate dimension tables. If those records match, when you build / append the fact tables, you'll just substitute the natural key for the surrogate key. If there is no match, you go through a process of generating a record in the dimension tables, including assigning a surrogate key.

As an example, you may bring in lead data from your CRM, so your ETL will compare the AccountID field in your source to the AccountID field in the dimAccount dimension table. For the records where there is a match, later in the process when you append records to the factLead table, AccountID will simply be replaced with AccountSurrogateKey. For the records that don't match, a new record is created in the dimAccount table, including generation of the AccountSurrogateKey and all appropriate fields (AccountName, AccountAddress, etc.).",3.0
g1uxkxp,ib9tkf,Thank you for your clear explanation.,1.0
g1uk15l,ib9tkf,"In one of the DW I worked on, they created a surrogate key in the DW to the natural key in the OLTP system. Then further surrogate keys were created in the slowly changing dimension as the entity changed over time.

Here's the sample:

    -----------------------------------------
    |CustomerID|CustomerKey|OLTP_Natural_Key|
    -----------------------------------------
    |100001    |100001     |53              |
    |100001    |100003     |53              |
    |100001    |100004     |53              |
    |100002    |100002     |78              |

And indexes were optimized to search on customerid in a lot of cases.

Most DW's I work with have something like this instead:

    ------------------------------
    |CustomerKey|OLTP_Natural_Key|
    ------------------------------
    |100001     |53              |
    |100003     |53              |
    |100004     |53              |
    |100002     |78              |",1.0
g1ulhwy,ib9tkf,"So the surrogate keys are generated by the ERP already? Not after the ETL/ELT has happened to the DW already?

Also I have rephrased my question in the reply above.

`Thank you for your reply.`

`How would each table know which surrogate key to assign to the corresponding values?`

`There is client ABC with CustomerId 170 in table DimCustomer`

`How does the CustomerId get assigned to the correct customer in the fact order table?`

`Is this something happening already in the stored procedure? (I cannot access them to review to code sadly)`

&amp;#x200B;

Hopefully this is clear.

&amp;#x200B;

Thank you very much.",1.0
g1umq6o,ib9tkf,It's created automatically. And we could have used the OLTP_Natural_Key column interchangeably with the CustomerId. But it was handy for when we started bringing in data from multiple systems when we had multiple OLTP natural keys that corresponded to each customer. Not all of whom had a value in each key. The Id field gave us a surrogate key to work with that carried no business meaning and allowed grouping without needing to make a complicated join on the multiple natural keys that it is attached to.,1.0
g1uw1fz,ib9tkf,"That is a very clear explanation.

Thank you very much.",1.0
g1uwwpy,ib9tkf,No problem. And good luck with your ERP implementation.,1.0
g1yod1j,ib9tkf,Thank you very much,1.0
g1tgip1,ib9t3t,"&gt; Does it annoy anyone else how .sql is the ""standard"" filename extension for all SQL scripts?

No, it does not annoy me at all. 

Your IDE should know which DBMS it is connected to, and then do proper syntax highlighting based on that. 

But no one keeps you from choosing a different extensions.",2.0
g1u17v2,ib9t3t,instead of `some_file_1.sql` and `some_file_2.sql` why not get your editors to create `some_file_1_mysql.sql` and `some_file_2_mssql.sql`,2.0
g1uuydb,ib9t3t,"Yeah I have done a bit of stuff like `.my.sql` ... although if there's still a final `.sql` most programs ignore the rest anyway.

There's also the option to just do `.mysql` and manually add them as custom extensions to editors etc, assuming they let you.

Just more of a rant about it not being the norm really, heh :)",1.0
g1uu21r,ib9t3t,"This is actually a pretty valid point.

You could use those extensions if you'd like. All you have to do is tell windows which extension goes with which IDE.

The downside is anybody else who uses those scripts would need to do the same setup.",2.0
g1tep2x,ib9t3t,You have too much time on yr hands,4.0
g1vjw40,ib9t3t,"Most people (especially in a commercial environment) will only be using one RDBMS, in every company I've worked for that's been MSSQL so all .sql files are associated with that. I very rarely double-click on a .sql file though, as sometimes I want to open it in SSMS and sometimes in notepad++, so I usually end up dragging the file onto the program I want to use it - in which case the file association within the operating system becomes less important.",1.0
g1tf9bv,ib4e12,SQL Server is telling you the msdb database on your database server (maint-hp\sqlexpress) is corrupt. You might find this article helps - https://www.mssqltips.com/sqlservertip/3658/how-to-fix-a-corrupt-msdb-sql-server-database/,2.0
g1tfdlc,ib4e12,"Are you able to check the errorlog and tell us what it says? If not i posted a link down below from something that has helped in the past.

[https://forums.asp.net/t/1878582.aspx?Database+msdb+cannot+be+opened+It+has+been+marked+SUSPECT+by+recovery](https://forums.asp.net/t/1878582.aspx?Database+msdb+cannot+be+opened+It+has+been+marked+SUSPECT+by+recovery)",1.0
g1t9wit,ib61cp,"I am struggling with multiple JOINS, JOINS in general really and how to use them, for some reason it just doesn't click for me. Its driving me nuts, but i will be sure to check out the channel when i get home.",1.0
g1si8ph,ib0nyv,"Just SQL won't get you many jobs unless you're a stone cold rock star.

But as part of a larger skill set.... you just have to be OK.",17.0
g1sllml,ib0nyv,"&gt; But as part of a larger skill set.... you just have to be OK.

Applies to Data Engineering as well?",6.0
g1swtn4,ib0nyv,"You will want to know as much about databases, SQL, and ETL as you can if you want to be a data engineer.",2.0
g1sx5g5,ib0nyv,"I'm taking a Uni course but they are teaching us Entity-Relationship modeling, EER diagram, Relational Algebra and what not (SQL queries were covered in the first two weeks). Most of the things are quite theoretical in nature and I don't understand it. This has been kinda demoralizing for me. I know SQL queries, is it enough for real world jobs?",2.0
g1sz0ov,ib0nyv,"You're in luck, SQL is easy to learn and there are many resources online to practice. There are far fewer resources for learning the theory, and knowing your way around an ERD is very helpful.

Also the vast majority of queries you're ever going to write are a tiny fraction of the SQL that exists.

I know how DECODE and CONNECT BY work, but I've only ever written like 15 statements with either of them, whereas a simple 

    select x from y  inner join b on y.z = b.a where c is not null

 will be all you need most of the time.",1.0
g1t0hz3,ib0nyv,"Pardon me, do you mean SQL queries is all I need to know and don't need to worry about the theoretical parts? My career track is Data Engineering.",1.0
g1t1vjx,ib0nyv,no he's saying that knowing SQL is a part of the whole package.  learning the theory is harder to get when out of uni while there's tons of resources for learning SQL online.,2.0
g1t3xdt,ib0nyv,What should I do then? I am struggling with the theoretical parts and likely to do very bad in the assessments.,1.0
g1v5x8m,ib0nyv,"While there are fewer resources online there aren't none for this kind of thing. Talk to your prof or start googling.

You really need an intuitive understanding of data modeling to be a data engineer, because in best-case scenario the data architects will tell you what you need to build but you still need to be able to read the blueprints, or worse case scenario all you have is the customer and you have to do it all yourself.",1.0
g1sov9v,ib0nyv,It was a general statement. Don't be obtuse.,-5.0
g1srqxq,ib0nyv,I think they genuinely didn't know.,4.0
g1s1clb,ib0nyv,"Companies will want you to know the basics, how to select, order and filter records, how to group and aggregate results, and how to join tables. They may also want you to know about indexes, what they are, how to create them, how they're used. Maybe also the basics of stored procedures, conditional sratements, and variables. They won't expect you to know everything, but they'll definitely want you to be enthusiastic and able to learn.

I've been a sql developer for years and I still have to google lots of things, what's important to an employer is that you can find and understand answers reasonably quickly.",27.0
g1s2dxz,ib0nyv,"Thank you for the fast response , as a sql developer did they also want you to know python , java , hadoop or anything else or do you strictly work with Sql?",5.0
g1s5a5g,ib0nyv,"Personally I've worked with sql (sql server) and vb.net, but familiarity with other languages wouldn't be a bad thing. As a new employee you'd be asked to learn and eventually tweak existing code rather than write anything from scratch, so if you can find your way around code written in other languages you'll be ok.",9.0
g1sv8cn,ib0nyv,"My degree is Telecommunications, like video work, and I have been in I.T., web dev, and database management for close to 20 years. Do not lose hope. The last listing I wrote for a high entry level to mid-level was degree or experience. I would take pleasant to work with and good work ethic over degrees and certs everyday. 

A recommendation, if you want some quick experience, and can afford the time to do it, there may be volunteer opportunities in you it area. Small non-profits, schools, and activist/political groups are almost always in need of help. It is some experience and a chance to help with a cause you are passionate about. These can be way to make contacts and lead to paid gigs.",7.0
g1t3gbw,ib0nyv,"As a side note, Whatever you do, DO NOT name your production server close to the name of the testing server. Ya know, incase you want to pull a github and drop the wrong database ;)",2.0
g1sjqho,ib0nyv,Skill &gt; degree in almost all fields,6.0
g1sqfft,ib0nyv,"I think yes you may get a job. And regarding testing your skills, I'd say test your skills on Leetcode and Stratascratch first. These platforms will also give you SQL problems and interview questions to practice.",3.0
g1thzxl,ib0nyv,"Migrations are one of the fields where you just use SQL. But not sure how many companies are actually hiring for it, I ended up doing that accidentally",3.0
g1ubzqe,ib0nyv,Do you mean migration to Azure? I'm currently in a situation of having to migrate from DTS to SSIS because the previous guy did absolutely nothing to keep databases and systems up to date or even best practices (I won't even go into the lack of licensing on some systems). Oddly he took a secondment to a project lead role. I don't care where he is so long as he never gets given admin accounts again!,1.0
g1ujkh8,ib0nyv,"Nope. We have migration from one system to another (banking, if that is important) so my role is basically SQL'ing every column to check if results are what we expect. Sometimes it takes days to think about single query!",2.0
g1urkj0,ib0nyv,Ah ok. Sounds similar to some of the jobs I have to do. Although we have a BI Analyst who can do the basic stuff which helps me a lot. Another thing that my predecessor neglected to have was dev environments. That was fun when we switched from 13 FY periods to 12!,1.0
g1sc7gk,ib0nyv,"you will most certainly be prepared enough skill-wise, but a lot of companies will gatekeep demanding a degree.",4.0
g1tdlrs,ib0nyv,"Of course you can get a job but not as a general SQL programmer. SQL is just one skill. As with any job, you have to have a set of skills to succeed. Look at successful DBAs like Brent Ozar.",1.0
g1untif,ib0nyv,"Database Team Manager here.

Not sure why you’re getting responses saying just SQL won’t get you a job.  That’s literally what the following roles are:

Data Analyst

Database Analyst

Reporting Analyst

SQL Developer

Database Developer

Sure, once you get more skill you’ll be gravitating toward additional skill sets like administration, ETL/Data Warehousing, and the like.  However that doesn’t mean having “just sql” won’t get you anywhere.  If you can show me that you can build a basic select statement and build it into a reporting query, you’re almost always an insta-hire with me.  You’d be surprised how hard that is to find.",1.0
g1s2xpp,ib0nyv,[deleted],-4.0
g1s39jd,ib0nyv,Thank you for the feedback.,3.0
g1sbruj,ib0nyv,"I just want to chime in and say that myself, and several others I know in the field, all obtained 'analyst' roles without having an applicable degree. Many others only obtained an Associate's in a similar field, but a degree is far from necessary. Just stay disciplined, be sure that you have a well-balanced education and you can land an entry-level analyst role very quickly.",2.0
g1sc27o,ib0nyv,"Thank you , I’m currently a welder and fabricator and will be done with my associates in may and want a career change. so that was relieving to hear .",3.0
g1sdw8n,ib0nyv,"I went to uni for Comp Sci and flunked out around 5 semesters in. Took a year off, went to community college for Computer Programming specifically and landed an analyst role before completing that degree. I've had 5 or 6 different titles in the 4 years since, now 'Business Technical Analyst', making solid money doing minimal work.

It's a really great industry and quite easy to break into. You'd be surprised how many office folk are blown away by people capable of doing Pivot Tables and VLOOKUPs in Excel so imagine their reaction when you can query against a legitimate data warehouse, write your own functions, etc.

Good luck! Don't hesitate to shoot me a PM if you ever get stuck, I'm happy to assist where I can.",3.0
g1sergz,ib0nyv,Thank you so much I will definitely send you a message if I have any questions .,3.0
g1sje2v,ib0nyv,Like _Royalty_ I’m also currently in an Analyst role without an applicable degree. I picked it up when a coworker became the only SQL capable person remaining when someone else left the organization.  I offered to learn so I could cover any ad-hoc requests that came in while they were on PTO. Good luck!,2.0
g1sbwcn,iavu4x,"When you want to murder performance, destabilize your caches, and push your storage to is limit.

In all seriousness, if it's a table that is read very frequently but written to very rarely it might be OK if you have a LOT of those reads joining to or otherwise searching that GUID.  Updates are also OK if the data types are all fixed sizes (no varchar or nvarchar).  Of course, frequent read and rare insert implies a smaller table, where the clustering key has little effect and even a heap might not be noticeable to performance.

Using anything that does not grow in only one direction causes excessive page splits.

A split immediately generates several writes and creates unused space in your table (from the splits), increasing io load to read the same amount of data from disk later.

A growing clustered index (like a sequential ID or a creation timestamp) on the other hand will always append data to the end of the most recently allocated page, preventing the issues with page splits on INSERT.",3.0
g1qp41e,iatcuf,"Assuming you've successfully imported the .sql file into MySQL (not Microsoft SQL Server as your post label states, right?), in MySQL Workbench you can right-click a table or tables in the object browser, click Table Data Export Wizard, and it's easy to generate CSVs from there.",1.0
g1rjdtk,iatcuf,"I'm new to this so probably did something wrong.

When I imported the file in workbench it doesn't load at tables but instead the query/code is shown.",1.0
g1ruksn,iatcuf,"You're not looking at the data, you're looking at a query that will return the data from your database. Or the data in its ""dumped"" format which needs to inserted into one or more tables, then queried to produce a CSV file.

It's unclear which you have from your description. Do you have statements starting with `INSERT` or `SELECT`?",2.0
g1s0kwf,iatcuf,"You need to connect the workbench to an actual MySQL server -- the workbench is just a 'client' it really doesn't do anything on it's own.

Oh, and I fixed your post flair for ya.",1.0
g1qpi5o,iatcuf,"If you have azure data studio, you can directly save the result set as an excel or csv",1.0
g1r3z9h,iatcuf,Is this data already in an existing database? If it is there is an EXPORT AS directive in sql,1.0
g1rdog3,iaszqy,"Explicitly create the table with the right columns names and data types you need, and insert into that table.",2.0
g1seg5j,iaszqy,Thank you.,1.0
g1ry5jc,iaszqy,"Welcome to hell. If you don't know the data types of the source system(has happened to me on multiple occasions) the easiest and dirtiest way is to just create a staging table with exceedintly wide columns(think nvarchar(1000), or (max) if it fails on 1000) import the data into the staging table, then use SQL to find the proper data types and lengths and import into your final table. 


I did write a small C# console app that I need to find and share on here. You give it a CSV, and it churns through the entire thing row by row and spits out a data type list. Was able to profile a 2gb CSV in around 30 seconds or so. Much more accurate and much faster than the one Integration Services uses.",2.0
g1sedi1,iaszqy,"Thank you! Good suggestion and good to know, in some weird way, that I am not encountering hell entirely alone.  Again..thank you.",1.0
g1rv4br,iasrq1,"While I question the design, it is 3NF, as much as one table can be. Now, had you also had a field called PrimaryIngredientCategory that stores the category of the primary ingedient(meat, veg,fruit,etc), then no, it wouldn't be 3NF. 

Normalization is not so much concerned with substituting repeatable text fields with an ID and lookup table as it is concerned with dependencies, and removing unnecessary redundancy and preserving data integrity. In your example, the primary ingredient could easily be enforced in the application, or via the database itself with another table with Ingredientname as a natural key. But even if said table doesn't exist, This design does not violate 3NF. 

However, In my above example, storing PrimaryIngredientCategory in recipe would be bad practice, and violate 3NF, as it's dependent not on the recipe but on the primary ingredient. And that would be, 100% of the time, redundant data.


Now, on to the real question we need to be asking...who the heck puts bacon in spaghetti??",7.0
g1ryqdv,iasrq1,I completely agree (especially with the last part),1.0
g1rrwm2,iasrq1,"Per the example given, I think it's ambiguous. The PrimaryIngredient also happens to always be included in the RecipeName; is that a requirement?",2.0
g1rs1pq,iasrq1,Assume the name can be anything regardless of the primary ingredients,2.0
g1qkhal,iasrq1,"yes, it's 3NF, assuming `ID` is the primary key

give me a value of `ID` and i can **unequivocably** tell you the value of each of the other columns in that row

those other three columns are **non-transitively dependent** on the PK

the fact that some people like to pull a text column out of a table, like `Category`, and set it up as a separate table, with a numeric id, and use that numeric id in the `Recipes` table, **does not change the fact that `Recipes` was already in 3NF**",4.0
g1qlgp1,iasrq1,That's exactly what I think but the guy I talked to gave the same answer as u/ijustwannagoonthereg,2.0
g1qmbpa,iasrq1,"I think this is the difference btw common practice and theoretical. I never took a CS class at Uni where I was taught 3NF, I learned by doing and so I don't even use the word ""Transitive"" -- if you are a CS major, go with the transitive answer. If you are going for job interview with a guy who's been coding for 30 years, explain that you can go at the problem either way but be careful waving around the transitive answer - some of those old guys have pretty big egos bc they had to learn COBOL before the internet was a thing.",1.0
g1qtq8h,iasrq1,"some of us old guys learned assembler language before cobol

and then we *embraced* logical data modelling and the promise of relational data integrity provided by primary and foreign keys",2.0
g1r5eun,iasrq1,"yes, but do you believe the first example is in 3NF or do you believe it should be broken out into three tables?",1.0
g1r685w,iasrq1,"yes, it's in 3NF

should it be broken out into 3 tables?

that depends on several factors, but if this recipes thing is going to scale up to a meaningful size, then yes",2.0
g1qvprz,iasrq1,"&gt; I think this is the difference btw common practice and theoretical. I never took a CS class at Uni where I was taught 3NF

Computer science classes don't teach this.",1.0
g1r6i0p,iasrq1,"So in what setting are learners exposed to terms like ""transitive dependency for non-prime attributes""?",1.0
g1r6v31,iasrq1,"Just because computer science classes don't teach 1950s deep SQL doesn't mean 1950s deep SQL isn't computer science.

You have four years in college.  Computer science is a lot more than you can cover in four years, and in college they don't focus on a single subject.

You can't look at your college education and say ""this is theoretical because my college education didn't cover it.""",1.0
g1r9kvd,iasrq1,"**ME:  I never took a CS class to learn about 3NF.**   
CS doesn't teach it.   
**ME:  Okay, so, where do people learn it using terms like ""transitive dependency"" and ""prime attributes""?**   
CS *should* teach it.  
**ME:  Okay, so, where do people learn it?**   
College is short, topic is vast.   
**ME:  Okay, so, where do people learn it?**   
You can't say this is theoretical because college didn't teach it  
**ME:** **Okay, so, where do people learn it?**",1.0
g1r9syr,iasrq1,None of these are what I said,0.0
g1raxah,iasrq1,"Of course they weren't.  For example, you said:

&gt; Computer science classes don't teach this. 

And I changed it to read: 

&gt; CS doesn't teach it. 

It's so different, it's as if I'm trying to frame you for murder.",2.0
g1qlf8n,iasrq1,"So, is it not a feature of 3NF that text values should not be stored repeatedly bc of the increased possibility of introducing errors? Was that just something that was done back in the day bc storage was more costly?",2.0
g1qm9dc,iasrq1,"Storage was more costly, renaming can be more difficult, it can be harder to standardize on spelling, and if you ever need to add a field that depends on the category you will need another table anyways. It can also make it easy to retain categories that have no recipes (i.e., a validation, enumeration or lookup table).

As far as introducing errors, that depends on the application. Just because it's free text doesn't mean the application should submit anything it's given.

It's not more normalized to use another table, but it's often a better design.",5.0
g1qng0s,iasrq1,So the table itself is in 3rd NF ? It's just a best practice to use 3 tables but it does not make it more normalized ?,0.0
g1qs439,iasrq1,"&gt; So the table itself is in 3rd NF ? It's just a best practice to use 3 tables but it does not make it more normalized ?

correct",2.0
g1qokv3,iasrq1,"It depends on your application if it's best practice. Often? Yes. Always? No.

Say that you have a order status field in a web store application that you sell. The values are Open, Completed, Partial, Cancelled, Returned. So you've got 5 *fixed* values. An end user operating the web store can't add or change statuses because each status changes the way the application functions. They might want to set the status to Archived, but you have to modify the application to add that feature. You don't really benefit from a validation table at all here.",1.0
g1qtksa,iasrq1,"&gt;  You don't really benefit from a validation table at all here.

sure you do

the database ensures that even application logic errors do not introduce bad data",2.0
g1r97gt,iasrq1,"Good luck when it's a single application database and that application uses an enum for that value.

Also, while I've seen countless databases applications and all of them have some number of validation tables, only a very small number of them use foreign keys. I've never seen one do something like validate quantities as nonnegative. They're very powerful, but they're also almost entirely academic.",1.0
g1r9vv3,iasrq1,"i'm sorry you've had such unfortunate experiences

non-negative numbers can be achieved with a CHECK constraint

p.s. ENUM is the spawn of the devil",1.0
g1ru8ve,iasrq1,"&gt; non-negative numbers can be achieved with a CHECK constraint

The point I'm making is that *nobody uses that*.

&gt; p.s. ENUM is the spawn of the devil

I'm not talking about PostgreSQL's extension. I'm taking about C#'s System.Enum. I'm talking about a Python's enum standard library. I'm talking about the application having data types which prohibit inserting or updating values outside of the valid values because the application is logically incapable of doing so.",1.0
g1qw4vr,iasrq1,"&gt; So the table itself is in 3rd NF ?

There's no such thing as a table being in third normal form.

Third normal form is about ***the relationship between more than one table***.

A table can only ever be `in third normal form with respect to a different one`.

Put together any table that someone says ""this is in third normal form"" and make sure there's a bunch of columns and a bunch of repeated values, and I'll construct a second table that'll give you 3nf violations.",0.0
g1r7f9t,iasrq1,I'll try again: *So the table* *~~itself~~*  *as is* *~~in~~* *does not violate 3NF?*,0.0
g1r9evt,iasrq1,"Same answer as before.  There is no such thing as a table violating 3NF.  This question is nonsense.

You might as well ask what sound the number seven makes.",-1.0
g1rbbuf,iasrq1,"Oh! I know this one, dickhead. It makes this sound:
ssssss - ehh - vuh - nnnnn",0.0
g1qlsik,iasrq1,"Actually I could not find where the fact that text values should not be stored repeatedly is a normalization condition for 1NF,2NF or 3NF. If you have a reference I'd be glad to read it !",1.0
g1qno6l,iasrq1,"But I think we've highlighted the difference already. 3NF has a university definition that many, many people never learned and database developers have a colloquial understanding of 3NF that is conflated with other db design principles.   


So, know your audience and use the right version (esp if you are prepping for an interview). I was answering it in a way that I would say 98% of db developers and about half of all db professors/educators teach it (again conflated with db design principles). So if you're interview prepping, be aware of both ways of describing 3NF.   


It won't matter if you give the theoretical definition if your interviewer is using the colloquial version.",1.0
g1r5413,iasrq1,"&gt; Actually I could not find where the fact that text values should not be stored repeatedly is a normalization condition for 1NF

... this is the definition of 1nf",1.0
g1r6nsc,iasrq1,Where ?,0.0
g1qmc0q,iasrq1,"I agree with you. While putting ingredients and categories in separate tables may be a better DB design, I don't think it relates to 3NF. Whether you call it ""Chicken"" or ""1"" you can only derive the main ingredient from the PK. Adding another column for IngredientAnimal (Chicken, Pig) on the other hand would violate 3NF because it can be derived directly from primaryingredient.",1.0
g1qohsr,iasrq1,"I understand your point, and have taken to heart the idea that I am conflating 3NF principles with DB design principles. I plan to follow up on that. 

However, I cannot resolve your statement that ""you can only derive the main ingredient from the PK"" This doesn't make sense to me bc I don't see how we could know that. IOW, we don't know what other tables exist (and we can create other tables to provide a mapping from recipe name to primary ingredient). 

Most data loaded into a database is coming from things known on the outside of the db. The way you would know the main ingredients in my recipe table is that I would have a many to many table where the recipe name is matched with the main ingredient. Maybe I am reading your statement wrong.",1.0
g1rykpq,iasrq1,"this is a great answer, I just wasn't ready to understand it at the time you first wrote it.  now I get it and it' was well put.",1.0
g1qth79,iasrq1,"Strictly, you don't actually know what external tables want to reference inwards with.  As such you have no idea what the candidate keys are.

From your comment elsewhere:

&gt; in practice, you can do whatever you like, but please do not post incorrect information, mkay?

I'd like to say the same thing to you.

The strictly correct answer is ""we can't actually tell if this is in third normal form, but it is in the sensible interpretations.""",0.0
g1qtzj8,iasrq1,"&gt; Strictly, you don't actually know what external tables want to reference inwards with.

could you please rephrase this in English",0.0
g1qv5ff,iasrq1,"Honestly, given that you were rude in another thread, I'm tempted to refuse.

&gt; please cite a reference for this, because to me, i think you just made that up

To me, that feels like you calling me a liar.  But for now, I'll keep being nice.

.

&gt; Strictly, you don't actually know what external tables want to reference inwards with.

3NF is 100% about external tables.

What makes the fault isn't that they have repeated data; that's either 1nf or 2nf as a fault.

What makes the fault is that an ambiguous candidate key can be constructed.  3nf is about the ambiguity of candidate keys.

Candidate keys only exist in terms of being referenced by some other table.  

So, you appear to be learning this from and referencing Wikipedia; as such, let's use their example.  (I used it too.)  They use an annual tournament of tennis players.

Their structure is `(winner string, year date, courttype land)`

In their system, the tennis player is faulted on 1nf and named by string, so if you want to make a new table with stuff about the tennis player, you have to go by `(winner, year)`.  That's the 3nf fault.

.

&gt; Strictly, you don't actually know what external tables want to reference inwards with.

The internal table to the normal form claim is the wimbledon annual result.  The external table is the player information table.  The inwards reference here would be `(name, year)`.

Let's invent a story for a website about tennis that fits Wikipedia's example to explain, because this happens when someone comes to the system two years later and adds a new feature.  3NF is a maintenance problem, not a design problem.  Nobody makes this fault up front; it's too stupid.  With a story, you can get a better view.

When the original author wrote this webpage, no 3NF problem existed, because it was just the tournament table.  There was no inwards reference to `(name, year)`, so there was no 3NF problem.

Two years later, I hire an intern to make profile pages for the winners.  The intern doesn't refactor the people out into their own table; instead the intern just tacks on a new table and handles it in the app.  ***That's where the 3NF problem comes from***.

The 3NF problem in the base table is created by the referencing table.  It's made possible by a design fault in the base table, sure, but without the referencing table it doesn't exist.

The reason that viewpoint is important is that ***the correct structure from one 3nf dependent's viewpoint can be different than the correct structure from a different 3nf dependent's viewpoint***, and whereas these things don't generally happen up front, if you have a `B` that's dependent on an `A`, and add a `C` that's ***also*** dependent on an `A`, what was fine with respect to `B` may no longer with respect to `C`, and some fairly serious overhaul, including risky changes to complex queries, can occasionally be the needed result.

People will do terrible things to avoid changing the scary giant query someone who doesn't work here anymore wrote ten years ago without tests.",0.0
g1r8iq3,iasrq1,"dude, this is such a mess, it's hard to even think of the starting point to correct you. Please stop 'inventing' stuff, just learn some.

P.S. WAIT, are you Joe Biden?",-1.0
g1qjw8q,iasrq1,"No, it is not. Primary ingredient should be a table. Category should be a table.

3 tables total.

EDIT: So I answered this thinking this was some kid looking for help on homework and I thought he had some teacher wanting him to split it into 3 tables so I threw him ""3 tables"" as a bone. I figured that was enough for him to stumble forward and finish his assignment 

In actuality this became a fairly sophisticated discussion teasing apart theoretical 3NF and some very common db design best practices that are also pretty much 3NF, but not strictly 3NF. So,  cool question, really.",4.0
g1qo83m,iasrq1,"Whereas these should be tables, they're also not causes to believe that this isn't in third normal form.

You're giving good advice but it's a wrong answer to this question.",4.0
g1qokhc,iasrq1,this is an interesting discussion and I'm learning from it.,2.0
g1qtj3i,iasrq1,"Please consider editing your original post.  A lot of people are learning wrong information from it, and ""I'm not a purist"" is not a good explanation of giving wrong answers.

[Here's an answer to learn from](https://www.reddit.com/r/SQL/comments/iasrq1/is_it_3rd_nf/g1qt84k/)",0.0
g1r6rwo,iasrq1,"I don't think there's consensus. Honestly, I answered it in an oversimplified way bc I thought it was some kid asking for help on homework. To change it would be confusing -  I think we all benefit from dialectical learning.",0.0
g1r76lh,iasrq1,"This isn't a consensus topic.  There's a simple right and wrong.

This is ***math***.",-1.0
g1rahwo,iasrq1,"Yay, math. I am so glad you are not my coworker bc I would smile, say hello and run as fast as I could toward the other end of the building. People like you delude yourself into thinking that you're doing us all a favor by always finding some tiny word that you think should negate everything else we said. You are exhausting and you don't care because you think there's some nobility in being an asshole and calling it math.",2.0
g1qlhie,iasrq1,Why ?,2.0
g1qlz8e,iasrq1,"I was taught that you don't repeat text values because if you introduce an typo then when you try to use Primary Ingredient as a filter  or a dimension table (for example) you might end up with 4 Chickens, 1 Chucken, and 1 Chiccen.

So, I am not a 3NF purist and maybe you were looking for a more theoretical perspective, but in practice, if you are building a transactional database you would put these in three different tables - give categories and primary ingredients their own ids and store those in the Recipes table.",6.0
g1qt8oq,iasrq1,"&gt; I was taught that you don't repeat text values because if you introduce an typo then when you try to use Primary Ingredient as a filter  or a dimension table (for example) you might end up with 4 Chickens, 1 Chucken, and 1 Chiccen.

this is a good point

making a typo is easy, right?

okay, let's say you store ingredients in a separate table, where Chicken has a numeric id of 17

then instead of ""4 Chickens, 1 Chucken, and 1 Chiccen"" you would have 6 rows with foreign key value 17

**unless you make a typo** and enter 18 for one of them and 16 for another!!!

then you'd have 4 chickens, 1 salmon, and 1 rabbit

do you follow this reasoning?

so extracting a text field and replacing it with a numeric id does not in and of iteself do *anything* to prevent errors",5.0
g1qwuy4,iasrq1,"That seems like a fair point, why are people downvoting ?",2.0
g1rvgru,iasrq1,"Hard to argue such a theoretical case, but usually you probably wouldn't enter numerical IDs directly and instead use a (sub)query. Then you have `SELECT id FROM ingredients WHERE name = 'Chicken'` which either returns the correct ID or null, never something wrong.",2.0
g1qs1i4,iasrq1,[deleted],1.0
g1qtavt,iasrq1,"Whoever downvoted this should not have.

`r3pr0b8` is correct.  Giving advice that is incorrect, then saying ""oh well I'm not a purist,"" is holding the student back.",1.0
g1qk24y,iasrq1,"wrong, wrong, wrong

no offence, okay?",-8.0
g1ql7ox,iasrq1,Not offended. But wrong 3 times?,4.0
g1qtgcc,iasrq1,"yes, three times

""No, it is not."" -- yes it is

""Primary ingredient should be a table"" -- not necessarily

""Category should be a table"" -- not necessarily",0.0
g1qmm0a,iasrq1,"If the first row of Bacon also said Snack, would that open up the possibility of  a different answer?",1.0
g1qupbz,iasrq1,"nope

go ahead and make that change

now tell me what category id=3 belongs to

any difficulty doing that?  no?  that's because *category depends on the PK*",1.0
g1rbnow,iasrq1,"Okay - can anyone suggest what values could change in either the Primary Ingredient or Category field that would cause this table to violate 3NF ?

(and no, u/StoneCypher, I dont care that  3NF is about relationships so a table by itself cannot violate 3NF, everyone else knows what I mean by this - and so do you, or you wouldn't be able to tell me I'm not saying it ""correctly"")",1.0
g1rcu7z,iasrq1,If you want my opinion I'm still pretty convinced that this table is compliant with 3NF (and I did not understand StoneCypher's point) haha,2.0
g1rhy3d,iasrq1,"Yes, several people have agreed with you and I am open to understanding that it *is* 3NF, I just need a little more help getting there. 

The illustrations so far have not helped me see it clearly so I'm asking for an illustration of how it could fall out of compliance bc I think that might help me. 

Can you think of a way that the table information might change that would cause it to no longer be 3NF?",1.0
g1rn2nn,iasrq1,"First, I believe that there cannot be a 2 NF violation because the primary key is not a composite one, hence there cannot be any partial dependency. The PK here is simply the recipe (one attribute)

Now for a 3 NF violation to occur we would need a transitive dependency. For instance, assume for a moment that we introduce a new field derived from the **PrimaryIngredient** column like **Vegan** such that if the PrimaryIngredient is any kind of meat, then Vegan is FALSE else TRUE.

So we would have a 5th column called **Vegan** filled with FALSE values in our example

This is a transitive dependency and it is not compliant with 3 NF : Despite the fact that **Vegan** ultimately describes the recipe, it derives its value from the **PrimaryIngredient** column.

\-&gt; In this case we would need to create a new table composed of the PrimaryIngredient and the Vegan field. i.e. we would remove the 5th column and place it to a separate table

\--

This is where I learned everything :

[http://dinesql.blogspot.com/2017/06/database-normalization-1nf-2nf-3nf-bcnf-4nf-5nf-with-samples.html](http://dinesql.blogspot.com/2017/06/database-normalization-1nf-2nf-3nf-bcnf-4nf-5nf-with-samples.html)",3.0
g1rpi6e,iasrq1,"So basically, you need to add a row that says:

ID |  RecipeName          | PrimaryIngredient | Category

1   | Chicken Parmesan | Chicken                   | Side

Like if the Chicken Parmesan came in two sizes. Once you need the Category filed to identify which Chicken Parmesan you have an issue.",1.0
g1rrf9z,iasrq1,"Yes, that is not the case I was describing, but with what you are saying you would have a 2 NF violation I believe :

If RecipeName is not unique, then the natural PK becomes RecipeName x Category (a composite primary key). However, PrimaryIngredient only refers to the RecipeName (partial dependency), that is a 2 NF violation",2.0
g1rsyo8,iasrq1,"I am starting to see the point being made here. The way this Dinesh guy explains things is really very interesting. 

I have been doing db dev work for 20 years and I have never used words like determinant, transitive dependency, etc so this is an interesting way of looking at things. It's intriguing and it does challenge some assumptions I've been rewarded for making for a long time so I am enjoying this shift in perspective. 

Re 2NF, from a gut level I would recognize and say, for example, ""Province is an attribute of town so if you know town, don't store Province as an attribute of Student"" -- but the language Dinesh uses is more...? Is it mathematical kind of? Algebraic almost? 

Anyway - if you simplified his reasoning on 2NF - is he saying something like this: 

If you have a composite key, everything else in the row must be dependent on both pieces of the key - nothing in the row should be dependent on only one part of the key?",1.0
g1rx299,iasrq1,"Yes ! I love how methodical he is in his explanations. It's like once you know the functional dependencies, it's pure logic !

&gt; If you have a composite key, everything else in the row must be dependent on both pieces of the key - nothing in the row should be dependent on only one part of the key? 

Yes exactly, otherwise it belongs to a separate table",2.0
g1rmeko,iasrq1,"You need some sort of dependency  not involving the key. Say if primary ingredient chicken (or other meat) implies entree, sugar implies dessert, etc. Then you have id-&gt;primary ingredient and ingredient -&gt; category.",2.0
g1rtn40,iasrq1,"okay - this is starting to be more clear. I think I get your point. 

What if we have a weird (but workable) attribute like ""Protein color"" and for Chicken it will always be brown and for bacon dishes it will always be dark red. Would that be be the issue bc you have that attribute dependent on the Primary Ingredient instead of the Recipe?

(I know that this is a weird example, but I'm trying to stick with the og rows in the table)",1.0
g1rucaf,iasrq1,"yes, it will not be 3nf conforming

ps. also, there IS a way for the original table to be not in 3NF.

Notice how you provided _dependencies_ in your example? OP did NOT. it's kind of pointless to discuss 3NF if the dependencies are not known and only data is presented.

A set(**a**,b,c) {(1,1,1), (2,2,2)} could be 3NF. Or might not be.",2.0
g1rv3vn,iasrq1,"thank you thank you thank you thank you!!! I get it! 

It's really cool to learn something nuanced about something I thought I knew pretty well! Thank you and OP for your patience!! this was awesome!",2.0
g1rwo5f,iasrq1,I did not provide the dependencies because they were not provided in the original problem I was facing but I think it's pretty safe to assume that there is a one to many relationship between recipe and primary ingredient as well as recipe and category. Further it's pretty safe to assume that there is a many to many relationship between primary ingredient and category,1.0
g1rydkp,iasrq1,"btw, you keep saying 'recipe'. you don't have an attribute like that though. You do have an id and a name. A very common mistake could be to say (and define your tables that way) is your only key is ID (so, allowing multiple records with the same name) yet have other attributes (say, primary ingredient) be dependent on the recipeName. there goes your 3nf",1.0
g1rzje1,iasrq1,"I guess technical primary keys are irrelevant when we discuss normalization, only natural keys matter. Since it's a Recipe table, you can assume 1 row = 1 recipe. It would be weird to imagine two recipes with the same name but different attributes. 

This is actually an exam question from the MTA 98-364 with no further information so I guess we have to make some assumptions to answer",1.0
g1s17q6,iasrq1,"... 3nf does not care what way you obtain data, only whether certain restrictions are met, so your guess would be wrong.",1.0
_,iasrq1,,
g1qt84k,iasrq1,"There isn't actually enough information to tell whether there's a third normal form violation here, but it's safe to say that on common sense there probably isn't.

Third normal form is when some other table wants to look something up in your table, guaranteeing that the only lookup they'll need is the primary key.

The reasoning is that if they need more than just your primary key - if they have to slap together their own `candidate key` - then what they slap together may not be constrained in a way that is unique to the outside, and that in turn can be the source of problems.

Let's look at the standard example from Codd's original paper - sports players.  I don't sports, so I'm going to lie about the data.

As such, let's search the multiverse and find Tennis Rick's reality, and look who's been winning Wimbledon

    +-----------------+----------+-----------------+
    | Winner          | Win year | Opponent        |
    +-----------------+----------+-----------------+
    | Rick Sanchez    | 2019     | Serena Williams |
    | Serena Williams | 2018     | Rick Sanchez    |
    | Rick Sanchez    | 2017     | Serena Williams |
    | Serena Williams | 2016     | Rick Sanchez    |
    | Beth Smith      | 2015     | Serena Williams |
    | Serena Williams | 2014     | Rick Sanchez    |
    | Rick Sanchez    | 2013     | Serena Williams |
    | Rick Sanchez    | 2012     | Serena Williams |
    | Serena Williams | 2011     | Rick Sanchez    |
    | Rick Sanchez    | 2010     | Serena Williams |
    +-----------------+----------+-----------------+

Predictably, it's Tennis Rick and Serena Williams pretty much holding the sport down, except for the one Beth episode where Rick gave his daughter the cheating shoes and then tried to commit suicide and was just barely too drunk to succeed.

Now, if we go to a webserver that has this data and start trying to add a page that describes the winners, the easiest thing to do is to make a third form normalization mistake.

See, the problem here is if you want to reference a person, the only clean way to do it is by the candidate key `(Winner, Year)`, because this database offers no mecahnism to assure that the Serena from 2017 is the same one from 2012 (and actually it isn't, it's her sister in disguise, because Serena was fighting undead President David with the Vindicators.)

So if you make a page, then you have to point at *one* of the Serenas, and if you later make some merch and make a similar reference, ***you might end up pointing at a different Serena***.

Similarly, nothing stops me from making a page for 2018 Serena vs 2016 Serena, and giving her different information in each.

The correct way to handle this is to perform a `third form normalization`, meaning we pull those people out into their own table, and start referencing them by ID instead.  Then, instead of some name, it's Tennis Person 8, and we no longer have the ambiguity.",0.0
g1qrdhf,iasrq1,"So, if Chicken is always Entree and Bacon is always Snack, then it's violating 3NF but since they  are independent, it's okay? Is that right? 

And the preference for separating them into different tables and tying them back to Recipe with numeric id fields is a DB Design preference, not a 3NF principle?",0.0
g1qsf3n,iasrq1,"&gt; And the preference for separating them into different tables and tying them back to Recipe with numeric id fields is a DB Design preference, not a 3NF principle?

correct",1.0
g1qvxg2,iasrq1,"&gt; And the preference for separating them into different tables and tying them back to Recipe with numeric id fields is a DB Design preference, not a 3NF principle?

No.  Those are 1NF.  For any normal form, the lower normal forms are part.

Pulling repeated data out into a table that can be referenced is first normal form, and first normal form is a part of all the normal forms above it.

This isn't a design preference at all.  This is a basic step in getting it right, like 3nf is; just a different step.",0.0
g1r6gvw,iasrq1,So then my first answer about pulling data into three tables was not wrong?,0.0
g1r70qe,iasrq1,"No, your first answer was also wrong, for the other reasons that I also gave there.",1.0
g1qqkcd,iasrq1,"Trying to see where i misunderstood this whole ""Transitive"" thing. Reading on [geeksforgeeks.org](https://geeksforgeeks.org). 

First normal form is very straightforward to me - don't have a single field called \[Ingredients\] and fill it up with a list of values separated by commas. If you want a list of all ingredients for Chicken Parmesan, make it a separate table and tie them back to the Recipes table with a foreign key like RecipeID = 1.   


Second Normal Form - geeksforgeeks says this:  
"" **Note –** 2NF tries to reduce the redundant data getting stored in memory. For instance, if there are 100 students taking C1 course, we don't need to store its Fee as 1000 for all the 100 records, instead once we can store it in the second table as the course fee for C1 is 1000. ""

To me, this explains exactly why we should not be storing Chicken, Chicken, Bacon, Bacon, Entree, Entree, Entree, Snack in OPs Recipe table. 

Also, am I wrong to assume you can't have 3rd if you don't have 2nd and 1st?",-1.0
g1qsdo4,iasrq1,"&gt; Also, am I wrong to assume you can't have 3rd if you don't have 2nd and 1st?

no, you aren't wrong

1NF and 2NF are **requirements** for 3NF

it's always a good idea to start with [wikipedia](https://en.wikipedia.org/wiki/Third_normal_form)",2.0
g1qtlz0,iasrq1,"Honestly, the way wikipedia explains this is in my opinion profoundly difficult to understand even if you already know what this means",1.0
g1qyc1f,iasrq1,"&gt;2NF tries to reduce the redundant data getting stored in memory. For instance, if there are 100 students taking C1 course, we don't need to store its Fee as 1000 for all the 100 records, instead once we can store it in the second table as the course fee for C1 is 1000

If the table's PK is StudentID x Course then Fee is a partial dependency as it exclusively relates to Course (this violates 2NF) but I don't think that's relevant for the discussion. In the Recipe table there is no partial dependency",2.0
g1qkh30,iasrq1,"Nope. The existence of an arbitrary ID column breaks all the rules of normalization. It has no transitive relationship to the data in the row and only exists to serve as a unique row value. Due to this, it is not even 1NF now. I can insert a million duplicate rows for Bacon Popcorn/Bacon/Snack and it will trash the validity of the data in your table, but it would not generate an error because they all have different IDs.",-6.0
g1qlw9e,iasrq1,What?,4.0
g1qsmoo,iasrq1,"you have some learning to do, young padawan

if `ID` is the primary key, all the other columns could be repeated multiple times in different rows, **and the table remains in 3NF**",0.0
g1qtsw0,iasrq1,"Respectfully, you're not correct either

Whether something is in third normal form cannot be determined from a single table unless that single table is composed 100% of its primary key, at which point it's just guaranteed on a technicality.  

Third normal form is a topic between multiple tables.  Without knowing what the other table is, you cannot guarantee that some idiot hasn't made a phenomenally stupid candidate key

When dealing with normal form issues, the phrase `you cannot guarantee that some idiot hasn't` is kryptonite",1.0
g1qtwot,iasrq1,"&gt; Third normal form is a topic between multiple tables

please cite a reference for this, because to me, i think you just made that up",0.0
g1qu2r6,iasrq1,"The reference you already gave, Wikipedia, is 100% built on this, actually.

You're asking me for help in another thread and I'm gritting my teeth being helpful to you after you called me a liar in public.  Reconsider whether this behavior is smart.",1.0
g1qv8e1,iasrq1,"&gt; You're asking me for help in another thread

oh? which one?

and i don't believe i've ever called you a liar",0.0
g1qvedt,iasrq1,"&gt; i think you just made that up
&gt;
&gt; and i don't believe i've ever called you a liar

You know, I answered you in the other thread, but I now regret it.",1.0
g1r7gm4,iasrq1,"I knew almost everyone would not like that comment because we were all taught wrong. I also designed with ID columns for the first 11 years of my career, until I had an interested conversation with another senior database engineer on the topic about 6 years ago. It took me several discussions to finally have it click, but the use of surrogate keys (ID columns) should only be reserved for the star or snowflake schema design. 

The reason why we were all taught the incorrect use of IDs is rooted in the steep costs of hard drive storage in the late 70s and 80s, when 20 MB of storage would cost $5k. It was economical to reduce the data footprint of the keys in a database. When you can store a key as a single byte (or double byte) as an ID, instead of using the natural key which might be 100 bytes of storage from a char(100). Back then it would have made a big difference, and today that would still have an effect in a VLDB star schema database with tables that contain billions and billions of rows. Aside from data warehousing design principals, storage is cheap now-a-days, so the ACTUAL natural key of the table should be used as the primary key.

The use of an surrogate key (ID column) as the primary key that auto increments with every tuple, still allows for the duplication of your actual data, which is the RecipeName, PrimaryIngerdient, and Category columns. This violates the rules of 1NF, which means you can't be 3NF. 

I get what you are trying to say with the ID column, that is makes the row unique, but take this example. You insert 2 more rows for ""Bacon Popcorn"" that are IDs 5 &amp; 6. Now later on you are asked to report the most viewed recipe on the website. Is it going to be ""Bacon Popcorn"" ID 4, or 5, or 6? Now you are going to have to introduce business logic of grouping by RecipeName and summing totals to answer these questions. Another solution would be to introduce a unique constraint on the RecipeName column to ensure that you don't get duplicates. Well in that case, you just discovered the ACTUAL natural key of the table and it should be used as the primary key.

You will find that following these design principals, your queries will become more simple and more efficient. Your queries might even have less joins because the natural key provides the answer you were looking for, where-as the surrogate key could not. For example, joining to this recipe table. If you just needed the RecipeName, then you don't need to join by RecipeID just to get the name. It would be stored in your other table as the foreign key.",1.0
g1r9hpw,iasrq1,"&gt; This violates the rules of 1NF

respectfully, no, 1NF talks about PKs and atomic values, but duplicating data across rows is not covered

but i love the rest of your post

i've often argued that a natural key avoids the need to do a join to ""translate"" the artificial key

best example i can think of is the ""tag"" (a word or phrase) that you can apply to a blog post

you would still use it as a foreign key to a tags table, but that's only to ensure that only valid tags are used -- otherwise, if you allow free form tags, there's no tags table needed, and no foreign key in any case",1.0
g1sg3t6,iasrq1,"1NF has a requirement of ""Eliminate repeating groups in individual tables"", or ""There are no duplicate rows"". My argument is that the arbitrary use of an auto incrementing ID, ALLOWS people to insert duplicate data into a table. 

https://en.wikipedia.org/wiki/First_normal_form",1.0
g1t6hat,iasrq1,"your argument is flawed

duplicate rows means duplicate **rows**

if every row has a different auto-increment PK value, there are no duplicate rows",1.0
g1x7o6b,iasrq1,"Does the PK mean anything to the rest of the data in the row? No.
Does it exist to solely allow a uniqueness to every single row? Yes.
The idea of an auto-incrementing ID as a PK allows you to put bad data into your table. If I had a ""State"" table to hold all the US States, and I used a StateID column, I would be able to put Texas into that table as many times as I wished. All would be valid, none would be duplicate rows, but it would be bad data",1.0
g1x90jk,iasrq1,"&gt; If I had a ""State"" table to hold all the US States, and I used a StateID column, I would be able to put Texas into that table as many times as I wished.

not if you employed that other best practice concerning artificial PKs -- a UNIQUE constraint on the natural key, so you could only enter Texas once

and i think i know where you're going next, which is that one could still enter Texas and Texus

but we're getting pretty far off topic of what Normal Forms are",1.0
g1y8vig,iasrq1,"I wasn’t going to go there. I was going to say that the unique constraint is often times the table’s natural key, which in my argument should be the  primary key! Maybe in this case you could use the state abbreviation, but it wouldn’t be a surrogate key that has no meaning to the data it represents. You would find that queries that might need to join to a State table before (to translate StateID into the name), no longer even need to be joined because the data you were looking for is already being used as the foreign key in the other table. Now your queries are more efficient and execute faster.",1.0
g1y9xk8,iasrq1,"&gt; You would find that queries that might need to join to a State table before (to translate StateID into the name), no longer even need to be joined because the data you were looking for is already being used as the foreign key in the other table. Now your queries are more efficient and execute faster.

good explanation, i'm glad someone else also gets this",1.0
g1rxd89,iasrq1,"&gt; You insert 2 more rows for ""Bacon Popcorn"" that are IDs 5 &amp; 6. Now later on you are asked to report the most viewed recipe on the website. Is it going to be ""Bacon Popcorn"" ID 4, or 5, or 6? Now you are going to have to introduce business logic of grouping by RecipeName and summing totals to answer these questions.

Then you did not use the proper constraints for your data. If the name is unique, than use a unique constraint. Otherwise the recipes with the IDs 4,5 and 6 are **different** recipes. If you want/need to group them, you fucked up somewhere else.

In most cases there is no ""natural id"", see these recipes as an example. It's probably pretty obvious that a recipe's name doesn't define the dish, there can be 100 different ""Spaghetti with Bacon"" recipes.",1.0
g1sfm46,iasrq1,"That was literally the next sentence after the quote you used. This recipe example might not be the best one to illustrate my point, so how about a ""State"" table for US states. There is no need to use a StateID column, and a StateName column. Just use the StateName as the primary key.",1.0
g1q8o0i,iaq832,You might want to look into CTEs,7.0
g1qacn0,iaq832,"Wow, this looks very promising",1.0
g1qai3f,iaq832,"Keep in mind that depending on the PG version, the optimizer might opt to materialize the CTEs before running the queries.",1.0
g1rfwpg,iaq832,Which isn't necessarily a bad thing. After upgrading to Postgres 12 I had many queries where I had to add the `materialized` keyword to the CTEs to get the performance as before.,1.0
g1q4rle,iaq832,"I might be misunderstanding, but couldn't you create a temp table with the joined and filtered data? i.e.

    CREATE TEMP TABLE whatever AS
    SELECT d.sale_date, s.seller_name
    FROM delivery_transaction d
         INNER JOIN seller s 
              ON d.seller_id = s.id
    WHERE d.sale_date BETWEEN '2020-08-01' AND '2020-08-02'

And then use the temp table to do your counts and unions?",3.0
g1q56z8,iaq832,This is what I would do as well. Just dump all that data in a temp table and query that temp table for each UNION.,2.0
g1q5n27,iaq832,"Does it require the same privileges as a normal create? If so, my user is not allowed to create tables. Also, I forgot to state it in my question, my main goal is to make the query more performant, not easier to read. Does this method have better performace?",1.0
g1q85fq,iaq832,[deleted],1.0
g1qj90p,iaq832,"This assumes that the `pkey` follows the same ""order"" as the `sale_date` which might not be the case. 

If there is an index on `sale_date`, using 

```
where sale_date &gt;= date '2020-08-01'
  and sale_date &lt; date '2020-08-03'
```

won't be slower then comparing integer values (in fact a `date` is just a 4 byte `integer` and a `timestamp` is a 8 byte `bigint`)",1.0
g1qy19n,iaq832,"Yea, you're right about the pkey. And now that I'm playing with it, this probably isn't a good option of their problem without some additional logic (ex. if there are no records for a certain date, you get null). Not to mention it works for me in MS SQL, not Postgre.

I would think the difference between relying on the clustered index vs a non-clustered index would have an impact, though. Got me curious and looking at one of the situations I use this. Using this logic in the where clause (of 2 sub-queries pulling a few rows from tables with dozens of millions of records) at the start of the process, the whole query runs in &lt;1 second every time. Switching it out for 'where date &gt;= @date' results in first a 29 second run-time, then a 8-9 second run-time every run after that. 

Looking at the execution plans, the main difference in the faster version seems to be the use of both the clustered and nonclusted index in a nested loop using Parallelism (Distribute Streams). The slower one is just using the clustered index. I don't fully understand it, but my original explanation could be false equivalence.",1.0
g1r2bry,iaq832,"&gt; I would think the difference between relying on the clustered index vs a non-clustered index would have an impact, though

Postgre**s** does not have clustered indexes. Its optimizer also works quite differently from the one used in SQL Server, so that is also not comparable.",2.0
g1r3042,iaq832,"Ahhhhh, interesting! Didnt think that would differ among sqls.",1.0
g1qlej2,iaq832,"It might perform better, yes, that's why I suggested it. Instead of doing the join 3 times and the filter 3 times you're only doing it once to create the temp table. 

It's hard to be sure though, I'd suggest trying it.",1.0
g1q73jr,iaq832,"The first query won't work to begin with, as you have `seller_name` in the SELECT list, but you are not grouping by it (or is this a typo and `seller_name` and `company` should actually be the same column?)

What data type is `sale_date`? If that is a `timestamp` then, `BETWEEN '2020-08-01' AND '2020-08-02'` will not include sales on `2020-08-02` - or to be precise, only sales that occurred exactly at `2020-08-02 00:00:00.00000`

Btw: `count(date(d.sale_date))` is exactly the same as `count(*)` in your case. 

If you want the totals per seller **and** per day, maybe you are looking for `group by rollup (sales_date::date, seller_name)`?",2.0
g1qab0x,iaq832,"Sorry. I didn't have access to the original query, and it is much much complex than the one I have posted, so I had to put together one without checking it thoroughly. You are right on that one.

I will check rollup, thanks :)",1.0
g1qafw4,iaq832,"Another option to consider (if it's OK for the counts to be columns) is filtered aggregation:


```
count(*) filter (where seller_name = 'Jack') as total_for_jack
```",1.0
g1s1tpk,iaq832,"The most performant and elegant way to do this is to query only once with a group by on date and seller. If you want to see the totals/subtotals I.e. you cannot summarise them in your front end/report then use rollup() on the group by. 

Alternatively do not use rollup() and create a second column showing a count for a particular seller e.g 

Select date, seller, count(*) as count_total, sum(case when seller = ‘jack’ then 1 else 0 end) as count_jack
From...",1.0
g1q7hur,iapvpt,Try Brent Ozar's blog and/or videos.  They're great.,6.0
g1qbq3a,iapvpt,"Here's a few tips:

* Look for parameter sniffing.  If you've got stored procedures that branch completely different logic due to IF statements depending on the value supplied for input parameters, the execution plan may be saved for the just one of those branches.  You can add WITH RECOMPILE to the definition.
* Generally speaking, if you have sprocs which are filling up table variables and then joining those, convert them to to temp tables.  Table variables don't carry statistics and are not optimized for joining medium or large data sets.
* Make sure you're not joining on table-valued functions.  If you can, you may be able to call the table-valued function once, dump the results into a temp table, then join on that.
* Sometimes you can get better performance by using windowing functions instead of other methods (for example, using LAG to get the prior value instead of using a correlated subquery).

As far as investigating, you can write metadata queries to search the definition.  Google ""how to query stored procedure definitions.""",5.0
g1rqztg,iapvpt,"Shouldn't we be careful with Adding option recompile
Wouldn't it make the CPU usage go up",1.0
g1s71yw,iapvpt,I think it would make it go up in the same way that using the hair dryer in your bathroom would make the temperature in your entire house increase.,0.0
g1qp30z,iapvpt,What's wrong with joining to a table valued function? It will use the indexes/statistics of the underlying query tables. It's just a reusable CTE.,-1.0
g1qb6fz,iapvpt,"What did he mean by optimization?
Are you guys facing any issues as of now?
What are you trying to fix? 

Here's a couple of YouTube channels that might help you understand some of the basics.
Brent Ozar &amp; 
Bert Wagner

If I am going in blind, I would at least look for if there are any heaps or heaps with RID lookups.
Heaps are generally bad. Then look for clustered index scans (this is just a table scan) or table scan (heap scan), this means every query is reading the whole table. This is bad when your queries run very frequently",3.0
g1qbsbf,iapvpt,"Sometimes the response time isnt great. I found an easy one the other day. A small table was joined to a table with 5 million rows. The join wasn't required any more so I removed that. 

I think my manager wants me to audit the procedures and look for glaring issues really. Indexing is definitely worth considering.",1.0
g1qv2pl,iapvpt,Als0 check kendra little,1.0
g1qsffy,iapvpt,"If you can call the stored procedures. Run them with ""view actual execution plan"" turned on. That will show you the bottlenecks in each sproc.",1.0
g1q9rsi,ianlp2,"Succinct answer 

https://stackoverflow.com/a/17269406",3.0
g1qh3kv,ianlp2,Thanks. The wiki link states that they can be replaced by Joins or Views in most parts.,2.0
g1qhk4e,ianlp2,Yeah. It really just depends on what you are doing and how your execution plan ends up. Good luck.,2.0
g1q1w26,ianlp2,That's a word I need to Google now.,1.0
g1q0nx6,iamrek,"&gt;  create a new table specifically for each user

no, just a second table -- the first for users, and the second for user attributes",2.0
g1rlqfa,iamrek,"thanks, but i misphrased my question. the attributes are not unique, let's say i have a list of emails associated with a user. Now how do I solve this issue? I need to store all these emails under one unique attribute name for each user, what to do?",1.0
g1ro5e9,iamrek,"perhaps because i don't know flask, i think the answer is simple 

    userid  attribute  value
      21     email     todd@gmail.com
      22     email     fred@hotmail.com
      22     email     biff@gmail.com

so a user can have multiple attributes (email, phone?) and multiple values for each

does this work?",1.0
g1rzcqg,iamrek,"I'm very bad at communicating this issue my bad :P
 
    userid   attribute       value(s)
    21        emails            todd@gmail.com, jeff@gmail.com, biff@gmail.com
    22        emails           home@gmail.com, yolo@gmail.com


this is what I need - one userid, with multiple emails linked to it. now i recognize this is not how sql is meant to be used, but how would i properly follow best practice with this requirement?",1.0
g1rzi7e,iamrek,"Wait, you actually did answer my question, but I'm missing something here. isn't userid the primary key? how can you have 22, and then 22 again with a different email (first fred, then biff)? am i missing some fundamental feature within sql? shouldn't userid 22 only be able to show up once",1.0
g1s79k6,iamrek,"if a user can have multiple emails, there would be multiple rows

and you did say ""let's say i have a list of emails associated with a user'

so the primary key would be a composite key consisting of all three columns (so that the same user cannot have the same email more than once)

and if you really only want one row back when asking for a user's emails (possinbly more than one), you would use GROUP_CONCAT to make that string of emails",1.0
g1saafr,iamrek,"wait, so 

1) is userid the primary key?

2) can the userid 22 be in multiple rows, each time iwth a unique email

3) my other comment that i replied with - is the way you explained the same as the otucome i want?",1.0
g1sbhvt,iamrek,"1. no, a composite PK

2. yes

3. um, i think so   🙂",1.0
g1sdior,iamrek,TYSM!! may i pm u if i have any more questions?,1.0
g1sfaa6,iamrek,"prefer posts in the sub, but sure",1.0
g1pkwo3,iamrek,Why do you think this is a unique problem? Attributes per user is very common.,1.0
g1rlnsx,iamrek,"very helpful, thank you for your solution!",1.0
g1phuw0,ialwx7,"I liked w3schools. I don't really like the whole video lecture thing either, and prefer reading, and it's basically a semi-interactive first couple chapters of a SQL book. If you'd prefer a literal book, the MySQL Bible ([free .pdf](https://www.programming-book.com/mysql-administrator-bible/)) is excellent, but I'd get comfortable before you dive headfirst into that, since it's more of a manual than textbook. I personally like to just power through dense, unappealing text like that, but I've been told I'm weird.

I think you'll get there fastest by learning the bare essentials with the w3schools intro, and then just set up your own DB and play with it. Kaggle has plenty of free datasets, and if you want a large one, IMDb lets you [download their data free](https://www.imdb.com/interfaces/) as well.

For the particular flavor you want, if you're on Linux, then MariaDB (literally just MySQL except FOSS afaik) is on the repos by default for most major distros, and MySQL is quite popular, so it's not a bad one to learn. Postgre is FOSS as well iirc, but I haven't worked with it before. Microsoft SQL Server is also popular, and I think can be set up in Windows easily, or if you comfortable with it, there's a Microsoft-official Docker container image for it for Linux that's fairly easy to set up.

For your client, on Linux (and maybe Windows, I haven't messed with Powershell) you can access it CLI if you want, though most people use a GUI. DBeaver is a good ""one size fits all"" GUI that can connect to pretty much anything and work reasonably well, though it's a little buggy. MySQL has all sorts of clients unique to it that are often better than DBeaver, I personally enjoy Sequel Pro for Mac. Microsoft has their own proprietary client called SQL Server Management Studio that's quite nice for SQL Server, though I prefer more minimal interfaces, and it's a little cluttered.

I won't get into ETL/integration tools here since you probably won't need them till later on, but one of the major perks of the Microsoft ecosystem is that it comes with their integration tools, which are quite nice. There are better out there, but they cost a fortune. Alternatively, you may find it easier to write your own with Python or JS, which seems to be common.

Hopefully someone else comes along and recommends other stuff, I haven't worked with DB2, PostgreSQL, or Oracle at all, and those are also very popular and powerful. SQL syntax won't change drastically between DBMSs, but the differences can be substantial- for example, MySQL and MariaDB don't use schemas at all (schema and database are one and the same).",3.0
g1pvtap,ialwx7,"Codecademy is very good for learning basics, also interactive.",2.0
g1pp5r7,ialwx7,Kharnacademy has a good interactive course. I also like https://www.sql-ex.ru/ but the website design is from the 90s. Not everything from the 90s is cool.,1.0
g1pxnbm,ialwx7,"I was applying as an intern in one company and I knew nothing about sql. Senior developer, who also was interviewing me, opened w3school and told me: you have 2 hours to learn mysql, DML not DDL. After 2 hours he come back with some easy tasks. I did it. So w3school is really good for starting.",1.0
g1q17gl,ialwx7,"This is SQL in a nutshell at its most basic :

SELECT.   Column(s)


FROM.  Table 


WHERE. Something equals something



ORDER BY.   


It does get complex at entreprise level but for you you don't need that.



https://learnxinyminutes.com/docs/sql/",1.0
g1q7yzr,ialwx7,"Do you know any other programming/scripting languages?

I'm also a pretty bad student, so rather than doing a course/class I'd just try and build something that uses a database, that way you can find out how SQL works and getting a better idea of how that applies to real life.",1.0
g1qqecd,ialwx7,No I do not. All I really have is a little bit of self taught googling but very minimal,1.0
g1sw9bz,ialwx7,"Another thing you could do is get some of the free datasets suggested elsewhere in this thread loaded into a DB like PostgreSQL or sqllite and connect a report building tool like Superset (free) or Tableau (not free) to the database so you can build different reports to examine different aspects of the data. They both allow you to use the report building tools to generate SQL, or you can write it by hand and display the results.

That way you'd be able to see real world application of SQL without having to code a whole app and just loading up and figuring out the structure of the data would teach you a lot about how to use databases and how to query the data with SQL.",1.0
g22ddef,ialwx7,SQL is too easy to learn. I'd say start with W3Schools and get the basics. Would also recommend strata scratch as they provide basic to advance interactive exercises.,1.0
g1p8meh,iaerzh,"I got my start as a Developer and then DBA with SQL Server about 10 years ago, then I got a new job as an Oracle DBA about 5 years ago.  Now I’m the manager of the data team but am also the principal Oracle and SQL Server DBA and architect.

Transition for me was rough. Not only are they two different dialects of the same language, but the engines are also a bit (sometimes a lot) different.  

Example of functionality differences.

    SELECT column 
    INTO table2 
    FROM table1;

In sql server, that creates a new table using the results of the select statement.

In oracle, the SELECT ... INTO ... FROM command is the equivalent of setting a parameter or variable.  Similar to this in t-sql:

    SELECT @variable= column 
    FROM table;

In oracle, the syntax to create a table from a select statement is:

    CREATE TABLE newtable 
    AS
    SELECT column1, column2
    FROM oldTable;

How about using one table to update another?

    UPDATE t2
    SET t2.column2 = t1.column2
    FROM table2 t2
    LEFT JOIN table1 t1 
    ON t1.key = t2.key;

Nope.  You’re gonna have to use a MERGE or a sub query instead.  That or a cursor.

    UPDATE table2
    SET table2.column2 = (SELECT column2 FROM table1 WHERE table1.key = table2.key);

Want to return data via Stored procedures?  Better use a COLLECT or sys_refCursor!  You can’t have a select statement (or multiple select statements be the output of a stored procedure by themselves.  They need to be put into some sort of object.

Empty strings? No. NULL.  No such thing as empty string in Oracle when the data is actually stored.

Also, don’t forget that semi colon!


     SELECT /* + INDEX(table index1) NOINDEX(table2 index2) */
    Column1,
    Colum2
    FROM table1;

Notes to self?  Nope.  Those are index hints in Oracle.

However, PL/SQL is much more programmatically versatile vs SQL server’s transactional form.  You can get a lot accomplished, but as newer versions of SQL Server roll out, Oracle is just ABSOLUTELY NOT WORTH THE PRICE!!   The price per core + support is just insane!

My best resources:

https://livesql.oracle.com  This is is a FREE 19c database that you can use within your browser.  Great for testing quick concepts.  The LiveSQL portion is also great.

https://devgym.oracle.com  this is a training camp directly from oracle with worksheets and quizzes.  Also great and also free.

Burleson and AskTom are also life.  I have not found an equivalent to BrentOzar, Erik Darling, Kendra Little, etc. and boy have I looked.",8.0
g1nn470,iaerzh,"If you're not dealing with DBA tasks then I can't really see how it would be all that different. You're apparently not even involved with indexing. Some syntax is different - e.g. date conversions, but the bulk is still ANSI SQL.

What T-SQL specific constructs do you use that you are worried about when moving to PL/SQL?",6.0
g1nttac,iaerzh,"Well, I take that back. I will be creating and monitoring indexing, and doing performance tuning on existing queries, but I don't think I'll be the one to be maintaining/rebuilding them. 

More just curious about the syntactical difference, and how different functions and such are. For instance, I've read that pl/SQL can tend to be more iterative at times, with cursors and such, thing that tend to be frowned upon in T-SQL.",2.0
g1of3ae,iaerzh,"I used to work in a Microsoft shop and now live in an oracle one.  

I like oracle better.   I can make it literally do anything.",3.0
g1ozc8f,iaerzh,I learned PL/SQL first and feel lost in T-SQL.,3.0
g2adgr6,iaerzh,"The differences are huge. Sure, `SELECT * FROM MyTable` is the same, but it ends quickly afterwards.

Differences include:

* Semantics around triggers
* Handing of NULL and `''` empty string
* Schema vs. database owner
* Multiple databases in the server vs. schemas
* Security
* Index types
* Data types; precision, dates, localized character sets
* Built-in functions
* UDFs, stored procs returning multiple rows
* Procedural language significant differences
* Query hints are entirely different

Under the cover, the differences are huge, too.  Oracle has a different name for any internal feature (""blocks"" instead of ""pages"", and going on from there ...) and the QO has a radically different architecture.",1.0
g1nszht,iaerzh,"As someone who developed with Oracle, I preferred it. I prefer packages for organizing. My organization is moving to SQL Server, and I'm going to find a different job once that transition is made because I'm more interested in supporting Oracle DBs. Oracle is pretty robust, and has some great features. I think the one feature of SQL Server I liked the most that isn't an option, creating temp tables. Very useful for some tasks. As a dba, the idea that Oracle allows transactions to be rollback, and SQL Server doesn't makes it more robust imo.

Of course this is going to bring out the Oracle haters. Yea, they are spendy. Yea, they are bloated. But in this industry, it's been my experience that working with Oracle earns me significantly more than if I worked with SQL Server. And for me, that is worth it.[http://www.dba-oracle.com/oracle\_news/2005\_12\_16\_sql\_syntax\_differences.htm](http://www.dba-oracle.com/oracle_news/2005_12_16_sql_syntax_differences.htm)",0.0
g1obp94,iaerzh,SQL Server has rollback of transactions.,12.0
g1oiai4,iaerzh,Yeah seriously where did this idea come from?,7.0
g1o7ara,iaerzh,"&gt;As a dba, the idea that Oracle allows transactions to be rollback, and SQL Server doesn't makes it more robust imo.

Can you please elaborate on this? I know that the default for Oracle is for the auto commit to be turned off while auto commit for mssql is on by default. Is there more to your statement than this?",10.0
g1oi2vu,iaerzh,"I'd be interested in learning more about your preference for Oracle and features it supports. I'm very knowledgeable in SQL Server, and only somewhat in Oracle, Postgres/Redshift, etc. Postgres has some cool advanced stuff for data structures and a lot of little things that only very advanced users would probably use. Certainly not something an ORM would take advantage of for example.

However overall, from what I've seen of language features, it seems like overall SQL Server is most robust. Also their tooling is great IMO -- SSMS is really great these days (since VS shell) but it does lack integration w/ other platforms. SSIS has improved a ton in recent years. All the additional, not SQL-standard stuff on top of SQL Server is great... documentation is solid, community is huge, etc. Explain is useful, but visual execution plans can be way faster to parse for me, and there's tons more info if I want to dig into it.

Is it just my experience, or do Oracle devs tend to write queries where they never properly JOIN tables? (they use WHERE instead as a filter instead of join predicate) I keep getting these queries sent my way and they just seem terribly archaic.",3.0
g1oqoeb,iaerzh,"https://docs.microsoft.com/en-us/sql/t-sql/language-elements/rollback-transaction-transact-sql

https://oracle-base.com/articles/18c/private-temporary-tables-18c",3.0
g1ntuw0,iaerzh,"Also that linked site, Burleson Consulting and AskTOM are great references for Oracle issues or questions. Like Brent Ozar is for SQLServer.",1.0
g1nvt76,iaerzh,"Oh man, I totally forgot about that. How's the community surrounding Oracle? I do enjoy the SQL server community, and have learned a ton from them.",1.0
g1nx3kz,iaerzh,"AskTom and support on StackOverflow are the main supports. Most companies that can afford Oracle, also pay for Oracle support. Depending on how much they are pretty responsive. That mostly impacts DBA issues though. 

I recommend checking out local Oracle user groups for monthly meetings (after the pandemic), plus there used to be a yearly user group conference in Vegas. And of course the Oracle conference, that used to be in San Francisco.",1.0
g1nmi5p,iaerzh,Follow,-3.0
g1r1juf,iady77,"    select *
    from (
    select *, min(ID) over (partition by name) as minID
    from players
    )
    where ID = minID

This will produce a table with unique names and the row information from the first row having that name.  You can then do your insert from there.",1.0
g1r67e6,iady77,"Oops, I forgot to add \[SOLVED\]... I solved the problem yesterday but thanks anyway!",1.0
g1r7ihi,iady77,"Can you help me with another SQL related problem? Do you know if it's possible to make this able to select two rows as one if they have the same `name`?

    require_once 'inc/config.php';
      if (isset($_POST['query'])) {
        $inpText = $_POST['query'];
        $sql = 'SELECT name FROM user_items WHERE name LIKE :name';
        $stmt = $conn-&gt;prepare($sql);
        $stmt-&gt;execute(['name' =&gt; '%' . $inpText . '%']);
        $result = $stmt-&gt;fetchAll();
    
        if ($result) {
          foreach ($result as $row) {
            echo '&lt;a href=""#"" class=""list-group-item list-group-item-action border-1""&gt;' . $row['name'] . '&lt;/a&gt;';
          }
        } else {
          echo '&lt;p class=""list-group-item border-1""&gt;No Record&lt;/p&gt;';
        }
      }

and then display both images in this file:

    require_once 'inc/config.php';
      if (isset($_POST['submit'])) {
        $playerName = $_POST['search'];
        $sql = 'SELECT * FROM user_items WHERE name = :name';
        $stmt = $conn-&gt;prepare($sql);
        $stmt-&gt;execute(['name' =&gt; $playerName]);
        $row = $stmt-&gt;fetch();
      } else {
        header('location: .');
        exit();
      }
    .
    .
    .
    &lt;center&gt;&lt;?php echo ""&lt;div class='display'&gt;&lt;img src=../store/img/"" . $row['card_img']. ""&gt;&lt;/img&gt;&lt;/div&gt;""; ?&gt;&lt;/center&gt;",1.0
g1rd0ab,iady77,"Is this PHP? I'm not really familiar with it. If you think there will be only two rows, you can do something like:

    select a.*, b.*
    from user_items a
    left join user_items b
    where a.name = b.name
    and a.id &lt;&gt; b.id

But you would probably be better off seeing what options PHP has by counting the number of rows with the same name than trying to make SQL do it. You can use OVER, like I did in the previous comment, to get those counts.  The following will show the number of rows with the same name next to each row. You could then use that in an if statement to treat those differently.

    select *, count(*) over (partition by name) as CountDistinctName
    from user_items

Or maybe you could do it with some pivot function shenanigans.",2.0
g1redr9,iady77,Aight... Thanks for your help.,1.0
g1m3jyx,ia8wbd,"My two bits having done this 25+ years:


Just start scripting in a text editor. A step up would be to use a database IDE like DBeaver (open source) or DataGrip ($150) or Aqua Data Studio (paid). Eventually you'll meant and IDE if you soley work with data. But a text editor is fine to start.


ALWAYS script out your objects. It pays dividends. ALWAYS make the script idempotent. You should be able to run it over and over with the same end result, with ZERO errors. Never stop until zero errors are achieved. Make doing that muscle memory. You always start objects with:


DROP TABLE IF EXISTS my_table;
CREATE TABLE my_table (....);


Include insert statements for lookup tables (tables that restrict the domain of values on some column in one or more tables), and any other seed data (like a row for your company in the organization table so it's always id values one).



ALWAYS create a surrogate primary key of either INT or BIGINT, and make it an Identity column (auto-generated, staring at one, increment by one).


Learn about normalization. Unless you're boss tells you your job is in jeapordy unless you bail on the time tested rules of normalization (like in cases where the developers revolt if they can't store crap in JSON columns), NEVER EVER violate First Normal Form. Hold as fast as you can to not break Second Normal Form. You can budge a little in a few select cases with Third Normal Form.


The ONLY valid case to denormalize is for performance. Period. NEVER EVER EVER (EVER) for developer convenience. And only denormalize after all other options to boost performance have fully played out (server tuning).


Use default constraints where you can. Constrain column values by having the column be INT or BIGINT with a foreign key constraint to a lookup table (also called a domain, or in a milti-dimensional schema, a.k.a. a data warehouse, a dimension table).


ALWAYS add created_at and updated_at columns  with a default value of now() or current_timestamp() or GetUTCDate...whatever your db flavor uses for now. 


ALWAYS set your database to UTC. NEVER local time.


If your database has a user entity, ALWAYS add created_by and upfated_by columns that are INT or BIGINT data type and are FK to users.id.


-----------------------------------
On naming: many dba's gave strong preferences here. The KEY thing I'd to be consistent.


Mine: (begin ever more highly opinionated section)


Surrogate key (primary key) named id on all tables (no exceptions, see above rule). 


NEVER plural table names. All risk the statement that most dba's say singular names. The name of the table should describe what a single row (tuple) contains, NOT what the table contains. And a single row should never store more than one thing (neither should a single column, like an array...but that's a larger discussion).


ONLY if you are forced to use an opinionated framework like CakePHP where the framework demands plural names should you even think about using plural names. But do you really need an ORM? They are more Hassel in the long run, IMHO (I'll grant they make time to market shorter).


NEVER mixed case names. All lowercase. Use underscores (never dashes) to separate things. It's buyeraccount, not buyer_account table. The name buyer_accoint implies a link table of two columns (buyer_ud,vaccount_id). See next.


It's buyeraccount_note, not buyer_account_note This is a secondary entity that is associated (I'd say related, but that's a loaded term)  to a first class entity so the name contains the name of the primary entity then and underscore then the name of the secondary entity. 


Don't be stupid and append or prepend class info (buyeraccount_tb, or for stored procedures sp_get_buyeraccount). Exceptions: index names, constraint names. See below.


DO suffix index names to indicate if a unique index (_ux). The first part of the name should be the entity name (table name) then two underscores. Include the names of the columns in the index name. Yes that makes for ugly long names. Get over that. This first time you go looking in the data dictionary (pg_class.relname in PostgreSQL, sysobjects.name on SQL Server, etc) to see what indexes exist in a table, you'll thank me. If the name approached 60 chars, abbreviation of column names is ok:


CREATE INDEX buyeraccount__account_issuer_org_id_account_type_id_ux ON buyeraccount(account_issuer_id,  account_type_id);


ALWAYS NAME YOUR CONSTRAINTS!!!!! Don't let's the db name them something meaningless by forgetting to name your foreign key constraints. The constraint name should start with the name of the table, the two underscores, then the column name, then an underscore, the ""fk"":


CREATE TABLE buyeraccount..., account_issuer_org_id INT NOT NULL CONSTRAINT buyeraccount__account_isssuer_org_id_fk REFERENCES organization(I'd),...


Seriously- get over aversion to long names. In tables, in constraints, in everything.


-----------------------------------


ALWAYS use constraints! ALWAYS ALWAYS ALWAYS! NEVER say to yourself ""I'll just skip them until later*"".


* Just before going to production is the usual story.


Fuck that. Seriously. There is exactly and only one case where you fly without constraints: super ultra high volume situations when inserts need to have sub-100 millisecond latency. And then, you need a batch based watchdog process to ensure data integrity.


Yes, you'll be upset when you've got to drop and recreate a table with lots of constraints. You will be annoyed you have to drop all the constraints first. Life sucks. Get over your lazy self. ALWAYS use constraints! I said always use scripting because especially in the beginning, before the db is fully hydrated, it's WAY easier to just drop the whole database, change things and rebuild the whole db. 


The script is also great documentation. And it can be loaded into diagramming tools. But those will read your db directly...IF YOU LISTENED AND USED CONSTRAINTS!


This is key whether you are the only developer or not. Especially important when there are dozens of developers! 


Don't use restricted length VARCHAR columns. I.e., don't do VARCHAR(100). 


ALWAYS use VARCHAR, never TEXT (if your db supports TEXT). PostgreSQL supports text. My OCD drives me mad with other devs using TEXT. In PG, TEXT is a synonym of VARCHAR. Just use the ANSI type. In SQL Server, TEXT is a deprecated type. Poor SS devs get confused reading pages on the interwebs and copy SQL from some other db and get into trouble. Let's all help them out and stop using TEXT!


In addition to a db create script, have an evolution script. As databases evolve, you need to add columns, drop columns, alter data types (usually an INT to a BIGINT). Again, make sure script is idempotent and will always run without errors. If adding a column use regular SQL (advanced maneuver)  or  procedural SQL (easier) to check if the column exists first




There's 100+ other things, but that will get you going.",4.0
g1nnjng,ia8wbd,"Don't use restricted length varchar columns is a terrible suggestion if you are using SQL Server. It will mess with the query optimizer memory grants. For some reason, the number of ""ALWAYS"" you've used in your reply terrifies me lol",3.0
g1qfuex,ia8wbd,"Ha! Yeah, I thought about a disclaimer. Those were born of year of frustration with developers who don't really think about data (reporting, governance), and a career spent cleaning up the mess. There are always exceptions to any rule. I guess I was going for shock value, to make new folk think about the longer term consequences of bad data design.

As for SS memory, tell me more. Link to an internals article? Is the inefficiency (that MS needs to correct!) incurred if the where clause doesn't contain VARCHAR columns?",1.0
g1qibl6,ia8wbd,https://sqlperformance.com/2017/06/sql-plan/performance-myths-oversizing-strings,1.0
g1m8v2e,ia8wbd,"The amount of companies having applications without any db constraints is crazy. I dont even understand it, it takes one minute to create a constraint and saves you from many data integrity failures in the next 10 years.

I dont neccesarily agree with never use a length for varchar columns though. It serves as additional documentation for the column content. You might have some self generated license id thats stored. Making the column a varchar(10) automatically tells you the length of the license id and whether a specific text might be one.

I dont always have an id column too, but I guess it usually doesnt hurt to have one just in case.",1.0
g1mccs9,ia8wbd,"The amount of times I wish I had made an id column taught me over the years to just always add one. Far easier for selecting single rows (vs a composite natural key) for maintenance operations.

I'll give you the use of length restriction on VARCHAR columns. There are cases where it make sense to have the database enforce business rules like that.

Side note: definitely avoid using triggers for business rules or app logic! Triggers have a tendency of growing out of control and don't scale well (procedural SQL is slow when you're talking thousands or more transactions per second).

I think the key issue with constraints is the ""constraints"" it places on the developers (even though quality goes way up), and on the dba (have to fight them when altering the table). Once you're in production, it becomes much more work to refactor a table. But come on people! The bennies FAR outweigh the cost.",3.0
g1pi4dx,ia8wbd,"Thank you man you've given alot of great advices, I try to follow them and I decided to write a script and then Run the script.",1.0
g1lphvg,ia8wbd,"Earlier, I used a software like MySQL Workbench to design my database schema, then export it to SQL. Nowadays I just open an .sql file and write my schema by hand because I realized I am not really saving a lot of time with mysql workbench, but I am understanding and learning more with doing it by hand.

I dont see any gain by creating a PDF file before... if you can write a pdf file about it, you can also write the schema straight away.

PDF files might make sense if you talk with non developers about it, but usually these non developers wont be interested in your DB layout.",3.0
g1mnhk1,ia8wbd,"I use ssms, but i think you can use mysql workbench if you're using mysql.

Dont bother with excel, you may want to hand draw your schema before you get started just to get organized but dropping the tables and recreating them isnt a big deal. And theres a high chance you'll change things mid development.

Command line is frustrating to work with myssql on, id recommend atleast using an editor like vscode and coding that way. So you can see more then 1 line of code at a time.

I would suggest normalizing your database, but honestly if this is your first project just focus on building something that works. Optimize on your next project. :)",2.0
g1pil5a,ia8wbd,"I decided to go with script file, I'll write all of my query and run them direct. Drawing it first on paper would also be a good approach so I'll try that one too.",2.0
g1ndr6d,ia8a03,Would be good to load data into sqlite and then run process it with a query to do similar things like you would in pandas.,2.0
g1nml9s,ia8a03,RemindMe! 1 month,1.0
g1om43a,ia8a03,"There is a 3 hour delay fetching comments.

I will be messaging you in 1 month on [**2020-09-15 21:17:34 UTC**](http://www.wolframalpha.com/input/?i=2020-09-15%2021:17:34%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/SQL/comments/ia8a03/planning_on_delivering_an_sqlite_for_data/g1nml9s/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FSQL%2Fcomments%2Fia8a03%2Fplanning_on_delivering_an_sqlite_for_data%2Fg1nml9s%2F%5D%0A%0ARemindMe%21%202020-09-15%2021%3A17%3A34%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20ia8a03)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
g1lgkmq,ia6qrb,"Your question is a little confusing so I think you may be mixing some concepts.

In general terms - typically when you have a certain set of filter logic (or calculation logic) that needs to be re-used you will use a view. This makes it so you don't need to copy around WHERE clauses, it is already done for you.

Sometimes views get very complex, and you need to utilize features such as temp tables, option hints, loops, or multiple distinct queries returning multiple results. Sometimes you also need a repeatable set of write logic, instead of read. These are the typical cases that you would graduate to a stored procedure. As a stored procedure is more heavyweight and has limited optimizability compared to a view, you should prefer the latter.

A function comes in two main forms, a scalar function, which is  a query that produces a single value. Think for instance the POWER() function. This is pretty unique - you can't do this with a view or a stored procedure, which returns a result set (table) instead of a single scalar value. 

Another type of function is a table valued function. Let's say you have a view - someone running your view can filter by adding a WHERE clause. You can still do that with a table valued function, but I find generally the difference with TVFs is that it allows you to substitute a parameter in multiple places in the query, which can be more performant if the execution plan does not do this well. Another common use case is when the WHERE clause is complicated - you can encapsulate this in the table valued function and expose a simple parameter. Finally, there may be a user input you want to use in the SELECT itself, e.g. a coefficient or a ratio.

Don't know if I answered your question, but the above is in simplest terms possible.",4.0
g1lo94d,ia6qrb,"Its a great answer mostly for the `VIEW` vs `STORED PROCEDURE` vs `FUNCTION` part. And yes, I most likely might be mixing some things or just wrote an opaque question.

But what I'm really asking, while the shop uses extensively the `FUNCTION` which do some really complex conditional `CASE` s and `JOIN`s to calculate some values in the database, that I myself see as pure business logic level stuff and not just querying for the data. I've accustomed to do the logic in application code myself where I'm able to validate the results with unit tests.

So question being: why/when to use complex procedures in db level and when should one do the things in application logic (might be opianated answer since this a SQL subreddit)?",0.0
g1lqjng,ia6qrb,"As you pointed out it's an opinion question. My rule of thumb is that if it can be done in the database easily, then you should do it there. Especially if it's filter logic as there is a cost to returning more data. So don't do what SQL can do for you - filters, joins, and basic arithmetic. When people try to do these things in the application space they often run into memory limits or resource constraints.

At the same time, if you find yourself doing many temp tables, local variables, or worse yet, loops, then it's a clear sign you are trying to write application logic in a database. Which I would also avoid.

The other factor is what your firm's philosophy is. Some firms are very SQL-centric and employ DBAs, data engineers, and others so there is a wealth of knowledge around RDBMS. If you don't have sufficient knowledge to maintain and optimize views, store procs, etc then it doesn't make sense. This is fundamentally why so many startups use JavaScript for everything - they have people who know JS so it makes sense to use it for the backend and the frontend.",3.0
g1lt817,ia6qrb,"Thanks for the replies.

I'm quite accustomed to writing the MySQL dialect and many of the things you said I already knew (or had a hunch), but your messages pointed out some aspects I need to read more about.",1.0
g1kmn4k,ia48lg,your home router already already blocks the port to externally access it.,8.0
g1k1s74,ia48lg,"Allow only connections from localhost by setting 

```
listen_addresses = 'localhost'
```

in `postgresql.conf`. 

That will prevent any connections from other computers. 

That's typically the default though.",6.0
g1k1u0w,ia48lg,Thank you very much.,1.0
g1k1j6r,ia48lg,"If you access it only from your local machine - just set up a firewall to block the port it is using (5432 by default)

Detailed instructions on how to set up a firewall depends on your OS. Which OS you're using?",2.0
g1k1lhi,ia48lg,"Hey, thanks for the answer. I’m using Windows.",1.0
g1k2z2u,ia48lg,"You could also block all incoming traffic, but it could possibly affect other programs you use (file sharing, online games, etc)

Here a guide on how to block specific ports:
https://thegeekpage.com/how-to-block-ports-in-windows-10-firewall/",3.0
g1pyzpd,ia48lg,"You can test it with a second pc, but afaik windows blocks all by default",1.0
g1k6aik,ia48lg,"You may want to encrypt your data as well, if you think there's a threat of physical theft",2.0
g1okk21,ia48lg,"I guess my question is, what are you storing that is valuable enough to be stolen? If you're storing someone else's personal information, I'd reconsider hosting it locally given that you're not totally sure on how to secure that information.

If it's just sample data because you're learning SQL, then you haven't much to worry about. It's no more vulnerable than everything else on your PC.",1.0
g1pvnsi,ia48lg,"That’s what I wanted to hear, that it’s no more vulnerable than everything else on my pc.
By hosting it locally, you mean installing it on the computer I use and not remote accessing it?",1.0
g1l0y1j,ia3r2k,"Verify that SSIS was installed during installation, then add that feature with Setup.

Open Windows Services, and enable the SQL Agent service, and Start it. Refresh SSMS, and SQL Agent will appear.",1.0
g1j018z,ia0a25,"depending on what kind of date/time values you store, you might need to adjust the date condition

     select tablea.uniqueID, tableC.itemID, sum( tableC.quantityPerItem)
     from tablea
     join tableB on 
            tableB.uniqueID = tablea.uniqueID and
            juliandate( tableB.dateOfService) &gt;= juliandate(tableA.startDate) and 
            juliandate( tableB.dateOfService) &lt;= juliandate(tableA.startDate) + 7
     join tableC on tableC.tableB_ID = tableB.tableB_ID
     group by tablea.uniqueID, tableC.itemID",2.0
g1j3wg1,ia0a25,"There is no time data. It’s just a date (yr-mo-day I believe. Don’t have it open). As for the item, I don’t have quantity’s per se. I have, for example, date 1: pear, date 2: apple, date 3: orange, date 4: pear and I want to count the number of unique fruits for the first week. So I want to select the distinct fruits and sum them per unique ID. Thank you for your help the time math you did I will try",1.0
g1i5y8l,i9xivq,"Drop the DISTINCT and add GROUP BY FIRST_NAME after FROM PEOPLE.

    SELECT FIRST_NAME,
    COUNT (FIRST_NAME) AS ""Number of people with this name""
    FROM PEOPLE
    GROUP BY FIRST_NAME;",8.0
g1ic89p,i9xivq,Thank you so much!,1.0
g1hiolv,i9u5mn,You need to look up CASE statements.,1.0
g1hj54j,i9u5mn,"    UPDATE table1
       SET column2 =
           CASE WHEN column1 BETWEEN 10000000 AND 11000000
                THEN 12.97
                WHEN column1 BETWEEN  4000000 AND  5000000
                THEN 8.57
                WHEN column1 BETWEEN 14000000 AND 15000000
                THEN 9.37
                /* and so on for 8 cases */
                ELSE 0.00 END

works exactly the same in MS Access, MS SQL Server, and MySQL",1.0
g1ht3df,i9u5mn,Tha k you.  Will give it a shot.,1.0
g1ti1is,i9u5mn,Can between be used greater than equal to and less than equal to?,1.0
g1u05st,i9u5mn,"yes... `NOT BETWEEN`

otherwise, you have to use two inequality conditions",1.0
g1h33gw,i9ryh1,"    SELECT ProductID
         , MAX(TransactionDate) AS latest_date
      FROM yourtable
    GROUP
        BY ProductID",2.0
g1hee3e,i9ryh1,"Okay this helped a lot. I did this before and it didn't work.  You what what I messed up.. I put the  TransactionDate  in my group by also, which duplicated the rows like crazy.. Any idea why it does this?? I got it fixed thanks a ton!",0.0
g1hgsz7,i9ryh1,"&gt; Any idea why it does this?? 

sure... you'll get one result for every ~combination~ of product and transaction date",1.0
g1h386s,i9ryh1,You could select distinct.,0.0
g1h5mtz,i9ryh1,"no, that would return 2 rows for ProductID 1",2.0
g1h2bui,i9r9x4,"Yes, put it into a ##table, which is a global temp table. Only one can exist on a server whereas an #table is based on your individual session.",2.0
g1hc3ti,i9r9x4,"I know about global temps, but that doesn't apply to this case because I'm not the one creating them. I'm wondering if there are any secret squirrel ways to get into regular temps. For the sake of discussion, assume I have all the permissions.

The case is, there's a report that creates (but in this case, didn't drop) temp tables as part of the pipeline and I need to get into one of the temp staging tables to understand the data flow and look for problems. I didn't run the report-- if I had, I would have written to perm kill and fill staging tables instead.",1.0
g1hca26,i9r9x4,"Nope. Have them create a global, or put it into a real table.",2.0
g1gvrhf,i9qt04,"On the second “when” column_2 should be hr
Or maybe  case when column_1 in (cn,dn) and column_2 = hr then true else false end",1.0
g1gw4k7,i9qt04,thank you kind stranger,1.0
g1h2wba,i9qt04,"    SELECT column_1
         , column_2
         , CASE WHEN column_1 IN ('cn','dn')
                 AND column_2 = 'hr 
                THEN 'true'
                ELSE 'false' END AS column_3
      FROM dbo.table",1.0
g1gou2u,i9q813,"Create a separate table to store a time stamp, old count and new row count. Then runs job that checks the count on the table you are monitoring and compare against the reference table. If the count is higher, update the table, send yourself the notification, otherwise update the table and send no notification.",1.0
g1h8son,i9q813,I would add a colomn with a timestamp as default value. Create a storedprocerure that checks the time  difference since last entry. And call that via a job,1.0
g1glath,i9pmd2,    WHERE filename LIKE '%.txt',2.0
g1gq3qd,i9pmd2,Thanks!,2.0
g1gl19e,i9pmd2,"If I'm understanding correctly, you're looking for wildcards. Sqllite is % sign. '%.txt%' will give you contains .txt",1.0
g1gp20s,i9p9u8,"Can you link to the sub you're talking about?

Jk.",40.0
g1gvfpi,i9p9u8,YOU ARE HERE.,9.0
g1hjp0h,i9p9u8,suck it /r/nosql,16.0
g1hyepd,i9p9u8,"That sub doesn't exist, or does it?",7.0
g1i0zew,i9p9u8,Why wouldn't it exist?,3.0
g1i1lu8,i9p9u8,"I was attempting a joke referencing acid compliant transactions.

It could have been better.",7.0
g1i32gp,i9p9u8,"Clever, but it's hard to land a joke as a question.

I'm also a few drinks deep at this point so I'm sure that plays a role.",3.0
g1gi3uo,i9p9u8,"STFU, noob!!!!    ;)",53.0
g1gtymk,i9p9u8,Gotttttt emmmmmm,10.0
g1gv089,i9p9u8,wait are we in r/sqlJerk /s,3.0
g1imt3p,i9p9u8,I wish this was real.,3.0
g1huo2o,i9p9u8,You clearly didn't ask for help with a homework problem.,4.0
g1hzetf,i9p9u8,That's because he tabled it for later.   ... I'll see myself out.,8.0
g1jz0lr,i9p9u8,I don't do homework. Im a working dev and even if I was a student Id ask the lecturer before posting on a sub.,6.0
g1imqye,i9p9u8,Select * from wholesome,4.0
g1kxp9e,i9p9u8,"I guess I'll pay attention, then. I do need help with ma sheequell!",2.0
g1l02f3,i9p9u8,Don't we all at some stage.,1.0
g1gk9gq,i9p9u8,"&gt; patience 

I was going to reply bu",3.0
g1hcbhj,i9p9u8,r/sqljerk,2.0
g1gf6p7,i9nrdn,"Will it be worth your time to learn, build, debug, make secure? Not doubting your ability but I'm guessing you already have a role in the business and limited free time.",2.0
g1ggxsm,i9nrdn,I'm a fast learner.,1.0
g1ihsrm,i9nrdn,"You didn't answer the question.

There isn't a product you can just buy, throw somewhere, and have it all magically work. Someone built the current application. They worked off a set of requirements they were given. It probably took 6-8 months. How did the Silverlight application get created in the first place? This has _nothing_ to do with the database. Who's responsible for the server hosting this application? Who's doing code and security reviews? Where's the IT department in this whole conversation in the first place?

Once you build this thing, you're going to be on the hook for _everything_ that doesn't work. Something isn't pixel-perfect? Helen in Accounting needs it fixed immediately. Something's off by a fraction of a percent? Bob in HR needs it fixed yesterday. Coffee maker's busted &amp; half the lights in the parking structure are burned out? Yep, that's on you too.

But let's say, for argument's sake, that none of that happens - is your management actually going to give you the 6-8 months to learn how to program _and_ reverse-engineer this Silverlight application and _not_ do your regular job because you're dedicating all your time to doing this project?",2.0
g1ij7kc,i9nrdn,"The application was built by a team of contractors hired by my company. Silverlight has been deprecated in Chrome and Firefox for years, and never ran in Safari. Right now we're actually ""replacing"" it with a Google Sheets document with five formulae (that I wrote) that builds the SQL queries I need. But it's kludgy and requires someone to copy the resulting queries into the SQL manager and execute them. 

The application has been scheduled for replacement for at least three years, but we've been prioritizing work from paying clients. 

I'm an old school nerd. I've always worked at companies where my role was switched by a supervisor asking me to take on a new task. ""We need you to learn how to do builds in XCode."" ""It will be two days before we can allocate a programmer to run that query for you. Why don't you try it?"" ""The client wants to implement On Hand Checking. Acme already has it. You'll need to write a new JavaScript function and update the config file."" ""Can you figure out why the filter isn't working. I know you don't know C, but please look at it.""

I already know what scripts to run to do this work by hand. I feel 100% confident that if I were told what program to use for the front end that I could build an application.",1.0
g1j6cnn,i9nrdn,"So you want a web app that sends values ( like 'how many widgets' ) to a database and runs / causes to run some SQL involving those values. For the sake of argument, Python/Flask might help. And to show the data, maybe Bokeh.

I do think you can learn all this but it's a longer timeframe than the other 'shoot from the hip' examples you mentioned.",1.0
g1jerza,i9nrdn,"It can't take me longer than the three years we've already waited.

Thanks for the suggestion.",1.0
g1gph8c,i9nrdn,We typically use .Net MVC or Angular.,1.0
g1jk8so,i9nrdn,".Net MVC, Entity Framework, Database First.

In a nutshell this will give you what you need.

However, depending on your requirements you also need to factor in security, permissions, schema changes, auditing etc etc.

I'd probably use something like SyncFusion to speed up development and get more advanced datagrid features out the box.

Don't forget about source control too ;)",1.0
g1fqbpn,i9l429,"&gt;so i dont have to write out the entire select statement every time I create a new store proc? 

Does copy and paste not work on your computer?

You should be able to insert the results into a #table.  (Or at least your can in SQL server). If you're hitting the same tables you are creating a performance problem for future you. If you need data from other tables, it shouldn't matter.

    Insert into #data
    Exec sproc name",2.0
g1fri1v,i9l429,"I mean yes, I just figured there may be away around the old copy paste solution",1.0
g1ftaxp,i9l429,"Whoever inherits this from you will curse you if you they have to open 8 sprocs to find where a column is sourced from.

IMO, if you have a base that is reused many times that's cool too start with, but if you have nested things more that 3 deep, something is wrong.",3.0
g1fti0a,i9l429,"Each stored proc returns a query result that is different?  For example, if there's 15 columns, are there 10 columns that are the same on all and 5 that could be different?  And you want to return them all, regardless of columns, as a single select?

Also, I have no idea what you meant by  `storeproc(consisting of 10 other tables)` in the select statement, that returns a single value?",1.0
g1g04x8,i9l429,"really it is just from one table, just joined on distinct things:

&amp;#x200B;

so it would be something like:  


`tb5.price on` [`tb1.date`](https://tb1.date) `+ 1 =` [`tb5.date`](https://tb5.date)`,`

`tb5.price on` [`tb1.date`](https://tb1.date) `+ 2 =` [`tb5.date`](https://tb5.date)`,` 

etc.",1.0
g1g1d98,i9l429,So you're getting the price from different dates relative to your date? That's what each stored proc does? As in each one is a different increment from the date?,1.0
g1g6dhw,i9l429,Would creating multiple Views be the answer? That way they are always available when you need them and you can query against all of them at once if that is what you need? Just change the stored proc code into a view if possible.,1.0
g1frcts,i9hpae,"Isn't your 3rd table suitable to solve that?

    Select *
    from [table3] t3
    join [reservations] r
        on t3.reservation_id = r.reservation_id
    Where t3.table_id in ('Your', 'List', 'Of', 'Table', 'Ids')
        and r.datetime?
        and r.status?
    

I don't know how you'd end that where clause though, because your table doesn't have an end date for the reservation?  You do it solely on status? Then how do you book a table when the reservation is currently in progress? Or even, is upcoming?",2.0
g1fuvgq,i9hpae,"&gt; I don't know how you'd end that where clause though, because your table doesn't have an end date for the reservation? 

We know that usually people have lunch for not more than 2 hours and have dinner for not more than 3 hours. So if a reservation starts at 8pm we don't allow bookings between ```8pm - 3 hours``` and ```8pm + 3 hours```. The status field is used to know if it's upcoming or cancelled.

&gt; Isn't your 3rd table suitable to solve that?

Do you mean the pivot table? I can place reservations on multiple tables, using the pivot table. 

However, I am trying to figure out a way of ""describing"" which tables can be combined with other tables and ""discard"" a combination if one of the tables in that combination is already reserved.

For example I have 3 small restaurant tables (not database tables :D):
- \#1 for 2-4 people
- \#2 for 2-4 people
- \#3 for 2-4 people

Tables **#1** and **#2** can be combined to make a table for 4-6 people and tables **#2** and **#3** can be combined to make a table for 4-6 people. 

**But!** tables **#1** and **#3** cannot be combined, as they are too far away from each other.

Here's a little drawing of the situation: [Floorplan &amp; Combos](https://i.imgur.com/V7enYtR.png)

I want to manually ""describe"" which table can be combined with different table. I don't know how to store this data so I can reserve table **#1** and that automatically ""disqualifies"" **Combination #1** and **Combination #3** (from the drawing) for the specific date &amp; time.",1.0
g1fyyyb,i9hpae,"Ahhhh okay, I misunderstood, that's an interesting problem.  In your example diagram, would you want someone to input that combination 1,2 and 3 or would they input 1,2 and the system would infer 3?",1.0
g1g15rd,i9hpae,"Every combination is manually entered:
- I create record for table #1 and #2
- I create record for table #2 and #3
- I create record for table #1, #2 and #3

We do not change the tables very often, so it's not a problem. Also it gives me more control.",1.0
g1g6sac,i9hpae,"Ah okay, well in that case.

Can try something like this (disclaimer, i do more MS SQL than MYSQL so let me know if some things won't work):

    -- Create table of combos, where (ComboID, TableID) is your PK
    TableCombos (
        ComboID INT,
        TableID INT
    )
    

Then your query could be like:

    WITH TableCTE 
    AS 
    (
        --So first, build the uniform table list i.e. all tables
        -- and combos where applicable
        -- also add a count column for easy checking of combos later
    
        SELECT
            COALESCE(ComboID, -TableID) AS ComboID -- here if it is null it will be a negative table id, just to save null joining later
            , t.*
            , COUNT(1) OVER (PARTITION BY ComboID) AS TableCount
        FROM [Tables] t
        LEFT JOIN [TableCombos] c
            ON t.TableID = c.TableID
    ),
    Reservations
    AS
    (
        -- Do normal reservation check to get tables that are available
        SELECT TableID
        FROM TablesCTE t
        LEFT JOIN [Reservations] r
            ON t.tableid = r.tableid
            AND (your clause to limit reservations)
    ),
    -- then from those available tables, remove those that have combos
    -- and return only tables/combos that are available
    SELECT
        t.*
    FROM TableCTE t
    JOIN (
        -- remove tables if the counts don't match
        SELECT
            ComboID, TableCount
        FROM TablesCTE t
        JOIN ReservationsCTE r
            on r.TableID = t.tableID
        GROUP BY ComboID, TableCount
        HAVING COUNT(1) = TableCount 
    ) res
        ON t.ComboID= res.ComboID -- where if the table is not in a combo, it will be a -number",1.0
g1g9zyw,i9hpae,"Thank you! I am not familiar with MSSQL, but I will try to translate it, test it and report back.  


Again thank you very much for your time.",2.0
g1fxa3m,i9hpae,Are you designing this database? Are you stuck with only these tables?,1.0
g1fxp0o,i9hpae,"I'm extending our current functionality, so anything that achieves this result is welcomed. I will try to transfer old reservation following the current structured to the new implementation. If I fail, I'll just archive them. :D",1.0
g1fxvtz,i9hpae,"So you're not asking how to query, you're asking how to store the data. And you're building/ extending the application that retrieves the data too, right?",1.0
g1fz65y,i9hpae,"Seems like the join of the association table to the reservation table already gives you all tables in the reservation plus the time &amp; status of that reservation, so you should easily be able to calculate remaining availability from that. If you want that info to be more readily available for the application, some of those fields could be moved / duplicated over to the association at the table level (reserve time, free time, status). Then you have all the info you need in a single table.

Does your application need to automatically know which table combos can be grouped to accommodate a large party and which can't, or do staff take care of that part?",2.0
g1g24j1,i9hpae,You could even make a trigger that calculates an “available_time” column when reservations are added if you don’t want your other software to be responsible for that,1.0
g1g2vj1,i9hpae,"&gt; Seems like the join of the association table to the reservation table already gives you all tables in the reservation plus the time &amp; status of that reservation, so you should easily be able to calculate remaining availability from that.

That is correct. What I am doing right now when accepting reservations through a form on our wordpress website is the following:

Guest wants to reserve a table for Tonight (2020-08-14 20:00) for party of 4:


    SELECT * FROM tables
        LEFT JOIN reservation_table 
	    ON reservation_table.table_id = tables.id
        LEFT JOIN reservations 
	    ON reservations.id = reservation_table.reservation_id
	    ON reservations.datetime &gt; '2020-08-14 17:00' // Here I subtract the ""dinner duration"" which is 3 hours
	    ON reservations.datetime &lt; '2020-08-14 23:00' // Here I add the ""dinner duration"" which is 3 hours
	    ON reservations.status = 'Upcoming' // Exclude cancelled reservations 

    WHERE tables.min_capacity &lt;= 4 and tables.max_capacity &gt;= 4 // This is to remove bigger or smaller tables than the number of guests

    HAVING COUNT(reservations.id) = 0 // Remove tables that have bookings matching the criteria above

&gt; Does your application need to automatically know which table combos can be grouped to accommodate a large party and which can't, or do staff take care of that part?

I'd like that to be done automatically, so guests have instant confirmation whether we can accommodate them or not. So I want to _somehow_ store the table combinations and use them in a similar fashion as the query above.",1.0
g1g4oy2,i9hpae,"I assume each table could be part of more than one combo, while some combos will never be used, right?",1.0
g1g68kr,i9hpae,"&gt; I assume each table could be part of more than one combo

Yes, in my example, table #1 is in a combination with table #2 only and in a combination with table #2 **AND** table #3.

&gt; while some combos will never be used, right?

Sorry, do you mean never reserved by ""used""?",1.0
g1getmj,i9hpae,"I mean depending on the physical layout of the restaurant, seems like table #1 and table #10 might never ever be used together. I'm asking b/c I'm trying to understand whether permissible combos need to be predefined",1.0
g1gfmd1,i9hpae,"Anyway it seems like you might want to put your permissible table combos in the same table as the regular tables, maybe with a new flag for whether it's a grouping, then create a new association table to manage the groupings in a many-to-many setup. Then you'd be able to scan the main table for availability but you'd always have to check the association for conflicts and you'd have to run updates through it (but you can prob create and set flags to help manage it).",1.0
g1fy4gx,i9hpae,"Yes, I'd like to know how to store the data. I am building the whole app, too.",1.0
g1ez313,i9f7zf,Nvarchar is your friend,1.0
g1fp0a0,i9f7zf,you meant dynamic SQL? I tried to create a tempdb table and I did success in making one. I'll try to add it into my procedure to see if it work or not.,1.0
g1ic79s,i9f7zf,"Solution: using dynamic SQL to create a dynamic number of columns table.Code:

&amp;#x200B;

    --my pivot table up here--
    --Tempdb--
    DECLARE @sql nvarchar(max);
    If(OBJECT_ID('tempdb..##temp') Is Not Null)
    Begin
        Drop Table ##temp
    End
    --This is my desired table structure.
    --CREATE TABLE ##temp(ID int,Name nvarchar(50),col1 int,col2 int,col3 int,col4 int,...) 
    --Initialize CREATE TABLE statement with the first two fixed columns
    SET @sql = N'CREATE TABLE ##temp(ID int,Name nvarchar(50)'
    --Begin Looping to create a dynamic number of columns table
    DECLARE @index int
    SET @index =1
    WHILE @index &lt;=@totalcols --totalcols is how many cols you want to create
    BEGIN
            --Adding new column with dynamic name (ex: col1,col2,...) and with type of int
    	SET @sql = @sql + ',COL'+ CAST(@index as nvarchar(10)) +' int' 
    	SET @index = @index +1
    END
    SET @index =0;
    --End Looping to create a dynamic number of columns table
    SET @sql = @sql + ')'; --Finalize our CREATE TABLE string with ')' to complete the CREATE TABLE statement.
    
    SELECT @sql --preview your string
    EXEC sp_executesql @sql -- execute string @sql to create a temptable
    --Tempdb--
    INSERT INTO ##temp EXEC(@query) --insert data from my pivot table into temptable
    SELECT * FROM ##temp --select data from temptable

Now EF won't yielding  ""The selected procedure or function return no column"" at me anymore.",1.0
g1e1ell,i9b9hj,"All selecting the rows does is generate a query window within SSMS with the SQL prefilled for that select statement. 

You can do this yourself by clicking the ""new query"" option within SSMS and typing the following: 


SELECT TOP 1000 * FROM table_name


As for right clicking I'm not sure if this is SQL or RDP via your MacBook that's causing the issue.

Hope that helps, let me know if you need anything else.",2.0
g1eilv3,i9b9hj,"If I had to guess it's an RDP / Mac issue... 

Keyboard shortcut for the context menu is shift+F10, but I'm not sure if that'll work from a Mac keyboard... OP might have to open `osk.exe` / on-screen keyboard and select shift + f10",1.0
g1e0sqh,i9b9hj,Can you right click anywhere and get a menu? It may be a Remote Desktop restriction and not a SSMS issue.,1.0
g1dspet,i98ay7,"You need to open the port Postgres is running on on your machine to allow other IPs in. Once it’s open you can connect the same way you would now by replacing localhost with your machines IP.

Not sure what you are trying to do but desktop computers aren’t really the best for this setup, you’re likely to run into some problems at some point. Check out Google Clouds free trial. You can spin up a small virtual machine using compute instance, install Postgres, open the ports and have a cleaner setup. You can even just use Cloud SQL and have a serverless Postgres instance, but if your trying to learn the infrastructure/setup side too that won’t help.",12.0
g1dtj48,i98ay7,"Thank you for the suggestions. I’m doing this just to get a feel of how it’s done, and in the future how I could have a database users could interact with through a gui. Thank you for the Google Clouds suggestion; I have no background in IT so I’m oblivious to these things. Have to learn about servers. Thanks!",1.0
g1dzv0t,i98ay7,No worries. You can send me a direct message if you really get stuck. Good luck.,2.0
g1exhl0,i98ay7,"Google (AWS too) even has PostgreSQL DB services where you can just spin up a database without having to install it and worry about updates, OS security etc. Everyone can access it at the same time via the Internet:

[https://cloud.google.com/sql](https://cloud.google.com/sql)",2.0
g1f9ssx,i98ay7,Thank you!,1.0
g1dgcmg,i98ay7,"There are several ways to do it. There's [libpq](https://www.postgresql.org/docs/12/libpq.html), [ECPG](https://www.postgresql.org/docs/12/ecpg.html), [JDBC](https://jdbc.postgresql.org/), and [ODBC](https://odbc.postgresql.org/) (and likely others).

All can connect from a network as long as the necessary TCP/IP ports are open.",2.0
g1e57fh,i98ay7,"Your question has mostly been answered by the other responses, but to answer the part specifically about accessing it from outside the LAN: yes it is possible but it's not trivial. You would need a static IP address for the desktop (assigned through your ISP) and then you would also need to have the correct port (5432) open as well. Generally speaking, it's not a good idea from a security perspective to open an unprotected computer to the wider internet. There are bots all around the world that spend every minute of the day looking for vulnerable machines to do nasty things to.",2.0
g1e5lrn,i98ay7,What beaverhair (another user who commented here) said; wouldn’t that be what you are talking about? The first paragraph.,2.0
g1e96bh,i98ay7,"Yes partially. You need to have the right port open, like beaverhair said, but you also need to know what IP to connect to. When you're on the LAN, you just use whatever IP was assigned by DHCP to the machine where Postgres is running (usually 192.168.1.1/24, but I'm no network engineer so that may be totally wrong). Trying to connect from the WAN (i.e., the wider internet) is much more complicated and gets into port forwarding and having a static IP address, so you actually know what to connect to. That's more what I was talking about.",2.0
g1f9qtg,i98ay7,"Thank you for the answer. Final question and than I’ll leave you be: does “opening the right port” leave me vulnerable to attacks you mentioned in your original comment (when joining from LAN)? I assume the answer is no, since when connecting from LAN I don’t need to configure connections for new incoming IPs? Is that right?

What I haven’t understood quite right: when connecting from LAN, do I need to “open the port”? Or do I just “use whatever IP was assigned by DHCP to the machine where Postgres is running”?
Thank you.",1.0
g1d6r3y,i97avg,"    put 4 blanks
    at the front
    of each line
    of code",5.0
g1dlylw,i97avg,What they said,3.0
g1f7zlc,i97avg,"It's a method for calculating the median it looks like. 

Not exactly sure of the syntax, but you can calculate the median of a range of ascending values in Excel by by copying the values into a new column and then ordering them in descending order, then you just look for the middle based on id = id + 1.

If you Google ""median in SQL"" you can find more information.",1.0
g1dbp8o,i962jf,"    SELECT TOP 1
           col
         , MaxDate
      FROM ( SELECT 'Date1' AS col
                  , [Date1] AS MaxDate 
               FROM table1
             UNION ALL
             SELECT 'Date2' ,  [Date2] FROM table1 UNION ALL
             SELECT 'Date3' ,  [Date3] FROM table1 UNION ALL
             SELECT 'Date4' ,  [Date4] FROM table1 UNION ALL
             SELECT 'Date5' ,  [Date5] FROM table1 UNION ALL
             SELECT 'Date6' ,  [Date6] FROM table1 UNION ALL
             SELECT 'Date7' ,  [Date7] FROM table1 UNION ALL
             SELECT 'Date8' ,  [Date8] FROM table1 UNION ALL
             SELECT 'Date9' ,  [Date9] FROM table1 UNION ALL
             SELECT 'Date10', [Date10] FROM table1 UNION ALL
             SELECT 'Date11', [Date11] FROM table1 UNION ALL
             SELECT 'Date12', [Date12] FROM table1 UNION ALL
             SELECT 'Date13', [Date13] FROM table1 UNION ALL
             SELECT 'Date14', [Date14] FROM table1 UNION ALL
             SELECT 'Date15', [Date15] FROM table1 UNION ALL
             SELECT 'Date16', [Date16] FROM table1 UNION ALL
             SELECT 'Date17', [Date17] FROM table1 
            ) u
    ORDER
        BY MaxDate DESC",1.0
g1eb7qy,i962jf,"Heres a CTE that can create a date list for you. I typed this out via phone, so it may not look nice, but it should work.

DECLARE @Begin_Date AS Date

--Change the date below to whatever you choose
SET @Begin_Date ='01/25/2020'

;With [Get_Max_Date_CTE] AS

(
SELECT
@Begin_Date AS [MaxDateValue]
,DATENAME(MONTH,@Begin_Date) AS [MaxDateName]

UNION ALL SELECT
DATEADD(DAY, +1, [MaxDateValue])
,DATENAME(MONTH,DATEADD(DAY, +1, [MaxDateValue]))
FROM [Get_Max_Date_CTE]
WHERE [MaxDateValue] &lt; DATEADD(DAY, +15, @Begin_Date)
)
SELECT
*
FROM [Get_Max_Date_CTE]
ORDER BY [MaxDateValue] DESC

OPTION (MAXRECURSION 0)",1.0
g1clfj1,i93sws,"Something like:

```sql
select *
from some_table
where some_date = date '2020-08-12'
  and another_column = 42
```

Performance can be improved with an index on the `some_date` column.",4.0
g1crmob,i93sws,I would probably select the required columns instead of * for minimal response time.,6.0
g1ii2qi,i93sws,"`select *` can do all kinds of wonky things to a query plan, it definitely should not be used if you care about performance and don't actually need _every column on the table_ (spoiler alert: you almost certainly don't need every column)",2.0
g1co2sk,i93sws,Thank you !,1.0
g1cmsa3,i93sws,"You should also mention the DBMS system you are using. When it comes to performance, not all are the same",3.0
g1cnne3,i93sws,So I'm not totally sure what you mean but I am using JDBC and sqlite inside of anylogic but i don't believe the anylogic bit is what you ment,1.0
g1cp5z2,i93sws,I meant the Sql Lite part. It could be this or MSSQL / Oracle / Postgres /Mysql,1.0
g1eatn2,i93sws,"Totes, I don't have any idea what hints are available in databases other than Oracle.",1.0
g1d5qan,i93sws,Make sure you have the date column with an actual date type,2.0
g1ebcn4,i93sws,"Same with numbers. I'm so sick of watching developers misuse quotes for implicit date and number conversions and for all the records to convert instead of vice versa. And off the topic of speed, seeing dates sorted by their string format of DD-MON-YY.",1.0
g1clgrj,i93sws,I think it's best to pull all entries for that particular date and then query over it. Although I am sure people here will have better answers.,1.0
g1co45f,i93sws,It should do the same thing either way the optimizer is usually better at speeding it up than you are. The biggest exception being if your doing weird stuff with joins,2.0
g1cnpn7,i93sws,thanks for the reply !,1.0
g1cmnlm,i93sws,"Large is a varying attribute - for some 1 GB is large and for another 10 TB is.

Rule of thumb, index the table to optimize your read and write scenarios. Create an index such that it contains the equality check column(s) first and then the ranged column such as date or datetime. That way you can check for example

SELECT created, count(*) from Foo where event='FINISHED' and created &gt; '2019-01-01' and created &lt;= '2019-07-30' group by created order by created;",1.0
g1cnvr2,i93sws,"""contains the equality check column(s)"" could you elaborate on what you mean by this",1.0
g1cplmc,i93sws,"So if you do a ranged query or with an inequality operator the index kind of stops there and you don't match any further columns (in general, YMMV).

So if you want to find rows that match with equals operator then put those columns first. Sometimes you may need multiple indexes for different use cases. You can have multiple of them in the index, for example an index with columns user_id, event, created. If you put created first in the index your query might perform badly if you also filter by user_id and ebent",1.0
g1cofdy,i93sws,Just do it all in one query if it's all from over table. Most SQL languages have an optimizer. The optimizer is way better at it than 99% of people at doing 99% of queries unless you do something weird and confuse it. If it's still slow you can add an index.,1.0
g1cwh4j,i93sws,For basic ones that’s true but as you start doing more complex data retrieval the optimiser becomes less efficient. You can see this when you check the execution plan,2.0
g1cwpfy,i93sws,"Yeah, I've had to do that before for queries with a ton of weird joins before. This one didn't sound that way though",2.0
g1ii6gt,i93sws,"You just have to know how the optimizer thinks, and then code to its strengths.",1.0
g1ccf25,i92hst,"This struck me as an interesting problem.   That build number corresponds to SQL 2012 RTM-- no patches at all (!).

I'd personally just fire up DataGrip and pull the databases-- you can then restore them to a more convenient platform for further development.   If you don't have a specific client, you could use any recent version of SSMS.    

I would probably start with a simple database backup and restore, to get a grip on the data side of the problem.   Once you've got the db loaded somewhere, you have more options like porting it to a new db \[if you want.\]

This should be a pretty simple process:   you can do the backup and restore operation with a few right clicks in SSMS.   Let us know how it works out.    I imagine the IIS stuff is going to be way more difficult to deal with than the data stuff.",1.0
g1d17i8,i92hst,"Wow where to start... Ok I would try to isolate where the slow-downs are.. Is it repeatable? Is it a particular  asp page that is slow  or is it spread out everywhere? 

You can run a db trace (which logs all database activities) then go through the steps that are slow. Then you'll be able to optimise the steps that you find out that need the most attention. Like adding missing indexes on frequently accessed tables/fields.

Another approach would be to use something like bent ozar's blitz scripts to do the checks for you. 

All of this is bread and butter to any competent developer and/or DBA who is familiar with the stack. I would recommend finding someone to help you if you are truly struggling. 

Also that sql sever needs urgent patching :) and I'd do some general os health checks on both servers while I'm at it (disk space, event logs, os patching levels)",1.0
g1i07t4,i92hst,"Sorry for the late reply, had to deal with some other work stuff yesterday (yes I'm overworked). 

I have isolated the slow down to the connection issue between the sql server and the server running the asp.net and IIS. I'm not really sure how this stuff connects under the hood so I apologize if it just sounds like I'm throwing jargon out, cuz that kind of is what I'm doing. 

I would love to find someone to help me as I am clearly unqualified yet put up to the task. I just don't know where to start to look for someone to work on a system so old and specific to our use case.",1.0
g1i748w,i92hst,OK we need to rule out the network being the problem. RDP onto the iis server and open a command prompt then ping the DB server. You don't have to post the exact results.. But you are looking for timings that look weird. If that looks OK then ping the server name and again look for anything that looks weird.,1.0
g1i9j9c,i92hst,No the hardware between the servers are fine and the bandwidth is fine. This is the one area of expertise I do have as a network engineer. Both are on VM's hosted with a 10gbps virtual switch between them. It's like a less than 1ms ping between the servers,1.0
g1lesdy,i92hst,"Right .. so we can rule out network.. Next lets confirm that the VMs are healthy (you will want to rule out the simple stuff first before jumping into the DB &amp; making any changes). Note that this is just a brain dump so apologies for the formatting.

When was the last time they were both rebooted/restarted? If not for a while, find out when is a good time to have downtime (i.e. if Sundays are best you've got a day before having to wait a week).

Are they fully patched? OS patching/windows update.

What is the disk layout look like for both? Is there ample free space on drives or are some looking close to capacity?

Do the VMs just host one web (for the web-server) and one SQL Db (on the DB server) or are they used for other websites &amp; databases?

Backups.. where are they .. check that you have the most recent ones for the website home directory, DB backups, any file-shares

Website code - where is the solution, is it in source-control, how does it get built, how does it get deployed (this one is super important to document).

How are the VMs performance stats? at a minimum you'll want to check Memory, CPU &amp; IO. If other processes are running you want to know about them (e.g. anti-virus actively scanning your data directories on the sql server). Note that sql server likes to grab as much ram as possible by default and that could show up.

You want to get the above stats for the VMs at an idle state and at the busy state (so monitor them while initiating the steps to reproduce the slow data retrieval) .

Can the VMs be resized? Is doubling the ram on the db server a simple step? Do you have the ability &amp; authority to take such a step?

Can you add disks to the VMs if needed? On the DB server it is normally good practice to put TEMPDB on  a different disk from the data files.

Do you have a separate test/development environment? Is it possible to restore a recent DB backup onto a different VM altogether?

RDP to each VM and check the event logs... any administrative events that standout?

Do you have [SSMS](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15) installed? Do you have admin permissions on the DB server?",1.0
g1v23pd,i92hst,"&gt; When was the last time they were both rebooted/restarted? 

During this process, they have been rebooted several times. The uptime on them is less than a few days at this time and they can be rebooted if they need to be. 

&amp;#x200B;

&gt; Are they fully patched? OS patching/windows update. 

Not really, but not so out of the realm of up to date that it would likely cause issues. The databases and software and for sure not up to date. 

&amp;#x200B;

&gt; Do the VMs just host one web (for the web-server) and one SQL Db (on the DB server) or are they used for other websites &amp; databases? 

This is kinda on the edge of what I know about the current system. It looks to be only 1 SQL database but I know that there is a test portal (dev), a production environment, and the SQL database hosts some external portals that are interfaced with another VM that faces the web. I can see on the surface level how stupid this would be to have them all on the same database but I can't rule that out based on how the old programmer was. 

&gt; Backups

Yes. The VM's are being checkpointed and replicated to a separate server. As of January (when the programmer left) I do not believe the databases themselves are being backed up.

&amp;#x200B;

&gt; Website code  

Unfortunately, this is firmly outside of my skillset :/

 

&gt; How are the VMs performance stats? 

This was the first place I checked, being that is a step for troubleshooting in what I actually do for my job (this might be changing now lol). Performance on all VM's relating to CPU and Memory is fine. The one thing that I saw was that the IO on the network interface seemed to have been capped at one point. Every time an instance of our portal is open on a machine, the IIS server creates a separate connection with that device and then starts to pull the data from the SQL server. What I found was that when a request was made, the network would seem to be the only thing on both the SQL and IIS server that was being used but the bottleneck did not appear to be the speed of connection (both have their own 1GBPS interface on the VM server) it just would be a lot of data being sent between the two. This implied to me that database inefficiency was the problem relating to the SQL. Idle usage and active usage both confirm my findings above.

&amp;#x200B;

&gt; Can the VMs be resized? Is doubling the ram on the db server a simple step? Do you have the ability &amp; authority to take such a step? 

Yes. Yes, and Yes if I can convince my supervisor that it would make a difference. 

&gt; Can you add disks to the VMs if needed?  

Yes, but in the form of a partition on the disk pool. Unfortunately, the entirety of these servers run on the same disk pool RAID from the main server (and furthermore not an SSD RAID). 

&amp;#x200B;

&gt; Do you have a separate test/development environment? Is it possible to restore a recent DB backup onto a different VM altogether? 

&amp;#x200B;

Yes, and I know where to access this server. I do not know how to integrate any changes though after being made in the test environment. This test/dev database and portal run on a separate VM. 

&amp;#x200B;

&gt; RDP to each VM and check the event logs... any administrative events that standout? 

One event keeps occurring but I don't know what it has to do with anything: 

""A worker process with process id of : '\*\*\*\*' serving application pool '\*\*\*\* Portal' was shutdown due to inactivity. Application Pool timeout configuration was set to 20 minutes. A new worker process will be started when needed. 

(Note: \*\*\* indicate a changing value or company-specific data)

&amp;#x200B;

&gt; Do you have [SSMS](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15) installed? Do you have admin permissions on the DB server? 

Yes, version 11.0.200 on SQL server 2012. And yes full administrator permissions.

&amp;#x200B;

Some things to note just to mark it down: 

* The VM's (the IIS and SQL) were originally sharing a network adapter. The first thing I tried was to separate the two on to their own adapters. 
* Static compression was enabled but not Dynamic compression on the IIS server. I enabled it hoping to save network IO because the CPU speeds were fine.
* There is no virtual switch on the VM host server so the interface between the VM's all go to a separate gigabit switch and then back down to the server. I cannot create one either.",1.0
g1v4nrd,i92hst,Another note: All VM's are running Windows server 2012 R2,1.0
g2kggzf,i92hst,"I think you can ignore the re-occurring event about the application pool. IIS work process recycling is on by default.

If you are still having performance issues, and you have ruled out the VMs and network, and assuming that at one point the site was fine/responsive and performance has degraded over time, then it is likely that the problem is caused by one or more of the following:

* Data size has increased massively over time.  You can compare backup file sizes over time to confirm if this is the case. 
* Indexes and Statistics on the DB have not been updated (normally you would have a SQL Server agent maintenance job  that would rebuild these on periodic basis (nightly/weekends). This assumes that indexes were set up in the first place but is an easy step to check and implement if not. A rough 'n' ready way to do this on a DB is to open SSMS , connect to the DB server and copy paste the below code (replacing the \[**MYDB**\] with your database name) and run it. Note that it may take a while to run on a big DB and will also kill performance until it is complete :) 

&amp;#x200B;

`-- Taken/borrowed from` [`https://www.mssqltips.com/sqlservertip/1367/sql-server-script-to-rebuild-all-indexes-for-all-tables-and-all-databases/`](https://www.mssqltips.com/sqlservertip/1367/sql-server-script-to-rebuild-all-indexes-for-all-tables-and-all-databases/) 

`USE [MYDB]`  
`GO`  
`EXEC sp_MSforeachtable @command1=""print '?' DBCC DBREINDEX ('?', ' ', 80)""`  
`GO`  
`EXEC sp_updatestats`  
`GO` 

* Indexes have not been created on the tables that need it most. This is harder to figure out and will be the next step if the above doesn't work.
* Stored procs (if used) not written to cope with large datasets. We'll leave this and the next step last.
* [ASP.net](https://ASP.net) / SQL query code is inefficient and not written to cater for large datasets (e.g.  running a query with no paging)

*// note a much better way to keep indexes in good heath would be to have something like* [*Ola Hallengren's*](https://ola.hallengren.com/sql-server-index-and-statistics-maintenance.html) *solution in place.. but I'd recommend getting used to it on a non-production server first.*

*// also be sure to document everything you do so that you can evidence your work to whoever needs to know. Your boss should know better IMHO.*",1.0
g1celdh,i91m6y,"Do you already do customer and market analysis? What tools do you use and are they holding you back? If so, maybe doing that work in-database will add value.",1.0
g1bzth3,i90zf3,"select distinct percentile_cont(0.5) within group(order by MyValueColumn) over() from MyTableSource  
or  
select distinct percentile_disc(0.5) within group(order by MyValueColumn) over() from MyTableSource",3.0
g1cbhzz,i90zf3,Thanks man. But I want to calculate median using count and case. Can I get through it,1.0
g1f8jf7,i90zf3,https://codingsight.com/calculate-the-median-by-using-tsql/,2.0
g1c3ab6,i8y39o,"Yeah... that order by part of tbl_groups is definetly going to cost you. First, your primary key is grp_id, which means your clustered index is pre-sorted by that field and that is how your table is optimized. 

Second, I see no covering non-clustered index that included the date field. This execution engine has to put a lot of work to a) find all the date values by scanning the table (because it is not indexed), b) sorting them (because it is not pre-sorted), and c) selecting the top 1, which is a trivial operation and costs very little.

And that is just the date_from field. The engine has to do twice the work because you also have an order by clause for the date_to field. And you have a function on one of those fields, which makes this un-SARGable (search arguement-able) to the query optimizer. Case expressions would have been my choice instead.",2.0
g1bpvpk,i8y39o,"Post the query in question, at least.

Copied from stackexchange

      SELECT grp_fk_obj_id, grp_name
      FROM tbl_groups as g1
         CROSS APPLY (SELECT TOP 1 grp_id as gid
                      FROM tbl_groups as g2
                      WHERE g1.grp_fk_obj_id = g2.grp_fk_obj_id
                      ORDER BY g2.date_from DESC, ISNULL(date_to, '4000-01-01') DESC) as a
       WHERE g1.grp_id = gid


The obvious improvement would be to remove the cross apply section altogether since you are not selecting anything from it and it's not filtering either, since existence is guaranteed.

But, if this was not the actual query, then you need to provide it and get the execution plan of the actual query",1.0
g1b3ri7,i8uvjp,"Well I started using SQL about 2 months ago, and I started with sqlite3, so far I've created two tiny projects using databases. Honestly,  it's not too hard to pick it up; I learned the basics and started using it in about 2 days. I hope this helped",1.0
g1b8ekm,i8uvjp,Thanks a bunch,1.0
g1doum1,i8uvjp,"It’s easy and you can build a basic database within 3 days. Check out codecademy free course on SQL; you write code as you learn new concepts, making it easy to learn. Course can be passed in 4, 5 hours.",1.0
g1b1nqw,i8swe4,"    SELECT roomNum
      FROM yourtable
     WHERE packageID IN ( 1 , 2 )
    GROUP
        BY roomNum
    HAVING COUNT(DISTINCT packageID) = 2",2.0
g1brfex,i8swe4,Thank you for the reply! I think this will work better for what I need to do,1.0
g1a585c,i8i7hf,"What an annoying web site you have linked to. Even disregarding the length of the problem you are asking us to look at, it displays an unmoveable pop-up saying ""this question has been solved, click to see the answer"" which blocks the text. When you click the button the see the answer it asks you to sign up to the site.

Did you post the original home work question to that site? Can you re-post the text here in a formatted way?

What does your query look like so far? What have you been able to do by yourself, before we assist? (i.e. this is not a homework help website)",1.0
g1au38o,i8i7hf,"Agreed. The link is hella annoying. Btw, it's not my ""homework"", I have mentioned in my post that I was just going through a few sql examples online and came across this link. With regards to the question itself, I have used left joins to join the 4 tables together but not sure if it's the right way to do it. What would you suggest?",1.0
g24zyl5,i8i7hf,"@Dr Data, anything on this?",1.0
g1bl736,i8qcvb,"i think in my experience the imdb dataset is relatively uniform and predictable. it might have null values and missing keys or different shaped objects, but i dont know if you need to ""pre-process"" it. it is intended to be used ""out of the box"". 

&gt; And how do i know what kind of preprocessing is required on dataset?

if you are using a strongly typed language and want to guarantee all values are the correct type and objects have the same shape and so forth, you'll have to check for that somehow, and either ""fix"" or remove objects and entries that don't conform to your needs.

data in general can be of any type. and ""pre-processing"" is generally about **removing** values that 
- dont make sense
- have been corrupted
- are null or undefined somehow
- are not needed for the task
- conflict with other values
- can't be guaranteed ,etc

and **shaping** or **changing/editing** values that
- need to conform to a spec, like two digits after the decimal separator.
- conflict with other values
- are of the wrong type
- are in the wrong format, like farenheit to celcius
- etc

basically any time a set of data becomes sufficiently large, the chance for ""corrupt"" or incorrect representational state becomes infinite. data processing is mostly about handling everything from human data input error, to weird computer bugs and so on.",2.0
g1ekead,i8qcvb,Thanks a lot @hp4k1h5,2.0
g199alc,i8lwzi,"&gt; Is this possible?

certainly: left join both tables if you know all possible names in advance or build a dynamic sql (string) to execute.

ps. also, in your case you should have kept all cat and dog attributes inside the animal table.",1.0
g19aqqj,i8lwzi,Thanks. My case was a minimal viable example. What would justify separate tables in your opinion. I am modeling class inheritance where most of the data is in the subclasses and not the parent class.,1.0
g19ce8p,i8lwzi,"vertical split is relatively rare. I'd use it, for example, if my partitioning strategy would be different for parts of the table - i.e 'animal' table is in relatively large (say, billions of records) and I want to partition it and 'cat'  in thousands where i wouldnt need to partition.",1.0
g19a72e,i8lwzi,"&gt; without knowing the table name before making the query

Not possible. All tables used in a query must be known before the query starts. 

The closest thing you could do in Postgres, is to use inheritance:

```
CREATE TABLE animal 
(
  id bigserial PRIMARY KEY,
  name TEXT
);

CREATE TABLE dog 
(
  agression INT
) inherits (animal);

CREATE TABLE cat 
(
  aloofness int
) inherits (animal);

INSERT INTO cat (name) values ('garfield');
INSERT INTO dog (name) values ('scooby');
```

If you run 

```sql
select *
from animal;
```

you will see garfield and scooby. If you want to know from which table the row came, you can use the system column `tableoid` to display that: 

```sql
select a.*, tableoid::regclass::text as what
from animal a
order by id;
```

Will return:

```none
id | name     | what
---+----------+-----
 1 | garfield | cat 
 2 | scooby   | dog 
```",1.0
g19b3gw,i8lwzi,I haven't heard of \`inherits\` in postgres before. I'm wanting to model class inheritance and I need a single id across all subclasses. Maybe \`inherits\` is the correct solution to what I'm trying to do.,1.0
g19bmto,i8lwzi,"You probably don't want to go down that road. Inheritance has quite a few drawbacks, the most important one is, that you can't have proper foreign keys. You can have a foreign key referencing cat or dog, but you can't have a foreign key referencing animal, because animal doesn't actually contain any rows. 

Nowadays I would probably neither use ""real"" inheritance nor model ""class inheritance"" to cope with different attributes for different types of ""animals"". I'd probably just add a `jsonb` column to the animal table to store anything that is ""dynamic"" and have everything that is common for all types of animals as columns in the animal table.",1.0
g19cyzh,i8lwzi,Ya. Not having foreign keys kind makes it inappropriate for what we are doing. `jsonb` also won't work because we have natural keys and other constraints on the child tables. Maybe I will just execute 2 queries with my current schema and then resort to a dynamic query if that becomes a bottleneck.,1.0
g1b1i4a,i8lwzi,google **supertable/subtable**,1.0
g192k06,i8jnft,Lmao this is literally a blatant repost of a top post on this subreddit.,26.0
g19515o,i8jnft,I literally would never search the top posts in a sql subreddit. Lol,19.0
g19qp2b,i8jnft,"no, but you would...query it",26.0
g1c4lk7,i8jnft,"As long as there’s an order by on the search, go for it",1.0
g1cgwxr,i8jnft,I agree. I’ve had my coworkers groaning at this joke for a week!,1.0
g32gk06,i8jnft,u/repostsleuthbot,1.0
g32gmee,i8jnft,"Looks like a repost. I've seen this image 3 times. 

First seen [Here](https://redd.it/f2njxg) on 2020-02-12 96.88% match. Last seen [Here](https://redd.it/g60isb) on 2020-04-22 100.0% match 

**Searched Images:** 146,505,816 | **Indexed Posts:** 581,413,300 | **Search Time:** 5.91587s 

*Feedback? Hate? Visit r/repostsleuthbot - I'm not perfect, but you can help. Report [ [False Positive](https://www.reddit.com/message/compose/?to=RepostSleuthBot&amp;subject=False%20Positive&amp;message={""post_id"": ""i8jnft"", ""meme_template"": 153148}) ]*",1.0
g32h5yi,i8jnft,good bot,1.0
g18s2h5,i8jnft,"Hello u/sohail_ansari - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",0.0
g1auzgz,i8jnft,"Oh, thats me",0.0
g1b5xx4,i8jnft,Hahahaha!  I'm going to send the image to my stupid boss.,0.0
g1a57ko,i8jnft,Haha that's actually funny,-1.0
g191odm,i8jnft,Sassy meme right here,-4.0
g1b8ela,i8hct6,"What do you want to get? What do you mean by sequence number
Do you want to calculate a rank for each type of date in your final select?",1.0
g1b8zfq,i8hct6,"As far as I can understand you want for every project starting from the first date it exist for every subsequent day a record.

with the limited info you give I would start thinking in this direction, pseudo code,

&amp;#x200B;

SELECT  \* -- your calculation stuff

FROM    mytable t1

CROSS APPLY

(

SELECT  \*

FROM    mytimetable timetable

where timetable.dayid&gt;=t1.startingdate

AND timetable.dayid&lt;=current\_date

) myTimeFrame",1.0
g18z6n7,i8gdeq,"When in doubt, [window fuctions](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-ver15)! Based on your description I think this is what you are trying to achieve?

    DROP TABLE IF EXISTS #WeeklyStats;
    
    CREATE TABLE #WeeklyStats (
        WeekNum smalldatetime PRIMARY KEY,
        Metric int
    )
    
    INSERT INTO #WeeklyStats
    VALUES
        ('2020-06-14',10),
        ('2020-06-21',20),
        ('2020-06-28',30),
        ('2020-07-05',40),
        ('2020-07-12',50),
        ('2020-07-19',60),
        ('2020-07-26',70),
        ('2020-08-02',80);
    
    SELECT
        WeekNum,
        SUM(Metric) OVER (ORDER BY WeekNum ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS RollingMetric
    FROM #WeeklyStats
    ORDER BY WeekNum;
    
    INSERT INTO #WeeklyStats
    VALUES
        ('2020-08-09',90);
    
    SELECT
        WeekNum,
        SUM(Metric) OVER (ORDER BY WeekNum ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS RollingMetric
    FROM #WeeklyStats
    ORDER BY WeekNum;
    
    DROP TABLE IF EXISTS #WeeklyStats;",2.0
g17v0oo,i8eo99,"    SELECT member
         , month
         , STRING_AGG(Service) AS services
      FROM yourtable
    GROUP
        BY member
         , month

[requires SQL Server 2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-ver15)",5.0
g17zzvj,i8eo99,This does seem like a good solution... unfortunately the table I'm using is on an older server. I'll try making a copy of it in a newer server to try this.,1.0
g17usy0,i8eo99,"under normal circumstances, I believe that GROUP\_CONCAT is what you're looking for, but I don't think MS SQL has it. Here's a [thread](https://stackoverflow.com/questions/451415/simulating-group-concat-mysql-function-in-microsoft-sql-server-2005) where people have discussed workarounds.",3.0
g18bf62,i8eo99,MS SQL 2017+ has `STRING_AGG()` which will do the job here.,5.0
g17uzfd,i8eo99,"Looks like a job for STRING_AGG(), assuming you're not on an older SQL Server. Some more info here: https://database.guide/how-to-return-query-results-as-a-comma-separated-list-in-sql-server/  
  
If you are on an older version, I don't know the solution offhand, but this page (same site) suggests a ""combination of STUFF(), FOR XML, and PATH()"". https://database.guide/the-sql-server-equivalent-to-group_concat/",3.0
g17y76v,i8eo99,Unpivot that table and make each service a Boolean flag or just use the service name. Then concatenate across each of the new service columns. That’ll give you what you want.,3.0
g180cy3,i8eo99,"I've tried that, but I keep ending up with something like this:

Member | Month | Wash | Wax
---|---|--- | ----
1 | April | 1 | 0
1 | April | 0 | 1

Because I just apply a CASE WHEN to each row.

I'm not sure how to get it so that it becomes:

Member | Month | Wash | Wax
---|---|--- | ----
1 | April | 1 | 1",1.0
g185uus,i8eo99,"Group by member and month, and take the max of your Boolean columns.

Then you can replace the values or do a case when to replace with wash and wax services. Then you just concatenate these columns as usual.",4.0
g18dlld,i8eo99,"Makes sense, thanks!",2.0
g1c31m4,i8eo99,"No problem, if your question was answered, please mark as solved :)",1.0
g18dkw7,i8eo99,"I should’ve said pivot, not unpivot. My bad. You’re trying to denormalize tabular data and turn it into a cross tab, that’s what pivot is for. Grouping isn’t going to work. Take that table with the Boolean flags, pivot Wash and Wax (and whatever other services) into columns. When you write the select, now you can just create a concatenation across each row.",2.0
g186di1,i8eo99,Try using MAX() on [wash] and [wax] columns and group by member and month,1.0
g17tyd1,i8eo99,"Is wash &amp; wax a combo of both the individual wash and wax service types, or is wash and wax it's own thing?",1.0
g17uj3v,i8eo99,"Wash and Wax are individual services, but I want the result to combine them to say ""this was a wash &amp; wax customer this month"". I don't want to change the source table, but do want to write a query that combines them in a new table. So this new table will have a field that's like ServicesCombined with the 4 potential values of Wash, Wax, Wash &amp; Wax, No Service.

Maybe I didn't use the best example, but overall I'm trying to create a field that places members into cohorts IF they had specific services in a given month.",1.0
g181fc6,i8eo99,"String_agg could work but would give you duplicates. i.e. if a customer had two washes in the same month, you'd get ""wash"" twice. 

If that doesn't matter you could use that. It sounds like you have a bigger problem you're trying to solve though. What is it in more detail?",1.0
g19bchx,i8eo99,"Hi - this should work return your desired result without using group\_concat or string\_agg:

`SELECT Member,` 

`[Month],` 

`[Combined_Services]  = REPLACE(`

`(SELECT [Services] AS [data()]`

`FROM tbl`

`WHERE Member = a.Member`

`AND [Month] = a.[Month]`

`ORDER BY Member,[Month]`

`FOR xml path('')), ' ', ' &amp; ')`

`FROM         tbl a`

`GROUP BY Member,`

		`[Month]`",1.0
g176ave,i8aflz,"try escaping the bracket as well i.e. \),\s*",1.0
g17lwb3,i8aflz,"Can you explain?

&amp;#x200B;

Thats the same regex as what I put in ""Find"".",1.0
g17otch,i8aflz,"sorry, looking back at my comment it looks like it didn't show properly, there should be a backslash before the close bracket. Seems like I needed to escape the escape symbol!

\\),\s*",1.0
g16yzuf,i89i3t,"The WHERE goes after the FROM clause - and the JOIN clauses are considered part of the FROM

```
select ..
from ...
  join ...
  join ...
where ...
```",1.0
g16zu55,i89i3t,Thanks!,1.0
g17010n,i89i3t,This and you need Quotation marks around '?' because it is a string.,1.0
g171aq1,i89i3t,I assume that `?` is a placeholder for whatever programming language is used to run the query (e.g. for Java/JDBC it is correct to not quote the placeholder),1.0
g179myp,i89i3t,Yeah it’s to prevent those pesky sql injections,1.0
g167uco,i8480e,"If the app is about keeping track of your outfits, what is the weather data for? 

It depends on your use case, but thay other thread was probably talking about using a 3rd party api for the weather stuff, that way you don't need to store it in your database.",1.0
g1693ag,i8480e,"Well, a column for my table is a 1-10 rating. I was thinking I could make interesting graphs or something so I could maybe regress rating by weather or chance of wearing dark/light colored clothes by weather.",1.0
g1a5mk9,i8480e,Yeah so use an external API for that. There are a ton of free weather data API's out there. Get the data from there and then process it however you'd like.,1.0
g15987r,i802f1,"Use substring and char index. 

Charindex @hotmail.com that will return the position of the @ sign.

Then use substring to move 12 characters to the left and capture 11 character.

Example : substring(textfield,charindex('@hotmail.com',textfield,1)-12,11)",3.0
g15cs6f,i802f1,"That worked! Had to do -11 instead of -12 but other than that, perfect. Thanks!",1.0
g15gxg6,i802f1,"Ah makes sense, can't ever remember if the starting position is included or not. Glad it worked.",1.0
g156jvu,i802f1,"If @hotmail.com will only occur once then POSITION () is your friend for an online solution.

If occurs more than once, the you can loop in a UDF, chopping the string down using POSITION().

Or, you can use a regular expression. MySQL doesn't support back references (capture groups) ala ""\({11}\d\)@hotmail[.]com"". But it could be the basis of a single occurance extract, or used inside a UDF in a (string chopping) loop.",1.0
g156w68,i802f1,Unfortunately no POSITION () in MS SQL.,1.0
g15hdh3,i802f1,"Crap! I read that as MySQL.

Ah well, almost all applies (Microsoft is allergic to regular expressions). But the function in MS SQL is called PATINDEX().",1.0
g159x0m,i802f1,"SUBSTRING and CHARINDEX are your friend

    DECLARE @MyStuff nvarchar(100)
    SET @MyStuff = 'asdasdfasdfasdfasdfasdffasdfasdfa@hotmail.com'
    
    SELECT SUBSTRING(@MyStuff,0,CHARINDEX('@',@MyStuff,0))",1.0
g1f2s0p,i7zyhw,"Both achieve the same thing - giving you wrong results, this is a very dangerous thing to do and if you don’t think about it then you will end up logically corrupting your database. Why not just use Read Committed Snapshot Isolation? This means your queries will only return committed data as of the start time of the execution and don’t have to wait on locks to read. You get the benefits of nolock (writers don’t block readers) but you still get a correct result.",2.0
g1fvrai,i7zyhw,I've never heard of Snapshot Isolation. Thanks for bringing this up. It would be nice if there were standards for data reads for people like analysts.,1.0
g155urh,i7zyhw,"They do the same thing. You can apply table hint ""nolock"" to each individual table, or you can say ""read uncommitted"" at the top of the query. It's generally better to do the latter, as it is less typing for the same result, but there are times where you might want to use the nolock hint, i.e. if you're reaching from one server into another linked one, I don't think you can say ""read uncommitted"" without an error.",4.0
g156brj,i7zyhw,Thank you. Would the read uncommitted have issues if I join multiple databases or just servers?,2.0
g157aoh,i7zyhw,"I honestly don't know enough about the backend maintenance/installation of servers, or databases onto servers, so there may be a setup that allows for setting ""read uncommitted"" - but in my experience, if I want to use openquery, or a linked server, any sort of ""set blah blah"" will fail, and I'll have to go in and manually set nolock table hints per each table on my origin server.",2.0
g16a9lv,i7zyhw,"Note the vast majority of databases don't support such a ""NOLOCK"" feature. Not trying to nitpick, but your post sounds like it's a general database thing. So I just thought I'd clarify.

Edit: OK, apparently the above is not factual? Would love to be corrected if I'm wrong rather than downvoted.  ¯\\\_(ツ)\_/¯",0.0
g15w25h,i7zl5x,"SELECT FLOOR(SomeCoord*1000)/1000

Multiplying by 1000 accounts for the 3 decimal places. Floor will round down to nearest integer and ignore the pre-conversion values after 10^-3. Dividing by 1000 converts it back to 3 decimal places. Any digits past 10^-3 will now be dropped.

Floor and ceiling are extremely useful.",4.0
g157dyc,i7zl5x,combine substr and CHARINDEX()?,2.0
g17tsuf,i7zl5x,"Trailing zeros after the decimal will always be dropped, just like leading zeroes before it.  That's just numbers in general.

You put them back on with formatting when you read the data, or possibly even at the application layer somewhere right before it hits the user's eyes.",2.0
g15nwww,i7zdva,"The pivot operator is so clunky that I hate working with it unless I absolutely have to.

Will you know the Customer Equipment Models beforehand? Will you need to build it dynamically? 

    SELECT CustomerName 
       , CASE WHEN [Model1] &gt; 0 THEN 'Yes' ELSE 'No' END Model1
       , CASE WHEN [Model2] &gt; 0 THEN 'Yes' ELSE 'No' END Model2
    ...
       , CASE WHEN [Modeln] &gt; 0 THEN 'Yes' ELSE 'No' END Modeln
    FROM (
        SELECT CustomerModelId, CustomerName, ModelName
        FROM Customer c
        JOIN CustomerModel cm ON c.CustomerId = CustomerModel.CustomerId
        JOIN Model m ON cm.ModelId = m.ModelId
    ) f
    PIVOT (
        COUNT(CustomerModelId) 
    FOR ModelName in ([Model1],[Model2],...,[Modeln])
    ) p

That's the way I would do it using PIVOT.",12.0
g169c82,i7zdva,"&gt;The pivot operator is so clunky that I hate working with it unless I absolutely have to.

This times a million. I'm now picking up Python for a lot of report automation and I've just resorted to getting the data there and pivoting in Python. Pivoting in SQL is a mess.",5.0
g17vz5k,i7zdva,"Unless we can build the list of Models via code automatically instead of hand coding it one by one, its use is really limited as it will require a change every time the list of Models updates.

I can do it via dynamic SQL but that means I can't take advantage of intellisense and I hate parsing code as it's between quotes.

So I usually form it first and parameterize values that need to be parameterized then encapsulate in quotes after and then figure out how I screwed up any special characters then go back and forth a bunch until I'm so bloody fed up that I just want to throw my machine off the balcony as I'm working from home.",2.0
g18f0re,i7zdva,I do know know all the models (there are a lot) but from my understanding this is just a one time report so I will give this a try listing out each model and see if this will suffice. Thank you!,1.0
g155u8c,i7zdva,You can use CROSS APPLY,3.0
g15sibz,i7zdva,"Create  a fake indicator variable and then spread across columns. Note that this is a huge pain in the ass if you have a lot of different values for orders. See below for example:

    
    create table orders (c_id int, prod varchar2(20));
    insert into orders values (1, 'burger');
    insert into orders values (1, 'taco');
    insert into orders values (2, 'pizza');
    insert into orders values (3, 'burger');
    insert into orders values (3, 'taco');
    insert into orders values (4, 'hot dog');
    select c_id, coalesce(""burger"", 0) as burger, coalesce(""taco"", 0) as taco, coalesce(""pizza"", 0) as pizza, coalesce(""hot dog"", 0) as hot_dog
    from (select c_id, prod, 1 as ind from orders)
    pivot(max(ind) for prod in ('burger' as ""burger"", 'taco' as ""taco"", 'pizza' as ""pizza"", 'hot dog' as ""hot dog""));

Click [here](https://dbfiddle.uk/?rdbms=oracle_18&amp;fiddle=b7d92c530b873005468e4188a6b202ae) for a db fiddle:

Edit: this uses Oracle syntax - not sure what you’re using",2.0
g15i7a1,i7zdva,"So without knowing what table structures look like, or the data there are a few ways this could be approached..

One might be to do a left join back on the table, and use a 

    CASE WHERE IDofSOMEkind IS NULL THEN 0 ELSE 1 END 

For the columns in your pivot, another may be cross apply, or a cte.",1.0
g17bu91,i7zdva,"You already have some good responses for SQL Server, but I wanted to draw your attention to another option, Power Query in Excel. 

I posted a brief about it while participating in a challenge back in May. 

https://codebankblog.wordpress.com/2020/05/13/30dquery-challenge-day-9/",1.0
g18ff5w,i7zdva,If I can’t get it I will check this out thanks!,1.0
g15mao3,i7z8t6,It doesn't.   It's basically explained in the linked text of https://en.m.wikipedia.org/wiki/Transitive_dependency.     Shitty Wikipedia.    Author should just been an ID to author table and author nationality should be an attribute of that table.,1.0
g15twws,i7z8t6,It's not. You are right.,1.0
g15bfzq,i7z8lg,"Not at all, ive talked to a lot of people who have trouble querying more then 1 table say the know sql and its easy. 

Sql can get pretty complicated, more then a lot of people give it credit for. The important thing is that you're learning and improving. The more you work with it the easier it will get.

Also if you're struggling with sets It couldn't hurt to grab one of those venn diagrams for different kinds of joins and keep it nearby. Should be plenty on google.",2.0
g15kt7c,i7z8lg,Thanks i will use the venn diagrams idea actually that will be helpful to keep at my desk!,1.0
g16x3b7,i7z8lg,"You might want to read these: 

* https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/
* http://datamonkey.pro/blog/why_venn_diagram_is_a_bad_choice/

Venn diagrams don't describe joins, they describe the results of INTERSECT, UNION or EXCEPT.",2.0
g17zk8a,i7z8lg,thanks!,1.0
g15kymg,i7z8lg,"Take a look in other disciplines for electives that can help you develop ways of thinking that can help you conceptualize SQL. Thinking back to my own undergrad, some of the highlights for me were rhetorical logic from the philosophy department, and also proof writing and abstract algebra from the math department.",2.0
g1514tb,i7z8lg,"Everyone is different. Just keep plugging away. It will come. It will come. 

I found https://www.w3schools.com/sql/ really helpful when I was starting out. That and a quick google. 

Keep persevering",1.0
g151ej3,i7z8lg,"Thank you so much that is reassuring to hear i was under the impression from others everyone finds it easy so i was very much doubting my intelligence and doubting myself.

&amp;#x200B;

I will keep plugging away it's enjoyable learning SQL and how data works",1.0
g1525no,i7z8lg,"i have always found is if I break my question down, and solve it in small chunks, then I can combine it back together and make a cohesive, performant query. doing it this way has always helped me to understand the data model I am working with. 

As the gentleman said, keep persevering.  Being open minded and just explore and learn. 

don’t be afraid to ask for help. Seeing how other people solve questions can also help to understand how we process information.",1.0
g154ce5,i7z8lg,Yeah im finding that the best way for me to do qeries so far thanks!,2.0
g153zwc,i7z8lg,"Not SQL related but M and DAX...

I've recently been learning as much as possible. Sometimes it's difficult and there are things that aren't immediately obvious. As other have said, keep plugging away at it. You'll find it gets easier over time. You also might notice that when you look back over previous work you notice better ways if doing the same thing, or your able to add in functionality that you didn't know how to do.

I'd highly recommend using something like one note alongside coding to record code and capture notes as it'll be a great resource to return to in the future.",1.0
g1543ro,i7z8lg,"I believe it is normal. Normally programmers are used to think algebraically. But, as you said, in SQL you have to think set-theoretically, and that shift can be hard.

Just keep practicing and good luck.",1.0
g15j2p3,i7z8lg,"Not bad at all. I find academic data things to be impossible a lot of the time. Once you know the data you are working with, it really is not as hard. 

Also in the “real world” I have seen people reuse code very often and they don’t always worry about remembering certain parts of the code. And also don’t let anyone tell you they do not google things...",1.0
g15kuqn,i7z8lg,"&gt; be impossible a lot of the time. Once you know the data you are working with, it really is not as hard.
&gt; 
&gt; 
&gt; 
&gt; Also in the “real world” I have seen people reuse code very often and they don’t always worry about remembering certain parts of the code. And also don’t let anyone tell you they do not google things...
Well that's good to know ty ty &lt;3",2.0
g1ph8tl,i7z8lg,"When I got out of college, the only sql I knew was Ms Access, and that was about it.  I think what helped me the most was breaking the queries apart.  A select to me contains at least 2 but can contain 3 parts, the data I want (select/group), where I'm getting that data (from), and possibly the filter or limiter on that data (where).  Tackle each part separately.  I like to start with the from, its the map of how to get there, add in your filters needed in the where, then fill in the select last.  The good thing about sql is that there is generally multiple ways to solve a problem.",1.0
g157sp5,i7yal1,why not replicate?,2.0
g159b1i,i7yal1,"Look at MERGE, I use that method quite a lot for syncing tables.",1.0
g15c2qr,i7yal1,"Visual studio enterprise has a tool for this. Itll generate scripts for bringing one database up to another. 

Its pretty effective at it.",1.0
g17hov6,i7yal1,"If they're on the same database server, this is pretty easy. You can use the fully qualified names of the tables with the database name:

    INSERT INTO [prod].[dbo].[tablea]
    SELECT * FROM [staging].[dbo].[tablea]

If they're on separate servers, it gets a little more complicated, but still possible: https://stackoverflow.com/questions/14153657/insert-into-from-two-different-server-database",1.0
g15dfge,i7yal1,"I don't think there is an easy way of Updating/Inserting at the database level other than having to do a backup/restore. At the table level, there is no quick way to update/insert multiple tables at once unless you use stored procedures and dynamic sql. Here's a link for how to build a dynamic INSERT statement so that it can be reused for multiple tables:  [https://stackoverflow.com/questions/20634653/build-a-dynamic-list-of-insert-statement-values](https://stackoverflow.com/questions/20634653/build-a-dynamic-list-of-insert-statement-values) 

A JOIN needs to be added to the script so that you can insert data from one table to another. I'm sure a similar script can be created for an UPDATE statement.",0.0
g14rzxq,i7wyti,"&gt; Items (id, checklist_id, content, order).
&gt; how would you insert a row that you want in the middle of the list?

the most obvious and immediate answer is to +1 every item's order value after the insertion position. Is that something you considered and didnt like for some reason?",3.0
g14v78g,i7wyti,for starters it requires a transaction lock to combine INSERT and UPDATE as a single piece of work,1.0
g14voti,i7wyti,"""transaction lock""?",1.0
g157pqq,i7wyti,"    START TRANSACTION;
    INSERT INTO ...;
    UPDATE ...;
    COMMIT;",1.0
g15iatn,i7wyti,That's simply a transaction and you should do that even with a single statement. Autocommit is a spawn of the Satan.,1.0
g14l8m7,i7wyti,"In a query, you would just use ROW\_NUMBER() - that's what it's there for. If you're doing this on a database: adding a new row can sometimes change what value you want for an already-present row, so of course your solution has to involve updating other rows. If this is a table loaded ad-hoc, you'll need to use triggers - they exist for exactly this sort of purpose, but there's the usual performance risk implementing them. If you have a nightly ETL type of thing, you can just tap in to your incremental handling to get the list for what needs to be updated . Either way, your mission is to get a list of every checklist id seen in a changed item, then update ALL items that have that checklist id (even if those items weren't changed) using ROW\_NUMBER().",2.0
g14mavc,i7wyti,"i will leave aside the question of how to store the rank

but i do know the difficulties involved in inserting a new row that belongs somewhere in the middle of the existing ranks

one way is to use FLOAT for the rank numbers, and insert your new row with a rank that's halfway between the row ranks of the preceeding and succeeding rows

and if you're worried about how often you can do this, the answer is very many

    1.0000
    2.0000
    2.5000  -- row added between 2 and 3
    2.7500  -- row added between 2.5 and 3.0
    3.0000
    4.0000

but an even better strategy is *not to store the rank at all*

think about what that might mean",1.0
g14zv6d,i7wyti,Thanks for your reply! I’m not sure I follow when you say that I should consider not storing the rank at all? What if it’s one of the application requirements?,1.0
g1585pu,i7wyti,"&gt; What if it’s one of the application requirements?

provide it in the SELECTs that the application requires

if necessary, define a view

    CREATE VIEW table_plus_rank
    AS
    SELECT *
         , RANK() OVER ...  AS rank
      FROM table",1.0
g147gp2,i7uxlw,"your little diagram shows `ColumnX` and `ColumnY` and `ExResult`

your sample SQL uses `ColumnA` and `ColumnB` and `ColumnD`

i am so friggin lost",3.0
g148way,i7uxlw,"Ha! That's entirely fair! Sorry, I was writing this post in between additional query testing and running a few reports, and my brain lost its train of thought a few times.

&amp;#x200B;

Okay, so Ex Result is basically the result I'm expecting to get from the distinct count of columns X and Y on a row by row basis. As you've discovered, I occasionally write things in a way that make sense in my head, but not necessarily to the people reading them! :)

&amp;#x200B;

Columns A-D have 0 impact on the Columns X and Y. Those are other columns present in my CASE WHEN to define which rows are and aren't empty. I'll put an edit in my post above to clarify as well, but basically what I'm asking is twofold:

1. How can I write something in SQL to create a distinct count of the (combined) values of column X and Y? (""Ex Results"" would be an example of what I'm looking for out of the equation in each row--these results can be visualized in a column if necessary, but otherwise it can just be an equation running in the CASE WHEN if that's possible)
2. Does the structure of my nested CASE WHEN generally make sense?",1.0
g14d85r,i7uxlw,"i'm sorry, i do not understand any part of your clarification, but i do know how to do this --

&gt; How can I write something in SQL to create a distinct count of the (combined) values of column X and Y

    SELECT ColumnX
         , ColumnY
         , COUNT(*) AS howmany
    GROUP
        BY ColumnX
         , ColumnY

why don't you run this, examine what it produces, and then come back with any other questions you might have",2.0
g14vg4u,i7uxlw,"So, what you've provided above appears to work (data is way to large to verify on the fly, but it looks about right). The issue I'm having now is integrating this into the larger query.

My query is \~60 columns wide, and it took some heavy modification to bring this in and calculate (seemingly) correctly. However, once I get to that point, I can't figure out how to actually reference this in my CASE WHEN statement.",1.0
g158jwi,i7uxlw,"&gt; I can't figure out how to actually reference this in my CASE WHEN statement.

you'll need a join 

you can treat the GROUP BY query i gave you as a table, by putting it into the FROM clause in parentheses, with a table alias, and the join columns would be `ColumnX` and `ColumnY`",1.0
g13ryui,i7rk3p,"I tend to use the stuff for XML function for this type of scenarios:

declare @test table  (id int , name varchar(10))
insert into @test values
(1, 'Peter'),
(1, 'Jane' ),
(1, 'Nick' ),
(1, 'Stu'  ),
(1, 'Nick' ),
(2, 'Pete' ),
(2, 'Mary' ),
(2, 'Ed'   ),
(2, 'Mary' )  



select distinct id, STUFF((SELECT ',' + name from @test b where a.id = b.id FOR XML PATH ('')), 1, 1, '')
from @test a",2.0
g175vmw,i7rk3p,"&gt;select distinct id, STUFF((SELECT ',' + name from @test b where a.id = b.id FOR XML PATH ('')), 1, 1, '') from @test a

Brilliant, this works like a charm. Many thanks!",2.0
g1cpkle,i7rk3p,Glad to help,1.0
g13gwzj,i7qy8f,"This can be done in a bit more flexible way:

```sql
select product_name, 
       (select string_agg(upper(left(word, 1)), '')
        from regexp_split_to_table(product_name, '\s+') as t(word)
       ) as abbreviation
from superstore;
```

[Online example](https://dbfiddle.uk/?rdbms=postgres_12&amp;fiddle=4e229bd18992fac3a4dec3438f29a8b7)

If you need this a lot, put it into a function:

```sql
create or replace function abbrev(p_input text)
  returns text
as
$$
  select string_agg(upper(left(word, 1)), '')
  from regexp_split_to_table(p_input, '\s+') as t(word)
$$
language sql 
immutable;
```",2.0
g13h98q,i7qy8f,"&gt;select product\_name,   
(select string\_agg(upper(substr(word, 1, 1)), '')  
from regexp\_split\_to\_table(product\_name, '\\s+') as t(word)  
) as abbreviation  
from superstore;

Brilliant! Thanks for a shorter and more readable way of executing it.",2.0
g13i0v4,i7qy8f,"    create or replace function abbrev(p_input text)   
    returns text 
    as 
    $$   
        select string_agg(upper(left(word, 1)), '')   
        from regexp_split_to_table(p_input, '\s+') as t(word) 
    $$ 
    language sql  
    immutable;

I am not familair with creating functions, and I'm not sure how to execute this in pgAdmin, but I will look into it.",1.0
g13izq3,i7qy8f,"Well, you just run that CREATE FUNCTION statement.",2.0
g13k4fx,i7qy8f,Ah it worked! Thanks.,1.0
g1316lz,i7nzed,"Queries, not quarries. Probably not MSSQL, as it does not support REGEXP.
Go to https://regexr.com/ and enter your different search patterns, it will show you what they actually do and which results they can yield.
- t match exact letter ""t""
- . match anything except line breaks
- asterisk any number of the token before

Edit: formatting on mobile",6.0
g131m7v,i7nzed,And what does ' . * '  does,-2.0
g13cvgd,i7nzed,"If you want to learn, understand and retain then you should experiment and solve the problem yourself.",3.0
g13cs5v,i7nzed,Looks like hackerrank,1.0
g137jd4,i7kc7x,"I think this is what you're looking to do, use an int variable @pYear as the alias for a column, YEAR1?

    DECLARE @pYear int = 2020

    DECLARE @SQL NVARCHAR(MAX);

    SELECT @SQL = 'SELECT ID, YEAR1 AS ""' + CAST(@pYear as nvarchar) + '"" FROM table1;'

    exec sp_executesql @SQL",3.0
g17bwnt,i7kc7x,Thank you! But I decided to change the column's name when displaying the data on DataGrid (C# WPF UI app),1.0
g134c1v,i7kc7x,"I'm not sure I understand what you're attempting here. If you mean use @pYear as a column (and give it an alias) then either of these should work

    DECLARE @pYear INT=2020; /* Just for demonstration */

    SELECT  CAST(@pYear AS NVARCHAR(10)) AS Year1

    SELECT Year1=CAST(@pYear AS NVARCHAR(10))",1.0
g17c1e1,i7kc7x,I want to set the column's name (originally is YEAR1) to a dynamic value (@pYear). But later I decided to do that inside C# instead of SQL,1.0
g12b12b,i7jps3,"In Linux, you need to install standalone SQL Server instance in the nodes first, and then configure the instance as a SQL Server cluster instance.

This tutorial will detail steps on deploying SQL Server 2019 with RHEL 8.x, however it is possible to use SQL Server 2017 in RHEL 7.x or RHEL 8 to configure FCI.",1.0
g126vnt,i7j1mr,"That sounds $10,000 - $20,000 low.",5.0
g127lcw,i7j1mr,Instead of indeed what other sites would you recommend finding an SQL position?,1.0
g127t57,i7j1mr,"clearancejobs.com has some, if you are interested in working for the fed.",3.0
g13bi4e,i7j1mr,"I started at $15/hr with a bachelors, no intern experience, and no valuable knowledge. Was at $50k+ in half a year (partially due to negotiating using a new job offer as leverage). Moved to a new job for an even bigger bump 4 years later.

Id take whatever you can get now (in the field you want) and keep applying to other jobs.",5.0
g13qxzh,i7j1mr,What would be a good entry level career?,1.0
g14480v,i7j1mr,"IT Analyst, Data Analyst, IT Intern, Data Intern, anything similarly named. Just check the job's responsibilities and requirements first to make sure it's what you're looking for and you're qualified.",1.0
g148oc5,i7j1mr,Thank you so much for your reply! 🙏,1.0
g1294q3,i7j1mr,"Yeah that sounds low. I started my BI career January 2019 and started at $50,000. Though was using WebFOCUS as the tool/language, we are transitioning to PowerBI so been learning TSQL. 

Guess it also depends what type of job you're looking for like being a DBA or like what I do which is creating reports and dashboards. I do have a bachelor's too but in project management. 

I would keep looking or if you're able to take that lower salaried role just to get the experience..",3.0
g13qz30,i7j1mr,What would be a good entry level position?,1.0
g13uaf1,i7j1mr,I think like an ETL developer. My company has roles called Jr. EDW Developer which are entry level. Think you just need to have a positive job interview and show your willingness to learn and be a professional in the role.,1.0
g148moc,i7j1mr,Thank you so much for your reply 🙏,1.0
g223yq0,i7j1mr,Take what you can get and build experience.  And continue building on your skills in the meantime - plenty of free or low-cost online options right now.,2.0
g22545z,i7j1mr,What do you mean low cost online options?,1.0
g225iz1,i7j1mr,"Coursera

edX

Linkedin Learning

Pluralsight

Your local technical college

and so forth",2.0
g227b6f,i7j1mr,"Ohhh I see what you mean, like gaining experience and that type of thing.",1.0
g2292hg,i7j1mr,"No - specifically working thru projects and tasks using software packages that you are likely to encounter and trying to absorb as much knowledge as you can rather than pass and move on.  Distinguish yourself and build confidence, confidence is contagious and WILL get you the job.",1.0
g12b0ug,i7j1mr,"My first analyst job was for 48k about 10 years ago in Nevada. I assume wages have risen a little for entry level since then.

Since you don't have a degree, it would be worth taking if offered just for the work experience. After a few years, degrees become less important in IT.",1.0
g139lp7,i7j1mr,I have an AS degree in Information technology?,1.0
g12lh0v,i7j1mr,"For any specific job with wages average, you can easily google and it's also varied by states. You will be lucky of getting a job without a bachelor but it's not impossible, even an entry-level job nowadays requires 2-3 years experience.",1.0
g13hipg,i7j1mr,Do you need the bachelor degree in IT or any other field is ok too?,1.0
g12akgz,i7j1mr,"It is low, but entry level can be hard to find, I’d accept it, work there a year then find something better that’s looking for experience.",0.0
g12659d,i7i3yd,"I would suggest sql over Python to be honest. As a person very close but a little more “comfortable” with sql I would recommend this 4hr long tutorial. 

https://youtu.be/HXV3zeQKqGY

I used to watch his clips when I would need better way of thinking about things

Once you are comfortable I’d also suggest 

https://mystery.knightlab.com

Murder mystery, but you have to use sql to solve the murder

Please let me know if you need help.",23.0
g12ysuv,i7i3yd,"&gt;https://mystery.knightlab.com

This was really fun and a great learning/practice experience. Thanks!",3.0
g128lq9,i7i3yd,That sounds great to both! Much appreciated,2.0
g12jctl,i7i3yd,"USE w3schools for learning SQL. It has great tons of resources with practice exercises. Also check out course from Coursera offered by UC DAVIS ""SQL FOR DATA SCIENCE"". It covers topic from basics to advance.",3.0
g129d1c,i7i3yd,"Of course, like I said don’t be afraid to reach out to me. Everyone knows something that someone else doesn’t.",1.0
g13dnok,i7i3yd,I can vouch for that YouTube tutorial. Most comprehensive and easy to learn.,2.0
g136sp6,i7i3yd,The Murder Mystery game was fun! Thanks.,1.0
g13aw7c,i7i3yd,"Mystery from knight lab was so much fun! Loved it, and wish more people would do games like these",1.0
g12eg6w,i7i3yd,"It kinda depends what you want to do. You mention “coding” but it’s such a broad term.

If you want to learn how to speak with databases SQL is very good and is what you should aim for.
I started my SQL learning with the MSSQL books from Microsoft. If you think literature is something that can help you, i do recommend the books.

If you want to learn coding applications i would say python and javascript is good for a start. If you’re in need of a database then, try sqlite.
Also if you need ideas for projects, some simple stuff to do is a command line based to-do app, and file manipulation.

I know it’s not simple, but try to think of what you’d like to work with inside the realms of coding.
I don’t think even i know how many different possibilities there are, but to try and name a few: data manipulation (sql based), data analysis, making reports (I know, couldn’t find a better description), integration development (software based), web development, backend development (broad) and of course a lot more stuff with ai and machine learning.",4.0
g15a2ii,i7i3yd,Thanks for input and will definitely try to narrow down language in future but most part am keeping it broad right now as I don’t know a whole lot,1.0
g12eloq,i7i3yd,"I am also very new to MySQL and have not done much (just downloaded it yesterday). I wanted to have some sort of structure to learn a new program. I got overwhelmed with the amount of free online videos, and wanted to learn several programs and how they all connected (SQL, Tableau, and Python). I found a course on Udemy. There is a flash sale going on RIGHT NOW. ENDS SOON (hopefully it’s not too late). Courses are relatively inexpensive (do not buy at full price). They have MySQL courses on there for purchase. If you don’t like it, you can get a refund before 30 days. Check it out and see if that interests you! Good luck!",2.0
g12w1ej,i7i3yd,"Pm me if you want help with sql. I'm pretty solid on it - and it's flavors. I'm looking to learn(more) python more so if you go that route hey, more power too ya.                                        Case statements are just like they sound - well what if this case situation happens... Or this... Or this... Well what else could you say but to end it now. They end on the first argument.

Cheers",2.0
g15a3ql,i7i3yd,Will keep in mind thanks!,1.0
g13277a,i7i3yd,DataCamp is my favourite way to learn and practice!!,2.0
g13dl4h,i7i3yd,Do you have to pay for it?,1.0
g13ft7e,i7i3yd,"I just wanted to post and say thank you to OP for posting, I'm in pretty much same position and have been bit scared to try and learn this. Then you guys come in and be all helpful. I've saved the YouTube tutorial to start this week and hopefully u will be able to solve a murder mystery too :)",1.0
g15a7vv,i7i3yd,Glad to hear it’s helping answer questions for others as well that are like me! If you find anything that works really well feel free to post back in this thread!,1.0
g14fx86,i7i3yd,"Hi bro! Am on the same page, I learned python and know am learning SQL to have a database in my Python programs, i really would suggest to learn Programming Logic first, then python and finally SQL
If you are really into coding, and from tutorials, i would really recommend Freecodecamp, just the best brotha, remember to put in practice what you are learning always, that's how you remember well what you learn. God bless!",1.0
g15aayp,i7i3yd,Will check them out and consider such appreciate it!,2.0
g12f5um,i7h94k,"i just tested your code, no error

what's the exact error message you get?  what's your mysql version?",1.0
g12kt3n,i7h94k,"It was downloaded from a docker LAMP stack off of [https://hub.docker.com/r/mattrayner/lamp](https://hub.docker.com/r/mattrayner/lamp) . When testing only the TAG table, I get the error "" ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '(TagID),

FOREIGN KEY (EmployID) REFERENCES EMPLOYEE(EmployID)""",1.0
g13jb9l,i7h94k,"&gt; When testing only the TAG table

where are you doing this testing?

as i said, that `CREATE TABLE TAG` statement runs fine, assuming table `EMPLOYEE` exists",1.0
g15l8jd,i7h94k,"I tested it on SQLFiddle as well and it worked fine there, but when running the code on terminal, it gave me that error",1.0
g121ovk,i7h8i1,Love the look of this I'll have to check it,1.0
g12tk0t,i7h8i1,Thanks :) Love,1.0
g11s37k,i7gf83,"I am actually on a Redshift database, not postgresql",1.0
g1355ja,i7gf83,"`date_trunc('week',transactiondate) + interval '5 day'` should do what you want (at least it does in PostgreSQL). 

If I pass 2020-08-11 to that expression it returns 2020-08-15 which is the ""next Saturday"" - the last day of the week if Sunday is the first of the next week.",1.0
g13xq2w,i7gf83,"When I pass a date that is a Sunday, it returns the Saturday prior, for example, try '2020-08-09'. I get '2020-08-08' as output",1.0
g1359yy,i7eqhr,"Bit late for your interview and I don't know anything about Macs but there is a Firefox Add-On (Sqlite Manager) which allows basic management of Sqlite database files. 

Importing data takes two forms. 

1. Running SQL statements which insert data. 
2. Using the .import command to import directly into a table.

The SQLite docs are the best starting point - specifically : https://www.sqlite.org/cli.html",1.0
g110y5p,i7bngd,You can add a UNIQUE constraint to your word column.,4.0
g112aij,i7bngd,"if the only meaningful column in the words table is the word itself, that is sufficiently succinct to be a strong candidate for PK as the sole column in the table

i mean, think of the efficiency -- you no longer need to join on your artificial key just to find out which word is being referenced in other tables, because those other tables would use a FK which is the word itself, so no join is needed

but the words table would still be needed to ensure only legitimate words are referenced -- relational integrity 

i mean, otherwise you wouldn't need a words table at all, right?",3.0
g112ts1,i7bngd,"Thank you for your response! You make a very good point about removing the join/having it be cleaner just using the word as the primary key. My reasoning against this is the following: I intend to have a few other tables have a foreign key into the word table, and these tables will have many thousands of entries. Thus, if they just have to store an int as oppose to a varchar(30) or whatever I'm using for the word, I thought the space decrease might be important/worth the loss in join. I don't suppose you have an opinion on this? Should I just opt against optimizing for space until I see if I will need to?",1.0
g113z78,i7bngd,"&gt; I thought the space decrease might be important/worth the loss in join. I don't suppose you have an opinion on this?

yes i do

&gt; Should I just opt against optimizing for space until I see if I will need to?

and you guessed it!  well done

seriously, if they are even close in performance, i would always opt for the much simpler solution

if you're really concerned, find a way to measure performance under load

otherwise, the simpler solution is usually best",2.0
g119evr,i7bngd,Great I appreciate your help!,1.0
g111eot,i7bngd,"For whatever process is handling inserting new words into the table, it needs to first check the table to see if the word exists. There are lots of ways to go about doing so, and I'm not familiar enough with MYSQL to say which is the most efficient. I believe you should be able to specify that the 'word' column is UNIQUE (columns other than just the primary key can be forced to be unique) in the table definition and then use INSERT IGNORE rather than INSERT as a kind of ""try inserting this value into the table, and if there's an error because it already exists then just ignore it and move on"" command.",2.0
g11234e,i7bngd,"Thank you very much for your answer! I looked into this and found \[here\]([https://dba.stackexchange.com/questions/205352/is-there-any-way-to-have-unique-constraint-and-not-have-unique-index](https://dba.stackexchange.com/questions/205352/is-there-any-way-to-have-unique-constraint-and-not-have-unique-index)) that adding a unique constraint adds an index, which takes storage. I don't have a great understanding of indexes myself, but just to clarify, can it still be a net decrease in storage using the surrogate key despite this additional index due to the fact that other tables will just need to store an int and not a varchar? I'm sure it depends on the particulars, but I just wanted to make sure I'm not underestimating the extra space an index takes",1.0
g1143ia,i7bngd,"It will still result in a net decrease in storage space since each other table will only be storing the integer value and not a character string, as long as the average length of the words are long enough that the character strings take more space to store than the integer does. If, for instance, you are using a bigint (8 bytes) to store char(4) ""words"" that are never more than 4 characters long (4 bytes) then you're doubling the data size instead.

It will also make things much simpler if something like the spelling of a word needs to change, one update in a single table vs many updates in potentially many tables.",3.0
g10zd12,i7ba35,"Easy. Divide the input value by the base, apply the floor to the result, then multiply by the base. If you want to round down to the nearest 50, then

SELECT FLOOR(175/50)*50

If the target values are enumerated and aren’t a set base, that’s more difficult. You could just write it as case statements if it’s not that many.",2.0
g1113gu,i7ba35,"The rest of values in table are actually increased by 100 units, and 1000 units. 

Cant make case statement either since its a SSRS report where user will enter multiple values,",0.0
g10zj8s,i7ba35,"     select top 1 qty 
     from ""atable"" 
     where qty &gt;= ?MyValue
     order by qty asc",1.0
g110ps4,i7ba35,"Im creating report in SSRS,

so  the user can enter multiple values.

And this query is not working with multiple values

Thanks",1.0
g112ebw,i7ba35,"&gt;so the user can enter multiple values.

what does this mean in terms of what your sql gets?",2.0
g10zu6x,i7ba35,"Find the remainder and subtract it:

declare @test int = 237

select @test - (@test % 50)",1.0
g1135ie,i7ba35,"You should be able to drop the supplied array into a table and then get the max number less than or equal to a given supplied value with a join and group by:

    DROP TABLE IF EXISTS #temparray
    SELECT value INTO #temparray FROM STRING_SPLIT(@param, ',')
    
    SELECT 
    b.value, MAX(a.QTY) as [RoundedDown]
    FROM sometableA a
    JOIN #temparray b ON a.QTY &lt;= b.value
    GROUP BY b.value",1.0
g11770t,i7ba35,"im using 2014 studio, and string_split function is nt working, do you know any other function?? 
Thanks",1.0
g119qz2,i7ba35,"You could try splitting it with XML:

    DECLARE @XMLSplit XML
    
    SELECT @XMLSplit = CAST('&lt;i&gt;' + REPLACE(@param, ',', '&lt;/i&gt;&lt;i&gt;') + '&lt;/i&gt;' AS XML)
    
    SELECT x.i.value('.','int') AS value INTO #temparray FROM @XMLSplit.nodes('//i') x(i)
    
    SELECT 
    b.value, MAX(a.QTY) AS [RoundedDown]
    FROM sometableA a
    JOIN #temparray b ON a.QTY &lt;= b.value
    GROUP BY b.value",3.0
g11tcw8,i7ba35,"it worked, Thanks!!!!!!!!!",3.0
g117zvy,i7ba35,"I guess upgrading to 2016 is probably out. :D

Do you have a permanent number table? How high do these quantities go?",1.0
g11j7pl,i7ba35,"I think the key here will be the lag window function. I'm using Postgres but lag() should be the same on SQL Server. I'll test on SQL Server in the end.

So setting up the table:

    mwdb=# create table t (qty int);
    CREATE TABLE
    mwdb=# insert into t values(150);
    INSERT 0 1
    mwdb=# insert into t values(200);
    INSERT 0 1
    mwdb=# insert into t values(300);
    INSERT 0 1
    mwdb=# insert into t values(1000);
    INSERT 0 1
    
    select * from t;
     qty  
    ------
      150
      200
      300
     1000
    (4 rows)

What I would do is first write a query to get the next lower value of qty for each row by using the aforementioned lag() function.

    mwdb=# select qty, lag(qty) over (order by qty) as lag_qty
    mwdb-# from t;
    
     qty  | lag_qty 
    ------+---------
      150 |        
      200 |     150
      300 |     200
     1000 |     300

Now you can join on the inputs to get the next lower value for each, and we should be able to do this for any number of inputs. Let's first mentally work out how some inputs will map:

* 151 maps to 150
* 225 maps to 200
* 100 maps to nothing - no result as it is not greater than any value of qty. (Let me know if this is correct.)
* 9999 maps to 1000. (Again let me know if this is correct.)
* One more: What if it's an exact match? I would assume 300 would map to 300 is all.

So now we can create a list of the inputs: 151, 2295, 100, 999, 300.

VALUES(151), (2295), (100), (999), (300) inputs (qty)

But how do we join them? Perhaps we could make the join condition BETWEEN lag\_qty and qty.

I'm going to rename lag\_qty and qty to lower\_qty and upper\_qty because I find that easier to grasp. I'll also use a CTE to make life a bit easier.

    mwdb=#
    WITH t_cte AS (
      SELECT qty AS upper_qty, LAG(qty) OVER (ORDER BY qty) AS lower_qty
      FROM t
    )
    SELECT inputs.qty AS input_qty, t_cte.lower_qty AS qty
    FROM t_cte
    JOIN ( VALUES(151), (225), (100), (9999), (300) ) inputs (qty)
    ON inputs.qty BETWEEN t_cte.lower_qty AND t_cte.upper_qty;
    
     input_qty | qty 
    -----------+-----
           151 | 150
           225 | 200
           300 | 200
           300 | 300
    (4 rows)

Looks decent except we lose the the edge case of 9999 mapping to 1000. We also have a duplicate for the input 300. This is because the BETWEEN operator is inclusive for a range comparison, therefore both 200 to 300 and 300 to 1000 match. We probably should replace the BETWEEN with inclusive on the lower end, not inclusive on the greater end.

So rewriting:

    WITH t_cte AS (
      SELECT qty AS upper_qty, LAG(qty) OVER (ORDER BY qty) AS lower_qty
      FROM t
    )
    SELECT inputs.qty AS input_qty, t_cte.lower_qty AS qty
    FROM t_cte 
    JOIN ( VALUES(151), (225), (100), (9999), (300) ) inputs (qty)
    ON inputs.qty &gt;= t_cte.lower_qty AND inputs.qty &lt; t_cte.upper_qty;
    
    input_qty | qty 
    -----------+-----
          151 | 150
          225 | 200
          300 | 300
    (3 rows)

OK better, but now to handle the case of 9999 mapping to 1000? We could add a special row with a huge value of qty to t\_cte. There may be a way to tinker with the join logic to get this. This is the simplest solution I can think of though.

    SELECT qty AS upper_qty, LAG(qty) OVER (ORDER BY qty) AS lower_qty
    FROM (SELECT qty FROM t UNION SELECT 99999999 AS qty) sub_t
    
    upper_qty | lower_qty 
    -----------+-----------
          150 |          
          200 |       150
          300 |       200
         1000 |       300
     99999999 |      1000
    (5 rows)

Now put it all together:

    WITH t_cte AS (
      SELECT qty AS upper_qty, LAG(qty) OVER (ORDER BY qty) AS lower_qty
      FROM (SELECT qty FROM t UNION SELECT 99999999) AS sub_t --is there a better solution?
    )
    SELECT inputs.qty AS input_qty, t_cte.lower_qty AS qty
    FROM t_cte JOIN
      ( VALUES(151), (225), (100), (9999), (300) ) inputs (qty)
    ON (inputs.qty &gt;= t_cte.lower_qty AND inputs.qty &lt; t_cte.upper_qty)
    
      input_qty | qty  
     -----------+------
            151 |  150
            225 |  200
            300 |  300
           9999 | 1000

OK lovely, with the only catch being this is a Postgres query, but good news - I just tested this on sqlfiddle using SQL Server 2017, and it worked without a hitch.

[http://sqlfiddle.com/#!18/8ec0b/1](http://sqlfiddle.com/#!18/8ec0b/1)",1.0
g10um1x,i7ba35,FLOOR,0.0
g10uyg7,i7ba35,"Nope,

 The SQL FLOOR() function rounded up any positive or negative decimal value down to the next least integer value",1.0
g10zldf,i7ba35,FLOOR(amount/ 50) * 50,1.0
g1273c2,i7ba35,Please respond if this worked or not (nooby noob here),3.0
g10qixn,i7ap20,How did you get the data sorted in that order? Is there another column that can be used to identify which columns should be assigned which id?,15.0
g10rfyx,i7ap20,"its from our fleet management service providers report which comes in excel file, I've converted it into csv and have loaded it into the db. It's done with future BI reports in mind.The layout is silly I do admit, unfortunately no other columns exit for id.",5.0
g11dqkz,i7ap20,"Data analyst / sales engineer here

I would REALLY push for this process to be done on the file prior to upload to your db.

If its in csv, this is something you can do easily with any programming language of your choice - probably with no original code necessary if you do some googling. You'll likely just need to edit the code suggestion for your specific column and file names.

Keep in mind whatever your solution to this problem is will become the standard process that you will have to maintain and troubleshoot if there are issues or changes later. 

You know better than I if the scope of this is something that is going to lessen over time, in which case do whatever you get working first. If you think this might be somewhat of a building block for future work or just a process that will be around for a long time - I'd definitely focus on getting the data formatted properly outside of SQL.  

Data pipelines for me tend to look like this:

Data source -&gt; format processing -&gt; db upload -&gt; db abstractions -&gt; reporting layers",25.0
g12hqg1,i7ap20,Umm... What the hell is a sales engineer?,5.0
g13fonn,i7ap20,Typically a sales engineer supports the sales team by providing technical details.,3.0
g149ps2,i7ap20,"[Sales engineering](https://en.wikipedia.org/wiki/Sales_engineering)  is a hybrid of sales and engineering that exists in industrial and commercial markets. Buying decisions in these markets are made differently than those in many consumer contexts, being based more on technical information and rational analysis and less on style, fashion, or impulse",1.0
g11dcx1,i7ap20,"You probably should fix this in your transformation step of an ETL Process.  


Extract the data from Excel into a programming language (or other similar tool)  
Transform it to be usable in the database schema.  
Load it into a database.  


This is mostly trivial for any programming language to do. You'd just make the database work harder for data that should have been cleaned up first before loading.",21.0
g11dy20,i7ap20,I agree you should organize the data first.  This will hurt down the line when you have new requirements,4.0
g12ub8j,i7ap20,This is the right answer. If you read this with pandas into a dataframe you would simply use the forward fill parameter for the fillna method,1.0
g11yjx2,i7ap20,"Repeat after me:

&gt; A table is an *unordered* collection of records.

That's the relational definition of a table. If you don't have a way to maintain the order of records in your database, then you lose that order when you import the data. The system *does not* guarantee any specific order of records returned unless you specify an ORDER BY. That's by design. It's integral to the relational model. If you need to know the order of records, you *must* store a field that preserves the order of records.

Frankly, I would process your data file before it's ever imported into a database. SQL is a poor choice of language to do this because SQL assumes that records aren't related to each other. This type of operation is much easier with a general purpose scripting language. It should be relatively easy with Powershell and either `Import-Csv` (if it's a CSV file) or `Import-Excel` from the ImportExcel module on PowerShell Gallery (if it's an .xlsx file).

Let's say the file is a CSV for simplicity's sake. This should work in any recent version of Powershell:

    $PathToImportFile = '...'
    $PathToExportFile = '...'
    $ID = 'NOID'
    Import-Csv $PathToImportFile | ForEach-Object {
        if (-not [String]::IsNullOrWhiteSpace($_.ID)) {
            $ID = $_.ID
        }
        else {
            $_.ID = $ID
        }
    } | Export-Csv $PathToExportFile -NoTypeInformation",9.0
g11yuz7,i7ap20,If you're going to be using this in Power BI there's some transformation wizards that do exactly this.,2.0
g11bbkf,i7ap20,Performing some update statements with the LAG function. Keep updating the first null row with the LAG of the previous?,8.0
g13207h,i7ap20,"For this you need ordering, and this data is, apparently, unsortable in sql.",3.0
g14lbiq,i7ap20,Oh. Scratch that then. Need to order it in the OVER.,1.0
g10qkg8,i7ap20,"How much data are you dealing with? I think this can be done, but I don’t have an SQL solution off the cuff. Easiest way that I see to do it at this time is to drag bro an excel spreadsheet - provided this is somewhat of a one-off task. 

If it is something you are doing more than once, I’ll see what I can come up with. Probably just some spin on ISNULL.",5.0
g10rr9x,i7ap20,"this is a small amount of data since it comes from a fleet management report, since this is part of a future BI project, I think I still need to figure out ETL. So no, unfortunately not a one-off task",2.0
g18c3oa,i7ap20,"In Excel, super easy to do, you can do a simple formula to do this.  You'd also want to delete the rows with just I'd too.",1.0
g116m0z,i7ap20,"You could use the recursive CTE method as described in the other post, but I would suggest you first see if it is possible to change the output of the report so that you have the ID filled out for every row. In the current format there is no relation between the ID and the date fields other than the order of the rows, which isn’t a very good data model.",4.0
g11ells,i7ap20,"you can so this easier in excel in a new column with an if function, the trick is to compare the value form the id column with the value of the cell above the current one in the new column.",4.0
g11wf5m,i7ap20,Try the LAST_VALUE() function.,5.0
g12fdau,i7ap20,"I would fix that in excel, don't even need vba, just select the first column, press ctrl + g and from there select blanks. Then in the first blank cell write = a1 for example and press ctrl shift enter",3.0
g11xjod,i7ap20,"if you can preprocess as many mentioned with Excel, that's great. If not, Python+Pandas should help immensely. You can use ffill method of [pd.fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html). If you're loading manually, you could probably bypass that too with Python (one script to clean + push data into DB)",4.0
g11gpo2,i7ap20,"Lag, ignore nulls",2.0
g11mfez,i7ap20,South African vehicle number plates?,2.0
g13bsmw,i7ap20,Yup looks that way plus the dates in a proper format /s,2.0
g11xgnr,i7ap20,"I just had this problem and it was a nightmare for large data sets in terms of performance.

Create a function that returns a table where you pass the ID and a date, or date time stamp to it, then use a TOP 1 where ID = ID and Date &lt; Date.

Then access your function using an OUTER APPLY.

Then write a view that ties the function together with all of the possible dates you have per ID, which you can establish using a cross join and a dates table.

Easy peasy, lemon squeezy.

edit: [Here](https://www.reddit.com/r/SQL/comments/hwmy1f/ms_sql_pretty_sure_this_can_be_solvd_with_a/) is my post on the topic. [This](https://tomaztsql.wordpress.com/2018/08/05/filling-propagading-empty-values-with-last-nonnull-value-using-t-sql/) solution provided some initial hope but became inefficient when working with large sets of data so we settled on the solution I described above. Works very fast for a hypothetical set of 1B rows where there are 90% of the rows that are null and need to be filled in.

Do NOT use a recursive subquery or a LOOP of any kind if you are working with fairly large sets of data, or if you expect them to ever grow that way. This problem can be solved many ways, and you will pay for your sins when things grow. The nice thing about the approach I described is that its all a VIEW populated by a function so you aren't storing the data as a table, you're ust accessing it as you need it and adding indexes, or base tables, to make that an efficient process.",2.0
g12dski,i7ap20,"You're getting it in Excel.  You may as well finish your ETL there before moving it around.  

      Private Sub ProcessID()
        Dim wb As Workbook, ws As Worksheet
        Dim i As Long
        Dim xlEndRow As Long, xlCol As Long
        Dim rng As Range
        Dim strID As String
        
        
        Set wb = ActiveWorkbook
        Set ws = wb.Sheets(""Sheet1"") 'Replace with your sheet name
    
        xlCol = 2 'Pick a column that will always have data in your bottom row  
        xlEndRow = ws.Cells(Rows.Count, xlCol).End(xlUp).Row
        
        For i = 1 To xlEndRow
            Set rng = ws.Range(""A"" &amp; i)
            
            If Len(rng.Value) &gt; 0 Then
                strID = rng.Value
            Else
                rng.Value = strID
            End If
        Next i
    
       'Below this is a little extra. Once you've joined your IDs to your Data, you don't need the ID rows, this steps backwards through the table and removes them.  If you have a larger dataset there are other more efficent methods. 
    
        For i = xlEndRow To 1 Step -1
            Set rng = ws.Range(""B"" &amp; i)
            
            If Len(rng.Value) = 0 Then
                rng.EntireRow.Delete
            End If
        Next i
        
    End Sub",2.0
g12jsgx,i7ap20,"Check this out. It might help.

https://www.learningsql.com.mx/t-sql/using-first_value-and-last_value/",2.0
g10sw9a,i7ap20,"Can be achieved using recursive cte. But that again depends on number of records. If it exceeds 100 you can use max recursion option on your query. 
It goes like this. I'd suggest you have some sort of id column added to your data. The query I'm giving is based on the assumption that the data is already stored in the way you presented. 


    ;WITH CTE AS(
        SELECT ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) AS newID, *
        FROM TABLE
    ),
    CTE2 AS (
        SELECT CTE.ID, CTE.STARTTIME, CTE.ENDTIME
            , 1 AS GROUPID
        FROM CTE 
        WHERE newID = 1
        UNION ALL
        SELECT CTE.ID, CTE.STARTTIME, CTE.ENDTIME,
            IF Id IS NULL THEN GROUPID
            ELSE GROUPID + 1
            END AS GROUPID
        FROM CTE
        INNER JOIN CTE2 
        ON CTE.newId = CTE2.newId - 1
    )
    Select id, starttime, endtime, groupid
    From CTE2 
    Option (maxrecursion 0)

You will get all nulls below an id with same GROUPID. 
You can segregate them with GROUPID. 
If you don't want to run the same query multiple times, you can store it into a temp table and query the temp for different id, GROUPID.",5.0
g11yxxo,i7ap20,"Note that `CTE` here is 100% non-deterministic. There is *absolutely nothing* here that guarantees any order of records returned. This may return results which appear to be accurate, but there is nothing at all guaranteeing accuracy.

In other words, this is a *bad* solution.",16.0
g111esm,i7ap20,Recursive CTEs are awesome,1.0
g10t4ew,i7ap20,"Will do, Thank you very much",0.0
g119x4r,i7ap20,"If you’re pulling this into Excel or Power BI, power query has a nifty fill feature that will do just that. If you’re stuck in sql, then the window functions described eloquently in other posts will be your best bet. Ideally, you would find a way to fix the data at the source.",1.0
g11xkrm,i7ap20,What do you mean stuck with SQL? Go stand in the corner.,4.0
g125e7u,i7ap20,"Before uploading into db, open in Excel &gt; select first column &gt; home tab &gt; fill down",1.0
g12b1yg,i7ap20,"Ive had a similar  issue before, this is what i did. Right click the table and edit top 200 rows, then update them manually. 

Sql server doesnt have to keep the data in that order when performing retrieval or updates. So you have to be pretty creative with your conditions to do this programmatically. People have showed some examples already.",1.0
g12pdlz,i7ap20,You guys have been of great help thanks!,1.0
g134kby,i7ap20,"If it comes in Excel, just build an Excel template with Power Query to pull in the report, fill down the column and then export that to SQL database.",1.0
g134kf9,i7ap20,"Add an identity column to the table, it will automatically create the unique index column that you need to order by - but as all the others say, better to have this in the excel directly.
When you finish this share with us what solution you found :)",1.0
g13zh2x,i7ap20,"Hello guys just a quick update. I did a power query in excel as lots of you suggested, and what  do you know it worked like a charm, took my about 10 minutes to figure it out. You guys are the best!!",1.0
g18bew8,i7ap20,"Super easy to do with Excel, a real pain to do in SQL. If there was any way to do a mass update with a CSV file, I'd try and do that, or go back to your old files, fix them, and reupload them to a new table. The last thing I'd try and do is do it in SQL. 

But you could do a loop with a counter, and do a case when if that field is empty, subtract 1 from the counter, copy that info, and update the field. Then do a case when if it it already has a value, do nothing. 

Do that in a temp table, verify the results before updating your main table.",1.0
g11o2hm,i7ap20,Google “sql flash fill” for ideas.,0.0
g10u5vn,i7a8tw,"There is a difference between what is pure and theoretical and what is practical from a business or performance point of view. To get to a pure normilization for some databases you end up with a column that is basically some sort of unique key whether it is a next number or some type of GUID. Some of the easiest databases to work with just use this as the identity column for every table regardless of the data within each table.

I work mostly with commercial applications and they cheat when it provides a performance gain. For the most part every list of values ends up in one table with something within to uniquely identify each value. But in the application when you can choose multiple values instead of having a separate table to show this relationship, they will just concatenate the unique keys from the list of value tables in such a way as to quickly uncat the data for display.

I accidently started a flame war on an old news group for MS Access around this topic with one person holding out to the bitter end claiming 4NF or at least 3NF or die. His point is that NULLS were never allowed. If you have NULLS then you ahave a poor table design was his position as data should only be entered when you know the value. I rarely see a table in a commercial application that does not have at least one field where NULLS are allowed. 

I am not defending what I see in commercial apps but it is what it is. Take it for what it's worth.",3.0
g11671t,i7a8tw,"TLDR: Normalization is based on transforming values in a set, and when it's not reversible there are bugs

&gt; In some cases, a non-BCNF table cannot be decomposed into tables that satisfy BCNF and preserve the dependencies that held in the original table.

The key thing you're missing is the

&gt; and preserve the dependencies that held in the original table.

It _is_ possible to make a relation be in BCNF, but it is possible that doing so causes a computational error that is not addressed at all by normalization.  If you look at [the example in the BCNF page](https://en.wikipedia.org/wiki/Boyce%E2%80%93Codd_normal_form#Achievability_of_BCNF), the first table is in 3NF, and the second set of two tables is in BCNF.  You can derive the original 3NF relation from denormalizing the BCNF value, like:

    SELECT snp.Person, Shop.""Shop type"", snp.Shop
      FROM Shop JOIN ""Shop Near Person"" snp ON snp.Shop=Shop.Shop;

and these tables meet the strict criteria of 3NF and BCNF and have not lost information.  The problem is that they introduce a bug, which is that you can supply values which, when you decompose them, make the result no longer in 3NF.  The point of the 1979 article on which this section is based (the 4NF is from 77, BCNF is from 74) is that normalization starts with a set of values and applies set math to the values that exist, but does not account for what happens when you use the tables in the real world thereafter.  On the Wikipedia page, the second version of the tables allows you to subsequently add a second optometrist shop to the ""Shop Near Person"" table because the ""Shop Type"" is no longer part of that table's candidate key.  So, even though the data that existed at the starting point in 3NF can be broken down into BCNF (and subsequently into 4NF if appropriate, although that doesn't apply to the example dataset there), the actual use of the resulting schema can produce results which, when re-composed, violate those forms.

In reality, this will happen very infrequently/never.  For example, the case shown in the Wikipedia article is because the relation contains an unexpressed relationship, namely the location of the Person and the Shop and the dependent calculation of ""nearest"".  The ideal way to represent this data is something more like:

    CREATE TABLE person(name VARCHAR2, longitude NUMBER, latitude NUMBER);
    CREATE TABLE shop(name VARCHAR2, type VARCHAR2, longitude NUMBER, latitude NUMBER);

...and then you use some geospatial operators in your DBMS or some math in your software to join these two tables and calculate a distance between each, which you essentially window over by person.name and shop.type taking the MIN for that partition.  This is in BCNF and has none of the problems that come from storing the nearest shop alongside the person.",2.0
g11684l,i7a8tw,"**I found links in your comment that were not hyperlinked:**

* [person.name](https://person.name)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g11671t&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1.0
g119yrh,i7a8tw,Bad bot,1.0
g1578jx,i7a8tw,Many thanks for your thorough explanation!,1.0
g10pjmm,i7a8tw,"&gt; a set of functional dependencies {AB → C, C → B} cannot be represented by a BCNF schema

i'm going to blame my lack of sleep and overcaffeination if im wrong, but why cant you normalize this as  {AC, C-&gt;B}(where AC relation has both AC as the key and no other attributes)?",1.0
g10zb9j,i7a8tw,The examples on the BCNF page OP linked to walk through an example where this is not possible.,1.0
g10npnd,i79ydq,"For a web based tool SSRS is your best bet. If all you have is SQL Server and Excel then I suggest downloading  Powerquery for Excel and using Powershell scripts to automate e-mailing the spreadsheets to people.

So the basic work method is develop your query in SSMS.

Copy and paste into PowerQuery in Excel.

Save your spreadsheet.

Use Powershell to automate opening and refreshing your spreadsheet, saving it and then e-mailing it.

If you can make a business process powner own an e-mail group and instead of sending this to an ever expanding list of people just send it to a group and let the business person decide who si in or out of the group.

This is one way to do this process.",2.0
g10o8m5,i79ydq,"Thanks for your help! We use Crystal reports which is a nightmare but maybe I can make an argument for SSRS. I did not think to automate it with powershell, but that's a great idea!",1.0
g11c7a3,i79ydq,"Converting from Crystal to SSRS is a major pain, but worth it to get out of the torture that is Crystal.

Suggestion: Pitch a pilot project w/ SSRS. Build out a proof of concept. Demonstrate the things that Crystal can't do, or makes really difficult. And don't forget to point out the cost savings - you've already paid for SSRS if you're using SQL Server!

Once people are on board, start migrating reports as changes to them are requested, and any new development starts exclusively on SSRS.

The idea is to have things migrated, or mostly migrated, before your next upgrade or license renewal for Crystal.",2.0
g14eks5,i79ydq,"Unfortunately our ERP system relies on Crystal, so doing away with it isn't an option.  I was however able to quickly demonstrate the capabilities of SSRS and had a few reports deployed that day. I appreciate your insight!",1.0
g10rc70,i79nki,"get min([date]) first, then use this as derived table/subquery to do counts

      select count( distinct t.min_date)
      from ( select min([date]) from .... ......
              )t",1.0
g11edte,i79nki,"SQL Query (Oracle):

SELECT COUNT( DISTINCT CREATE_DT) FROM SAMPLE_TABLE WHERE CREATE_DT= (SELECT MIN(CREATE_DT) FROM SAMPLE_TABLE);

Now if I understood your question correctly you want to count the number of distinct records in your table where the date is the minimum value.
MIN() will always return a single value. Hence I assumed that if you want the minimum value then it must the distinct number of rows that you are looking for this date value that you found using the min function.",1.0
g10j5vb,i78m2o,"I recommend you to go through its documentation, that is pretty explanatory itself.",1.0
g10atny,i77qui,"I would say no, so long as we are assuming that both tables only hold a single value. That sort of join is called a Cartesian Product, which is explicitly defined as the product of two sets.  
If you want to take the lunch-break long-road with me, consider 2 scenarios: 

1) every element of Emp maps to every element of Dept. It stands to reason that for this to be true, the cardinality of DeptNo in each table is 1. For each DeptNo in Emp, every DeptNo in Dept matches, and for each DeptNo in Dept, every DeptNo in Emp matches. Therefore, all elements in both tables must be equal. So for each row in Emp, we return every row in Dept - the Cartesian Product.  
2) Assume otherwise - there exists at least one element in Emp that maps to a subset of Dept. It then follows that there is a complimentary subset of Dept that this element does not map to. Suddenly, we have a lower number of rows.  


So if we imagine an example where the number of rows is greater than D\*E, then there must exist at least one row in D that maps to at least one row in E more than once, which would violate the initial assumption that both tables only store singular values.",2.0
g10ekjv,i77qui,Thank you for your answer with a proof!,3.0
g10cf43,i77qui,"The join above can be converted to 

 `SELECT * FROM Emp e INNER JOIN Dept d ON e.deptno = d.deptno`

Having said that, this is not a cartesian product. It may result in duplicates, but it's not a full cartesian product in the sense of a cross join.  And since a full on cross join would produce at max d\*e, there is no way the above would produce more than that.   


As for the possible answers for a and c, it depends on the assumed normalization/constraints on the system. Are there FK relationships? Is d.deptno a primary key(unique) and is the relationship enforced, aka not able to have deptno in emp that doesn't exist in d?

If it is a properly constrained system:

the answer to A would be Yes, not no, as there would be records in both tables, indicating relations exist, so records would be returned.

The answer to C could be yes, if there were only 1 department, and all employees were assigned to the single department. Otherwise the answer would be no, as the join would eliminate non-matching rows.",1.0
g10ehsq,i77qui,"Thanks for your answer!

I was also wondering about keys etc, but I came to the conclusion I cannot assume what the question did not tell me. For this reason I voted for a 'no' in a).

I hate exams, in part because of stuff like this.

How do I interpret ""pessimistic time complexity""?? To my understanding (which may be wrong), when this phrase is used, the exact algorithm and constraints in use are FIXED and KNOWN, only the data and/or the state of the random number generator (if any is used) is not known. So, if stuff like constraints, primary and foreign keys, indexes, etc, are not known (as they question does not tell them), then we CANNOT ask about pessimistic time complexity, since this question cannot be answered without the information about keys etc. In particular, depending on such constraints, indexes, functional dependencies, etc, perhaps pessimistic time complexity is actually reduced to something logarithmic or even linear?",1.0
g10f52w,i77qui,"&gt; But I'm not sure what to do with b). Can this query ever return more than E ⋅ D rows? 

no it cannot. ""where"" can only restrict (reduce the number of records) the result set, the result of ""from emp e, dept d"" is a cartesian product (D*E rows).",1.0
g1011iz,i75ri3,"If you have one year of extensive MySQL 8.0 under your belt I would suggest trying for the exam.  That would be if you were doing serious development day in and day out my MySQL 8.0 and really knew your stuff.  General knowledge will greatly help your future prospects and I really like your reading list.  But I would be hesitant to recommend that you take the exam now.

Why? I do not think you have enough MySQL knowledge to pass the exam that is specific to MySQL  [https://elephantdolphin.blogspot.com/2020/07/mysql-80-certifications.html](https://elephantdolphin.blogspot.com/2020/07/mysql-80-certifications.html) 

The MySQL 8.0 DBA and 8.0 Developer exams are very tough. You need to be 'up to your elbows' in the guts of MySQL and know how to not only diagnose problems but fix them.  These are not for basic SQL skills.",4.0
g105q7k,i75ri3,"This is very helpful. I am using Google's BigQuery as the daily driver and I still need to learn so much more. 

I will practise and learn more in depth as you have suggested that the exam isn't a cake walk. The blog post increased my enthusiasm and scared me too. I am reading the books like a novel one by one in the order mentioned as they are more like discussions and mindset to approach data. 

Do you have any suggestions for hardcore/advanced MySql and Database Processing, guides/books?

THANK YOU SO MUCH FOR THE HEADS UP.",1.0
g10f0lr,i75ri3,Jesper Wisborg Krogh's MySQL 8.0 Performance Tuning is a great book and I highly recommend it,1.0
g10fxlq,i75ri3,Thank You :),1.0
g0zn41j,i74c4h,Do the tables already exist in the database? The script will execute in order and throw up that error at the first few lines if those tables don’t already exist to drop.,5.0
g0zpodv,i74c4h,"Employes table references on Departments, which does not exist yet. 
So your first run failed on this table. 
Second run of this script fails on droppring unexisted table Department.

You need  CREATE tables without constraints and add them afterwords
With
Alter table add constraint...

Also you can make script rerunnable if you convert drop table into plsql block where you can catch exception that table does not exists ( or select from user_tables to check if it exists",2.0
g10ex68,i74c4h,"I have altered the tables to add the foreign keys however now I have a multitude of other errors:

SQL Error: ORA-02449: unique/primary keys in table referenced by foreign keys
02449. 00000 -  ""unique/primary keys in table referenced by foreign keys""

And

SQL Error: ORA-00955: name is already used by an existing object
00955. 00000 -  ""name is already used by an existing object""",1.0
g10to3n,i74c4h,"You get ora-2449 When you drop table with primary key when you have child table with foreign key pointing to this prinary key. Oracle does not allow to do it because he does not know what to do with references. 

You should drop constraints first. Or drop tables which do not have any children. In your case order of dropping is

JobHistory
Employess
Departments
Locations
Countries
Jobs


I would suggest to google what is ERD diagram. It will help to model data and clearly define order of deployment.",1.0
g10yhvj,i74c4h,"I understand, however I removed the foreign keys from the tables and after creating tables i added the alter table add constraint which threw those errors, I rearranged slightly the order of dropping as you recommended also. Should this not have rectified the above issues",1.0
g110ot9,i74c4h,Could you provide the latest version of your script ?,1.0
g114k7y,i74c4h,"
DROP TABLE JobHistory;
DROP TABLE Employees;
DROP TABLE Departments;
DROP TABLE Locations;
DROP TABLE Countries;
DROP TABLE Jobs;

CREATE TABLE Jobs(
    job_id INTEGER,
    job_title VARCHAR(20),
    min_salary INTEGER,
    max_salary INTEGER,
    PRIMARY KEY (job_id)
    );  
    
CREATE TABLE Countries(
    country_id INTEGER,
    country_name VARCHAR(20),
    PRIMARY KEY (country_id)
    );
  
CREATE TABLE Locations(
    location_id INTEGER,
    street_address VARCHAR(40),
    postal_code INTEGER,
    city VARCHAR(20),
    state_province CHAR(3),
    country_id INTEGER,
    PRIMARY KEY (location_id)
    ); 

CREATE TABLE Departments(
    department_id INTEGER,
    department_name VARCHAR(20),
    manager_id INTEGER,
    location_id INTEGER,
    PRIMARY KEY (department_id)
    );
    
CREATE TABLE Employees(
    employee_id	INTEGER,
    first_name	VARCHAR(20),
    last_name	VARCHAR(20),
    phone_number INTEGER,
    hire_date	DATE,
    job_id	INTEGER,
    salary INTEGER,
    department_id INTEGER,
	PRIMARY KEY (employee_id)
 );

CREATE TABLE JobHistory(
    employee_id INTEGER,
    start_date DATE,
    end_date DATE,
    job_id INTEGER,
    department_id INTEGER,
    PRIMARY KEY (employee_id, start_date, end_date)
    );
    
    ALTER TABLE Locations
    ADD FOREIGN KEY (country_id) REFERENCES Countries(country_id);
    
    ALTER TABLE Departments
    ADD FOREIGN KEY (manager_id) REFERENCES Employees(employee_id);
    
    ALTER TABLE Departments
    ADD FOREIGN KEY (location_id) REFERENCES Locations(location_id);
    
    ALTER TABLE Employees
    ADD FOREIGN KEY (department_id) REFERENCES Departments(department_id);
    
    ALTER TABLE JobHistory
    ADD FOREIGN KEY (employee_id) REFERENCES Employees(employee_id);
    
    ALTER TABLE JobHistory
    ADD FOREIGN KEY (job_id) REFERENCES Jobs(job_id);
    
    ALTER TABLE JobHistory
    ADD FOREIGN KEY (department_id) REFERENCES Departments(departments_id);",1.0
g11drbs,i74c4h,"    begin 
      for rc in (select * from user_constraints 
                   where constraint_type ='R'
                   and table_name in ('JOBHISTORY','DEPARTMENTS','EMPLOYEES','LOCATIONS')) loop 
        execute immediate 'alter table '|| rc.table_name||' drop constraint '|| rc.constraint_name; 
      end loop; 
    end; 
    /
    
    DROP TABLE JobHistory;
    DROP TABLE Employees;
    DROP TABLE Departments;
    DROP TABLE Locations;
    DROP TABLE Countries;
    DROP TABLE Jobs;
    
    CREATE TABLE Jobs( job_id INTEGER, job_title VARCHAR(20), min_salary INTEGER, max_salary INTEGER, PRIMARY KEY (job_id) );
    CREATE TABLE Countries( country_id INTEGER, country_name VARCHAR(20), PRIMARY KEY (country_id) );
    CREATE TABLE Locations( location_id INTEGER, street_address VARCHAR(40), postal_code INTEGER, city VARCHAR(20), state_province CHAR(3), country_id INTEGER, PRIMARY KEY (location_id) );
    CREATE TABLE Departments( department_id INTEGER, department_name VARCHAR(20), manager_id INTEGER, location_id INTEGER, PRIMARY KEY (department_id) );
    CREATE TABLE Employees( employee_id INTEGER, first_name VARCHAR(20), last_name VARCHAR(20), phone_number INTEGER, hire_date DATE, job_id INTEGER, salary INTEGER, department_id INTEGER, PRIMARY KEY (employee_id) );
    CREATE TABLE JobHistory( employee_id INTEGER, start_date DATE, end_date DATE, job_id INTEGER, department_id INTEGER, PRIMARY KEY (employee_id, start_date, end_date) );
    
    ALTER TABLE Locations   ADD CONSTRAINT Locations_FK1    FOREIGN KEY (country_id) REFERENCES Countries(country_id);
    ALTER TABLE Departments ADD CONSTRAINT Departments_FK1  FOREIGN KEY (manager_id) REFERENCES Employees(employee_id);
    ALTER TABLE Departments ADD CONSTRAINT Departments_FK2  FOREIGN KEY (location_id) REFERENCES Locations(location_id);
    ALTER TABLE Employees   ADD CONSTRAINT Employees_FK1    FOREIGN KEY (department_id) REFERENCES Departments(department_id);
    ALTER TABLE JobHistory  ADD CONSTRAINT JobHistory_FK1   FOREIGN KEY (employee_id) REFERENCES Employees(employee_id);
    ALTER TABLE JobHistory  ADD CONSTRAINT JobHistory_FK2   FOREIGN KEY (job_id) REFERENCES Jobs(job_id);
    ALTER TABLE JobHistory  ADD CONSTRAINT JobHistory_FK3   FOREIGN KEY (department_id) REFERENCES Departments(department_id);",1.0
g11eazo,i74c4h,"you have a circling referencing constraints:

Employees references to Departments 

Departments references to Employees.

that's why you need to drop constraint explicitly or you can use this instead:  


    DROP TABLE JobHistory   CASCADE CONSTRAINTS;
    DROP TABLE Employees    CASCADE CONSTRAINTS;
    DROP TABLE Departments  CASCADE CONSTRAINTS;
    DROP TABLE Locations    CASCADE CONSTRAINTS;
    DROP TABLE Countries    CASCADE CONSTRAINTS;
    DROP TABLE Jobs         CASCADE CONSTRAINTS;",1.0
g1001l0,i72zmx,The suggestions seem to ignore the information you can get from the Sys schema.  You would probably be better off buy AND READING a copy of Jesper Wisborg Krogh's  *MySQL 8 Query Performance Tuning: A Systematic Method for Improving Execution Speeds*,2.0
g0z5vq8,i7005e,"Have you decided on a particular database to work with? Microsoft, Oracle and MySql have good market presence. I'd suggest looking at job listings for your locality and in particular if you have identified an employer you would like to work for and pick one.

Once you have selected a database download a developer's copy and start using the included datasets. (There are also free cloud based deployments available)

SQL is reasonably standardized so moving from one to another is usually no big deal. However many databases also have a procedural language which is specific to the database. T-SQL for MS Sql Server and PL/SQL for Oracle. Once you have the hand of SQL you can move on to the procedural language. If you have any general coding experience and SQL knowledge these are not difficult to pick up.

If you still don't know where to go I'd suggest Oracle or MS SqlServer as they have lots of online learning resources. Also the certifications are well recognized by employers.",3.0
g0z7e82,i7005e,"Thank you, this was very informative and has given me some well needed direction. Much appreciated",2.0
g0z62re,i7005e,"Since you mentioned MSSQL as a flair, I 6hink 70761 is good for showing you know basic t-Sql

You can also look into other MS certs. You can take these from home through Pearson Vue. Please note that employers may not care about your certifications at all. These are nice to show that you care and that you know the bare minimum. It may give some edge in interviews",3.0
g0z79vs,i7005e,"Thank you, I will definitely check that out",2.0
g0zfweq,i7005e,"Those Microsoft certs are being retired in 6 months or so. Unless you plan on completing them in the next couple months, focus time/effort/money on the replacements.",2.0
g1777dc,i7005e,"Definitely, sorry I forgot to mention that. I think they expire on 31st Jan 2021",1.0
g0xixeg,i6rh4n,Where you need to do a lot of writes quickly and don't really need to do joins.,44.0
g0yx13t,i6rh4n,you can do that with SQL too. Normal SQL databases perform good on the official TPC benchmarks.,1.0
g0yxlnx,i6rh4n,"Benchmarks don’t paint the whole picture. Relational Databases are definitely able to handle similar loads and provide equal performance but each use case is different. Therefore you need to measure the specific requirements for each one and then decide.

E.g: if writing a small document like a Json away just to serve it as it is later on and scale this to millions of such requests and serves, a write optimized store like dynamo or Cassandra is hard to beat in terms of throughput. Especially considering the hardware cost to run the same scale.

But: only because write load is an issue in a use case  does not mean NoSQL wins straight out.

Edit: see /u/BrentOzar answer for an example",3.0
g0z7m1h,i6rh4n,"The preferred way to interact with cassandra nowadays is CQL though (I think thrift is deprecated), which is basically SQL and even supports joins etc.

Sure, it doesn't fully support any SQL standard (which database does...) and isn't an RDBMS in the traditional sense, but you can still use a subset of SQL meaningfully instead of inventing your own querying language.

Similarly there's a JDBC adapter for mongodb, so that you can query mongodb using SQL instead of their own query language",1.0
g104jxr,i6rh4n,Personally I don’t define NoSQL as something that DOES NOT have an access layer similiar to ANSI sql as it feels like NoSQL describes a family of tools that deviates away from the traditional relational model. But you are right that Cassandra would be a bad example in a sense that it’s not as “NoSQL” as others.,2.0
g1374p1,i6rh4n,"Yeah, I mean, the definitions of ""NoSQL"" I've heard are ""No SQL"" and ""Not Only SQL"", but cassandra is pretty much only SQL these days so ¯\\\_(ツ)_/¯.",2.0
g13bxf7,i6rh4n,It feels to me like we should stop using the term overall :D but nobody asks me I guess :),2.0
g13c9hi,i6rh4n,agreed,2.0
g0zk5z6,i6rh4n,"You *can*, but there are use-cases where NoSQL is superior. For example, for tracking metadata on a streaming platform, where you need to validate certain orders of operations. Ex: pipeline B can't process a record until pipeline A has processed a parent record. Pipeline A can write a timestamp to NoSQL and that timestamp existing can be a prerequisite for pipeline B.  
  
NoSQL has the benefit of extremely fast writes and lookups *at scale*, and you can create new metadata flags like the ex above at will, no `alter table` required. I use NoSQL in parallel to our relational database for lookups to avoid tying up resources.  
  
So for reporting and normal SQL stuff, I always thought NoSQL was a lazy way for devs to avoid learning about data (I still do!). But in the ETL/Data Engineering space, especially for streaming/'big data', it's extremely useful.",1.0
g0xjepp,i6rh4n,"Our [PasteThePlan.com](https://PasteThePlan.com) lets you store execution plans in the cloud, for free. We don't need to do joins between rows, and we don't do any processing on the rows. We just take a key (the URL) and return a value (the location on disk (S3) of the plan). We chose to use Amazon DynamoDB to store the keys &amp; values because it's way, way cheaper than a traditional relational database system.",50.0
g0xsnm4,i6rh4n,BrentOzar! The legend! I have used your Whois and Blitz tools for some years now! Blessings upon you sir!,27.0
g0y76pt,i6rh4n,"Thanks, I appreciate it!",10.0
g0xkalx,i6rh4n,"From the development side, Mongo is easier. The DB just accepts new fields and writes quick. Whereas relational would require multiple schema changes. The downside is moving that data in to a EDW that is likely relational. I've worked with 20+ years exp SQL guys that refuse to learn Mongo.",13.0
g0xmf3g,i6rh4n,"Ok I get that it can do injestion of unstructured data,  but if you need to run queries against unstructured data ,you may not get valid results... I'm not looking to refuse to learn I just really want to hear a good use cases, where I should reach for noSQL first",9.0
g0xnoh4,i6rh4n,"You have to know what data you're looking for. What do you mean by valid results? I've written bad SQL queries that have given invalid results. Chances are, I'll probably do it again at some point.",7.0
g0xugcs,i6rh4n,"Relational model requiring a new field is literally just alter table create column, that's it.",3.0
g0y0no0,i6rh4n,"Sure a simple field addition is an alter table but that may require a different person, like a DBA. That field may be across a dozen tables or more likely a lookup table is created and a key is in a bunch of other tables. 

Then I think special stuff happens when the field is an index or constraint because it has to rebuild (I think there's another name for it?) the table. It may require more downtime. 

I am just thinking about my past experiences, other places may have their entire process streamlined.",4.0
g0yazkv,i6rh4n,"If it needs an index is not a simple addition for nosql either.

If it needs a dba, that's not a technical issue.

A dozen tables, a lookup table - nosql doesn't fix that.

There is a general predisposition that relational is wrong/worse/less efficient, and I think it's false.",3.0
g0ydxfg,i6rh4n,"Back on our C#/SQL Server stack DBAs did schema changes where they aren't required with a MEAN stack. Devs just deploy everything now. A document in nosql wouldn't need a lookup table. The data could just be in the document. I've seen collections like a lookup table but I've also seen it just in the transaction document so it wouldn't have to query another collection again. 

I definitely don't think relational databases are worse, I prefer them but not every company uses them exclusively.",1.0
g0ygru0,i6rh4n,"&gt;  Back on our C#/SQL Server stack DBAs did schema changes where they aren't required with a MEAN stack. Devs just deploy everything now. 

No reason you can't do that with relational.


&gt; A document in nosql wouldn't need a lookup table.

So why does the relational ?",2.0
g0yljfk,i6rh4n,"Meaning the DBAs don't have any schema changes because there wouldn't be any for Mongo. It's all just done in code.

I assume the relational table is normalized so it would have lookups. Some DB2 dbs just put everything in one table and it's difficult to navigate with self joins and such. 

I think this is off topic for OP's question but it was a good chat.",1.0
g0z80xc,i6rh4n,"You can also just ship your schema change with your code for relational databases, there's different approaches of managing migrations. No need to have DBAs in your organization either. I agree though that they're more explicit in traditional RDBMSes.

I also think that's broadly a good thing, because your database ends up being very messy and fragmented, and doing any analytics on it becomes extremely challenging and full of gotchas if you don't keep your data integrity high.

For instance consider a document store where you used to have a field ""weight"" that was in kg. Then later you deprecated the field and split up into ""brutto_weight"" and ""netto_weight"", both in kg. Then later you decided that you needed to accept other units of weight as well, so you added another field ""weight_units"" which is e.g. ""lb"", ""kg"" or ""stone"".

Now imagine someone trying to do analytics on this, to answer some questions like ""how much weight did we handle in total?"" ""what's the average weight of a package shipped from country A to country B?

You can easily see how anyone who wasn't there the whole time would have a 99% chance to get the wrong answer to all those questions.

Synthetic example, but this stuff does play out like this, I've seen it many times. It can still be an issue with SQL/RDBMSes of course, but they give you more tools to sustainably maintain a clean data model",1.0
g0zl7zt,i6rh4n,"It's easier for the devs, but it's a nightmare when the business tries to solve problems with that data and there's no relational structure.  
  
It *can* be set up correctly with objects in the proper parent/child grain, but if a Dev knows how to do that, then they understand relational data and wouldn't be using Mongo ""because it's easier"". Relational data requires a relational solution.  
  
Edit: That said, your answer is correct; that is why NoSQL gets used most often. Whether that makes it a 'good' use case is another story...",2.0
g0y98vf,i6rh4n,How much quicker tho? I’m an Oracle DBA. It’s been a looooong time since my commits or alter statements haven’t been almost instantaneous. Sell me on this please I’m begging cause all I see is “it’s faster writing” or “it’s easier to develop on”. I manage massive databases for massive companies on a SaaS or DBaaS service and I still don’t see the benefit of going NoSQL over relational.,3.0
g0ygrlk,i6rh4n,Hijack question! Why would I ever us oracle over Postgres or MySQL?,2.0
g0yu0oo,i6rh4n,Because you are huge corporation and won't touch anything that doesn't have name of other huge corporation on it? Then you go either Microsoft or Oracle.,5.0
g0zbdr5,i6rh4n,"Because you're vendor locker into Oracle.  Oracle also a query optimizer that MySQL doesn't have which automatically optimizes your queries.  

But lets be real,  Postgres is king.",3.0
g0zxxhc,i6rh4n,"When you think of IT solutions, there are two major expenses: licensing fees and labor costs. Arguably, certain software can allow your developers to create solutions more quickly and reliably than they could without it (think about something like a really good debugging tool.) If this licensing fee is lower than comparable labor cost and increased productivity it allows, it would actually *save* you money. Here's an example discussing using [SSDT in SQL Server](https://www.reddit.com/r/Database/comments/fodzg4/how_much_of_a_concern_should_licensing_costs_be/flfyk7d/).

Additionally, again using SQL Server as an example, it can offer a bunch of different packages which can be useful to developers. A few examples are SSRS, SSAS, and SSIS.

If you're a .NET shop, it may make more sense to pick SQL Server because it interfaces better with .NET.

Lastly, if the talent pool in your area is mostly focused on a particular database, it may make more sense to choose a database that developers are more productive in.",2.0
g0xrl1y,i6rh4n,"As someone who only does analytics, I hate document stores.  The benefit is that they're easy to write into, they'll accept almost anything as long as the insert statement is properly formatted.  But the downside is that they'll accept almost anything as long as the insert statement is properly formatted.  Fortunately, Postgres, which is the database I primarily use, has a lot of fantastic querying capabilities that make it not too painful to convert JSON into a relational structure.  I just spent the weekend cleaning up an enormous document store for a client who was passed essentially a large document store.  Nearly 60% of the database was duplicate data (such as having two points for the same point in time but with a difference of say .000001 in their latitude which is like pinpointing the left side of my monitor vs the right side of my monitor).  

If I had a tno of infrastrcuture that was heavily reliant on having quick writes with no downtime and eventual consistency was acceptable, then I'd possibly use something like Mongo or Cassanda... but then agan, I can use effectively the same paradigm with tables and just not make them relational so maybe not.",11.0
g0xmwjf,i6rh4n,"One use case is for storing data in a complex schema that is really a union of types. Let’s say you’re making a polling app, and you support lots of different question types. For each different question type you support lots of different options, some of them you might nest complex objects as choices, some of them you might embed links, some of them you might include image URLs. Let’s say there’s a lot of potential complexity, lots of options for lots of different question types. 

In your code you might define this as a union type, there’s no real hierarchy, just lots of different types that all count as a Question. 

If you wanted to store this in SQL, you’d need a table for each question type. Or you could have a Questions table but it would have loads and loads of optional fields. Neither of these are great options: if you have lots of different tables then you might need to do a pretty complex join on lots of tables just to present a poll with different question types. If you have loads of optional fields you’re going to have to write custom server logic to parse that back into your union of types, and you could easily introduce bugs that write invalid data.

With NoSQL, you just convert all your types to JSON, and you put them in a list against the poll object in your database. Your programming language probably has a really good library for converting complex types to and from JSON so there’s not much heavy lifting, nor many places to introduce bugs. 

Of course, if your SQL database can store and query JSON fields this becomes much less convincing! But that’s at least a use case where storing something that looks more like JSON than SQL makes more sense, and NoSQL databases are great at storing and querying JSON.",5.0
g0yc1xj,i6rh4n,"If I were doing this in a relational structure, I'd heavily use unary relationships.  That avoids both having many tables as well as having many optional fields.  And, to be fair, if you have a complex workflow you describe, any paradigm you choose will require custom server logic to handle the complexity.  

At least, if Qualtrics and some of the other survey companies APIs are representative of how surveys are done generally. Its a hot mess and takes forever to get something that is easily queryable but once you do its actually a really simple structure. At the end of the day you have questions and answers to those questions.   If it is stored in two such tables with questions and sub-questions belonging to the same table with a foreign key to its parent, it is remarkably trivial to pull out arbitrarily nested data and convert each survey to a single flat file since that, after-all, is what someone utilizing it for analysis would actually end up doing anyway.",2.0
g0xyni1,i6rh4n,"My company uses MongoDB for some stuff but we are mostly on Postgres these days. Much happier to be moving off if it. It was more viable about 5-7 years ago, but today they are dated. Here’s why you might be inclined to use them:

1) in the days before RDS/Aurora/“Managed SQL as a Service” novice admins could build performance and/or reliability oriented clusters very simply. Today cloud providers and advances in open source SQL DBs make this moot

2) You have a write-heavy workload of irregular data that will be queried simply without joins or aggregations 

3) You want a persistent, clustered cache of materialized views in the form of JSON blobs (MongoDB’s original use case) that’s more sophisticated than memcache but aren’t using Redis or just another SQL server because it’s 2011 or something.

4) You go to war with the army you have and that army happens to be JS developers who want to work in JavaScript across the entire stack.",3.0
g0xlf8d,i6rh4n,"As a cache where speed and scalability outweigh consistency.

When the data is not relational.

When you need to persist user settings.

When deployment speed outweighs deployment maturity.

When the external hosting provider doesn't support SQL.

When you have many different kinds of queries and so need to index copies of the data in many different ways.

When the application cannot tolerate downtime.

When single point of failure is very bad for the app.

When you want a simple query API and to write your own implementation.

When the schema is constantly evolving.",6.0
g0xujsk,i6rh4n,[deleted],3.0
g0yc6ah,i6rh4n,Yeah the biggest player in the healthcare space (at least in US) still doesn't even use an RDBMS yet except for their reporting backend.  I've worked at a few hospitals with Epic and they basically run an enormous ETL script that starts at like 8PM and runs until 6PM to get the reporting database in such a state that you can get yesterday's information.,1.0
g0yu6la,i6rh4n,"Sure, if your data is relational you use a relational (SQL) database. (E.g. webshop, finance application.)

But if your data is not relational you could use a 'noSQL' database.

If you want to store 'documents' then Elasticsearch (or MongoDb) would be a better fit. (E.g. aggregating all logs into a central system where they could be searched.)

If you want to log/monitor (performance) metrics, then a time-series database would be a better fit. (E.g. all cpu/mem/disk statistics into Influxdb and graph them with Grafana.)

It's all about choosing the best tool for the job.",2.0
g0yxb7g,i6rh4n,documents can also be stored and indexed in a relational database. They can even be text indexed.,2.0
g0xn7ul,i6rh4n,"How many apis have you written and how complex are your objects? 

When you start getting into objects where they have 12 tables to normalize attributes, child objects and relationships and you have API endpoints that consume the entire object at a time... You'll long to have a 1 for 1 schema and a simple API. 

If that isn't enough, migrations on SQL servers are a pain. If you have a muilti-client system all your clients must upgrade at once. If you version your objects you can have different versions of your API connecting to the same database you can run clients on different versions of your application utilizing the same shared database.

Then you simply have devs that give zero crap about SQL. They don't respect it, they don't consider what will happen to the data when they drop/recreate tables. They don't even think what will happen when they deploy a change to a shared production system and it breaks another clients system.

I say all the above from experience and I love SQL for what it does. 

Object version and using a schema-less document database is great for a transactional database. For analytics, extract transform and load it into RDBMS.

It's the best of both worlds in my opinion.",2.0
g0z8i6y,i6rh4n,"&gt; When you start getting into objects where they have 12 tables to normalize attributes

I think here you either over-engineered or your data is inherently extremely complex. If it's the latter, I don't think using something like a document store will really fix the issue for you, just move the difficulties around a little

&gt; If that isn't enough, migrations on SQL servers are a pain

Actually also true for any other kind of datastore tbh

&gt; If you have a muilti-client system all your clients must upgrade at once

Not necessarily true, you can provide in-database APIs (through views or functions for instance) to work around this issue, just like versioned objects. But fundamentally the problem is the same for any kind of datastore. If I have a GeoJson field in my database and I switch to using WTK, then all my clients will need to start understanding WTK ahead of time, or I can't start inserting WTK data.

There's workarounds like having both fields available for some time etc, but these apply equally to SQL/noSQL

Also, if breaking other clients because of schema changes is becoming a big problem for you, you should very seriously think about whether you might not actually want to give each its own data store and create an API interface between the two or something like that.",1.0
g0xvp6w,i6rh4n,"Definitely for serverless apps on AWS.

Other than that I don’t know!",1.0
g0yuhcj,i6rh4n,The only NoSQL databases I see that have reason to exist are key-value stores for very simple things and graph databases where the data you want to store is an actual graph. It doesn't make sense to squeeze such data into a relational database. It's possible. But it's not the right tool for the job.,1.0
g0z1b37,i6rh4n,"how i see it, in relational databases the cost of the ""thinking power"" is spent on building the structure so it is easy and low effort to work with it latter. in noSQL it's the other way around, it's easy to dump the data but you need to spend the grey matter on retrieving what you want.",1.0
g0zoru0,i6rh4n,"Core use case seems to be when you want to store unstructured data that doesn't really correlate across different users.

For example, maybe a user has custom data that they import, trying to store that in SQL is difficult.

I will say that I personally think JSONB columns basically eliminate this use case, but I guess if it were literally ALL you did you could make the argument for NoSQL.

If you intend to scale, including adding analytics and data teams in general, I really think SQL is the way to go.",1.0
g0xrtf5,i6rh4n,"Distributed databases are much easier to implement as NoSql. Good distributed databases are way more faster while reading and writing, because you are potentialy reading from many machines simultaneously. Distributed SQL is not really a thing except on analytics systems like Hadoop or Spark.",0.0
g0xu9ef,i6rh4n,"It's web scale, didn't you know? https://youtu.be/b2F-DItXtZs

I've yet to find a use case for it either.",-1.0
g0wxq9w,i6ny3n,`select date_column + interval '30 days' from your_table`,3.0
g0wzy67,i6ny3n,"If the value is a `date`, just add an integer, e.g. `current_date + 30`, if the value is a `timestamp` add an interval `current_timestamp + interval '30 days'`

&gt; to check if a product has been ordered between that 30 day window

To find products that have been ordered in the last 30 days, something like this might be what you are looking for:

```
select *
from orders
where product_id = 42
  and order_date &gt;= current_date - 30
```",3.0
g0wyxi1,i6ny3n,"Most SQL instances have some form of date add function you can use


https://www.w3schools.com/sql/func_sqlserver_dateadd.asp",2.0
g0x39uz,i6ny3n,why are you linking a SQL Server tutorial for a PostgreSQL user?,2.0
g0x3gzq,i6ny3n,Because I’m a moron who can’t read,13.0
g0xjav4,i6ny3n,what database engine?,1.0
g0wwix8,i6ny3n,date '2001-09-28' + integer '30’?,-1.0
g0x2g6a,i6ny3n,Yes,1.0
g0x3xg5,i6ny3n,"plus ""integer""  LOLOLOL",3.0
g0x4641,i6ny3n,"plus ""integer""  LOLOLOL",1.0
g0x0ftj,i6ny3n,"I don't understand what your trying to do but hope this helps?.......

If your... PRODUCTDATE = 02/01/2020

DATEADD(D,PRODUCTDATE,30) AS THIRTYDAYWINDOW = 03/02/2020 

DATEORDERED BETWEEN STOCKEDDATE AND THIRTYDAYWINDOW

OR

DATEDIFF(D,PRODUCTDATE,ORDERDATE)&lt;=30

???",-2.0
g0x1v7f,i6ny3n,"I will try this 
Thanks a bunch",1.0
g0x44d9,i6ny3n,i don't think PostgreSQL has this DATEADD function,2.0
g0wy67f,i6n4l8,"I propose you use a stringent enough regular expression that allows for example a leading +, numbers, dashes or spaces. Has at least three digits after the plus.
CHECK (phone ~ '^\+?[0-9]{3}[0-9]*$')


Typing this on a phone so hope it's at least almost parsing and properly working solution. See https://stackoverflow.com/questions/35822354/how-to-use-regex-with-postgresql-to-constrain-a-columns-input-as-alpha-numeric/35822507",3.0
g0x70nm,i6n4l8,"```sql
alter table the_table 
  add constraint check_phone
  check (phone_number ~ '^[0-9]{8,10}$');
```

This will only allow digits. It will enforce a length between 8 and 10 characters. If you want other characters as well (e.g. allow a leading `+` sign) you will need to adjust the regex",3.0
g0wuire,i6n4l8,Could do but better to keep it as varchar,1.0
g0wtrzc,i6n4l8,cant you make the column an INT (10) datatype?,1.0
g0wxono,i6n4l8,"No, no, no. Leading zeros? Perfectly valid in some locations. International format, starting with a plus? Also valid.",2.0
g0x8d67,i6n4l8,"Max size of an int is 2,147,483,647, which is way too close for comfort to store phone numbers in, plus any leading zeroes will be lost.

I mean for example assume you wanted to store the number 1-888-999-9999. That's lower than the maximum but not by much at all. If you were to change it to 2-888-999-9999 (which would be nonsensical) you would now be larger than the maximum size. This doesn't seem to matter much but you could get a number that starts with a 9-1-areacode-number, or some other similarly strange values.

Better to keep it as a varchar in my opinion, and I will often create a cleansing type of process for phone numbers where we break the raw values down to remove -'s, or ('s, or any other symbols to create a uniform array of numbers.

Lots of this depends on the application layer and how phone numbers are being input by a user.",2.0
g0x6rvr,i6n4l8,There is no `INT (10)` data type in Postgres.,1.0
g0wo3f1,i6m76n,What is the error message you get?,1.0
g0writ1,i6m76n, [https://imgur.com/a/D4euxXN](https://imgur.com/a/D4euxXN),1.0
g0wtj3l,i6m76n,"You don't restore .sql files like a .bak \[to use SQL Server terminology\].   You execute them: if you open the .sql file, you will see that it contains lines of executable SQL.

We see from the 'pg\_restore' bit in your output that pg\_restore is trying to process the file.   It needs to be executed with psql instead.",1.0
g0wy7mm,i6m76n,"`psql` is a command line program **not** a SQL command. You need to run it from the command line, not from within pgAdmin (or any other SQL client).",1.0
g0vh68b,i6f1w7,"`c.fetchall()` will return a list of tuples of values (i.e. a list of SQL rows). You are using an aggregate function, so there will only be one row returned, and since you are only selecting one column, th result you get
&gt;the result is: 

&gt;[(135.0,)] 

is a 1-element list of a 1-element tuple, containing a float.

For starters, I would use `c.fetchone()`, which is good practice when the you only want one row. That should return just the tuple `(135.0,)`. You can access the elements of lists and tuples in Python with square brackets, so you can do something like

`res = c.fetchone()`

`grocery_month = int(res[0])`",6.0
g0vi9w3,i6f1w7,Was about to say this. You can just access and pick out the element of the tuple and put it into a variable.,2.0
g0vlewh,i6f1w7,Thanks!,1.0
g0v8x83,i6daag,"Open the command line and type

Sqlite3

That should give you a new prompt where you can enter SQL commands. 

I'm puzzled why this kind of question comes up every day on here. If I wanted to know if mysql is free to download I'd look on their website because that would give me an immediate answer, faster even then asking reddit.",3.0
g1be5mr,i6daag,"This is late, but [postgres.app](https://postgresapp.com/) is the easiest way to get up and running.",2.0
g0xg016,i6daag,brew install mysql,1.0
g0uir4r,i6as7x,Maybe someone can fact check me but pretty sure bigquery uses the syntax of sql?,1.0
g0ujado,i6as7x,"it does 100 percent, but it's different in terms of loading data, etc.",1.0
g0uchee,i69qx3,"read some Joe Celko. He writes the SQL standard

SQL for Smarties is a good place to start

otherwise, here is my list of things an expert should understand:

* why (NULL = NULL) is NULL
* A primary key means “make sure the values in this column are unique and not null”. You can only have one primary key on a table. a primary key is a special unique key
* foreign keys point to unique keys (and primary keys are a special unique key)
* self-referencing tables (foreign key can point to own table)
* keys (foreign, unique, primary) can span multiple columns
* values in a foreign key column can be null (on either side)
* relational division
* hot swapping tables/views/procedures by using schemas and search_path (aka `create synonym` in some dbs) like language xlations, secured views 
* trees (ex. comments), fast tree querying, preferably using recursive common table expressions
* GIS querying, indexing
* graphs/network models, querying, indexing
* temporal tables, and how foreign keys are affected
* auditing / revisions
* history table pattern
* versioning (of a database schema) eg liquibase
* multi-tenancy via tenant-per-db, tenant-per-schema, etc
* full-text search
* check constraints
* triggers
* soft deletes
* optimistic concurrency control
* database inheritance models (Single Table Inheritance, Class TI, Concrete TI), and dealing w foreign keys
* The party model
* drafts (draft version of an entity)
* aggregate/window (over) partition functions
* recurring events -&gt; using RRules and ExDate and materialized views
* declarative row security and column security",65.0
g0uybwd,i69qx3,"This is great man, thanks for sharing this",6.0
g0vzan0,i69qx3,"Thank you for this, I know the first few on the list, so I guess thats not too bad :D I will check those sources out",4.0
g0w19sw,i69qx3,enjoy the rabbit hole!,5.0
g0w3lhl,i69qx3,Thank you!,3.0
g0w82ti,i69qx3,Beautiful,3.0
g0uhkgi,i69qx3,"Depends on what you want to learn or be able to do. Just want to be able to write a wide range of query types? Or do you want to do things like architecture, administration, optimization?

I like to recommend SQLServerCentral's Stairway series (free).

 [https://www.sqlservercentral.com/stairways](https://www.sqlservercentral.com/stairways) 

&amp;#x200B;

Just looking through the list of available topics, you should be able to find something you don't yet know but find interesting. They tend to give a pretty in-depth explanation of a particular topic through a linear series of articles. For instance, I think the Stairway To Advanced T-SQL would be beneficial to most anyone working in some capacity with MSSQL -- it's got 9 articles that all handle syntax and commands beyond basic SQL.  
 [https://www.sqlservercentral.com/stairways/stairway-to-advanced-t-sql](https://www.sqlservercentral.com/stairways/stairway-to-advanced-t-sql) 

&amp;#x200B;

If you are interested in a more comprehensive course (or are interested in getting a cert) and are willing to spend some money and serious time, I got a lot out of training from CBTNuggets. The format of their video-based training worked really well for me, but everyone learns differently. There's no shortage of great resources for learning SQL. Good luck!",8.0
g0vzfw1,i69qx3,"Well for now, just want to generally improve on SQL, since I think i'm still on a beginner level. I also use it for my internship, but not sure to what extent I will need it yet.

Thank you I will check those out for sure.",1.0
g0uxfaj,i69qx3,https://www.reddit.com/r/SQL/comments/i05qpr/unemployed_looking_to_take_up_sql_recommendations/fzn9s48/,3.0
g0x5h9w,i69qx3,"If you are writing TSQL, I  really would recommend books by Itzik Ben-Gan he's a great teacher with a deep understanding of the internals.  ""TSQL Fundamentals"" and ""TSQL Querying"" are both excellent and go in to more depth than the titles suggest. Really useful,  whether you want to  focus on SQL Development, Database Admin or anything in between.

Nothing beats practice, rewrite old queries you've written using new methods you learn like Windows Functions, Cross Apply values or Pivots etc.",2.0
g0x8njt,i69qx3,"I will check it out thanks. Well I would write new queries, but no idea what new things to apply to them, hence I made this post",1.0
g0xr2r7,i69qx3,"Sorry didn't mean to make a ""draw the owl"" response. My learning path was very much on the job so I had the chance to refactor queries over the years as I learned new techniques. Just getting the hours in coding was rewarding in itself and reinforced any training courses or books",1.0
g0wz8wq,i69qx3,SQL Cookbook by Anthony Molinaro,1.0
g0x8h8g,i69qx3,Thank you,1.0
g0x236l,i69qx3,"If you are looking for a job, you can Practice easy to difficult level sql interview questions on Sqlpad.io.  Disclaimer: I created sqlpad.io recently. Cheers.",1.0
g0x8d60,i69qx3,"I already got the job, but I will use it during the job. But thanks",1.0
g0xh3wp,i69qx3,"Great free and fun way to improve your skill is to do HackerRank challenges on SQL.

After you finished them you can find detailed solutions on how I solved them on my youtube channel:
https://www.youtube.com/channel/UCfGTc8zyBjCGg-Ilc4oAxEg",1.0
g0u31fo,i66z48,"This is a great post, but it would be helpful if you were to provide a link to a CSV so someone could download, import, and then use it to run this query.",2.0
g0u4jec,i66z48,"it doesn't need anything like that?

Take a base number of cases per day, that's @i  &lt;--it's your initial number of infected people 

Take your infection rate (from pretty much any country's daily briefings cases today / cases yesterday), that's @r

that's it",2.0
g0u7nxz,i66z48,"&gt; from covid where

Oh, sorry, I didn't see that. I thought you were pointing to an external data source and checked your post out while I was waiting for a job to finish running. That's even more clever. Nice work.",2.0
g0u8mdw,i66z48,"no worries dude :)

at least it's being looked at!",1.0
g0u8sba,i66z48,It's slick.,1.0
g0ua0kj,i66z48,I like this combination of recursion and the lag function. Nice one!,2.0
g0tnwbn,i661kh,"There's lots of carry over, especially at beginner level.

I'm guessing it's an analyst role, not DBA.",4.0
g0tpt7y,i661kh,Correct,2.0
g0to440,i661kh,"Learn ANSI standard SQL.  Then you'll be good to go most anywhere, and can just pick up any platform-specific syntax &amp; command differences as needed, since there won't be many.",3.0
g0uxddt,i661kh,Oracle has lots of tutorials online with a free Oracle login.,1.0
g0tmclf,i661kh,"&gt; a job I want land requires me to learn SQL

do you know SQL?

if not, how do you propose to get them to hire you?  serious question",-2.0
g0tmx3l,i661kh,"No I don't know SQL. I'm going to learn, but I'm just trying to see how much carry-over there is between all the SQL (Oracle, etc). Should I be studying directly for Oracle SQL if the job I want use Oracle SQL or is there enough carry over that I could use a manual on Postgre SQL that I have to get familiar with SQL.",2.0
g0tp5g9,i661kh,"yeah, the basics don't change from one database to the next

SQL is very similar across platforms",1.0
g0thejr,i659vq,"ASC = ascending meaning going up. DESC = descending means going down.  
  
So for numbers ASC = 1,2,3; DESC = 3,2,1  
For strings, ASC = A,B,C; DESC = C,B,A",7.0
g0u12td,i659vq,I honestly never considered that someone could be confused between the two.,1.0
g0u4cnk,i659vq,"How does sorting strings work in a case-sensitive setup? Does it ignore case, or does it sort by ASCII character code? I don't have a system handy to try, and I haven't had to do it in a very long time.",1.0
g0xroqe,i659vq,"I tried it using collation SQL_Latin1_General_CP1_CS_AS. In ascending order, it went 0,1,2... 9,A,a,B,b,C,c... Z,z.

However, when I tested with collation Latin1_General_100_CS_AS, I got 0,1,2... 9,a,A,b,B,c,C... z,Z. That, I did not expect.",1.0
g11jhui,i659vq,Most databases have the concept of a character collation that specifies how strings are sorted and compared.,1.0
g0tubtn,i659vq,"When I was just starting out as an analyst with excel I would always get them mixed up too. Best retained advice I was ever given about it was this:

You only have to remember that **(A)**scending order starts with **A**, the **(1)**st letter of the alphabet. So long as you can associate the **A** in **(A)**scend, with **A** being the **(1)**st letter in the alphabet, you will always know what ASC will return for you at the top of the list.

As for Descending order. You don't have to think about it. If you memorize what ASC will do. this is ""the other one"". You don't have to remember something for both of them. Just memorize the first one, and know the other is just the exact opposite of the one you know.

Breaking that down is this:

* A and 1 are the letter or number you'd start with when ascending. 
* Descending is the one you didn't try to memorize outright.",2.0
g0tviks,i659vq,I change descending for decreasing then it makes more sense in my head. Nowadays it comes natural.,1.0
g0tbo75,i613en,"&gt; any advice how can i optimize it

get rid of all those monthly subqueries",2.0
g0tbzne,i613en,"Also, perhaps substituting the remaining subqueries for CTEs could help?",1.0
g0tfgto,i613en,Right! Dump the whole year into a cte and sum(case when month=1 end),1.0
g0tf9vm,i613en,Cte to the rescue for sure.,2.0
g0so8c2,i60sex,"If you’re talking standard queries then search job interview sql will cover some good examples.
Otherwise code challenges are good.",1.0
g0sot44,i60sex,Where to look for code challenges,1.0
g0sp1m5,i60sex,"https://www.codewars.com/

They have challenges for other languages too",2.0
g0spezw,i60sex,Thanks man,2.0
g0tad34,i60sex,I think leetcode has some practice problems too.,1.0
g0tfzgg,i5z7wt,"https://stackoverflow.com/a/37518944/1324345

https://community.cloudera.com/t5/Support-Questions/Spark-SQL-Update-Command/td-p/136799",2.0
g0ruwoo,i5vvly,"1. yes

2. two FKs in `BookingLocations`, one to `Bookings`, the other to `Locations`

3. PKs of `Bookings` and `Locations`",2.0
g0rwosx,i5vvly,Thank you.,1.0
g0sd2tn,i5vvly,"That's the correct answer. Look up Many-to-Many relationship if you need any more info, as that's what you have here.",1.0
g0sg4sl,i5vvly,"Great, thank you, I’ll look that up. I’ve always worked with small datasets in single tables. I’m guessing INNER JOIN is what I would need to pull data out and INSERT would work in a similar way to normal?",1.0
g1dv0qe,i5vvly,"Thanks again for your help, once I had that definitive advice and I was in front of my computer it was an easy task even though I’d not used more than one table before. Thanks for the confidence boost to do it myself!",1.0
g0wyh8a,i5vroj,We will work on putting something like this together!,1.0
g0sadab,i5vroj,"There's already a wiki that links to a bunch of stuff, and the About page. One thing I would like is easily seen resources for PIVOT/UNPIVOT queries, since that's a big part of what new SQL developers are asking about.",15.0
g0uglvv,i5vroj,"&gt;There's already a wiki that links to a bunch of stuff, and the About page.

I don't think either of those are as immediately obvious as a sticky post, though. Reddit directs focus to the list of topics first.",2.0
g0s8c4r,i5vroj,"Completely agree, there have been far too many of those posts recently",5.0
g0sl1bm,i5vroj,I'm stuck in the area where I want to learn more advanced SQL but I can't really seem to find resources that focus on the stuff for those of us that have been working with it pretty consistently and know the beginner and intermediate concepts,5.0
g0tfn5t,i5vroj,"Got you, boo.

https://www.reddit.com/r/SQL/comments/i05qpr/unemployed_looking_to_take_up_sql_recommendations/fzn9s48/",3.0
g0sx2z0,i5vroj,"I'm all for this.

I'm in the same boat - took a class in SQL/ python a few months ago. Python I could peruse with all day no problem. SQL, I'm having trouble finding practical ways to use it.

I'm going through Google BigQuery for practice, I guess I don't know of many 'end goals' in mind.

[One of the top posts has a written SQL test that Amazon required](https://redd.it/e441y4) - I had a recent job interview (different company, I didn't pass) where I had similar questions and I'd love a place to share. Maybe include a ""practice test"" of standard questions employers ask for.",3.0
g0tfoo1,i5vroj,"Got you, boo.

https://www.reddit.com/r/SQL/comments/i05qpr/unemployed_looking_to_take_up_sql_recommendations/fzn9s48/",1.0
g0tvi4w,i5vroj,You are amazing. Thank you!,1.0
g0tyepv,i5vroj,"There are two 'tricks' I learned early which have really served me well.

1. Before you start writing something new, or complex, open up Excel and mock up some data. Engage with your business partners to see if the sample data meets their expectations. If you want to do do something like make a prediction based on historic data, you can do this really easily in Excel and come up with a 'simple' solution. Once you have this, it becomes way easier to figure out where to start in SQL because you already know where you're trying to end.
2. This advice is hard to swallow. Sometimes you will be 99% there, and just need to add one little thing... and your entire process will fucking blow up. What had previously been a very effective solution will suddenly become unwieldy. A lot of people miss the forest for the trees in this situation and keeps trying, and trying, and trying to come up with some kind of work around to finish things up. Many times you will spend more time doing this than if you were to just start over. So don't be afraid to literally start over.

Starting over sucks, but each time you start over you'll remember where you got jammed up and you will be coding *for that* with all of your other exceptions or edge cases in mind. Instead of hitting that problem at the very end, you'll be working around it from the beginning.

I recently had a large project that I finished. I mean it actually gave 100% accurate results... but I couldn't efficiently query the objects I had built unless I wanted to wait *hours* to get the results. It just wasn't practical. Even though I was ""done""... it wasn't usuable. I didn't waste any time, I just nuked it and started over. Tough pill to swallow, and your boss won't like to hear it always... but you'll learn more, and often times it will actually solve your problem faster than if you just keep trying to improve something that is broken from inception. You just don't know it's broken from inception until you get to the very end. Othertimes as data grows and tables become larger you will have solutions that were working perfectly fine for months, or years, and all of a sudden they break and become effectively unusable.

You can't per se blame the original developer there because it worked for so long, but it was still *flawed from inception* and had the original developer understood that, they could have architected their solution more effectively.

You will never invent the iPhone if all you're trying to do is improve a Nokia. Sometimes you will NEED to literally start over and focus on what you want to achieve, which hopefully has changed once you learned what the inception flaw was.",1.0
g0tfl0u,i5vroj,I fully agree. I literally post a comment I made and just recycle it every time someone asks that same question.,3.0
g0ruzly,i5vpiz,"&gt; is it slower?

perhaps, but you won't notice until you get into millions or billions of rows",2.0
g0rxn6s,i5vpiz,"The main problem is having non sequential values in a clustered index (clustered primary key). Non sequential values will inserted into the index based on the order and can cause the index to page split and fragment. This can slow inserts and force you to require more frequent index rebuilds. 
If you have a non sequential value you wish to use as a PK you can create a non clustered key (if you system supports it) or have a sequential into as the ok and create a unique non clustered key on the other value.",1.0
g0siaf6,i5vpiz,"Only very few DBMS products use clustered indexes as the primary key index. So this limitation (disadvantage) doesn't apply to ""SQL"" in general (mainly to MySQL and slightly less so for SQL Server as you can choose to not use a clustered index for the primary key).",1.0
g0spfc8,i5vpiz,"SQl Server creates a unique clustered index by default on primary keys.  You *can* choose a non-clustered index. Oracle does this too. DB2 has clustered indexes as well.

I wouldn't argue that it is rare or few do this. 

Like most design choices, the advantages/disadvantages are dependent on the uses.  Clustered indexes are a way of physically organizing data.  They are not great for rapidly changing data, but highly effective and fast for data that accrues more than evolves.",1.0
g0svt36,i5vpiz,"&gt;  Oracle does this too. 
Oracle does **not** create a clustered index (called ""index organized table"" there) when you define a primary key.

My point was that only a few engines (essentially SQL Server and MySQL/MariaDB) _default_ to clustered indexes for the primary key.",1.0
g0sxnb0,i5vpiz,"Forgive my mistake.  I should not have stated that it is the default behavior of Oracle primary key indexes.

Since MySQL and SQL Server are well-known and widely-used products your comment that only a very few DBMS products use this method didn't ring true to me.",1.0
g0rhabs,i5tpgc,Whats the reason to make this visible to the user?,1.0
g0rjmsv,i5tpgc,"As reference for example, if you want to give a control number for audit purposes.",1.0
g0rkb66,i5tpgc,For audit purposes use a log mechanism on front and backend side. Audit shouldn’t influence the daily work in any kind.,1.0
g0rnm2s,i5tpgc,"Well, I dont mean just for audit purposed. I mean a series of logs for example employee id numbers. Anyway,  does this mean that I should create a separate autogenerated control number aside from the primary key?",1.0
g0sifih,i5tpgc,"If you do, be prepared for questions like ""Why is there a gap between the last ID and the new one?"" (and the inevitable bug report to ""fix"" that)",1.0
g0snapu,i5tpgc,"I see. Other than that, is there a security risk if I do that?",1.0
g138yvb,i5tpgc,"There are different schools of thought here. Some say that autogenerated primary keys should not hold any meaning and you should (in theory) be able to change them without any impact on users. If you display it to users then they will likely use that information elsewhere and you can't really change it. Conversely (and probably more importantly) users like to change their identifiers which is a lot easier to do when it's not the real primary key. And if you've got a separate unique value that you are displaying to users (e.g. a product_code in a product table) why muddy the waters by also displaying product_id?

On the other side : Lots of systems lend themselves to sequential identifiers (e.g. Order Numbers or Ticket Numbers in a helpdesk system) and so you're going to be displaying an ID to users anyway. And that ID will have to be unique so why not just use the system key you're already generating? 

In general I would lean to keeping the primary key non-visible, but I don't think it's necessarily harmful in every case. I don't think keeping your IDs secret is really giving you much of a security edge.

Maybe this is one of the benefits to using GUIDs - no-one wants to see that shit in their front end.",1.0
g0rgmgo,i5t7lc,"SELECT results FROM Google WHERE search = 'SQL Tutorial'

For real tho, I recommend sqlzoo if you are at a very beginner level.",20.0
g0rjg5n,i5t7lc,Thanks! Also should I learn tableau or SQL first?,1.0
g0rogq2,i5t7lc,Tableau is a reporting tool. I guess you can start with both or get basics of Sql before starting with reporting,5.0
g0sicxk,i5t7lc,"Eh I guess sql, but you should probably begin by looking up how relational databases function. Look up some ER (entity relationship) diagrams. Tableau is fun and easy to grasp, but you can easily misinterpret what your seeing/doing if you don't understand how data works.",1.0
g0s1fyt,i5t7lc,W3schools SQL,5.0
g0rnk9r,i5t7lc,"For very basic SQL I had completed topics in JetBrains Academy https://hyperskill.org/knowledge-map/520?v=table
If you are interested here is my referral link with 5 months of free access (instead of standard 7 days free trial).

(+) after each topics they have practice.

If you understand Russian there is very good free course: https://stepik.org/course/Интерактивный-тренажер-по-SQL-63054",3.0
g0s8vbn,i5t7lc,"I always recommend that newbies start with Wise Owl SQL on YouTube.

Its a ELI5 starter.

Also, powerbi &gt; tableau",3.0
g0rjljk,i5t7lc,Thanks! I enrolled in a SQL course on Coursera by Duke University. So far it seems good,2.0
g0rz6l0,i5t7lc,Udemy. There are so many courses on there and they always have sales.,2.0
g0s173u,i5t7lc,Kharnacademy has a pretty good interactive course,2.0
g0t33kk,i5t7lc,Microsoft Learn as well https://docs.microsoft.com/en-us/learn/,2.0
g0tz1if,i5t7lc,"I'd say you should learn SQL first, it will give you way more opportunities.   I would try to code first on your own even if it is just a few weeks.  It would be good for your to make some mistakes on your own so that when you go to take a course the information will sink in better.  There are a number of courses online and YouTube videos you will not have issue getting good information.

However, problem solving is a skill so you will want to create your own project using SQL and Tableau if you want so that you can run into issues and overcome them.

I have a free PDF with basic SQL Syntax:

 [https://pages.decodeanalytics.net/](https://pages.decodeanalytics.net/) 

Here is the playlist that reviews all of the above examples.

[https://www.youtube.com/playlist?list=PLht0txJzVxKFt-Ty4W58z8LM4DHxRUdd2](https://www.youtube.com/playlist?list=PLht0txJzVxKFt-Ty4W58z8LM4DHxRUdd2)",2.0
g0v25je,i5t7lc,Thank you so much! Well I have some basic knowledge in python,1.0
g0rh3fi,i5t7lc,"I am also a newbie. I bought a course from udemy. But idk if it is still onsale.

Idk if it is good....but I found this website for quick practice.
hackerrank.com

And i am in the middle of this video (its 4 hours long). So far it is really helpful for just extensive eli5 knowledge and explanations.
https://youtu.be/HXV3zeQKqGY

Also this activity uses sql to solve a mystery.
https://mystery.knightlab.com/",1.0
g0sb0r2,i5t7lc,"&gt;https://youtu.be/HXV3zeQKqGY

I watched this too! Its a good one for a very basic intro. Super helpful to start from such a baseline. You have to really spell it out for me sometimes :)",1.0
g0tu3jd,i5t7lc,"He has videos for a bunch of other languages too. So it has opened up, ""I could learn x,y,z"", now for me. And it feels less intimidating.",1.0
g0ro8fo,i5t7lc,https://www.udemy.com/share/101WhkBUEZd1xXTQ==/,1.0
g0rwob2,i5t7lc,I'm on codeacademy and think it's fine.,1.0
g0zbqlq,i5t7lc,Dude i replied to your  **coinboxs.co**  post..,1.0
g1ahct7,i5t7lc,Start learning the basics from W3Schools and learn advance part on DataCamp and Strata Scratch.,1.0
g0rs7qh,i5rvcg,"unless a patient can have more than one visit per day, your count of visits (which is the right table in your left join) will be either 1 or 0 becuae you are grouping on the day

also, your various statistics in the UNIONs don't need GROUP BY at all -- in fact, they can all be combined into one SELECT",2.0
g0y8orv,i5rvcg,Hey thanks for your help! I rly appreciate it. What do you mean by my stats can be in one select?,1.0
g0yliqh,i5rvcg,"    SELECT MIN(num_visits) AS visits_min
         , MAX(num_visits) AS visits_max
         , AVG(num_visits) AS visits_mean
         , STDDEV(num_visits) AS std
         , PERCENTILE_CONT(0.25)
             WITHIN GROUP (ORDER BY num_visits::float) AS 25q 
         , PERCENTILE_CONT(0.5)
             WITHIN GROUP (ORDER BY num_visits::float) AS 5q 
      FROM visits",1.0
g0rf8h0,i5rvcg,what's is the expected output. Care to share some sample rows in the table,1.0
g0qzpqt,i5qg9u,does the `synonyms` table have field for the synonym itself? or are there foreign keys for the two synonymous words?,2.0
g0r6b7r,i5qg9u,Ahh you're quite right! I've edited the post to add a field for the synonym itself,3.0
g0r8se5,i5qg9u,"the schema looks mostly ok to me. im wondering how you intend to query a word for its synonyms. are you looking for words that share a meaning and part of speech? 

if you create a join table in synonyms with two foreign keys, one for each primary_word, you can query that directly. without doing a join on meanings and parts of speech, but i dont know what your data sets look like. otherwise i think you have a mostly normalized db.",2.0
g0rchc5,i5qg9u,"You're pretty much right. The query cases will be 

1) all of a word's synonyms that share a particular meaning \[this won't be done for one of the thesauruses which, as mentioned above in my edit, lacks meanings\]

2) all of a word's synonyms that share a part of speech 

3) either of the two above categories with the added stipulation that the synonyms similarity value must be sufficiently high

4) Doing all of the above for multiple primary words at a time

As discussed in my edit, for one of the thesauruses, meaning will be NULL so I will only do #2 for that one.

I'm not entirely sure that I understand your suggestion. Do you mean I could connect a particular synonym with 2 different primary\_words that it is a synonym for? That's an interesting idea, but I was planning on keeping entries in the synonym table unique to a particular word because they come with a similarity rating which would be different for different words. For instance, 'horrible' is a synonym for both 'bad' and 'terrible,' so it would have two entries in the synonym table with different similarity ratings. 

Now that I've written this out it sounds like it may actually be bad because there will be some data redundancy. Perhaps it would be good to have a table for 'words' in general, which can be either synonyms or primary\_words, and then the synonym table can just be a similarity rating,  a foreign key into the words table for the synonym itself, and a foreign key into the meanings table for the meaning the word is a synonym for with this similarity rating. I could then get rid of the primary\_word table and have meanings have a foreign key into the word table for the word it is for.

This gets complicated haha, but I think that may be better. 

By the way, just to be clear, did you see my edit regarding supporting multiple thesauruses when you said the structuring seems OK? No worries if you haven't, you've been generous enough with your time for me already and I understand that you probably wouldn't want to go back and think it over again. I'm just curious if your prior judgment had incorporated that.",2.0
g0s5q31,i5qg9u,"i do see the EDIT, but if there were earlier changes i missed them. i think you understand the tradeoffs to some of the alternatives you are thinking about. 

&gt; Do you mean I could connect a particular synonym with 2 different &gt; primary_words that it is a synonym for?

yes, if you create a static join table of ""synonyms"", just two foreign keys that are related, you might have implicit data duplication with what is stored in ""meanings"". you also have to consider that a table entry with fk1:cold, fk2:cool, is not the same as fk1:cool, fk2: cold, your table either has these left-right duplicates or not and your queries will have to take that into account. 

you might want to create some tables and add a few pieces of data so that you can see what queries will look like. 

i think the fact that you have at least 2 data sources that dont match up very well means this will be a challenge. 

if you search for words in meanings that have the same meaning and pos as the word you are looking for synonyms for, then you can avoid some duplication and it might be the most normalized data you can get. this approach sounds kind of innovative and maybe the most interesting from a nlp standpoint. one issue is that unless your definitions/meaning are very vague, no words will really have synonyms, they will all have essentially unique meanings that nothing else ""connects"" to.",1.0
g0qvxiy,i5qg9u,"I don't see anything wrong with this design. For maximum efficiency make sure each table has an index on the primary key.

Also there's nothing wromg with having a table that's just a key and a description. Integers take up less space than strings so this is one way to store data efficiently.",3.0
g0qx1mr,i5qg9u,A primary key is also an index.,5.0
g0r5pxm,i5qg9u,"Since you mentioned this is a personal project, I recommend exploring if available NLP libraries already out there. I.e. nltk in python is really easy to get started with.

This doesn't answer your question precisely, so it depends on what the goal of the project is (learn more about MySQL or if this is an NLP-based project).",1.0
g0r6rw4,i5qg9u,"Thank you for the recommendation. I looked into NTLK and may decide to also use it's synonym data as a third source, but I would still like to figure out how to use the first two sources I made the post about (reasons being: I want to gain experience in working with the database, NTLK seems to lack a ranking system similar to the similarity ratings one of my thesauruses has, and the other thesaurus I'm only including for comparison because it's results are awful.)",2.0
g0sooxb,i5qg9u,"Check existing thesaurus databases such as [WordNet](https://wordnet.princeton.edu/download/current-version), [Moby](ftp://svr-ftp.eng.cam.ac.uk/pub/comp.speech/dictionaries/moby/) or [OO](https://extensions.openoffice.org/en/project/english-dictionaries-apache-openoffice). 

I'd change `synonyms` table to `relation` as one word can be a synonym of another and antonym of the third and so on.",1.0
g0qibjx,i5nir0,"I'm so confused. Your error message resembles Postgres, your post's label says MySQL, but you mention PATINDEX which is a Microsoft SQL Server function.",4.0
g0qru0r,i5nir0,"I’m sorry, I just started using SQL this week. All I know is that my company uses SQL Explorer",1.0
g0rba6v,i5nir0,"Well you're probably on Postgres given that error message, but that's something you should confirm. All database systems are different, some vastly so. (They all have different JSON-handling functions for sure.) SQL Explorer is a client that you use to talk to a database, which isn't too relevant. A rough analogy is you want to configure your operating system, but you're not sure if it's Windows, MacOS or Linux. But maybe you know that you're logged in using ssh (an analogy for using SQL Explorer).

Can you try running this?

    select version();

If Postgres, it will succeed and return a result that looks something like:

&gt;PostgreSQL 9.3.10 on x86\_64-unknown-linux-gnu, compiled by gcc (Ubuntu 4.8.2-19ubuntu1) 4.8.2, 64-bit

I think this will error on any other database. (Maybe some Postgres derivatives notwithstanding.)",2.0
g0qiutz,i5nir0,"I am really unfamiliar with MySQL but it looks like you have JSON and that looks like at least reasonably valid JSON... maybe check out the documentation here:

https://dev.mysql.com/doc/refman/8.0/en/json.html

I'd probably just query ""cell_growth_mode""'s value out of the JSON.",3.0
g0pzmi6,i5lw9m,"Generally speaking, a data warehouse would de-normalize data.  But in your example, I would use 2nd approach.

The problem with the first approach is you're locking yourself into a max number of data elements without further schema changes.  Performance will vary depending on the distribution of data, indexing, etc.",4.0
g0q52yg,i5lw9m,"Fortunately, there's a two step method for determining the preferable implementation in each situation. Unfortunately, the two steps are:

1. spend 4-10 years getting DB design and tuning experience
2. leverage that experience to make the right judgement call

Generally the 2nd approach would be preferred as an operational structure. (Kind of. But you wouldn't have a productname column, but a product ID which references a products table. Maybe the name is that key, but names can change down the line, but again, experienced judgement calls.)

But from the column names it looks like you're building a reporting function rather than anything that actually supports the sales process. So the 1st option might be better, might not, depending on the sort of queries you want to run over your warehouse, which depends on the use cases.",3.0
g0qqu6u,i5lw9m,"In a data warehouse you often might actually store a product name column. Since the goal is fast reportability and not transactional needs, you often denormalize and trade increased storage requirement for less joins to improve read performance.

OP wants to go with the second approach. The first is like the kinda crap people do in excel because they think data should be laid out like a graph in excel. It doesn't scale and the second approach is a format that will be more easily consumable by most BI tools.",2.0
g0qs2du,i5lw9m,"You'd store a product name *as well as* and not *instead of* a product ID, if that's what you're doing.

&gt;The first is like the kinda crap people do in excel because they think data should be laid out like a graph in excel

And sometimes this dumb junk is your spec, and there's nothing you can do about it, that's how some other part of the business is insisting on retrieving it. And sometimes that's still the easiest way for less technically minded analysts to query your warehouse, depending on how your org is set up. In which case consider a materialised view set up like (1) over a sensible denormalisation in the style of (2).

&gt;by **most** BI tools

Yeah, but we don't know if OP is playing into one of those, or one of the others.

Given how little we know about what OP is doing, I'd be more inclined to phrase the advice as ""OP wants to go with the 2nd approach because it's less bad in the majority of the small number of situations where it's the wrong call.""",2.0
g0r4dbq,i5lw9m,"Thanks u/IDontLikeBeingRight and u/caveat_cogitor for such information.

It seems like keeping aside all design ethics or how the data is about to be used further, the second approach (with more rows and less column) is better if we r just thinking from perspective of running a simple SQL query. (Kindly correct this point if i m wrong). I m trying to get an idea at the most basic level of simple query execution as to which approach is better. Of course from complete architectural perspective there would be tonnes of considerations to be made which unfortunately are vary widely for each use case.


And if u must know from the further usage of this data, it would be later on used on stuff like PowerBI and as I gather from the discussion, the first approach(with more columns) might be more useful. Of course I would have to look into that before coming to a judgement.


Update: 

I have seen an existing model with 1st approach and asked the people working with it..... They said that more rows means more time taken to scan the records and this would make a difference when dealing with millions of rows. So more column approach is better from this perspective as it reduces number of rows.


They also added that this shall work upto 200 columns beyond which we might get to see performance issues...(the number 200 may vary).",1.0
g0qshgq,i5lw9m,"The second approach.  If you have a lot of users running a query where they expect the results from the first approach exactly, you MIGHT consider setting it up as a materialized view, but other than that, its a horrible table design.

The performance difference will be negligible if your queries are well written, your db design is solid, and you keep up with maintenance.",1.0
g0r4w29,i5lw9m,"Thanks u/wolf2600.  Of course the approach choice would take a lot of considerations into factor but seems like just from perspective of running simple SQL queries, the engine would be more comfortable with ""less column and more rows"" rather than ""more columns and less rows"".

Update: 

I saw an existing model with 1st approach and asked the people working with it..... They said that more rows means more time taken to scan the records and this would make a difference when dealing with millions of rows. So more column approach is better from this perspective as it reduces number of rows. (They might have other reporting requirements too but this was one of the factors.)


They also added that this is fine upto 200 columns beyond which we might get to see performance issues...(the number 200 may vary).

So it seems like the concept i wrote at beginning of this comment about SQL being more comfortable with more rows is kind of inaccurate.",1.0
g0ptrmk,i5l2sz,"Hello u/Lostinspace44 - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g0qarl0,i5l2sz,"Hay, David C. *Data Model Patterns: Conventions of Thought*. New York: Dorset House Publishing, 1996: 115.",1.0
g0p9dxz,i5h5an,"You have to think of this in 2 steps.

Step 1, load the data from the file into database.

Step 2, run the insert / select statement.

The bad file is only applicable if one of the rows in your CSV does. It match the table you describe. It only is created at step 1. Step 2 will not influence this.

You can see this if you were to add a bad line in the CSV, for example too long a string.",1.0
g0pfw4o,i5h5an,"The badfile would be stored on the database server, in the directory pointed to by the Oracle directory "" data\_to\_input "". Notice in your statement the line that says ""  DEFAULT DIRECTORY data\_to\_input ""

To find the OS directory, do the following

    select * from all_directories where directory_name = 'DATA_TO_INPUT' ;
    

But if you are already looking in there and can't find anything, that means that there was no badfile because there were no bad records in your input file.",1.0
g0oxxiv,i5gcai,"if a document can belong to only one project, add a column `IsDefault`

edit:  add to the document table",12.0
g0p28w0,i5gcai,"That's actually a good idea and quite a simple one, how did I not think of that lol",3.0
g0popi4,i5gcai,fistbump,1.0
g0pv84y,i5gcai,"The problem then is, without a trigger, you could have multiple “default” documents for a given project.",1.0
g0q27ec,i5gcai,"On SQL Server, unique filtered index would also allow this type of constraint.

    CREATE UNIQUE INDEX IX_&lt;table&gt;_&lt;column_key&gt;_is_default
    ON &lt;table&gt;(&lt;column_key&gt;)
    WHERE is_default = 1",3.0
g0q399x,i5gcai,"Did not know about this: since the syntax for Postgres and MS SQL Server seem the same, is this a part of the standard?",2.0
g0q3zif,i5gcai,"I honestly don't know. I don't delve into this side of SQL much since most of my work is in BI/reporting versus application development. When the other poster showed the Postgres code, I was curious if SQL Server had something similar and then found the above on StackOverflow.",1.0
g0q0vy0,i5gcai,"With Postgres this would be quite easy with a partial index:

```sql
create unique index on document (project_id)
where is_default
```",2.0
g0q38th,i5gcai,"Did not know about this: since the syntax for Postgres and MS SQL Server seem the same, is this a part of the standard?",2.0
g0rp9g3,i5gcai,The SQL standard doesn't defined indexes (and how they are created) at all. So the answer is no,1.0
g0yls4d,i5gcai,"on postgres, oracle, or db2 you eliminate this with a check constraint

on sql server you eliminate this with a filtered constraint

on mysql, you eliminate this with a unique two-column constraint by abusing that mysql gets null wrong in right joins

on mongo you eliminate this by switching to postgres",1.0
g0p2gpa,i5gcai,"Not if you create a many:many table: document_project(document_id, project_id).  referencing tables «document» and «project». That way, you can assign as many documents as you want to any project. Creating this table, also means that you don’t have to specify any relationships from document or project. They are all in document_project.",3.0
g0povsn,i5gcai,"I just want to point out that the type of problem you're describing is exactly what databases and normalization were developed to do. So it isn't a bad thing that something needs to reference something else. You're already thinking about the problem in the right manner. 

As /u/r3pr0b8 mentioned, add the is default column for the one it's supposed to use, but then you can have more.

We have the exact scenario where users in our system can have multiple external IDs from various third parties. We have a ""preferred"" column that we set to 1, which means, here are all their IDs, but if you want to know which one we recommend to use, it's WHERE preferred = 1",3.0
g0pvppj,i5gcai,"You could store a `document` attribute on the project table with a `NOT NULL` constraint and a foreign key constraint pointing back to the projects table. You could then have a `default_document` attribute on the projects table, pointing to a document that presumably already points to that project. 

The benefit of this structure is that you can be sure that a given project will have no more than one default document.",1.0
g0q0wjx,i5gcai,"As long as you're not creating a circular reference you should be OK.

&gt;it's never a good thing when elements become dependent of each other

That's the nature of primary/foreign key relationships. You just have to be diligent that you don't break things.",1.0
g0tvyr7,i5gcai,You could add a flag on the document table to show weather a document is default or not.,1.0
g0owivk,i5g6gz,"I may be wrong in thinking you have installed this on Windows...but if you have - you will probably have installed the database engine which runs as a service (check ""Services"" application). To have a graphical frontend to the database you probably need to install Sql Server Management Studio.",7.0
g0oysrs,i5g6gz,That makes sense thanks,1.0
g0pmjdg,i5g6gz,https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15,2.0
g0pp6z8,i5g6gz,"You need to install SSMS also so you can develop code. It was a two step process.  Check out this video it will show you the steps.

[https://youtu.be/3ijtZF1n1ME](https://youtu.be/qaFOp0CO8xM)",2.0
g0qlfwi,i5g6gz,"Just as a note for anyone else SSMS used to be included in the base installer but no longer is, this was mainly done so that development was not tied down and could be done independently of each other.

In the flip side it can be confusing if you are expecting the ""portal/editor"" to be baked into the installer still.",1.0
g0ozbwo,i5fy8c,"&gt;  Table xxx has a count of ~2.1 mil and table xx has a count of ~2.5 mil. So I need table xx's counts to match ~2.1 mil exactly

my initial reaction is ""dude! inner join!""

but you say you can't use inner join

so making the counts equal just doesn't make any sense -- to me, anyway

regarding your code, i can see at least two instances where you probably should be using parentheses to properly evaluate your ANDs and ORs

    WHERE ( 
          category ILIKE '%Aqueous Shunt%' 
       OR category ILIKE '%Aqueous Shunt, revision%' 
       OR category ILIKE '%Canaloplasty with stent%' 
       OR category ILIKE '%Canaloplasty without stent%' 
       OR category ILIKE '%Cataract surgery%' 
       OR category ILIKE '%Endoscopic cyclophotocoagulation%' 
       OR category ILIKE '%ExPress shunt%' 
       OR category ILIKE '%Goniotomy/Trabeculotomy%' 
       OR category ILIKE '%Laser trabeculoplasty%' 
       OR category ILIKE '%Postoperative revisions%' 
       OR category ILIKE '%Removal of device%' 
       OR category ILIKE '%Suprachoroidal bypass, Cypass%' 
       OR category ILIKE '%Trabecular bypass, iStent/Hydrus%' 
       OR category ILIKE '%Trabeculectomy%' 
       OR category ILIKE '%Trabeculectomy, Revision%' 
       OR category ILIKE '%Transscleral cyclophotocoagulation%' 
       OR category ILIKE '%central corneal thickness%' 
       OR category ILIKE '%gonioscopy%' 
       OR category ILIKE '%optic nerve fiber layer%' 
       OR category ILIKE '%visual field%'
          ) 
      AND practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%aaa%' 
       OR practice_code ILIKE '%v%' 
      AND effective_date BETWEEN '2013-01-01 00:00:00' 
                             AND '2019-12-31 00:00:00'

can you see the two issues?",3.0
g0qwd8a,i5fy8c,Hey! Thanks so much for your reply. I figured out that it didnt make sense for them to have an equal amount of rows because of the conditions in my where clause. I rly appreciate the help anyways!,1.0
g0rrubv,i5fy8c,"&gt; because of the conditions in my where clause

did you understand why i said you have issues with your ANDs and ORs?",1.0
g0p2p8t,i5fy8c,"&gt; I need table xx's counts to match ~2.1 mil exactly

half-jokingly: delete the ones you don't like?

and seriously: if you only matching counts not contents is not even half of the battle (which you kind of allude to by saying "" because I will be dropping counts from xxx that I need (they will be too low -- I cant just drop my missings"").

determine your output granularity -&gt; apply group by -&gt; figure out measures get those, apply filters, THEN match the numbers BASED on your granularity. 

if i'm selecting GPA for 10 students but ended up selecting 10 individual grades for a single student, my expected record count matches but my data is wrong.",1.0
g0qwkrt,i5fy8c,Hey! Thanks so much for your reply. I figured out that it didnt make sense for them to have an equal amount of rows because of the conditions in my where clause. I rly appreciate the help anyways!,2.0
g0pxhv8,i5fy8c,"Not answering your question, but you really shouldn't be using 'like' and 'or' like this.  Use a look up table (or create one for your local analysis).  This query is giving me PTSD.  If there's any real data volume, this will be extremely slow and use tons of resources.",1.0
g0op0eb,i5de38,"I make beekeeper studio - it's open source, beekeeperstudio.io

We're moving quickly and the app works great on both database types. 

Please file issues on GitHub if you hit any problems! We're super nice :-)",4.0
g0op1vf,i5de38,Oh looks great oh high dpi displays and you can zoom the interface too :-),2.0
g0or3si,i5de38,I had never heard of this. Definitely checking it out! Thank you for the suggestion!!,1.0
g0p16yk,i5de38,No worries! Please let us know if there are cook features you'd want!,2.0
g0xxskw,i5de38,This app is pretty fantastic so far!!,2.0
g0py9el,i5de38,"Hey, thanks for the suggestion, I’ll make sure to check it out and will let you know my thoughts if I end up using it!",1.0
g0vxam0,i5de38,"Hey, I checked it out and it's by far the best looking and easiest to use one, it's just that I feel some features are missing (I realize the project was started just few months ago).

Any place where I can make suggestions?

Thank you!",1.0
g0vzp6k,i5de38,Yes! File an issue on github.com/beekeeper-studio/beekeeper-studio we're adding features weekly and so your input would be super duper helpful!,1.0
g0wjvqq,i5de38,"Ok, made an issue :)",2.0
g0oty16,i5de38,DBeaver and Datagrip are popular interfaces as well,3.0
g0oykmw,i5de38,second this - my two fav IDEs,1.0
g0p2ofx,i5de38,"Yeah I was using DBeaver for awhile but switched over to DataGrip a few months ago. Both are great. I bought the ""everything"" license for the 2nd window although recently my workflows have let me heavily utilize the built in Datagrip integration with PyCharm... I have my Leftmost pane is the Python directory tree, left zone is Python, right zone is SQL, and rightmost pane is the database directory tree.  Since I've done this, I've almost entirely switched from using my vertical monitor to my horizontal monitor.  Just works so amazingly well for me.",3.0
g0p32vy,i5de38,"That's a super sweet setup friend. I used to use pycharm then switched to jupyter notebook for python scripts - but I am tempted to get DG and pycharm integrated like you're doing. I love datagrip but been using Knime a bit more lately for multi-DB workflows. The company I work for currently has a fantasy sports database, sportsbetting database, horse racing db, casino, etc etc so until the data lake is finished I need to continue using Knime to query multiple DBs. 

I might hit you up on the PyCharm/DG integration later though!",1.0
g0po60h,i5de38,"Just to give you a little more incentives... pycharm sometime earlier this year released Jupyter notebook integration so you can manage them inside the IDE. You can also use them alongside regular py files, even with the same VENV so you can do interactive and then dump it in a py file. That's generally how I've been doing webscraping. Or api ingestion. Let's me interactively build in the IDE environment.",1.0
g0pysa6,i5de38,"If you don’t mind my silly question, me being a beginner in sql: how do you utilize python with sql? I see that you mentioned webscraping. You webscrape with python and than save to sql? Do you utilize something like pandas? Thanks in advance.",1.0
g0q0lqr,i5de38,"Yep!  I largely use Python to move data around.  So take data from source A, do some kind of data architecture, and then do analysis on the data.  I heavily use SQLAlchemy, which is an ORM (Object relational mapper) so I can query objects from within python, although truthfully, I prefer having Python make schema for me, so I'll establish all my tables as objects and then sometimes just use SQL to fill the tables up or do other manipulations on them.  I like my pipes to be reproducible, so when possible, I'll have it wipe out the entire thing and start over so I know I can always get to the end-result from just the raw data. 

I'll either use sql inline, so something like:

    cmd = 'insert into blahblahblah select columns etc, from table join etc. etc.'
    session.execute(cmd)

Or, more frequently, put all the SQL files in a folder and then read them:

    file list = ['file1.sql', 'file2.sql']
    for file  in file_list:
        with open(file, 'r') as f:
            session.execute(f.read())

When I'm doing webscraping or using an API, I use the database to make sure I'm scraping intelligently.  For example, if I'm pulling data from a government site for a set of CSVs, I'll first update a table that has a list of CSVs and the date they were updated last... then if the site provides the day it was updated, I won't download them unless the update date is after the version that I have.  There's a reason that relational databases have been in use for so long, they're great at storing data and ensuring correctness within a certain set of parameters.  Sure, you could use flat files or possibly objects within the language itself but I find that I almost always resort to having Python act as a layer of compatibility between the database and whatever it is that I'm trying to do.  The database essentially dictates what Python should do, but Python ties it all together and knows what to ask the database (e.g. what order to do things in).",2.0
g0q1knp,i5de38,"Thank you for the thorough answer, I see that I have much more to learn. Am studying SQL now and working on some basic projects like setting up a database from terribly sorted information in excel. After I get up to speed with SQL I was thinking of starting to learn python so I can make a program that has a UI and can insert and read data from an SQL database for example.",1.0
g0q6j03,i5de38,"That's a great project.  Depending on what your needs are, what I'd recommend doing is using Python to read your Excel files into a database engine.  Pandas is a good tool for that for sure.  I highly recommend Postgres although there is a bit of a frustrating learning curve setting up any database on your machine.  SQLite is a good alternative. Its just a file that acts like a database that you can run SQL from.  I tend to separate my data into different layers.  So your first layer (which I call staging usually) is just a 1:1 copy of the source.  The next layer (which I often call a warehouse) involves taking those source files and organizing them into either a normal form, dimensional model, or whatever makes sense but this is where you enforce key constrains (primary &amp; foreign), datatypes, etc.  Then the third layer is what I do with the data (I often call this a datamart though the warehouse &amp; datamart terminology are not really precise in this context but its just kind of what I learned so it stuck).  So if you want to make it easy to query dataset A, I'll make a view or set of views that have that subset of the data.  

If you want some good material on organizing data, I always recommend both the wiki on database normalization. Its fantastically presented and not too dense, like some textbooks tend to be, but with just enough to actually learn it and use it as a reference.  Usually you won't go any higher than 3NF.  A book that is great for learning dimensional models is the book by Kimball called ""The Data Warehouse Toolkit"".  That's not specific to Python though I use Python to build both types of models literally daily!",2.0
g0q8bct,i5de38,Thanks once again for the thorough answer!,2.0
g0qadst,i5de38,You are the beneficiary of today's data pipelines taking forever to insert :),1.0
g0q4mir,i5de38,"My advice is to put pandas aside for now. Pandas is for small data light processing for use mainly with visualization (charts).

Go to /r/datasets and try to pick some data to play with, load it into a database, and further using SQL to try to query it for some answers",2.0
g0q560o,i5de38,"I’m studying SQL now, and working on building a database from erratic data in excel. In a month or two I’ll look into python, my main wish is to build a gui that connects to SQL so people with no knowledge of SQL can easily add or look up data from the database. Thanks for the suggestion tho.",1.0
g0pybh3,i5de38,"Thank you for the suggestions, will check them out!",1.0
g0oleow,i5d9in,"your backup software LOCKS files? and is this software designed to be run when the service is online?

if it is not, then you are supposed to shut down your service before running your backup.",1.0
g0omiyb,i5d9in,"It may be a crappy software, I've tried several, and ended up with this one (the others were worse lol).

I'm not sure if it actually locks the file, or if it does something that is perceived by SQL as a lock of some sort.
I don't think it's specifically designed to work with online databases, that can easily be the issue and a mismanagement on my part.

I could change it so a job does daily backups, and the backup software backs those up instead I guess... that would be better. 
I'm not very knowledgeable in database admin stuff (I'm at developer level) so I'll have to read some on how to do backups on the same file reliably (so the backup cloud service does not get flooded by old backups).",1.0
g0p9dfj,i5d9in,"I would recommend using [OLA scripts](https://ola.hallengren.com/sql-server-backup.html). They are free, and so popular that there is a lot of documentation online on how to use them. More importantly, they won't lock your database.",1.0
g0pdecu,i5d9in,"Will read, thank you!",1.0
g0o8lhm,i5d2ac,"The first correct hit on the test would set the returned value.

Not sure about your dataset, you are testing two different fields and are having an overlap. Perhaps on each test, you test on both fields? Then you can tune this to get your desired response.

Hope it helps.

(BTW formatting the query would help make this easier to read.)",2.0
g0o9c1s,i5d2ac,thanks for the tip..  I did use the formatting code tip putting four spaces before each line though.,1.0
g0ojljx,i5d2ac,Apologies for sounding snippy! I get to deal with poorly formatted queries all the time.,2.0
g0oqym7,i5d2ac,"&gt; Example, some of those codes might have ChargeType = 'H' so they get assigned as 'Overhead' before they would get assigned as 'Accrued Benefits'.

actually it's after, not before

you do realize that `WBS1='H'` and `ChargeType='H'` are for two separate columns, yeah?

and that the second one overrides the first, yes?

so that's down to how you coded your CASE",2.0
g0o7vqc,i5d2ac,"Whats your table look like and what's the desired results? It sounds like maybe you should do a join with a temp table storing the ChargeType and ChargeDescription instead of a CASE statement, but that's just a guess.

Order matters in CASE statements, so if there are multiple matches whichever occurs first will be the ""returned"" value.",2.0
g0olfij,i5d2ac,thank you! :),1.0
g0o7yhn,i5d2ac,The lines in a case statement are solved sequentially. So if you want  one to take precedence over another just put it further up in the order!,2.0
g0o9030,i5d2ac,"thats what I figured, I put the first case statement I wanted to take precedence first, but ""Accrued Benefits' is still only getting applied to the 'S' in that list I created",1.0
g0o83k1,i5d2ac,It doesn’t make sense to make it both in the first When (‘H’) and then used again in another case. It has to be one or the other. You need to use some other condition to determine when it should be Accrued Benefits versus Overhead.,1.0
g0o9gy1,i5d2ac,"ok, let me think about this",1.0
g0okimb,i5d2ac,"before we would decide that we need another physics maybe pull all the base data that you are working with to see what is being fed into the case statement? like 


    select apex.*, pr.*, &lt;your other columns&gt; 
    ......

and since this is character data check for some weirdness like non-printable characters, characters from other encodings, etc.",1.0
g0sd3cw,i58v1g,"Hi,

I prefer to implement this kind of rule in code, not at the database level. If the rule changes, I find it a bit difficult.",1.0
g0orxgf,i568sy,I have seen it done with a windowing function and and a recursive CTE.  I will check to see if I can find the code.,1.0
g0n5216,i55hi7,Make a sub query the the max(period) for each student then join back to the rest of your query to get all the info you need,1.0
g0n2ab4,i54xd9,"Try this:

insert /*+ ignore_row_on_dupkey_index ( enzyme ( enz_name ) ) */
into enzyme
SELECT enz_name FROM EXTERNAL ((
  construct_id NUMBER(10),
  n_term VARCHAR2 (50),
  enz_name VARCHAR2 (3),
  c_term VARCHAR2 (50),
  cpp VARCHAR2 (50),
  mutations VARCHAR2 (50),
  mw_kda NUMBER (6, 2))

    TYPE ORACLE_LOADER
    DEFAULT DIRECTORY data_to_input
    ACCESS PARAMETERS (
        RECORDS DELIMITED BY NEWLINE
        skip 1
        FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""'
        MISSING FIELD VALUES ARE NULL 
       ) 
    LOCATION ('CONSTRUCT.CSV')
    LOG ERRORS INTO ERR$_ENZYME ('INSERT') REJECT LIMIT UNLIMITED)) ext
    where not exists (
        select * from enzyme e
        where e.enz_name = ext.enz_name
    );",1.0
g0o6j01,i54xd9,How is this different from my code?,1.0
g0o7xua,i54xd9,Added extra ) before ext per error msg,1.0
g0oqvb8,i54xd9,"ahh thanks! tried that, but it still gives me the same error message.

&amp;#x200B;

Error at Command Line : 81 Column : 5

Error report -

SQL Error: ORA-00907: missing right parenthesis

00907. 00000 -  ""missing right parenthesis""",1.0
g0p3wzx,i54xd9,"Okay, i would just syntax for select from external tables. I am not familiar with oracle but it seems to be the case of misplaced or missing paranthesis either you have an extra ( or you are missing )",1.0
g0pgkj7,i54xd9,"OK I tried reformatting it but I don't find an issue with the parenthesis:

insert

	/\*+ ignore\_row\_on\_dupkey\_index ( enzyme ( enz\_name ) ) \*/

into

	enzyme

SELECT

	enz\_name

FROM

	EXTERNAL (

( construct\_id NUMBER(10)

, n\_term VARCHAR2 (50)

, enz\_name VARCHAR2 (3)

, c\_term VARCHAR2 (50)

, cpp VARCHAR2 (50)

, mutations VARCHAR2 (50)

, mw\_kda NUMBER (6, 2)

) 

TYPE ORACLE\_LOADER DEFAULT DIRECTORY data\_to\_input 

ACCESS PARAMETERS ( RECORDS DELIMITED BY NEWLINE skip 1 

FIELDS TERMINATED BY ',' 

OPTIONALLY ENCLOSED BY '""' 

MISSING FIELD VALUES ARE NULL 

) 

LOCATION ('CONSTRUCT.CSV') 

LOG ERRORS INTO ERR$\_ENZYME ('INSERT') REJECT LIMIT UNLIMITED

) ext

			

where

	not exists

	(

		select \*

		from

			enzyme e

		where

			e.enz\_name = ext.enz\_name

	)

;

&amp;#x200B;

Have you tried pullling out the select piece and checking to make sure that works? Also I've had more luck with MERGE when it comes to managing inserts when you need to handle exceptions.

Also curious why you don't just use MINUS to find the new records then insert.",1.0
g0priss,i54xd9,"I figured out that I just had the 

    LOG ERRORS INTO ERR$_ENZYME ('INSERT') 

statement in the wrong place. 

&amp;#x200B;

It should go in the last line, into the outer insert. 

&amp;#x200B;

The select statement by itself only worked if the LOG ERRORS line wasn't in there. 

&amp;#x200B;

Oh and because I've never heard of using MINUS to find new records! How do you do that?",1.0
g0pv6t6,i54xd9,"SELECT Enzyme

FROM A

MINUS 

SELECT ENZYME 

FROM B

will only give you the rows in A not in B. And as a side benefit it should eliminate duplicates as well.  Some SQL implementations use the keyword EXCEPT instead of MINUS but IIRDC Oracle uses MINUS",1.0
g0mrmhl,i53qxr,"SQL first, but both eventually.",8.0
g0mw526,i53qxr,But why is python making more sense to me then? People say sql is easier but I feel like python helps you understand the data your analyzing. Idk. I guess I just find python more interesting so far but maybe when it gets very complicated I’ll understand why I don’t necessarily need it for analytics role,-1.0
g0mrxvu,i53qxr,"That depends if you are the one retrieving the data you analyze. SQL is more useful if you are pulling the data yourself or if you need to look at raw data sources to investigate data quality problems. If your expertise is in the actual analysis, python can be useful for postprocessing the data. But honestly? If you're going to be an analyst, you're most likely working with some sort of business intelligence software like Tableau or Business Objects. Regardless, you should pursue a solid foundation in statistics.",2.0
g0mu2y8,i53qxr,"SQL is a must. It is going to be a crucial part of any data analyst job. However, if you're just starting out, learn Python. Learning Python before SQL follows the classic manual before automatic car analogy. If you can program in Python, and can do merges and joins in Pandas, you'll be able to learn SQL in a month. It doesn't work the other way around.",2.0
g0mvsu9,i53qxr,"Okay thank you so much for saying this because that has been my exact thoughts. I’ve gone through about 6 chapters of python and then I started sql and I was like... this stuff makes sense to me because of the little info I’ve already learned about python? So I’ve been confused. Yet, everyone keeps saying for analytics roles I should do sql and tableau",2.0
g0n5wtv,i53qxr,It does depend on the job. Some firms might have separate people doing model building using SQL and analysts doing their whole job in tableau. Others won't even hire an analyst who can't write their own SQL. I've worked in both and much prefer the second.,3.0
g0mx3sp,i53qxr,"Might be a good idea to hop on Indeed or LinkedIn and take a look at the job postings you desire. See what the skills needed are. For example, is it visual analytics you are interested in? If so, Tableau/SQL are great tools to use. If you’re more interested in ETL than actual visualization, then maybe Python is the way to go. I don’t think any language or software is “easy” to learn. For example, I know people who think they know Tableau but are they making sure to read up on visualization best practices, like removing chart junk, using colors/fonts appropriately, telling a story with data, providing the best visualization given the type of data?  I do agree that if you know python you’ll very quickly understand SQL but again, that doesnt mean you “know SQL”. 

Would be pretty sucky if you devote time to python and you land a job where you are writing queries to pull data in via sql and then visualize with Tableau...but all you really truly “know” is python because that is what you’ve spent your time learning. Now you’re having to invest time learning the other skills mentioned and very quickly those python skills start to fade.  Just my .02",1.0
g0mxykk,i53qxr,Thank you SO MUCH for this information. I have been looking at different qualifications for jobs and they all very. I think my worry was that since my degree is in marketing and have no analytical experience- I don’t want do take the easiest of the courses. I just wanted to try to take the most relevant course that’s of the most difficulty to prove motivation/ and that I’m able to learn other programs. My background isn’t in computer science or economics and I feel that puts me at a huge disadvantage. What your saying makes complete sense though.,1.0
g0mzjxi,i53qxr,"No problem. It’s a big decision but try to have a vision for what you want and then execute. If this is something you want to make a career out of then no matter what path you take it’s going to be a lot of dedication of time, energy, and willpower. Hopefully it’s something you’re passionate about because it makes things much easier when you know that all the sacrifices you are making are for a reason. For me, I made a transition to data analytics about 2.5 years ago. I started learning Python and took a Data Science immersive course at Galvanize (no affiliation).  I learned a lot but I’m not using those data science skills at the moment. So the time and money I spent are fading, although I try to keep up as much as I can.  In hindsight, There was a very slim chance that I would be a data scientist or be trusted to do machine learning projects right out of the gate - most companies want software developers and people with advanced python skills (or people really good with statistics/probability).  That is where I want to go but first step first...For me, I’m in a data analyst role focused primarily on data visualization. SQL/Tableau serve me well and I am the guy that winds up making visualizations for the actual research scientists at work. I get to learn from them (they mostly use R) and I also get to better my visualization skills at work at the same time. The way I look at it is I’m getting good with SQL/Tableau as my day job and then in my spare time I practice statistics/probability or play around with python on the side.  

But again it’s all about your interests and goals...really think about that first and then your decision will be a little more clear ;)

Edit: grammar",1.0
g0mxqt0,i53qxr,"Yeah, that's the weird thing. Tableau and SQL are the things that will land you the job. So you'll need those. That said, everyone knows those and your options end there. With Python, data analysis can springboard into data science, data architecture, statistics, and AI (which is also statistics).

So don't neglect them but give Python 6 months using Pandas and then you'll fly through those courses. 😄",1.0
g0myb50,i53qxr,That makes sense. Worry about pleasing the hiring manager then worry about your personal growth after. Lol. Just how the world works.,1.0
g0mytrd,i53qxr,"Wouldn't it be nice if it wasn't... ಠ_ಠ

Best of luck. hmu whenever if you want to talk python. And consider getting a normal job that doesn't use programming and automating it. Programming is a superpower for every job that doesn't require programming ;)",1.0
g0mz12x,i53qxr,Thank you! You’ve helped me a ton. I will,2.0
g0mvy3j,i53qxr,Honestly I feel like sql without python knowledge is hard because Atleast in my mind I want to understand why/ how these data tables exist before I try to analyze it.,2.0
g0mxutu,i53qxr,I couldn't agree with you more!,1.0
g0n63hg,i53qxr,"Here to agree with this. I learned SQL first. I took a Python programming course, and the Professor mirrored this statement, ""SQL is as elegant a language as there is. If you are coming with knowledge of SQL, you may have smoke coming out of your ears.""

It was exactly how I felt. I goof around in R, and STILL have trouble and need some hand-holding. I may need to migrate back to Python.",2.0
g0n9isz,i53qxr,I truly feel when I am learning SQL it’s  going in one ear and out of the other. What’s the point in analyzing data without understanding how it’s being created. It just seems backwards to me. SQL is memorization and Python is problem solving which I enjoy.,2.0
g0n9xbz,i53qxr,"Haha, I can see that. Sounds like an honest teach. Clanzomaelan, come back to us. SQL is elegant and R is unbeatable at statistics but can either email your boss  an automated report at 2 am to show how hard you work for them? 😁",1.0
g0zkwve,i53qxr,Ooooooh... I am intrigued... Any suggested reading on this automation you speak of?,1.0
g10b41c,i53qxr,"Automate the boring stuff by al sweigart is free and classic. Or you just need to learn the mail module, how to read files, and the time module at minimum. 😅",2.0
g0mfwue,i52lid,"It sounds like you are describing a common table expression.

;with cte_temp as (select this from that) select this from cte_temp;",5.0
g0mget0,i52lid,"yup this was it, thank you!
i haven't used it in a while i prefer it over select into #temp",1.0
g0o8h7y,i52lid,"Cte's are good, but can only be referenced by the statement immediately after. The other syntax is table variables which you can create as follows:

declare @table table (ID int, column varchar(max))

Then you can use @table as a regular table for the duration of your query, and it will be deleted at the end of the query.",2.0
g0mt3hs,i52lid,Is that the same concept as an inline table?,1.0
g0o87xs,i52lid,"    SELECT *
    INTO new_table
    FROM old_table",1.0
g0n86zm,i52lid,It seems like a google search would have been quicker,-1.0
g0o6tdx,i52lid,"yeah i was searching and couldnt find it, reddit is my last resort. thanks for the tip einstein",2.0
g0nzvmx,i52lid,Try this: https://lmgtfy.com/?q=selecting+into+a+non+existent+table+mssql,0.0
g0o6oyr,i52lid,"wowwww it's 2010 all over again, i love throwback humor. thanks for the memories. you're the last human being that thinks this is clever",1.0
g0on8mm,i52lid,I'll take it as a compliment - at least I can think,1.0
g0onnkk,i52lid,"Yeah ok Einstein over here can't resolve that google was already my first place for inquiry. Welcome to the 21st century. Reddit is my last resort, ok shit for brains? stop acting like you never come here for help",1.0
g0oordr,i52lid,"Ah ok I didn't know that
Sorry for my insult",1.0
g0mflls,i51orl,"There's a lot of different options for databases. But you can install developer edition of sql server 2019 for free.

Id recommend picking up a sql book by itzik ben-gan that seems like the quickest way i can think of to get you started.

Im also a huge fan of sql in 10 minutes by ben forta. But its more of a short reference book then something id suggest reading and learning from.",1.0
g0mhjs5,i51orl,"As someone who took an oracle class in college this last semester (crappy book, btw), I'm really happy to see reference to both books here.  I appreciate it!  I'm going to add in here: tons of youtube videos on SQL that are good, as well.",2.0
g19at1g,i51orl,thanks allot,1.0
g0oe75r,i51orl,"Check out w3schools, they have explanations and exercises. Check out codecadamy, they have a free course on sql and as you learn something new you are required to program it, very beginner friendly. You can look up freecodecamp sql on youtube. 

I would recommend codecademy for start.",1.0
g19atx9,i51orl,thanks allot,1.0
g19awcz,i51orl,"Sure thing, hope it helps.",1.0
g0of0ok,i51orl,Kudvenkat and Wise SQL Owl on YouTube have great tutorials,1.0
g19auxx,i51orl,thanks allot,1.0
g0lmeqx,i4yob1,In MSSQL 2012+ you can just use SUM(transamt) OVER (ORDER BY transdt ASC ROWS UNBOUNDED PRECEDING PARTITION BY accountid). Check the difference between ROWS and RANGE UNBOUNDED.,6.0
g0ln1af,i4yob1,"Something, something ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. 

Read this:

https://www.red-gate.com/simple-talk/sql/t-sql-programming/introduction-to-t-sql-window-functions/",2.0
g0s17pt,i4yob1,"This might work...

&amp;#x200B;

     -- assuming 1 starting balance record per accountid
    SELECT tt.id, tt.accountid, tt.transDt, tt.transAmt
         , balance = ISNULL(sb.[Start], 0) 
                   + SUM(tt.transAmt) OVER (PARTITION BY tt.accountid ORDER BY tt.id ASC ROWS UNBOUNDED PRECEEDING)
    FROM dbo.tmptransactions tt
    LEFT OUTER JOIN dbo.startbalance sb
    ON tt.accountid = sb.accountid
    ORDER BY tt.id ASC;
    
     -- assuming multiple starting balance records per accountid based on period
    SELECT tt.id, tt.accountid, tt.transDt, tt.transAmt
         , balance = ISNULL(sb.[Start], 0)
                   + SUM(tt.transAmt) OVER (PARTITION BY tt.accountid, ISNULL(sb.id, 0) ORDER BY tt.id ASC ROWS UNBOUNDED PRECEEDING)
    FROM dbo.tmptransactions tt
    LEFT OUTER JOIN dbo.startbalance sb
    ON tt.accountid = sb.accountid
    AND tt.transDt &gt;= CAST(DATEFROMPARTS(sb.[year], sb.[month], 1) AS DATETIME)
    ORDER BY tt.id ASC;",1.0
g0lob6e,i4ya3r,So you will need to start with the sub query and find the min date for that user. Do that in the where clause. The data you want to pull out should be in the select clause Then you can use the data returned by that sub query to compare to in the outer query.,1.0
g0lw79f,i4ya3r,"```sql
select ur1.user_id, ur1.date, ur1.revenues
from users_revenue ur1
where revenues &gt; (select ur2.revenues
                  from users_revenue ur2
                  where ur2.user_id = ur1.user_id
                  order by ur2.date
                  limit 1)
order ur1.by user_id, ur1.date;
```

For each user in the outer query, the sub-query returns the revenue of the first date of that user. This is called a co-related sub-query. The outer query only returns those, where the revenue is bigger than that. 

Another option is to use a window function: 

```sql
select user_id, date, revenues
from (
  select user_id, date, revenues,
         first_value(revenues) over (partition by user_id order by date) as first_revenue
  from users_revenue
) t
where revenues &gt; first_revenue
order by user_id, date;
```",1.0
g0kzln3,i4v9rg,Make a pivot but use MAX as the aggregate.,19.0
g0lcgpf,i4v9rg,"bingo, as long as you can guarantee that the uniqueness between the rows, columns and the aggregated value, MAX or MiN will return the original value. Even better, it works on varchar not just numbers, unlike in excel.",6.0
g0lchxi,i4v9rg,"This is what I was missing, didn't realize it was this simple, thank you very much!",7.0
g0l03eg,i4v9rg,"First, that's a Common Table Expression (CTE), not a temporary table. Not trying to nitpick, but they are different concepts in Oracle.

You don't want to use an aggregate function, but you seem to be dropping the third row in temp. What are you doing with that? Offhand it looks like you're taking the minimum price per product\_id/shop combination. 

So this gives you your desired result: [http://sqlfiddle.com/#!4/0afd3/2](http://sqlfiddle.com/#!4/0afd3/2) (I used min(price).)",4.0
g0li3s2,i4v9rg,"select product\_id

,min(case when shop = 'Ex1' then price else null end) over(partition by product\_id,shop order by price) as Ex1

,min(case when shop = 'Ex2' then price else null end) over(partition by product\_id,shop order by price) as Ex2

,min(case when shop = 'Ex3' then price else null end) over(partition by product\_id,shop order by price) as Ex3

from temp

might also work and save you from using a CTE.",2.0
g0lgy28,i4v9rg,There is a PIVOT function in Oracle you could leverage. I’m not sure what you are trying to do but I think it’s better to leave the data normalized in that format.,2.0
g0l60wr,i4v9rg,"You have to tell the database what to do if there are multiple data items ending up in the same cell; that's the point.

Such as it is, MIN(item) is an aggregrate function that, when used on only one item, returns the item itself.",1.0
g0lcbuq,i4v9rg,"This is actually the right solution. I can't believe I missed this, thank you very much :)",1.0
g0kydd1,i4v9rg,"I usually use a CTE for this. I haven't written Oracle in awhile but this should be the basic gist of it:

    with t1 as (
    select product_id,
        case when shop = 'example1.com' then price as example1.com,
        case when shop = 'example2.com' then price as example2.com ,
        case when shop = 'example3.com' then price as example3.com 
    from whatever_the_left_table_is)
    , t2 as (select product_id, max(example1.com) as example1.com,
    max(example2.com) as example2.com,
    max(example3.com) as example3.com
    group by product_id
    )
    select * from t2

If you want to do this dynamically, unfortunately I'm not aware of any method of doing this outside of using some language to dyanmically generate the above style of syntax.  Again, I haven't used Oracle in awhile so they might have some syntax that lets you do this.  SQL Server does have some syntax for this but when I was more heavily using it, I ended up resorting to the above style query because the syntax was also not dynamic in SQL Server so I wrote something more agnostic.  You can use PL/SQL to generate it if you're familiar with it.  If I need to dynamically pivot, I usually write a function in Python that handles that generates the SQL function to run, personally.",1.0
g0ldj1d,i4v9rg,"Thank you for your help, the solution was actually to just use min or max as an aggregate function which returns the correct price as there aren't duplicates. This flew over my head somehow, but I appreciate your feedback.",1.0
g0kyeb0,i4v9rg,"**I found links in your comment that were not hyperlinked:**

* [example1.com](https://example1.com)
* [example2.com](https://example2.com)
* [example3.com](https://example3.com)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g0kydd1&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",-5.0
g0l9us4,i4v9rg,"There is a way to do dynamical pivots in Oracle, but it's not the simplest thing.

https://technology.amis.nl/2006/05/24/dynamic-sql-pivoting-stealing-antons-thunder/

Though from what it seems OP doesn't need it to be dynamic, and can just use the in-built PIVOT function.",1.0
g0l07nl,i4v9rg,"Never used oracle before, but based on my SQL experience, you'd need to use an aggregate function. When pivoting, you're reducing the number of rows by aggregating the data. This is the same as pivoting in excel.

I see there's duplicate values from your example, so AVG function (average) should give you your desired results.",1.0
g0l0dkf,i4v9rg,"with FakeCrossTab as

(select product\_id

, case when shop = 'example1dotcom' then price else null end as example1dotcom

, case when shop = 'example2dotcom' then price else null end as example2dotcom

, case when shop = 'example3dotcom' then price else null end as example3dotcom

from temp

)

, as example1dotcomPriceSorted

(

select product\_id

,example1dotcom

,row\_number() over (partition by product\_id order by example1dotcom ) as SortedPrice

from FakeCrossTab

where example1dotcom is not null

)

\--repeat previous CTE for each product (two more times)

\--then bring it all together

select product\_id

,example1dotcom

,example2dotcom

,example3dotcom

from example1dotcomPriceSorted A

full out join example2dotcomPriceSorted B

on A.product\_id = B.product\_id

and B.SortedPrice = 1

full out join example3dotcomPriceSorted C

on A.product\_id = C.product\_id

and C.SortedPrice = 1

where  A.SortedPrice = 1

 

note I have had issues sometimes with SQL not honoring the full outer join, in those cases create another CTE of just the poduct IDs. I've also had SQL not honor the row\_counter either and in those just do another set of CTEs to choose row 1 from the prior CTEs.

(So technically I'm cheating as I'm treating row\_number as a non-aggregating function which in thise case it is not used as an aggregator)",1.0
g0ldjy9,i4v9rg,"Thank you for your help, the solution was actually to just use min or max as an aggregate function which returns the correct price as there aren't duplicates. This flew over my head somehow, but I appreciate your feedback.",2.0
g0l0eeu,i4v9rg,"**I found links in your comment that were not hyperlinked:**

* [example1.com](https://example1.com)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g0l0dkf&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",-1.0
g0kxh50,i4ulxr,"You can store any vendor specific data in json column and determine what to do in application code. Maybe extract some ""shared"" information, that'll always be for all providers.

So minimal working version would be (id, payment\_method\_id, json\_data). And by payment\_method\_id your application should be able to determine how (to which class) to deserialize data and then process it.",3.0
g0kyyt9,i4ulxr,"That makes sense, thank you!",1.0
g0l2q6q,i4ulxr,"You have a few options. If there is going to be huge variance between the data stored for each type then your foreign key would probably be to a payment methods table which only held the payment type and then had it's own links to which ever payment specific tables you wanted to setup. It would also probably link to a donor_id type table depending on how you're handling that (i.e. requiring registration or allowing guest donation).

Or for simplicity do what you said - just keep everything in the one table. Yeah there'll be a bunch of NULL values, that may not matter as much as you think until you get to very high numbers of transactions. That doesn't feel elegant though.

In reality, you would hand off some of this to a third party payment provider and you would only store their response (either as a raw JSON/XML object or just external references) and very high level details. You would avoid storing their actual payment details as much as possible.",3.0
g0kqhzd,i4ulxr,"Hello u/BytesBeltsBiz - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g0kr4i4,i4ulxr,PostgresSQL,1.0
g0l3wci,i4th5b,"Does Postgress support 

SELECT TOP 1 \* INTO NEWTABLE FROM OLDTABLE ; 

DELETE FROM NEWTABLE;",1.0
g0l5i7s,i4th5b,I don't think so,1.0
g0l42ey,i4th5b,"what about pgdump with the --schema-only option?

[https://www.postgresql.org/docs/9.1/app-pgdump.html](https://www.postgresql.org/docs/9.1/app-pgdump.html)",1.0
g0jn5zq,i4odb9,"What have you got so far? If the problem is that you're not sure how to approach it, try creating a BEFORE INSERT trigger on the table. 

In the trigger, check your constraints and if they're violated then use SIGNAL to signal an error. (e.g. SIGNAL SQLSTATE '45000').

If a BEFORE INSERT trigger errors out then the insert won't be done.",1.0
g0k8isr,i4odb9,Have you looked at the constraint checks (8.0.16 or later -  [https://dev.mysql.com/doc/refman/8.0/en/create-table-check-constraints.html](https://dev.mysql.com/doc/refman/8.0/en/create-table-check-constraints.html) ) for a through d? Or an ENUM for e?,1.0
g0l5w3n,i4odb9,"a, b, c, d should be check constraints

e should be a foreign key to a table listing valid types

f - in Oracle I would create a check constraint on a materialized view. mySQL has materialized views but you would have to check if you can put a check constraint on it. Oracle example: [https://www.sqlsnippets.com/en/topic-12896.html](https://www.sqlsnippets.com/en/topic-12896.html)",1.0
g0ls36o,i4odb9,No materialized view in MySQL.,1.0
g0lt4hz,i4odb9,"You're right, you can implement your own, but it's not built-in. Example [https://fromdual.com/mysql-materialized-views](https://fromdual.com/mysql-materialized-views#implement)",1.0
g0jb9jp,i4muwf,"You may get biased results in this sub, but yeah I'd say SQL is an important skill for web/mobile developers. It's possible that some of your projects will use some other kind of backend but sooner or later it is extremely likely you will interact with a SQL database and SQL queries in one form another. In fact, I would be fairly surprised if you didn't. Sometimes the actual SQL will be abstracted away via other tools but it helps to know what the hell is going on. Even something as basic as administering a Wordpress site can benefit from knowing some SQL.

I can't comment on that course as I've never done it but the price seems very low so it wouldn't seem a particularly high risk investment. If you're strapped for cash then there are free options out there though.

So yes. And I wouldn't even consider it a detour.",5.0
g0jckh0,i4muwf,"Thank you, very helpful suggestion. I havent done much data work outside of excel and some hobby projects in Python I feel that SQL is so widely used it would be a useful investment to dip.",1.0
g0jdybg,i4muwf,SQL is always good to have in your toolbox.,5.0
g0ko9uo,i4muwf,Listen to this man op...hez right.,1.0
g0iz427,i4jh38,"There are three main ways:

update of an inline view (what I usually do)

update ... where exists ...

merge

&amp;#x200B;

All three are described here:

[https://oracle-base.com/articles/misc/updates-based-on-queries](https://oracle-base.com/articles/misc/updates-based-on-queries)",2.0
g0lqrvo,i4jh38,Alias the outer and you can reference in the subquery. So tablename t in the outer and ...field = t.field in the subquery.,1.0
g0ivonl,i4jdzt,"It's not that there's alreaydy PKs in the database with those names but that your tables have two or more rows that have the same value in the field you're trying to define as the PK. You can confirm that this is actually the case if the following query returns any results:

    select Section_ID, count(*)[NumRows]
    from Section 
    group by Section_ID
    having count(*) &gt; 1",4.0
g0l2rx5,i4jdzt,"okay, I don't fully understand this... YET.  It does however, give me a new direction to research and learn. So, thank you for the explanation and help!",2.0
g0l4th9,i4jdzt,"What he's saying is that the error isn't coming from your database design, it's coming from the data that's in the tables. You're saying that an index must be unique but the data in the table isn't unique.",3.0
g0iqzv5,i46hfq,This is fantastic,3.0
g0j77gy,i46hfq,ELIF: What's the white dot represent?,2.0
g0jw15x,i46hfq,That’s by mistake,3.0
g0jx7qz,i46hfq,Might be syntax error 😁,1.0
g0jb9ns,i46hfq,It's midnight and I'm laughing hysterically. Thank you.,2.0
g0jk3bh,i46hfq,JoHn* 😂😂,2.0
g0ihovb,i45c0r,I hate you... take my upvote.,16.0
g0inngm,i45c0r,Lol you take mine too,7.0
g0j6x5s,i45c0r,Can I JOIN you?,5.0
g0j2lz0,i45c0r,"I've always heard the version:

Why do front end developers always eat lunch alone?

Because they can't join tables.",11.0
g0j3scg,i45c0r,I saw a meme for it lool!!,3.0
g0j8k6g,i45c0r,"Why do you never ask SQL developers to help you move house?

Because they always drop the tables.",6.0
g0igvr8,i45c0r,There's a joke in here about turning in your primary keys but I'm not funny enough to find it,12.0
g0j8yv4,i45c0r,You're RIGHT. It's time I LEFT.,3.0
g0jlid4,i45c0r,I’m OUTER here...,2.0
g0jx50l,i45c0r,This thread is making me CROSS,1.0
g0iz1j6,i45c0r,"I laughed.

Now I'm sad.",3.0
g0i6pyr,i45c0r,Classic,5.0
g0i6set,i45c0r,Lol,1.0
g0jj0iv,i45c0r,I hate the fact that I laughed. An upvote I'm not proud of.,2.0
g0kkvdl,i45c0r,Hilarious 😂 up vote!,2.0
g0i9v7j,i45c0r,And the left table says: ERROR 1064 (42000),2.0
g0ip9pg,i45c0r,Haaaaaaa,1.0
g0hxxg5,i4f86z,"this is a semantic error

you've overlooked an important characteristic of compound logical conditions -- **ANDs take precedence over ORs***

you this is what you wrote ==

    SELECT COUNT(GuestId)TotalBooked 
      FROM GuestStayDetails 
     WHERE (@ArrivalDate = ArrivalDate) 
        OR (@DepartureDate = DepartureDate) 
        OR (@ArrivalDate &lt; ArrivalDate AND u/DepartureDate = DepartureDate) 
        OR (@ArrivalDate = ArrivalDate AND u/DepartureDate &lt; DepartureDate) 
        OR (@ArrivalDate &lt; ArrivalDate AND u/DepartureDate &gt; DepartureDate) 
        OR (@ArrivalDate &lt; DepartureDate AND u/DepartureDate &gt; DepartureDate) 
        OR (@ArrivalDate BETWEEN ArrivalDate AND DATEADD(Day, -1, DepartureDate)) 
        OR (@DepartureDate BETWEEN DATEADD(Day, -1, ArrivalDate)AND DepartureDate) 
       AND Student = 1 
       AND Cancelled = 0 
       AND GuestStayDetails.GuestId != u/GuestId

and this is how it's evaluated -- note how i have used parentheses to combine the ANDs to illustrate which OR condition they attach to --

    SELECT COUNT(GuestId)TotalBooked 
      FROM GuestStayDetails 
     WHERE (@ArrivalDate = ArrivalDate) 
        OR (@DepartureDate = DepartureDate) 
        OR (@ArrivalDate &lt; ArrivalDate AND u/DepartureDate = DepartureDate) 
        OR (@ArrivalDate = ArrivalDate AND u/DepartureDate &lt; DepartureDate) 
        OR (@ArrivalDate &lt; ArrivalDate AND u/DepartureDate &gt; DepartureDate) 
        OR (@ArrivalDate &lt; DepartureDate AND u/DepartureDate &gt; DepartureDate) 
        OR (@ArrivalDate BETWEEN ArrivalDate AND DATEADD(Day, -1, DepartureDate)) 
        OR ( /* OR block */ 
           (@DepartureDate BETWEEN DATEADD(Day, -1, ArrivalDate)AND DepartureDate) 
       AND Student = 1 
       AND Cancelled = 0 
       AND GuestStayDetails.GuestId != u/GuestId
           /* end OR block */ )

so you see, those last 3 ANDed conditions only combine with the last OR condition

what i ~think~ you meant was this --

    SELECT COUNT(GuestId TotalBooked 
      FROM GuestStayDetails 
     WHERE (
           @ArrivalDate = ArrivalDate 
        OR @DepartureDate = DepartureDate 
        OR @ArrivalDate &lt; ArrivalDate AND u/DepartureDate = DepartureDate 
        OR @ArrivalDate = ArrivalDate AND u/DepartureDate &lt; DepartureDate 
        OR @ArrivalDate &lt; ArrivalDate AND u/DepartureDate &gt; DepartureDate 
        OR @ArrivalDate &lt; DepartureDate AND u/DepartureDate &gt; DepartureDate 
        OR @ArrivalDate BETWEEN ArrivalDate AND DATEADD(Day, -1, DepartureDate 
        OR @DepartureDate BETWEEN DATEADD(Day, -1, ArrivalDateAND DepartureDate 
           )
       AND Student = 1 
       AND Cancelled = 0 
       AND GuestStayDetails.GuestId != u/GuestId

note the explicit parentheses

please note also how i have removed the unnecessary parentheses you had around each OR condition",4.0
g0i3r4c,i4f86z,Thank you so much! Great help and thank you for the explanation. I am just a  beginner so learning as I go.,2.0
g0hvtbe,i4f86z,"The only two things that jump out at me right away are GuestId != 'u/GuestId'

or possibly wrap all your ors in an over all (), and all of your ands in an over all () something like this?

    SELECT COUNT(GuestId)TotalBooked FROM GuestStayDetails WHERE 
    	(
    		(@ArrivalDate = ArrivalDate) 
    		OR (@DepartureDate = DepartureDate) 
    		OR (@ArrivalDate &lt; ArrivalDate AND u/DepartureDate = DepartureDate)
    		OR (@ArrivalDate = ArrivalDate AND u/DepartureDate &lt; DepartureDate) 
    		OR (@ArrivalDate &lt; ArrivalDate AND u/DepartureDate &gt; DepartureDate) 
    		OR (@ArrivalDate &lt; DepartureDate AND u/DepartureDate &gt; DepartureDate) 
    		OR (@ArrivalDate BETWEEN ArrivalDate AND DATEADD(Day, -1, DepartureDate)) 
    		OR (@DepartureDate BETWEEN DATEADD(Day, -1, ArrivalDate)AND DepartureDate) 
    	)
    	AND Student = 1 AND Cancelled = 0 AND GuestStayDetails.GuestId != 'u/GuestId'",1.0
g0hwfop,i4f86z,"I think the u/ is how my code copied over. They have an at symbol in front. I tried all the brackets, quotes everything I could think of. Rearranged the AND/OR/NOT statement. I tried it then with a more basic  query and it seems that there is something more specific needed to exclude a row from a count",1.0
g0hwzx5,i4f86z," 

[andrewsmd87](https://www.reddit.com/user/andrewsmd87/) your suggestion worked for me!! Something so simple, I have been at this for 2 hours. Thank you so much!!!",1.0
g0hx6ot,i4f86z,"When I see stuff like this at work, it can often be cleaned up by judicious use of coalesce.",1.0
g0ic0xl,i4f86z,"   Conditional count:

 Select sum(case when col1='a' then 1 else 0 end) as myCount from mytable;",1.0
g0hrzj2,i4ea8x,"&gt; so, only 2004/01/01, not twice or more

GROUP BY will accomplish this

however, you will need to place every other column in the SELECT list into an **aggregate function** such as COUNT(), SUM(), MIN(), MAX(), etc.",3.0
g0jalnk,i4ea8x,Thanks for the reply! I will try it out.,1.0
g0knyx1,i4ea8x,"So it seems to have worked. Thanks! 

This is the code after modifying it:

    SELECT  Type || "" "" || Category || "" "" || Attackon, 
    substr(Dateandtime,7,4) || substr(Dateandtime, 3,3) || ""/"" || substr(Dateandtime,0,3),
    Type,
    Category,
    Attackon,
    sum(Civiliankia),
    sum(Enemykia),
    sum(Iraqforceskilled),
    sum(Coalitionforceskilled)
    FROM Deaths_only
    GROUP by substr(Dateandtime,7,4) || substr(Dateandtime, 3,3) || ""/"" || substr(Dateandtime,0,3)
    ORDER by substr(Dateandtime,7,4) || substr(Dateandtime, 3,3) || ""/"" || substr(Dateandtime,0,3) ASC
    LIMIT 50",1.0
g0kp94z,i4ea8x,"&gt; So it seems to have worked

**seems** being the operative word

please read https://dev.mysql.com/doc/refman/8.0/en/group-by-handling.html, especially the part about transitive dependency

you would not be able to run your revised query in MySQL because ONLY_FULL_GROUP_BY is enabled by default

presumably SQLite works in a similar fashion

just please be aware that any non-aggregated columns in your SELECT that aren't in your GROUP BY may not show the values that you might have expected",2.0
g0krd20,i4ea8x,"I simplified the query a bit. I forgot that I don't need certain columns, and that just having each year would be more interesting. So I have the following query:

    SELECT  
    substr(Dateandtime,7,4) AS ""Deaths in Iraq War (Wounded on the left side, killed on the right)"",
    sum(Civilianwia) AS ""Civil Wound"",
    sum(Enemywia) AS ""Enemy Wound"",
    sum(Iraqforceswounded) As ""IrqForces Wound"",
    sum(Coalitionforceswounded) AS ""Coal Forces Wound"",
    sum(Civiliankia) As ""Civil Killed"",
    sum(Enemykia) As ""Enemy Killed"",
    sum(Iraqforceskilled) As ""Irq Forces Killes"",
    sum(Coalitionforceskilled) AS ""Coal Forces Killed""
    FROM Deaths_only
    GROUP by substr(Dateandtime,7,4)
    ORDER by substr(Dateandtime,7,4)  ASC
    LIMIT 50

I'm not sure what transitive dependency is, but I will take a look at the link you sent me.

Thanks for the help.",1.0
g0ktiod,i4ea8x,"okay, this is great

your SELECT and GROUP BY are now in accordance

by the way, instead of `substr(Dateandtime,7,4)` i would use `strftime('%Y',Dateandtime)`",2.0
g0lmy2q,i4ea8x,Thanks for giving an example of strftime. I haven't used that function yet.,1.0
g0hdqsd,i4cooj,"""Doesn't work"" isn't a valid Postgres error message, but if both columns are defined as `date`, the following should work:

```sql
select count(*)
from table_name
where start_date &gt;= date '2002-01-01'
  and completion_date &lt; date '2007-01-01'
```",5.0
g0hsaj4,i4cooj,"&gt; ""Doesn't work"" isn't a valid Postgres error message

upvote",3.0
g0heun9,i4cooj,"Sorry, the error was a syntax error. 

And thank you so much, this worked!! 🙏🏽",0.0
g0il85d,i4cooj,"This should work also:

SELECT COUNT(student_id)
FROM table_name
WHERE date_col
BETWEEN start_date AND end_date;

note that both ‘start_date’ and ‘end_date’ are inclusive.",1.0
g0gveu4,i49z9f,"No,  there's no need for uniqueness or anything like a key at all. Be aware though that you might be getting multiple ""duplicate"" combinations unless you take care of those in some other way.

E.g. you can write ""a inner join b on 1 = 1"" but, essentially you get the same result as ""a cross join b"".",3.0
g0gvq0d,i49z9f,"Okay, what if a key is actually necessary? Let's say it is compulsory to have a key. In that case, if there are 2 different rows with 1 same value(duplicate) of the column, won't it cause a confusion?  

Or will the rows of the other table (of the same value of the key), match to both the 2 rows with duplicate values of the key?",1.0
g0gyjoo,i49z9f,"&gt; Let's say it is compulsory to have a key. 

i'm not sure i follow this: ""compulsory"" to whom and by whose order?

&gt; if there are 2 different rows with 1 same value(duplicate) of the column, won't it cause a confusion?

declaring a subset of columns a key ensures that the combination does not have nulls and the combination is unique in the table.

&gt; will the rows of the other table (of the same value of the key), match to both the 2 rows with duplicate values of the key?

as i said, keys are unique, but that aside, logically you can assume that every combination of the records from both tables will be tested with the same condition if you have 2 tables with 2 records each a(a_id, a_value) and b(b_id, a_value) and a_value is 0 for every record, ""a join b on a.a_value = b.a_value"" will generate 4 result records.",1.0
g0h2n2o,i49z9f,"The unique and not null (depending on the flavor of the sql engine) is not enforced in general.

Primary key has to be both. Unique key has to be unique but null values may or may not be considered always unique (as null = null won't produce true but null, undefined value).

A key or an index can contain multiple identical values and nulls.",1.0
g0h7gci,i49z9f,"&gt;Primary key has to be both.

Interestingly, this is not always true in sqlite, it will allow Primary Keys with NULL values in some but not all cases.",2.0
g0hbiro,i49z9f,"the way I heard/read about it is a known bug that's just kept around for backward compatibility though, it is not a desired, recommended or standard behavior.",1.0
g0h4ev3,i49z9f,"&gt; Unique key

this is a frankenstein of terms. please avoid it. there's a perfectly standard 'unique constraint' term if you want to discuss it.

""A key"" is a superkey of a relation, not an index, and as such it cannot contain duplicate combinations.",1.0
g0j7ix1,i49z9f,I stand corrected. Came here to point out bad use of terminology and did it myself.,1.0
g0ikm4t,i49z9f,"&gt;i'm not sure i follow this: ""compulsory"" to whom and by whose order?

It's a software which demands so.. (at least to my knowledge)",1.0
g0ixzcb,i49z9f,"Still kinda vague, buy overall it's not a bad thing - usually sql engines can get better execution if some sort of cardinality ( and uniqueness is probably the best kind) can be assured.

It can also help writing queries if the desired granularity matches existing keys.

BUT you really need to explain yourself better for people to be able to help",1.0
g0h1v0k,i49z9f,"no, you can join on any condition, including `on true`

you do need a unique key (a primary key is a type of unique key) on the foreign side of a foreign key though",3.0
g0h32ac,i49z9f,"So, in any table, 1 key has to be unique? As in, the values shouldn't repeat.",1.0
g0h3fs2,i49z9f,"you can join 2 tables without any keys at all. you do not need a foreign key between tables to join them

you are confusing joins with foreign keys",2.0
g0ijqln,i49z9f,"Just like I mentioned in the description, let's assume that keys are necessary.",1.0
g0guy7s,i49z9f,"Hello u/ConsequenceRegular72 - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",2.0
g0i7brh,i49z9f,"No, you can join on any Boolean expression. I join on like statements all the time.",1.0
g0ijsh7,i49z9f,What if keys are compulsory!,1.0
g0ioi7j,i49z9f,"I've read some other comments, and I think asdf604asdf up there is right, I'm not sure what you're asking here. A primary key by definition must be unique (not to be confused with `index`), but this has nothing to do with a join per se. I think the best I can do to answer this is that you could join a key to a non-key with an equals if the values happen to match up. For example, you could have `Table A` with something like:

`KEY | VAL`

`1  | a, b, c`

and rearrange it as

`KEY | VAL`

`1 | a`

`1 | b`

`1 | c`

in which case `KEY` can no longer be a primary key, because it isn't unique. However, you could make it an `index` instead to preserve the efficiency, but still join on it like usual.

Foreign keys are about preserving the structure/normalization of your database, so for instance, if you had `col1` as a primary key somewhere, and `col2` as a foreign key tied to that, then if you were to drop a row with `col1` in it, you could choose one of a number of behaviors. You could have it drop the corresponding row from the child table, you could have it raise an error and refuse the transaction, you could prevent other actions from inserting into the child table, etc. It isn't necessarily mandatory, but it's generally a good idea to prevent unanticipated problems, and having a primary key, index, etc. will increase performance significantly.",2.0
g0gwjsv,i49isp,"Many times an error like that might be caused by not specifying a schema name, or specifying the wrong schema name. ""relation"" is another word for table.",3.0
g0guasf,i49isp,"without anything further to go on, that error message means that the relation ""conditions"" does not exist

care to share the statement that produced the error?",1.0
g0hcwsz,i49isp,Maybe your database is case sensitive?,1.0
g0igu3n,i49isp,"When you created the table did you put the name in quotes?  If so, you have to use the quotes and the exact same case every time you reference it.  Never use quotes around your table or column names when creating a table... that will keep the names case insensitive.",1.0
g0gkuzm,i47fou,"Select table1.columns
From table_with_records_I_need as table1
Left join table_missing_some_records as table2 on (conditions)
Where table2.not_null_column is null",6.0
g0hw662,i47fou,"    SELECT *
    FROM TableA
    LEFT JOIN TableB ON TableA.TableBId = TableB.Id
    WHERE TableB.Id IS NULL

Note that if you need this to be super optimized, using a where not exists is more performant than a left join",3.0
g0gr014,i47fou,"From what I understand is you don't need to join, you just need info from the Users table, but I'm a little confused. If so, then I would use a sub-query and the NOT IN function in the WHERE clause. Something like this:

    SELECT id, start_date
    FROM users 
    WHERE id NOT IN (SELECT user_id FROM survey)
    

Edit: I don't work with MySQL much, so I know this works for postgres, but not sure about MySQL.",5.0
g0ib7w1,i47fou,"I agree with the not in comparison operator. It is very simple and it does not seem that a hobby application would have enough data to worry about indexes.

No need to complicate with a join when a sub query will do, unless you just want to learn. In that case, I can’t think of a better reason to use a join.",3.0
g0ggb6o,i47fou,Use joins left or right and depending on the join add a where clause where you check your join key(a or b depending on your left or right join) to be null.,2.0
g0ghotm,i47fou,"Hey Razzzvan,

Can you give me an example? I just posted my table column names",1.0
g0gfjgy,i47fou,"Can you post your tables?
Also are you passing in a parameter of a user of and want to know if they did a survey, or do you just want a query that returns all users who did not complete the survey. 
And why do you need a date of the user has not completed the survey?  Like are there multiple surveys or date ranges you are interested in?  Because it seems like if you want to know who hasn’t completed the survey, just take the distinct list of user ids and return those not in the survey table. 
Select userid from user table where userid not in( select distinct userid from survey table)
This is not the most efficient query, but depends on scalability, is this for a commercial app or an assignment, or something else.",1.0
g0ghlko,i47fou,"&gt;Hey, I just posted my table. This is hobby but also something that may come in handy at the business I work for, so I might send it in. I am learning Android development. I would do a list of IDs but I have a getID methed . I don't think your method would work because of scalability. I need the start\_date because this will show me how late the survey is. For example, if the id does not exist, I can get how many days late it is from the start\_date. Let me know if this makes anymore sense",1.0
g0gngld,i47fou,So these two tables don’t have any common fields between them. You will either need a new table that tracks user surveys or add the userid onto the survey table because right now if you look at your survey table how would you know who completed the survey?,1.0
g0gomgc,i47fou,"The only shared column is ID. I know who completed my survey with a query like this. mci\_table is surveytable

$sql = ""SELECT mci\_table.\*, *users\_table*.*id*, *users\_table*.*start\_date*, *users\_table*.*end\_date* FROM mci\_table JOIN users\_table where  *mci\_table*.*id*='$id' &amp;&amp; *mci\_table*.*id*=*users\_table*.*id* "";

I want a query that will show that the ID does not exist in the table. So reading the IDs absence",1.0
g0gpsg6,i47fou,"Well both tables shouldn’t have the same primary ID. That is something you’ll want to change. 
You should have the user_id be a foreign key in your survey tables. And you’ll want the survey table to have it’s own primary key. 
What if a user does two surveys? Now the survey table has two of the same id’s?

Anyway.

If your query returns a null value you know that the user did not complete the survey.

Also if you just want a full list what I said earlier will work. Select * from user_table where id not in(select id from survey_table). You can change what you are selecting from the user table, start date, end_date whatever.",2.0
g0h2k91,i47fou,"Use NOT IN, Intersect or Minus function to check for the presence/absence of records across 2 tables",1.0
g0haotg,i47fou,"Does this ""entry date"" column already exist? Also, from what I gather from your post and from the comments, you can write a 1.Case statement  2.IF statement. 3.Create a stored procedure to get alerted when there is a date missing.",1.0
g0happn,i47fou,"**I found links in your comment that were not hyperlinked:**

* [1.case](https://1.case)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g0haotg&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",2.0
g0hbb56,i47fou,"Apps usually go for a nosql approach, everything retrieved via api. Not much use for tables anymore",1.0
g0he910,i47fou,"I like sub queries.  Not sure on your schema though.  I'm confused at which datas you want.You can get the number of active surveys and then decide what to do via app logic.

    SELECT
        usr.*,
        ( SELECT COUNT(SVY.USER_ID) FROM SURVEYS SVY WHERE SVY.USER_ID = USR.USER_ID ) as ACTIVE_SURVEY_COUNT 
    FROM 
        USERS usr",1.0
g0fw8rw,i44sg8,Yeah that’ll work. Although I think DATE_DIFF is a little more clear the math is the same (and that might just be my preference).,4.0
g0fwj84,i44sg8,will try thanks for the tip m8 !,1.0
g0f9n46,i41aw4,"SQLite might suit you. It runs on your own computer.

 [https://www.sqlitetutorial.net/sqlite-import-csv/](https://www.sqlitetutorial.net/sqlite-import-csv/)",1.0
g0fb30e,i41aw4,"My MAC Catalina doesn't allow me to install, any other recommendations for maybe a website instead of a local program on my computer? :)

Thanks for the help!",1.0
g0fk9ae,i41aw4,"Sqlite is preloaded onto all macs. You can open up a connection by using `sqlite3` on the terminal. From there you can import csv like so (taken from stackoverflow)

    sqlite&gt; create table foo(a, b);
    sqlite&gt; .mode csv
    sqlite&gt; .import test.csv foo",3.0
g0fncuq,i41aw4,"Oh okay wow this is so helpful! Thanks so much. New to this, I've always had data sets to work with at work but want to try playing around with my own data set. 

I'm not entirely sure how to run it from my mac. Is this from the command prompt? 

Such a newb sorry haha",2.0
g0ftj9t,i41aw4,"&gt;Is this from the command prompt? 

Yeah, in Terminal. When you run sqlite3 there it brings you into sqlite's own command line shell inside the Terminal and then you can issue sqlite commands like the ones /u/pilotInPyjamas posted above.",1.0
g0ir9xn,i41aw4,"&gt;create table foo(a, b);  
sqlite&gt; .mode csv  
sqlite&gt; .import test.csv foo  


Okay, i got it, awesome!!! Thanks so much! :)",1.0
g0fciuv,i41aw4,It might already be there. Try sqlite3 on the command line,2.0
g0fkvnn,i41aw4,Sqlfiddle,1.0
g0ffs8m,i41aw4,"If the file isn't too big, then maybe [ElephantSQL](https://www.elephantsql.com/plans.html) is an option",1.0
g0eipo3,i3wcgu,Are you the owner of this project? I came across it the other day,1.0
g0e9e0n,i3vhnn,"It's ok to repeat columns across different tables as long as the data itself isn't repeated. An sms message will be different to an email message, so it's ok to have an sms table and an email table both containing a message column. You can create a view joining all the tables later if you want to make it look like all the data's in the same table.",1.0
g0hodg3,i3vhnn,"Yes. The normalization problem.

If you have no experience in database design, but have some experience with OO programming languages, you can do a lot of normalization ""by accident"" by identifying your objects and attributes.

So for example: an SMS message is an object, a Messenger message is an object, an email is an object, a phone call is an object, etc.

Polymorphism in relational database design is something of an advanced (and controversial) topic, so it would be easier to identify the lower level objects first, and defer polymorphism (which you may or may not need) until later in the design process.",1.0
g0eqdzx,i3usom,"i think you figured it already - ""in"" takes a list of values (comma-separated one), not an array. So you can do ""in (?,?,?)"" with a known number of elements.

To get unknown number of elements you either will need to create that dataset in the db somehow (do inserts into a temp table, for example)

Another way is to use dynamic sql (basically, create a string and execute)",2.0
g0gh3bb,i3usom,Thanks for the help! It's just strange because I was using mySQL before I switched over to SQLite and in mySQL the action of passing in an array for the IN() worked. Didn't know they would be that different.,1.0
g0e9ucg,i3u11b,"Use a substr function to extract the relevant parts, and use functions like date\_add, date\_diff, parse\_duration to get time differences.

[https://prestodb.io/docs/current/functions/datetime.html](https://prestodb.io/docs/current/functions/datetime.html)",1.0
g0eru5o,i3u11b,"Unfortunately, there's no straightforward way to do that at this time. You'll have to parse it by hand (you can use the [`split`](https://prestosql.io/docs/current/functions/string.html#split) function for that):

    presto&gt; WITH t(x) AS (VALUES '45:22:59')  
    SELECT  
       cast(split(x, ':')[1] AS bigint) hour,  
       cast(split(x, ':')[2] AS bigint) minute,  
       cast(split(x, ':')[3] AS bigint) second  
    FROM t  
    
     hour | minute | second
    ------+--------+--------
       45 |     22 |     59
    (1 row)

In theory, it should be possible to convert that string directly into a day-time interval, but that cast is currently missing. I filed an issue to track this improvement: [https://github.com/prestosql/presto/issues/4697](https://github.com/prestosql/presto/issues/4697)",1.0
g0dz0zn,i3sziv,"you don't really give enough information to suggest specific code (what are the descriptions for the MetricTypes? how come your sample results don't show any values?).

but there are a few general tips / rules of thumb. I don't know anything about your skill level, so forgive me if I oversimplify or state the obvious. and keep in mind that everyone has their own ways to do things....

do NOT use cursors! they make things run slow, and they're hard on the processor when dealing with large datasets. I do use loops for smallish datasets, though....

try using temp tables or table variables, in place of your bigger database tables, to restrict the data your working with.  so, if you have to join two big tables, create temp tables for each of the real tables, and insert just the data you need from the big tables (for example, transations from the first of the month forward would go into a temp table, instead of querying against the entire transactional table every time), then do the data crunching using the temp tables. you could then, for example, loop through the accounts, one by one, using temp tables to pull the relevant data for that account, and do your calculations.

make sure you've defined good indexes for what you're doing. I can't stress how much of an impact that can have. if the tables don't have good indexes for your doing, and you can't get approval to add some, then I go back to temp tables or table variables, where you CAN put in the indexes you want.

if you DO have the ability to add indexes and new tables, and you have a lot of joins that need to be done between (relatively) static tables, and if that's where a lot of your processing time is spent, consider building a denormalized (or flattened) database table that permanently retains the joined dataset. then you could just pull THAT data into a temp table and start joining to whatever other (presumably transactional) data you need for your calculations.

I typically build a temp table that, structurally, reflects what I want my final result set to look like, plus any additional columns that I'll use along the way (IDs the base data, columns for the raw metrics behind a calculation, etc). then I'll populate this main temp table with as much of the base data as I can, to begin with (such as a join for your Account and Metric data). I also build any of the intermediary temp tables I need (restricting the data I need from other tables) then I use update statements to join my main temp table to my intermediary temp tables and pull what I need into my main temp table. for example, do an update to populate the product information, then updates for any raw metrics, then an update to perform a calculation using the raw metrics I've collected (pulling the result in the column that I'll actually include in my output). then, once the work is done, I pull the data that I want to see straight out of the main temp table.",5.0
g0dyfxh,i3sziv,"seems relatively straightforward: from account metrics (table1) join to ""report feeding metrics"" (table3) then join to  the metric definition (table2) on either MetricID or ValueMetricID fields.",1.0
g0j1jeb,i3sziv,"Ooooh, that IS a neat problem! Is there just the one level of calculated metrics? As in... are there also 3XXX metrics that are calculated using 2XXX metrics, or are there only ever 2XXX metrics calculated with 1XXX metrics? 2XXX would be simpler, but your mentioning cursors and while loops makes it sound like there might also be 3XXX (...&amp; more?). 

If it's the simpler, I'd get a count of how many different ValueMetricIDs appear for each MetricID in the calculated metrics table and then compare that count to a new count of how many different ValueMetricIDs remain after joining on the list of metrics available for the specific account. If the two counts match, you know that specific MetricID can be calculated for the account.

If it's the more complicated type, think I'd try something using [recursive cte's](https://www.sqlservertutorial.net/sql-server-basics/sql-server-recursive-cte/), since their construction mirrors what seems would be the construction of compound(?) calculated metrics, at least if all 3XXX metrics only depend on 2XXX metrics and not any 1XXX metrics... but I don't even know 3XXX metrics are even a thing, so I'll leave it at at that.",1.0
g3gsjx2,i3r7rc,So the patch to fix Scalar UDF inlining issues didn't fix anything at all for me,1.0
g0drh4v,i3p9fg,"As long as they are all MySQL you’ll be fine.
If it turns out it’s a different engine (say sqlserver / informix) it’s just some syntax that changes for certain aggregates/functions.

Your everyday selects from join where are all the same.

My everyday is sqlserver, but have to work with MySQL sometimes... you wish everything was the same! So you end up googling “t-sql equivalent blah...”",2.0
g0gpglw,i3p9fg,Thanks for the feedback.  Also do you have any thoughts on CAP the main functions?  I'm on a test server and I always have to manually CAP the functions because I was taught it's good practice.  But in a work place doesn't the server automatically CAP and maybe even color codes it for you?,1.0
g0hpp70,i3p9fg,"Depending on your ide you can set it to format the code for you in your preferred style.

I tend to caps the main functions, and anything that is reserved. Always trying to match the case of the objects as well.

#1 pet peeve is reading a query that has no indenting, and all lower case. Makes it hard to follow what it’s trying to do.",1.0
g0hptiv,i3p9fg,"Turns out putting a ‘hash’ before text makes it REALLY BIG!
Not trying to yell in that post sorry!",2.0
g0uc1l9,i3p9fg,"Ah that's good to know.  I've been skipping over indenting.  I will get into the habit of doing it now. 

Any recommendations?  I've done a few google searches and a lot of people says it varies.  But this 1 (most answered 1) seems like what my udemy instructor does sometimes.

 [https://stackoverflow.com/questions/272210/sql-statement-indentation-good-practice](https://stackoverflow.com/questions/272210/sql-statement-indentation-good-practice)",1.0
g0cwo3m,i3ovhd,"I’m trying to understand what you are looking for here. 
It looks like you are looking for the first score for each ID, ranked on test? Is there a particular ranking for test? SAT then ACT then other?",1.0
g0cxjgu,i3ovhd,"Yes, pretty much. Each test will have an associated score. It doesn't really matter what order they are in as long as the same test is in the same field.",1.0
g0cyp8n,i3ovhd,"Lol maybe I misunderstood it then, can’t you just select * from table order by test, Id?",1.0
g0d0p08,i3ovhd,No because the IDs will be on different rows. I need to get each of the IDs that are the same on the same row.,1.0
g0dk7dx,i3ovhd,"There was a very recent post about a similar situation, see if it helps:

https://www.reddit.com/r/SQL/comments/i0ocv9/comment/fzr2bm7?context=1",1.0
g0cm0no,i3nia8,"&gt; an id, a user's name, and their task
&gt;
&gt; How would one store each user's tasks more efficiently?

replace id with a timestamp

replace user's name with integer userid, and keep userids in a user table along with user name",1.0
g0cnems,i3nia8,"&gt;replace user's name with integer userid, and keep userids in a user table along with user name

How does this help the database? Is seems like the same data is present, but there's just a middle man handeling transactions.",1.0
g0cq3ey,i3nia8,"&gt; How does this help the database? Is seems like the same data is present, but there's just a middle man handeling transactions.

please allow me to say that for a beginner, you have grasped something very fundamental about data, and this solid foundation will help you in future

that's #1... you understand the ""middle man"" 

database design is all about pros and cons

suppose you wanted to change a user's name

with a user table, you change it once, and bob's your uncle

without, you'd have to change that user name in every table where it exists, and while yours is a simple app with only one data table, in the real world you will find users in a bazillion tables 

so you're updating all those bazillion tables to change that user's name in each one, and imagine what happens if the database goes down part way through... you're toast

in summary, this little TED talk has been about efficiency

but you have grasped something that is far more valuable",1.0
g0crsh9,i3nia8,"&gt; table that stored an id, a user's name, and their task. 

i just going to assume these are fields and you are creating a record per task

&gt; the order in which the tasks were stored

shouldnt matter (sql engines can change the storage of the data and unless explicitly instructed with a specific ""order by"", can retrieve records in any order)

&gt; it's not well constructed. And trying to select the correct data out of this schema is very convoluted.

i'm not seeing it. why is it not well constructed (given the above assumption)? what's convoluted about retrieving this data?",1.0
g0d1wa0,i3nia8,"&gt;what's convoluted about retrieving this data?

You're right, all you have to do is do a SELECT query and just put a condition for the name, like

SELECT * FROM users WHERE name= 'UserA'

The problem that I really ran into was when the user wanted to delete a task from a specific user and if there was also two tasks with the same name.

So my process kinda went as such:

I ended up retrieving all of a user's tasks

Then I put it into an array and then get the index of the task wanted gone, like 'Walk the dog' 

After that I took the ids of the tasks and compared them to the tasks place on the GUI

Because of my schema a certain task in the database could have an id of 13, but it appears on screen as 1 because it was the users first task entered in

But then I then compared them so that I deleted the correct task that the user clicked on, this was the hardest part because the task ids could differ wildly to their appearance on screen

I hope that made a bit of sense. It was really my first real SQL project, I kinda wanted to know if I was going about all right",1.0
g0d9vdj,i3nia8,"I guess it depends on what you really want to do: if you want task descriptions/names to be unique, add the constraint to the table.

If you are ok with duplicate names, make your dropdown/selection widget list all copies with relevant IDs, and when you make a selection you get an ID back - so you dont have any ambiguity on what to delete (you have an ID now).

&gt; Because of my schema a certain task in the database could have an id of 13, but it appears on screen as 1 because it was the users first task entered in

if you care about the order (i.e. order of the records is part of your dataset - store it in your table (seq_no kind of column)",1.0
g0clm4q,i3nbyr,"You're saying if you use an arbitrary date string, rows of EditID not in inserted get updated?


Side note: 
AND COALESCE (NULLIF(Edits.rtrdate, '')) IS NOT NULL",1.0
g0crh1w,i3nbyr,"Thanks for the reply.

No, I am saying that when I use the arbitrary date I do not get any errors and everything works as I would expect it to. 

What I was attempting to say is that I know the problem comes from the convert for that function,  because when I remove it I no longer get the error. 

With the coalesce, I do not seem to be able to get that to work.  the is null and not equal to  are part of my where clause.  I do not want to call that function and assign the value to rtrdate if rtrdate is currently null or blank.

I think you may have just led me to my solution though.  I changed my where clause to be:
    where ISNULL(i.RtrDate, '' ) &lt;&gt; ''
and that is appearing to work.",1.0
g0cu4ml,i3nbyr,"&gt; Conversion failed when converting the nvarchar value '9/4/2018' to data type int.

you are converting NVARCHAR to int.

And getting an error, because what kind of integer is '9/4/2018'? I dont know and MSSQL server doesnt know either. Now if you are trying to get number of days from 1/1/1900 to the date '9/4/2018' you'll need a DATE: 

     Convert(int, cast( rtrdate as datetime))",1.0
g0cwamv,i3nbyr,"Thanks for the reply.

I do understand that part.  My question was coming from why it was trying to convert those already converted dates.  The ones that are already in a proper date format like '9/4/2018'  had been inserted previously and are not part of the current insert set of data.

I would think that having 
    where (Edits.EditID in (select EditID from INSERTED i))
or 
    inner join inserted i on i.EditID = Edits.EditID 
should stop that function from trying to process the values that were previously inserted into my edits table.",1.0
g0ce70b,i3mqj2,"i've heard about it happening via php, and also via phpmyadmin 

i've never encountered it using HeidiSQL",1.0
g0dwv4v,i3ly6a,SSIS Performance totally depends on the hardware you install it on. Since it is a RAM heavy operation consider increasing the RAM. Regarding SSIS alternatives - have you considered Azure Data Factory? Informatica is great as well.,1.0
g0ev8v2,i3ly6a,"Ive heard good things about Alteryx. 

Im in a similar boat but without money to buy a new tool. Ive been learning python hoping i can simplify the process.",1.0
g0chwyk,i3lhge,Slide into an analyst position first,1.0
g0fvyzj,i3lhge,"I’m in the exact same position as you, have you tried sql murder mystery? Also google sql data base. I stumbled upon a site that had questions along with a database that I had to query to answer the questions.",1.0
g0c8l2i,i3ldbo,"&gt;  could it be the INT

not the INT but definitely the `(1,1)` behind it

which website was this from?",2.0
g0cmu21,i3ldbo,"Yup , I converted it to number and it worked with a couple tweaks, thank you

The website was: [https://www.sqlservertutorial.net/sql-server-basics/delete-duplicates-sql-server/](https://www.sqlservertutorial.net/sql-server-basics/delete-duplicates-sql-server/)",1.0
g0cqu5b,i3ldbo,"see, that was your first mistake

i would've looked at the URL -- http://www.**sqlserver**tutorial.net

you see, SQL Server and Oracle are as different as car boots and car trunks

if you're going to use something off the internet, please makje sure it's for your particular database platform

also, it didn't say `INT(1,1)` -- it said `IDENTITY(1,1)`",1.0
g0cjgoe,i3ldbo,INTEGER does not have scale. Oracle also internally converts to NUMBER anyway.,2.0
g0cmzqw,i3ldbo,"You saved me a lot of research , thank you, im still struggling with understanding the defaults of oracle.",1.0
g0cgkj1,i3gzuy,"Ahh, the ole ""gimmie the data"" analyst.

""You know you can go directly to the warehouse and...""

""Just gimmie the data in Excel.""",47.0
g0d50pj,i3gzuy,"In a different company, far far away

Ahh, the “just tell me what you need” db admin

“You just tell me what data you need and I’ll dump it into a stored procedure for you and...”

“Just give me read only access”",24.0
g0epkz0,i3gzuy,"Asking from ignorance, is the “just give me read only access” guy in the wrong here?

Cuz I’m the “just give me read only access” guy.",3.0
g0eqsx9,i3gzuy,"No, that's the smart guy who ends up using a lot of #tables.

In reality you need your own server with write access to be a proper analyst. Or at a minimum your own database.",3.0
g0fvcma,i3gzuy,"It took me over a year at my analyst job to acquire proper access. But can confirm, use too many temp tables",2.0
g0eqe5g,i3gzuy,"Nah, I’ve only been places where I’m allowed read only access",1.0
g0flchu,i3gzuy,"No sorry, that is the good analyst.",1.0
g0egsti,i3gzuy,Select * from dbo.CleanAndPristineTable is the running joke in my dept. Shared a tweet meme and we all died at first when we saw it.,4.0
g0thtuj,i3gzuy,"I know it's a running joke but building pristine OLAP style environments is a thing. Most servers/environments aren't configured for them, and/or most businesses don't understand their value.

I'm finishing a project that has taken several months where we have designed an entire clean and pristine suite of tables that are designed to be joined together. All of the data they hold could have been calculated from other 'dirty' tables but the complexity of our queries has been so greatly reduced now that peer review is much easier, and it is much easier to fulfill any requests we have which will massively improve our overall efficiency.

In turn this also lets us get really complex and do things like predictive modeling efficiently because we handle all of the complexity, cleansing, and transformations on the stored procedure side. Basically we account for all known edge cases in order to just spit out a very elegant way to access our data.

The really slick part is that the size of our new DB is only 100GB, and only takes about 20 minutes to refresh daily, which solidly puts us at the bottom of the pack when compared with the rest of the company. 

Then we use some pretty clever functions and views that can create 'theoretical' sets of data that come in well over 1B rows of data, which solidly puts us at the very top of the pack when compared to the rest of the company and the datasets they work with, and our queries can ping those large sets to segment the data we're interested, or aggregate it very quickly. 

The final slick piece is that our aggregates or segments all come with an approximate PK to the raw source tables, so if we're missing some strange dimension that we chose to exclude from the database, we can just join over an grab it, or join over to validate the numbers we're seeing on an account level in order to increase the confidence we have in the accuracy of our reporting.

It has been a painful project that got a lot of push back from partner groups who didn't see the value, and for several months now there really hasn't been any demonstrable progress. I mean the project itself has progressed, but we haven't really *used* it yet to begin making improvements, so senior management (VP+) have been sort of gambling by letting me finish it. They hear from other groups it isn't necessary, but they have had enough trust and faith in me, and my direct counterpart on this project to give us the time needed to finish it.

It was only in the last week that we began to show it off to external groups (other data scientists, modelers, etc.) and the feedback has been explosive.",3.0
g0ulzkv,i3gzuy,"Yes, fully understand when it’s set up right from the get-go. Everyone’s dream to have things nice and clean, but often times you’re coming in behind other folks with 10-20 years of messy ETL, data, etc. I find that it’s a constant race to keep up with the system enhancements, adding data elements and things breaking when said enhancements are made. Lose-lose most of the time, and the data is usually last to be thought of. 

The running joke is more along the line of a project manager saying, “Well the data is on the screen, so just go pull it from X table! It’s pretty simple!”

Um, no...not that easy a lot of times. 

My three bad words are easy, simple, and quick.",1.0
g0usqji,i3gzuy,"I wasn't trying to disagree with you, only to add that analytics architecture is an actual field / title / job / role.

It isn't sexy, per se, but once you have a proper OLAP you can really work efficiently and accurately. It is work I am good at and enjoy, and I'm happy to be with a company that let's me work in that capacity.

I build solutions that are easy, simple, and quick.and they work great until devs upstream change something without notification and break my process. 

Call that job security.",1.0
g0dugx5,i3gzuy,"Yep, I’m trying to capture some classic analyst experiences in memes for some fun.

“Gimme the data” would be even more common, unfortunately",3.0
g0e6rh5,i3gzuy,"Or, alternatively, ""Yeah just give me the data from the undoubtedly super clean and already curated and conformed table we were provided the client.""",2.0
g0ecx9g,i3gzuy,Data processing is a fucking bitch -_-,1.0
g0eqccc,i3gzuy,"Ugh, I have a coworker that asks for data all the time. Like bro, it’s in a tableau extract just grab it",2.0
g0i2al5,i3gzuy,"I've politely been nudging this guy on our team to learn basic sql.
The amount of times I become a bottleneck because he can't do a 3 line query with select * and a simple where clause is too damn high...",2.0
g0i4nga,i3gzuy,"He can select fields from one table and maybe a where x = y. I’m hoping that the more we work together the more he picks up bc we screen share about 2 hours a day. 

He’s a damn good analyst though - as far as mindset and vision so that’s nice",1.0
g0dmqsw,i3gzuy,"I'm... a bit confused by the people indicating you can't ""code"" SQL.  That might be true of the abstract ANSI implementation of SQL but most RDBMS have their own version that you can 100% ""code"" in if you want to be really pedantic.  Huge enterprises have historically and continue to have enormous swaths (if not all) of their data infrastructure coded in some variation of SQL, be it PL/SQL, TSQL, or something else.",16.0
g0drafw,i3gzuy,"Right now I'm porting MSSQL to Snowflake and the ""syntactic sugar"" between languages may have different words, but it's surprising how much of it cognitively maps.

Reminds me of that cookbook (PL, T, MySql) collecting dust on my bookshelf.",3.0
g0boo60,i3gzuy,Me IRL when someone new gets hored,28.0
g0bsk0u,i3gzuy,I too like it when my new whores know sql,43.0
g0btecw,i3gzuy,Who wouldn't!,7.0
g0btpd2,i3gzuy,Tease me with some group by aggregates before you drop that prod table bby ;),11.0
g0cihcs,i3gzuy,"Good lord, they let analysts touch prod these days?",4.0
g0cizxl,i3gzuy,We can look but we can’t touch :(,4.0
g0djk52,i3gzuy,Let’s be honest with ourselves. Do you really want the whole farm? The milk tastes just as nice in read only format,4.0
g0em7jt,i3gzuy,This query is tighter than dick skin,1.0
g0c38rl,i3gzuy,"&gt;code SQL

As a developer and DBA, this makes a little vein pop out of my forehead",46.0
g0cxd9x,i3gzuy,Code is nothing more and nothing less than a set of symbols and rules for sending instructions to a computer. This elitism is unwarranted and useless gate-keeping.,51.0
g0e09vc,i3gzuy,Gatekeeping indeed.  Are you coding English at me?,5.0
g0ff5nn,i3gzuy,"If we say that you are a biological robot with a biological cpu in your skull, and you follow a cooking recipe, could we say the cooking recipe is a program being executed by you?

I would say, yes.",3.0
g0d2j7v,i3gzuy,"There's neither elitism nor gate keeping intended. It's simply an important distinction which should be emphasized, especially to HR monkeys writing the position postings. I've spent significant time attempting to master SQL. Downplaying its value would be stupid. I also have a CS degree and do application development. SQL can, and is often, integrated into code but it is not handled in remotely the same fashion.",-10.0
g0e2w9w,i3gzuy,"what is the technical definition for ""coding""?",5.0
g0htu8y,i3gzuy,Writing code?,2.0
g0ees1t,i3gzuy,But people want to emotionally identify as coders even if they just know SQL and damn your for not letting them self identify however they damn well please! ‘Tis the way of this age. Personally I self identify as a software developer because I occasionally make an excel sheet.,-1.0
g0htzo8,i3gzuy,"SQL is a Turing complete language, so you can absolutely call it coding.

I can 'code' in C variations, COBOL, RPG, Fortran, PERL, Java/JS, some Python, and some R.

I am by far best at SQL, and it absolutely is a full language.",2.0
g0ik3od,i3gzuy,You mean you can warp the meaning almost plausibly if you desperately want to stick the label “coding” on it but these kinds mental gymnastics will ultimately only lead to confusion.,0.0
g0iul91,i3gzuy,Huh? I 'code' in SQL as a career. It is the language I have chosen to specialize and focus on. I have been coding since about 95. How about you?,2.0
g333h50,i3gzuy,"In 2020, I’m not sure SQL alone is enough for a career in databases. Are you sure you don’t accompany with a group of other skills?",1.0
g334fdz,i3gzuy,You're right. I have certificates in many other languages.,1.0
g0j4upv,i3gzuy,I barely know what I'm talking about tbh,0.0
g0eitk0,i3gzuy,"Well, it rustled some jimmies, that's evident.",-1.0
g0c779n,i3gzuy,"Yeah, you write SQL or use SQL or know SQL but you don’t “code” sql.",31.0
g0dyde4,i3gzuy,maybe they meant morse code,2.0
g0ff7sh,i3gzuy,Why is that.,2.0
g0h1108,i3gzuy,"Probably due to the fact that SQL is declarative in nature and differs from most other languages in that regard. It’s more of a tradition / grammar at this point but if I heard you say you code SQL it just doesn’t match how the community at large speaks about it. I guess it’s kind of like how in English we say I drive my car, my bike or my motorcycle but I don’t drive my skateboard or roller blades.",1.0
g0c7jyt,i3gzuy,lol.... whenever someone ask me to send them my SQL code.  I always repeat do you mean my SQL script lol,14.0
g0e6iqw,i3gzuy,"As a data analyst I’m always keen to learn the technical nuances from DBAs. And so it’s good to start a discussion about SQL not technically being code.

However in the world of data analytics/business analytics, “code” is commonly used. Especially by people in the business. I’ve also heard and used script, logic, and query. They are kind of used interchangeably and it’s in this loose sense that I’m using the word. 
Should I be? technically no. I do understand how it can be important in many situations to specify the difference clearly (eg on job applications as mentioned earlier).

But for a meme with a separate point, I don’t think the semantics matter too much. Thoughts?",4.0
g0csrjz,i3gzuy,"I was going to say this but didn't want to be called an asshole. I regard SQL as scripting. I know SQL and PS but don't call either ""coding"".",5.0
g0cijr4,i3gzuy,could be PSQL you never know!,1.0
g0c07pw,i3gzuy,I’m an EDI analyst but my skills are definitely growing from knowing next to nothing.,4.0
g0d4g1m,i3gzuy,coding sql sounds a bit weird. I guess technically its not wrong. we just dont refer it to coding.,2.0
g0du9yv,i3gzuy,How long did it take you?,1.0
g0e8nb9,i3gzuy,"To make the meme, or to learn SQL?",2.0
g0g1fgn,i3gzuy,Few hours for sql,0.0
g0e8qin,i3gzuy,What does EDI stand for?,1.0
g0cpl6b,i3gzuy,SQL isn't code,-7.0
g0cujm2,i3gzuy,How come?,5.0
g0cumxl,i3gzuy,It's a language,-2.0
g0d56hh,i3gzuy,"Are you aware than a language is in fact a code, used for communication?",9.0
g0d51ju,i3gzuy,"It is but it isn't. 

SQL is a Query Language which isn't the same thing as a full ""coding"" language like say Java or .NET

For it to be ""Coding"" in my mind you have to be able to build an app with it. You can't really build Applications with SQL. You USE SQL when you build applications, but it's a small piece of it.

Even SQL Report development relies on other forms of code besides SQL, we write the ""Expressions"" for reports in VB, but the queries in SQL.",-1.0
g0d8gss,i3gzuy,"SQL is technically a turing complete language, as long you're using one of the versions that enables recursive CTE's. So you can write applications in SQL, but you definitely wouldn't want to.",9.0
g0eo371,i3gzuy,Without data your app is shit,2.0
g0fjrtx,i3gzuy,Good cuz I’m a data guy not an app guy.,2.0
g0brdyo,i3gzuy,[deleted],-100.0
g0c0s28,i3gzuy,Knowing python but not SQL wouldn't really be helpful in most shops.,23.0
g0bvd43,i3gzuy,Maybe this time you shouldnt have been a prick.,14.0
g0bszdf,i3gzuy,"I mean is this an actual post? Lol

SQL is a foundational skill. If you’re working with data and can’t do simple queries... your life is going to be really damn hard. I say this as someone who is a developer / analyst. 


I will say I agree that soft skills will get you really far. But that doesn’t mean you’d be a good analyst / data scientist / etc...",30.0
g0ee0ss,i3gzuy,"Without at least knowing syntactically how SQL works or interacts with the database, it would be nearly impossible to do my job",2.0
g0btqc8,i3gzuy,[deleted],-53.0
g0bufge,i3gzuy,You're a weird guy,40.0
g0c0696,i3gzuy,[removed],-41.0
g0c7285,i3gzuy,You're a weird gal,29.0
g0c26pe,i3gzuy,such a technical point... just out of curiosity - why do you think it matters or what does it change in the conversation?,12.0
g0c5ueh,i3gzuy,[deleted],0.0
g0c6jto,i3gzuy,... and that's relevant how?,5.0
g0co1uf,i3gzuy,You really need to calm down.,5.0
g0bxigm,i3gzuy,"If you think a 12-year old learned SQL well, then you don't know SQL. The basics are simple but there is much more to it than that.",16.0
g0ce0aj,i3gzuy,"I’m definitely getting off track, but I feel like a 12 year old could become more proficient at SQL than most professional users. Especially users on the “analyst” side.

In my opinion, the beauty of SQL is that it’s so simple

Edit: keyword is could",0.0
g0cjye3,i3gzuy,You must know smarter twelve-year-olds than I do!,1.0
g0bul6q,i3gzuy,"No. You literally cannot. Sometimes there might be an ORM, sure, but most ORMs are just abstracted SQL anyway. Good luck using an ORM without any SQL knowledge. 


Again, SQL is a foundation skill. Not saying you only learn SQL and nothing else, but it is required.


If it is the easiest to obtain, why would you not obtain the skill?


Something tells me you might on the younger side or not actually within industry yet. I’d urge you to look at job postings for data analyst, BI analyst, data scientist, python developer... I would guess 80% mention SQL and the 20% that don’t just assume it lol.

Edit: I just realized this is the SQL sub. So you’re just obviously a troll. Nice.",19.0
g0by5m3,i3gzuy,"Im new to sql. Could you tell me what ORM is?  And also, that guy must be trolling.",4.0
g0dcx20,i3gzuy,"Object-relational mapping, it's how applications play with data. You can kinda think of it as a type of SQL for specific applications.",3.0
g0dk895,i3gzuy,Thank you for clearing it up for me.,1.0
g0c08lq,i3gzuy,"The most ironic part is he's saying this like python isn't easy. Or python's popularity isn't because it's super easy and abstracts away a lot of the details that make other programming languages ""hard."" (What are types? How do for loops work?). 

Not to mention that most python ""programming"" is just importing a bunch of packages and calling them to fit your needs. Like congrats, you can call methods in beautiful soup, or requests, or pandas.",5.0
g0crzji,i3gzuy,Who is this panda 0eople keep speaking of,1.0
g0dl3n8,i3gzuy,"not gonna lie, you had me in the first half",1.0
g0c2ocl,i3gzuy,You must be fun at parties,8.0
g0bsi5r,i3gzuy,You seem really angry. Are you okay?,23.0
g0crt2z,i3gzuy,"No he's not, must have run his UPDATE without a WHERE constraint.",7.0
g0d7ov0,i3gzuy,Used IN when they meant to use NOT IN.,3.0
g0e1t1y,i3gzuy,He truncated something perhaps :P,3.0
g0e1rr3,i3gzuy,Uh... literally SQL is the most important skill for anyone working with DBs.,3.0
g0e7iru,i3gzuy,[deleted],-5.0
g0ek1dj,i3gzuy,[deleted],1.0
g0emz1o,i3gzuy,[deleted],1.0
g0en4xl,i3gzuy,[deleted],1.0
g0eny8z,i3gzuy,[deleted],1.0
g0cs2e5,i3gzuy,I work for the Navy and I only have used SQL for analytics,3.0
g0d9djj,i3gzuy,[deleted],-9.0
g0d9ovo,i3gzuy,"Does that mean my job isn't really a job? Do I tell my employer? Am I supposed to return my paycheck? 
And here I thought I got a good job from the skills I learned in the Navy. Man, thanks for enlightening me.",3.0
g0db7oh,i3gzuy,[deleted],-7.0
g0dc5u2,i3gzuy,"I took a vacation to San Francisco once on that money. It was cool. Saw a music festival. Took a picture of myself in front of the Golden Gate Bridge. Ate good food. 

I'd say its going pretty well, especially since I'm working in the civilian market now. Yeup. 

I mean, I. Looking at going to a new work place since I want more experience and do more. But that's more of a personal problem. 

So yah, things are going really well",4.0
g0ddph7,i3gzuy,[deleted],-1.0
g0dg155,i3gzuy,OK? None of that made sense? Is it me? Am I the one stroking out here?,3.0
g0dkouq,i3gzuy,[deleted],-1.0
g0dm1yn,i3gzuy,"I'm actually working on that now. I'm trying to double major in history and computer science and boy is it hard. But I feel like it'll be worth it.  
While I've been pretty success without a degree, I feel like it'll elevate me career wise. 

And I'm in the reserves now. It allows me to still serve while focusing on my career and education so I'm very fortunate for the opportunity and thankful for everyone I serve with. 

And please put some respect for our LSs. They have secret ways of solving every problem known to man. 

But I'm an IS, I like it. I got to do some pretty cool things and I hope to continue in that career as well. Who knows. Maybe once I get the degrees I'll go officer. That'd be awesome.",3.0
_,i3gzuy,,
g0e4k0h,i3gzuy,"I believe you’re in the wrong subreddit my friend.
Your experience is probably very specific to your company.

My point is that, however strong your soft skills are, starting out as an analyst you need hard skills. Whether it’s excel, SQL, Python. whatever is relevant to your specific job.",3.0
g0e7alo,i3gzuy,"I've been working with sql and data exclusively for the last ten years, it's a skill very much in demand. My work helps keep organisations such as health providers and banks keep their data organised, up to date, correct and accessible. SQL may well not be as new, cool or trendy as python but it's definitely used in more places. The world doesn't revolve around ""apps"", you know.",3.0
g0eqlfn,i3gzuy,I've been programming for 25 years and it is my ability to write a correct query that is more often than not my most valued asset.,4.0
g0tz2i2,i3gzuy,"100% agreed. I'm not saying that SQL developers should all have a past life of working on other languages, but it sure helps. In other languages you are forced to check your work to see that an 'app' is behaving correctly. In SQL you don't ""have to"" do that, because a query will run and spit out results.

So many companies rely on metrics that aren't really correct.",2.0
g0epo5c,i3gzuy,Without data your app is useless. The purpose of an app is an interface to the data. It is a pretty front end. Your sole purpose is to collect data. What we do with it is what keeps a business afloat. Writing a CORRECT query is what we SQLers do and is non trivial. There aren't any libraries for that.,2.0
g0b0oel,i3f4v4,"Hello u/ShaLouVic - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g0n4jpt,i3f4v4,"You can learn it from DataCamp, Strata Scratch, Leetcode or W3Schools.",1.0
g0ay97g,i3d84a,"What data _type_ is that column? If it's a `DATE` (or `TIMESTAMP`) column, `to_char()` is the correct thing to use. 

But it sounds as if the designers of that database didn't know about best practices around data modelling and decided to store date values in a VARCHAR column which is a really horrible idea. But anyway, you are left with that mess now. 

To deal with it, you need to first convert the varchar to a `date` (which it should have been from the start), then you can apply `to_char()` to format the output of the date.

To convert the varchar to a proper date you can use `to_date(the_column, 'yyyymmdd')`. That date value can then be formatted in any way you want: 

```sql
to_char(to_date(the_column, 'yyyymmdd'), 'mm/yyyy')
```",2.0
g0aqspw,i3d84a,"Working with dates is amongst one of the harder things one has to deal with in the field of programming. You’re not just dealing with one date; 


https://youtu.be/-5wpm-gesOY


What you are looking for anyhow is the TO_DATE functionality which takes a string as date and a format.

Give these two a read:

http://infolab.stanford.edu/~ullman/fcdb/oracle/or-time.html


https://www.oracletutorial.com/oracle-basics/oracle-date/",1.0
g0auqzf,i3d84a,I'm going to try that. Thank you,2.0
g0brflw,i3d84a,"Yeah I finally figured out. I had to do it like
Select to_char(to_date(x, 'YYYYMM'), 'MM/YYYY') from *database* where x &gt; 0;

Note that it's my second day of the job. I'm Germany we have apprenticeships that are called Ausbildung. In those you get teached for 3 years by a special teacher that helps you all the time.
Well, seems like he doesn't want to because in 2 days I haven't seem him once.

Crazy part is that he is obligated to control and help me all the time. So basically I'm learning by myself which is kinda hard. I struggle a lot.",1.0
g0c219z,i3d84a,es scheint als wärst du heute bereits richtig selbst ausgebildet geworden,1.0
g0ayh76,i3d02s,"at first it might be better to just watch youtube videos.. traversy media, web dev simplified, dev ed, etc.. these videos are very helpful and free so you might wanna give it a try",3.0
g0bmab4,i3d02s,Have you done the sql murder mystery? It will teach you the basics. I am learning myself and I feel miles better after figuring it out.,3.0
g0bq4ze,i3d02s,Yes this. I’m working on the SQL murder mystery and loving it.,1.0
g0bqziy,i3d02s,Have you found any other similar tools that one may find helpful?,1.0
g0efb2b,i3d02s,No man I’m still kind of a beginner. The best thing that’s helped me learn sql is doing a project on the side when you’re going through a Udemy course or wherever you’re learning from.,1.0
g0mpyas,i3d02s,"I have found this

https://sqlzoo.net

or google sqlzoo and you will find databases that have questions you will answer with query  at the top easy, medium, hard (mimics the real world I think) 

It has given me confidence in my learning. Also some of the other databases (other than the top, two-three databasis are the only ones I found to be working) 


I hope this helps you like it has done me.",1.0
g0avrg3,i3d02s,"any coding classes in your local area? But I think most people learn how to code on the internet, if not from school",2.0
g0avzby,i3d02s,"I'm learning myself and weighed up the whole ""pay someone  or learn online"".

Being honest with myself I realised by hoping to pay someone to teach me it felt like I was almost subcontracting part of my learning process and I was just hoping I could pay someone to make it easier for me......

I chose to try the cheaper option first and am glad I did.

Udemy has been great but I've tried quite a few until one worked for me.

Honestly, it is best to knuckle down and start learning from the cheap and massive quantity of courses online before trying to subcontract your learning. 

And when you do eventually get stuck you can try learning the same subject from a different online teacher or ask about it in the billions of forums available.
There is always a kind soul willing to guide someone genuinely trying.",2.0
g0ay70a,i3d02s,"I'll tutor you,  but you can do it on your own too.

https://www.reddit.com/r/SQL/comments/i05qpr/unemployed_looking_to_take_up_sql_recommendations/fzn9s48/",1.0
g0b831q,i3d02s,SQL ZOO is a good site for the basics of SQL,1.0
g0e6eef,i3d02s,"sign up for ""automate the boring stuff"" on udemy                    
https://www.reddit.com/r/learnprogramming/comments/i1veyy/automate_the_boring_stuff_with_python_online/

its free for the next few hours                       
the author makes it free the first few days of every month               

its a great python course",1.0
g0atq7g,i3d02s,So many websites and resources on the internet for that. Just ask Google,1.0
g0ao1zw,i3d02s,"Hello u/voicespiritstances - thank you for posting to r/SQL! Please do not forget to flair your post with the DBMS (database management system) / SQL variant that you are using. Providing this information will make it much easier for the community to assist you.
 
If you do not know how to flair your post, just reply to this comment with one of the following and we will automatically flair the post for you: MySQL, Oracle, MS SQL, PostgreSQL, SQLite, DB2, MariaDB (this is not case sensitive)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SQL) if you have any questions or concerns.*",1.0
g0b7qre,i3d02s,Sentences end with a single period (.).,0.0
g09qvka,i37a61,"For database mirroring, I'd look into sql server's replication features, which would allow you to keep 2 databases in sync. 

As for web, you could write your own framework if you're not in a hurry, many people start with php although javascript seems to be becoming the norm. You can create a web front end pretty quick with scaffolding on a full framework asp.net project with razor pages, or you could do an angular or react front end and tie it all up with javascript. The tech you use is up to you but if you just need a simple, quick, easy way to display data from a class then I'd personally go the razor route since scaffolding is super simple.",2.0
g09qwng,i37a61,"**I found links in your comment that were not hyperlinked:**

* [asp.net](https://asp.net)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g09qvka&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",0.0
g5pspvr,i37a61,"I'd suggest a different approach.  You can create stored procedures to execute the CRUD operations on your database and trigger them with APIs.  This means the website becomes a wrapper that displays the results from the API and submits request to update the database.  There is an open source framework/library called JsonAutoService that makes building REST APIs that execute CRUD SQL stored produres very easy.  check out these videos

Here's how you implement the framework/library

[https://youtu.be/FTn0IcZi\_70](https://youtu.be/FTn0IcZi_70)

and here's some background on the methodologies that JsonAutoService uses

[https://youtu.be/Px-\_i61fOnE](https://youtu.be/Px-_i61fOnE)",1.0
g681wgd,i37a61,You can easily create a frontend wrapping your tables using [Magic](https://polterguy.github.io/),1.0
g09nkuk,i373lx,"Personally I'd put it all in one table with a field for month and a field for year. That way you could pull data a lot easier.  Indexes are your friend here. Anything you could put in relational tables may help. For example airline could be mapped using an id number, etc. This is assuming decent resources for the SQL server. IF this is running a raspberry pi 3 or something separate tables may be a necessity. Personally we have tables with 100+million records with no issues.

As for import, workbench might be able to handle it but it's tough. If it's just a map and import from file. Navicat is pretty good and ALOT fast than workbench. I have an old copy before they started the monthly subscription bs. But for $30 for the month might be worth it. 

Alternatively, a simple python or php script might be better and gives you flexibility.",2.0
g09o9a0,i373lx,"52 x 300k records @ \~150MB is only around 15.6m records @ 7.8GB. MySQL can handle **way** more than this in one DB so long as you have the hardware so there's no reason to split your databases into 3 unless you're actually storing different data in each DB. Just remember to use indexes.

CSV is a good starting file format to import, since there is little pre-processing to be done. The best way is just to load the data directly, as you suggested. It'll take as long as it takes.",1.0
g0abwnw,i373lx,"old school load of ""Big Data""

1) remove indexes
2) load a ton of data
3) rebuild indexes.",1.0
g08uwkn,i32t9b,"I believe queries are more legible if you use explicit Joins, and this is my personal reason why they are better",4.0
g08vc1d,i32t9b,I never thought about it that way - but that is a very valid point!,1.0
g08s6q5,i32t9b,"The only reason I can see for arguing to keep ""oracle style"" joins is if you have a whole bunch of existing code that uses that style, and you don't want to change the old code.

The ANSI style is much better because it allows more options (e.g. FULL OUTER JOIN) and because it separates the join clauses from the filtering clauses that properly belong in the WHERE (like where my\_date &gt; date '2020-01-01')

I'm wondering what the defenders of the ""oracle-style"" join have to say.",3.0
g08su7k,i32t9b,"In our office, there is no ""reason"" when asked why (preferring Oracle-style), other than ""I went to an Oracle training and that's how they teach it"". This person was responsible for training 1/2 of our office, so that half only knows that way unless they've self-taught ANSI",2.0
g08q5cb,i32t9b,"explicit JOINs (ANSI joins) have been standard SQL since 1992

old style comma joins (not just in Oracle) are much harder to understand when there are many tables, and they are ~notoriously~ difficult to write for outer joins",6.0
g08tcq1,i32t9b,Thank you!,1.0
g092azn,i32t9b,"Strictly speaking the implicit joins in the WHERE clause are ""ANSI joins"" as well. Before the introduction of the explicit JOIN operator (in SQL:1992) this was the only way to specify a join. 

I don't see any reason to stick to the old (error prone) implicit joins. For outer joins, even Oracle recommends to use the explicit LEFT JOIN operator.",2.0
g08v8d4,i32t9b,If you work with a relational database it is quite natural to write the relations in the SQL query with the most intuitive and clear way and that is ANSI.,1.0
g08nonn,i318v4,"You don't need a drug test. This seems like normal behaviour to me. Example:

Set up tables

    create table people (id number, name varchar2 (30)) ;
    insert into people (id, name) values (1, 'SMITH') ;
    insert into people (id, name) values (2, 'JONES') ;
    insert into people (id, name) values (3, 'WILLIAMS') ;
    create table phone_number (id number, person_id number, phone_number number) ;
    insert into phone_number (id, person_id, phone_number) values (1, 1, 8003331212) ;
    insert into phone_number (id, person_id, phone_number) values (2, 1, 2083455454) ;
    insert into phone_number (id, person_id, phone_number) values (3, 2, 3654562121) ;
    insert into phone_number (id, person_id, phone_number) values (4, 2, 4147862327) ;
    insert into phone_number (id, person_id, phone_number) values (5, 2, 4582213789) ;
    commit ;
    alter table people add (primary key (id)) ;
    alter table phone_number add (primary key (id)) ;
    alter table phone_number add (foreign key (person_id) references people (id)) ;

The People table contains 3 rows. The PhoneNumber table contains 5 rows.

&amp;#x200B;

If you do a COUNT() on PersonID in the PhoneNumber table, it returns only 1 for each row (copying your query)

    SQL&gt; SELECT
      2    ID
      3    ,phone_number
      4    ,person_id
      5    ,COUNT(person_id) AS ""num""
      6   FROM
      7    phone_number
      8   GROUP BY
      9    ID, phone_number, person_id ;
    
            ID PHONE_NUMBER  PERSON_ID        num
    ---------- ------------ ---------- ----------
             1   8003331212          1          1
             2   2083455454          1          1
             3   3654562121          2          1
             4   4147862327          2          1
             5   4582213789          2          1

The query below returns 6 rows, more than rowcount of People, and also more than rowcount of Phone\_Number

    SQL&gt; SELECT
      2    p.ID
      3    ,p.name
      4    ,n.phone_number
      5   FROM
      6     people p
      7    LEFT JOIN
      8     phone_number n ON p.ID = n.person_id
      9   ORDER BY
     10    p.name ;
    
            ID NAME                           PHONE_NUMBER
    ---------- ------------------------------ ------------
             2 JONES                            4147862327
             2 JONES                            3654562121
             2 JONES                            4582213789
             1 SMITH                            8003331212
             1 SMITH                            2083455454
             3 WILLIAMS
             
    6 lignes sélectionnées.",9.0
g08q9xk,i318v4,great example. have my upvote.,2.0
g08o2sx,i318v4,"&gt; Foreign keys are unique, I thought, right? 

 wrong

&gt; If you do a COUNT() on PersonID in the PhoneNumber table, it returns only 1 for each row, indicating that there are no duplicates.

not the way you have done it, it does not.

&gt; do a COUNT() on PersonID 
&gt; .... ,COUNT(PersonID) AS 'Num'

as a side note (not that it matters here) count(personID) has rather little to do what count function counts: count(&lt;expression&gt;) returns how many non-null results of the &lt;expression&gt; you get per your group bucket. ""PersonID"" is used as an expression, not to indicate a particular column.

to find out if there are non-unique values of ""PersonID"" you can do this:

          select personID, count(*)
          from phoneNumber
          group by personID
          having count(*)&gt;1",2.0
g08qz0b,i318v4,"ah, yeah, that's right. FKs just designate a relationship between tables then--referential integrity?

&gt;count(&lt;expression&gt;) returns how many non-null results of the &lt;expression&gt; you get per your group bucket

""group bucket"" please explain. You can put anything in the &lt;expression&gt; math or logical and it would filter on the result of the expression?",1.0
g08zoob,i318v4,"&gt; FKs just designate a relationship between tables then

yes

&gt; ""group bucket"" please explain.

combinations of values of your ""group by"" list separate your base record set into sub-sets (""buckets"" is not an official name) and aggregate operations work on these subsets.

&gt; You can put anything in the &lt;expression&gt; math or logical and it would filter on the result of the expression?

it should be an expression returning a valid DB datatype, some implementations allow for booleans and these might allow logical expressions. There's no additional filtering based on the result of the expression though - count counts non-null values and that's it.",1.0
g09o2x0,i318v4,"A left outer join simply means that if there is no match on the right side, the left side will still be returned (with all nulls from the right side).

If one record on the left matches two records on the right, you still get two rows returned for that one left record.

100 rows, 106 results, I'd expect 6 duplicate join keys (or more) on the right side.  I'd also expect to see at least 11 rows with no match on the right side (95 rows with 6 duplicated join keys, 89 max unique matches).",2.0
g08dzl9,i318v4,"Can a phone number be shared by more than one person?

Try counting PersonID and grouping by phone number and see if anything returns more than 1",1.0
g08h7y7,i318v4,"Some people have multiple phone numbers, others have none. The ID and number in group by in the second query prevents grouping by person.",1.0
g08kdl8,i318v4,"For someone to have multiple phone numbers, that would require a new record with a duplicate PersonID and it would have showed up in the COUNT(PersonID) query, right?


ID | Number | PersonID 
--- | --- | ---
10 | 4808675309 | 14
11 | 1234567890 | 14",1.0
g0b857j,i318v4,not if you don't group by personId and by that only. You group by ID and number in that table so each group has only one row.,1.0
g0b890j,i318v4,"SELECT personId, count(number) from phonenumber group by personId having count(number) &gt; 1;

(sql dialect may vary on having clause)",1.0
g08c8sr,i30ejg,"Can I just joke that panda (animals) have a hard enough life with their dwindling habitat and being forced to procreate in zoos, that they should be left alone to slowly, and adorably graze on bamboo instead of having to work on writing SQL code? Insert gif of panda sneezing.",2.0
g08kt4g,i30ejg,Like [pandasql?](https://pypi.org/project/pandasql/) That’s been pretty useful for me.,1.0
g096imz,i30147,"That is pretty much all a data dictionary is, yes. Often there is other ""meta-data"" such as column data types, ""NULLability"", constraints, etc.

There are a number of [data modeling tools](https://en.wikipedia.org/wiki/Comparison_of_data_modeling_tools) that will do much of the work for you. Of course, all have something of a learning curve.",2.0
g0a9g4t,i30147,"Some companies want to have a single definition of something. For example, credit could mean cash or units (video game currency). The data dictionary would tell you what the usage and meaning of the data stored there.",2.0
g07yzez,i2yvem,"Currently working as an analyst, and I work with SQL all day. My advice is to go through the book ""Querying Data With Transact-SQL"" by Itzik Ben-Gan, in full. If taught me things suchs as, information on database objects, the order of query execution, and understanding set operations. As well as how NULL values can effect your queries. Which you really want to have a solid grasp on. Everything I learned in that book I still use now. Annotate the book and you'll have a great reference on your bookshelf. He has other books not geared towards the exam. But this is the one I've read and provided me with usefull information throughout the whole thing. I tried a few other things such as SQL ZOO. But this book helped me understand how things worked. The implementation of SQL came easier after. Hope that helps!",28.0
g08num1,i2yvem,"His book 'T-SQL Fundamentals' is very good as well, especially if you anticipate using SQL Server. The exercises in the book are great but I suggest pairing it with problems from StrataScratch, HackerRank and elsewhere to really solidify each concept.",7.0
g08q2b1,i2yvem,"Thanks for the response, which exam are you referring too?",4.0
g096mmk,i2yvem,"There is a Microsoft certification exam, don't recall the number off hand but its about querying SQL.  It goes into some advanced stuff like parsing XML.

Honestly I wish I had read that book earlier in my career.  Power through the book.  If a concept don't make sense today, in 6 months you'll kinda remember there was something like that in the book and can start researching it - vs not even knowing something exists.",4.0
g09qmqa,i2yvem,"&gt;vs not even knowing something exists.

THIS, right here, is a massive power multiplier in nearly any situation.  I personally spent years manually pivoting data with SUMIFS and the like because I was too busy to figure out pivot tables (ha!), and then spent far too long wrangling massive blobs of data in Excel for pivoting when a direct-SQL option and various data viz suites were just a (relatively mild) learning curve away.  Moral of the story - don't be me!",3.0
g0a3xa4,i2yvem,Is this geared to beginners or just a very complete book in terms of functionality?,1.0
g09iea9,i2yvem,"My background is a bit different than other analysts, as I went to school and received a BA in Music. I started off post grad teaching lessons full time and started a job at a small fin tech startup learning about fraud prevention.

I took a job in NYC as a glorified customer support agent ($42k salary) but doing fraud related tickets (chargebacks, AML, ATO, etc). I realized how important SQL was to gathering user info and took it upon myself to start learning.

I went through the whole TreeHouse SQL beginner - advanced courses. I started to reverse engineer coworkers github commits, ask questions, seek unofficial mentors in data scientists and engineers. I worked my way up from the customer facing role to full on data analyst ($63k and through raises ended up at 73k). 

As of today, I just received an offer for $150k base salary ($190k TC) as a Sr. Fraud Data Analyst. I love writing SQL and solving problems (big video game/board game guy) and if you enjoy the reward and satisfaction of finding solutions to complex problems then I highly recommend learning SQL.",12.0
g09qel0,i2yvem,"That's 190 in NYC, right? Was it heavily work from home before the recent unpleasantness, or were you in the office most days? Also where in NYC? Just curious. I'm batting a bit lower than you but in a much cheaper city.",1.0
g09qj7k,i2yvem,Naw it's a fully remote position based in Chicago so I get to avoid that nasty 5% city tax. I still live in NYC though currently - but plans to purchase a home once lease is up.,1.0
g0a0gkw,i2yvem,"Thats awesome. I am a realtor in Massachusetts  but i can connect you with a dedicated trustworthy realtor in NY. Feel free to ask me any questions as well.
Jessica Grace Owuriedu
Realtor Exp Realty
TeamGracesales.com",2.0
g0a7nn5,i2yvem,do you have any realtors in CT? I was born and raised on coastal CT and looking to stay close to family.,1.0
g0a97be,i2yvem,"We sure do. Simply submit the quick form  at TeamGracesales.com  and i will see to it that you get a diligent agent to get your questions  answered whiles you get your ducks in a row.

Jessica Grace Owuriedu
Realtor - Exp Realty
TeamGracesales.com",2.0
g09ultj,i2yvem,"Seems I might need to up my game a bit then. I work for a company out of SF but they pay me based on the city I live in which is Phoenix. Job was 50/50 remote before the unpleasantness began. 

I'm not complaining about my pay, and my benefits are outstanding, but there's a bit more of a gap in our salaries than I'd like.",1.0
g0a2kmg,i2yvem,"I am currently doing the TreeHouse SQL basic courses and have found SQL to be pretty interesting so far. I work in the nonprofit sector and have a masters degree in Nonprofit Administration but I find myself wanting to do more in the realm of data.

My current role involves data but only at a very basic level (data collection, data entry into databases and keeping up with report standards and logic rules in order to accurately create and submit monthly reports). Where I work does not have any real kind of analyst position. Any suggestions on breaking into working with SQL to start gaining experience and developing my skills further in the area of data anlaytics?",1.0
g0au2rn,i2yvem,This is awesome! I started as just a low level analyst on a reporting team for a large AML/BSA group just supporting reports that were already created. I knew nothing about SQL but now have a good basic understanding of it and how it works. For me the tough piece is understanding the underlying data and where everything comes from via source systems. But to add to the OP’s question...W3schools has been a great starting point to reference and get familiar with querying a database.,1.0
g08m18c,i2yvem,"https://www.w3schools.com/sql/

This is where I started. 

Best of luck!",6.0
g08b9dj,i2yvem,"My favorite resource, if starting from scratch, is the SQL tutorial by Mode Analytics. It's free and goes through relevant topics in a concise manner.

I would then try to practice as many SQL problems as possible on platforms where you can execute SQL on datasets. There are a few resources out there that have a free tier like LeetCode, StrataScratch, HackerRank. I think these are the most complete and have the best explanations, questions, and datasets. They have all a free tier and a premium tier if you want to try more questions.",5.0
g08iyc8,i2yvem,"You can also have a look at Udemy courses and when you feel you are up to par, I would recommend getting your Microsoft certification",3.0
g08nynt,i2yvem,"I have worked with SQL for years. The best way to learn it is to do it.  I have a course which I will provide you with a coupon code if interested: [https://store.decodeanalytics.net/learn-sql?coupon=COLLEGEGRAD\_CAREERCHANGE](https://store.decodeanalytics.net/learn-sql?coupon=COLLEGEGRAD_CAREERCHANGE)

But if you want to go the free route I do have some playlists on my YouTube channel to get you started.  You will be able to install a database, load some sample data, and go through some basic fundamentals.  From there you will be able to find other channels going through various topics from intermediate to advanced on YouTube.  The important thing is for you to practice writing the code on your own:  [https://www.youtube.com/channel/UCKURNKWVYXO1Yoev1Htrznw/playlists?disable\_polymer=1](https://www.youtube.com/channel/UCKURNKWVYXO1Yoev1Htrznw/playlists?disable_polymer=1)

I also have a free code reference pdf for beginners :   [https://pages.decodeanalytics.net/](https://pages.decodeanalytics.net/) 

Good luck!",3.0
g0abp9a,i2yvem,Hello could you provide me with a coupon code for the course?,1.0
g0bl9jl,i2yvem,"&gt;[https://store.decodeanalytics.net/learn-sql?coupon=COLLEGEGRAD\_CAREERCHANGE](https://store.decodeanalytics.net/learn-sql?coupon=COLLEGEGRAD_CAREERCHANGE)

Here is the code. The link will take you directly to the course with the discount applied already. This code is good for 100 users.",1.0
g08dv4i,i2yvem,"I recommend Youtube channels.

You should search ""learn sql"". It will help a lot.

If you get stuck ( which you will ) , you can ask for help here.",3.0
g08spt9,i2yvem,"Find freecodecamp on YouTube, or w3schools. Good Luck :)",1.0
g08xpw4,i2yvem,"We have a tool that might interest you.

[https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/](https://www.dolthub.com/blog/2020-06-01-learn-sql-dolt/)",1.0
g096oq0,i2yvem,"Hey OP I’m in a somewhat similar position, looking to add SQL to my skill set. I started looking at the sources in the “wiki” section of this sub, seems like there are a ton of resources there. 

But if you find anything that’s exceptionally good or easy to get started with, let me know. Thanks",1.0
g09ax74,i2yvem,"Thanks for the reply, I will check out the wiki section",1.0
g09qatg,i2yvem,"This is now a link to a link.

https://www.reddit.com/r/SQL/comments/i05qpr/unemployed_looking_to_take_up_sql_recommendations/fzn9s48/",1.0
g0a84j9,i2yvem,"Udemy courses can be a cheap way to get a basic intro to SQL. You have to wait for them to go on sale, but that happens at least once a month.",1.0
g0alcvn,i2yvem,I'd also recommend W3Schools to begin learning SQL. But would also say that it's great to learn the basics. To advance your skills I would recommend Leetcode and Strata Scratch. Give them a try!,1.0
g07ow0j,i2yc1k,"&gt; I want my latest record from 1st source ignored

is ""first"" source determined by date?

what if the first source is a B?

what if there are A rows and then a C row, do you still want the latest A ignored?",1.0
g07s240,i2yc1k,First source determined by source column,1.0
g07sgxf,i2yc1k,"sorry, that doesn't make sense

do you mean `MIN(SourceofData)`?

and what if it's not A?",1.0
g07wcaz,i2yc1k,The source is either A or B.No other sources,1.0
g080gsl,i2yc1k,"okay, great

so, please confirm, ""first"" and ""latest"" are based on DateofTraining",1.0
g080xxr,i2yc1k,"Yes, basically if source A has 3 records and source B has 1 record I want to pull out the record with the latest DateofTraining from source A and replace with the record from source B -&gt; in essence having only 3 records and not 4(if I had unioned it with two sub queries)",1.0
g07nxxq,i2yc08,"foreign keys always go into the child table, and refer back to the parent table primary (or unique) key

if you put the poLineId into the PO table, you'd have to use an array if there's more than one, so that breaks first normal form",2.0
g08txlg,i2yc08,Ok thanks,1.0
g07qknk,i2xhy6,"Hi, thanks for this.  Your article would benefit from spell-checking.",1.0
g07xenq,i2xhy6,"Hi, I am not the author of the article. I find this article very helpful so I posted here.",1.0
g07fln7,i2x2b4,"What are you trying to accomplish with this script? It seems like it's doing a lot of work for no real reason.

If you don't want to run multiple selects which return different results and result windows, remove the while loop",2.0
g0996mq,i2x2b4,"I agree with you, I cannot see the purpose. If the OP wanted to, I guess they could insert everything into a temp table and then select from that, but based on what I think I am reading through, selecting the whole table with a row number works the same.",2.0
g078yqr,i2w42q,"I’m 40 and just starting...so no it’s not too late. Its only too late if you don’t bother trying. 

The hardest part for me isn’t learning, it’s finding time to read and practice after managing my day job, house work, and children. 

Come up with a realistic plan and just get started. Even a little bit of effort will put you miles ahead.",4.0
g0794ff,i2w42q,"this is very encouraging. yes, youre right, its balancing the time that's really challenging. thank you and goodluck to you. 👍",1.0
g079nnq,i2w42q,"No. I have a data science background and am also an MBA candidate for 2021.

&amp;#x200B;

Having any ability with SQL will be like having a drill vs only having a screwdriver for you beforehand. The big thing will be knowing your limitations when working with the data and how to communicate with the tech team. This will be like having a whole set of power tools vs only a screwdriver beforehand.  


Your job will not be to build the crazy models, but to understand when &amp; where to trust the models, to question things, and what improvements one could reasonably make. You will be the person to say, ""If we bought some market data, what are our optimistic and pessimistic outcomes? Would this lift justify the expense, opportunity cost, and labor cost?""

&amp;#x200B;

Your other job will be to play around with things and to spark more, good questions in the tech team's heads :-)",1.0
g09sp25,i2w42q,"this is exactly what i am aiming for. what i just realized for sometime now is that a lot of people in the organization know how to retrieve data but are poor on communicating it in a way that it becomes vital to decision making. 

my educational background is advertising but i have been operations for 5 years now and i regret not starting earlier but my experience also helped me understand what is needed. 

thank you for the uplifting input.",2.0
g0976ft,i2w42q,"I'm with you. 37, graduating with an MBA at the end of the year. My background is in accounting, so almost zero transferable skills. I recently set up this Reddit account to get a better feel for where I am and what I still need to do, and reading about others' skills and experiences leaves me feeling downright panicked at times. I'm hoping it's not too late for me either, but there are days when it's hard to feel like I made the right choice.

SQL is definitely worth learning, as it seems that any open analytics job wants a base in SQL. Plus, it's a pretty useful skill.",1.0
g09t4pr,i2w42q,"Yes. i understand exactly what you mean. but the thing is i believe youre in a better position with the accounting degree. 

it helps to know that there still a lot of people trying to learn at a wider range of age.",1.0
g09rd7a,i2w42q,"I changed to a DB career in my late 30's. Had an internship in my masters at 38. It's never too late. At 50, I've moved up in the field and it's been a worthwhile change.",1.0
g09t9xt,i2w42q,hi! whats your background if you dont mind me asking. Your career is a very interesting setup.,1.0
g09ypuq,i2w42q,"I had a BS in education. I did a bunch of random jobs then I started to take post-bac courses in IT. I loved working with databases. After about 30 credits in post bac work, I found a Master's in Database Systems program. After I started the program I found an internship with a big company. I transitioned into an employee before I finished. It was using MS SQL server, DB2. When I finished my masters I was very lucky to get an entry job using Oracle in the defense industry as a developer then an architect. Now I'm a  DBA. It's been interesting and challenging. 

If I were starting today I would focus more in AWS and cloud database work. Of course Oracle isn't going anywhere for some companies, but cloud is the future.",1.0
g0b2e8v,i2w42q,"wow thats a long journey. like you, my background is far out. how long before you got comfortable with the  IT side?",1.0
g07d33k,i2vo06,"Use a localhost alias when connecting i.e.: ., (local), localhost, 127.0.0.1",2.0
g07eta0,i2vo06,Thanks for the reply!  Will this allow me to query the data?  There are a few replies in the original thread that suggest using Sequel Server Management Studio (SSMS).,1.0
g07g2rb,i2vo06,"Yes, I would use SSMS for manual query access. When asked for a server name, use those aliases.",2.0
g07h31b,i2vo06,Great!  Thank you 😁,1.0
g0ardye,i2vo06,Google didn't come up with the result?,1.0
g0atbto,i2vo06,"Haha!  It returned a lot of results, but I'm such a rookie that I hardly knew how to make sense of it.

The good people of reddit have have me tremendously, so now I know the tool to use is SSMS :)",1.0
g070bky,i2tliu,"Use a sub query is pretty easy

`select * from x_table 
where name not in (
                    SELECT name 
                    FROM x_table 
                    WHERE dsc in ('W2','W3')
                   )`



There are probably some typos, you could use a join instead.",1.0
g099r5u,i2tliu,"Select id, count(\*) as total

from x\_table

where dsc not in('W2',W3')

group by dsc

having count(\*) &gt;1",1.0
g8q1jsi,jakcjf,"I don't understand why you mean with ""connect to SQL""? 

SQL is a query language used by all relational databases - including PostgreSQL. So, you don't ""connect to SQL"", you connect to a database server, like PostgreSQL (where you then _use_ SQL to retrieve and manipulate data).",2.0
g8q1tjn,jakcjf,Derp I meant connect excel to the database,1.0
g8pw0kl,jajecc,"Without knowing what your tables contain, here is a general thought: if I do

    from tableA inner join tableB on (tableA.id = tableB.id)

I should check to see if [ ](https://b.id)tableB.id is a primary key or unique constraint.

Example: I have three tables:

    student: student_id / name )student_id is unique)
    student_class: student_id / class_id (student_id + class_id is unique)
    student_major: student_id / major (student_id + major is unique)

I have one student id 22 name smith Smith. He is taking 4 classes (Algebra I / English Literature / Ceramics / Biology). He has two majors (Math / Biology).

    select s.name, sc.class_id
     from student s
      inner join student_class sc on s.student_id = sc.student_id
     where s.student_id = 22 ;

will return 4 rows (the 4 rows in student\_class joined to 1 row in student)

&amp;#x200B;

    select s.name, sm.major
     from student s
      inner join student_major sm on s.student_id = sm.student_id
     where s.student_id = 22 ;

will return 2 rows (the 2 rows in student\_major joined to 1 row in student)

&amp;#x200B;

    select s.name, sc.class, sm.major
     from student s
      inner join student_class sc on s.student_id = sc.student_id
      inner join student_major sm on s.student_id = sm.student_id
     where s.student_id = 22 ;

will return 8 rows (1 row in student \* 4 rows in student\_class \* 2 rows in student\_major)",2.0
g8pw1ca,jajecc,"Take a look at the data you're adding the join to. 

It sounds like either a) your join is insufficient or b) the data in the answers table is not as expected.",2.0
g8pygnj,jajecc,"I think the `SELECT DISTINCT` is likely messing you up, suppressing your row counts. I would suggest removing the `DISTINCT`, then building your row results table by table to see how your record count changes.",1.0
g8q0yb6,jaj2ks,COUNT(hands) from dbo.confusedUsers,2.0
g8p9npx,jae9rr,"WITH tbl1 AS (
    SELECT 
        rank,
        ID,
        date
    FROM
        src
)
SELECT
    a.rank,
    a.ID,
    a.date,
    CASE WHEN date_diff(a.date,COALESCE(b.date, a.date)) &lt;= 90 THEN 1 ELSE 0 END 
FROM
    tbl1 a
    LEFT OUTER JOIN tbl1 b
        ON a.ID=b.ID and a.rank=(b.rank - 1)

Something like that should work. Do a self join on customer id and rank. Then you can compare the dates on the same row. 

Of course date and rank are both reserved words and won’t work in some dbms. Also date_diff syntax is different for different dbms. I don’t remember how todo it in Oracle.",1.0
g8pewli,jae9rr,"Hey! Thank you for sending me this logic and taking the time to send this.  I tried this logic, but it still isn't addressing the underlying problem.  So, by doing a self join,  it is comparing Rank 1-2, 2-3, 3-4,....etc.  What I am trying to do is have a date compare to the last valid date for a coupon.  So in the above example, lets say I am trying to determine if Rank 4 is a valid coupon date, the last valid coupon date would be date three so the comparison for Rank 4 is Rank 3.  For Rank 5, the comparison would be Rank 3 since that is the last valid date.",1.0
g8pw1pe,jae9rr,"Make the self join query a CTE and then select from it. 

Select ID, MAX(date) 
Where valid=1",1.0
g8q01ck,jae9rr,Thanks for the response! The issue is I am trying to make the Valid Coupon field.  It does not exist. The secondary table is trying to illustrate what I am trying to create.,1.0
g8p59y6,jada4u,"always determine and plan on granularity of your inputs and outputs.

always include data grain in the output.

very simple check for duplication/filtering errors is that count of returned records is exactly the number you need, without any 'distinct' or any other dubious math.",2.0
g8ouo6d,jacrno,"I'm not entirely sure why it's complaining about the Over clause, however I did notice that ""pid"" is in your select clause, but not your group by clause... With any luck, mysql has just lost its mind and forgot how to report an error, and that'll solve your problem? Lol

Other than that, I don't see anything wrong with your code at all, looks good to me!",4.0
g8p3qkm,jacrno,"Just to clarify: You're running a version of MySQL &amp;ge; 8, correct?",3.0
g8p5lzg,jacrno,yes,1.0
g8pibie,jabp6x,I am interested in getting involved and learning more. Please add me on Discord: Jhunter1#9560,1.0
g8owasg,jabblx,"I've never heard of the framework in this video, but OData is a mature, Microsoft-sponsored, widely-used technology for this.",2.0
g8oxzs7,jabblx,"For those wondering what OData is, below is an example

```
GET ~/$crossjoin(Products,Sales)
                         ?$expand=Products($select=Name),Sales($select=Amount)
                         &amp;$filter=Products/ID eq Sales/ProductID
```

Taken from OData's website, to do the equivalent that I did in the video (more or less) ...",2.0
g8ox0fp,jabblx,"Yup, but it doesn't add authentication or authorisation, and it requires (some) coding - But yes, OData can be used for this in the .Net space. However, Magic is also (almost) as fast as Dapper - Implying it would probably significantly outperform OData. Hyperlambda endpoints are also _""async by default""_, implying the developer doesn't even need to know how async works. And then of course, Magic also creates an entire backend for you (check out its [main landing page](https://polterguy.github.io/)) - And an entire frontend for you too (check out a [screenshot here](https://polterguy.github.io/tutorials/theming/)).

Not to mention, a guy who knows OData would still have to learn the API for how it transpiles QUERY parameters into SQL - Sometimes also sub-optimal may I add, leaving the developer little or no control over the generated SQL. With Magic, it's a copy/paste operation from Microsoft SQL Enterprise Manager or MySQL Workbench ...

But you *are* right, OData is widely used, and Microsoft sponsored, so was ActiveX, Silverlight, FoxPro, IE6, Visual Basic and ASP.NET Ajax ...

... ;)",1.0
g8pa6pz,jabblx,Any chance you might post a link that lets me read about your brillant solution rather than watching a video? I hate watching videos.,1.0
g8pbts2,jabblx,"Hehe, sorry man :)

https://polterguy.github.io/tutorials/sql-http-endpoints/

I *should* have created some screenshots - Note2self ...",2.0
g8pdjua,jabblx,Thanks!,1.0
g8pe76b,jabblx,"NP, [here's how it will look like](https://servergardens.files.wordpress.com/2020/10/screenshot-2020-10-13-at-7.20.50-pm.png). I didn't have room on my screen for everything though, but at least you can see roughly how the process looks like. Basically ...

1. Select database type (MySQL or MS SQL)
2. Select database
3. Create your SQL and arguments to endpoint
4. Click _""create""_
5. You have an HTTP REST endpoint

Configuring authorisation can be done in the _""Authorization""_ textbox ... :)",1.0
g8oifi0,jaarxr,Just create a new database for each data set. Using different installations would be overkill (unless you want to practice installing Postgres as well),1.0
g8ojdlb,jaarxr,"Will do. Can you help me understand the hierarchy in the pic in my OP? What is 1. PostgreSQL and 3. postgres? Is 3 the user postgres? I read somewhere that once you setup a server it's good practice to create a user and give that superuser privileges. If I did, where would this new user be? Under 3b. postgres? 

1. Servers

2. PostgreSQL 13

3. Databases

4. postgres",1.0
g8ojsif,jaarxr,"No, that's the name of the database. 

When you initialize [a data directory](https://www.postgresql.org/docs/current/creating-cluster.html) (aka ""database cluster"" or ""instance"") a default database named `postgres` is created. 

&gt; I read somewhere that once you setup a server it's good practice to create a user and give that superuser privileges

No, don't use a superuser for your ""normal"" work. What you can do, is create a regular user and make that user the owner of the newly created databases:

```sql
create user warmduscher password 'very secret';
create database tutorial_1
   owner warmduscher;
```

Now the (regular) user `warmduscher` can create tables and manage everything inside the database `tutorial_1` without being a superuser.",1.0
g8oyn4g,ja8hle,SQL for Data Science on Coursera.,2.0
g8of4a2,ja8hle,What position is the internship for?,1.0
g8ogl4o,ja8hle,Datacamp/dataquest,1.0
g8onl7v,ja8hle,"These Reddit posts I've seen previously here may help. Note the course is no longer free but may still be of use

[https://www.reddit.com/r/SQL/comments/j9f9bs/free_sql_server_fundamentals_book_10112020/?utm_medium=android_app&amp;utm_source=share](https://www.reddit.com/r/SQL/comments/j9f9bs/free_sql_server_fundamentals_book_10112020/?utm_medium=android_app&amp;utm_source=share)

[https://www.reddit.com/r/SQL/comments/j12zlc/free_sql_course_on_udemy_with_realworld_exercises/?utm_medium=android_app&amp;utm_source=share](https://www.reddit.com/r/SQL/comments/j12zlc/free_sql_course_on_udemy_with_realworld_exercises/?utm_medium=android_app&amp;utm_source=share)",1.0
g8p1l84,ja8hle,"Personally, I enjoyed the exercises on [sql-ex.ru](https://sql-ex.ru) in the past.",1.0
g8o854a,ja8bae,"    SELECT [ProductID]
    FROM [Production].[ProductInventory]
    GROUP BY [ProductID]
    HAVING SUM([Quantity]) &gt; 1800;",1.0
g8o8g9q,ja8bae,Perfect thank you! Would you mind explaining why you need to group it?,1.0
g8o9k78,ja8bae,"The GROUP BY clause groups rows that have the same value (in this case  \[ProductID\] ) into summary rows so that aggregate functions like SUM, COUNT, AVG, etc. can be performed. You can't perform aggregations without grouping at least one column.

[https://www.w3schools.com/sql/sql\_groupby.asp](https://www.w3schools.com/sql/sql_groupby.asp)",1.0
g8o9m90,ja8bae,Okay thank you!,1.0
g8nh0ws,ja42mm,"Use the generate\_series function in PostgreSQL

[https://www.citusdata.com/blog/2018/03/14/fun-with-sql-generate-sql/](https://www.citusdata.com/blog/2018/03/14/fun-with-sql-generate-sql/)",1.0
g8nh4rb,ja42mm,"How is your current view defined? If it's a simple ""select * from table1"" that's all you need to do. Views run the query underneath them whenever you query the view, so if you hit your view after data was added your view will reflect that automatically.",1.0
g8nil4q,ja42mm,"currently there isnt any view, a select* would get me the 1st table.",1.0
g8nj97f,ja42mm,"Ahh, gotcha. I misread that as you were going to have new data inserted into the table each week, not that you'd ONLY have that data and you needed to create new rows against it.

I think there are a few different ways you could go about this then. This likely isn't the most efficient, but the first one that comes to mind would be to select distinct week from your calendar table and then cross join that to table1. This would give you one row for each week/product combination for the weeks you specify in your calendar subquery, and you could use a dynamic date filter to adjust the weeks over time as needed.",2.0
g8neyw1,ja3p1z,"Query looks sound to me, sure you didn’t fat finger something?",2.0
g8nnhae,ja3p1z,"you cant insert data into an existing table using select into..you need to use an INSERT INTO statement. 



drop the existing table in your database and run the select into statement and see what it does.",1.0
g8n593e,ja26q5,"Sql, because pandas means having to use python which I try to avoid. You can use your sql queries in any language.",1.0
g8nitc6,ja26q5,Good video. Subscribed,1.0
g8mxdj2,ja01gj,"The immediate cause are bad statistics. The ultimate cause is a poorly designed schema and/or indices that leads to statistics not scaling properly.

I don't think it's so cut and dry. CTEs and sub queries can and often will be faster than temp table due to the performance cost of storing the data in tempdb. And obviously, temp tables just aren't an option in views and table-valued functions, and no, I do not recommend forcing everything into a stored procedure.

This is where your DBA earns his paycheck - understanding/analyzing the query plan, add query hints if needed, and most importantly, understanding why the generated query plan falls short.",36.0
g8nmmyc,ja01gj,"&gt; The ultimate cause is a poorly designed schema and/or indices that leads to statistics not scaling properly.

Eh, I wouldn't go that far. It's merely an example of asking a system a question that it wasn't designed to answer. That doesn't mean it's a bad design. It's just that the design was created with other things in mind.

There are queries in my system where I'd literally have to join 30 tables to answer some of the questions I need to ask of them, and I'd have to use multiple self-joins and joins against calendar tables with inequalities as the join conditions. We're talking about tables creating near cross-joins against each other with filter conditions 8 to 10 tables apart. And the data needs to answer the questions being asked of it... but not with complete 100% up-to-the-minute data.

Like, there's a reason data warehousing is a thing. Data warehousing is in many ways, ""Hey, let's just make the temp table for this report permanent.""",3.0
g8nqymq,ja01gj,"""Every system is perfectly designed to achieve the results that it gets."".  Or something like that.",1.0
g8o7uru,ja01gj,"""perfectly designed"" makes me cringe. How many times the first words coming from a top manager were ""perfectly designed"", only to realise that the initial design was far from perfect and the subsequent tunes were all ad-hoc solutions to save a sinking boat.",3.0
g8oztzg,ja01gj,"&gt;  It's merely an example of asking a system a question that it wasn't designed to answer.

Hey, don't you know you're not supposed to live in the real world when you talk about DB design?

You're the type of person I'd want on our team. Understanding something may not be designed perfectly, but there are a million reasons why it is the way it is. Nothing pisses me off more than when we have a problem and someone's answer is ""well the system was designed wrong!"" It was built 20 years ago, and we're in the process of a rebuild, but we have to work with what we have. 

Also, for the most part, out system works pretty damn good, and that's a credit to it's original designer. We're just hitting weird edge cases with massive amounts of data.",1.0
g8nmsel,ja01gj,"It sounds like you are agreeing. Not recognizing when you need to denormalize data is very much in the ""poor schema design"" wheelhouse.",1.0
g8nsz05,ja01gj,"This is a great comment and one of my main points of frustration with our BI team. They heavily criticize whenever we denormalize data... but our designs work way faster than there's, they solve problems, and they do it more efficiently. They whine because it takes up extra storage... which is true... but who gives a fuck? Storage is cheap and we aren't talking about multiple TB's, we're talking a few hundred MB or possibly a few GB.",1.0
g8p0i56,ja01gj,"&gt; They whine because it takes up extra storage... which is true... but who gives a fuck

OMG been there. I was so pissed about a similar situation last week I said I'll fucking buy you a TB hard drive out of my own pocket just so I don't have to deal with this issue again. Somehow someone though 90 chars should be the limit for a name, to save space.",1.0
g8nhity,ja01gj,"This is why I think I'd be entry level dba at best.

Although I am getting way more familiar with the limits of CTEs and sub queries over temp tables",2.0
g8nsrz2,ja01gj,"I agree with you but only at a certain point. A simple CTE, or simple sub-query is fine. When you start throwing in a ton of strange joins, and a ton of work in your SELECT (like CASE statements, functions, whatever) then generally speaking (in my experience) a #table will perform better than running the entire query as a whole.

It's a doulbe edged sword though because it is simply easier to write a query with subqueries or as a CTE, and its only once you reach a certain point of complexity that it starts making sense to chunk the job out into #tables and creating indices on those #tables.

Other people may be correct that it's due to statistics, or whatever, but at the end of the day I don't have time for a lot of that shit and I've taken jobs that take hours to run and gotten them down to running in 15 minutes by breaking the job up into chunks.",1.0
g8omyyg,ja01gj,"&gt;  A simple CTE, or simple sub-query is fine.

Unless you're referencing that CTE/subquery more than once in the same query. As soon as you do that, it's time to think about using a temp table because you're now executing that whole CTE/subquery _each time_ it's referenced.

I took a 15-minute query down to under 90 seconds just by moving a subquery referenced multiple times out to a temp table.",0.0
g8n11mx,ja01gj,"There are a lot of scenarios where a temp table is not an alternative to a subquery so I wouldn't really consider them competitors in that sense. It's like saying buses are better than pickup trucks or tanks. It depends on what you're doing.

But yes, there are scenarios when you're doing batch processing like you're mainly talking about where putting things into a temporary table (or even a normal staging table) can be the most effective way of processing data. You see this approach a lot with ETLs. But if you're building a front end system where a query might be run tens of thousands of times a day, I'd question whether populating a temporary table each execution would make sense.",9.0
g8nh5v9,ja01gj,"Well I'd argue that if you have a query that's running that often, it's likely worth caching it, unless it 100% needs to be in real time.

Most of the stuff I've dealt with in that scenario, benefits greatly from even a 5 to 15 second cache, hardware permitting. But that's more the software developer in me, not the sql part. Trying to get a better understanding of the database stuff",0.0
g8myjky,ja01gj,The place where temp tables won't work is if you need to have one atomic piece of SQL in a script.,9.0
g8myz76,ja01gj,"This doesn't answer your question specifically, but in my organization, the data science team does not have rights to create temp tables.  So, we are forced to use CTEs and subqueries.",7.0
g8n0ypy,ja01gj,"Same for me... we can’t create temps and can’t generate Data-staging forms unless you are “IT” which sucks.   CTEs and sub queries may take a little longer, but makes it easier to read for new employees IMO.",5.0
g8ndth9,ja01gj,Rules like this infuriate me as a (SQL Server) DBA. I can think of no legitimate reason to have this rule unless you _want_ to spend more money on hardware and licensing.,7.0
g8ndpx7,ja01gj,"In SQL Server, if you can execute a `select` query you have rights to create temp tables. It's not possible to revoke that permission AFAIK.

By ""temp tables"" I mean _actual_ temp tables, prefixed with `#` or `##`. Not creating a ""real"" table in the database and then having to remember to drop it when you're done.",2.0
g8nqdau,ja01gj,Not sure why you got downvoted. I agree there is no way to restrict user access to the TempDb space. I assumed that the OP was talking about local and global temp tables where the table names are prefixed with `#` or `##`. I don't think some people understand there's a difference between a real table and temp table.,2.0
g8omtud,ja01gj,"Probably because people didn't check the flair to know that we're talking about SQL Server here.

* MySQL requires a distinct security grant to create temp tables.
* Oracle requires `CREATE TABLE` to create a global temporary table which is kind of a weird beast. But in newer versions, anyone can create a private temporary table which behaves more like a SQL Server temp table except that it's in-memory instead of materialized to disk. Oracle CTEs can be materialized, which probably leads people to think of and use them like read-only temp tables (prior to the availability of private temp tables).",1.0
g8ph2nc,ja01gj,That makes sense. I'm not too familiar with the other flavors of SQL. Working in MySQL without temp tables sounds absolutely miserable. Do you know if MySQL has equivalent alternatives to temp tables if temp tables are restricted?,1.0
g8nhdfk,ja01gj,"If I were running your team you'd have access to that. Maybe only a select few, but junior level people could develop with that notion, knowing it would get reviewed.

If I code review anything that gets to production and dicks it up, that's on me, not the person who wrote it",1.0
g8muggz,ja01gj,When dealing with systems that use the tempdb like you are describing to break things down that can reach a volume point where the tempdb becomes a bottleneck even with appropriate hardware and configuration. So I've recently had to go the other direction for more straightforward complexities to CTEs which tend to use RAM to relieve the tempdb.,5.0
g8nhpp2,ja01gj,"So I know this is dumbing down things, but are you saying if we threw more RAM at the server it could help? Because we have plenty not used right now in anticipation of growth.

I know more processing power isn't bad, but I've been constantly told by our senior person that what we have is fine, when it's obviously not, during peak traffic",1.0
g8nryik,ja01gj,If CTE can fit in memory why write it to disc in a temp table?,5.0
g8nwers,ja01gj,"It's possible that performance is tanking on the CTE due to limited RAM but the query itself could also not be great and the temp table allowed you to restructure the query.

Check out SentyOne Plan Explorer to review the execution plan of the query. It's the same plan you can get in SSMS but it's easier to navigate. Also ignore the percentage cost it assigns to nodes, it's frequently misleading. Missing indexes may also be factor.

I don't want to make it sound like temp tables are a bad answer though.  I still use them where needed. Best case you can make the queries as basic as possible and have the code do any business logic stuff instead of tempdb or CTEs. It really depends on the usage volume if your going to get bottlenecks by tempdb.",3.0
g8n3dmn,ja01gj,"Generally, CTEs are better for smaller tables (&lt;100,000 records) and are good for recursion.",5.0
g8ngpey,ja01gj,That would make sense with what I've seen working here,1.0
g8nllyr,ja01gj,"With less than 100k records, performance shouldn't be an issue.",1.0
g8nog6c,ja01gj,"If nothing else it’s less code to create a CTE than to  create a table. Also, IIRC there’s less server overhead using a CTE vs a temp table",1.0
g8n466z,ja01gj,"i don't come from the MS SQL world... are temp tables also physical tables in this DBMS? are there some special optimizations for temp tables? because if the answer is no (as in some other DBMSs) then I'd much rather stick with CTEs, unless as you said, the table becomes very large.",2.0
g8nf492,ja01gj,"It is just a physical table, in a special database (tempdb).",1.0
g8ngnf9,ja01gj,"They are physical tables in a reserved database (tempdb). You could bring down a server if you filled up temp db with a bad query and also had no limits on it, but that is kind of the point.

If you have your temp db restrictions in place, when someone tries to slam 50G worth of data into tempdb it fails, but the server is fine",1.0
g8negge,ja01gj,"In SQL Server, with the exception of recursive CTEs, CTEs are syntactic sugar for subqueries and they'll behave pretty much identically. Unless you start nesting the CTEs, in which case you'll eventually hit a tipping point and performance will start to suck.

There are cases where the overhead of a temp table may outweigh the performance improvement over a CTE by a small amount, but in situations where I have a choice between the two, I go for the temp table by default and then move to a CTE if it makes more sense.",2.0
g8ngb8o,ja01gj,"Yea they've always just seemed to be a better alternative to me. And I'm not a sql guru, it's why I just wanted input",1.0
g8nhil8,ja01gj,What about table variables?,2.0
g8nizpg,ja01gj,Root of all evil. Kidding.,2.0
g8onhpu,ja01gj,"They're OK for very specific use cases. Like passing a set of data into a stored procedure or function. But they have serious performance issues until you get to SQL Server 2019 because they can totally hose your cardinality estimates.

* People think table variables exist only in memory, which means they'll somehow be faster. Table variables, if they get large enough, will spill into Tempdb - which is where temp tables live.
* People think temp tables _only_ live on disk. But just like any other table, the pages have to be in the buffer pool to operate on. So as long as your temp tables fit there (likely), they're sitting in memory anyway.

I've yet to encounter a situation where a table variable resulted in significantly better performance than a temp table (To fend off the inevitable response: I'm sure someone has. I just haven't seen it myself). I reserve them exclusively for passing data in and out of stored procs because there's very few ways to do that.",2.0
g8nm1oj,ja01gj,I think they have their place. We just deal with large enough data sets where table variables lose their efficiency,1.0
g8oibe3,ja01gj,"Thank you sooooo much for posting this discussion, it has been so very helpful",2.0
g8mrry1,ja01gj,I agree completely.  Temp tables &gt; CTEs.,5.0
g8o669f,ja01gj,What's your reasoning? There's been other posts in this thread outlining which scenarios CTEs are better in.,1.0
g8p4cml,ja01gj,"Depending on your data size, CTEs are all in memory until they run out and spill over to disk.  Temp tables can be indexed is another benefit.  Also, execution plans are saved for queries that have temp tables, I’m not sure CTE queries have saved execution plans.",1.0
g8muzm3,ja01gj,"Agreed, but I also use table functions, views, and rework the JOINS in place of subqueries/CTEs. Using fewer subqueries/CTEs also helps with code readability.",3.0
g8nhkmz,ja01gj,My man,1.0
g8mvovr,ja01gj,"to me, using tempdb's is better if you're dealing with processes or have to take the data into different steps to transform it. if it is legitimately faster than there is nothing wrong with it.",3.0
g8nqlph,ja01gj,"I wish.  They're incompatible with Visual Studio, or at least for what I need them for.  But when I can use them, I love temp tables.  Much easier to debug a query too.",1.0
g8ohsye,ja01gj,no DBA likes temp tables actually :),1.0
g8onk6x,ja01gj,"_raises hand_

DBA here. I love temp tables.",2.0
g8opex9,ja01gj,"me too.

I have all my users drilled

        if object_id('tempdb.dbo.#temp','U') is not null
        drop table #temp;
    
The only thing I'd add is creating large temp tables without defining data types can lead to problems on larger datasets where type isn't constant (table a joining to temp on string column a = temp column that only had ints in it's generation dataset)",1.0
g8otdjr,ja01gj,What do you use instead?,1.0
g8ou4lc,ja01gj,"never temp tables.
either subquery factorisation or sub queries
or well constructed joins.

but im coming from Oracle. maybe for other databases there might be other practices.",1.0
g8owddz,ja01gj,Yea I think it's different in MSSQL,1.0
g8pe21r,ja01gj,"It’s different for SQL Server, temp tables are heavily optimized in SQL Server.",1.0
g8pepu5,ja01gj,"From a SQL Server point of view:

I reserve CTEs for when I need to use the same subquery multiple times in a single statement.

I reserve subqueries for very basic lookup things such as getting a label/name using some ID.

Otherwise I use temp tables. You can garbage collect them yourself as you go, as well as index temp tables. They’re incredibly useful.

Then again, I keep my development extremely simplistic even for the most complex of tasks. Each SQL statement in a long stored procedure would generally do one “thing”. Much like in general programming a function does one “thing”.

I prefer this approach because it makes it much easier to read months/years down the line when I open it back up, or anyone else does.

People writing multiple nested subqueries and throwing everything into a single SQL statement, unless absolutely needed, really aren’t doing any favors for themselves.",1.0
g8pgt0p,ja01gj,You pretty much share all the same view points I do as well.,1.0
g8n0ve2,j9zwa1,[https://stackoverflow.com/questions/17905873/how-to-select-current-date-in-hive-sql](https://stackoverflow.com/questions/17905873/how-to-select-current-date-in-hive-sql),1.0
g8nco7e,j9zpn2,"I've had to work on a few databases where the names of the tables and fields were not easy to work out what they are and only time analysing the data allowed me to understand what the fields were and how the tables were joined.  The advantage you have with this one is you do have a diagram of how the tables are linked.

As for trying to work out the naming of the fields, do a simple select * on each table and look at the data.",1.0
g8nuhsn,j9zpn2,"Use the sample questions and sql answers to determine what’s in the table. The question will tell you what the business logic is, the example SQL specifies the tables used and the code to implement the logic.",1.0
g8msffl,j9z3xy,"&gt; is there an easy way to dynamically add columns with the column name being the key name stored in the json field

No. 

One fundamental restriction of the SQL language is, that the number, names and data types of all columns of a query must be known to the engine _before_ the query is executed (i.e. while it's parsed). 

What you can generate dynamically is one row per JSON key, but I assume that's not going to help you.",1.0
g8mvyt1,j9z3xy,Got it- thanks for the heads up. Looks like I will use some Python on this then.,1.0
g8mgh3i,j9y7ho,"Do you know about ""LEFT JOIN""?",1.0
g8mgqrc,j9y7ho,"Nope, can you tell me about it?",1.0
g8migyo,j9y7ho,How would you do the query then?,1.0
g8mk3hk,j9y7ho,"It combines rows from multiple tables, using ""ON"" to specify how rows from each table are joined. Using left join you can find lesson hours without matching lessons.",2.0
g8ms0u0,j9y7ho,"Ah okay, but what's in the WHERE clause then?",1.0
g8o77ss,j9y7ho,"Form the query to join the tables without a WHERE, have a look at the results, and you'll see what the WHERE should be.",1.0
g8mbb95,j9wxen,"It's been a while since I've worked in Access but this may work:

    SELECT DISTINCT ssn
    FROM accdb
    WHERE date1 = date2
    GROUP BY ssn, moneyValue
    HAVING COUNT(ssn) &gt; 1",3.0
g8me1jq,j9wxen,"&gt;  select the ssn that appear at least twice

    SELECT ssn
         , COUNT(DISTINCT moneyvalue) AS moneyvalues
      FROM yourtable
    GROUP
        BY ssn
    HAVING COUNT(*) &gt; 1
    
&gt; From those results i want to only get the ones where date1 is equal to date 2

    SELECT yourtable.ssn
         , yourtable.date1
         , yourtable.date2
         , yourtable.moneyvalue 
      FROM ( SELECT ssn      
                  , COUNT(DISTINCT moneyvalue) AS moneyvalues              
               FROM yourtable    
             GROUP               
                 BY ssn          
             HAVING COUNT(*) &gt; 1 ) AS these
    INNER
      JOIN yourtable
        ON yourtable.ssn = these.ssn
       AND yourtable.date1 = yourtable.date2
       
&gt; From those results i want to get the results where there are different values in moneyvalue per ssn.       

    SELECT yourtable.ssn
         , yourtable.date1
         , yourtable.date2
         , yourtable.moneyvalue
      FROM ( SELECT ssn      
                  , COUNT(DISTINCT moneyvalue) AS moneyvalues              
               FROM yourtable    
             GROUP               
                 BY ssn          
             HAVING COUNT(*) &gt; 1 ) AS these
    INNER
      JOIN yourtable
        ON yourtable.ssn = these.ssn
       AND yourtable.date1 = yourtable.date2
     WHERE these.moneyvalues &gt; 1",1.0
g8o42wc,j9wxen,"&gt;SELECT ssn  
, COUNT(DISTINCT moneyvalue) AS moneyvalues  
  FROM yourtable  
GROUP  
BY ssn  
HAVING COUNT(\*) &gt; 1

Thank you, does not work in ms access however. It gives me a syntax error on the count.",1.0
g8m9306,j9ww7f,"I think it depends a bit on the DBMS being used. 

For Postgres, Linux is definitely the better choice, because many interesting tools and extensions are only available on Linux. 

For Oracle I don't think it matter, although I do have the feeling that the filesystem performance on Linux is a bit better. 

For SQL Server I can't tell, but probably Windows, given the fact that the Linux version doesn't have such a long history and many interesting tools are probably only available on Windows.",2.0
g8m8nqc,j9ww7f,"I’m not a DBA, but in my opinion Linux would provide you with a bit more experience just because of the command line. Really the gap is small and operating system should matter very little to a DBA. Why not both",1.0
g8nv3hr,j9ww7f,If you’re using big data choose Linux. Otherwise use what your dbms suggests.,1.0
g8m967q,j9wiuy,OK If I'm reading that XML correctly the data is stored in the attributes of the element Data. Data is an element of Level. Level has an attribute Number. Does that help?,1.0
g8mbuq5,j9wiuy,"Yeah so in a single row in the database I would have a value for 530, 540, 550, and 560, all sharing a single unique ID and time stamp. I'm trying to figure out how to say to put value 55.5555 in the 530 column and put 66.6666 in the 540 column.

I basically need to tie the number attribute to a column, as seen below:

https://i.imgur.com/HVqlvfX.png

So that screenshot is basically the goal",1.0
g8modct,j9wiuy,"So my thought is since you are using SQL Server is to do a pivot query against the XML. 

I would try using a CTE to parse out the XML data then take the results of the CTE and Pivot on that to get the result with the correct column headings and then insert that result into your database. This is a one query method that may perform poorly if you have a lot of data.

If the CTE does not work parse the XML into a temp table then pivot from there to another temp table if necessary then insert from that. This divides the work into smaller bites which can perform much quicker that the one big bite above. Also I usually add the appropriate index to each temp table as I go.

Good luck.",1.0
g8m28iu,j9vn5p,"&gt; Any advice on how to tackle this project?

throw each question into google, appending with ""in SQL""

then try out the queries that are suggested

i trust you have some way of verifying the actual numbers that your queries produce?  like, loading your sample data into excel and working out a few of the answers that way?  because running a query and getting some numbers back isn't enough, you have to know whether the answer is right or not",1.0
g8mk4qw,j9vn5p,I’ll have to check with my professor on whether the answers are correct. Good advice,1.0
g8mkt5o,j9vn5p,"&gt; we do not want to rely on the professor's help too much

no, seriously, dump your data into excel and do a couple of the problems there to confirm your query results",1.0
