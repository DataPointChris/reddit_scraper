post_id,post_title,post_body,upvotes,subreddit,date
j9q6bp,A scikit-learn compatible library to construct and benchmark rule-based systems that are designed by humans,,6,scikit_learn,2020-10-13
j93854,Best performance on Scikit-learn’s load_digits dataset,"On Scikit-learn’s load_digits dataset:
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html

Does anyone know what is the best performance achieved so far on this dataset? 

I tried googling around but can’t find examples with 100% score performance. I am thinking since this is a standard dataset, it would be easy to get a 100% score performance?",1,scikit_learn,2020-10-13
j8vepi,"could someone ELi5 the hyperparameters (penalty, C, tol, max_inter)",I am currently working on a beginner project on logistic regression using scikit\_learn. I am trying to fine tune my regression model but cant seem to find any websites that can explain what the parameters mentioned in the title mean exactly and how to use them. I was wondering if anyone could give me a quick explanation on what/how to use these parameters to fine tune my regression model.,0,scikit_learn,2020-10-13
j7jz85,SVC rbf kernel seems to be nonstandard?,"I am currently testing a precomputed version of rbf I implemented to get a better feel for how it works and possibly later check out some other kernels.

It seems that whatever I do, I get different results using my precomputed gram matrix vs using the scikit rbf kernel:

To calculate a kernel entry for datapoints xm &amp; xn, based on some extra parameters theta

`k = thetas[0] * np.exp(-(thetas[1]/2.) * (np.sqrt((xn-xm).T @ (xn-xm)))) + thetas[2] + thetas[3] * (xn.T @ xm)`

using theta = \[1,2,0,0\]

This should recover the formulation given [here](https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel) (setting gamma=1)

1 \* exp( - 2/2|xn-xm|^(2) )   


is there something I'm missing? \[here's the code if you wanna take a look\]([https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit\_test.py](https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit_test.py)) (only dependencies are matplotlib, scikit &amp; numpy, so you're probably good if you're on this sub)",1,scikit_learn,2020-10-13
j5pcm6,ImportError,"I've got a long error which ends with:  
ImportError: DLL load failed while importing \_arpack: Não foi possível encontrar o procedimento especificado. (rough translation: Unable to find the specified procedure)  


Any idea on the issue? It seems like I have some sort of update issue, but I'm unable to find what.",2,scikit_learn,2020-10-13
j2v11w,"Scikit-learn. In the case of a single point, k-nearest neighbours predictions doesn’t literally match with the literally nearest point. I think I know why. Correct me if I’m wrong.","Hello.  I’ve looked at the source code. 

Case population sizes in the range 10 ^ 2 to 10 ^ 5 ish. Vanilla, straight out the box knn from scikit-learn.  Except 1 nearest neighbours not the default 5.  

When I try to predict the nearest neighbour of a point, using 1 nearest neighbours. after using knn.fit to make a model, it doesn’t _always_ return the same value of the actual nearest neighbour.  I’ve worked out the actual real nearest neighbour myself to check, using trig, and unit tested it.  

I think that’s because for pragmatic reasons knn is just a probabilistic model applied at group level.  Not exactly the actual knn for each and every point.  

Am I right?

EDIT:  My. Trig. Was.  Wrong.  Due. To.  A. Data frame. Handling.  Issue.  Ggaaahhhh.",5,scikit_learn,2020-10-13
j1idtx,RadomizedSearch CV taking forever,"Hi ,

I have the below snippet.

Trying to run on GCP . its getting stuck and not even updating.

&amp;#x200B;

https://preview.redd.it/bp2zfi71sxp51.png?width=1463&amp;format=png&amp;auto=webp&amp;s=6d97d1ff6083f6eb65e62d983770c69f06d45f4c",2,scikit_learn,2020-10-13
iv8jv9,Neuraxle - a Sklearn-Based Clean Machine Learning Framework,,1,scikit_learn,2020-10-13
it82un,How the 'init' parameter of GradientBoostingRegressor works?,"i'm trying to create an ensemble of an determined regressor, with this in mind i've searched for some way to use the sklearn already existing ensemble methods, and try to change the base estimator of the ensemble. the bagging documentation is clear because it says that you can change the base estimator by passing your regressor as parameter to ""base_estimator"", but with GradientBoosting you can pass a regressor in the ""init"" parameter. my question is: passing my regressor in the init parameter of the GradientBoosting, will make it use the regressor i've specified as base estimator instead of trees? the documentation says that the init value must be ""An estimator object that is used to compute the initial predictions"", so i dont know if the estimator i'll pass in init will be the one used in fact as the weak learner to be enhanced by the bosting method, or it will just be used at the beginning and after that all the work is done by decision trees. If someone can help me with this question i would be grateful.",4,scikit_learn,2020-10-13
igtkry,Best way to get T-Stastic and P-value etc?,"I'm using scikit learn for linear regression.  Is there a way to use that library to generate things like T-Stastic and p-value and standard error etc?

On stack overflow i found this, but wondering if there's a way within scikit

    import statsmodels.api as sm
    from scipy import stats
    X2 = sm.add_constant(X)
    est = sm.OLS(y, X2)
    est2 = est.fit()
    print(est2.summary())

&amp;#x200B;",1,scikit_learn,2020-10-13
i4ulg2,"Data Visualization using ""Python"" with ""Seaborn"" | Part- I",https://youtu.be/X400eIcV-So,3,scikit_learn,2020-10-13
i3546z,Recommendation based on other user following,"Hello,

I try to build a recommendation system.

My service allow users to follow people (not rate them, just follow) and I would like to be able to propose to users to follow people based on other user’s database activity.

Is scikit a good path for this ? 

Do you recommend specific method or useful ressource to read to achieve this ?

For your help guys!",2,scikit_learn,2020-10-13
hzw282,How to use TensorFlow Object detection API to detect objects in live feed of webcam in real-time,,1,scikit_learn,2020-10-13
hwmcf1,sklearn CCA - how to get variance explained for first canonical relationship?,"Hi. I'm exploring multivariate brain-behaviour relationships with sklearn's canonical correlation analysis tool ([https://scikit-learn.org/stable/modules/generated/sklearn.cross\_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca)). I am interested mostly in the first canonical relationship between the two datasets. The decomposition is working fine and i have the weights/canonical scores etcetera - but what i'd really like to know is how much of the variance in either dataset is explained by that one relationship (analogous to eg variance explained by first principal component).

There is a method named 'score' that i can call on the CCA object but I am not quite sure this is what I need. This score is not the same as 'canonical scores above but will supposedly get some coefficient of determination r\^2 between 'observed' and 'predicted' - not sure how to understand this. The description on the webpage is quite terse and it does not behave the way i might expect.

I'm hoping to find someone who might know whether that 'score' method  will get me to what i want - and if so, maybe how to use it. Or point me otherwise in the right direction to get into the variance explained for CCA.

Cheers!",2,scikit_learn,2020-10-13
hu6y83,KMeans Algorithm Question,"Hey all.

I am new with using scikit-learn and had a question regarding the KMeans algorithm functions. After running the algorithm and plotting the clusters, are the clusters with the centroids plotted the final clusters after training is done or is there training that I have to do on the clusters? 

Thanks everyone",1,scikit_learn,2020-10-13
htugnl,"How to handle ""Missing Value"" from ""Dataset"" using ""Pandas"" &amp; ""Sci-Kit Learn""??",https://youtu.be/8IORSsZIyIQ,0,scikit_learn,2020-10-13
htmc59,"How to handle ""Text"" and ""Categorical Attributes"" using Python and Pandas??",https://youtu.be/4sO7Pezlegk,0,scikit_learn,2020-10-13
ht4ol1,Making ROC curves with results from cross_validate?,"I am running 5 fold cross validation with a random forest as such:

from sklearn.ensemble import RandomForestClassifier

from sklearn.model\_selection import cross\_validate

forest = RandomForestClassifier(n\_estimators=100, max\_depth=8, max\_features=6)

cv\_results = cross\_validate(forest, X, y, cv=5, scoring=scoring)

However, I want to plot the ROC curves for the 5 outputs on one graph. The documentation only provides an example to plot the roc curve with cross validation when specifically using StratifiedKFold cross validation (see documentation here: [https://scikit-learn.org/stable/auto\_examples/model\_selection/plot\_roc\_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py))

I tried tweeking the code to make it work for cross\_validate but to no avail.

How do I make a ROC curve with the 5 results from the cross\_validate output being plotted on a single graph?

Thanks in advance",2,scikit_learn,2020-10-13
hm6td7,Best performance on MNIST - Fashion dataset,Does anyone know what is the best performance achieved so far for the MNIST - Fashion dataset along with what model that was used?,1,scikit_learn,2020-10-13
hm22vz,"How to ""Predict"" my friends weight using ""Machine Learning"" and ""Sci-Kit Learn""??",https://youtu.be/A4JwnkTFEXI,2,scikit_learn,2020-10-13
hl4k0u,Factor analysis “model” in CS229,"In one of Stanford’s CS229 lecture by Andrew Ng (https://m.youtube.com/watch?v=tw6cmL5STuY), he talks about a factor analysis “model” in which is to deal with situations where you have a lot more features than samples in your dataset. He even said he used a modified version of this factor analysis “model” in some recent work he did for a manufacturing company in the lecture.

Now my understanding of factor analysis is just a dimension reduction technique. So how did Andrew used factor analysis to build a “model” which deals with datasets which has a lot more features than samples?",2,scikit_learn,2020-10-13
hkm9qn,StackingRegressor Inconsistent Output,"Is it intentional that StackingRegressor returns different accuracy outputs when running multiple times given the same parameters, models and using numpy set seed?",1,scikit_learn,2020-10-13
hjctba,"This lecture that talks about what Multilabel and Multioutput classifications are, along with their implementation using scikit learn.",,1,scikit_learn,2020-10-13
hg5j3s,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,2,scikit_learn,2020-10-13
hf60u0,"Hey guys, here is a lecture on how to implement gradient descent with scikit-learn. Enjoy :)",,1,scikit_learn,2020-10-13
hakk07,How do I create a linear regression for this groupedby dataframe?,"I have this assignment for a job interview and I really want to impress by using some machine learning. I don't know too much about it and I essentially don't have much time to learn that much about it. I have the following [dataframe](https://postimg.cc/0zNbyrV8) and I want to create a linear regression using scikitlearn of \['profit'\] vs \['dateReceived'\] for each \['Language'\]. 

Does anyone know what I can do for that to work? I guess it should be just a few lines of code, but I could be wrong?",0,scikit_learn,2020-10-13
h7ay1o,"Visualize Scikit-learn models – ROC, PR curves, confusion matrices etc",,8,scikit_learn,2020-10-13
h172dr,Scikit Learn Tutorial in One Hour,,5,scikit_learn,2020-10-13
h0w3xx,Books about classification algorithms,"Hi all,

I am completely new to data mining and have to write a seminar paper about classification and do some programming in python.

With the help of datacamp I was able to implement the classification algorithms in python.

Now I am looking for some sources that I can cite in my paper that briefly explain these algorithms.

My problem is that most books that I have look into so far are very mathematical and since I don’t have a data mining/computer science background, they are hard to understand.

Do you have any recommendations for some text books that explain classification algorithms such as SVM, Naive Bayes, Trees, etc. that are well recognized, but explain them in an easy way?

Many thanks in advance!",1,scikit_learn,2020-10-13
gziaus,How to choose best pair of random state and class label values?,"For the last few days, I was trying to implement the KMeans algorithm using SciKit Learn, But I came across a very confusing problem. I have a dataset that has two class labels ['ALL', 'AML'] where ALL has 47 and AML has 25 samples and 100 attributes to train from and now I want to use this dataset for KMeans clustering so that I can compare the predicted results with the original class labels. Before asking my question let me explain certain scenarios. In all the scenarios I have taken all the 100 attributes to fit the model.

Scenario 1:

In the first run, I started with a model that is created with pretty much default arguments i.e. model = KMeans(n_clusters=2). For comparing the predicted class labels(which are numeric) with the original labels(which are strings), I set the original class labels as ALL = 1 and AML = 0. After that, while comparing using a classification report I got an average accuracy of 35%. Then I run the algorithm once again and got an accuracy of 44%. For the third try, I got 33% and so on.

However, I looked about it and came to know that the random_state argument needs to have a fixed value to get same accuracy throughout all runs.

Scenario 2:

After knowing about random_state, this time I started with random state 0 and created the model as model = KMeans(n_clusters=2, random_state=0) and kept the original class labels as before i.e ALL as 1 and AML as 0. However, this time the output didn't change on different runs and I got an accuracy of 53%. But, out of curiosity, I swap the original class label i.e. I set ALL as 0 and AML as 1 which results in 47%.

Scenario 3:

This time I choosed random_state as 1 i.e. model = KMeans(n_cluster=2, random_state=1) and having ALL as 0 and AML as 1 gave 67% accuracy while considering ALL as 1 and AML as 0 gave 33% accuracy.

So, My question is what I am doing wrong here? Am I implementing something wrong? If I am right then why the result is changing so much depending on random_state and class labels? What's the solution and how to choose the best pair of random_state and class labels?",1,scikit_learn,2020-10-13
gwgzy9,estimate_transform works when using 'similar' but not when using 'affine',"I have two 512x512 grayscales images (src and dst). To try to understand estimate transform I applied the following transformation

    tform = transform.AffineTransform(scale=(1.3, 1.1), 
                                        rotation=0.5, 
                                        translation=(0, -200)) 

to the src to create the dst. Then I want to find back the parameters using estimate\_transform.

With the parameter 'similar' I obtain parameters very close to the one I used (as expected). But when I want to use 'affine', I obtain the following error :

     matmul: Input operand 1 has a mismatch in its core dimension 0, 
    with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 513 is different from 3) 

Any idea why ? Here is my code :

    src = rgb2gray(data.astronaut())
    dst = rgb2gray(data.astronaut())
    tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5,
                                      translation=(0, -200))
    dst = transform.warp(img1, tform)
    tform_fin = transform.estimate_transform('affine', src, dst)
    dst_corr = transform.warp(img3, tform.inverse)",1,scikit_learn,2020-10-13
gtw4p9,What can I do when I keep exceeding memory used while using Dask-ML,"I am using Dask-ML to run some code which uses quite a bit of RAM memory during training. The training dataset itself is not large but it's during training which uses a fair bit of RAM memory. I keep getting the following error message, even though I have tried using different values for n_jobs:

```
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
```

What can I do?

Ps: I have also tried using Kaggle Kernel (which allows up to 16GB RAM) and this didn't work. So I am trying Dask-ML now. I am also just connected to the Dask cluster using its default parameter values, with the code below:

```
from dask.distributed import Client
import joblib

client = Client()

with joblib.parallel_backend('dask'):
    # My own codes
```",1,scikit_learn,2020-10-13
gsx926,MLPRegressor newby with some (probably very basic) questions in need of some assitance,"Hello!

I'm building MLPRegressor for the first time ever (I've been learning how to code with online courses since end of March) and I know something is wrong but I don't know what. Bellow you can see my code so far. It runs and I have a value for r2 ( -9035355.06 ) and a plot. However the r2 score doesn't make sense (it should be around 0.7)  and the plot doesn't make sense either.

I have run this analysis with SPSS multilayer perceptron feature so I know more or less how my results should be and that's why I know whatever I am doing with python is wrong.

Any advice/suggestion of what I'm doing wrong is very welcome! This coding world is kinda of frustrating for me:/

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    
    from sklearn import neighbors, datasets, preprocessing 
    from sklearn.model_selection import train_test_split
    from sklearn.neural_network import MLPRegressor
    from sklearn.metrics import r2_score
    
    vhdata = pd.read_csv('vhrawdata.csv')
    vhdata.head()
    
    X = vhdata[['PA NH4', 'PH NH4', 'PA K', 'PH K', 'PA NH4 + PA K', 'PH NH4 + PH K', 'PA IS', 'PH IS']]
    y = vhdata['PMI']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 
    
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_test_norm = scaler.fit_transform(X_test)
    
    nnref = MLPRegressor(hidden_layer_sizes = [4], activation = 'logistic', solver = 'sgd', alpha = 0.1, learning_rate= 'constant',
                         learning_rate_init= 0.6, max_iter=200, random_state=0, momentum= 0.3, nesterovs_momentum= False)
    nnref.fit(X_train_norm, y_train)
    
    y_predictions= nnref.predict(X_test_norm)
    
    print('Accuracy of NN classifier on training set (R2 score): {:.3f}'.format(nnref.score(X_train_norm, y_train)))
    print('Accuracy of NN classifier on test set (R2 score): {:.3f}'.format(nnref.score(X_test_norm, y_test)))
    print('Current loss : {:.2f}'.format(nnref.loss_))
    
    plt.figure()
    plt.scatter(y_test,y_predictions, marker = 'o', color='blue')
    plt.xlabel('PMI expected (hrs)')
    plt.ylabel('PMI predicted (hrs)')
    plt.title('Correlation of PMI predicted by MLP regressor and the actual PMI')
    plt.show()",1,scikit_learn,2020-10-13
gsol3k,What are the default values for the parameters in Dask-ML's Client() function,"I am trying to understand Dask-ML's Client() function parameters. Say I have the following code using Dask-ML's Client() function:

```
from dask.distributed import Client
import joblib

client = Client()
```

If I don't specify any values for the parameters in the Client() function, what are the default values for the parameters:

(i) n_workers

(ii) threads_per_worker

(iii) memory_limit

From my understanding, Python has the Global Interpreter Lock (GIL) feature which prevents multi-threading. If so, why does Dask-ML's Client() function have the parameter threads_per_worker when multi-threading is prevented in Python?

Does memory_limit refers to the maximum memory limit allowed for **each** worker/machine/node or does this refer to the maximum memory limit allowed for **all combined** worker/machine/node?

I have already looked through the documentation in Dask-ML (see here: https://docs.dask.org/en/latest/setup/single-distributed.html), but the documentation is not clear in regards to these questions above.

Thank you in advance if anyone could explain this?",1,scikit_learn,2020-10-13
glb4cy,Why does PolynomialFeatures has multiple pair of coefficient after fitted the data?,"After I create an PolynomialFeatures object, and fit the data by :

[`poly.fit`](https://poly.fit)`(x,)`

I wanted to look for the coefficient, so I do:

`poly.transform(x,y)`

&amp;#x200B;

And it will return an array with (n\_samples, n\_coeff), but why does the polynomial fit with multiple pair of coefficient? Wouldn't the model fit the data and get a final best coefficient?

&amp;#x200B;

And what is the final coefficient that Polynomial get after fitting?",1,scikit_learn,2020-10-13
gi4jw3,How to add sample_weight into a scikit-learn estimator,"I have recently developed a scikit-learn estimator (a classifier) and I am now wanting to add sample_weight to the estimator. The reason is so I could apply boosting (ie. Adaboost) to the estimator (as Adaboost requires sample_weight to be present in the estimator).

I had a look at a few different scikit-learn estimators such as linear regression, logistic regression and SVM, but they all seem to have a different way of adding sample_weight into their estimators and it's not very clear to me:

Linear regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_base.py#L375

Logistic regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_logistic.py#L1459

SVM: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/svm/_base.py#L796

So I am confused now and wanting to know how do I add sample_weight into my estimator? Is there a standard way of doing this in scikit-learn or it just depends on the estimator? Any templates or any examples would really be appreciated. Many thanks in advance.",2,scikit_learn,2020-10-13
get2kh,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,,2,scikit_learn,2020-10-13
gdb7a8,Code a Decision Tree in 20 lines.,,0,scikit_learn,2020-10-13
gd81x4,why does Scikit Learn's Power Transform always transform the data to zero standard deviation?,"all of my input features are positive. Whenever I tried to apply PowerTransformer with box-cox method, the lambdas are s.t. the transformed values have zero variance. i.e. the features become constants

&amp;#x200B;

I even tried with randomly generated log normal data and it still transform the data into zero variance.

&amp;#x200B;

I do understand that mathematically, finding the lambda s.t. the standard deviation is the smallest, would mean the distribution would be the most normal-like.

&amp;#x200B;

But when the standard deviation is zero, then what's the point of using it?

&amp;#x200B;

&amp;#x200B;

p.s. so one of the values of lambda I get by using PowerTranformer is -4.78 

If you apply it into the box-cox equation for lambda != 0.0, then for any input feature y values, you technically get the same values. i.e. (100\^(-4.78)-1.0)/(-4.78) is technically equals to (500\^(-4.78)-1.0)/(-4.78)",2,scikit_learn,2020-10-13
gcvtsm,how to combine recursive feature elimination and grid/random search inside one CV loop?,"I've seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:

[Feature selection and cross-validation](https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751)

[Nested cross-validation and feature selection: when to perform the feature selection?](https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec)

[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)

&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...

[Here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.

However I'd like to do recursive feature elimination inside random/grid CV, so that ""feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)"", so that data from other folds influence neither feature selection nor hyperparameter optimization.

Is this possible natively with sklearn methods and/or pipelines? Basically, I'm trying to find an sklearn native way to do this before I go code it from scratch.",3,scikit_learn,2020-10-13
gc2z28,How to write a scikit-learn estimator in PyTorch,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",3,scikit_learn,2020-10-13
g5ugws,Code a Neural Network in 20 lines.,,2,scikit_learn,2020-10-13
g4yc73,Basic question re: gaussian mixture models,"I wasn't able to find this in the documentation, but is the covariance parameter you access with model.covariances\_ sigma or sigma\^2? Seems like it can be either thing as I've seen the notations N(x| mu, sigma\^2) and N(x|mu, sigma) both used in various places.",1,scikit_learn,2020-10-13
fzm7mm,"Should scikit-learn include an ""Estimated Time to Arrival"" (ETA) feature? Discuss.",,7,scikit_learn,2020-10-13
fx6kdy,Clustering of t-SNE,"Hello,

I have recently tried out t-SNE on the sklearn.datasets.load_digits dataset. Then i applied KNeighborClassifier to it via a GridSearchCV with cv=5.

In the test set (20% of the overall dataset) i get a accuracy of 99%

I dont think i overfitted or smth. t-SNE delivers awesome clusters. Is it common to use them both for classifying? Because the results are really great. I will try to perform it on more data. 

I am just curious on what you (probably much more experienced users than me) think.",1,scikit_learn,2020-10-13
fx0i7x,Search over preprocessing and ensemble hyperparameters?,"In scikit-learn there are some handy tools like `GridSearchCV` for tuning the hyperparameters to a model or pipeline.

Suppose you'd like the preprocessing in your pipeline to include some user-defined options (e.g. whether to encode a certain categorical variable via one-hot encoding or something weird like frequency encoding) and you'd like to include those options among the hyperparameters you're searching over.

Suppose further that you're using an ensemble model -- e.g. a random forest plus few linear regression specifications, and you'd like to tune the hyperparameters for each of them, as well as the voting weight of each.

Does scikit-learn provide a predefined way to search over such spaces? It looks like the parameter space is intended only to dictate the behavior of a single model, not preprocessing steps or ensemble parameters.",1,scikit_learn,2020-10-13
ft2pcp,"How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?","How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?",1,scikit_learn,2020-10-13
ft1kmb,facing an error,"import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

&amp;#x200B;

\# Importing the dataset

dataset = pd.read\_csv('50\_Startups.csv')

X = dataset.iloc\[:, :-1\].values

y = dataset.iloc\[:, 4\].values

X2=dataset.iloc\[:, 3\].values

\# Encoding categorical data

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

le = LabelEncoder()

X2 = le.fit\_transform(X2)

oh = OneHotEncoder(categories = 'X\[:, 3\]')

X= oh.fit\_transform(X).toarray()

&amp;#x200B;

https://preview.redd.it/7bgiwdp238q41.png?width=871&amp;format=png&amp;auto=webp&amp;s=d2386c5cda100706a85803c036586beec8b9e843",1,scikit_learn,2020-10-13
fm1oov,I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?,"I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?

    preprocess =     make_column_transformer((SimpleImputer(strategy='median'), cols_numeric),     
    (SimpleImputer(strategy='constant', fill_value='missing'), cols_onehot),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_target),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_ordinal),     (OneHotEncoder(handle_unknown='ignore'), cols_onehot),     
    (TargetEncoder(), cols_target),     
    (OrdinalEncoder(), cols_ordinal),     
    (StandardScaler(), cols_numeric)) 
    lr_wpipe = make_pipeline(preprocess, LinearRegression()) 
    lr_scores = cross_val_score(lr_wpipe, X_train, y_train) 
    np.mean(lr_scores) 
    print(""Linear Regression R^2: "", lr_scores)",1,scikit_learn,2020-10-13
flg6a1,how to find 'the math' being done in sklearn source code?,"hi.  I'm trying to find where in sklearn the actual math is being done, mostly for my own learning so I can answer questions like 'when using `sklearn.neighbors` , what math is being used to calculate Euclidean distance?'    


If you see here: [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/\_base.py#L360](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/_base.py#L360)  
you'll see that Euclidian and other distance calculations can be *specified*, but I don't see anywhere where the actual math is being done in code.",1,scikit_learn,2020-10-13
fl4fj1,Adding Standard Scaler to GridSearchCV,"I'm looking to use the Standard Scaler as a hyper parameter, i.e check if performance is higher with/without scaling the inputs. In order to tune with other hyperparameters, I would like to incorporate it into my GridSearchCV function (provided by Scikit Learn). Can someone advise me on how to do it?",1,scikit_learn,2020-10-13
ffx9zw,How to use tfidfvectorizer fit_transform for multiple docs,"Hey, 

Let's say my corpus is a list of lists , each of the inner lists represent a parsed doc (each value is a word) 

I want to compute a tf-idf score for my corpus. 

It's seems like the fit-transform function can't use my corpus as its inputs should be itratable with string values (which is each of my docs) 


    V = tfidfvectorizer ()
    For doc in corpus:
       Vectors = v.fit_trabsform(doc)

So my question is, how does it calculate IDF if it get only one doc at a time?",1,scikit_learn,2020-10-13
ff9602,Classifiers' score method clarification,"Hi,

I don't fully understand what the score method of classifiers does. For example, the [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method's documentation says ""Return the mean accuracy on the given test data and labels."" Now, I know what is accuracy: (TP+TN)/(TP+TN+FP+FN), but I don't understand why ""mean"" is in there. Mean over what? of what?  

That is, I give the method as parameters a dataset with true labels, and it can calculate the accuracy from that (given the model), but where does the mean come into place? 

Thanks in advance!",3,scikit_learn,2020-10-13
fbfky4,Is epsilon in dbscan a euclidean measure?,"Hello everyone,
I'm writing yet another dbscan question. For those who are familiar with the inputs to the dbscan, the principal parameters are epsilon and minPts.

Epsilon is the neighborhood radius, and I'm curious if anyone can point me to a reference or tell me if epsilon is a euclidean metric",3,scikit_learn,2020-10-13
f1dizs,Identifying smallest frequently occurring value,"I'm not a data science person, but thinking Scikit learn might be able to help here, and looking for suggestions for ideas I should investigate.

Essentially, I'm looking for a way to consistently identify a baseline power readings. If I have minute by minute power consumption readings from a bunch of electrical motors.  For any motor, we want to identify what a 'baseline' or 'normal unloaded steady-state' power value is.

There is definitely noise in the signal, and not even noise - legitimate power reading that are smaller than what we would consider 'normal unloaded steady state'.  The catch is this could be different for the same motor when production composition changes, so there is not just one value that we can look at historical data to arrive at. (Think motors running pumps moving different fluid mixtures / slurry ad different times.

This does not have to be real-time, just take the dataset of power readings for any motor for any production batch and post-process the data in such a way we can identify times the motor is doing its job at a 'near-idle' state.

Currently we just have a basic calculation that looks at a rolling window of 20 per-minute readings and finds the lowest value that occurs at least twice. (basically throwing out the lowest few outliers)

The reason I'm considering Scikit or similar is we can graph these power readings for a time period (say 1 day) and visually we can easily see these 'baselines' we are looking for.  There will be spikes and dips, and time windows where we are definitely running a heavy load (motors spun up on demand), but we can identify when the mixture changes because the visual changes in this baseline value.

Hope that made at least a little sense, if there are details I can clarify, please ask.  I appreciate everyone's thoughts and ideas!",1,scikit_learn,2020-10-13
eylu4p,What's wrong with Scikit-Learn.,,3,scikit_learn,2020-10-13
ev1as7,Is it possible to use a custom-defined decision tree classifier in Scikit-learn?,"I have a predefined decision tree, which I built from knowledge-based splits, that I want to use to make predictions. I could try to implement a decision tree classifier from scratch, but then I would not be able to use build in Scikit functions like predict. Is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? Or do I need to do something completely different? My first attempt was to use “fake training data” to force the algorithm to build the tree the way I like it, this would end up in a lot of work because I need to create different trees depending on the user input.",1,scikit_learn,2020-10-13
eu9qgc,Is HistGradientBoosting the same as LightGBM or is the SKLearn's version different?,"If so, how?",2,scikit_learn,2020-10-13
em9fxf,Is this the proper way to do ML with scikit_learn?,"I have a dataset with 8 features (numeric) and 1 target (0 or 1).  
 I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc.  
 For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

But sometimes I get these errors:  
 The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

What's the proper way to predict binary values?  
 I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Thanks for your support. I'm a beginner.",2,scikit_learn,2020-10-13
effb98,What module/algorithm should I use in order to predict the time in which a certain action will be completed?,"Basically the title, but to explain it even more :

I have to device a model which will predict the total time a patient will have to wait in a hospital environment. For that, we have a dataset consisting of various patients with several diseases and their time durations already recorded. I want to know which module or algorithm should I use to carry this out? This is my first ML project and I could use any help that you guys can do. Thank you!!",2,scikit_learn,2020-10-13
e0h8ao,Column transformer throwing away some features?,"My data frame  has numerical columns a, b, c. And it has categorial text features d, e, f, g, h.

I build a preprocessor like the following.

    num_features = ['a','b','c']
    nom_features = ['d','e','f','g','h']
    
    preprocesser = ColumnTransformer([
        (""scale_numeric"", StandardScaler, num_features),
        (""encode_nominal"", OneHotEncoder(handle_unknown=""ignore""), nom_features)],
        remainder=""drop""
    )
    
    preprocessor.fit_transform(dataframe)

Since I started with 8 features, after one hot encoding I expected to get 8 or more features back, but the result of `preprocessor.fit_transform(dataframe)` only has 3 columns. Not sure what I am doing wrong if anyone can help me.",1,scikit_learn,2020-10-13
dyzwn9,How to Modify(Make unique) the Scikit-learn Multilayer perseptron algorithm (MLP),"Hi folks,

I've been trying to build a rainfall prediction model for last few days. I've used the **Scikit-learn Multilayer perseptron regressor function** straight up. 

1) The accuracy was OK(78%) but I want to increase it

2) I don't want to use the same predominantly given function (I just want to **add uniqueness in my code**, but I want to use scikit-learn)

Is there any way to modify the function or not use the ready-made function? Can anyone please help me with this?

Thanks in advance!",1,scikit_learn,2020-10-13
dtnh3f,The best alpha for ridge regression is... -85???,,3,scikit_learn,2020-10-13
dtiy98,difference between Kfold.split() and shufflesplit.split() in scikitlearn,"I read this [post](https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv), I get the difference when it comes to computation and shufflesplit randomly sampling the dataset when it creates the testing and training subsets, but in the answer on stackoverflow, there is this paragraph  


""**Difference when doing validation**

In KFold, during each round you will use one fold as the test set and *all* the remaining folds as your training set. However, in ShuffleSplit, during each round **n**  you should *only* use the training and test set from iteration **n** ""

I couldn't quite get it. since in kfold, you're bounded by using the training buckets (k-1) and testing bucket (k) in the **k** iteration and in shufflesplit you use the training and testing subsets made by the shufflesplit object in iteration **n.**  so for me it feels like he's saying the same thing.

can anyone please point out the difference for me?",1,scikit_learn,2020-10-13
dcvd2r,When to use these unsupervised algorithms?,"There are a lot of modules in sklearn. I am interested when these unsupervised algorithmes (bellow )are used.  
When to use a Gaussian mixture model? When to use Manifold Learning, When to Biclustering? etc.

&amp;#x200B;

* [2.1. Gaussian mixture models](https://scikit-learn.org/stable/modules/mixture.html) 
* [2.2. Manifold learning](https://scikit-learn.org/stable/modules/manifold.html)
* [2.4. Biclustering](https://scikit-learn.org/stable/modules/biclustering.html) 
* [2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html) 
* [2.6. Covariance estimation](https://scikit-learn.org/stable/modules/covariance.html) 
* [2.7. Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html) 
* [2.8. Density Estimation](https://scikit-learn.org/stable/modules/density.html)",6,scikit_learn,2020-10-13
dar9z6,pattern recognition on texts that are bash commands or software signature?,"hi all.

so I've got my hands on a daily dose of 100,000 connections per day to our servers, and I've got millions of rows of data that includes commands our users have executed on our servers, (\`cd\`, \`ehlo\`, \`scp ....\`, etc). and I have the same amount of data of their application signatures while connecting. like (Firefox 59, Firefox 60, google chrome),... and user agents, ...

basically all the data one can extract out of a socket or using an IDS.

I like to do some pattern matching on these data. like for the commands they are executing and stuff like that...

so to cluster the commands, I've got commands that look like this:

cd Project

cd Images/personal

cd Project/map

cat /var/log/nginx/web\_ui.log

the problem is, I can just split the texts and take in the first part(cd, cat) and make plot out of the commands, but i really would like to make it more automatic and intelligent. so people who \`cd\` into the \`Project/map\` are distinguished from people who cd into \`Images\` folder. I like to know what people are doing on out servers. so a plot that all people whith \`cd\` commands are close to each other, but are really distinguished for each folder that they have \`cd\` into. 

this is just an example of what I want:)

&amp;#x200B;

turns out that scikit\_learn only works on numbers? how can i utilize it for that kind of data? I don't know if this is a nltk problem?",3,scikit_learn,2020-10-13
d8sxus,Exporting Models to build own inference server,"Hello, I was hoping to get pointed in the right direction.  After training a random forest classifier I am looking to export the model in such a way that I can recreate each of the trees in C++.  I am trying to figure out the best approach to this, or if it is even possible.  My research online mainly shows examples of how to visually represent these, and how to create a pickle project for python serialization.  

Am I missing some key terms in my search? Could you point me to what i should be doing to figure this out?

&amp;#x200B;

My approach so far has been exploring the clf.estimators.trees\_ part of the estimators object, but I am not sure if I am on the right track.

&amp;#x200B;

Any help is much appreciated.

&amp;#x200B;

Thanks!",1,scikit_learn,2020-10-13
d244x4,Predict device from flow,"Hey guys, I applied to a competition about AI and my task is to predict device class from flow. I have 13 types of classes which are all in train set but the test set is missing that one column. After I run training and then I try to predict it, I receive an error stating this: ValueError: query data dimension must match training data dimension.

How can I predict a column that is not there? I don't believe that I have to manually put the column to the test.json 

Thanks for advices.",5,scikit_learn,2020-10-13
cvtjk6,Predicting Churn With Nested Data,"Hello All!

Ok, so this is a bit of a challenge and I'm trying to figure out if it is even worth worrying about the nesting aspect of the data. Basically, I'm trying to predict subscription-level churn with a combination of subscription-level and user-level variables.

Since users own subscriptions I figured I should try to account for nesting in my model. Does anyone have any recommendations on how to attack churn predictions using a nested model? Any suggestions would be greatly appreciated. Again, I have code working, but I've never built anything that requires nested analysis.

Basically my question is: Is it possible to run a multi-level SVM?",2,scikit_learn,2020-10-13
cs2urp,What is the most efficient way to implement two-hot encoding using scikit learn?,"I have two very similar features in my dataframe, and I would like to combine their one-hot encoded versions. They are both categorical data, and they both contain the same categories. I was thinking about using OneHotEncoder from scikit learn and getting the union of the corresponding columns. Is there a function or more efficient way that I do not know about?",3,scikit_learn,2020-10-13
cnnacy,Feature elimination doesn't really eliminate anything.,"I had a fairly simple dataset, after plotting the correlation matrix I noticed that one variable has very low correlation with the target (0.04) but instead of deleting it manually I decided to try feature elimination.
I tried both RFE and RFECV with Logistic Regression as an estimator, RFE eliminated some features which seemed correlated with the output and kept that feature.
RFECV didn't eliminate anything at all.

Am I missing something here?",1,scikit_learn,2020-10-13
cnl2a5,k-means output issue,"Hello I've run a k-means over my voice data. I got two class (for best). My problem is why i got this line at the right side? I sit an issue in my dataset?

https://preview.redd.it/n5lz9pggx7f31.png?width=852&amp;format=png&amp;auto=webp&amp;s=e84cc13f9579c026fc6bfa5cbd85849b1ea2939e",1,scikit_learn,2020-10-13
cmmbi5,Running scikit validation on 24 cores is slow?,"Hello guys, maybe anyone can help me out here. I am running following validation code:

```
from sklearn.linear_model import LinearRegression
model = LinearRegression()
from sklearn.preprocessing import PolynomialFeatures
poly_transformer = PolynomialFeatures(degree=2, include_bias=False)
from sklearn.pipeline import Pipeline
pipeline = Pipeline([('poly', poly_transformer), ('reg', model)])
train_scores, valid_scores = validation_curve(estimator=pipeline, # estimator (pipeline) X=features, # features matrix y=target, # target vector param_name='pca__n_components', param_range=range(1,50), # test these k-values cv=5, # 5-fold cross-validation scoring='neg_mean_absolute_error') # use negative validation
```

in the same .py file on different machines, which I would name #1 localhost, #2 staging, #3 live, #4 live. localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds
live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.
In order to get more ""trustworthy"" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different?",1,scikit_learn,2020-10-13
clz2bl,vectorization,"Hi, I just want to know if I can vectorize a text even if its on another language using Count Vectorization",2,scikit_learn,2020-10-13
clpubv,Machine learning final year project,"design and implement an intelligent agent that can detect a fault and can trouble a faulty server on a network

Its a network anormaly project 
But dont know where to start from",1,scikit_learn,2020-10-13
cl88rf,No Scikit-learn after I installed Anaconda in Sublime Text 3," 

I started using Sublime Text as my Text Editor/IDE (not sure what the difference is) to do some Python projects. After watching 2 episodes of the machine learning course by Google Developers. I installed the Anaconda package which has the Scikit-learn included.

Following the video I typed:

    import sklearn

This error appeared:

    ModuleNotFoundError: No module named 'sklearn'

Is there a way to install the Scikit-learn using the Sublime Text 3 or using a different method?",1,scikit_learn,2020-10-13
cgijm0,Unable to find/import,"edit: Title - Unable to find/import IterativeImputer

&amp;#x200B;

&amp;#x200B;

Hello fellow users, I'm wondering if yall could help me out with importing/finding IterativeImputer...

**&gt;&gt;&gt;** *# explicitly require this experimental feature*

**&gt;&gt;&gt; from** **sklearn.experimental** **import** enable\_iterative\_imputer *# noqa*

**&gt;&gt;&gt;** *# now you can import normally from impute*

**&gt;&gt;&gt; from** **sklearn.impute** **import** IterativeImputer

**ModuleNotFoundError**: No module named 'sklearn.impute.\_iterative'; 'sklearn.impute' is not a package

&amp;#x200B;

$pip freeze states I have scikit-learn==0.21.2 and sklearn==0.0

Python version 3.6

&amp;#x200B;

After researching the issue online I see that there's an experimental version I need to install, but I can't seem to find it! Further, I can't find it on their website.. [https://scikit-learn.org/dev/versions.html](https://scikit-learn.org/dev/versions.html)

What did I overlook/miss?",1,scikit_learn,2020-10-13
cc2mxr,How to re-structure a numpy dataframe into a format I can use in sklearn?,"Assuming the dataframe column 0 is the target and columns 1: are the features, and that each column is named, what's the easiest way to split the data for use in sklearn?",1,scikit_learn,2020-10-13
cbm4g6,How to classify dots,"Hello, 

I have a graph with two groups, red and blue dots. These groups are clearly separated, but the problem is that I want to say if a new dot belongs to the red group, to the blue, or to none of them.

What method do you recommend?

Thank you",1,scikit_learn,2020-10-13
c4rlf7,Regression is not yielding many useful predictions,"Hello all,   


I'm using a linear regression to predict continuous values (how long until a client churns measured in months). I have a dataset of cancelled accounts and active account. I'm using the cancelled accounts to predict the active accounts. I have a variety of explanatory variables and in total I have an R-squared of around 35% (obviously R-squared isn't perfect)   


Overall, this works pretty well; however, one issue I'm running into is that, of the predictions I get back, some are negative and very few actually predict that these active clients should still be active. In other words, many of the predicted cancellation dates are in the past.  


Dumb question, but is there a method I could be using to help this? Overall, I'm getting about 10k useful observations from 60k predictions.  Any suggestions would be greatly appreciated.",1,scikit_learn,2020-10-13
c4q5i6,I can't import Kmeans into compiler,"I'm currently using sklearn 0.21.2, and when I do:  


`import sklearn.cluster.KMeans`

&amp;#x200B;

the compiler returns error:

&amp;#x200B;

`no module named sklearn.cluster.KMeans`

&amp;#x200B;

I've found that in the cluster package, there is an module named 'cluster.k\_means\_'  


But when I tried to use this instead, it shows error

&amp;#x200B;

`Module is not callable`

&amp;#x200B;

Now I don't know why I can't import the kmeans package in cluster.",1,scikit_learn,2020-10-13
bylpjd,Sklearn regression with two datasets,"Hello all,   


basically, as the title implies I'm trying to train a regression model on one dataset and the apply that predictive model to another dataset. In other words, I have a model which predicts cancelled accounts and the amount of time in which those accounts cancel.   


I have another dataset full of active accounts (with the same variables) and I'm attempting to use the model from the cancelled accounts to predict when my active accounts will cancel. I'm having trouble with this.  Is there a way to do this without forcing a t  


Is there a way to use the ""active dataset"" without enforcing a Train\_test\_split? Any help would be greatly appreciated. Thank you!",2,scikit_learn,2020-10-13
bvjxzj,Get the function that fits my data,"I have fit a polynomial regressor to a two dimensional data. 
Is there a way to see the function that fits this data?",2,scikit_learn,2020-10-13
bqtxes,Kmeans clustering cache the result,"Hello,

&amp;#x200B;

I am new to scikit and I was wondering if I could cache the result of Kmeans so next time when I run my script I do not create the centroids again - that means save the result of [`kmeans.fit`](https://kmeans.fit)`()`.",2,scikit_learn,2020-10-13
bp7dv5,Get classes name of each estimator in OneVsOneClassifier,"Are there any ways to do that ? I am trying to directly access the classes\_ attributes in the estimator but it only returning \[0,1\]",2,scikit_learn,2020-10-13
bevq9d,Using Blob Detection methods on huge images,"I'm trying to use common blob detection methods from

[https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob\_dog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog)

on a huge images (about 6000x6000 pixels). It takes way too long to compute and show the result. How could I resolve this?",1,scikit_learn,2020-10-13
bcwbv4,Calculate variance of accuracy,"Hello, how can I calculate the variance of accuracy between two models in Random forest.
I mean I made a simple model with DecisionTreeClassifier() and one more with BagginClassofier() using the first model on it.
The accuracy climb +0.237.

How to get variance of that accuracy?
Thansk",1,scikit_learn,2020-10-13
bcbduy,Classification: Minimizing the amount of false positives,"Hey there,

I posted an earlier post (now deleted) that phrased this a bit wrong (thanks Imericle). Here is another try: 

Many  (most?) classification algorithm seem to be about maximizing accuracy  (true positives + negatives). My aim is to minimize the amount of false positives. How would I achieve this?

Only options I see to achieve this is through parameters tuning, is that the right approach?

(Thinking on applying it to a RandomForest),

Thanks,

Bb",2,scikit_learn,2020-10-13
bbxi9t,KMeans: Extracting the parameters/rules that fill up the clusters,"Hi all,

&amp;#x200B;

I have created a 4-cluster k-means customer segmentation in scikit learn. The idea is that every month, the business gets an overview of the shifts in size of our customers in each cluster. 

My question is how to make these clusters 'durable'. If I rerun my script with updated data, the 'boundaries' of the clusters may slightly shift, but I want to keep the old clusters (even though they fit the data slightly worse). My guess is that there should be a way to extract the paramaters that decides which case goes to their respective cluster, but I haven't found the solution yet. 

I would appreciate any help",1,scikit_learn,2020-10-13
b6nfvs,Question about FeatureUnion,"    pipe = Pipeline([
            ('features', FeatureUnion([
                    ('feature_one', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_one')),
                        ('vec', cvec) # Count vectorizer
                    ])),
                    ('feature_two', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_two')),
                        ('vec', tfidf) # Tf-idf vectorizer
                    ]))
                ])),
            ('clf', OneVsRestClassifier(clf)) #clf is a support vector machine
        ])

I'm using this pipeline for a project I'm working on, and I just want to make sure I understand how FeatureUnion works. I'm building a classifier which takes in two different text features and attempts to make a multi-class classification.

&amp;#x200B;

To give a little more detail, I'm trying to classify news articles into one of several categories (sports, business, etc.) Feature one is a list of tokens taken from the article's url, which often, though not always, explicitly states the name of the topic. Feature two is a list of tokens from the body of the article.

&amp;#x200B;

Does it make sense to separate the two features this way? Does this have a different effect than if I had just merged all of the tokens into a single list and vectorized them? My intention was to allow the two features to effect the model to different degrees, since I figured one would be more predictive in most scenarios (and I am getting pretty great results.)",2,scikit_learn,2020-10-13
b36h5a,Ranforest random behaviour,"If I give random forest parameters as RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019) Should it be creating 10 same decision trees? But it is not. I am asking the random forest to
     1.Sample without replacement (bootstrap=False) and each tree have same number of sample (ie the total data )(verified using plot)
     2.Select all features in all trees.
But model.estimators_[2] and model.estimators_[5] are different

",2,scikit_learn,2020-10-13
axgj2c,Predicting the runtime of scikit-learn algorithms,"Hey guys,

We're two friend who met in college and learned Python together, we co-created a package which can provide an estimate for the training time of scikit-learn algorithms.

The main function in this package is called “time”. Given a matrix vector X, the estimated vector Y along with the Scikit Learn model of your choice, time will output both the estimated time and its confidence interval. 

Let’s say you wanted to train a kmeans clustering for example, given an input matrix X. Here’s how you would compute the runtime estimate:

    From sklearn.clusters import KMeans
    from scitime import Estimator 
    kmeans = KMeans()
    estimator = Estimator(verbose=3) 
    # Run the estimation
    estimation, lower_bound, upper_bound = estimator.time(kmeans, X)

We are able to predict the runtime to fit by using our own estimator, we call it meta algorithm (meta\_algo), whose weights are stored in a dedicated pickle file in the package metadata.

The meta algos estimate the time to fit using a set of ‘meta’ features, including the parameters of the algo itself (in this case kmeans) and also external parameters such as cpu, memory or number of rows/columns. 

We built these meta algos by generating the data ourselves using a combination of computers and VM hardwares to simulate what the training time would be on the different systems, circling through different values of the parameters of the algo and dataset sizes . 

Check it out! https://github.com/nathan-toubiana/scitime

Any feedback is greatly appreciated.",5,scikit_learn,2020-10-13
aahf76,"Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?","Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?

&amp;#x200B;

So in the sense that if one applies e.g. a gain factor  (or a function depicting gain changes), then it's applied to the correct direction.",3,scikit_learn,2020-10-13
a75oid,"classification_report + MLPClassifier(): UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn_for)","classification\_report on a prediction done on MLPClassifier() sometimes throws:

&amp;#x200B;

*UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn\_for)*

&amp;#x200B;

but not on all the time.

&amp;#x200B;

What could be wrong?

&amp;#x200B;

\---

&amp;#x200B;

Doing

&amp;#x200B;

set(y\_test) - set(y\_pred) 

&amp;#x200B;

I'm able to see that sometimes some label is missing from y\_pred. But why does this occur only occasionally?

Is something wrong with how I use MLP?",1,scikit_learn,2020-10-13
a753f2,"{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification_report()","{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification\_report()

&amp;#x200B;

Why?

&amp;#x200B;

I call it like:

&amp;#x200B;

    classification_report(y_test, y_pred)

where y\_pred is predicted using a model I built.

&amp;#x200B;

""Quite obviously"" the arguments are incompatible somehow, but how can I find out, how? And how can I make them compatible?

&amp;#x200B;

\---

&amp;#x200B;

I tried:

&amp;#x200B;

    from sklearn.utils.multiclass import type_of_target

&amp;#x200B;

    &gt;&gt;&gt; type_of_target(y_test)
    'multiclass'
    
    &gt;&gt;&gt; type_of_target(y_pred)
    'continuous'

&amp;#x200B;",1,scikit_learn,2020-10-13
a746h0,"Is there any way to ""estimate"" how long a given computation in sklearn will take?","Is there any way to ""estimate"" how long a given computation in sklearn will take?

&amp;#x200B;

So that one doesn't need to wait longer than what one can?

&amp;#x200B;

Also, since Windows Task Manager shows only modest CPU use (&lt; 10%), then how is one supposed to know, what's occurring in the model?",2,scikit_learn,2020-10-13
a73oda,What are the most important parameters in LogisticRegression()?,What are the most important parameters in LogisticRegression()?,3,scikit_learn,2020-10-13
a71xf2,How does one feed hidden_layer_size tuples into GridSearchCV's param_grid?,How does one feed hidden\_layer\_size tuples into GridSearchCV's param\_grid?,1,scikit_learn,2020-10-13
a0yprf,Code review,"Hello,

I'm new to ML and scikit - hope this is the correct place for this. Have created the below code that appears to be working but wanted to get the opinions of people with more experience then me, to check I haven't a made any major errors or if there are any obvious improvements?

&amp;#x200B;

I am trying to train a model on a data set of potentially hundred of thousands emails. Every few days I want to retrain the exported model using incremental learning on the new emails received since the model was last trained.

The below reads the initial data from a csv, runs HashingVectorizer then SGDClassifier. The OnlinePipeline is used to allow me to use partial\_fit when I try to retrain later in the process.

`import pandas as pd`

`data = pd.read_csv('customData1.csv')`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`from sklearn.feature_extraction.text import HashingVectorizer`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

&amp;#x200B;

`from sklearn.linear_model import SGDClassifier`

`text_clf = OnlinePipeline([('vect', HashingVectorizer()),`

`('clf-svm', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=5, random_state=None)),`

`])`

`text_clf = text_clf.fit(X_train,Y_train)`

`predicted = text_clf.predict(X_test)`

`np.mean(predicted == Y_test)`

The above gives me an accuracy of 0.55

&amp;#x200B;

A few days later when I have new emails I import the previously exported model and use partial\_fit on a new csv file.

`import pandas as pd`

`data = pd.read_csv('customData2.csv') #text in column 1, classifier in column 2.`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

&amp;#x200B;

`from sklearn.externals import joblib`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

`text_clf2 = joblib.load('text_clf.joblib')`

&amp;#x200B;

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`text_clf2 = text_clf2.partial_fit(X_train,Y_train)`

&amp;#x200B;

`predicted = text_clf2.predict(X_test)`

`np.mean(predicted == Y_test)`

This returns the improved accuracy of: 0.84

&amp;#x200B;

Sorry for so much code!  I obviously need to tidy it all up so its a single method and handle the import/export logic properly.

&amp;#x200B;

Have a made any major errors or are there any obvious improvements? Thanks!

&amp;#x200B;",1,scikit_learn,2020-10-13
a0iz4i,Does cross_val_score tell something about generalizability?,"Does cross\_val\_score tell something about generalizability?

&amp;#x200B;

Or do I need to use something else for measuring generalizability?",0,scikit_learn,2020-10-13
a0c8dw,"Is there a problem if MLPRegressor doesn't converge for max_iter=100, but nor max_iter=5000 either?","Is there a problem if MLPRegressor doesn't converge for max\_iter=100, but nor max\_iter=5000 either?

&amp;#x200B;

Anything else I could try?",1,scikit_learn,2020-10-13
a0b260,What do cv (number of folds) and the number of outputs in cross_val_score correspond to?,"What do cv (number of folds) and the number of outputs in cross\_val\_score correspond to?

&amp;#x200B;

Does it mean that it produces cv number of different scores? Or (as I read somewhere) that only the last score might be the meaningful one (I read something like the others than the last used to ""fit"", while the last is the score)?",2,scikit_learn,2020-10-13
a0awuh,"Getting values in range [-191806. ..., 0.77642 ...] from cross_val_score, am I doing something wrong?","Getting values in range \[-191806. ..., 0.77642 ...\] from cross\_val\_score, am I doing something wrong?

&amp;#x200B;

    mlp = MLPRegressor(hidden_layer_sizes=(7,))

mlp.fit(X\_train,y\_train) mlp\_y\_pred = mlp.predict(X\_test)

&amp;#x200B;

y\_pred is an earlier prediction using LinearRegression().

&amp;#x200B;

I call cross\_val\_score like:

&amp;#x200B;

    cross_val_score(mlp, y_pred, mlp_y_pred, cv=10)

&amp;#x200B;

Output is:

&amp;#x200B;

    00 = {float64} -4.4409160725075605
    01 = {float64} -673636.0674512024
    02 = {float64} -51282.162171235206
    03 = {float64} -399557.4789466267
    04 = {float64} -35.73093353875776
    05 = {float64} -1406.9741325253574
    06 = {float64} -80853.84044929259
    07 = {float64} -5132.870883709122
    08 = {float64} -283.7432365432288
    09 = {float64} -2.860321933844385

&amp;#x200B;

I think I should be getting values in range \[0,1\].",1,scikit_learn,2020-10-13
a0at1q,"Is MLPRegressor's hidden_layer_sizes=(7,) equivalent to hidden_layer_sizes=7?","Is MLPRegressor's hidden\_layer\_sizes=(7,) equivalent to hidden\_layer\_sizes=7?",1,scikit_learn,2020-10-13
a09ukl,"Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train_test_split(Xy,shuffle = False, test_size = 0.33)?","Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train\_test\_split(Xy,shuffle = False, test\_size = 0.33)?

Xy has been constructed like:

&amp;#x200B;

    X = dat.data
    y = dat.target 
    Xy = np.hstack((X,np.array([y]).T))

It seems that it returns only two arrays, even when I saw an example ([https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input](https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input)) do 

 

    X_train, X_test, y_train, y_test = train_test_split(Xy,shuffle = False, test_size = 0.33) ",1,scikit_learn,2020-10-13
a06iin,Runtime Error in RandomizedSearchCV,"I've been running a RandomForestClassifier on a dataset I took from UCI repository, which was taken from a research paper. My accuracy is \~70% compared to the paper's 99% (they used Random Forrest with WEKA), so I want to hypertune parameters in my scikit learn RF to get the same result (I already optimized feature dimensions and scaled). I use the following code to attempt this (random\_grid is simply some hard coded values for various parameters):

&amp;#x200B;

    rf = RandomForestClassifier()
    # Random search of parameters, using 2 fold cross validation,
    # search across 100 different combinations, and use all available cores
    rf_random = RandomizedSearchCV(estimator = rf,  param_distributions = random_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)
    # Fit the random search model
    rf_random.fit(x_train, x_test)

When I attempt to run this code though my python runs indefinitely (for at least 40 min before I killed it) without giving any results. I've tried reducing the \`cv\` and \`n\_iter\` as much as possible but this still doesn't help. I've looked everywhere to see if there's a mistake in my code but can't find anything. I'm running Python 3.6 on Spyder 3.1.2, on a crappy laptop with 8Gb RAM and i5 processor :P

&amp;#x200B;

Here is the random\_grid if it helps:

    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
    max_features = ['auto', 'sqrt']
    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
    max_depth.append(None)
    min_samples_split = [2, 5, 10]
    min_samples_leaf = [1, 2, 4]
    bootstrap = [True, False]
    
    # Create the random grid
    random_grid = {'n_estimators': n_estimators,
                   'max_features': max_features,
                   'max_depth': max_depth,
                   'min_samples_split': min_samples_split,
                   'min_samples_leaf': min_samples_leaf,
                   'bootstrap': bootstrap}

&amp;#x200B;",1,scikit_learn,2020-10-13
9yhcqq,Does sklearn have built-in routines for testing results of LinearRegression()?,Does sklearn have built-in routines for testing results of LinearRegression()?,1,scikit_learn,2020-10-13
9ygvsi,How does fit_transform allow for other data to be processed with the same transformer?,"How does fit\_transform allow for other data to be processed with the same transformer?

&amp;#x200B;

Like here:

&amp;#x200B;

[https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)

&amp;#x200B;

Particularly, since one first calls fit\_transform, then why does it allow one to call transform afterwards and still get the same fit? Like how is this kind of functionality implemented?

&amp;#x200B;

&amp;#x200B;",1,scikit_learn,2020-10-13
9tf8yw,Principal component Analysis: predicting values,"I am attempting to forecast a set of multivariate time series data. I have run a PCA (using the scikit-learn module) and have run an AR(1) auto-regression of the 3 components.

Now that I have the projects component values, how do I recast those components into the original variables, in order to find the projection for those variables?",2,scikit_learn,2020-10-13
9t3xwo,Extract a single stratified part of a dataset,"I have a multi-label dataset with N samples, and I want to take a chunk out to reserve for validation, e.g. reserve k% of the dataset.

Note that I want to do this just once, else I could use stratifiedKFold.  
Is there a function to produce such a single chunk, ensuring stratification with respect to  the labels?  
(A workaround would be to produce N\*k  KFold splits, concatenate  all parts but one  for training, and use the last for validation.)

Thanks.",1,scikit_learn,2020-10-13
9sbqvm,Stepping through each iteration of the LogisticRegression fit() function,"Hello guys,

I'm using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class to find the decision functions between my classes. I wanted to ask you - *how can I step through each step of the algorithm?* I know I can give the parameter `max_iter` to determine the number of iterations, but I want to step through each of those `max_iter` iterations - to see how the values of the coefficients change.

Thanks in advance!",1,scikit_learn,2020-10-13
9pcack,"modAL: A modular active learning framework for Python, built on top of scikit-learn",,5,scikit_learn,2020-10-13
8ndofu,Parse Twitter feed and suggest domain names • r/nltk,"I'm working on a hackathon, and I'd like to parse a user's last 100 tweets or so and make recommendations for a domain name using a new TLD.

The plan I've got in my head is

1\) Scrape twitter for a bit and get some data \(How much? How many records?\)

2\) Run tf\-idf against it, save that dataset

3\) split the initial twitter data into groups based on which tweets contain each TLD \- supplies, computer, kitchen, etc.

a\) Run some kind of clustering algorithm against each set? 250 or so TLDs

\-\- This is where I have questions

4\) Scrape their twitter feed and get 100 tweets

5\) Use the tf\-idf data from step 2 to spit out keywords

6\) use those keywords using some kind of distance formula against the clustered data to pick a tld?

7\) use the bigrams or keywords to make up an SLD.

This seemed off to a good start, but can I somehow pickle the cluster results? Or have multiple sets of cluster results in the same object?

Note: 95&amp;#37; of my knowledge on this topic comes from this blog post: [http://brandonrose.org/clustering](http://brandonrose.org/clustering)",2,scikit_learn,2020-10-13
8l4imo,"move partial of decision models from server to client - side, is it good idea?","Hi,
Some time ago tenser flow for js was released. I'm wondering about build bridge for some scikit learn models to move some part of learning and prediction to the client side. I think that it could help minor companies reduce server resource usage and make models and prediction much more personalised. Do you think it's a good idea? Do you know whether someone has tried something similar before?",1,scikit_learn,2020-10-13
8emc3b,How to combine num values with text data for classification?,I build website classifier and use text of each webpage (transformed to bag of words) as train data. But I also want to add each website's PageRank as feature. How can I do that?,2,scikit_learn,2020-10-13
89u8tj,PLSRegression Issues,"I'm working with scikit's cross\_decomposition.PLSRegression(). According to their documentation, x = np.multiply( x\_scores\_, x\_loadings\_.T ). I'm not getting anything close to the same value values. I've tried every combination of using scale=False and sklearn.preprocessing.scale(x) to try and find how this works out, but I haven't been able to find one that works.

    plsr = PLSRegression(n_components=x_df.shape[0]-1).fit(x_df.T,y)
    print(np.matmul(plsr.x_scores_,plsr.x_loadings_.T).T)
    print(x_df)

Using scaled data (i.e. PLSRegression.fit( scale(x\_df).T, scale(y) ) and changing n\_components doesn't help either. If anyone has any idea of what mistake I have made, or if this just a bug in sklearn?",1,scikit_learn,2020-10-13
81xksg,How to remove terms from a term-document matrix?,"Hello,

I have a term document matrix that I've created using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) like so:
    
    X = vectorizer.fit_transform(corpus)
    X
    &lt;1000x10022 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    	with 94340 stored elements in Compressed Sparse Row format&gt;

I'd now like to remove any terms that do not appear in at least 3 documents, and then calculate the TF-IDF scores for each term, and select the vocabulary as the top n terms ordered by TF-IDF scores.

Is there an easy way of removing terms from the term document matrix that do not appear in at least 3 documents, while still conserving the mapping from feature names to feature indices?

I guess one way to do it would be to get the feature names of the terms that appear in at least 3 documents using numpy on the sparse matrix directly, assign them a mapping to indices, and then pass that mapping to the vocabulary parameter in the CountVectorizer constructor.

Any ideas on how to do this more easily?",1,scikit_learn,2020-10-13
7zfjba,How to use partial_fit to update the model trained with fit() instead of training from scratch,"I tried partial_fit with various scikit online learning classifiers like perceptron, passive aggresive classifiers, SGDclassifer... like here: https://ideone.com/uOtRTZ.  I just dont understand why i cant train the new data on top of already trained data. I am doing image classification. I have trained my 10,000 images with fit(). Now i got 1 new image to add to this dataset of already trained images. I want to update the trained model instead of training all 10,001. Is this possible with partial_fit() ? If so, please tell me how ? ",1,scikit_learn,2020-10-13
7z4qi7,SGDClassifier.partial_fit returns error of “classes should include labels”,"I tried to predict label of my newly added data through SGDClassifer.partial_fit as below:

        from sklearn import neighbors, linear_model
    import numpy as np
    
    
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""hinge"")#loss
    
        sgd_clf.fit(X, y)
    
        print(sgd_clf.predict([[6, 9]]))
    
        X.append([6, 9])
        y.append(5)
    
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here

However, this results in error: ValueError: `classes=array([1, 2, 3, 4, 5])` is not the same as on last call to partial_fit, was: array([1, 2, 3, 4])

Any ideas or references ? ",1,scikit_learn,2020-10-13
7vw4ap,Retrain a KNN classified model (scikit),"I trianed my knn classifer over multiple images and saved the model. I am getting some new images to train. I dont want to retrain the already existing model.

How to add the newly tranied model to the existing saved model ?

Could someone guide if this is possible or any articles describing the same ?

Thank you,",2,scikit_learn,2020-10-13
70m5l1,How do I add matplotlib to a django webapp and display the code's output on the webpage?,Trying to make a User Interface for a Support Vector Machine from the SVM function in the matplotlib,1,scikit_learn,2020-10-13
6lec9n,"K-NN and custom metrics, speed up sklearn using Cython",,2,scikit_learn,2020-10-13
6k5uvu,Build my first CART based algorithm feedback is welcome!,hey guys! i just made this: https://github.com/lucas-aragno/pokemon-classifier im pretty new to scikit so I'll appreciate any kind of feedback :),2,scikit_learn,2020-10-13
6ev2y7,FastICA,"It seems like all of the examples using fastICA involves taking 2 frequencies, mixing them a certain way, then unmixing them.

What about if I have a wav file. How can I use fastICA to break it down into multiple parts?

Any help would be appreciated. Thank you!",1,scikit_learn,2020-10-13
6eu5cr,Automate your Machine Learning in Python – TPOT and Genetic Algorithms,,2,scikit_learn,2020-10-13
69huhq,[P] Tracking and reproducibility in data projects (CLI tool),,1,scikit_learn,2020-10-13
5xk0iy,Scikit learn vs Open Cv for small problems in image processing,I am a Image processing noob. I've used Numpy and Scipy for some matrix related stuff before and OpenCV for some image processing problems. I recently learned that scipy lets me manipulate images too. What are the pros and cons of using OpenCv and Scipy I am not able to figure out which would be better for me. Appreciate your help!,5,scikit_learn,2020-10-13
5rb5ww,Using Category Encoders library in Scikit-learn,,1,scikit_learn,2020-10-13
5m0352,MLPClassifier: Multiple output activation,"I'm using MLPClassifier but some of the outputs have more than one activation, i.e. [0 1 1 0].
How can I get only one activation?

My code is:
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                    hidden_layer_sizes=(15,), random_state=1, activation='relu')

Thank you!",1,scikit_learn,2020-10-13
5fasve,Need help on scikit kfold validation,"Objective: To create 5 folds of training and test dataset using StratifiedKFold method. I have referred the documentation at http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.cross_validation.StratifiedKFold.html

I am able to print the indices alright but am unable to generate the actual folds. Here follows my code

from sklearn.cross_validation import StratifiedKFold
import pandas as pd
df=pd.read_csv('C:\Comb_features_to_be_used.txt')

##Getting only numeric columns
p_input=df._get_numeric_data()
## Considering all the features except labels
p_input_features = p_input.drop('labels',axis=1)
## Considering only labels [single column]
p_input_label = p_input['labels']
skf = StratifiedKFold(p_input_label, n_folds=5, shuffle=True)
i={1,2,3,4,5}
for i,(train_index, test_index) in enumerate(skf):
    ##print(""TRAIN:"", train_index, ""TEST:"", test_index)
    p_input_features_train = p_input_features[train_index] 
    p_input_features_test =  p_input_features[test_index]
        
I am getting the error: IndexError: indices are out-of-bounds

",2,scikit_learn,2020-10-13
54exlw,scikit-learn doc translation,"translate sklearn doc to chinese
feel free to join us
https://github.com/lzjqsdd/scikit-learn-doc-cn",2,scikit_learn,2020-10-13
52b7od,Improving the Interpretation of Topic Models,,1,scikit_learn,2020-10-13
50qfgg,Topic Modeling with Scikit Learn,,3,scikit_learn,2020-10-13
4rjkcq,Overfit Random Forest,"I have data where Random Forest models overfit to noise whatever 
hyperparameter I put.
(= excellent accuracy on training, but poor accuracy on prediction).


So, this is the process I did to over-come:
    1) Tweak the input data and reduce the sampling of noise (negative example)

    2) Fit the RF and test (confusion matrix) on cross-validation data. 

    3) Repeat it and choose the best cross validation data.

Is there a way to overcome this monte carlo approach,
using OOBag process during training ?

Also incorporate Cross validation to reduce the over-fitting ?

Importance features change every time a new RF is fit (it seems a lot of co-linearity and noise into the data).










",1,scikit_learn,2020-10-13
4h6ypj,Building scikit-learn transformers,,3,scikit_learn,2020-10-13
3zvwqw,Hello everyone! I want to write an oversampling module in compliance with scikit-learn. Advice needed!,"As mentioned in the title I want to write a module for oversampling classes in skewed datasets. I recently came to need such a module and I noticed that no such thing exists officialy in scikit-learn. I want it to be compatible with scikit-learn as I very often use it. Do you have any resources to redirect me to, apart from the official scikit-learn developer guidelines? Any tips for writing a python module in general?

Thanks in advance!",2,scikit_learn,2020-10-13
2f830i,Official Scikit-Learn page.,,1,scikit_learn,2020-10-13
j9q6bp,A scikit-learn compatible library to construct and benchmark rule-based systems that are designed by humans,,7,scikit_learn,2020-10-13
j93854,Best performance on Scikit-learn’s load_digits dataset,"On Scikit-learn’s load_digits dataset:
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html

Does anyone know what is the best performance achieved so far on this dataset? 

I tried googling around but can’t find examples with 100% score performance. I am thinking since this is a standard dataset, it would be easy to get a 100% score performance?",1,scikit_learn,2020-10-13
j8vepi,"could someone ELi5 the hyperparameters (penalty, C, tol, max_inter)",I am currently working on a beginner project on logistic regression using scikit\_learn. I am trying to fine tune my regression model but cant seem to find any websites that can explain what the parameters mentioned in the title mean exactly and how to use them. I was wondering if anyone could give me a quick explanation on what/how to use these parameters to fine tune my regression model.,0,scikit_learn,2020-10-13
j7jz85,SVC rbf kernel seems to be nonstandard?,"I am currently testing a precomputed version of rbf I implemented to get a better feel for how it works and possibly later check out some other kernels.

It seems that whatever I do, I get different results using my precomputed gram matrix vs using the scikit rbf kernel:

To calculate a kernel entry for datapoints xm &amp; xn, based on some extra parameters theta

`k = thetas[0] * np.exp(-(thetas[1]/2.) * (np.sqrt((xn-xm).T @ (xn-xm)))) + thetas[2] + thetas[3] * (xn.T @ xm)`

using theta = \[1,2,0,0\]

This should recover the formulation given [here](https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel) (setting gamma=1)

1 \* exp( - 2/2|xn-xm|^(2) )   


is there something I'm missing? \[here's the code if you wanna take a look\]([https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit\_test.py](https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit_test.py)) (only dependencies are matplotlib, scikit &amp; numpy, so you're probably good if you're on this sub)",1,scikit_learn,2020-10-13
j5pcm6,ImportError,"I've got a long error which ends with:  
ImportError: DLL load failed while importing \_arpack: Não foi possível encontrar o procedimento especificado. (rough translation: Unable to find the specified procedure)  


Any idea on the issue? It seems like I have some sort of update issue, but I'm unable to find what.",2,scikit_learn,2020-10-13
j2v11w,"Scikit-learn. In the case of a single point, k-nearest neighbours predictions doesn’t literally match with the literally nearest point. I think I know why. Correct me if I’m wrong.","Hello.  I’ve looked at the source code. 

Case population sizes in the range 10 ^ 2 to 10 ^ 5 ish. Vanilla, straight out the box knn from scikit-learn.  Except 1 nearest neighbours not the default 5.  

When I try to predict the nearest neighbour of a point, using 1 nearest neighbours. after using knn.fit to make a model, it doesn’t _always_ return the same value of the actual nearest neighbour.  I’ve worked out the actual real nearest neighbour myself to check, using trig, and unit tested it.  

I think that’s because for pragmatic reasons knn is just a probabilistic model applied at group level.  Not exactly the actual knn for each and every point.  

Am I right?

EDIT:  My. Trig. Was.  Wrong.  Due. To.  A. Data frame. Handling.  Issue.  Ggaaahhhh.",6,scikit_learn,2020-10-13
j1idtx,RadomizedSearch CV taking forever,"Hi ,

I have the below snippet.

Trying to run on GCP . its getting stuck and not even updating.

&amp;#x200B;

https://preview.redd.it/bp2zfi71sxp51.png?width=1463&amp;format=png&amp;auto=webp&amp;s=6d97d1ff6083f6eb65e62d983770c69f06d45f4c",2,scikit_learn,2020-10-13
iv8jv9,Neuraxle - a Sklearn-Based Clean Machine Learning Framework,,1,scikit_learn,2020-10-13
it82un,How the 'init' parameter of GradientBoostingRegressor works?,"i'm trying to create an ensemble of an determined regressor, with this in mind i've searched for some way to use the sklearn already existing ensemble methods, and try to change the base estimator of the ensemble. the bagging documentation is clear because it says that you can change the base estimator by passing your regressor as parameter to ""base_estimator"", but with GradientBoosting you can pass a regressor in the ""init"" parameter. my question is: passing my regressor in the init parameter of the GradientBoosting, will make it use the regressor i've specified as base estimator instead of trees? the documentation says that the init value must be ""An estimator object that is used to compute the initial predictions"", so i dont know if the estimator i'll pass in init will be the one used in fact as the weak learner to be enhanced by the bosting method, or it will just be used at the beginning and after that all the work is done by decision trees. If someone can help me with this question i would be grateful.",3,scikit_learn,2020-10-13
igtkry,Best way to get T-Stastic and P-value etc?,"I'm using scikit learn for linear regression.  Is there a way to use that library to generate things like T-Stastic and p-value and standard error etc?

On stack overflow i found this, but wondering if there's a way within scikit

    import statsmodels.api as sm
    from scipy import stats
    X2 = sm.add_constant(X)
    est = sm.OLS(y, X2)
    est2 = est.fit()
    print(est2.summary())

&amp;#x200B;",1,scikit_learn,2020-10-13
i4ulg2,"Data Visualization using ""Python"" with ""Seaborn"" | Part- I",https://youtu.be/X400eIcV-So,3,scikit_learn,2020-10-13
i3546z,Recommendation based on other user following,"Hello,

I try to build a recommendation system.

My service allow users to follow people (not rate them, just follow) and I would like to be able to propose to users to follow people based on other user’s database activity.

Is scikit a good path for this ? 

Do you recommend specific method or useful ressource to read to achieve this ?

For your help guys!",2,scikit_learn,2020-10-13
hzw282,How to use TensorFlow Object detection API to detect objects in live feed of webcam in real-time,,1,scikit_learn,2020-10-13
hwmcf1,sklearn CCA - how to get variance explained for first canonical relationship?,"Hi. I'm exploring multivariate brain-behaviour relationships with sklearn's canonical correlation analysis tool ([https://scikit-learn.org/stable/modules/generated/sklearn.cross\_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca)). I am interested mostly in the first canonical relationship between the two datasets. The decomposition is working fine and i have the weights/canonical scores etcetera - but what i'd really like to know is how much of the variance in either dataset is explained by that one relationship (analogous to eg variance explained by first principal component).

There is a method named 'score' that i can call on the CCA object but I am not quite sure this is what I need. This score is not the same as 'canonical scores above but will supposedly get some coefficient of determination r\^2 between 'observed' and 'predicted' - not sure how to understand this. The description on the webpage is quite terse and it does not behave the way i might expect.

I'm hoping to find someone who might know whether that 'score' method  will get me to what i want - and if so, maybe how to use it. Or point me otherwise in the right direction to get into the variance explained for CCA.

Cheers!",2,scikit_learn,2020-10-13
hu6y83,KMeans Algorithm Question,"Hey all.

I am new with using scikit-learn and had a question regarding the KMeans algorithm functions. After running the algorithm and plotting the clusters, are the clusters with the centroids plotted the final clusters after training is done or is there training that I have to do on the clusters? 

Thanks everyone",1,scikit_learn,2020-10-13
htugnl,"How to handle ""Missing Value"" from ""Dataset"" using ""Pandas"" &amp; ""Sci-Kit Learn""??",https://youtu.be/8IORSsZIyIQ,0,scikit_learn,2020-10-13
htmc59,"How to handle ""Text"" and ""Categorical Attributes"" using Python and Pandas??",https://youtu.be/4sO7Pezlegk,0,scikit_learn,2020-10-13
ht4ol1,Making ROC curves with results from cross_validate?,"I am running 5 fold cross validation with a random forest as such:

from sklearn.ensemble import RandomForestClassifier

from sklearn.model\_selection import cross\_validate

forest = RandomForestClassifier(n\_estimators=100, max\_depth=8, max\_features=6)

cv\_results = cross\_validate(forest, X, y, cv=5, scoring=scoring)

However, I want to plot the ROC curves for the 5 outputs on one graph. The documentation only provides an example to plot the roc curve with cross validation when specifically using StratifiedKFold cross validation (see documentation here: [https://scikit-learn.org/stable/auto\_examples/model\_selection/plot\_roc\_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py))

I tried tweeking the code to make it work for cross\_validate but to no avail.

How do I make a ROC curve with the 5 results from the cross\_validate output being plotted on a single graph?

Thanks in advance",2,scikit_learn,2020-10-13
hm6td7,Best performance on MNIST - Fashion dataset,Does anyone know what is the best performance achieved so far for the MNIST - Fashion dataset along with what model that was used?,1,scikit_learn,2020-10-13
hm22vz,"How to ""Predict"" my friends weight using ""Machine Learning"" and ""Sci-Kit Learn""??",https://youtu.be/A4JwnkTFEXI,2,scikit_learn,2020-10-13
hl4k0u,Factor analysis “model” in CS229,"In one of Stanford’s CS229 lecture by Andrew Ng (https://m.youtube.com/watch?v=tw6cmL5STuY), he talks about a factor analysis “model” in which is to deal with situations where you have a lot more features than samples in your dataset. He even said he used a modified version of this factor analysis “model” in some recent work he did for a manufacturing company in the lecture.

Now my understanding of factor analysis is just a dimension reduction technique. So how did Andrew used factor analysis to build a “model” which deals with datasets which has a lot more features than samples?",2,scikit_learn,2020-10-13
hkm9qn,StackingRegressor Inconsistent Output,"Is it intentional that StackingRegressor returns different accuracy outputs when running multiple times given the same parameters, models and using numpy set seed?",1,scikit_learn,2020-10-13
hjctba,"This lecture that talks about what Multilabel and Multioutput classifications are, along with their implementation using scikit learn.",,1,scikit_learn,2020-10-13
hg5j3s,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,2,scikit_learn,2020-10-13
hf60u0,"Hey guys, here is a lecture on how to implement gradient descent with scikit-learn. Enjoy :)",,1,scikit_learn,2020-10-13
hakk07,How do I create a linear regression for this groupedby dataframe?,"I have this assignment for a job interview and I really want to impress by using some machine learning. I don't know too much about it and I essentially don't have much time to learn that much about it. I have the following [dataframe](https://postimg.cc/0zNbyrV8) and I want to create a linear regression using scikitlearn of \['profit'\] vs \['dateReceived'\] for each \['Language'\]. 

Does anyone know what I can do for that to work? I guess it should be just a few lines of code, but I could be wrong?",0,scikit_learn,2020-10-13
h7ay1o,"Visualize Scikit-learn models – ROC, PR curves, confusion matrices etc",,8,scikit_learn,2020-10-13
h172dr,Scikit Learn Tutorial in One Hour,,4,scikit_learn,2020-10-13
h0w3xx,Books about classification algorithms,"Hi all,

I am completely new to data mining and have to write a seminar paper about classification and do some programming in python.

With the help of datacamp I was able to implement the classification algorithms in python.

Now I am looking for some sources that I can cite in my paper that briefly explain these algorithms.

My problem is that most books that I have look into so far are very mathematical and since I don’t have a data mining/computer science background, they are hard to understand.

Do you have any recommendations for some text books that explain classification algorithms such as SVM, Naive Bayes, Trees, etc. that are well recognized, but explain them in an easy way?

Many thanks in advance!",1,scikit_learn,2020-10-13
gziaus,How to choose best pair of random state and class label values?,"For the last few days, I was trying to implement the KMeans algorithm using SciKit Learn, But I came across a very confusing problem. I have a dataset that has two class labels ['ALL', 'AML'] where ALL has 47 and AML has 25 samples and 100 attributes to train from and now I want to use this dataset for KMeans clustering so that I can compare the predicted results with the original class labels. Before asking my question let me explain certain scenarios. In all the scenarios I have taken all the 100 attributes to fit the model.

Scenario 1:

In the first run, I started with a model that is created with pretty much default arguments i.e. model = KMeans(n_clusters=2). For comparing the predicted class labels(which are numeric) with the original labels(which are strings), I set the original class labels as ALL = 1 and AML = 0. After that, while comparing using a classification report I got an average accuracy of 35%. Then I run the algorithm once again and got an accuracy of 44%. For the third try, I got 33% and so on.

However, I looked about it and came to know that the random_state argument needs to have a fixed value to get same accuracy throughout all runs.

Scenario 2:

After knowing about random_state, this time I started with random state 0 and created the model as model = KMeans(n_clusters=2, random_state=0) and kept the original class labels as before i.e ALL as 1 and AML as 0. However, this time the output didn't change on different runs and I got an accuracy of 53%. But, out of curiosity, I swap the original class label i.e. I set ALL as 0 and AML as 1 which results in 47%.

Scenario 3:

This time I choosed random_state as 1 i.e. model = KMeans(n_cluster=2, random_state=1) and having ALL as 0 and AML as 1 gave 67% accuracy while considering ALL as 1 and AML as 0 gave 33% accuracy.

So, My question is what I am doing wrong here? Am I implementing something wrong? If I am right then why the result is changing so much depending on random_state and class labels? What's the solution and how to choose the best pair of random_state and class labels?",1,scikit_learn,2020-10-13
gwgzy9,estimate_transform works when using 'similar' but not when using 'affine',"I have two 512x512 grayscales images (src and dst). To try to understand estimate transform I applied the following transformation

    tform = transform.AffineTransform(scale=(1.3, 1.1), 
                                        rotation=0.5, 
                                        translation=(0, -200)) 

to the src to create the dst. Then I want to find back the parameters using estimate\_transform.

With the parameter 'similar' I obtain parameters very close to the one I used (as expected). But when I want to use 'affine', I obtain the following error :

     matmul: Input operand 1 has a mismatch in its core dimension 0, 
    with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 513 is different from 3) 

Any idea why ? Here is my code :

    src = rgb2gray(data.astronaut())
    dst = rgb2gray(data.astronaut())
    tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5,
                                      translation=(0, -200))
    dst = transform.warp(img1, tform)
    tform_fin = transform.estimate_transform('affine', src, dst)
    dst_corr = transform.warp(img3, tform.inverse)",1,scikit_learn,2020-10-13
gtw4p9,What can I do when I keep exceeding memory used while using Dask-ML,"I am using Dask-ML to run some code which uses quite a bit of RAM memory during training. The training dataset itself is not large but it's during training which uses a fair bit of RAM memory. I keep getting the following error message, even though I have tried using different values for n_jobs:

```
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
```

What can I do?

Ps: I have also tried using Kaggle Kernel (which allows up to 16GB RAM) and this didn't work. So I am trying Dask-ML now. I am also just connected to the Dask cluster using its default parameter values, with the code below:

```
from dask.distributed import Client
import joblib

client = Client()

with joblib.parallel_backend('dask'):
    # My own codes
```",1,scikit_learn,2020-10-13
gsx926,MLPRegressor newby with some (probably very basic) questions in need of some assitance,"Hello!

I'm building MLPRegressor for the first time ever (I've been learning how to code with online courses since end of March) and I know something is wrong but I don't know what. Bellow you can see my code so far. It runs and I have a value for r2 ( -9035355.06 ) and a plot. However the r2 score doesn't make sense (it should be around 0.7)  and the plot doesn't make sense either.

I have run this analysis with SPSS multilayer perceptron feature so I know more or less how my results should be and that's why I know whatever I am doing with python is wrong.

Any advice/suggestion of what I'm doing wrong is very welcome! This coding world is kinda of frustrating for me:/

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    
    from sklearn import neighbors, datasets, preprocessing 
    from sklearn.model_selection import train_test_split
    from sklearn.neural_network import MLPRegressor
    from sklearn.metrics import r2_score
    
    vhdata = pd.read_csv('vhrawdata.csv')
    vhdata.head()
    
    X = vhdata[['PA NH4', 'PH NH4', 'PA K', 'PH K', 'PA NH4 + PA K', 'PH NH4 + PH K', 'PA IS', 'PH IS']]
    y = vhdata['PMI']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 
    
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_test_norm = scaler.fit_transform(X_test)
    
    nnref = MLPRegressor(hidden_layer_sizes = [4], activation = 'logistic', solver = 'sgd', alpha = 0.1, learning_rate= 'constant',
                         learning_rate_init= 0.6, max_iter=200, random_state=0, momentum= 0.3, nesterovs_momentum= False)
    nnref.fit(X_train_norm, y_train)
    
    y_predictions= nnref.predict(X_test_norm)
    
    print('Accuracy of NN classifier on training set (R2 score): {:.3f}'.format(nnref.score(X_train_norm, y_train)))
    print('Accuracy of NN classifier on test set (R2 score): {:.3f}'.format(nnref.score(X_test_norm, y_test)))
    print('Current loss : {:.2f}'.format(nnref.loss_))
    
    plt.figure()
    plt.scatter(y_test,y_predictions, marker = 'o', color='blue')
    plt.xlabel('PMI expected (hrs)')
    plt.ylabel('PMI predicted (hrs)')
    plt.title('Correlation of PMI predicted by MLP regressor and the actual PMI')
    plt.show()",1,scikit_learn,2020-10-13
gsol3k,What are the default values for the parameters in Dask-ML's Client() function,"I am trying to understand Dask-ML's Client() function parameters. Say I have the following code using Dask-ML's Client() function:

```
from dask.distributed import Client
import joblib

client = Client()
```

If I don't specify any values for the parameters in the Client() function, what are the default values for the parameters:

(i) n_workers

(ii) threads_per_worker

(iii) memory_limit

From my understanding, Python has the Global Interpreter Lock (GIL) feature which prevents multi-threading. If so, why does Dask-ML's Client() function have the parameter threads_per_worker when multi-threading is prevented in Python?

Does memory_limit refers to the maximum memory limit allowed for **each** worker/machine/node or does this refer to the maximum memory limit allowed for **all combined** worker/machine/node?

I have already looked through the documentation in Dask-ML (see here: https://docs.dask.org/en/latest/setup/single-distributed.html), but the documentation is not clear in regards to these questions above.

Thank you in advance if anyone could explain this?",1,scikit_learn,2020-10-13
glb4cy,Why does PolynomialFeatures has multiple pair of coefficient after fitted the data?,"After I create an PolynomialFeatures object, and fit the data by :

[`poly.fit`](https://poly.fit)`(x,)`

I wanted to look for the coefficient, so I do:

`poly.transform(x,y)`

&amp;#x200B;

And it will return an array with (n\_samples, n\_coeff), but why does the polynomial fit with multiple pair of coefficient? Wouldn't the model fit the data and get a final best coefficient?

&amp;#x200B;

And what is the final coefficient that Polynomial get after fitting?",1,scikit_learn,2020-10-13
gi4jw3,How to add sample_weight into a scikit-learn estimator,"I have recently developed a scikit-learn estimator (a classifier) and I am now wanting to add sample_weight to the estimator. The reason is so I could apply boosting (ie. Adaboost) to the estimator (as Adaboost requires sample_weight to be present in the estimator).

I had a look at a few different scikit-learn estimators such as linear regression, logistic regression and SVM, but they all seem to have a different way of adding sample_weight into their estimators and it's not very clear to me:

Linear regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_base.py#L375

Logistic regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_logistic.py#L1459

SVM: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/svm/_base.py#L796

So I am confused now and wanting to know how do I add sample_weight into my estimator? Is there a standard way of doing this in scikit-learn or it just depends on the estimator? Any templates or any examples would really be appreciated. Many thanks in advance.",2,scikit_learn,2020-10-13
get2kh,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,,2,scikit_learn,2020-10-13
gdb7a8,Code a Decision Tree in 20 lines.,,0,scikit_learn,2020-10-13
gd81x4,why does Scikit Learn's Power Transform always transform the data to zero standard deviation?,"all of my input features are positive. Whenever I tried to apply PowerTransformer with box-cox method, the lambdas are s.t. the transformed values have zero variance. i.e. the features become constants

&amp;#x200B;

I even tried with randomly generated log normal data and it still transform the data into zero variance.

&amp;#x200B;

I do understand that mathematically, finding the lambda s.t. the standard deviation is the smallest, would mean the distribution would be the most normal-like.

&amp;#x200B;

But when the standard deviation is zero, then what's the point of using it?

&amp;#x200B;

&amp;#x200B;

p.s. so one of the values of lambda I get by using PowerTranformer is -4.78 

If you apply it into the box-cox equation for lambda != 0.0, then for any input feature y values, you technically get the same values. i.e. (100\^(-4.78)-1.0)/(-4.78) is technically equals to (500\^(-4.78)-1.0)/(-4.78)",2,scikit_learn,2020-10-13
gcvtsm,how to combine recursive feature elimination and grid/random search inside one CV loop?,"I've seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:

[Feature selection and cross-validation](https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751)

[Nested cross-validation and feature selection: when to perform the feature selection?](https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec)

[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)

&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...

[Here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.

However I'd like to do recursive feature elimination inside random/grid CV, so that ""feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)"", so that data from other folds influence neither feature selection nor hyperparameter optimization.

Is this possible natively with sklearn methods and/or pipelines? Basically, I'm trying to find an sklearn native way to do this before I go code it from scratch.",3,scikit_learn,2020-10-13
gc2z28,How to write a scikit-learn estimator in PyTorch,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",3,scikit_learn,2020-10-13
g5ugws,Code a Neural Network in 20 lines.,,2,scikit_learn,2020-10-13
g4yc73,Basic question re: gaussian mixture models,"I wasn't able to find this in the documentation, but is the covariance parameter you access with model.covariances\_ sigma or sigma\^2? Seems like it can be either thing as I've seen the notations N(x| mu, sigma\^2) and N(x|mu, sigma) both used in various places.",1,scikit_learn,2020-10-13
fzm7mm,"Should scikit-learn include an ""Estimated Time to Arrival"" (ETA) feature? Discuss.",,7,scikit_learn,2020-10-13
fx6kdy,Clustering of t-SNE,"Hello,

I have recently tried out t-SNE on the sklearn.datasets.load_digits dataset. Then i applied KNeighborClassifier to it via a GridSearchCV with cv=5.

In the test set (20% of the overall dataset) i get a accuracy of 99%

I dont think i overfitted or smth. t-SNE delivers awesome clusters. Is it common to use them both for classifying? Because the results are really great. I will try to perform it on more data. 

I am just curious on what you (probably much more experienced users than me) think.",1,scikit_learn,2020-10-13
fx0i7x,Search over preprocessing and ensemble hyperparameters?,"In scikit-learn there are some handy tools like `GridSearchCV` for tuning the hyperparameters to a model or pipeline.

Suppose you'd like the preprocessing in your pipeline to include some user-defined options (e.g. whether to encode a certain categorical variable via one-hot encoding or something weird like frequency encoding) and you'd like to include those options among the hyperparameters you're searching over.

Suppose further that you're using an ensemble model -- e.g. a random forest plus few linear regression specifications, and you'd like to tune the hyperparameters for each of them, as well as the voting weight of each.

Does scikit-learn provide a predefined way to search over such spaces? It looks like the parameter space is intended only to dictate the behavior of a single model, not preprocessing steps or ensemble parameters.",1,scikit_learn,2020-10-13
ft2pcp,"How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?","How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?",1,scikit_learn,2020-10-13
ft1kmb,facing an error,"import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

&amp;#x200B;

\# Importing the dataset

dataset = pd.read\_csv('50\_Startups.csv')

X = dataset.iloc\[:, :-1\].values

y = dataset.iloc\[:, 4\].values

X2=dataset.iloc\[:, 3\].values

\# Encoding categorical data

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

le = LabelEncoder()

X2 = le.fit\_transform(X2)

oh = OneHotEncoder(categories = 'X\[:, 3\]')

X= oh.fit\_transform(X).toarray()

&amp;#x200B;

https://preview.redd.it/7bgiwdp238q41.png?width=871&amp;format=png&amp;auto=webp&amp;s=d2386c5cda100706a85803c036586beec8b9e843",1,scikit_learn,2020-10-13
fm1oov,I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?,"I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?

    preprocess =     make_column_transformer((SimpleImputer(strategy='median'), cols_numeric),     
    (SimpleImputer(strategy='constant', fill_value='missing'), cols_onehot),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_target),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_ordinal),     (OneHotEncoder(handle_unknown='ignore'), cols_onehot),     
    (TargetEncoder(), cols_target),     
    (OrdinalEncoder(), cols_ordinal),     
    (StandardScaler(), cols_numeric)) 
    lr_wpipe = make_pipeline(preprocess, LinearRegression()) 
    lr_scores = cross_val_score(lr_wpipe, X_train, y_train) 
    np.mean(lr_scores) 
    print(""Linear Regression R^2: "", lr_scores)",1,scikit_learn,2020-10-13
flg6a1,how to find 'the math' being done in sklearn source code?,"hi.  I'm trying to find where in sklearn the actual math is being done, mostly for my own learning so I can answer questions like 'when using `sklearn.neighbors` , what math is being used to calculate Euclidean distance?'    


If you see here: [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/\_base.py#L360](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/_base.py#L360)  
you'll see that Euclidian and other distance calculations can be *specified*, but I don't see anywhere where the actual math is being done in code.",1,scikit_learn,2020-10-13
fl4fj1,Adding Standard Scaler to GridSearchCV,"I'm looking to use the Standard Scaler as a hyper parameter, i.e check if performance is higher with/without scaling the inputs. In order to tune with other hyperparameters, I would like to incorporate it into my GridSearchCV function (provided by Scikit Learn). Can someone advise me on how to do it?",1,scikit_learn,2020-10-13
ffx9zw,How to use tfidfvectorizer fit_transform for multiple docs,"Hey, 

Let's say my corpus is a list of lists , each of the inner lists represent a parsed doc (each value is a word) 

I want to compute a tf-idf score for my corpus. 

It's seems like the fit-transform function can't use my corpus as its inputs should be itratable with string values (which is each of my docs) 


    V = tfidfvectorizer ()
    For doc in corpus:
       Vectors = v.fit_trabsform(doc)

So my question is, how does it calculate IDF if it get only one doc at a time?",1,scikit_learn,2020-10-13
ff9602,Classifiers' score method clarification,"Hi,

I don't fully understand what the score method of classifiers does. For example, the [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method's documentation says ""Return the mean accuracy on the given test data and labels."" Now, I know what is accuracy: (TP+TN)/(TP+TN+FP+FN), but I don't understand why ""mean"" is in there. Mean over what? of what?  

That is, I give the method as parameters a dataset with true labels, and it can calculate the accuracy from that (given the model), but where does the mean come into place? 

Thanks in advance!",3,scikit_learn,2020-10-13
fbfky4,Is epsilon in dbscan a euclidean measure?,"Hello everyone,
I'm writing yet another dbscan question. For those who are familiar with the inputs to the dbscan, the principal parameters are epsilon and minPts.

Epsilon is the neighborhood radius, and I'm curious if anyone can point me to a reference or tell me if epsilon is a euclidean metric",3,scikit_learn,2020-10-13
f1dizs,Identifying smallest frequently occurring value,"I'm not a data science person, but thinking Scikit learn might be able to help here, and looking for suggestions for ideas I should investigate.

Essentially, I'm looking for a way to consistently identify a baseline power readings. If I have minute by minute power consumption readings from a bunch of electrical motors.  For any motor, we want to identify what a 'baseline' or 'normal unloaded steady-state' power value is.

There is definitely noise in the signal, and not even noise - legitimate power reading that are smaller than what we would consider 'normal unloaded steady state'.  The catch is this could be different for the same motor when production composition changes, so there is not just one value that we can look at historical data to arrive at. (Think motors running pumps moving different fluid mixtures / slurry ad different times.

This does not have to be real-time, just take the dataset of power readings for any motor for any production batch and post-process the data in such a way we can identify times the motor is doing its job at a 'near-idle' state.

Currently we just have a basic calculation that looks at a rolling window of 20 per-minute readings and finds the lowest value that occurs at least twice. (basically throwing out the lowest few outliers)

The reason I'm considering Scikit or similar is we can graph these power readings for a time period (say 1 day) and visually we can easily see these 'baselines' we are looking for.  There will be spikes and dips, and time windows where we are definitely running a heavy load (motors spun up on demand), but we can identify when the mixture changes because the visual changes in this baseline value.

Hope that made at least a little sense, if there are details I can clarify, please ask.  I appreciate everyone's thoughts and ideas!",1,scikit_learn,2020-10-13
eylu4p,What's wrong with Scikit-Learn.,,3,scikit_learn,2020-10-13
ev1as7,Is it possible to use a custom-defined decision tree classifier in Scikit-learn?,"I have a predefined decision tree, which I built from knowledge-based splits, that I want to use to make predictions. I could try to implement a decision tree classifier from scratch, but then I would not be able to use build in Scikit functions like predict. Is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? Or do I need to do something completely different? My first attempt was to use “fake training data” to force the algorithm to build the tree the way I like it, this would end up in a lot of work because I need to create different trees depending on the user input.",1,scikit_learn,2020-10-13
eu9qgc,Is HistGradientBoosting the same as LightGBM or is the SKLearn's version different?,"If so, how?",2,scikit_learn,2020-10-13
em9fxf,Is this the proper way to do ML with scikit_learn?,"I have a dataset with 8 features (numeric) and 1 target (0 or 1).  
 I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc.  
 For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

But sometimes I get these errors:  
 The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

What's the proper way to predict binary values?  
 I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Thanks for your support. I'm a beginner.",2,scikit_learn,2020-10-13
effb98,What module/algorithm should I use in order to predict the time in which a certain action will be completed?,"Basically the title, but to explain it even more :

I have to device a model which will predict the total time a patient will have to wait in a hospital environment. For that, we have a dataset consisting of various patients with several diseases and their time durations already recorded. I want to know which module or algorithm should I use to carry this out? This is my first ML project and I could use any help that you guys can do. Thank you!!",2,scikit_learn,2020-10-13
e0h8ao,Column transformer throwing away some features?,"My data frame  has numerical columns a, b, c. And it has categorial text features d, e, f, g, h.

I build a preprocessor like the following.

    num_features = ['a','b','c']
    nom_features = ['d','e','f','g','h']
    
    preprocesser = ColumnTransformer([
        (""scale_numeric"", StandardScaler, num_features),
        (""encode_nominal"", OneHotEncoder(handle_unknown=""ignore""), nom_features)],
        remainder=""drop""
    )
    
    preprocessor.fit_transform(dataframe)

Since I started with 8 features, after one hot encoding I expected to get 8 or more features back, but the result of `preprocessor.fit_transform(dataframe)` only has 3 columns. Not sure what I am doing wrong if anyone can help me.",1,scikit_learn,2020-10-13
dyzwn9,How to Modify(Make unique) the Scikit-learn Multilayer perseptron algorithm (MLP),"Hi folks,

I've been trying to build a rainfall prediction model for last few days. I've used the **Scikit-learn Multilayer perseptron regressor function** straight up. 

1) The accuracy was OK(78%) but I want to increase it

2) I don't want to use the same predominantly given function (I just want to **add uniqueness in my code**, but I want to use scikit-learn)

Is there any way to modify the function or not use the ready-made function? Can anyone please help me with this?

Thanks in advance!",1,scikit_learn,2020-10-13
dtnh3f,The best alpha for ridge regression is... -85???,,3,scikit_learn,2020-10-13
dtiy98,difference between Kfold.split() and shufflesplit.split() in scikitlearn,"I read this [post](https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv), I get the difference when it comes to computation and shufflesplit randomly sampling the dataset when it creates the testing and training subsets, but in the answer on stackoverflow, there is this paragraph  


""**Difference when doing validation**

In KFold, during each round you will use one fold as the test set and *all* the remaining folds as your training set. However, in ShuffleSplit, during each round **n**  you should *only* use the training and test set from iteration **n** ""

I couldn't quite get it. since in kfold, you're bounded by using the training buckets (k-1) and testing bucket (k) in the **k** iteration and in shufflesplit you use the training and testing subsets made by the shufflesplit object in iteration **n.**  so for me it feels like he's saying the same thing.

can anyone please point out the difference for me?",1,scikit_learn,2020-10-13
dcvd2r,When to use these unsupervised algorithms?,"There are a lot of modules in sklearn. I am interested when these unsupervised algorithmes (bellow )are used.  
When to use a Gaussian mixture model? When to use Manifold Learning, When to Biclustering? etc.

&amp;#x200B;

* [2.1. Gaussian mixture models](https://scikit-learn.org/stable/modules/mixture.html) 
* [2.2. Manifold learning](https://scikit-learn.org/stable/modules/manifold.html)
* [2.4. Biclustering](https://scikit-learn.org/stable/modules/biclustering.html) 
* [2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html) 
* [2.6. Covariance estimation](https://scikit-learn.org/stable/modules/covariance.html) 
* [2.7. Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html) 
* [2.8. Density Estimation](https://scikit-learn.org/stable/modules/density.html)",8,scikit_learn,2020-10-13
dar9z6,pattern recognition on texts that are bash commands or software signature?,"hi all.

so I've got my hands on a daily dose of 100,000 connections per day to our servers, and I've got millions of rows of data that includes commands our users have executed on our servers, (\`cd\`, \`ehlo\`, \`scp ....\`, etc). and I have the same amount of data of their application signatures while connecting. like (Firefox 59, Firefox 60, google chrome),... and user agents, ...

basically all the data one can extract out of a socket or using an IDS.

I like to do some pattern matching on these data. like for the commands they are executing and stuff like that...

so to cluster the commands, I've got commands that look like this:

cd Project

cd Images/personal

cd Project/map

cat /var/log/nginx/web\_ui.log

the problem is, I can just split the texts and take in the first part(cd, cat) and make plot out of the commands, but i really would like to make it more automatic and intelligent. so people who \`cd\` into the \`Project/map\` are distinguished from people who cd into \`Images\` folder. I like to know what people are doing on out servers. so a plot that all people whith \`cd\` commands are close to each other, but are really distinguished for each folder that they have \`cd\` into. 

this is just an example of what I want:)

&amp;#x200B;

turns out that scikit\_learn only works on numbers? how can i utilize it for that kind of data? I don't know if this is a nltk problem?",3,scikit_learn,2020-10-13
d8sxus,Exporting Models to build own inference server,"Hello, I was hoping to get pointed in the right direction.  After training a random forest classifier I am looking to export the model in such a way that I can recreate each of the trees in C++.  I am trying to figure out the best approach to this, or if it is even possible.  My research online mainly shows examples of how to visually represent these, and how to create a pickle project for python serialization.  

Am I missing some key terms in my search? Could you point me to what i should be doing to figure this out?

&amp;#x200B;

My approach so far has been exploring the clf.estimators.trees\_ part of the estimators object, but I am not sure if I am on the right track.

&amp;#x200B;

Any help is much appreciated.

&amp;#x200B;

Thanks!",1,scikit_learn,2020-10-13
d244x4,Predict device from flow,"Hey guys, I applied to a competition about AI and my task is to predict device class from flow. I have 13 types of classes which are all in train set but the test set is missing that one column. After I run training and then I try to predict it, I receive an error stating this: ValueError: query data dimension must match training data dimension.

How can I predict a column that is not there? I don't believe that I have to manually put the column to the test.json 

Thanks for advices.",4,scikit_learn,2020-10-13
cvtjk6,Predicting Churn With Nested Data,"Hello All!

Ok, so this is a bit of a challenge and I'm trying to figure out if it is even worth worrying about the nesting aspect of the data. Basically, I'm trying to predict subscription-level churn with a combination of subscription-level and user-level variables.

Since users own subscriptions I figured I should try to account for nesting in my model. Does anyone have any recommendations on how to attack churn predictions using a nested model? Any suggestions would be greatly appreciated. Again, I have code working, but I've never built anything that requires nested analysis.

Basically my question is: Is it possible to run a multi-level SVM?",2,scikit_learn,2020-10-13
cs2urp,What is the most efficient way to implement two-hot encoding using scikit learn?,"I have two very similar features in my dataframe, and I would like to combine their one-hot encoded versions. They are both categorical data, and they both contain the same categories. I was thinking about using OneHotEncoder from scikit learn and getting the union of the corresponding columns. Is there a function or more efficient way that I do not know about?",3,scikit_learn,2020-10-13
cnnacy,Feature elimination doesn't really eliminate anything.,"I had a fairly simple dataset, after plotting the correlation matrix I noticed that one variable has very low correlation with the target (0.04) but instead of deleting it manually I decided to try feature elimination.
I tried both RFE and RFECV with Logistic Regression as an estimator, RFE eliminated some features which seemed correlated with the output and kept that feature.
RFECV didn't eliminate anything at all.

Am I missing something here?",1,scikit_learn,2020-10-13
cnl2a5,k-means output issue,"Hello I've run a k-means over my voice data. I got two class (for best). My problem is why i got this line at the right side? I sit an issue in my dataset?

https://preview.redd.it/n5lz9pggx7f31.png?width=852&amp;format=png&amp;auto=webp&amp;s=e84cc13f9579c026fc6bfa5cbd85849b1ea2939e",1,scikit_learn,2020-10-13
cmmbi5,Running scikit validation on 24 cores is slow?,"Hello guys, maybe anyone can help me out here. I am running following validation code:

```
from sklearn.linear_model import LinearRegression
model = LinearRegression()
from sklearn.preprocessing import PolynomialFeatures
poly_transformer = PolynomialFeatures(degree=2, include_bias=False)
from sklearn.pipeline import Pipeline
pipeline = Pipeline([('poly', poly_transformer), ('reg', model)])
train_scores, valid_scores = validation_curve(estimator=pipeline, # estimator (pipeline) X=features, # features matrix y=target, # target vector param_name='pca__n_components', param_range=range(1,50), # test these k-values cv=5, # 5-fold cross-validation scoring='neg_mean_absolute_error') # use negative validation
```

in the same .py file on different machines, which I would name #1 localhost, #2 staging, #3 live, #4 live. localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds
live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.
In order to get more ""trustworthy"" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different?",1,scikit_learn,2020-10-13
clz2bl,vectorization,"Hi, I just want to know if I can vectorize a text even if its on another language using Count Vectorization",2,scikit_learn,2020-10-13
clpubv,Machine learning final year project,"design and implement an intelligent agent that can detect a fault and can trouble a faulty server on a network

Its a network anormaly project 
But dont know where to start from",1,scikit_learn,2020-10-13
cl88rf,No Scikit-learn after I installed Anaconda in Sublime Text 3," 

I started using Sublime Text as my Text Editor/IDE (not sure what the difference is) to do some Python projects. After watching 2 episodes of the machine learning course by Google Developers. I installed the Anaconda package which has the Scikit-learn included.

Following the video I typed:

    import sklearn

This error appeared:

    ModuleNotFoundError: No module named 'sklearn'

Is there a way to install the Scikit-learn using the Sublime Text 3 or using a different method?",1,scikit_learn,2020-10-13
cgijm0,Unable to find/import,"edit: Title - Unable to find/import IterativeImputer

&amp;#x200B;

&amp;#x200B;

Hello fellow users, I'm wondering if yall could help me out with importing/finding IterativeImputer...

**&gt;&gt;&gt;** *# explicitly require this experimental feature*

**&gt;&gt;&gt; from** **sklearn.experimental** **import** enable\_iterative\_imputer *# noqa*

**&gt;&gt;&gt;** *# now you can import normally from impute*

**&gt;&gt;&gt; from** **sklearn.impute** **import** IterativeImputer

**ModuleNotFoundError**: No module named 'sklearn.impute.\_iterative'; 'sklearn.impute' is not a package

&amp;#x200B;

$pip freeze states I have scikit-learn==0.21.2 and sklearn==0.0

Python version 3.6

&amp;#x200B;

After researching the issue online I see that there's an experimental version I need to install, but I can't seem to find it! Further, I can't find it on their website.. [https://scikit-learn.org/dev/versions.html](https://scikit-learn.org/dev/versions.html)

What did I overlook/miss?",1,scikit_learn,2020-10-13
cc2mxr,How to re-structure a numpy dataframe into a format I can use in sklearn?,"Assuming the dataframe column 0 is the target and columns 1: are the features, and that each column is named, what's the easiest way to split the data for use in sklearn?",1,scikit_learn,2020-10-13
cbm4g6,How to classify dots,"Hello, 

I have a graph with two groups, red and blue dots. These groups are clearly separated, but the problem is that I want to say if a new dot belongs to the red group, to the blue, or to none of them.

What method do you recommend?

Thank you",1,scikit_learn,2020-10-13
c4rlf7,Regression is not yielding many useful predictions,"Hello all,   


I'm using a linear regression to predict continuous values (how long until a client churns measured in months). I have a dataset of cancelled accounts and active account. I'm using the cancelled accounts to predict the active accounts. I have a variety of explanatory variables and in total I have an R-squared of around 35% (obviously R-squared isn't perfect)   


Overall, this works pretty well; however, one issue I'm running into is that, of the predictions I get back, some are negative and very few actually predict that these active clients should still be active. In other words, many of the predicted cancellation dates are in the past.  


Dumb question, but is there a method I could be using to help this? Overall, I'm getting about 10k useful observations from 60k predictions.  Any suggestions would be greatly appreciated.",1,scikit_learn,2020-10-13
c4q5i6,I can't import Kmeans into compiler,"I'm currently using sklearn 0.21.2, and when I do:  


`import sklearn.cluster.KMeans`

&amp;#x200B;

the compiler returns error:

&amp;#x200B;

`no module named sklearn.cluster.KMeans`

&amp;#x200B;

I've found that in the cluster package, there is an module named 'cluster.k\_means\_'  


But when I tried to use this instead, it shows error

&amp;#x200B;

`Module is not callable`

&amp;#x200B;

Now I don't know why I can't import the kmeans package in cluster.",1,scikit_learn,2020-10-13
bylpjd,Sklearn regression with two datasets,"Hello all,   


basically, as the title implies I'm trying to train a regression model on one dataset and the apply that predictive model to another dataset. In other words, I have a model which predicts cancelled accounts and the amount of time in which those accounts cancel.   


I have another dataset full of active accounts (with the same variables) and I'm attempting to use the model from the cancelled accounts to predict when my active accounts will cancel. I'm having trouble with this.  Is there a way to do this without forcing a t  


Is there a way to use the ""active dataset"" without enforcing a Train\_test\_split? Any help would be greatly appreciated. Thank you!",2,scikit_learn,2020-10-13
bvjxzj,Get the function that fits my data,"I have fit a polynomial regressor to a two dimensional data. 
Is there a way to see the function that fits this data?",2,scikit_learn,2020-10-13
bqtxes,Kmeans clustering cache the result,"Hello,

&amp;#x200B;

I am new to scikit and I was wondering if I could cache the result of Kmeans so next time when I run my script I do not create the centroids again - that means save the result of [`kmeans.fit`](https://kmeans.fit)`()`.",2,scikit_learn,2020-10-13
bp7dv5,Get classes name of each estimator in OneVsOneClassifier,"Are there any ways to do that ? I am trying to directly access the classes\_ attributes in the estimator but it only returning \[0,1\]",2,scikit_learn,2020-10-13
bevq9d,Using Blob Detection methods on huge images,"I'm trying to use common blob detection methods from

[https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob\_dog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog)

on a huge images (about 6000x6000 pixels). It takes way too long to compute and show the result. How could I resolve this?",1,scikit_learn,2020-10-13
bcwbv4,Calculate variance of accuracy,"Hello, how can I calculate the variance of accuracy between two models in Random forest.
I mean I made a simple model with DecisionTreeClassifier() and one more with BagginClassofier() using the first model on it.
The accuracy climb +0.237.

How to get variance of that accuracy?
Thansk",1,scikit_learn,2020-10-13
bcbduy,Classification: Minimizing the amount of false positives,"Hey there,

I posted an earlier post (now deleted) that phrased this a bit wrong (thanks Imericle). Here is another try: 

Many  (most?) classification algorithm seem to be about maximizing accuracy  (true positives + negatives). My aim is to minimize the amount of false positives. How would I achieve this?

Only options I see to achieve this is through parameters tuning, is that the right approach?

(Thinking on applying it to a RandomForest),

Thanks,

Bb",2,scikit_learn,2020-10-13
bbxi9t,KMeans: Extracting the parameters/rules that fill up the clusters,"Hi all,

&amp;#x200B;

I have created a 4-cluster k-means customer segmentation in scikit learn. The idea is that every month, the business gets an overview of the shifts in size of our customers in each cluster. 

My question is how to make these clusters 'durable'. If I rerun my script with updated data, the 'boundaries' of the clusters may slightly shift, but I want to keep the old clusters (even though they fit the data slightly worse). My guess is that there should be a way to extract the paramaters that decides which case goes to their respective cluster, but I haven't found the solution yet. 

I would appreciate any help",1,scikit_learn,2020-10-13
b6nfvs,Question about FeatureUnion,"    pipe = Pipeline([
            ('features', FeatureUnion([
                    ('feature_one', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_one')),
                        ('vec', cvec) # Count vectorizer
                    ])),
                    ('feature_two', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_two')),
                        ('vec', tfidf) # Tf-idf vectorizer
                    ]))
                ])),
            ('clf', OneVsRestClassifier(clf)) #clf is a support vector machine
        ])

I'm using this pipeline for a project I'm working on, and I just want to make sure I understand how FeatureUnion works. I'm building a classifier which takes in two different text features and attempts to make a multi-class classification.

&amp;#x200B;

To give a little more detail, I'm trying to classify news articles into one of several categories (sports, business, etc.) Feature one is a list of tokens taken from the article's url, which often, though not always, explicitly states the name of the topic. Feature two is a list of tokens from the body of the article.

&amp;#x200B;

Does it make sense to separate the two features this way? Does this have a different effect than if I had just merged all of the tokens into a single list and vectorized them? My intention was to allow the two features to effect the model to different degrees, since I figured one would be more predictive in most scenarios (and I am getting pretty great results.)",2,scikit_learn,2020-10-13
b36h5a,Ranforest random behaviour,"If I give random forest parameters as RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019) Should it be creating 10 same decision trees? But it is not. I am asking the random forest to
     1.Sample without replacement (bootstrap=False) and each tree have same number of sample (ie the total data )(verified using plot)
     2.Select all features in all trees.
But model.estimators_[2] and model.estimators_[5] are different

",2,scikit_learn,2020-10-13
axgj2c,Predicting the runtime of scikit-learn algorithms,"Hey guys,

We're two friend who met in college and learned Python together, we co-created a package which can provide an estimate for the training time of scikit-learn algorithms.

The main function in this package is called “time”. Given a matrix vector X, the estimated vector Y along with the Scikit Learn model of your choice, time will output both the estimated time and its confidence interval. 

Let’s say you wanted to train a kmeans clustering for example, given an input matrix X. Here’s how you would compute the runtime estimate:

    From sklearn.clusters import KMeans
    from scitime import Estimator 
    kmeans = KMeans()
    estimator = Estimator(verbose=3) 
    # Run the estimation
    estimation, lower_bound, upper_bound = estimator.time(kmeans, X)

We are able to predict the runtime to fit by using our own estimator, we call it meta algorithm (meta\_algo), whose weights are stored in a dedicated pickle file in the package metadata.

The meta algos estimate the time to fit using a set of ‘meta’ features, including the parameters of the algo itself (in this case kmeans) and also external parameters such as cpu, memory or number of rows/columns. 

We built these meta algos by generating the data ourselves using a combination of computers and VM hardwares to simulate what the training time would be on the different systems, circling through different values of the parameters of the algo and dataset sizes . 

Check it out! https://github.com/nathan-toubiana/scitime

Any feedback is greatly appreciated.",6,scikit_learn,2020-10-13
aahf76,"Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?","Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?

&amp;#x200B;

So in the sense that if one applies e.g. a gain factor  (or a function depicting gain changes), then it's applied to the correct direction.",3,scikit_learn,2020-10-13
a75oid,"classification_report + MLPClassifier(): UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn_for)","classification\_report on a prediction done on MLPClassifier() sometimes throws:

&amp;#x200B;

*UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn\_for)*

&amp;#x200B;

but not on all the time.

&amp;#x200B;

What could be wrong?

&amp;#x200B;

\---

&amp;#x200B;

Doing

&amp;#x200B;

set(y\_test) - set(y\_pred) 

&amp;#x200B;

I'm able to see that sometimes some label is missing from y\_pred. But why does this occur only occasionally?

Is something wrong with how I use MLP?",1,scikit_learn,2020-10-13
a753f2,"{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification_report()","{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification\_report()

&amp;#x200B;

Why?

&amp;#x200B;

I call it like:

&amp;#x200B;

    classification_report(y_test, y_pred)

where y\_pred is predicted using a model I built.

&amp;#x200B;

""Quite obviously"" the arguments are incompatible somehow, but how can I find out, how? And how can I make them compatible?

&amp;#x200B;

\---

&amp;#x200B;

I tried:

&amp;#x200B;

    from sklearn.utils.multiclass import type_of_target

&amp;#x200B;

    &gt;&gt;&gt; type_of_target(y_test)
    'multiclass'
    
    &gt;&gt;&gt; type_of_target(y_pred)
    'continuous'

&amp;#x200B;",1,scikit_learn,2020-10-13
a746h0,"Is there any way to ""estimate"" how long a given computation in sklearn will take?","Is there any way to ""estimate"" how long a given computation in sklearn will take?

&amp;#x200B;

So that one doesn't need to wait longer than what one can?

&amp;#x200B;

Also, since Windows Task Manager shows only modest CPU use (&lt; 10%), then how is one supposed to know, what's occurring in the model?",2,scikit_learn,2020-10-13
a73oda,What are the most important parameters in LogisticRegression()?,What are the most important parameters in LogisticRegression()?,3,scikit_learn,2020-10-13
a71xf2,How does one feed hidden_layer_size tuples into GridSearchCV's param_grid?,How does one feed hidden\_layer\_size tuples into GridSearchCV's param\_grid?,1,scikit_learn,2020-10-13
a0yprf,Code review,"Hello,

I'm new to ML and scikit - hope this is the correct place for this. Have created the below code that appears to be working but wanted to get the opinions of people with more experience then me, to check I haven't a made any major errors or if there are any obvious improvements?

&amp;#x200B;

I am trying to train a model on a data set of potentially hundred of thousands emails. Every few days I want to retrain the exported model using incremental learning on the new emails received since the model was last trained.

The below reads the initial data from a csv, runs HashingVectorizer then SGDClassifier. The OnlinePipeline is used to allow me to use partial\_fit when I try to retrain later in the process.

`import pandas as pd`

`data = pd.read_csv('customData1.csv')`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`from sklearn.feature_extraction.text import HashingVectorizer`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

&amp;#x200B;

`from sklearn.linear_model import SGDClassifier`

`text_clf = OnlinePipeline([('vect', HashingVectorizer()),`

`('clf-svm', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=5, random_state=None)),`

`])`

`text_clf = text_clf.fit(X_train,Y_train)`

`predicted = text_clf.predict(X_test)`

`np.mean(predicted == Y_test)`

The above gives me an accuracy of 0.55

&amp;#x200B;

A few days later when I have new emails I import the previously exported model and use partial\_fit on a new csv file.

`import pandas as pd`

`data = pd.read_csv('customData2.csv') #text in column 1, classifier in column 2.`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

&amp;#x200B;

`from sklearn.externals import joblib`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

`text_clf2 = joblib.load('text_clf.joblib')`

&amp;#x200B;

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`text_clf2 = text_clf2.partial_fit(X_train,Y_train)`

&amp;#x200B;

`predicted = text_clf2.predict(X_test)`

`np.mean(predicted == Y_test)`

This returns the improved accuracy of: 0.84

&amp;#x200B;

Sorry for so much code!  I obviously need to tidy it all up so its a single method and handle the import/export logic properly.

&amp;#x200B;

Have a made any major errors or are there any obvious improvements? Thanks!

&amp;#x200B;",1,scikit_learn,2020-10-13
a0iz4i,Does cross_val_score tell something about generalizability?,"Does cross\_val\_score tell something about generalizability?

&amp;#x200B;

Or do I need to use something else for measuring generalizability?",0,scikit_learn,2020-10-13
a0c8dw,"Is there a problem if MLPRegressor doesn't converge for max_iter=100, but nor max_iter=5000 either?","Is there a problem if MLPRegressor doesn't converge for max\_iter=100, but nor max\_iter=5000 either?

&amp;#x200B;

Anything else I could try?",1,scikit_learn,2020-10-13
a0b260,What do cv (number of folds) and the number of outputs in cross_val_score correspond to?,"What do cv (number of folds) and the number of outputs in cross\_val\_score correspond to?

&amp;#x200B;

Does it mean that it produces cv number of different scores? Or (as I read somewhere) that only the last score might be the meaningful one (I read something like the others than the last used to ""fit"", while the last is the score)?",2,scikit_learn,2020-10-13
a0awuh,"Getting values in range [-191806. ..., 0.77642 ...] from cross_val_score, am I doing something wrong?","Getting values in range \[-191806. ..., 0.77642 ...\] from cross\_val\_score, am I doing something wrong?

&amp;#x200B;

    mlp = MLPRegressor(hidden_layer_sizes=(7,))

mlp.fit(X\_train,y\_train) mlp\_y\_pred = mlp.predict(X\_test)

&amp;#x200B;

y\_pred is an earlier prediction using LinearRegression().

&amp;#x200B;

I call cross\_val\_score like:

&amp;#x200B;

    cross_val_score(mlp, y_pred, mlp_y_pred, cv=10)

&amp;#x200B;

Output is:

&amp;#x200B;

    00 = {float64} -4.4409160725075605
    01 = {float64} -673636.0674512024
    02 = {float64} -51282.162171235206
    03 = {float64} -399557.4789466267
    04 = {float64} -35.73093353875776
    05 = {float64} -1406.9741325253574
    06 = {float64} -80853.84044929259
    07 = {float64} -5132.870883709122
    08 = {float64} -283.7432365432288
    09 = {float64} -2.860321933844385

&amp;#x200B;

I think I should be getting values in range \[0,1\].",1,scikit_learn,2020-10-13
a0at1q,"Is MLPRegressor's hidden_layer_sizes=(7,) equivalent to hidden_layer_sizes=7?","Is MLPRegressor's hidden\_layer\_sizes=(7,) equivalent to hidden\_layer\_sizes=7?",1,scikit_learn,2020-10-13
a09ukl,"Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train_test_split(Xy,shuffle = False, test_size = 0.33)?","Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train\_test\_split(Xy,shuffle = False, test\_size = 0.33)?

Xy has been constructed like:

&amp;#x200B;

    X = dat.data
    y = dat.target 
    Xy = np.hstack((X,np.array([y]).T))

It seems that it returns only two arrays, even when I saw an example ([https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input](https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input)) do 

 

    X_train, X_test, y_train, y_test = train_test_split(Xy,shuffle = False, test_size = 0.33) ",1,scikit_learn,2020-10-13
a06iin,Runtime Error in RandomizedSearchCV,"I've been running a RandomForestClassifier on a dataset I took from UCI repository, which was taken from a research paper. My accuracy is \~70% compared to the paper's 99% (they used Random Forrest with WEKA), so I want to hypertune parameters in my scikit learn RF to get the same result (I already optimized feature dimensions and scaled). I use the following code to attempt this (random\_grid is simply some hard coded values for various parameters):

&amp;#x200B;

    rf = RandomForestClassifier()
    # Random search of parameters, using 2 fold cross validation,
    # search across 100 different combinations, and use all available cores
    rf_random = RandomizedSearchCV(estimator = rf,  param_distributions = random_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)
    # Fit the random search model
    rf_random.fit(x_train, x_test)

When I attempt to run this code though my python runs indefinitely (for at least 40 min before I killed it) without giving any results. I've tried reducing the \`cv\` and \`n\_iter\` as much as possible but this still doesn't help. I've looked everywhere to see if there's a mistake in my code but can't find anything. I'm running Python 3.6 on Spyder 3.1.2, on a crappy laptop with 8Gb RAM and i5 processor :P

&amp;#x200B;

Here is the random\_grid if it helps:

    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
    max_features = ['auto', 'sqrt']
    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
    max_depth.append(None)
    min_samples_split = [2, 5, 10]
    min_samples_leaf = [1, 2, 4]
    bootstrap = [True, False]
    
    # Create the random grid
    random_grid = {'n_estimators': n_estimators,
                   'max_features': max_features,
                   'max_depth': max_depth,
                   'min_samples_split': min_samples_split,
                   'min_samples_leaf': min_samples_leaf,
                   'bootstrap': bootstrap}

&amp;#x200B;",1,scikit_learn,2020-10-13
9yhcqq,Does sklearn have built-in routines for testing results of LinearRegression()?,Does sklearn have built-in routines for testing results of LinearRegression()?,1,scikit_learn,2020-10-13
9ygvsi,How does fit_transform allow for other data to be processed with the same transformer?,"How does fit\_transform allow for other data to be processed with the same transformer?

&amp;#x200B;

Like here:

&amp;#x200B;

[https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)

&amp;#x200B;

Particularly, since one first calls fit\_transform, then why does it allow one to call transform afterwards and still get the same fit? Like how is this kind of functionality implemented?

&amp;#x200B;

&amp;#x200B;",1,scikit_learn,2020-10-13
9tf8yw,Principal component Analysis: predicting values,"I am attempting to forecast a set of multivariate time series data. I have run a PCA (using the scikit-learn module) and have run an AR(1) auto-regression of the 3 components.

Now that I have the projects component values, how do I recast those components into the original variables, in order to find the projection for those variables?",2,scikit_learn,2020-10-13
9t3xwo,Extract a single stratified part of a dataset,"I have a multi-label dataset with N samples, and I want to take a chunk out to reserve for validation, e.g. reserve k% of the dataset.

Note that I want to do this just once, else I could use stratifiedKFold.  
Is there a function to produce such a single chunk, ensuring stratification with respect to  the labels?  
(A workaround would be to produce N\*k  KFold splits, concatenate  all parts but one  for training, and use the last for validation.)

Thanks.",1,scikit_learn,2020-10-13
9sbqvm,Stepping through each iteration of the LogisticRegression fit() function,"Hello guys,

I'm using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class to find the decision functions between my classes. I wanted to ask you - *how can I step through each step of the algorithm?* I know I can give the parameter `max_iter` to determine the number of iterations, but I want to step through each of those `max_iter` iterations - to see how the values of the coefficients change.

Thanks in advance!",1,scikit_learn,2020-10-13
9pcack,"modAL: A modular active learning framework for Python, built on top of scikit-learn",,4,scikit_learn,2020-10-13
8ndofu,Parse Twitter feed and suggest domain names • r/nltk,"I'm working on a hackathon, and I'd like to parse a user's last 100 tweets or so and make recommendations for a domain name using a new TLD.

The plan I've got in my head is

1\) Scrape twitter for a bit and get some data \(How much? How many records?\)

2\) Run tf\-idf against it, save that dataset

3\) split the initial twitter data into groups based on which tweets contain each TLD \- supplies, computer, kitchen, etc.

a\) Run some kind of clustering algorithm against each set? 250 or so TLDs

\-\- This is where I have questions

4\) Scrape their twitter feed and get 100 tweets

5\) Use the tf\-idf data from step 2 to spit out keywords

6\) use those keywords using some kind of distance formula against the clustered data to pick a tld?

7\) use the bigrams or keywords to make up an SLD.

This seemed off to a good start, but can I somehow pickle the cluster results? Or have multiple sets of cluster results in the same object?

Note: 95&amp;#37; of my knowledge on this topic comes from this blog post: [http://brandonrose.org/clustering](http://brandonrose.org/clustering)",2,scikit_learn,2020-10-13
8l4imo,"move partial of decision models from server to client - side, is it good idea?","Hi,
Some time ago tenser flow for js was released. I'm wondering about build bridge for some scikit learn models to move some part of learning and prediction to the client side. I think that it could help minor companies reduce server resource usage and make models and prediction much more personalised. Do you think it's a good idea? Do you know whether someone has tried something similar before?",1,scikit_learn,2020-10-13
8emc3b,How to combine num values with text data for classification?,I build website classifier and use text of each webpage (transformed to bag of words) as train data. But I also want to add each website's PageRank as feature. How can I do that?,2,scikit_learn,2020-10-13
89u8tj,PLSRegression Issues,"I'm working with scikit's cross\_decomposition.PLSRegression(). According to their documentation, x = np.multiply( x\_scores\_, x\_loadings\_.T ). I'm not getting anything close to the same value values. I've tried every combination of using scale=False and sklearn.preprocessing.scale(x) to try and find how this works out, but I haven't been able to find one that works.

    plsr = PLSRegression(n_components=x_df.shape[0]-1).fit(x_df.T,y)
    print(np.matmul(plsr.x_scores_,plsr.x_loadings_.T).T)
    print(x_df)

Using scaled data (i.e. PLSRegression.fit( scale(x\_df).T, scale(y) ) and changing n\_components doesn't help either. If anyone has any idea of what mistake I have made, or if this just a bug in sklearn?",1,scikit_learn,2020-10-13
81xksg,How to remove terms from a term-document matrix?,"Hello,

I have a term document matrix that I've created using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) like so:
    
    X = vectorizer.fit_transform(corpus)
    X
    &lt;1000x10022 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    	with 94340 stored elements in Compressed Sparse Row format&gt;

I'd now like to remove any terms that do not appear in at least 3 documents, and then calculate the TF-IDF scores for each term, and select the vocabulary as the top n terms ordered by TF-IDF scores.

Is there an easy way of removing terms from the term document matrix that do not appear in at least 3 documents, while still conserving the mapping from feature names to feature indices?

I guess one way to do it would be to get the feature names of the terms that appear in at least 3 documents using numpy on the sparse matrix directly, assign them a mapping to indices, and then pass that mapping to the vocabulary parameter in the CountVectorizer constructor.

Any ideas on how to do this more easily?",1,scikit_learn,2020-10-13
7zfjba,How to use partial_fit to update the model trained with fit() instead of training from scratch,"I tried partial_fit with various scikit online learning classifiers like perceptron, passive aggresive classifiers, SGDclassifer... like here: https://ideone.com/uOtRTZ.  I just dont understand why i cant train the new data on top of already trained data. I am doing image classification. I have trained my 10,000 images with fit(). Now i got 1 new image to add to this dataset of already trained images. I want to update the trained model instead of training all 10,001. Is this possible with partial_fit() ? If so, please tell me how ? ",1,scikit_learn,2020-10-13
7z4qi7,SGDClassifier.partial_fit returns error of “classes should include labels”,"I tried to predict label of my newly added data through SGDClassifer.partial_fit as below:

        from sklearn import neighbors, linear_model
    import numpy as np
    
    
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""hinge"")#loss
    
        sgd_clf.fit(X, y)
    
        print(sgd_clf.predict([[6, 9]]))
    
        X.append([6, 9])
        y.append(5)
    
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here

However, this results in error: ValueError: `classes=array([1, 2, 3, 4, 5])` is not the same as on last call to partial_fit, was: array([1, 2, 3, 4])

Any ideas or references ? ",1,scikit_learn,2020-10-13
7vw4ap,Retrain a KNN classified model (scikit),"I trianed my knn classifer over multiple images and saved the model. I am getting some new images to train. I dont want to retrain the already existing model.

How to add the newly tranied model to the existing saved model ?

Could someone guide if this is possible or any articles describing the same ?

Thank you,",2,scikit_learn,2020-10-13
70m5l1,How do I add matplotlib to a django webapp and display the code's output on the webpage?,Trying to make a User Interface for a Support Vector Machine from the SVM function in the matplotlib,1,scikit_learn,2020-10-13
6lec9n,"K-NN and custom metrics, speed up sklearn using Cython",,2,scikit_learn,2020-10-13
6k5uvu,Build my first CART based algorithm feedback is welcome!,hey guys! i just made this: https://github.com/lucas-aragno/pokemon-classifier im pretty new to scikit so I'll appreciate any kind of feedback :),2,scikit_learn,2020-10-13
6ev2y7,FastICA,"It seems like all of the examples using fastICA involves taking 2 frequencies, mixing them a certain way, then unmixing them.

What about if I have a wav file. How can I use fastICA to break it down into multiple parts?

Any help would be appreciated. Thank you!",1,scikit_learn,2020-10-13
6eu5cr,Automate your Machine Learning in Python – TPOT and Genetic Algorithms,,2,scikit_learn,2020-10-13
69huhq,[P] Tracking and reproducibility in data projects (CLI tool),,1,scikit_learn,2020-10-13
5xk0iy,Scikit learn vs Open Cv for small problems in image processing,I am a Image processing noob. I've used Numpy and Scipy for some matrix related stuff before and OpenCV for some image processing problems. I recently learned that scipy lets me manipulate images too. What are the pros and cons of using OpenCv and Scipy I am not able to figure out which would be better for me. Appreciate your help!,5,scikit_learn,2020-10-13
5rb5ww,Using Category Encoders library in Scikit-learn,,1,scikit_learn,2020-10-13
5m0352,MLPClassifier: Multiple output activation,"I'm using MLPClassifier but some of the outputs have more than one activation, i.e. [0 1 1 0].
How can I get only one activation?

My code is:
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                    hidden_layer_sizes=(15,), random_state=1, activation='relu')

Thank you!",1,scikit_learn,2020-10-13
5fasve,Need help on scikit kfold validation,"Objective: To create 5 folds of training and test dataset using StratifiedKFold method. I have referred the documentation at http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.cross_validation.StratifiedKFold.html

I am able to print the indices alright but am unable to generate the actual folds. Here follows my code

from sklearn.cross_validation import StratifiedKFold
import pandas as pd
df=pd.read_csv('C:\Comb_features_to_be_used.txt')

##Getting only numeric columns
p_input=df._get_numeric_data()
## Considering all the features except labels
p_input_features = p_input.drop('labels',axis=1)
## Considering only labels [single column]
p_input_label = p_input['labels']
skf = StratifiedKFold(p_input_label, n_folds=5, shuffle=True)
i={1,2,3,4,5}
for i,(train_index, test_index) in enumerate(skf):
    ##print(""TRAIN:"", train_index, ""TEST:"", test_index)
    p_input_features_train = p_input_features[train_index] 
    p_input_features_test =  p_input_features[test_index]
        
I am getting the error: IndexError: indices are out-of-bounds

",2,scikit_learn,2020-10-13
54exlw,scikit-learn doc translation,"translate sklearn doc to chinese
feel free to join us
https://github.com/lzjqsdd/scikit-learn-doc-cn",2,scikit_learn,2020-10-13
52b7od,Improving the Interpretation of Topic Models,,1,scikit_learn,2020-10-13
50qfgg,Topic Modeling with Scikit Learn,,3,scikit_learn,2020-10-13
4rjkcq,Overfit Random Forest,"I have data where Random Forest models overfit to noise whatever 
hyperparameter I put.
(= excellent accuracy on training, but poor accuracy on prediction).


So, this is the process I did to over-come:
    1) Tweak the input data and reduce the sampling of noise (negative example)

    2) Fit the RF and test (confusion matrix) on cross-validation data. 

    3) Repeat it and choose the best cross validation data.

Is there a way to overcome this monte carlo approach,
using OOBag process during training ?

Also incorporate Cross validation to reduce the over-fitting ?

Importance features change every time a new RF is fit (it seems a lot of co-linearity and noise into the data).










",1,scikit_learn,2020-10-13
4h6ypj,Building scikit-learn transformers,,3,scikit_learn,2020-10-13
3zvwqw,Hello everyone! I want to write an oversampling module in compliance with scikit-learn. Advice needed!,"As mentioned in the title I want to write a module for oversampling classes in skewed datasets. I recently came to need such a module and I noticed that no such thing exists officialy in scikit-learn. I want it to be compatible with scikit-learn as I very often use it. Do you have any resources to redirect me to, apart from the official scikit-learn developer guidelines? Any tips for writing a python module in general?

Thanks in advance!",2,scikit_learn,2020-10-13
2f830i,Official Scikit-Learn page.,,1,scikit_learn,2020-10-13
j9q6bp,A scikit-learn compatible library to construct and benchmark rule-based systems that are designed by humans,,7,scikit_learn,2020-10-13
j93854,Best performance on Scikit-learn’s load_digits dataset,"On Scikit-learn’s load_digits dataset:
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html

Does anyone know what is the best performance achieved so far on this dataset? 

I tried googling around but can’t find examples with 100% score performance. I am thinking since this is a standard dataset, it would be easy to get a 100% score performance?",1,scikit_learn,2020-10-13
j8vepi,"could someone ELi5 the hyperparameters (penalty, C, tol, max_inter)",I am currently working on a beginner project on logistic regression using scikit\_learn. I am trying to fine tune my regression model but cant seem to find any websites that can explain what the parameters mentioned in the title mean exactly and how to use them. I was wondering if anyone could give me a quick explanation on what/how to use these parameters to fine tune my regression model.,0,scikit_learn,2020-10-13
j7jz85,SVC rbf kernel seems to be nonstandard?,"I am currently testing a precomputed version of rbf I implemented to get a better feel for how it works and possibly later check out some other kernels.

It seems that whatever I do, I get different results using my precomputed gram matrix vs using the scikit rbf kernel:

To calculate a kernel entry for datapoints xm &amp; xn, based on some extra parameters theta

`k = thetas[0] * np.exp(-(thetas[1]/2.) * (np.sqrt((xn-xm).T @ (xn-xm)))) + thetas[2] + thetas[3] * (xn.T @ xm)`

using theta = \[1,2,0,0\]

This should recover the formulation given [here](https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel) (setting gamma=1)

1 \* exp( - 2/2|xn-xm|^(2) )   


is there something I'm missing? \[here's the code if you wanna take a look\]([https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit\_test.py](https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit_test.py)) (only dependencies are matplotlib, scikit &amp; numpy, so you're probably good if you're on this sub)",1,scikit_learn,2020-10-13
j5pcm6,ImportError,"I've got a long error which ends with:  
ImportError: DLL load failed while importing \_arpack: Não foi possível encontrar o procedimento especificado. (rough translation: Unable to find the specified procedure)  


Any idea on the issue? It seems like I have some sort of update issue, but I'm unable to find what.",2,scikit_learn,2020-10-13
j2v11w,"Scikit-learn. In the case of a single point, k-nearest neighbours predictions doesn’t literally match with the literally nearest point. I think I know why. Correct me if I’m wrong.","Hello.  I’ve looked at the source code. 

Case population sizes in the range 10 ^ 2 to 10 ^ 5 ish. Vanilla, straight out the box knn from scikit-learn.  Except 1 nearest neighbours not the default 5.  

When I try to predict the nearest neighbour of a point, using 1 nearest neighbours. after using knn.fit to make a model, it doesn’t _always_ return the same value of the actual nearest neighbour.  I’ve worked out the actual real nearest neighbour myself to check, using trig, and unit tested it.  

I think that’s because for pragmatic reasons knn is just a probabilistic model applied at group level.  Not exactly the actual knn for each and every point.  

Am I right?

EDIT:  My. Trig. Was.  Wrong.  Due. To.  A. Data frame. Handling.  Issue.  Ggaaahhhh.",5,scikit_learn,2020-10-13
j1idtx,RadomizedSearch CV taking forever,"Hi ,

I have the below snippet.

Trying to run on GCP . its getting stuck and not even updating.

&amp;#x200B;

https://preview.redd.it/bp2zfi71sxp51.png?width=1463&amp;format=png&amp;auto=webp&amp;s=6d97d1ff6083f6eb65e62d983770c69f06d45f4c",2,scikit_learn,2020-10-13
iv8jv9,Neuraxle - a Sklearn-Based Clean Machine Learning Framework,,1,scikit_learn,2020-10-13
it82un,How the 'init' parameter of GradientBoostingRegressor works?,"i'm trying to create an ensemble of an determined regressor, with this in mind i've searched for some way to use the sklearn already existing ensemble methods, and try to change the base estimator of the ensemble. the bagging documentation is clear because it says that you can change the base estimator by passing your regressor as parameter to ""base_estimator"", but with GradientBoosting you can pass a regressor in the ""init"" parameter. my question is: passing my regressor in the init parameter of the GradientBoosting, will make it use the regressor i've specified as base estimator instead of trees? the documentation says that the init value must be ""An estimator object that is used to compute the initial predictions"", so i dont know if the estimator i'll pass in init will be the one used in fact as the weak learner to be enhanced by the bosting method, or it will just be used at the beginning and after that all the work is done by decision trees. If someone can help me with this question i would be grateful.",4,scikit_learn,2020-10-13
igtkry,Best way to get T-Stastic and P-value etc?,"I'm using scikit learn for linear regression.  Is there a way to use that library to generate things like T-Stastic and p-value and standard error etc?

On stack overflow i found this, but wondering if there's a way within scikit

    import statsmodels.api as sm
    from scipy import stats
    X2 = sm.add_constant(X)
    est = sm.OLS(y, X2)
    est2 = est.fit()
    print(est2.summary())

&amp;#x200B;",1,scikit_learn,2020-10-13
i4ulg2,"Data Visualization using ""Python"" with ""Seaborn"" | Part- I",https://youtu.be/X400eIcV-So,3,scikit_learn,2020-10-13
i3546z,Recommendation based on other user following,"Hello,

I try to build a recommendation system.

My service allow users to follow people (not rate them, just follow) and I would like to be able to propose to users to follow people based on other user’s database activity.

Is scikit a good path for this ? 

Do you recommend specific method or useful ressource to read to achieve this ?

For your help guys!",2,scikit_learn,2020-10-13
hzw282,How to use TensorFlow Object detection API to detect objects in live feed of webcam in real-time,,1,scikit_learn,2020-10-13
hwmcf1,sklearn CCA - how to get variance explained for first canonical relationship?,"Hi. I'm exploring multivariate brain-behaviour relationships with sklearn's canonical correlation analysis tool ([https://scikit-learn.org/stable/modules/generated/sklearn.cross\_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca)). I am interested mostly in the first canonical relationship between the two datasets. The decomposition is working fine and i have the weights/canonical scores etcetera - but what i'd really like to know is how much of the variance in either dataset is explained by that one relationship (analogous to eg variance explained by first principal component).

There is a method named 'score' that i can call on the CCA object but I am not quite sure this is what I need. This score is not the same as 'canonical scores above but will supposedly get some coefficient of determination r\^2 between 'observed' and 'predicted' - not sure how to understand this. The description on the webpage is quite terse and it does not behave the way i might expect.

I'm hoping to find someone who might know whether that 'score' method  will get me to what i want - and if so, maybe how to use it. Or point me otherwise in the right direction to get into the variance explained for CCA.

Cheers!",2,scikit_learn,2020-10-13
hu6y83,KMeans Algorithm Question,"Hey all.

I am new with using scikit-learn and had a question regarding the KMeans algorithm functions. After running the algorithm and plotting the clusters, are the clusters with the centroids plotted the final clusters after training is done or is there training that I have to do on the clusters? 

Thanks everyone",1,scikit_learn,2020-10-13
htugnl,"How to handle ""Missing Value"" from ""Dataset"" using ""Pandas"" &amp; ""Sci-Kit Learn""??",https://youtu.be/8IORSsZIyIQ,0,scikit_learn,2020-10-13
htmc59,"How to handle ""Text"" and ""Categorical Attributes"" using Python and Pandas??",https://youtu.be/4sO7Pezlegk,0,scikit_learn,2020-10-13
ht4ol1,Making ROC curves with results from cross_validate?,"I am running 5 fold cross validation with a random forest as such:

from sklearn.ensemble import RandomForestClassifier

from sklearn.model\_selection import cross\_validate

forest = RandomForestClassifier(n\_estimators=100, max\_depth=8, max\_features=6)

cv\_results = cross\_validate(forest, X, y, cv=5, scoring=scoring)

However, I want to plot the ROC curves for the 5 outputs on one graph. The documentation only provides an example to plot the roc curve with cross validation when specifically using StratifiedKFold cross validation (see documentation here: [https://scikit-learn.org/stable/auto\_examples/model\_selection/plot\_roc\_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py))

I tried tweeking the code to make it work for cross\_validate but to no avail.

How do I make a ROC curve with the 5 results from the cross\_validate output being plotted on a single graph?

Thanks in advance",2,scikit_learn,2020-10-13
hm6td7,Best performance on MNIST - Fashion dataset,Does anyone know what is the best performance achieved so far for the MNIST - Fashion dataset along with what model that was used?,1,scikit_learn,2020-10-13
hm22vz,"How to ""Predict"" my friends weight using ""Machine Learning"" and ""Sci-Kit Learn""??",https://youtu.be/A4JwnkTFEXI,2,scikit_learn,2020-10-13
hl4k0u,Factor analysis “model” in CS229,"In one of Stanford’s CS229 lecture by Andrew Ng (https://m.youtube.com/watch?v=tw6cmL5STuY), he talks about a factor analysis “model” in which is to deal with situations where you have a lot more features than samples in your dataset. He even said he used a modified version of this factor analysis “model” in some recent work he did for a manufacturing company in the lecture.

Now my understanding of factor analysis is just a dimension reduction technique. So how did Andrew used factor analysis to build a “model” which deals with datasets which has a lot more features than samples?",2,scikit_learn,2020-10-13
hkm9qn,StackingRegressor Inconsistent Output,"Is it intentional that StackingRegressor returns different accuracy outputs when running multiple times given the same parameters, models and using numpy set seed?",1,scikit_learn,2020-10-13
hjctba,"This lecture that talks about what Multilabel and Multioutput classifications are, along with their implementation using scikit learn.",,1,scikit_learn,2020-10-13
hg5j3s,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,2,scikit_learn,2020-10-13
hf60u0,"Hey guys, here is a lecture on how to implement gradient descent with scikit-learn. Enjoy :)",,1,scikit_learn,2020-10-13
hakk07,How do I create a linear regression for this groupedby dataframe?,"I have this assignment for a job interview and I really want to impress by using some machine learning. I don't know too much about it and I essentially don't have much time to learn that much about it. I have the following [dataframe](https://postimg.cc/0zNbyrV8) and I want to create a linear regression using scikitlearn of \['profit'\] vs \['dateReceived'\] for each \['Language'\]. 

Does anyone know what I can do for that to work? I guess it should be just a few lines of code, but I could be wrong?",0,scikit_learn,2020-10-13
h7ay1o,"Visualize Scikit-learn models – ROC, PR curves, confusion matrices etc",,6,scikit_learn,2020-10-13
h172dr,Scikit Learn Tutorial in One Hour,,6,scikit_learn,2020-10-13
h0w3xx,Books about classification algorithms,"Hi all,

I am completely new to data mining and have to write a seminar paper about classification and do some programming in python.

With the help of datacamp I was able to implement the classification algorithms in python.

Now I am looking for some sources that I can cite in my paper that briefly explain these algorithms.

My problem is that most books that I have look into so far are very mathematical and since I don’t have a data mining/computer science background, they are hard to understand.

Do you have any recommendations for some text books that explain classification algorithms such as SVM, Naive Bayes, Trees, etc. that are well recognized, but explain them in an easy way?

Many thanks in advance!",1,scikit_learn,2020-10-13
gziaus,How to choose best pair of random state and class label values?,"For the last few days, I was trying to implement the KMeans algorithm using SciKit Learn, But I came across a very confusing problem. I have a dataset that has two class labels ['ALL', 'AML'] where ALL has 47 and AML has 25 samples and 100 attributes to train from and now I want to use this dataset for KMeans clustering so that I can compare the predicted results with the original class labels. Before asking my question let me explain certain scenarios. In all the scenarios I have taken all the 100 attributes to fit the model.

Scenario 1:

In the first run, I started with a model that is created with pretty much default arguments i.e. model = KMeans(n_clusters=2). For comparing the predicted class labels(which are numeric) with the original labels(which are strings), I set the original class labels as ALL = 1 and AML = 0. After that, while comparing using a classification report I got an average accuracy of 35%. Then I run the algorithm once again and got an accuracy of 44%. For the third try, I got 33% and so on.

However, I looked about it and came to know that the random_state argument needs to have a fixed value to get same accuracy throughout all runs.

Scenario 2:

After knowing about random_state, this time I started with random state 0 and created the model as model = KMeans(n_clusters=2, random_state=0) and kept the original class labels as before i.e ALL as 1 and AML as 0. However, this time the output didn't change on different runs and I got an accuracy of 53%. But, out of curiosity, I swap the original class label i.e. I set ALL as 0 and AML as 1 which results in 47%.

Scenario 3:

This time I choosed random_state as 1 i.e. model = KMeans(n_cluster=2, random_state=1) and having ALL as 0 and AML as 1 gave 67% accuracy while considering ALL as 1 and AML as 0 gave 33% accuracy.

So, My question is what I am doing wrong here? Am I implementing something wrong? If I am right then why the result is changing so much depending on random_state and class labels? What's the solution and how to choose the best pair of random_state and class labels?",1,scikit_learn,2020-10-13
gwgzy9,estimate_transform works when using 'similar' but not when using 'affine',"I have two 512x512 grayscales images (src and dst). To try to understand estimate transform I applied the following transformation

    tform = transform.AffineTransform(scale=(1.3, 1.1), 
                                        rotation=0.5, 
                                        translation=(0, -200)) 

to the src to create the dst. Then I want to find back the parameters using estimate\_transform.

With the parameter 'similar' I obtain parameters very close to the one I used (as expected). But when I want to use 'affine', I obtain the following error :

     matmul: Input operand 1 has a mismatch in its core dimension 0, 
    with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 513 is different from 3) 

Any idea why ? Here is my code :

    src = rgb2gray(data.astronaut())
    dst = rgb2gray(data.astronaut())
    tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5,
                                      translation=(0, -200))
    dst = transform.warp(img1, tform)
    tform_fin = transform.estimate_transform('affine', src, dst)
    dst_corr = transform.warp(img3, tform.inverse)",1,scikit_learn,2020-10-13
gtw4p9,What can I do when I keep exceeding memory used while using Dask-ML,"I am using Dask-ML to run some code which uses quite a bit of RAM memory during training. The training dataset itself is not large but it's during training which uses a fair bit of RAM memory. I keep getting the following error message, even though I have tried using different values for n_jobs:

```
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
```

What can I do?

Ps: I have also tried using Kaggle Kernel (which allows up to 16GB RAM) and this didn't work. So I am trying Dask-ML now. I am also just connected to the Dask cluster using its default parameter values, with the code below:

```
from dask.distributed import Client
import joblib

client = Client()

with joblib.parallel_backend('dask'):
    # My own codes
```",1,scikit_learn,2020-10-13
gsx926,MLPRegressor newby with some (probably very basic) questions in need of some assitance,"Hello!

I'm building MLPRegressor for the first time ever (I've been learning how to code with online courses since end of March) and I know something is wrong but I don't know what. Bellow you can see my code so far. It runs and I have a value for r2 ( -9035355.06 ) and a plot. However the r2 score doesn't make sense (it should be around 0.7)  and the plot doesn't make sense either.

I have run this analysis with SPSS multilayer perceptron feature so I know more or less how my results should be and that's why I know whatever I am doing with python is wrong.

Any advice/suggestion of what I'm doing wrong is very welcome! This coding world is kinda of frustrating for me:/

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    
    from sklearn import neighbors, datasets, preprocessing 
    from sklearn.model_selection import train_test_split
    from sklearn.neural_network import MLPRegressor
    from sklearn.metrics import r2_score
    
    vhdata = pd.read_csv('vhrawdata.csv')
    vhdata.head()
    
    X = vhdata[['PA NH4', 'PH NH4', 'PA K', 'PH K', 'PA NH4 + PA K', 'PH NH4 + PH K', 'PA IS', 'PH IS']]
    y = vhdata['PMI']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 
    
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_test_norm = scaler.fit_transform(X_test)
    
    nnref = MLPRegressor(hidden_layer_sizes = [4], activation = 'logistic', solver = 'sgd', alpha = 0.1, learning_rate= 'constant',
                         learning_rate_init= 0.6, max_iter=200, random_state=0, momentum= 0.3, nesterovs_momentum= False)
    nnref.fit(X_train_norm, y_train)
    
    y_predictions= nnref.predict(X_test_norm)
    
    print('Accuracy of NN classifier on training set (R2 score): {:.3f}'.format(nnref.score(X_train_norm, y_train)))
    print('Accuracy of NN classifier on test set (R2 score): {:.3f}'.format(nnref.score(X_test_norm, y_test)))
    print('Current loss : {:.2f}'.format(nnref.loss_))
    
    plt.figure()
    plt.scatter(y_test,y_predictions, marker = 'o', color='blue')
    plt.xlabel('PMI expected (hrs)')
    plt.ylabel('PMI predicted (hrs)')
    plt.title('Correlation of PMI predicted by MLP regressor and the actual PMI')
    plt.show()",1,scikit_learn,2020-10-13
gsol3k,What are the default values for the parameters in Dask-ML's Client() function,"I am trying to understand Dask-ML's Client() function parameters. Say I have the following code using Dask-ML's Client() function:

```
from dask.distributed import Client
import joblib

client = Client()
```

If I don't specify any values for the parameters in the Client() function, what are the default values for the parameters:

(i) n_workers

(ii) threads_per_worker

(iii) memory_limit

From my understanding, Python has the Global Interpreter Lock (GIL) feature which prevents multi-threading. If so, why does Dask-ML's Client() function have the parameter threads_per_worker when multi-threading is prevented in Python?

Does memory_limit refers to the maximum memory limit allowed for **each** worker/machine/node or does this refer to the maximum memory limit allowed for **all combined** worker/machine/node?

I have already looked through the documentation in Dask-ML (see here: https://docs.dask.org/en/latest/setup/single-distributed.html), but the documentation is not clear in regards to these questions above.

Thank you in advance if anyone could explain this?",1,scikit_learn,2020-10-13
glb4cy,Why does PolynomialFeatures has multiple pair of coefficient after fitted the data?,"After I create an PolynomialFeatures object, and fit the data by :

[`poly.fit`](https://poly.fit)`(x,)`

I wanted to look for the coefficient, so I do:

`poly.transform(x,y)`

&amp;#x200B;

And it will return an array with (n\_samples, n\_coeff), but why does the polynomial fit with multiple pair of coefficient? Wouldn't the model fit the data and get a final best coefficient?

&amp;#x200B;

And what is the final coefficient that Polynomial get after fitting?",1,scikit_learn,2020-10-13
gi4jw3,How to add sample_weight into a scikit-learn estimator,"I have recently developed a scikit-learn estimator (a classifier) and I am now wanting to add sample_weight to the estimator. The reason is so I could apply boosting (ie. Adaboost) to the estimator (as Adaboost requires sample_weight to be present in the estimator).

I had a look at a few different scikit-learn estimators such as linear regression, logistic regression and SVM, but they all seem to have a different way of adding sample_weight into their estimators and it's not very clear to me:

Linear regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_base.py#L375

Logistic regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_logistic.py#L1459

SVM: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/svm/_base.py#L796

So I am confused now and wanting to know how do I add sample_weight into my estimator? Is there a standard way of doing this in scikit-learn or it just depends on the estimator? Any templates or any examples would really be appreciated. Many thanks in advance.",2,scikit_learn,2020-10-13
get2kh,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,,2,scikit_learn,2020-10-13
gdb7a8,Code a Decision Tree in 20 lines.,,0,scikit_learn,2020-10-13
gd81x4,why does Scikit Learn's Power Transform always transform the data to zero standard deviation?,"all of my input features are positive. Whenever I tried to apply PowerTransformer with box-cox method, the lambdas are s.t. the transformed values have zero variance. i.e. the features become constants

&amp;#x200B;

I even tried with randomly generated log normal data and it still transform the data into zero variance.

&amp;#x200B;

I do understand that mathematically, finding the lambda s.t. the standard deviation is the smallest, would mean the distribution would be the most normal-like.

&amp;#x200B;

But when the standard deviation is zero, then what's the point of using it?

&amp;#x200B;

&amp;#x200B;

p.s. so one of the values of lambda I get by using PowerTranformer is -4.78 

If you apply it into the box-cox equation for lambda != 0.0, then for any input feature y values, you technically get the same values. i.e. (100\^(-4.78)-1.0)/(-4.78) is technically equals to (500\^(-4.78)-1.0)/(-4.78)",2,scikit_learn,2020-10-13
gcvtsm,how to combine recursive feature elimination and grid/random search inside one CV loop?,"I've seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:

[Feature selection and cross-validation](https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751)

[Nested cross-validation and feature selection: when to perform the feature selection?](https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec)

[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)

&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...

[Here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.

However I'd like to do recursive feature elimination inside random/grid CV, so that ""feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)"", so that data from other folds influence neither feature selection nor hyperparameter optimization.

Is this possible natively with sklearn methods and/or pipelines? Basically, I'm trying to find an sklearn native way to do this before I go code it from scratch.",3,scikit_learn,2020-10-13
gc2z28,How to write a scikit-learn estimator in PyTorch,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",3,scikit_learn,2020-10-13
g5ugws,Code a Neural Network in 20 lines.,,2,scikit_learn,2020-10-13
g4yc73,Basic question re: gaussian mixture models,"I wasn't able to find this in the documentation, but is the covariance parameter you access with model.covariances\_ sigma or sigma\^2? Seems like it can be either thing as I've seen the notations N(x| mu, sigma\^2) and N(x|mu, sigma) both used in various places.",1,scikit_learn,2020-10-13
fzm7mm,"Should scikit-learn include an ""Estimated Time to Arrival"" (ETA) feature? Discuss.",,6,scikit_learn,2020-10-13
fx6kdy,Clustering of t-SNE,"Hello,

I have recently tried out t-SNE on the sklearn.datasets.load_digits dataset. Then i applied KNeighborClassifier to it via a GridSearchCV with cv=5.

In the test set (20% of the overall dataset) i get a accuracy of 99%

I dont think i overfitted or smth. t-SNE delivers awesome clusters. Is it common to use them both for classifying? Because the results are really great. I will try to perform it on more data. 

I am just curious on what you (probably much more experienced users than me) think.",1,scikit_learn,2020-10-13
fx0i7x,Search over preprocessing and ensemble hyperparameters?,"In scikit-learn there are some handy tools like `GridSearchCV` for tuning the hyperparameters to a model or pipeline.

Suppose you'd like the preprocessing in your pipeline to include some user-defined options (e.g. whether to encode a certain categorical variable via one-hot encoding or something weird like frequency encoding) and you'd like to include those options among the hyperparameters you're searching over.

Suppose further that you're using an ensemble model -- e.g. a random forest plus few linear regression specifications, and you'd like to tune the hyperparameters for each of them, as well as the voting weight of each.

Does scikit-learn provide a predefined way to search over such spaces? It looks like the parameter space is intended only to dictate the behavior of a single model, not preprocessing steps or ensemble parameters.",1,scikit_learn,2020-10-13
ft2pcp,"How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?","How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?",1,scikit_learn,2020-10-13
ft1kmb,facing an error,"import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

&amp;#x200B;

\# Importing the dataset

dataset = pd.read\_csv('50\_Startups.csv')

X = dataset.iloc\[:, :-1\].values

y = dataset.iloc\[:, 4\].values

X2=dataset.iloc\[:, 3\].values

\# Encoding categorical data

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

le = LabelEncoder()

X2 = le.fit\_transform(X2)

oh = OneHotEncoder(categories = 'X\[:, 3\]')

X= oh.fit\_transform(X).toarray()

&amp;#x200B;

https://preview.redd.it/7bgiwdp238q41.png?width=871&amp;format=png&amp;auto=webp&amp;s=d2386c5cda100706a85803c036586beec8b9e843",1,scikit_learn,2020-10-13
fm1oov,I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?,"I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?

    preprocess =     make_column_transformer((SimpleImputer(strategy='median'), cols_numeric),     
    (SimpleImputer(strategy='constant', fill_value='missing'), cols_onehot),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_target),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_ordinal),     (OneHotEncoder(handle_unknown='ignore'), cols_onehot),     
    (TargetEncoder(), cols_target),     
    (OrdinalEncoder(), cols_ordinal),     
    (StandardScaler(), cols_numeric)) 
    lr_wpipe = make_pipeline(preprocess, LinearRegression()) 
    lr_scores = cross_val_score(lr_wpipe, X_train, y_train) 
    np.mean(lr_scores) 
    print(""Linear Regression R^2: "", lr_scores)",1,scikit_learn,2020-10-13
flg6a1,how to find 'the math' being done in sklearn source code?,"hi.  I'm trying to find where in sklearn the actual math is being done, mostly for my own learning so I can answer questions like 'when using `sklearn.neighbors` , what math is being used to calculate Euclidean distance?'    


If you see here: [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/\_base.py#L360](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/_base.py#L360)  
you'll see that Euclidian and other distance calculations can be *specified*, but I don't see anywhere where the actual math is being done in code.",1,scikit_learn,2020-10-13
fl4fj1,Adding Standard Scaler to GridSearchCV,"I'm looking to use the Standard Scaler as a hyper parameter, i.e check if performance is higher with/without scaling the inputs. In order to tune with other hyperparameters, I would like to incorporate it into my GridSearchCV function (provided by Scikit Learn). Can someone advise me on how to do it?",1,scikit_learn,2020-10-13
ffx9zw,How to use tfidfvectorizer fit_transform for multiple docs,"Hey, 

Let's say my corpus is a list of lists , each of the inner lists represent a parsed doc (each value is a word) 

I want to compute a tf-idf score for my corpus. 

It's seems like the fit-transform function can't use my corpus as its inputs should be itratable with string values (which is each of my docs) 


    V = tfidfvectorizer ()
    For doc in corpus:
       Vectors = v.fit_trabsform(doc)

So my question is, how does it calculate IDF if it get only one doc at a time?",1,scikit_learn,2020-10-13
ff9602,Classifiers' score method clarification,"Hi,

I don't fully understand what the score method of classifiers does. For example, the [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method's documentation says ""Return the mean accuracy on the given test data and labels."" Now, I know what is accuracy: (TP+TN)/(TP+TN+FP+FN), but I don't understand why ""mean"" is in there. Mean over what? of what?  

That is, I give the method as parameters a dataset with true labels, and it can calculate the accuracy from that (given the model), but where does the mean come into place? 

Thanks in advance!",3,scikit_learn,2020-10-13
fbfky4,Is epsilon in dbscan a euclidean measure?,"Hello everyone,
I'm writing yet another dbscan question. For those who are familiar with the inputs to the dbscan, the principal parameters are epsilon and minPts.

Epsilon is the neighborhood radius, and I'm curious if anyone can point me to a reference or tell me if epsilon is a euclidean metric",3,scikit_learn,2020-10-13
f1dizs,Identifying smallest frequently occurring value,"I'm not a data science person, but thinking Scikit learn might be able to help here, and looking for suggestions for ideas I should investigate.

Essentially, I'm looking for a way to consistently identify a baseline power readings. If I have minute by minute power consumption readings from a bunch of electrical motors.  For any motor, we want to identify what a 'baseline' or 'normal unloaded steady-state' power value is.

There is definitely noise in the signal, and not even noise - legitimate power reading that are smaller than what we would consider 'normal unloaded steady state'.  The catch is this could be different for the same motor when production composition changes, so there is not just one value that we can look at historical data to arrive at. (Think motors running pumps moving different fluid mixtures / slurry ad different times.

This does not have to be real-time, just take the dataset of power readings for any motor for any production batch and post-process the data in such a way we can identify times the motor is doing its job at a 'near-idle' state.

Currently we just have a basic calculation that looks at a rolling window of 20 per-minute readings and finds the lowest value that occurs at least twice. (basically throwing out the lowest few outliers)

The reason I'm considering Scikit or similar is we can graph these power readings for a time period (say 1 day) and visually we can easily see these 'baselines' we are looking for.  There will be spikes and dips, and time windows where we are definitely running a heavy load (motors spun up on demand), but we can identify when the mixture changes because the visual changes in this baseline value.

Hope that made at least a little sense, if there are details I can clarify, please ask.  I appreciate everyone's thoughts and ideas!",1,scikit_learn,2020-10-13
eylu4p,What's wrong with Scikit-Learn.,,3,scikit_learn,2020-10-13
ev1as7,Is it possible to use a custom-defined decision tree classifier in Scikit-learn?,"I have a predefined decision tree, which I built from knowledge-based splits, that I want to use to make predictions. I could try to implement a decision tree classifier from scratch, but then I would not be able to use build in Scikit functions like predict. Is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? Or do I need to do something completely different? My first attempt was to use “fake training data” to force the algorithm to build the tree the way I like it, this would end up in a lot of work because I need to create different trees depending on the user input.",1,scikit_learn,2020-10-13
eu9qgc,Is HistGradientBoosting the same as LightGBM or is the SKLearn's version different?,"If so, how?",2,scikit_learn,2020-10-13
em9fxf,Is this the proper way to do ML with scikit_learn?,"I have a dataset with 8 features (numeric) and 1 target (0 or 1).  
 I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc.  
 For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

But sometimes I get these errors:  
 The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

What's the proper way to predict binary values?  
 I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Thanks for your support. I'm a beginner.",2,scikit_learn,2020-10-13
effb98,What module/algorithm should I use in order to predict the time in which a certain action will be completed?,"Basically the title, but to explain it even more :

I have to device a model which will predict the total time a patient will have to wait in a hospital environment. For that, we have a dataset consisting of various patients with several diseases and their time durations already recorded. I want to know which module or algorithm should I use to carry this out? This is my first ML project and I could use any help that you guys can do. Thank you!!",2,scikit_learn,2020-10-13
e0h8ao,Column transformer throwing away some features?,"My data frame  has numerical columns a, b, c. And it has categorial text features d, e, f, g, h.

I build a preprocessor like the following.

    num_features = ['a','b','c']
    nom_features = ['d','e','f','g','h']
    
    preprocesser = ColumnTransformer([
        (""scale_numeric"", StandardScaler, num_features),
        (""encode_nominal"", OneHotEncoder(handle_unknown=""ignore""), nom_features)],
        remainder=""drop""
    )
    
    preprocessor.fit_transform(dataframe)

Since I started with 8 features, after one hot encoding I expected to get 8 or more features back, but the result of `preprocessor.fit_transform(dataframe)` only has 3 columns. Not sure what I am doing wrong if anyone can help me.",1,scikit_learn,2020-10-13
dyzwn9,How to Modify(Make unique) the Scikit-learn Multilayer perseptron algorithm (MLP),"Hi folks,

I've been trying to build a rainfall prediction model for last few days. I've used the **Scikit-learn Multilayer perseptron regressor function** straight up. 

1) The accuracy was OK(78%) but I want to increase it

2) I don't want to use the same predominantly given function (I just want to **add uniqueness in my code**, but I want to use scikit-learn)

Is there any way to modify the function or not use the ready-made function? Can anyone please help me with this?

Thanks in advance!",1,scikit_learn,2020-10-13
dtnh3f,The best alpha for ridge regression is... -85???,,3,scikit_learn,2020-10-13
dtiy98,difference between Kfold.split() and shufflesplit.split() in scikitlearn,"I read this [post](https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv), I get the difference when it comes to computation and shufflesplit randomly sampling the dataset when it creates the testing and training subsets, but in the answer on stackoverflow, there is this paragraph  


""**Difference when doing validation**

In KFold, during each round you will use one fold as the test set and *all* the remaining folds as your training set. However, in ShuffleSplit, during each round **n**  you should *only* use the training and test set from iteration **n** ""

I couldn't quite get it. since in kfold, you're bounded by using the training buckets (k-1) and testing bucket (k) in the **k** iteration and in shufflesplit you use the training and testing subsets made by the shufflesplit object in iteration **n.**  so for me it feels like he's saying the same thing.

can anyone please point out the difference for me?",1,scikit_learn,2020-10-13
dcvd2r,When to use these unsupervised algorithms?,"There are a lot of modules in sklearn. I am interested when these unsupervised algorithmes (bellow )are used.  
When to use a Gaussian mixture model? When to use Manifold Learning, When to Biclustering? etc.

&amp;#x200B;

* [2.1. Gaussian mixture models](https://scikit-learn.org/stable/modules/mixture.html) 
* [2.2. Manifold learning](https://scikit-learn.org/stable/modules/manifold.html)
* [2.4. Biclustering](https://scikit-learn.org/stable/modules/biclustering.html) 
* [2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html) 
* [2.6. Covariance estimation](https://scikit-learn.org/stable/modules/covariance.html) 
* [2.7. Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html) 
* [2.8. Density Estimation](https://scikit-learn.org/stable/modules/density.html)",8,scikit_learn,2020-10-13
dar9z6,pattern recognition on texts that are bash commands or software signature?,"hi all.

so I've got my hands on a daily dose of 100,000 connections per day to our servers, and I've got millions of rows of data that includes commands our users have executed on our servers, (\`cd\`, \`ehlo\`, \`scp ....\`, etc). and I have the same amount of data of their application signatures while connecting. like (Firefox 59, Firefox 60, google chrome),... and user agents, ...

basically all the data one can extract out of a socket or using an IDS.

I like to do some pattern matching on these data. like for the commands they are executing and stuff like that...

so to cluster the commands, I've got commands that look like this:

cd Project

cd Images/personal

cd Project/map

cat /var/log/nginx/web\_ui.log

the problem is, I can just split the texts and take in the first part(cd, cat) and make plot out of the commands, but i really would like to make it more automatic and intelligent. so people who \`cd\` into the \`Project/map\` are distinguished from people who cd into \`Images\` folder. I like to know what people are doing on out servers. so a plot that all people whith \`cd\` commands are close to each other, but are really distinguished for each folder that they have \`cd\` into. 

this is just an example of what I want:)

&amp;#x200B;

turns out that scikit\_learn only works on numbers? how can i utilize it for that kind of data? I don't know if this is a nltk problem?",3,scikit_learn,2020-10-13
d8sxus,Exporting Models to build own inference server,"Hello, I was hoping to get pointed in the right direction.  After training a random forest classifier I am looking to export the model in such a way that I can recreate each of the trees in C++.  I am trying to figure out the best approach to this, or if it is even possible.  My research online mainly shows examples of how to visually represent these, and how to create a pickle project for python serialization.  

Am I missing some key terms in my search? Could you point me to what i should be doing to figure this out?

&amp;#x200B;

My approach so far has been exploring the clf.estimators.trees\_ part of the estimators object, but I am not sure if I am on the right track.

&amp;#x200B;

Any help is much appreciated.

&amp;#x200B;

Thanks!",1,scikit_learn,2020-10-13
d244x4,Predict device from flow,"Hey guys, I applied to a competition about AI and my task is to predict device class from flow. I have 13 types of classes which are all in train set but the test set is missing that one column. After I run training and then I try to predict it, I receive an error stating this: ValueError: query data dimension must match training data dimension.

How can I predict a column that is not there? I don't believe that I have to manually put the column to the test.json 

Thanks for advices.",4,scikit_learn,2020-10-13
cvtjk6,Predicting Churn With Nested Data,"Hello All!

Ok, so this is a bit of a challenge and I'm trying to figure out if it is even worth worrying about the nesting aspect of the data. Basically, I'm trying to predict subscription-level churn with a combination of subscription-level and user-level variables.

Since users own subscriptions I figured I should try to account for nesting in my model. Does anyone have any recommendations on how to attack churn predictions using a nested model? Any suggestions would be greatly appreciated. Again, I have code working, but I've never built anything that requires nested analysis.

Basically my question is: Is it possible to run a multi-level SVM?",2,scikit_learn,2020-10-13
cs2urp,What is the most efficient way to implement two-hot encoding using scikit learn?,"I have two very similar features in my dataframe, and I would like to combine their one-hot encoded versions. They are both categorical data, and they both contain the same categories. I was thinking about using OneHotEncoder from scikit learn and getting the union of the corresponding columns. Is there a function or more efficient way that I do not know about?",3,scikit_learn,2020-10-13
cnnacy,Feature elimination doesn't really eliminate anything.,"I had a fairly simple dataset, after plotting the correlation matrix I noticed that one variable has very low correlation with the target (0.04) but instead of deleting it manually I decided to try feature elimination.
I tried both RFE and RFECV with Logistic Regression as an estimator, RFE eliminated some features which seemed correlated with the output and kept that feature.
RFECV didn't eliminate anything at all.

Am I missing something here?",1,scikit_learn,2020-10-13
cnl2a5,k-means output issue,"Hello I've run a k-means over my voice data. I got two class (for best). My problem is why i got this line at the right side? I sit an issue in my dataset?

https://preview.redd.it/n5lz9pggx7f31.png?width=852&amp;format=png&amp;auto=webp&amp;s=e84cc13f9579c026fc6bfa5cbd85849b1ea2939e",1,scikit_learn,2020-10-13
cmmbi5,Running scikit validation on 24 cores is slow?,"Hello guys, maybe anyone can help me out here. I am running following validation code:

```
from sklearn.linear_model import LinearRegression
model = LinearRegression()
from sklearn.preprocessing import PolynomialFeatures
poly_transformer = PolynomialFeatures(degree=2, include_bias=False)
from sklearn.pipeline import Pipeline
pipeline = Pipeline([('poly', poly_transformer), ('reg', model)])
train_scores, valid_scores = validation_curve(estimator=pipeline, # estimator (pipeline) X=features, # features matrix y=target, # target vector param_name='pca__n_components', param_range=range(1,50), # test these k-values cv=5, # 5-fold cross-validation scoring='neg_mean_absolute_error') # use negative validation
```

in the same .py file on different machines, which I would name #1 localhost, #2 staging, #3 live, #4 live. localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds
live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.
In order to get more ""trustworthy"" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different?",1,scikit_learn,2020-10-13
clz2bl,vectorization,"Hi, I just want to know if I can vectorize a text even if its on another language using Count Vectorization",2,scikit_learn,2020-10-13
clpubv,Machine learning final year project,"design and implement an intelligent agent that can detect a fault and can trouble a faulty server on a network

Its a network anormaly project 
But dont know where to start from",1,scikit_learn,2020-10-13
cl88rf,No Scikit-learn after I installed Anaconda in Sublime Text 3," 

I started using Sublime Text as my Text Editor/IDE (not sure what the difference is) to do some Python projects. After watching 2 episodes of the machine learning course by Google Developers. I installed the Anaconda package which has the Scikit-learn included.

Following the video I typed:

    import sklearn

This error appeared:

    ModuleNotFoundError: No module named 'sklearn'

Is there a way to install the Scikit-learn using the Sublime Text 3 or using a different method?",1,scikit_learn,2020-10-13
cgijm0,Unable to find/import,"edit: Title - Unable to find/import IterativeImputer

&amp;#x200B;

&amp;#x200B;

Hello fellow users, I'm wondering if yall could help me out with importing/finding IterativeImputer...

**&gt;&gt;&gt;** *# explicitly require this experimental feature*

**&gt;&gt;&gt; from** **sklearn.experimental** **import** enable\_iterative\_imputer *# noqa*

**&gt;&gt;&gt;** *# now you can import normally from impute*

**&gt;&gt;&gt; from** **sklearn.impute** **import** IterativeImputer

**ModuleNotFoundError**: No module named 'sklearn.impute.\_iterative'; 'sklearn.impute' is not a package

&amp;#x200B;

$pip freeze states I have scikit-learn==0.21.2 and sklearn==0.0

Python version 3.6

&amp;#x200B;

After researching the issue online I see that there's an experimental version I need to install, but I can't seem to find it! Further, I can't find it on their website.. [https://scikit-learn.org/dev/versions.html](https://scikit-learn.org/dev/versions.html)

What did I overlook/miss?",1,scikit_learn,2020-10-13
cc2mxr,How to re-structure a numpy dataframe into a format I can use in sklearn?,"Assuming the dataframe column 0 is the target and columns 1: are the features, and that each column is named, what's the easiest way to split the data for use in sklearn?",1,scikit_learn,2020-10-13
cbm4g6,How to classify dots,"Hello, 

I have a graph with two groups, red and blue dots. These groups are clearly separated, but the problem is that I want to say if a new dot belongs to the red group, to the blue, or to none of them.

What method do you recommend?

Thank you",1,scikit_learn,2020-10-13
c4rlf7,Regression is not yielding many useful predictions,"Hello all,   


I'm using a linear regression to predict continuous values (how long until a client churns measured in months). I have a dataset of cancelled accounts and active account. I'm using the cancelled accounts to predict the active accounts. I have a variety of explanatory variables and in total I have an R-squared of around 35% (obviously R-squared isn't perfect)   


Overall, this works pretty well; however, one issue I'm running into is that, of the predictions I get back, some are negative and very few actually predict that these active clients should still be active. In other words, many of the predicted cancellation dates are in the past.  


Dumb question, but is there a method I could be using to help this? Overall, I'm getting about 10k useful observations from 60k predictions.  Any suggestions would be greatly appreciated.",1,scikit_learn,2020-10-13
c4q5i6,I can't import Kmeans into compiler,"I'm currently using sklearn 0.21.2, and when I do:  


`import sklearn.cluster.KMeans`

&amp;#x200B;

the compiler returns error:

&amp;#x200B;

`no module named sklearn.cluster.KMeans`

&amp;#x200B;

I've found that in the cluster package, there is an module named 'cluster.k\_means\_'  


But when I tried to use this instead, it shows error

&amp;#x200B;

`Module is not callable`

&amp;#x200B;

Now I don't know why I can't import the kmeans package in cluster.",1,scikit_learn,2020-10-13
bylpjd,Sklearn regression with two datasets,"Hello all,   


basically, as the title implies I'm trying to train a regression model on one dataset and the apply that predictive model to another dataset. In other words, I have a model which predicts cancelled accounts and the amount of time in which those accounts cancel.   


I have another dataset full of active accounts (with the same variables) and I'm attempting to use the model from the cancelled accounts to predict when my active accounts will cancel. I'm having trouble with this.  Is there a way to do this without forcing a t  


Is there a way to use the ""active dataset"" without enforcing a Train\_test\_split? Any help would be greatly appreciated. Thank you!",2,scikit_learn,2020-10-13
bvjxzj,Get the function that fits my data,"I have fit a polynomial regressor to a two dimensional data. 
Is there a way to see the function that fits this data?",2,scikit_learn,2020-10-13
bqtxes,Kmeans clustering cache the result,"Hello,

&amp;#x200B;

I am new to scikit and I was wondering if I could cache the result of Kmeans so next time when I run my script I do not create the centroids again - that means save the result of [`kmeans.fit`](https://kmeans.fit)`()`.",2,scikit_learn,2020-10-13
bp7dv5,Get classes name of each estimator in OneVsOneClassifier,"Are there any ways to do that ? I am trying to directly access the classes\_ attributes in the estimator but it only returning \[0,1\]",2,scikit_learn,2020-10-13
bevq9d,Using Blob Detection methods on huge images,"I'm trying to use common blob detection methods from

[https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob\_dog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog)

on a huge images (about 6000x6000 pixels). It takes way too long to compute and show the result. How could I resolve this?",1,scikit_learn,2020-10-13
bcwbv4,Calculate variance of accuracy,"Hello, how can I calculate the variance of accuracy between two models in Random forest.
I mean I made a simple model with DecisionTreeClassifier() and one more with BagginClassofier() using the first model on it.
The accuracy climb +0.237.

How to get variance of that accuracy?
Thansk",1,scikit_learn,2020-10-13
bcbduy,Classification: Minimizing the amount of false positives,"Hey there,

I posted an earlier post (now deleted) that phrased this a bit wrong (thanks Imericle). Here is another try: 

Many  (most?) classification algorithm seem to be about maximizing accuracy  (true positives + negatives). My aim is to minimize the amount of false positives. How would I achieve this?

Only options I see to achieve this is through parameters tuning, is that the right approach?

(Thinking on applying it to a RandomForest),

Thanks,

Bb",2,scikit_learn,2020-10-13
bbxi9t,KMeans: Extracting the parameters/rules that fill up the clusters,"Hi all,

&amp;#x200B;

I have created a 4-cluster k-means customer segmentation in scikit learn. The idea is that every month, the business gets an overview of the shifts in size of our customers in each cluster. 

My question is how to make these clusters 'durable'. If I rerun my script with updated data, the 'boundaries' of the clusters may slightly shift, but I want to keep the old clusters (even though they fit the data slightly worse). My guess is that there should be a way to extract the paramaters that decides which case goes to their respective cluster, but I haven't found the solution yet. 

I would appreciate any help",1,scikit_learn,2020-10-13
b6nfvs,Question about FeatureUnion,"    pipe = Pipeline([
            ('features', FeatureUnion([
                    ('feature_one', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_one')),
                        ('vec', cvec) # Count vectorizer
                    ])),
                    ('feature_two', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_two')),
                        ('vec', tfidf) # Tf-idf vectorizer
                    ]))
                ])),
            ('clf', OneVsRestClassifier(clf)) #clf is a support vector machine
        ])

I'm using this pipeline for a project I'm working on, and I just want to make sure I understand how FeatureUnion works. I'm building a classifier which takes in two different text features and attempts to make a multi-class classification.

&amp;#x200B;

To give a little more detail, I'm trying to classify news articles into one of several categories (sports, business, etc.) Feature one is a list of tokens taken from the article's url, which often, though not always, explicitly states the name of the topic. Feature two is a list of tokens from the body of the article.

&amp;#x200B;

Does it make sense to separate the two features this way? Does this have a different effect than if I had just merged all of the tokens into a single list and vectorized them? My intention was to allow the two features to effect the model to different degrees, since I figured one would be more predictive in most scenarios (and I am getting pretty great results.)",2,scikit_learn,2020-10-13
b36h5a,Ranforest random behaviour,"If I give random forest parameters as RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019) Should it be creating 10 same decision trees? But it is not. I am asking the random forest to
     1.Sample without replacement (bootstrap=False) and each tree have same number of sample (ie the total data )(verified using plot)
     2.Select all features in all trees.
But model.estimators_[2] and model.estimators_[5] are different

",2,scikit_learn,2020-10-13
axgj2c,Predicting the runtime of scikit-learn algorithms,"Hey guys,

We're two friend who met in college and learned Python together, we co-created a package which can provide an estimate for the training time of scikit-learn algorithms.

The main function in this package is called “time”. Given a matrix vector X, the estimated vector Y along with the Scikit Learn model of your choice, time will output both the estimated time and its confidence interval. 

Let’s say you wanted to train a kmeans clustering for example, given an input matrix X. Here’s how you would compute the runtime estimate:

    From sklearn.clusters import KMeans
    from scitime import Estimator 
    kmeans = KMeans()
    estimator = Estimator(verbose=3) 
    # Run the estimation
    estimation, lower_bound, upper_bound = estimator.time(kmeans, X)

We are able to predict the runtime to fit by using our own estimator, we call it meta algorithm (meta\_algo), whose weights are stored in a dedicated pickle file in the package metadata.

The meta algos estimate the time to fit using a set of ‘meta’ features, including the parameters of the algo itself (in this case kmeans) and also external parameters such as cpu, memory or number of rows/columns. 

We built these meta algos by generating the data ourselves using a combination of computers and VM hardwares to simulate what the training time would be on the different systems, circling through different values of the parameters of the algo and dataset sizes . 

Check it out! https://github.com/nathan-toubiana/scitime

Any feedback is greatly appreciated.",6,scikit_learn,2020-10-13
aahf76,"Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?","Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?

&amp;#x200B;

So in the sense that if one applies e.g. a gain factor  (or a function depicting gain changes), then it's applied to the correct direction.",3,scikit_learn,2020-10-13
a75oid,"classification_report + MLPClassifier(): UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn_for)","classification\_report on a prediction done on MLPClassifier() sometimes throws:

&amp;#x200B;

*UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn\_for)*

&amp;#x200B;

but not on all the time.

&amp;#x200B;

What could be wrong?

&amp;#x200B;

\---

&amp;#x200B;

Doing

&amp;#x200B;

set(y\_test) - set(y\_pred) 

&amp;#x200B;

I'm able to see that sometimes some label is missing from y\_pred. But why does this occur only occasionally?

Is something wrong with how I use MLP?",1,scikit_learn,2020-10-13
a753f2,"{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification_report()","{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification\_report()

&amp;#x200B;

Why?

&amp;#x200B;

I call it like:

&amp;#x200B;

    classification_report(y_test, y_pred)

where y\_pred is predicted using a model I built.

&amp;#x200B;

""Quite obviously"" the arguments are incompatible somehow, but how can I find out, how? And how can I make them compatible?

&amp;#x200B;

\---

&amp;#x200B;

I tried:

&amp;#x200B;

    from sklearn.utils.multiclass import type_of_target

&amp;#x200B;

    &gt;&gt;&gt; type_of_target(y_test)
    'multiclass'
    
    &gt;&gt;&gt; type_of_target(y_pred)
    'continuous'

&amp;#x200B;",1,scikit_learn,2020-10-13
a746h0,"Is there any way to ""estimate"" how long a given computation in sklearn will take?","Is there any way to ""estimate"" how long a given computation in sklearn will take?

&amp;#x200B;

So that one doesn't need to wait longer than what one can?

&amp;#x200B;

Also, since Windows Task Manager shows only modest CPU use (&lt; 10%), then how is one supposed to know, what's occurring in the model?",2,scikit_learn,2020-10-13
a73oda,What are the most important parameters in LogisticRegression()?,What are the most important parameters in LogisticRegression()?,3,scikit_learn,2020-10-13
a71xf2,How does one feed hidden_layer_size tuples into GridSearchCV's param_grid?,How does one feed hidden\_layer\_size tuples into GridSearchCV's param\_grid?,1,scikit_learn,2020-10-13
a0yprf,Code review,"Hello,

I'm new to ML and scikit - hope this is the correct place for this. Have created the below code that appears to be working but wanted to get the opinions of people with more experience then me, to check I haven't a made any major errors or if there are any obvious improvements?

&amp;#x200B;

I am trying to train a model on a data set of potentially hundred of thousands emails. Every few days I want to retrain the exported model using incremental learning on the new emails received since the model was last trained.

The below reads the initial data from a csv, runs HashingVectorizer then SGDClassifier. The OnlinePipeline is used to allow me to use partial\_fit when I try to retrain later in the process.

`import pandas as pd`

`data = pd.read_csv('customData1.csv')`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`from sklearn.feature_extraction.text import HashingVectorizer`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

&amp;#x200B;

`from sklearn.linear_model import SGDClassifier`

`text_clf = OnlinePipeline([('vect', HashingVectorizer()),`

`('clf-svm', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=5, random_state=None)),`

`])`

`text_clf = text_clf.fit(X_train,Y_train)`

`predicted = text_clf.predict(X_test)`

`np.mean(predicted == Y_test)`

The above gives me an accuracy of 0.55

&amp;#x200B;

A few days later when I have new emails I import the previously exported model and use partial\_fit on a new csv file.

`import pandas as pd`

`data = pd.read_csv('customData2.csv') #text in column 1, classifier in column 2.`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

&amp;#x200B;

`from sklearn.externals import joblib`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

`text_clf2 = joblib.load('text_clf.joblib')`

&amp;#x200B;

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`text_clf2 = text_clf2.partial_fit(X_train,Y_train)`

&amp;#x200B;

`predicted = text_clf2.predict(X_test)`

`np.mean(predicted == Y_test)`

This returns the improved accuracy of: 0.84

&amp;#x200B;

Sorry for so much code!  I obviously need to tidy it all up so its a single method and handle the import/export logic properly.

&amp;#x200B;

Have a made any major errors or are there any obvious improvements? Thanks!

&amp;#x200B;",1,scikit_learn,2020-10-13
a0iz4i,Does cross_val_score tell something about generalizability?,"Does cross\_val\_score tell something about generalizability?

&amp;#x200B;

Or do I need to use something else for measuring generalizability?",0,scikit_learn,2020-10-13
a0c8dw,"Is there a problem if MLPRegressor doesn't converge for max_iter=100, but nor max_iter=5000 either?","Is there a problem if MLPRegressor doesn't converge for max\_iter=100, but nor max\_iter=5000 either?

&amp;#x200B;

Anything else I could try?",1,scikit_learn,2020-10-13
a0b260,What do cv (number of folds) and the number of outputs in cross_val_score correspond to?,"What do cv (number of folds) and the number of outputs in cross\_val\_score correspond to?

&amp;#x200B;

Does it mean that it produces cv number of different scores? Or (as I read somewhere) that only the last score might be the meaningful one (I read something like the others than the last used to ""fit"", while the last is the score)?",2,scikit_learn,2020-10-13
a0awuh,"Getting values in range [-191806. ..., 0.77642 ...] from cross_val_score, am I doing something wrong?","Getting values in range \[-191806. ..., 0.77642 ...\] from cross\_val\_score, am I doing something wrong?

&amp;#x200B;

    mlp = MLPRegressor(hidden_layer_sizes=(7,))

mlp.fit(X\_train,y\_train) mlp\_y\_pred = mlp.predict(X\_test)

&amp;#x200B;

y\_pred is an earlier prediction using LinearRegression().

&amp;#x200B;

I call cross\_val\_score like:

&amp;#x200B;

    cross_val_score(mlp, y_pred, mlp_y_pred, cv=10)

&amp;#x200B;

Output is:

&amp;#x200B;

    00 = {float64} -4.4409160725075605
    01 = {float64} -673636.0674512024
    02 = {float64} -51282.162171235206
    03 = {float64} -399557.4789466267
    04 = {float64} -35.73093353875776
    05 = {float64} -1406.9741325253574
    06 = {float64} -80853.84044929259
    07 = {float64} -5132.870883709122
    08 = {float64} -283.7432365432288
    09 = {float64} -2.860321933844385

&amp;#x200B;

I think I should be getting values in range \[0,1\].",1,scikit_learn,2020-10-13
a0at1q,"Is MLPRegressor's hidden_layer_sizes=(7,) equivalent to hidden_layer_sizes=7?","Is MLPRegressor's hidden\_layer\_sizes=(7,) equivalent to hidden\_layer\_sizes=7?",1,scikit_learn,2020-10-13
a09ukl,"Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train_test_split(Xy,shuffle = False, test_size = 0.33)?","Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train\_test\_split(Xy,shuffle = False, test\_size = 0.33)?

Xy has been constructed like:

&amp;#x200B;

    X = dat.data
    y = dat.target 
    Xy = np.hstack((X,np.array([y]).T))

It seems that it returns only two arrays, even when I saw an example ([https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input](https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input)) do 

 

    X_train, X_test, y_train, y_test = train_test_split(Xy,shuffle = False, test_size = 0.33) ",1,scikit_learn,2020-10-13
a06iin,Runtime Error in RandomizedSearchCV,"I've been running a RandomForestClassifier on a dataset I took from UCI repository, which was taken from a research paper. My accuracy is \~70% compared to the paper's 99% (they used Random Forrest with WEKA), so I want to hypertune parameters in my scikit learn RF to get the same result (I already optimized feature dimensions and scaled). I use the following code to attempt this (random\_grid is simply some hard coded values for various parameters):

&amp;#x200B;

    rf = RandomForestClassifier()
    # Random search of parameters, using 2 fold cross validation,
    # search across 100 different combinations, and use all available cores
    rf_random = RandomizedSearchCV(estimator = rf,  param_distributions = random_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)
    # Fit the random search model
    rf_random.fit(x_train, x_test)

When I attempt to run this code though my python runs indefinitely (for at least 40 min before I killed it) without giving any results. I've tried reducing the \`cv\` and \`n\_iter\` as much as possible but this still doesn't help. I've looked everywhere to see if there's a mistake in my code but can't find anything. I'm running Python 3.6 on Spyder 3.1.2, on a crappy laptop with 8Gb RAM and i5 processor :P

&amp;#x200B;

Here is the random\_grid if it helps:

    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
    max_features = ['auto', 'sqrt']
    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
    max_depth.append(None)
    min_samples_split = [2, 5, 10]
    min_samples_leaf = [1, 2, 4]
    bootstrap = [True, False]
    
    # Create the random grid
    random_grid = {'n_estimators': n_estimators,
                   'max_features': max_features,
                   'max_depth': max_depth,
                   'min_samples_split': min_samples_split,
                   'min_samples_leaf': min_samples_leaf,
                   'bootstrap': bootstrap}

&amp;#x200B;",1,scikit_learn,2020-10-13
9yhcqq,Does sklearn have built-in routines for testing results of LinearRegression()?,Does sklearn have built-in routines for testing results of LinearRegression()?,1,scikit_learn,2020-10-13
9ygvsi,How does fit_transform allow for other data to be processed with the same transformer?,"How does fit\_transform allow for other data to be processed with the same transformer?

&amp;#x200B;

Like here:

&amp;#x200B;

[https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)

&amp;#x200B;

Particularly, since one first calls fit\_transform, then why does it allow one to call transform afterwards and still get the same fit? Like how is this kind of functionality implemented?

&amp;#x200B;

&amp;#x200B;",1,scikit_learn,2020-10-13
9tf8yw,Principal component Analysis: predicting values,"I am attempting to forecast a set of multivariate time series data. I have run a PCA (using the scikit-learn module) and have run an AR(1) auto-regression of the 3 components.

Now that I have the projects component values, how do I recast those components into the original variables, in order to find the projection for those variables?",2,scikit_learn,2020-10-13
9t3xwo,Extract a single stratified part of a dataset,"I have a multi-label dataset with N samples, and I want to take a chunk out to reserve for validation, e.g. reserve k% of the dataset.

Note that I want to do this just once, else I could use stratifiedKFold.  
Is there a function to produce such a single chunk, ensuring stratification with respect to  the labels?  
(A workaround would be to produce N\*k  KFold splits, concatenate  all parts but one  for training, and use the last for validation.)

Thanks.",1,scikit_learn,2020-10-13
9sbqvm,Stepping through each iteration of the LogisticRegression fit() function,"Hello guys,

I'm using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class to find the decision functions between my classes. I wanted to ask you - *how can I step through each step of the algorithm?* I know I can give the parameter `max_iter` to determine the number of iterations, but I want to step through each of those `max_iter` iterations - to see how the values of the coefficients change.

Thanks in advance!",1,scikit_learn,2020-10-13
9pcack,"modAL: A modular active learning framework for Python, built on top of scikit-learn",,6,scikit_learn,2020-10-13
8ndofu,Parse Twitter feed and suggest domain names • r/nltk,"I'm working on a hackathon, and I'd like to parse a user's last 100 tweets or so and make recommendations for a domain name using a new TLD.

The plan I've got in my head is

1\) Scrape twitter for a bit and get some data \(How much? How many records?\)

2\) Run tf\-idf against it, save that dataset

3\) split the initial twitter data into groups based on which tweets contain each TLD \- supplies, computer, kitchen, etc.

a\) Run some kind of clustering algorithm against each set? 250 or so TLDs

\-\- This is where I have questions

4\) Scrape their twitter feed and get 100 tweets

5\) Use the tf\-idf data from step 2 to spit out keywords

6\) use those keywords using some kind of distance formula against the clustered data to pick a tld?

7\) use the bigrams or keywords to make up an SLD.

This seemed off to a good start, but can I somehow pickle the cluster results? Or have multiple sets of cluster results in the same object?

Note: 95&amp;#37; of my knowledge on this topic comes from this blog post: [http://brandonrose.org/clustering](http://brandonrose.org/clustering)",2,scikit_learn,2020-10-13
8l4imo,"move partial of decision models from server to client - side, is it good idea?","Hi,
Some time ago tenser flow for js was released. I'm wondering about build bridge for some scikit learn models to move some part of learning and prediction to the client side. I think that it could help minor companies reduce server resource usage and make models and prediction much more personalised. Do you think it's a good idea? Do you know whether someone has tried something similar before?",1,scikit_learn,2020-10-13
8emc3b,How to combine num values with text data for classification?,I build website classifier and use text of each webpage (transformed to bag of words) as train data. But I also want to add each website's PageRank as feature. How can I do that?,2,scikit_learn,2020-10-13
89u8tj,PLSRegression Issues,"I'm working with scikit's cross\_decomposition.PLSRegression(). According to their documentation, x = np.multiply( x\_scores\_, x\_loadings\_.T ). I'm not getting anything close to the same value values. I've tried every combination of using scale=False and sklearn.preprocessing.scale(x) to try and find how this works out, but I haven't been able to find one that works.

    plsr = PLSRegression(n_components=x_df.shape[0]-1).fit(x_df.T,y)
    print(np.matmul(plsr.x_scores_,plsr.x_loadings_.T).T)
    print(x_df)

Using scaled data (i.e. PLSRegression.fit( scale(x\_df).T, scale(y) ) and changing n\_components doesn't help either. If anyone has any idea of what mistake I have made, or if this just a bug in sklearn?",1,scikit_learn,2020-10-13
81xksg,How to remove terms from a term-document matrix?,"Hello,

I have a term document matrix that I've created using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) like so:
    
    X = vectorizer.fit_transform(corpus)
    X
    &lt;1000x10022 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    	with 94340 stored elements in Compressed Sparse Row format&gt;

I'd now like to remove any terms that do not appear in at least 3 documents, and then calculate the TF-IDF scores for each term, and select the vocabulary as the top n terms ordered by TF-IDF scores.

Is there an easy way of removing terms from the term document matrix that do not appear in at least 3 documents, while still conserving the mapping from feature names to feature indices?

I guess one way to do it would be to get the feature names of the terms that appear in at least 3 documents using numpy on the sparse matrix directly, assign them a mapping to indices, and then pass that mapping to the vocabulary parameter in the CountVectorizer constructor.

Any ideas on how to do this more easily?",1,scikit_learn,2020-10-13
7zfjba,How to use partial_fit to update the model trained with fit() instead of training from scratch,"I tried partial_fit with various scikit online learning classifiers like perceptron, passive aggresive classifiers, SGDclassifer... like here: https://ideone.com/uOtRTZ.  I just dont understand why i cant train the new data on top of already trained data. I am doing image classification. I have trained my 10,000 images with fit(). Now i got 1 new image to add to this dataset of already trained images. I want to update the trained model instead of training all 10,001. Is this possible with partial_fit() ? If so, please tell me how ? ",1,scikit_learn,2020-10-13
7z4qi7,SGDClassifier.partial_fit returns error of “classes should include labels”,"I tried to predict label of my newly added data through SGDClassifer.partial_fit as below:

        from sklearn import neighbors, linear_model
    import numpy as np
    
    
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""hinge"")#loss
    
        sgd_clf.fit(X, y)
    
        print(sgd_clf.predict([[6, 9]]))
    
        X.append([6, 9])
        y.append(5)
    
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here

However, this results in error: ValueError: `classes=array([1, 2, 3, 4, 5])` is not the same as on last call to partial_fit, was: array([1, 2, 3, 4])

Any ideas or references ? ",1,scikit_learn,2020-10-13
7vw4ap,Retrain a KNN classified model (scikit),"I trianed my knn classifer over multiple images and saved the model. I am getting some new images to train. I dont want to retrain the already existing model.

How to add the newly tranied model to the existing saved model ?

Could someone guide if this is possible or any articles describing the same ?

Thank you,",2,scikit_learn,2020-10-13
70m5l1,How do I add matplotlib to a django webapp and display the code's output on the webpage?,Trying to make a User Interface for a Support Vector Machine from the SVM function in the matplotlib,1,scikit_learn,2020-10-13
6lec9n,"K-NN and custom metrics, speed up sklearn using Cython",,2,scikit_learn,2020-10-13
6k5uvu,Build my first CART based algorithm feedback is welcome!,hey guys! i just made this: https://github.com/lucas-aragno/pokemon-classifier im pretty new to scikit so I'll appreciate any kind of feedback :),2,scikit_learn,2020-10-13
6ev2y7,FastICA,"It seems like all of the examples using fastICA involves taking 2 frequencies, mixing them a certain way, then unmixing them.

What about if I have a wav file. How can I use fastICA to break it down into multiple parts?

Any help would be appreciated. Thank you!",1,scikit_learn,2020-10-13
6eu5cr,Automate your Machine Learning in Python – TPOT and Genetic Algorithms,,2,scikit_learn,2020-10-13
69huhq,[P] Tracking and reproducibility in data projects (CLI tool),,1,scikit_learn,2020-10-13
5xk0iy,Scikit learn vs Open Cv for small problems in image processing,I am a Image processing noob. I've used Numpy and Scipy for some matrix related stuff before and OpenCV for some image processing problems. I recently learned that scipy lets me manipulate images too. What are the pros and cons of using OpenCv and Scipy I am not able to figure out which would be better for me. Appreciate your help!,7,scikit_learn,2020-10-13
5rb5ww,Using Category Encoders library in Scikit-learn,,1,scikit_learn,2020-10-13
5m0352,MLPClassifier: Multiple output activation,"I'm using MLPClassifier but some of the outputs have more than one activation, i.e. [0 1 1 0].
How can I get only one activation?

My code is:
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                    hidden_layer_sizes=(15,), random_state=1, activation='relu')

Thank you!",1,scikit_learn,2020-10-13
5fasve,Need help on scikit kfold validation,"Objective: To create 5 folds of training and test dataset using StratifiedKFold method. I have referred the documentation at http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.cross_validation.StratifiedKFold.html

I am able to print the indices alright but am unable to generate the actual folds. Here follows my code

from sklearn.cross_validation import StratifiedKFold
import pandas as pd
df=pd.read_csv('C:\Comb_features_to_be_used.txt')

##Getting only numeric columns
p_input=df._get_numeric_data()
## Considering all the features except labels
p_input_features = p_input.drop('labels',axis=1)
## Considering only labels [single column]
p_input_label = p_input['labels']
skf = StratifiedKFold(p_input_label, n_folds=5, shuffle=True)
i={1,2,3,4,5}
for i,(train_index, test_index) in enumerate(skf):
    ##print(""TRAIN:"", train_index, ""TEST:"", test_index)
    p_input_features_train = p_input_features[train_index] 
    p_input_features_test =  p_input_features[test_index]
        
I am getting the error: IndexError: indices are out-of-bounds

",2,scikit_learn,2020-10-13
54exlw,scikit-learn doc translation,"translate sklearn doc to chinese
feel free to join us
https://github.com/lzjqsdd/scikit-learn-doc-cn",2,scikit_learn,2020-10-13
52b7od,Improving the Interpretation of Topic Models,,1,scikit_learn,2020-10-13
50qfgg,Topic Modeling with Scikit Learn,,3,scikit_learn,2020-10-13
4rjkcq,Overfit Random Forest,"I have data where Random Forest models overfit to noise whatever 
hyperparameter I put.
(= excellent accuracy on training, but poor accuracy on prediction).


So, this is the process I did to over-come:
    1) Tweak the input data and reduce the sampling of noise (negative example)

    2) Fit the RF and test (confusion matrix) on cross-validation data. 

    3) Repeat it and choose the best cross validation data.

Is there a way to overcome this monte carlo approach,
using OOBag process during training ?

Also incorporate Cross validation to reduce the over-fitting ?

Importance features change every time a new RF is fit (it seems a lot of co-linearity and noise into the data).










",1,scikit_learn,2020-10-13
4h6ypj,Building scikit-learn transformers,,3,scikit_learn,2020-10-13
3zvwqw,Hello everyone! I want to write an oversampling module in compliance with scikit-learn. Advice needed!,"As mentioned in the title I want to write a module for oversampling classes in skewed datasets. I recently came to need such a module and I noticed that no such thing exists officialy in scikit-learn. I want it to be compatible with scikit-learn as I very often use it. Do you have any resources to redirect me to, apart from the official scikit-learn developer guidelines? Any tips for writing a python module in general?

Thanks in advance!",2,scikit_learn,2020-10-13
2f830i,Official Scikit-Learn page.,,1,scikit_learn,2020-10-13
j9q6bp,A scikit-learn compatible library to construct and benchmark rule-based systems that are designed by humans,,5,scikit_learn,2020-10-13
j93854,Best performance on Scikit-learn’s load_digits dataset,"On Scikit-learn’s load_digits dataset:
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html

Does anyone know what is the best performance achieved so far on this dataset? 

I tried googling around but can’t find examples with 100% score performance. I am thinking since this is a standard dataset, it would be easy to get a 100% score performance?",1,scikit_learn,2020-10-13
j8vepi,"could someone ELi5 the hyperparameters (penalty, C, tol, max_inter)",I am currently working on a beginner project on logistic regression using scikit\_learn. I am trying to fine tune my regression model but cant seem to find any websites that can explain what the parameters mentioned in the title mean exactly and how to use them. I was wondering if anyone could give me a quick explanation on what/how to use these parameters to fine tune my regression model.,0,scikit_learn,2020-10-13
j7jz85,SVC rbf kernel seems to be nonstandard?,"I am currently testing a precomputed version of rbf I implemented to get a better feel for how it works and possibly later check out some other kernels.

It seems that whatever I do, I get different results using my precomputed gram matrix vs using the scikit rbf kernel:

To calculate a kernel entry for datapoints xm &amp; xn, based on some extra parameters theta

`k = thetas[0] * np.exp(-(thetas[1]/2.) * (np.sqrt((xn-xm).T @ (xn-xm)))) + thetas[2] + thetas[3] * (xn.T @ xm)`

using theta = \[1,2,0,0\]

This should recover the formulation given [here](https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel) (setting gamma=1)

1 \* exp( - 2/2|xn-xm|^(2) )   


is there something I'm missing? \[here's the code if you wanna take a look\]([https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit\_test.py](https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit_test.py)) (only dependencies are matplotlib, scikit &amp; numpy, so you're probably good if you're on this sub)",1,scikit_learn,2020-10-13
j5pcm6,ImportError,"I've got a long error which ends with:  
ImportError: DLL load failed while importing \_arpack: Não foi possível encontrar o procedimento especificado. (rough translation: Unable to find the specified procedure)  


Any idea on the issue? It seems like I have some sort of update issue, but I'm unable to find what.",2,scikit_learn,2020-10-13
j2v11w,"Scikit-learn. In the case of a single point, k-nearest neighbours predictions doesn’t literally match with the literally nearest point. I think I know why. Correct me if I’m wrong.","Hello.  I’ve looked at the source code. 

Case population sizes in the range 10 ^ 2 to 10 ^ 5 ish. Vanilla, straight out the box knn from scikit-learn.  Except 1 nearest neighbours not the default 5.  

When I try to predict the nearest neighbour of a point, using 1 nearest neighbours. after using knn.fit to make a model, it doesn’t _always_ return the same value of the actual nearest neighbour.  I’ve worked out the actual real nearest neighbour myself to check, using trig, and unit tested it.  

I think that’s because for pragmatic reasons knn is just a probabilistic model applied at group level.  Not exactly the actual knn for each and every point.  

Am I right?

EDIT:  My. Trig. Was.  Wrong.  Due. To.  A. Data frame. Handling.  Issue.  Ggaaahhhh.",5,scikit_learn,2020-10-13
j1idtx,RadomizedSearch CV taking forever,"Hi ,

I have the below snippet.

Trying to run on GCP . its getting stuck and not even updating.

&amp;#x200B;

https://preview.redd.it/bp2zfi71sxp51.png?width=1463&amp;format=png&amp;auto=webp&amp;s=6d97d1ff6083f6eb65e62d983770c69f06d45f4c",2,scikit_learn,2020-10-13
iv8jv9,Neuraxle - a Sklearn-Based Clean Machine Learning Framework,,1,scikit_learn,2020-10-13
it82un,How the 'init' parameter of GradientBoostingRegressor works?,"i'm trying to create an ensemble of an determined regressor, with this in mind i've searched for some way to use the sklearn already existing ensemble methods, and try to change the base estimator of the ensemble. the bagging documentation is clear because it says that you can change the base estimator by passing your regressor as parameter to ""base_estimator"", but with GradientBoosting you can pass a regressor in the ""init"" parameter. my question is: passing my regressor in the init parameter of the GradientBoosting, will make it use the regressor i've specified as base estimator instead of trees? the documentation says that the init value must be ""An estimator object that is used to compute the initial predictions"", so i dont know if the estimator i'll pass in init will be the one used in fact as the weak learner to be enhanced by the bosting method, or it will just be used at the beginning and after that all the work is done by decision trees. If someone can help me with this question i would be grateful.",4,scikit_learn,2020-10-13
igtkry,Best way to get T-Stastic and P-value etc?,"I'm using scikit learn for linear regression.  Is there a way to use that library to generate things like T-Stastic and p-value and standard error etc?

On stack overflow i found this, but wondering if there's a way within scikit

    import statsmodels.api as sm
    from scipy import stats
    X2 = sm.add_constant(X)
    est = sm.OLS(y, X2)
    est2 = est.fit()
    print(est2.summary())

&amp;#x200B;",1,scikit_learn,2020-10-13
i4ulg2,"Data Visualization using ""Python"" with ""Seaborn"" | Part- I",https://youtu.be/X400eIcV-So,3,scikit_learn,2020-10-13
i3546z,Recommendation based on other user following,"Hello,

I try to build a recommendation system.

My service allow users to follow people (not rate them, just follow) and I would like to be able to propose to users to follow people based on other user’s database activity.

Is scikit a good path for this ? 

Do you recommend specific method or useful ressource to read to achieve this ?

For your help guys!",2,scikit_learn,2020-10-13
hzw282,How to use TensorFlow Object detection API to detect objects in live feed of webcam in real-time,,1,scikit_learn,2020-10-13
hwmcf1,sklearn CCA - how to get variance explained for first canonical relationship?,"Hi. I'm exploring multivariate brain-behaviour relationships with sklearn's canonical correlation analysis tool ([https://scikit-learn.org/stable/modules/generated/sklearn.cross\_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca)). I am interested mostly in the first canonical relationship between the two datasets. The decomposition is working fine and i have the weights/canonical scores etcetera - but what i'd really like to know is how much of the variance in either dataset is explained by that one relationship (analogous to eg variance explained by first principal component).

There is a method named 'score' that i can call on the CCA object but I am not quite sure this is what I need. This score is not the same as 'canonical scores above but will supposedly get some coefficient of determination r\^2 between 'observed' and 'predicted' - not sure how to understand this. The description on the webpage is quite terse and it does not behave the way i might expect.

I'm hoping to find someone who might know whether that 'score' method  will get me to what i want - and if so, maybe how to use it. Or point me otherwise in the right direction to get into the variance explained for CCA.

Cheers!",2,scikit_learn,2020-10-13
hu6y83,KMeans Algorithm Question,"Hey all.

I am new with using scikit-learn and had a question regarding the KMeans algorithm functions. After running the algorithm and plotting the clusters, are the clusters with the centroids plotted the final clusters after training is done or is there training that I have to do on the clusters? 

Thanks everyone",1,scikit_learn,2020-10-13
htugnl,"How to handle ""Missing Value"" from ""Dataset"" using ""Pandas"" &amp; ""Sci-Kit Learn""??",https://youtu.be/8IORSsZIyIQ,0,scikit_learn,2020-10-13
htmc59,"How to handle ""Text"" and ""Categorical Attributes"" using Python and Pandas??",https://youtu.be/4sO7Pezlegk,0,scikit_learn,2020-10-13
ht4ol1,Making ROC curves with results from cross_validate?,"I am running 5 fold cross validation with a random forest as such:

from sklearn.ensemble import RandomForestClassifier

from sklearn.model\_selection import cross\_validate

forest = RandomForestClassifier(n\_estimators=100, max\_depth=8, max\_features=6)

cv\_results = cross\_validate(forest, X, y, cv=5, scoring=scoring)

However, I want to plot the ROC curves for the 5 outputs on one graph. The documentation only provides an example to plot the roc curve with cross validation when specifically using StratifiedKFold cross validation (see documentation here: [https://scikit-learn.org/stable/auto\_examples/model\_selection/plot\_roc\_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py))

I tried tweeking the code to make it work for cross\_validate but to no avail.

How do I make a ROC curve with the 5 results from the cross\_validate output being plotted on a single graph?

Thanks in advance",2,scikit_learn,2020-10-13
hm6td7,Best performance on MNIST - Fashion dataset,Does anyone know what is the best performance achieved so far for the MNIST - Fashion dataset along with what model that was used?,1,scikit_learn,2020-10-13
hm22vz,"How to ""Predict"" my friends weight using ""Machine Learning"" and ""Sci-Kit Learn""??",https://youtu.be/A4JwnkTFEXI,2,scikit_learn,2020-10-13
hl4k0u,Factor analysis “model” in CS229,"In one of Stanford’s CS229 lecture by Andrew Ng (https://m.youtube.com/watch?v=tw6cmL5STuY), he talks about a factor analysis “model” in which is to deal with situations where you have a lot more features than samples in your dataset. He even said he used a modified version of this factor analysis “model” in some recent work he did for a manufacturing company in the lecture.

Now my understanding of factor analysis is just a dimension reduction technique. So how did Andrew used factor analysis to build a “model” which deals with datasets which has a lot more features than samples?",2,scikit_learn,2020-10-13
hkm9qn,StackingRegressor Inconsistent Output,"Is it intentional that StackingRegressor returns different accuracy outputs when running multiple times given the same parameters, models and using numpy set seed?",1,scikit_learn,2020-10-13
hjctba,"This lecture that talks about what Multilabel and Multioutput classifications are, along with their implementation using scikit learn.",,1,scikit_learn,2020-10-13
hg5j3s,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,2,scikit_learn,2020-10-13
hf60u0,"Hey guys, here is a lecture on how to implement gradient descent with scikit-learn. Enjoy :)",,1,scikit_learn,2020-10-13
hakk07,How do I create a linear regression for this groupedby dataframe?,"I have this assignment for a job interview and I really want to impress by using some machine learning. I don't know too much about it and I essentially don't have much time to learn that much about it. I have the following [dataframe](https://postimg.cc/0zNbyrV8) and I want to create a linear regression using scikitlearn of \['profit'\] vs \['dateReceived'\] for each \['Language'\]. 

Does anyone know what I can do for that to work? I guess it should be just a few lines of code, but I could be wrong?",0,scikit_learn,2020-10-13
h7ay1o,"Visualize Scikit-learn models – ROC, PR curves, confusion matrices etc",,7,scikit_learn,2020-10-13
h172dr,Scikit Learn Tutorial in One Hour,,5,scikit_learn,2020-10-13
h0w3xx,Books about classification algorithms,"Hi all,

I am completely new to data mining and have to write a seminar paper about classification and do some programming in python.

With the help of datacamp I was able to implement the classification algorithms in python.

Now I am looking for some sources that I can cite in my paper that briefly explain these algorithms.

My problem is that most books that I have look into so far are very mathematical and since I don’t have a data mining/computer science background, they are hard to understand.

Do you have any recommendations for some text books that explain classification algorithms such as SVM, Naive Bayes, Trees, etc. that are well recognized, but explain them in an easy way?

Many thanks in advance!",1,scikit_learn,2020-10-13
gziaus,How to choose best pair of random state and class label values?,"For the last few days, I was trying to implement the KMeans algorithm using SciKit Learn, But I came across a very confusing problem. I have a dataset that has two class labels ['ALL', 'AML'] where ALL has 47 and AML has 25 samples and 100 attributes to train from and now I want to use this dataset for KMeans clustering so that I can compare the predicted results with the original class labels. Before asking my question let me explain certain scenarios. In all the scenarios I have taken all the 100 attributes to fit the model.

Scenario 1:

In the first run, I started with a model that is created with pretty much default arguments i.e. model = KMeans(n_clusters=2). For comparing the predicted class labels(which are numeric) with the original labels(which are strings), I set the original class labels as ALL = 1 and AML = 0. After that, while comparing using a classification report I got an average accuracy of 35%. Then I run the algorithm once again and got an accuracy of 44%. For the third try, I got 33% and so on.

However, I looked about it and came to know that the random_state argument needs to have a fixed value to get same accuracy throughout all runs.

Scenario 2:

After knowing about random_state, this time I started with random state 0 and created the model as model = KMeans(n_clusters=2, random_state=0) and kept the original class labels as before i.e ALL as 1 and AML as 0. However, this time the output didn't change on different runs and I got an accuracy of 53%. But, out of curiosity, I swap the original class label i.e. I set ALL as 0 and AML as 1 which results in 47%.

Scenario 3:

This time I choosed random_state as 1 i.e. model = KMeans(n_cluster=2, random_state=1) and having ALL as 0 and AML as 1 gave 67% accuracy while considering ALL as 1 and AML as 0 gave 33% accuracy.

So, My question is what I am doing wrong here? Am I implementing something wrong? If I am right then why the result is changing so much depending on random_state and class labels? What's the solution and how to choose the best pair of random_state and class labels?",1,scikit_learn,2020-10-13
gwgzy9,estimate_transform works when using 'similar' but not when using 'affine',"I have two 512x512 grayscales images (src and dst). To try to understand estimate transform I applied the following transformation

    tform = transform.AffineTransform(scale=(1.3, 1.1), 
                                        rotation=0.5, 
                                        translation=(0, -200)) 

to the src to create the dst. Then I want to find back the parameters using estimate\_transform.

With the parameter 'similar' I obtain parameters very close to the one I used (as expected). But when I want to use 'affine', I obtain the following error :

     matmul: Input operand 1 has a mismatch in its core dimension 0, 
    with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 513 is different from 3) 

Any idea why ? Here is my code :

    src = rgb2gray(data.astronaut())
    dst = rgb2gray(data.astronaut())
    tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5,
                                      translation=(0, -200))
    dst = transform.warp(img1, tform)
    tform_fin = transform.estimate_transform('affine', src, dst)
    dst_corr = transform.warp(img3, tform.inverse)",1,scikit_learn,2020-10-13
gtw4p9,What can I do when I keep exceeding memory used while using Dask-ML,"I am using Dask-ML to run some code which uses quite a bit of RAM memory during training. The training dataset itself is not large but it's during training which uses a fair bit of RAM memory. I keep getting the following error message, even though I have tried using different values for n_jobs:

```
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
```

What can I do?

Ps: I have also tried using Kaggle Kernel (which allows up to 16GB RAM) and this didn't work. So I am trying Dask-ML now. I am also just connected to the Dask cluster using its default parameter values, with the code below:

```
from dask.distributed import Client
import joblib

client = Client()

with joblib.parallel_backend('dask'):
    # My own codes
```",1,scikit_learn,2020-10-13
gsx926,MLPRegressor newby with some (probably very basic) questions in need of some assitance,"Hello!

I'm building MLPRegressor for the first time ever (I've been learning how to code with online courses since end of March) and I know something is wrong but I don't know what. Bellow you can see my code so far. It runs and I have a value for r2 ( -9035355.06 ) and a plot. However the r2 score doesn't make sense (it should be around 0.7)  and the plot doesn't make sense either.

I have run this analysis with SPSS multilayer perceptron feature so I know more or less how my results should be and that's why I know whatever I am doing with python is wrong.

Any advice/suggestion of what I'm doing wrong is very welcome! This coding world is kinda of frustrating for me:/

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    
    from sklearn import neighbors, datasets, preprocessing 
    from sklearn.model_selection import train_test_split
    from sklearn.neural_network import MLPRegressor
    from sklearn.metrics import r2_score
    
    vhdata = pd.read_csv('vhrawdata.csv')
    vhdata.head()
    
    X = vhdata[['PA NH4', 'PH NH4', 'PA K', 'PH K', 'PA NH4 + PA K', 'PH NH4 + PH K', 'PA IS', 'PH IS']]
    y = vhdata['PMI']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 
    
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_test_norm = scaler.fit_transform(X_test)
    
    nnref = MLPRegressor(hidden_layer_sizes = [4], activation = 'logistic', solver = 'sgd', alpha = 0.1, learning_rate= 'constant',
                         learning_rate_init= 0.6, max_iter=200, random_state=0, momentum= 0.3, nesterovs_momentum= False)
    nnref.fit(X_train_norm, y_train)
    
    y_predictions= nnref.predict(X_test_norm)
    
    print('Accuracy of NN classifier on training set (R2 score): {:.3f}'.format(nnref.score(X_train_norm, y_train)))
    print('Accuracy of NN classifier on test set (R2 score): {:.3f}'.format(nnref.score(X_test_norm, y_test)))
    print('Current loss : {:.2f}'.format(nnref.loss_))
    
    plt.figure()
    plt.scatter(y_test,y_predictions, marker = 'o', color='blue')
    plt.xlabel('PMI expected (hrs)')
    plt.ylabel('PMI predicted (hrs)')
    plt.title('Correlation of PMI predicted by MLP regressor and the actual PMI')
    plt.show()",1,scikit_learn,2020-10-13
gsol3k,What are the default values for the parameters in Dask-ML's Client() function,"I am trying to understand Dask-ML's Client() function parameters. Say I have the following code using Dask-ML's Client() function:

```
from dask.distributed import Client
import joblib

client = Client()
```

If I don't specify any values for the parameters in the Client() function, what are the default values for the parameters:

(i) n_workers

(ii) threads_per_worker

(iii) memory_limit

From my understanding, Python has the Global Interpreter Lock (GIL) feature which prevents multi-threading. If so, why does Dask-ML's Client() function have the parameter threads_per_worker when multi-threading is prevented in Python?

Does memory_limit refers to the maximum memory limit allowed for **each** worker/machine/node or does this refer to the maximum memory limit allowed for **all combined** worker/machine/node?

I have already looked through the documentation in Dask-ML (see here: https://docs.dask.org/en/latest/setup/single-distributed.html), but the documentation is not clear in regards to these questions above.

Thank you in advance if anyone could explain this?",1,scikit_learn,2020-10-13
glb4cy,Why does PolynomialFeatures has multiple pair of coefficient after fitted the data?,"After I create an PolynomialFeatures object, and fit the data by :

[`poly.fit`](https://poly.fit)`(x,)`

I wanted to look for the coefficient, so I do:

`poly.transform(x,y)`

&amp;#x200B;

And it will return an array with (n\_samples, n\_coeff), but why does the polynomial fit with multiple pair of coefficient? Wouldn't the model fit the data and get a final best coefficient?

&amp;#x200B;

And what is the final coefficient that Polynomial get after fitting?",1,scikit_learn,2020-10-13
gi4jw3,How to add sample_weight into a scikit-learn estimator,"I have recently developed a scikit-learn estimator (a classifier) and I am now wanting to add sample_weight to the estimator. The reason is so I could apply boosting (ie. Adaboost) to the estimator (as Adaboost requires sample_weight to be present in the estimator).

I had a look at a few different scikit-learn estimators such as linear regression, logistic regression and SVM, but they all seem to have a different way of adding sample_weight into their estimators and it's not very clear to me:

Linear regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_base.py#L375

Logistic regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_logistic.py#L1459

SVM: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/svm/_base.py#L796

So I am confused now and wanting to know how do I add sample_weight into my estimator? Is there a standard way of doing this in scikit-learn or it just depends on the estimator? Any templates or any examples would really be appreciated. Many thanks in advance.",2,scikit_learn,2020-10-13
get2kh,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,,2,scikit_learn,2020-10-13
gdb7a8,Code a Decision Tree in 20 lines.,,0,scikit_learn,2020-10-13
gd81x4,why does Scikit Learn's Power Transform always transform the data to zero standard deviation?,"all of my input features are positive. Whenever I tried to apply PowerTransformer with box-cox method, the lambdas are s.t. the transformed values have zero variance. i.e. the features become constants

&amp;#x200B;

I even tried with randomly generated log normal data and it still transform the data into zero variance.

&amp;#x200B;

I do understand that mathematically, finding the lambda s.t. the standard deviation is the smallest, would mean the distribution would be the most normal-like.

&amp;#x200B;

But when the standard deviation is zero, then what's the point of using it?

&amp;#x200B;

&amp;#x200B;

p.s. so one of the values of lambda I get by using PowerTranformer is -4.78 

If you apply it into the box-cox equation for lambda != 0.0, then for any input feature y values, you technically get the same values. i.e. (100\^(-4.78)-1.0)/(-4.78) is technically equals to (500\^(-4.78)-1.0)/(-4.78)",2,scikit_learn,2020-10-13
gcvtsm,how to combine recursive feature elimination and grid/random search inside one CV loop?,"I've seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:

[Feature selection and cross-validation](https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751)

[Nested cross-validation and feature selection: when to perform the feature selection?](https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec)

[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)

&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...

[Here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.

However I'd like to do recursive feature elimination inside random/grid CV, so that ""feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)"", so that data from other folds influence neither feature selection nor hyperparameter optimization.

Is this possible natively with sklearn methods and/or pipelines? Basically, I'm trying to find an sklearn native way to do this before I go code it from scratch.",3,scikit_learn,2020-10-13
gc2z28,How to write a scikit-learn estimator in PyTorch,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",3,scikit_learn,2020-10-13
g5ugws,Code a Neural Network in 20 lines.,,2,scikit_learn,2020-10-13
g4yc73,Basic question re: gaussian mixture models,"I wasn't able to find this in the documentation, but is the covariance parameter you access with model.covariances\_ sigma or sigma\^2? Seems like it can be either thing as I've seen the notations N(x| mu, sigma\^2) and N(x|mu, sigma) both used in various places.",1,scikit_learn,2020-10-13
fzm7mm,"Should scikit-learn include an ""Estimated Time to Arrival"" (ETA) feature? Discuss.",,8,scikit_learn,2020-10-13
fx6kdy,Clustering of t-SNE,"Hello,

I have recently tried out t-SNE on the sklearn.datasets.load_digits dataset. Then i applied KNeighborClassifier to it via a GridSearchCV with cv=5.

In the test set (20% of the overall dataset) i get a accuracy of 99%

I dont think i overfitted or smth. t-SNE delivers awesome clusters. Is it common to use them both for classifying? Because the results are really great. I will try to perform it on more data. 

I am just curious on what you (probably much more experienced users than me) think.",1,scikit_learn,2020-10-13
fx0i7x,Search over preprocessing and ensemble hyperparameters?,"In scikit-learn there are some handy tools like `GridSearchCV` for tuning the hyperparameters to a model or pipeline.

Suppose you'd like the preprocessing in your pipeline to include some user-defined options (e.g. whether to encode a certain categorical variable via one-hot encoding or something weird like frequency encoding) and you'd like to include those options among the hyperparameters you're searching over.

Suppose further that you're using an ensemble model -- e.g. a random forest plus few linear regression specifications, and you'd like to tune the hyperparameters for each of them, as well as the voting weight of each.

Does scikit-learn provide a predefined way to search over such spaces? It looks like the parameter space is intended only to dictate the behavior of a single model, not preprocessing steps or ensemble parameters.",1,scikit_learn,2020-10-13
ft2pcp,"How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?","How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?",1,scikit_learn,2020-10-13
ft1kmb,facing an error,"import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

&amp;#x200B;

\# Importing the dataset

dataset = pd.read\_csv('50\_Startups.csv')

X = dataset.iloc\[:, :-1\].values

y = dataset.iloc\[:, 4\].values

X2=dataset.iloc\[:, 3\].values

\# Encoding categorical data

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

le = LabelEncoder()

X2 = le.fit\_transform(X2)

oh = OneHotEncoder(categories = 'X\[:, 3\]')

X= oh.fit\_transform(X).toarray()

&amp;#x200B;

https://preview.redd.it/7bgiwdp238q41.png?width=871&amp;format=png&amp;auto=webp&amp;s=d2386c5cda100706a85803c036586beec8b9e843",1,scikit_learn,2020-10-13
fm1oov,I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?,"I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?

    preprocess =     make_column_transformer((SimpleImputer(strategy='median'), cols_numeric),     
    (SimpleImputer(strategy='constant', fill_value='missing'), cols_onehot),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_target),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_ordinal),     (OneHotEncoder(handle_unknown='ignore'), cols_onehot),     
    (TargetEncoder(), cols_target),     
    (OrdinalEncoder(), cols_ordinal),     
    (StandardScaler(), cols_numeric)) 
    lr_wpipe = make_pipeline(preprocess, LinearRegression()) 
    lr_scores = cross_val_score(lr_wpipe, X_train, y_train) 
    np.mean(lr_scores) 
    print(""Linear Regression R^2: "", lr_scores)",1,scikit_learn,2020-10-13
flg6a1,how to find 'the math' being done in sklearn source code?,"hi.  I'm trying to find where in sklearn the actual math is being done, mostly for my own learning so I can answer questions like 'when using `sklearn.neighbors` , what math is being used to calculate Euclidean distance?'    


If you see here: [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/\_base.py#L360](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/_base.py#L360)  
you'll see that Euclidian and other distance calculations can be *specified*, but I don't see anywhere where the actual math is being done in code.",1,scikit_learn,2020-10-13
fl4fj1,Adding Standard Scaler to GridSearchCV,"I'm looking to use the Standard Scaler as a hyper parameter, i.e check if performance is higher with/without scaling the inputs. In order to tune with other hyperparameters, I would like to incorporate it into my GridSearchCV function (provided by Scikit Learn). Can someone advise me on how to do it?",1,scikit_learn,2020-10-13
ffx9zw,How to use tfidfvectorizer fit_transform for multiple docs,"Hey, 

Let's say my corpus is a list of lists , each of the inner lists represent a parsed doc (each value is a word) 

I want to compute a tf-idf score for my corpus. 

It's seems like the fit-transform function can't use my corpus as its inputs should be itratable with string values (which is each of my docs) 


    V = tfidfvectorizer ()
    For doc in corpus:
       Vectors = v.fit_trabsform(doc)

So my question is, how does it calculate IDF if it get only one doc at a time?",1,scikit_learn,2020-10-13
ff9602,Classifiers' score method clarification,"Hi,

I don't fully understand what the score method of classifiers does. For example, the [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method's documentation says ""Return the mean accuracy on the given test data and labels."" Now, I know what is accuracy: (TP+TN)/(TP+TN+FP+FN), but I don't understand why ""mean"" is in there. Mean over what? of what?  

That is, I give the method as parameters a dataset with true labels, and it can calculate the accuracy from that (given the model), but where does the mean come into place? 

Thanks in advance!",3,scikit_learn,2020-10-13
fbfky4,Is epsilon in dbscan a euclidean measure?,"Hello everyone,
I'm writing yet another dbscan question. For those who are familiar with the inputs to the dbscan, the principal parameters are epsilon and minPts.

Epsilon is the neighborhood radius, and I'm curious if anyone can point me to a reference or tell me if epsilon is a euclidean metric",3,scikit_learn,2020-10-13
f1dizs,Identifying smallest frequently occurring value,"I'm not a data science person, but thinking Scikit learn might be able to help here, and looking for suggestions for ideas I should investigate.

Essentially, I'm looking for a way to consistently identify a baseline power readings. If I have minute by minute power consumption readings from a bunch of electrical motors.  For any motor, we want to identify what a 'baseline' or 'normal unloaded steady-state' power value is.

There is definitely noise in the signal, and not even noise - legitimate power reading that are smaller than what we would consider 'normal unloaded steady state'.  The catch is this could be different for the same motor when production composition changes, so there is not just one value that we can look at historical data to arrive at. (Think motors running pumps moving different fluid mixtures / slurry ad different times.

This does not have to be real-time, just take the dataset of power readings for any motor for any production batch and post-process the data in such a way we can identify times the motor is doing its job at a 'near-idle' state.

Currently we just have a basic calculation that looks at a rolling window of 20 per-minute readings and finds the lowest value that occurs at least twice. (basically throwing out the lowest few outliers)

The reason I'm considering Scikit or similar is we can graph these power readings for a time period (say 1 day) and visually we can easily see these 'baselines' we are looking for.  There will be spikes and dips, and time windows where we are definitely running a heavy load (motors spun up on demand), but we can identify when the mixture changes because the visual changes in this baseline value.

Hope that made at least a little sense, if there are details I can clarify, please ask.  I appreciate everyone's thoughts and ideas!",1,scikit_learn,2020-10-13
eylu4p,What's wrong with Scikit-Learn.,,3,scikit_learn,2020-10-13
ev1as7,Is it possible to use a custom-defined decision tree classifier in Scikit-learn?,"I have a predefined decision tree, which I built from knowledge-based splits, that I want to use to make predictions. I could try to implement a decision tree classifier from scratch, but then I would not be able to use build in Scikit functions like predict. Is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? Or do I need to do something completely different? My first attempt was to use “fake training data” to force the algorithm to build the tree the way I like it, this would end up in a lot of work because I need to create different trees depending on the user input.",1,scikit_learn,2020-10-13
eu9qgc,Is HistGradientBoosting the same as LightGBM or is the SKLearn's version different?,"If so, how?",2,scikit_learn,2020-10-13
em9fxf,Is this the proper way to do ML with scikit_learn?,"I have a dataset with 8 features (numeric) and 1 target (0 or 1).  
 I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc.  
 For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

But sometimes I get these errors:  
 The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

What's the proper way to predict binary values?  
 I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Thanks for your support. I'm a beginner.",2,scikit_learn,2020-10-13
effb98,What module/algorithm should I use in order to predict the time in which a certain action will be completed?,"Basically the title, but to explain it even more :

I have to device a model which will predict the total time a patient will have to wait in a hospital environment. For that, we have a dataset consisting of various patients with several diseases and their time durations already recorded. I want to know which module or algorithm should I use to carry this out? This is my first ML project and I could use any help that you guys can do. Thank you!!",2,scikit_learn,2020-10-13
e0h8ao,Column transformer throwing away some features?,"My data frame  has numerical columns a, b, c. And it has categorial text features d, e, f, g, h.

I build a preprocessor like the following.

    num_features = ['a','b','c']
    nom_features = ['d','e','f','g','h']
    
    preprocesser = ColumnTransformer([
        (""scale_numeric"", StandardScaler, num_features),
        (""encode_nominal"", OneHotEncoder(handle_unknown=""ignore""), nom_features)],
        remainder=""drop""
    )
    
    preprocessor.fit_transform(dataframe)

Since I started with 8 features, after one hot encoding I expected to get 8 or more features back, but the result of `preprocessor.fit_transform(dataframe)` only has 3 columns. Not sure what I am doing wrong if anyone can help me.",1,scikit_learn,2020-10-13
dyzwn9,How to Modify(Make unique) the Scikit-learn Multilayer perseptron algorithm (MLP),"Hi folks,

I've been trying to build a rainfall prediction model for last few days. I've used the **Scikit-learn Multilayer perseptron regressor function** straight up. 

1) The accuracy was OK(78%) but I want to increase it

2) I don't want to use the same predominantly given function (I just want to **add uniqueness in my code**, but I want to use scikit-learn)

Is there any way to modify the function or not use the ready-made function? Can anyone please help me with this?

Thanks in advance!",1,scikit_learn,2020-10-13
dtnh3f,The best alpha for ridge regression is... -85???,,3,scikit_learn,2020-10-13
dtiy98,difference between Kfold.split() and shufflesplit.split() in scikitlearn,"I read this [post](https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv), I get the difference when it comes to computation and shufflesplit randomly sampling the dataset when it creates the testing and training subsets, but in the answer on stackoverflow, there is this paragraph  


""**Difference when doing validation**

In KFold, during each round you will use one fold as the test set and *all* the remaining folds as your training set. However, in ShuffleSplit, during each round **n**  you should *only* use the training and test set from iteration **n** ""

I couldn't quite get it. since in kfold, you're bounded by using the training buckets (k-1) and testing bucket (k) in the **k** iteration and in shufflesplit you use the training and testing subsets made by the shufflesplit object in iteration **n.**  so for me it feels like he's saying the same thing.

can anyone please point out the difference for me?",1,scikit_learn,2020-10-13
dcvd2r,When to use these unsupervised algorithms?,"There are a lot of modules in sklearn. I am interested when these unsupervised algorithmes (bellow )are used.  
When to use a Gaussian mixture model? When to use Manifold Learning, When to Biclustering? etc.

&amp;#x200B;

* [2.1. Gaussian mixture models](https://scikit-learn.org/stable/modules/mixture.html) 
* [2.2. Manifold learning](https://scikit-learn.org/stable/modules/manifold.html)
* [2.4. Biclustering](https://scikit-learn.org/stable/modules/biclustering.html) 
* [2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html) 
* [2.6. Covariance estimation](https://scikit-learn.org/stable/modules/covariance.html) 
* [2.7. Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html) 
* [2.8. Density Estimation](https://scikit-learn.org/stable/modules/density.html)",7,scikit_learn,2020-10-13
dar9z6,pattern recognition on texts that are bash commands or software signature?,"hi all.

so I've got my hands on a daily dose of 100,000 connections per day to our servers, and I've got millions of rows of data that includes commands our users have executed on our servers, (\`cd\`, \`ehlo\`, \`scp ....\`, etc). and I have the same amount of data of their application signatures while connecting. like (Firefox 59, Firefox 60, google chrome),... and user agents, ...

basically all the data one can extract out of a socket or using an IDS.

I like to do some pattern matching on these data. like for the commands they are executing and stuff like that...

so to cluster the commands, I've got commands that look like this:

cd Project

cd Images/personal

cd Project/map

cat /var/log/nginx/web\_ui.log

the problem is, I can just split the texts and take in the first part(cd, cat) and make plot out of the commands, but i really would like to make it more automatic and intelligent. so people who \`cd\` into the \`Project/map\` are distinguished from people who cd into \`Images\` folder. I like to know what people are doing on out servers. so a plot that all people whith \`cd\` commands are close to each other, but are really distinguished for each folder that they have \`cd\` into. 

this is just an example of what I want:)

&amp;#x200B;

turns out that scikit\_learn only works on numbers? how can i utilize it for that kind of data? I don't know if this is a nltk problem?",3,scikit_learn,2020-10-13
d8sxus,Exporting Models to build own inference server,"Hello, I was hoping to get pointed in the right direction.  After training a random forest classifier I am looking to export the model in such a way that I can recreate each of the trees in C++.  I am trying to figure out the best approach to this, or if it is even possible.  My research online mainly shows examples of how to visually represent these, and how to create a pickle project for python serialization.  

Am I missing some key terms in my search? Could you point me to what i should be doing to figure this out?

&amp;#x200B;

My approach so far has been exploring the clf.estimators.trees\_ part of the estimators object, but I am not sure if I am on the right track.

&amp;#x200B;

Any help is much appreciated.

&amp;#x200B;

Thanks!",1,scikit_learn,2020-10-13
d244x4,Predict device from flow,"Hey guys, I applied to a competition about AI and my task is to predict device class from flow. I have 13 types of classes which are all in train set but the test set is missing that one column. After I run training and then I try to predict it, I receive an error stating this: ValueError: query data dimension must match training data dimension.

How can I predict a column that is not there? I don't believe that I have to manually put the column to the test.json 

Thanks for advices.",3,scikit_learn,2020-10-13
cvtjk6,Predicting Churn With Nested Data,"Hello All!

Ok, so this is a bit of a challenge and I'm trying to figure out if it is even worth worrying about the nesting aspect of the data. Basically, I'm trying to predict subscription-level churn with a combination of subscription-level and user-level variables.

Since users own subscriptions I figured I should try to account for nesting in my model. Does anyone have any recommendations on how to attack churn predictions using a nested model? Any suggestions would be greatly appreciated. Again, I have code working, but I've never built anything that requires nested analysis.

Basically my question is: Is it possible to run a multi-level SVM?",2,scikit_learn,2020-10-13
cs2urp,What is the most efficient way to implement two-hot encoding using scikit learn?,"I have two very similar features in my dataframe, and I would like to combine their one-hot encoded versions. They are both categorical data, and they both contain the same categories. I was thinking about using OneHotEncoder from scikit learn and getting the union of the corresponding columns. Is there a function or more efficient way that I do not know about?",3,scikit_learn,2020-10-13
cnnacy,Feature elimination doesn't really eliminate anything.,"I had a fairly simple dataset, after plotting the correlation matrix I noticed that one variable has very low correlation with the target (0.04) but instead of deleting it manually I decided to try feature elimination.
I tried both RFE and RFECV with Logistic Regression as an estimator, RFE eliminated some features which seemed correlated with the output and kept that feature.
RFECV didn't eliminate anything at all.

Am I missing something here?",1,scikit_learn,2020-10-13
cnl2a5,k-means output issue,"Hello I've run a k-means over my voice data. I got two class (for best). My problem is why i got this line at the right side? I sit an issue in my dataset?

https://preview.redd.it/n5lz9pggx7f31.png?width=852&amp;format=png&amp;auto=webp&amp;s=e84cc13f9579c026fc6bfa5cbd85849b1ea2939e",1,scikit_learn,2020-10-13
cmmbi5,Running scikit validation on 24 cores is slow?,"Hello guys, maybe anyone can help me out here. I am running following validation code:

```
from sklearn.linear_model import LinearRegression
model = LinearRegression()
from sklearn.preprocessing import PolynomialFeatures
poly_transformer = PolynomialFeatures(degree=2, include_bias=False)
from sklearn.pipeline import Pipeline
pipeline = Pipeline([('poly', poly_transformer), ('reg', model)])
train_scores, valid_scores = validation_curve(estimator=pipeline, # estimator (pipeline) X=features, # features matrix y=target, # target vector param_name='pca__n_components', param_range=range(1,50), # test these k-values cv=5, # 5-fold cross-validation scoring='neg_mean_absolute_error') # use negative validation
```

in the same .py file on different machines, which I would name #1 localhost, #2 staging, #3 live, #4 live. localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds
live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.
In order to get more ""trustworthy"" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different?",1,scikit_learn,2020-10-13
clz2bl,vectorization,"Hi, I just want to know if I can vectorize a text even if its on another language using Count Vectorization",2,scikit_learn,2020-10-13
clpubv,Machine learning final year project,"design and implement an intelligent agent that can detect a fault and can trouble a faulty server on a network

Its a network anormaly project 
But dont know where to start from",1,scikit_learn,2020-10-13
cl88rf,No Scikit-learn after I installed Anaconda in Sublime Text 3," 

I started using Sublime Text as my Text Editor/IDE (not sure what the difference is) to do some Python projects. After watching 2 episodes of the machine learning course by Google Developers. I installed the Anaconda package which has the Scikit-learn included.

Following the video I typed:

    import sklearn

This error appeared:

    ModuleNotFoundError: No module named 'sklearn'

Is there a way to install the Scikit-learn using the Sublime Text 3 or using a different method?",1,scikit_learn,2020-10-13
cgijm0,Unable to find/import,"edit: Title - Unable to find/import IterativeImputer

&amp;#x200B;

&amp;#x200B;

Hello fellow users, I'm wondering if yall could help me out with importing/finding IterativeImputer...

**&gt;&gt;&gt;** *# explicitly require this experimental feature*

**&gt;&gt;&gt; from** **sklearn.experimental** **import** enable\_iterative\_imputer *# noqa*

**&gt;&gt;&gt;** *# now you can import normally from impute*

**&gt;&gt;&gt; from** **sklearn.impute** **import** IterativeImputer

**ModuleNotFoundError**: No module named 'sklearn.impute.\_iterative'; 'sklearn.impute' is not a package

&amp;#x200B;

$pip freeze states I have scikit-learn==0.21.2 and sklearn==0.0

Python version 3.6

&amp;#x200B;

After researching the issue online I see that there's an experimental version I need to install, but I can't seem to find it! Further, I can't find it on their website.. [https://scikit-learn.org/dev/versions.html](https://scikit-learn.org/dev/versions.html)

What did I overlook/miss?",1,scikit_learn,2020-10-13
cc2mxr,How to re-structure a numpy dataframe into a format I can use in sklearn?,"Assuming the dataframe column 0 is the target and columns 1: are the features, and that each column is named, what's the easiest way to split the data for use in sklearn?",1,scikit_learn,2020-10-13
cbm4g6,How to classify dots,"Hello, 

I have a graph with two groups, red and blue dots. These groups are clearly separated, but the problem is that I want to say if a new dot belongs to the red group, to the blue, or to none of them.

What method do you recommend?

Thank you",1,scikit_learn,2020-10-13
c4rlf7,Regression is not yielding many useful predictions,"Hello all,   


I'm using a linear regression to predict continuous values (how long until a client churns measured in months). I have a dataset of cancelled accounts and active account. I'm using the cancelled accounts to predict the active accounts. I have a variety of explanatory variables and in total I have an R-squared of around 35% (obviously R-squared isn't perfect)   


Overall, this works pretty well; however, one issue I'm running into is that, of the predictions I get back, some are negative and very few actually predict that these active clients should still be active. In other words, many of the predicted cancellation dates are in the past.  


Dumb question, but is there a method I could be using to help this? Overall, I'm getting about 10k useful observations from 60k predictions.  Any suggestions would be greatly appreciated.",1,scikit_learn,2020-10-13
c4q5i6,I can't import Kmeans into compiler,"I'm currently using sklearn 0.21.2, and when I do:  


`import sklearn.cluster.KMeans`

&amp;#x200B;

the compiler returns error:

&amp;#x200B;

`no module named sklearn.cluster.KMeans`

&amp;#x200B;

I've found that in the cluster package, there is an module named 'cluster.k\_means\_'  


But when I tried to use this instead, it shows error

&amp;#x200B;

`Module is not callable`

&amp;#x200B;

Now I don't know why I can't import the kmeans package in cluster.",1,scikit_learn,2020-10-13
bylpjd,Sklearn regression with two datasets,"Hello all,   


basically, as the title implies I'm trying to train a regression model on one dataset and the apply that predictive model to another dataset. In other words, I have a model which predicts cancelled accounts and the amount of time in which those accounts cancel.   


I have another dataset full of active accounts (with the same variables) and I'm attempting to use the model from the cancelled accounts to predict when my active accounts will cancel. I'm having trouble with this.  Is there a way to do this without forcing a t  


Is there a way to use the ""active dataset"" without enforcing a Train\_test\_split? Any help would be greatly appreciated. Thank you!",2,scikit_learn,2020-10-13
bvjxzj,Get the function that fits my data,"I have fit a polynomial regressor to a two dimensional data. 
Is there a way to see the function that fits this data?",2,scikit_learn,2020-10-13
bqtxes,Kmeans clustering cache the result,"Hello,

&amp;#x200B;

I am new to scikit and I was wondering if I could cache the result of Kmeans so next time when I run my script I do not create the centroids again - that means save the result of [`kmeans.fit`](https://kmeans.fit)`()`.",2,scikit_learn,2020-10-13
bp7dv5,Get classes name of each estimator in OneVsOneClassifier,"Are there any ways to do that ? I am trying to directly access the classes\_ attributes in the estimator but it only returning \[0,1\]",2,scikit_learn,2020-10-13
bevq9d,Using Blob Detection methods on huge images,"I'm trying to use common blob detection methods from

[https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob\_dog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog)

on a huge images (about 6000x6000 pixels). It takes way too long to compute and show the result. How could I resolve this?",1,scikit_learn,2020-10-13
bcwbv4,Calculate variance of accuracy,"Hello, how can I calculate the variance of accuracy between two models in Random forest.
I mean I made a simple model with DecisionTreeClassifier() and one more with BagginClassofier() using the first model on it.
The accuracy climb +0.237.

How to get variance of that accuracy?
Thansk",1,scikit_learn,2020-10-13
bcbduy,Classification: Minimizing the amount of false positives,"Hey there,

I posted an earlier post (now deleted) that phrased this a bit wrong (thanks Imericle). Here is another try: 

Many  (most?) classification algorithm seem to be about maximizing accuracy  (true positives + negatives). My aim is to minimize the amount of false positives. How would I achieve this?

Only options I see to achieve this is through parameters tuning, is that the right approach?

(Thinking on applying it to a RandomForest),

Thanks,

Bb",2,scikit_learn,2020-10-13
bbxi9t,KMeans: Extracting the parameters/rules that fill up the clusters,"Hi all,

&amp;#x200B;

I have created a 4-cluster k-means customer segmentation in scikit learn. The idea is that every month, the business gets an overview of the shifts in size of our customers in each cluster. 

My question is how to make these clusters 'durable'. If I rerun my script with updated data, the 'boundaries' of the clusters may slightly shift, but I want to keep the old clusters (even though they fit the data slightly worse). My guess is that there should be a way to extract the paramaters that decides which case goes to their respective cluster, but I haven't found the solution yet. 

I would appreciate any help",1,scikit_learn,2020-10-13
b6nfvs,Question about FeatureUnion,"    pipe = Pipeline([
            ('features', FeatureUnion([
                    ('feature_one', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_one')),
                        ('vec', cvec) # Count vectorizer
                    ])),
                    ('feature_two', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_two')),
                        ('vec', tfidf) # Tf-idf vectorizer
                    ]))
                ])),
            ('clf', OneVsRestClassifier(clf)) #clf is a support vector machine
        ])

I'm using this pipeline for a project I'm working on, and I just want to make sure I understand how FeatureUnion works. I'm building a classifier which takes in two different text features and attempts to make a multi-class classification.

&amp;#x200B;

To give a little more detail, I'm trying to classify news articles into one of several categories (sports, business, etc.) Feature one is a list of tokens taken from the article's url, which often, though not always, explicitly states the name of the topic. Feature two is a list of tokens from the body of the article.

&amp;#x200B;

Does it make sense to separate the two features this way? Does this have a different effect than if I had just merged all of the tokens into a single list and vectorized them? My intention was to allow the two features to effect the model to different degrees, since I figured one would be more predictive in most scenarios (and I am getting pretty great results.)",2,scikit_learn,2020-10-13
b36h5a,Ranforest random behaviour,"If I give random forest parameters as RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019) Should it be creating 10 same decision trees? But it is not. I am asking the random forest to
     1.Sample without replacement (bootstrap=False) and each tree have same number of sample (ie the total data )(verified using plot)
     2.Select all features in all trees.
But model.estimators_[2] and model.estimators_[5] are different

",2,scikit_learn,2020-10-13
axgj2c,Predicting the runtime of scikit-learn algorithms,"Hey guys,

We're two friend who met in college and learned Python together, we co-created a package which can provide an estimate for the training time of scikit-learn algorithms.

The main function in this package is called “time”. Given a matrix vector X, the estimated vector Y along with the Scikit Learn model of your choice, time will output both the estimated time and its confidence interval. 

Let’s say you wanted to train a kmeans clustering for example, given an input matrix X. Here’s how you would compute the runtime estimate:

    From sklearn.clusters import KMeans
    from scitime import Estimator 
    kmeans = KMeans()
    estimator = Estimator(verbose=3) 
    # Run the estimation
    estimation, lower_bound, upper_bound = estimator.time(kmeans, X)

We are able to predict the runtime to fit by using our own estimator, we call it meta algorithm (meta\_algo), whose weights are stored in a dedicated pickle file in the package metadata.

The meta algos estimate the time to fit using a set of ‘meta’ features, including the parameters of the algo itself (in this case kmeans) and also external parameters such as cpu, memory or number of rows/columns. 

We built these meta algos by generating the data ourselves using a combination of computers and VM hardwares to simulate what the training time would be on the different systems, circling through different values of the parameters of the algo and dataset sizes . 

Check it out! https://github.com/nathan-toubiana/scitime

Any feedback is greatly appreciated.",6,scikit_learn,2020-10-13
aahf76,"Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?","Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?

&amp;#x200B;

So in the sense that if one applies e.g. a gain factor  (or a function depicting gain changes), then it's applied to the correct direction.",3,scikit_learn,2020-10-13
a75oid,"classification_report + MLPClassifier(): UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn_for)","classification\_report on a prediction done on MLPClassifier() sometimes throws:

&amp;#x200B;

*UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn\_for)*

&amp;#x200B;

but not on all the time.

&amp;#x200B;

What could be wrong?

&amp;#x200B;

\---

&amp;#x200B;

Doing

&amp;#x200B;

set(y\_test) - set(y\_pred) 

&amp;#x200B;

I'm able to see that sometimes some label is missing from y\_pred. But why does this occur only occasionally?

Is something wrong with how I use MLP?",1,scikit_learn,2020-10-13
a753f2,"{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification_report()","{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification\_report()

&amp;#x200B;

Why?

&amp;#x200B;

I call it like:

&amp;#x200B;

    classification_report(y_test, y_pred)

where y\_pred is predicted using a model I built.

&amp;#x200B;

""Quite obviously"" the arguments are incompatible somehow, but how can I find out, how? And how can I make them compatible?

&amp;#x200B;

\---

&amp;#x200B;

I tried:

&amp;#x200B;

    from sklearn.utils.multiclass import type_of_target

&amp;#x200B;

    &gt;&gt;&gt; type_of_target(y_test)
    'multiclass'
    
    &gt;&gt;&gt; type_of_target(y_pred)
    'continuous'

&amp;#x200B;",1,scikit_learn,2020-10-13
a746h0,"Is there any way to ""estimate"" how long a given computation in sklearn will take?","Is there any way to ""estimate"" how long a given computation in sklearn will take?

&amp;#x200B;

So that one doesn't need to wait longer than what one can?

&amp;#x200B;

Also, since Windows Task Manager shows only modest CPU use (&lt; 10%), then how is one supposed to know, what's occurring in the model?",2,scikit_learn,2020-10-13
a73oda,What are the most important parameters in LogisticRegression()?,What are the most important parameters in LogisticRegression()?,3,scikit_learn,2020-10-13
a71xf2,How does one feed hidden_layer_size tuples into GridSearchCV's param_grid?,How does one feed hidden\_layer\_size tuples into GridSearchCV's param\_grid?,1,scikit_learn,2020-10-13
a0yprf,Code review,"Hello,

I'm new to ML and scikit - hope this is the correct place for this. Have created the below code that appears to be working but wanted to get the opinions of people with more experience then me, to check I haven't a made any major errors or if there are any obvious improvements?

&amp;#x200B;

I am trying to train a model on a data set of potentially hundred of thousands emails. Every few days I want to retrain the exported model using incremental learning on the new emails received since the model was last trained.

The below reads the initial data from a csv, runs HashingVectorizer then SGDClassifier. The OnlinePipeline is used to allow me to use partial\_fit when I try to retrain later in the process.

`import pandas as pd`

`data = pd.read_csv('customData1.csv')`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`from sklearn.feature_extraction.text import HashingVectorizer`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

&amp;#x200B;

`from sklearn.linear_model import SGDClassifier`

`text_clf = OnlinePipeline([('vect', HashingVectorizer()),`

`('clf-svm', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=5, random_state=None)),`

`])`

`text_clf = text_clf.fit(X_train,Y_train)`

`predicted = text_clf.predict(X_test)`

`np.mean(predicted == Y_test)`

The above gives me an accuracy of 0.55

&amp;#x200B;

A few days later when I have new emails I import the previously exported model and use partial\_fit on a new csv file.

`import pandas as pd`

`data = pd.read_csv('customData2.csv') #text in column 1, classifier in column 2.`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

&amp;#x200B;

`from sklearn.externals import joblib`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

`text_clf2 = joblib.load('text_clf.joblib')`

&amp;#x200B;

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`text_clf2 = text_clf2.partial_fit(X_train,Y_train)`

&amp;#x200B;

`predicted = text_clf2.predict(X_test)`

`np.mean(predicted == Y_test)`

This returns the improved accuracy of: 0.84

&amp;#x200B;

Sorry for so much code!  I obviously need to tidy it all up so its a single method and handle the import/export logic properly.

&amp;#x200B;

Have a made any major errors or are there any obvious improvements? Thanks!

&amp;#x200B;",1,scikit_learn,2020-10-13
a0iz4i,Does cross_val_score tell something about generalizability?,"Does cross\_val\_score tell something about generalizability?

&amp;#x200B;

Or do I need to use something else for measuring generalizability?",0,scikit_learn,2020-10-13
a0c8dw,"Is there a problem if MLPRegressor doesn't converge for max_iter=100, but nor max_iter=5000 either?","Is there a problem if MLPRegressor doesn't converge for max\_iter=100, but nor max\_iter=5000 either?

&amp;#x200B;

Anything else I could try?",1,scikit_learn,2020-10-13
a0b260,What do cv (number of folds) and the number of outputs in cross_val_score correspond to?,"What do cv (number of folds) and the number of outputs in cross\_val\_score correspond to?

&amp;#x200B;

Does it mean that it produces cv number of different scores? Or (as I read somewhere) that only the last score might be the meaningful one (I read something like the others than the last used to ""fit"", while the last is the score)?",2,scikit_learn,2020-10-13
a0awuh,"Getting values in range [-191806. ..., 0.77642 ...] from cross_val_score, am I doing something wrong?","Getting values in range \[-191806. ..., 0.77642 ...\] from cross\_val\_score, am I doing something wrong?

&amp;#x200B;

    mlp = MLPRegressor(hidden_layer_sizes=(7,))

mlp.fit(X\_train,y\_train) mlp\_y\_pred = mlp.predict(X\_test)

&amp;#x200B;

y\_pred is an earlier prediction using LinearRegression().

&amp;#x200B;

I call cross\_val\_score like:

&amp;#x200B;

    cross_val_score(mlp, y_pred, mlp_y_pred, cv=10)

&amp;#x200B;

Output is:

&amp;#x200B;

    00 = {float64} -4.4409160725075605
    01 = {float64} -673636.0674512024
    02 = {float64} -51282.162171235206
    03 = {float64} -399557.4789466267
    04 = {float64} -35.73093353875776
    05 = {float64} -1406.9741325253574
    06 = {float64} -80853.84044929259
    07 = {float64} -5132.870883709122
    08 = {float64} -283.7432365432288
    09 = {float64} -2.860321933844385

&amp;#x200B;

I think I should be getting values in range \[0,1\].",1,scikit_learn,2020-10-13
a0at1q,"Is MLPRegressor's hidden_layer_sizes=(7,) equivalent to hidden_layer_sizes=7?","Is MLPRegressor's hidden\_layer\_sizes=(7,) equivalent to hidden\_layer\_sizes=7?",1,scikit_learn,2020-10-13
a09ukl,"Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train_test_split(Xy,shuffle = False, test_size = 0.33)?","Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train\_test\_split(Xy,shuffle = False, test\_size = 0.33)?

Xy has been constructed like:

&amp;#x200B;

    X = dat.data
    y = dat.target 
    Xy = np.hstack((X,np.array([y]).T))

It seems that it returns only two arrays, even when I saw an example ([https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input](https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input)) do 

 

    X_train, X_test, y_train, y_test = train_test_split(Xy,shuffle = False, test_size = 0.33) ",1,scikit_learn,2020-10-13
a06iin,Runtime Error in RandomizedSearchCV,"I've been running a RandomForestClassifier on a dataset I took from UCI repository, which was taken from a research paper. My accuracy is \~70% compared to the paper's 99% (they used Random Forrest with WEKA), so I want to hypertune parameters in my scikit learn RF to get the same result (I already optimized feature dimensions and scaled). I use the following code to attempt this (random\_grid is simply some hard coded values for various parameters):

&amp;#x200B;

    rf = RandomForestClassifier()
    # Random search of parameters, using 2 fold cross validation,
    # search across 100 different combinations, and use all available cores
    rf_random = RandomizedSearchCV(estimator = rf,  param_distributions = random_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)
    # Fit the random search model
    rf_random.fit(x_train, x_test)

When I attempt to run this code though my python runs indefinitely (for at least 40 min before I killed it) without giving any results. I've tried reducing the \`cv\` and \`n\_iter\` as much as possible but this still doesn't help. I've looked everywhere to see if there's a mistake in my code but can't find anything. I'm running Python 3.6 on Spyder 3.1.2, on a crappy laptop with 8Gb RAM and i5 processor :P

&amp;#x200B;

Here is the random\_grid if it helps:

    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
    max_features = ['auto', 'sqrt']
    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
    max_depth.append(None)
    min_samples_split = [2, 5, 10]
    min_samples_leaf = [1, 2, 4]
    bootstrap = [True, False]
    
    # Create the random grid
    random_grid = {'n_estimators': n_estimators,
                   'max_features': max_features,
                   'max_depth': max_depth,
                   'min_samples_split': min_samples_split,
                   'min_samples_leaf': min_samples_leaf,
                   'bootstrap': bootstrap}

&amp;#x200B;",1,scikit_learn,2020-10-13
9yhcqq,Does sklearn have built-in routines for testing results of LinearRegression()?,Does sklearn have built-in routines for testing results of LinearRegression()?,1,scikit_learn,2020-10-13
9ygvsi,How does fit_transform allow for other data to be processed with the same transformer?,"How does fit\_transform allow for other data to be processed with the same transformer?

&amp;#x200B;

Like here:

&amp;#x200B;

[https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)

&amp;#x200B;

Particularly, since one first calls fit\_transform, then why does it allow one to call transform afterwards and still get the same fit? Like how is this kind of functionality implemented?

&amp;#x200B;

&amp;#x200B;",1,scikit_learn,2020-10-13
9tf8yw,Principal component Analysis: predicting values,"I am attempting to forecast a set of multivariate time series data. I have run a PCA (using the scikit-learn module) and have run an AR(1) auto-regression of the 3 components.

Now that I have the projects component values, how do I recast those components into the original variables, in order to find the projection for those variables?",2,scikit_learn,2020-10-13
9t3xwo,Extract a single stratified part of a dataset,"I have a multi-label dataset with N samples, and I want to take a chunk out to reserve for validation, e.g. reserve k% of the dataset.

Note that I want to do this just once, else I could use stratifiedKFold.  
Is there a function to produce such a single chunk, ensuring stratification with respect to  the labels?  
(A workaround would be to produce N\*k  KFold splits, concatenate  all parts but one  for training, and use the last for validation.)

Thanks.",1,scikit_learn,2020-10-13
9sbqvm,Stepping through each iteration of the LogisticRegression fit() function,"Hello guys,

I'm using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class to find the decision functions between my classes. I wanted to ask you - *how can I step through each step of the algorithm?* I know I can give the parameter `max_iter` to determine the number of iterations, but I want to step through each of those `max_iter` iterations - to see how the values of the coefficients change.

Thanks in advance!",1,scikit_learn,2020-10-13
9pcack,"modAL: A modular active learning framework for Python, built on top of scikit-learn",,5,scikit_learn,2020-10-13
8ndofu,Parse Twitter feed and suggest domain names • r/nltk,"I'm working on a hackathon, and I'd like to parse a user's last 100 tweets or so and make recommendations for a domain name using a new TLD.

The plan I've got in my head is

1\) Scrape twitter for a bit and get some data \(How much? How many records?\)

2\) Run tf\-idf against it, save that dataset

3\) split the initial twitter data into groups based on which tweets contain each TLD \- supplies, computer, kitchen, etc.

a\) Run some kind of clustering algorithm against each set? 250 or so TLDs

\-\- This is where I have questions

4\) Scrape their twitter feed and get 100 tweets

5\) Use the tf\-idf data from step 2 to spit out keywords

6\) use those keywords using some kind of distance formula against the clustered data to pick a tld?

7\) use the bigrams or keywords to make up an SLD.

This seemed off to a good start, but can I somehow pickle the cluster results? Or have multiple sets of cluster results in the same object?

Note: 95&amp;#37; of my knowledge on this topic comes from this blog post: [http://brandonrose.org/clustering](http://brandonrose.org/clustering)",2,scikit_learn,2020-10-13
8l4imo,"move partial of decision models from server to client - side, is it good idea?","Hi,
Some time ago tenser flow for js was released. I'm wondering about build bridge for some scikit learn models to move some part of learning and prediction to the client side. I think that it could help minor companies reduce server resource usage and make models and prediction much more personalised. Do you think it's a good idea? Do you know whether someone has tried something similar before?",1,scikit_learn,2020-10-13
8emc3b,How to combine num values with text data for classification?,I build website classifier and use text of each webpage (transformed to bag of words) as train data. But I also want to add each website's PageRank as feature. How can I do that?,2,scikit_learn,2020-10-13
89u8tj,PLSRegression Issues,"I'm working with scikit's cross\_decomposition.PLSRegression(). According to their documentation, x = np.multiply( x\_scores\_, x\_loadings\_.T ). I'm not getting anything close to the same value values. I've tried every combination of using scale=False and sklearn.preprocessing.scale(x) to try and find how this works out, but I haven't been able to find one that works.

    plsr = PLSRegression(n_components=x_df.shape[0]-1).fit(x_df.T,y)
    print(np.matmul(plsr.x_scores_,plsr.x_loadings_.T).T)
    print(x_df)

Using scaled data (i.e. PLSRegression.fit( scale(x\_df).T, scale(y) ) and changing n\_components doesn't help either. If anyone has any idea of what mistake I have made, or if this just a bug in sklearn?",1,scikit_learn,2020-10-13
81xksg,How to remove terms from a term-document matrix?,"Hello,

I have a term document matrix that I've created using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) like so:
    
    X = vectorizer.fit_transform(corpus)
    X
    &lt;1000x10022 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    	with 94340 stored elements in Compressed Sparse Row format&gt;

I'd now like to remove any terms that do not appear in at least 3 documents, and then calculate the TF-IDF scores for each term, and select the vocabulary as the top n terms ordered by TF-IDF scores.

Is there an easy way of removing terms from the term document matrix that do not appear in at least 3 documents, while still conserving the mapping from feature names to feature indices?

I guess one way to do it would be to get the feature names of the terms that appear in at least 3 documents using numpy on the sparse matrix directly, assign them a mapping to indices, and then pass that mapping to the vocabulary parameter in the CountVectorizer constructor.

Any ideas on how to do this more easily?",1,scikit_learn,2020-10-13
7zfjba,How to use partial_fit to update the model trained with fit() instead of training from scratch,"I tried partial_fit with various scikit online learning classifiers like perceptron, passive aggresive classifiers, SGDclassifer... like here: https://ideone.com/uOtRTZ.  I just dont understand why i cant train the new data on top of already trained data. I am doing image classification. I have trained my 10,000 images with fit(). Now i got 1 new image to add to this dataset of already trained images. I want to update the trained model instead of training all 10,001. Is this possible with partial_fit() ? If so, please tell me how ? ",1,scikit_learn,2020-10-13
7z4qi7,SGDClassifier.partial_fit returns error of “classes should include labels”,"I tried to predict label of my newly added data through SGDClassifer.partial_fit as below:

        from sklearn import neighbors, linear_model
    import numpy as np
    
    
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""hinge"")#loss
    
        sgd_clf.fit(X, y)
    
        print(sgd_clf.predict([[6, 9]]))
    
        X.append([6, 9])
        y.append(5)
    
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here

However, this results in error: ValueError: `classes=array([1, 2, 3, 4, 5])` is not the same as on last call to partial_fit, was: array([1, 2, 3, 4])

Any ideas or references ? ",1,scikit_learn,2020-10-13
7vw4ap,Retrain a KNN classified model (scikit),"I trianed my knn classifer over multiple images and saved the model. I am getting some new images to train. I dont want to retrain the already existing model.

How to add the newly tranied model to the existing saved model ?

Could someone guide if this is possible or any articles describing the same ?

Thank you,",2,scikit_learn,2020-10-13
70m5l1,How do I add matplotlib to a django webapp and display the code's output on the webpage?,Trying to make a User Interface for a Support Vector Machine from the SVM function in the matplotlib,1,scikit_learn,2020-10-13
6lec9n,"K-NN and custom metrics, speed up sklearn using Cython",,2,scikit_learn,2020-10-13
6k5uvu,Build my first CART based algorithm feedback is welcome!,hey guys! i just made this: https://github.com/lucas-aragno/pokemon-classifier im pretty new to scikit so I'll appreciate any kind of feedback :),2,scikit_learn,2020-10-13
6ev2y7,FastICA,"It seems like all of the examples using fastICA involves taking 2 frequencies, mixing them a certain way, then unmixing them.

What about if I have a wav file. How can I use fastICA to break it down into multiple parts?

Any help would be appreciated. Thank you!",1,scikit_learn,2020-10-13
6eu5cr,Automate your Machine Learning in Python – TPOT and Genetic Algorithms,,2,scikit_learn,2020-10-13
69huhq,[P] Tracking and reproducibility in data projects (CLI tool),,1,scikit_learn,2020-10-13
5xk0iy,Scikit learn vs Open Cv for small problems in image processing,I am a Image processing noob. I've used Numpy and Scipy for some matrix related stuff before and OpenCV for some image processing problems. I recently learned that scipy lets me manipulate images too. What are the pros and cons of using OpenCv and Scipy I am not able to figure out which would be better for me. Appreciate your help!,6,scikit_learn,2020-10-13
5rb5ww,Using Category Encoders library in Scikit-learn,,1,scikit_learn,2020-10-13
5m0352,MLPClassifier: Multiple output activation,"I'm using MLPClassifier but some of the outputs have more than one activation, i.e. [0 1 1 0].
How can I get only one activation?

My code is:
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                    hidden_layer_sizes=(15,), random_state=1, activation='relu')

Thank you!",1,scikit_learn,2020-10-13
5fasve,Need help on scikit kfold validation,"Objective: To create 5 folds of training and test dataset using StratifiedKFold method. I have referred the documentation at http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.cross_validation.StratifiedKFold.html

I am able to print the indices alright but am unable to generate the actual folds. Here follows my code

from sklearn.cross_validation import StratifiedKFold
import pandas as pd
df=pd.read_csv('C:\Comb_features_to_be_used.txt')

##Getting only numeric columns
p_input=df._get_numeric_data()
## Considering all the features except labels
p_input_features = p_input.drop('labels',axis=1)
## Considering only labels [single column]
p_input_label = p_input['labels']
skf = StratifiedKFold(p_input_label, n_folds=5, shuffle=True)
i={1,2,3,4,5}
for i,(train_index, test_index) in enumerate(skf):
    ##print(""TRAIN:"", train_index, ""TEST:"", test_index)
    p_input_features_train = p_input_features[train_index] 
    p_input_features_test =  p_input_features[test_index]
        
I am getting the error: IndexError: indices are out-of-bounds

",2,scikit_learn,2020-10-13
54exlw,scikit-learn doc translation,"translate sklearn doc to chinese
feel free to join us
https://github.com/lzjqsdd/scikit-learn-doc-cn",2,scikit_learn,2020-10-13
52b7od,Improving the Interpretation of Topic Models,,1,scikit_learn,2020-10-13
50qfgg,Topic Modeling with Scikit Learn,,3,scikit_learn,2020-10-13
4rjkcq,Overfit Random Forest,"I have data where Random Forest models overfit to noise whatever 
hyperparameter I put.
(= excellent accuracy on training, but poor accuracy on prediction).


So, this is the process I did to over-come:
    1) Tweak the input data and reduce the sampling of noise (negative example)

    2) Fit the RF and test (confusion matrix) on cross-validation data. 

    3) Repeat it and choose the best cross validation data.

Is there a way to overcome this monte carlo approach,
using OOBag process during training ?

Also incorporate Cross validation to reduce the over-fitting ?

Importance features change every time a new RF is fit (it seems a lot of co-linearity and noise into the data).










",1,scikit_learn,2020-10-13
4h6ypj,Building scikit-learn transformers,,3,scikit_learn,2020-10-13
3zvwqw,Hello everyone! I want to write an oversampling module in compliance with scikit-learn. Advice needed!,"As mentioned in the title I want to write a module for oversampling classes in skewed datasets. I recently came to need such a module and I noticed that no such thing exists officialy in scikit-learn. I want it to be compatible with scikit-learn as I very often use it. Do you have any resources to redirect me to, apart from the official scikit-learn developer guidelines? Any tips for writing a python module in general?

Thanks in advance!",2,scikit_learn,2020-10-13
2f830i,Official Scikit-Learn page.,,1,scikit_learn,2020-10-13
j9q6bp,A scikit-learn compatible library to construct and benchmark rule-based systems that are designed by humans,,7,scikit_learn,2020-10-13
j93854,Best performance on Scikit-learn’s load_digits dataset,"On Scikit-learn’s load_digits dataset:
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html

Does anyone know what is the best performance achieved so far on this dataset? 

I tried googling around but can’t find examples with 100% score performance. I am thinking since this is a standard dataset, it would be easy to get a 100% score performance?",1,scikit_learn,2020-10-13
j8vepi,"could someone ELi5 the hyperparameters (penalty, C, tol, max_inter)",I am currently working on a beginner project on logistic regression using scikit\_learn. I am trying to fine tune my regression model but cant seem to find any websites that can explain what the parameters mentioned in the title mean exactly and how to use them. I was wondering if anyone could give me a quick explanation on what/how to use these parameters to fine tune my regression model.,0,scikit_learn,2020-10-13
j7jz85,SVC rbf kernel seems to be nonstandard?,"I am currently testing a precomputed version of rbf I implemented to get a better feel for how it works and possibly later check out some other kernels.

It seems that whatever I do, I get different results using my precomputed gram matrix vs using the scikit rbf kernel:

To calculate a kernel entry for datapoints xm &amp; xn, based on some extra parameters theta

`k = thetas[0] * np.exp(-(thetas[1]/2.) * (np.sqrt((xn-xm).T @ (xn-xm)))) + thetas[2] + thetas[3] * (xn.T @ xm)`

using theta = \[1,2,0,0\]

This should recover the formulation given [here](https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel) (setting gamma=1)

1 \* exp( - 2/2|xn-xm|^(2) )   


is there something I'm missing? \[here's the code if you wanna take a look\]([https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit\_test.py](https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit_test.py)) (only dependencies are matplotlib, scikit &amp; numpy, so you're probably good if you're on this sub)",1,scikit_learn,2020-10-13
j5pcm6,ImportError,"I've got a long error which ends with:  
ImportError: DLL load failed while importing \_arpack: Não foi possível encontrar o procedimento especificado. (rough translation: Unable to find the specified procedure)  


Any idea on the issue? It seems like I have some sort of update issue, but I'm unable to find what.",2,scikit_learn,2020-10-13
j2v11w,"Scikit-learn. In the case of a single point, k-nearest neighbours predictions doesn’t literally match with the literally nearest point. I think I know why. Correct me if I’m wrong.","Hello.  I’ve looked at the source code. 

Case population sizes in the range 10 ^ 2 to 10 ^ 5 ish. Vanilla, straight out the box knn from scikit-learn.  Except 1 nearest neighbours not the default 5.  

When I try to predict the nearest neighbour of a point, using 1 nearest neighbours. after using knn.fit to make a model, it doesn’t _always_ return the same value of the actual nearest neighbour.  I’ve worked out the actual real nearest neighbour myself to check, using trig, and unit tested it.  

I think that’s because for pragmatic reasons knn is just a probabilistic model applied at group level.  Not exactly the actual knn for each and every point.  

Am I right?

EDIT:  My. Trig. Was.  Wrong.  Due. To.  A. Data frame. Handling.  Issue.  Ggaaahhhh.",5,scikit_learn,2020-10-13
j1idtx,RadomizedSearch CV taking forever,"Hi ,

I have the below snippet.

Trying to run on GCP . its getting stuck and not even updating.

&amp;#x200B;

https://preview.redd.it/bp2zfi71sxp51.png?width=1463&amp;format=png&amp;auto=webp&amp;s=6d97d1ff6083f6eb65e62d983770c69f06d45f4c",2,scikit_learn,2020-10-13
iv8jv9,Neuraxle - a Sklearn-Based Clean Machine Learning Framework,,1,scikit_learn,2020-10-13
it82un,How the 'init' parameter of GradientBoostingRegressor works?,"i'm trying to create an ensemble of an determined regressor, with this in mind i've searched for some way to use the sklearn already existing ensemble methods, and try to change the base estimator of the ensemble. the bagging documentation is clear because it says that you can change the base estimator by passing your regressor as parameter to ""base_estimator"", but with GradientBoosting you can pass a regressor in the ""init"" parameter. my question is: passing my regressor in the init parameter of the GradientBoosting, will make it use the regressor i've specified as base estimator instead of trees? the documentation says that the init value must be ""An estimator object that is used to compute the initial predictions"", so i dont know if the estimator i'll pass in init will be the one used in fact as the weak learner to be enhanced by the bosting method, or it will just be used at the beginning and after that all the work is done by decision trees. If someone can help me with this question i would be grateful.",5,scikit_learn,2020-10-13
igtkry,Best way to get T-Stastic and P-value etc?,"I'm using scikit learn for linear regression.  Is there a way to use that library to generate things like T-Stastic and p-value and standard error etc?

On stack overflow i found this, but wondering if there's a way within scikit

    import statsmodels.api as sm
    from scipy import stats
    X2 = sm.add_constant(X)
    est = sm.OLS(y, X2)
    est2 = est.fit()
    print(est2.summary())

&amp;#x200B;",1,scikit_learn,2020-10-13
i4ulg2,"Data Visualization using ""Python"" with ""Seaborn"" | Part- I",https://youtu.be/X400eIcV-So,3,scikit_learn,2020-10-13
i3546z,Recommendation based on other user following,"Hello,

I try to build a recommendation system.

My service allow users to follow people (not rate them, just follow) and I would like to be able to propose to users to follow people based on other user’s database activity.

Is scikit a good path for this ? 

Do you recommend specific method or useful ressource to read to achieve this ?

For your help guys!",2,scikit_learn,2020-10-13
hzw282,How to use TensorFlow Object detection API to detect objects in live feed of webcam in real-time,,1,scikit_learn,2020-10-13
hwmcf1,sklearn CCA - how to get variance explained for first canonical relationship?,"Hi. I'm exploring multivariate brain-behaviour relationships with sklearn's canonical correlation analysis tool ([https://scikit-learn.org/stable/modules/generated/sklearn.cross\_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca)). I am interested mostly in the first canonical relationship between the two datasets. The decomposition is working fine and i have the weights/canonical scores etcetera - but what i'd really like to know is how much of the variance in either dataset is explained by that one relationship (analogous to eg variance explained by first principal component).

There is a method named 'score' that i can call on the CCA object but I am not quite sure this is what I need. This score is not the same as 'canonical scores above but will supposedly get some coefficient of determination r\^2 between 'observed' and 'predicted' - not sure how to understand this. The description on the webpage is quite terse and it does not behave the way i might expect.

I'm hoping to find someone who might know whether that 'score' method  will get me to what i want - and if so, maybe how to use it. Or point me otherwise in the right direction to get into the variance explained for CCA.

Cheers!",2,scikit_learn,2020-10-13
hu6y83,KMeans Algorithm Question,"Hey all.

I am new with using scikit-learn and had a question regarding the KMeans algorithm functions. After running the algorithm and plotting the clusters, are the clusters with the centroids plotted the final clusters after training is done or is there training that I have to do on the clusters? 

Thanks everyone",1,scikit_learn,2020-10-13
htugnl,"How to handle ""Missing Value"" from ""Dataset"" using ""Pandas"" &amp; ""Sci-Kit Learn""??",https://youtu.be/8IORSsZIyIQ,0,scikit_learn,2020-10-13
htmc59,"How to handle ""Text"" and ""Categorical Attributes"" using Python and Pandas??",https://youtu.be/4sO7Pezlegk,0,scikit_learn,2020-10-13
ht4ol1,Making ROC curves with results from cross_validate?,"I am running 5 fold cross validation with a random forest as such:

from sklearn.ensemble import RandomForestClassifier

from sklearn.model\_selection import cross\_validate

forest = RandomForestClassifier(n\_estimators=100, max\_depth=8, max\_features=6)

cv\_results = cross\_validate(forest, X, y, cv=5, scoring=scoring)

However, I want to plot the ROC curves for the 5 outputs on one graph. The documentation only provides an example to plot the roc curve with cross validation when specifically using StratifiedKFold cross validation (see documentation here: [https://scikit-learn.org/stable/auto\_examples/model\_selection/plot\_roc\_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py))

I tried tweeking the code to make it work for cross\_validate but to no avail.

How do I make a ROC curve with the 5 results from the cross\_validate output being plotted on a single graph?

Thanks in advance",2,scikit_learn,2020-10-13
hm6td7,Best performance on MNIST - Fashion dataset,Does anyone know what is the best performance achieved so far for the MNIST - Fashion dataset along with what model that was used?,1,scikit_learn,2020-10-13
hm22vz,"How to ""Predict"" my friends weight using ""Machine Learning"" and ""Sci-Kit Learn""??",https://youtu.be/A4JwnkTFEXI,2,scikit_learn,2020-10-13
hl4k0u,Factor analysis “model” in CS229,"In one of Stanford’s CS229 lecture by Andrew Ng (https://m.youtube.com/watch?v=tw6cmL5STuY), he talks about a factor analysis “model” in which is to deal with situations where you have a lot more features than samples in your dataset. He even said he used a modified version of this factor analysis “model” in some recent work he did for a manufacturing company in the lecture.

Now my understanding of factor analysis is just a dimension reduction technique. So how did Andrew used factor analysis to build a “model” which deals with datasets which has a lot more features than samples?",2,scikit_learn,2020-10-13
hkm9qn,StackingRegressor Inconsistent Output,"Is it intentional that StackingRegressor returns different accuracy outputs when running multiple times given the same parameters, models and using numpy set seed?",1,scikit_learn,2020-10-13
hjctba,"This lecture that talks about what Multilabel and Multioutput classifications are, along with their implementation using scikit learn.",,1,scikit_learn,2020-10-13
hg5j3s,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,2,scikit_learn,2020-10-13
hf60u0,"Hey guys, here is a lecture on how to implement gradient descent with scikit-learn. Enjoy :)",,1,scikit_learn,2020-10-13
hakk07,How do I create a linear regression for this groupedby dataframe?,"I have this assignment for a job interview and I really want to impress by using some machine learning. I don't know too much about it and I essentially don't have much time to learn that much about it. I have the following [dataframe](https://postimg.cc/0zNbyrV8) and I want to create a linear regression using scikitlearn of \['profit'\] vs \['dateReceived'\] for each \['Language'\]. 

Does anyone know what I can do for that to work? I guess it should be just a few lines of code, but I could be wrong?",0,scikit_learn,2020-10-13
h7ay1o,"Visualize Scikit-learn models – ROC, PR curves, confusion matrices etc",,8,scikit_learn,2020-10-13
h172dr,Scikit Learn Tutorial in One Hour,,5,scikit_learn,2020-10-13
h0w3xx,Books about classification algorithms,"Hi all,

I am completely new to data mining and have to write a seminar paper about classification and do some programming in python.

With the help of datacamp I was able to implement the classification algorithms in python.

Now I am looking for some sources that I can cite in my paper that briefly explain these algorithms.

My problem is that most books that I have look into so far are very mathematical and since I don’t have a data mining/computer science background, they are hard to understand.

Do you have any recommendations for some text books that explain classification algorithms such as SVM, Naive Bayes, Trees, etc. that are well recognized, but explain them in an easy way?

Many thanks in advance!",1,scikit_learn,2020-10-13
gziaus,How to choose best pair of random state and class label values?,"For the last few days, I was trying to implement the KMeans algorithm using SciKit Learn, But I came across a very confusing problem. I have a dataset that has two class labels ['ALL', 'AML'] where ALL has 47 and AML has 25 samples and 100 attributes to train from and now I want to use this dataset for KMeans clustering so that I can compare the predicted results with the original class labels. Before asking my question let me explain certain scenarios. In all the scenarios I have taken all the 100 attributes to fit the model.

Scenario 1:

In the first run, I started with a model that is created with pretty much default arguments i.e. model = KMeans(n_clusters=2). For comparing the predicted class labels(which are numeric) with the original labels(which are strings), I set the original class labels as ALL = 1 and AML = 0. After that, while comparing using a classification report I got an average accuracy of 35%. Then I run the algorithm once again and got an accuracy of 44%. For the third try, I got 33% and so on.

However, I looked about it and came to know that the random_state argument needs to have a fixed value to get same accuracy throughout all runs.

Scenario 2:

After knowing about random_state, this time I started with random state 0 and created the model as model = KMeans(n_clusters=2, random_state=0) and kept the original class labels as before i.e ALL as 1 and AML as 0. However, this time the output didn't change on different runs and I got an accuracy of 53%. But, out of curiosity, I swap the original class label i.e. I set ALL as 0 and AML as 1 which results in 47%.

Scenario 3:

This time I choosed random_state as 1 i.e. model = KMeans(n_cluster=2, random_state=1) and having ALL as 0 and AML as 1 gave 67% accuracy while considering ALL as 1 and AML as 0 gave 33% accuracy.

So, My question is what I am doing wrong here? Am I implementing something wrong? If I am right then why the result is changing so much depending on random_state and class labels? What's the solution and how to choose the best pair of random_state and class labels?",1,scikit_learn,2020-10-13
gwgzy9,estimate_transform works when using 'similar' but not when using 'affine',"I have two 512x512 grayscales images (src and dst). To try to understand estimate transform I applied the following transformation

    tform = transform.AffineTransform(scale=(1.3, 1.1), 
                                        rotation=0.5, 
                                        translation=(0, -200)) 

to the src to create the dst. Then I want to find back the parameters using estimate\_transform.

With the parameter 'similar' I obtain parameters very close to the one I used (as expected). But when I want to use 'affine', I obtain the following error :

     matmul: Input operand 1 has a mismatch in its core dimension 0, 
    with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 513 is different from 3) 

Any idea why ? Here is my code :

    src = rgb2gray(data.astronaut())
    dst = rgb2gray(data.astronaut())
    tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5,
                                      translation=(0, -200))
    dst = transform.warp(img1, tform)
    tform_fin = transform.estimate_transform('affine', src, dst)
    dst_corr = transform.warp(img3, tform.inverse)",1,scikit_learn,2020-10-13
gtw4p9,What can I do when I keep exceeding memory used while using Dask-ML,"I am using Dask-ML to run some code which uses quite a bit of RAM memory during training. The training dataset itself is not large but it's during training which uses a fair bit of RAM memory. I keep getting the following error message, even though I have tried using different values for n_jobs:

```
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
```

What can I do?

Ps: I have also tried using Kaggle Kernel (which allows up to 16GB RAM) and this didn't work. So I am trying Dask-ML now. I am also just connected to the Dask cluster using its default parameter values, with the code below:

```
from dask.distributed import Client
import joblib

client = Client()

with joblib.parallel_backend('dask'):
    # My own codes
```",1,scikit_learn,2020-10-13
gsx926,MLPRegressor newby with some (probably very basic) questions in need of some assitance,"Hello!

I'm building MLPRegressor for the first time ever (I've been learning how to code with online courses since end of March) and I know something is wrong but I don't know what. Bellow you can see my code so far. It runs and I have a value for r2 ( -9035355.06 ) and a plot. However the r2 score doesn't make sense (it should be around 0.7)  and the plot doesn't make sense either.

I have run this analysis with SPSS multilayer perceptron feature so I know more or less how my results should be and that's why I know whatever I am doing with python is wrong.

Any advice/suggestion of what I'm doing wrong is very welcome! This coding world is kinda of frustrating for me:/

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    
    from sklearn import neighbors, datasets, preprocessing 
    from sklearn.model_selection import train_test_split
    from sklearn.neural_network import MLPRegressor
    from sklearn.metrics import r2_score
    
    vhdata = pd.read_csv('vhrawdata.csv')
    vhdata.head()
    
    X = vhdata[['PA NH4', 'PH NH4', 'PA K', 'PH K', 'PA NH4 + PA K', 'PH NH4 + PH K', 'PA IS', 'PH IS']]
    y = vhdata['PMI']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 
    
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_test_norm = scaler.fit_transform(X_test)
    
    nnref = MLPRegressor(hidden_layer_sizes = [4], activation = 'logistic', solver = 'sgd', alpha = 0.1, learning_rate= 'constant',
                         learning_rate_init= 0.6, max_iter=200, random_state=0, momentum= 0.3, nesterovs_momentum= False)
    nnref.fit(X_train_norm, y_train)
    
    y_predictions= nnref.predict(X_test_norm)
    
    print('Accuracy of NN classifier on training set (R2 score): {:.3f}'.format(nnref.score(X_train_norm, y_train)))
    print('Accuracy of NN classifier on test set (R2 score): {:.3f}'.format(nnref.score(X_test_norm, y_test)))
    print('Current loss : {:.2f}'.format(nnref.loss_))
    
    plt.figure()
    plt.scatter(y_test,y_predictions, marker = 'o', color='blue')
    plt.xlabel('PMI expected (hrs)')
    plt.ylabel('PMI predicted (hrs)')
    plt.title('Correlation of PMI predicted by MLP regressor and the actual PMI')
    plt.show()",1,scikit_learn,2020-10-13
gsol3k,What are the default values for the parameters in Dask-ML's Client() function,"I am trying to understand Dask-ML's Client() function parameters. Say I have the following code using Dask-ML's Client() function:

```
from dask.distributed import Client
import joblib

client = Client()
```

If I don't specify any values for the parameters in the Client() function, what are the default values for the parameters:

(i) n_workers

(ii) threads_per_worker

(iii) memory_limit

From my understanding, Python has the Global Interpreter Lock (GIL) feature which prevents multi-threading. If so, why does Dask-ML's Client() function have the parameter threads_per_worker when multi-threading is prevented in Python?

Does memory_limit refers to the maximum memory limit allowed for **each** worker/machine/node or does this refer to the maximum memory limit allowed for **all combined** worker/machine/node?

I have already looked through the documentation in Dask-ML (see here: https://docs.dask.org/en/latest/setup/single-distributed.html), but the documentation is not clear in regards to these questions above.

Thank you in advance if anyone could explain this?",1,scikit_learn,2020-10-13
glb4cy,Why does PolynomialFeatures has multiple pair of coefficient after fitted the data?,"After I create an PolynomialFeatures object, and fit the data by :

[`poly.fit`](https://poly.fit)`(x,)`

I wanted to look for the coefficient, so I do:

`poly.transform(x,y)`

&amp;#x200B;

And it will return an array with (n\_samples, n\_coeff), but why does the polynomial fit with multiple pair of coefficient? Wouldn't the model fit the data and get a final best coefficient?

&amp;#x200B;

And what is the final coefficient that Polynomial get after fitting?",1,scikit_learn,2020-10-13
gi4jw3,How to add sample_weight into a scikit-learn estimator,"I have recently developed a scikit-learn estimator (a classifier) and I am now wanting to add sample_weight to the estimator. The reason is so I could apply boosting (ie. Adaboost) to the estimator (as Adaboost requires sample_weight to be present in the estimator).

I had a look at a few different scikit-learn estimators such as linear regression, logistic regression and SVM, but they all seem to have a different way of adding sample_weight into their estimators and it's not very clear to me:

Linear regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_base.py#L375

Logistic regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_logistic.py#L1459

SVM: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/svm/_base.py#L796

So I am confused now and wanting to know how do I add sample_weight into my estimator? Is there a standard way of doing this in scikit-learn or it just depends on the estimator? Any templates or any examples would really be appreciated. Many thanks in advance.",2,scikit_learn,2020-10-13
get2kh,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,,2,scikit_learn,2020-10-13
gdb7a8,Code a Decision Tree in 20 lines.,,0,scikit_learn,2020-10-13
gd81x4,why does Scikit Learn's Power Transform always transform the data to zero standard deviation?,"all of my input features are positive. Whenever I tried to apply PowerTransformer with box-cox method, the lambdas are s.t. the transformed values have zero variance. i.e. the features become constants

&amp;#x200B;

I even tried with randomly generated log normal data and it still transform the data into zero variance.

&amp;#x200B;

I do understand that mathematically, finding the lambda s.t. the standard deviation is the smallest, would mean the distribution would be the most normal-like.

&amp;#x200B;

But when the standard deviation is zero, then what's the point of using it?

&amp;#x200B;

&amp;#x200B;

p.s. so one of the values of lambda I get by using PowerTranformer is -4.78 

If you apply it into the box-cox equation for lambda != 0.0, then for any input feature y values, you technically get the same values. i.e. (100\^(-4.78)-1.0)/(-4.78) is technically equals to (500\^(-4.78)-1.0)/(-4.78)",2,scikit_learn,2020-10-13
gcvtsm,how to combine recursive feature elimination and grid/random search inside one CV loop?,"I've seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:

[Feature selection and cross-validation](https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751)

[Nested cross-validation and feature selection: when to perform the feature selection?](https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec)

[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)

&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...

[Here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.

However I'd like to do recursive feature elimination inside random/grid CV, so that ""feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)"", so that data from other folds influence neither feature selection nor hyperparameter optimization.

Is this possible natively with sklearn methods and/or pipelines? Basically, I'm trying to find an sklearn native way to do this before I go code it from scratch.",3,scikit_learn,2020-10-13
gc2z28,How to write a scikit-learn estimator in PyTorch,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",3,scikit_learn,2020-10-13
g5ugws,Code a Neural Network in 20 lines.,,2,scikit_learn,2020-10-13
g4yc73,Basic question re: gaussian mixture models,"I wasn't able to find this in the documentation, but is the covariance parameter you access with model.covariances\_ sigma or sigma\^2? Seems like it can be either thing as I've seen the notations N(x| mu, sigma\^2) and N(x|mu, sigma) both used in various places.",1,scikit_learn,2020-10-13
fzm7mm,"Should scikit-learn include an ""Estimated Time to Arrival"" (ETA) feature? Discuss.",,6,scikit_learn,2020-10-13
fx6kdy,Clustering of t-SNE,"Hello,

I have recently tried out t-SNE on the sklearn.datasets.load_digits dataset. Then i applied KNeighborClassifier to it via a GridSearchCV with cv=5.

In the test set (20% of the overall dataset) i get a accuracy of 99%

I dont think i overfitted or smth. t-SNE delivers awesome clusters. Is it common to use them both for classifying? Because the results are really great. I will try to perform it on more data. 

I am just curious on what you (probably much more experienced users than me) think.",1,scikit_learn,2020-10-13
fx0i7x,Search over preprocessing and ensemble hyperparameters?,"In scikit-learn there are some handy tools like `GridSearchCV` for tuning the hyperparameters to a model or pipeline.

Suppose you'd like the preprocessing in your pipeline to include some user-defined options (e.g. whether to encode a certain categorical variable via one-hot encoding or something weird like frequency encoding) and you'd like to include those options among the hyperparameters you're searching over.

Suppose further that you're using an ensemble model -- e.g. a random forest plus few linear regression specifications, and you'd like to tune the hyperparameters for each of them, as well as the voting weight of each.

Does scikit-learn provide a predefined way to search over such spaces? It looks like the parameter space is intended only to dictate the behavior of a single model, not preprocessing steps or ensemble parameters.",1,scikit_learn,2020-10-13
ft2pcp,"How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?","How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?",1,scikit_learn,2020-10-13
ft1kmb,facing an error,"import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

&amp;#x200B;

\# Importing the dataset

dataset = pd.read\_csv('50\_Startups.csv')

X = dataset.iloc\[:, :-1\].values

y = dataset.iloc\[:, 4\].values

X2=dataset.iloc\[:, 3\].values

\# Encoding categorical data

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

le = LabelEncoder()

X2 = le.fit\_transform(X2)

oh = OneHotEncoder(categories = 'X\[:, 3\]')

X= oh.fit\_transform(X).toarray()

&amp;#x200B;

https://preview.redd.it/7bgiwdp238q41.png?width=871&amp;format=png&amp;auto=webp&amp;s=d2386c5cda100706a85803c036586beec8b9e843",1,scikit_learn,2020-10-13
fm1oov,I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?,"I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?

    preprocess =     make_column_transformer((SimpleImputer(strategy='median'), cols_numeric),     
    (SimpleImputer(strategy='constant', fill_value='missing'), cols_onehot),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_target),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_ordinal),     (OneHotEncoder(handle_unknown='ignore'), cols_onehot),     
    (TargetEncoder(), cols_target),     
    (OrdinalEncoder(), cols_ordinal),     
    (StandardScaler(), cols_numeric)) 
    lr_wpipe = make_pipeline(preprocess, LinearRegression()) 
    lr_scores = cross_val_score(lr_wpipe, X_train, y_train) 
    np.mean(lr_scores) 
    print(""Linear Regression R^2: "", lr_scores)",1,scikit_learn,2020-10-13
flg6a1,how to find 'the math' being done in sklearn source code?,"hi.  I'm trying to find where in sklearn the actual math is being done, mostly for my own learning so I can answer questions like 'when using `sklearn.neighbors` , what math is being used to calculate Euclidean distance?'    


If you see here: [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/\_base.py#L360](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/_base.py#L360)  
you'll see that Euclidian and other distance calculations can be *specified*, but I don't see anywhere where the actual math is being done in code.",1,scikit_learn,2020-10-13
fl4fj1,Adding Standard Scaler to GridSearchCV,"I'm looking to use the Standard Scaler as a hyper parameter, i.e check if performance is higher with/without scaling the inputs. In order to tune with other hyperparameters, I would like to incorporate it into my GridSearchCV function (provided by Scikit Learn). Can someone advise me on how to do it?",1,scikit_learn,2020-10-13
ffx9zw,How to use tfidfvectorizer fit_transform for multiple docs,"Hey, 

Let's say my corpus is a list of lists , each of the inner lists represent a parsed doc (each value is a word) 

I want to compute a tf-idf score for my corpus. 

It's seems like the fit-transform function can't use my corpus as its inputs should be itratable with string values (which is each of my docs) 


    V = tfidfvectorizer ()
    For doc in corpus:
       Vectors = v.fit_trabsform(doc)

So my question is, how does it calculate IDF if it get only one doc at a time?",1,scikit_learn,2020-10-13
ff9602,Classifiers' score method clarification,"Hi,

I don't fully understand what the score method of classifiers does. For example, the [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method's documentation says ""Return the mean accuracy on the given test data and labels."" Now, I know what is accuracy: (TP+TN)/(TP+TN+FP+FN), but I don't understand why ""mean"" is in there. Mean over what? of what?  

That is, I give the method as parameters a dataset with true labels, and it can calculate the accuracy from that (given the model), but where does the mean come into place? 

Thanks in advance!",3,scikit_learn,2020-10-13
fbfky4,Is epsilon in dbscan a euclidean measure?,"Hello everyone,
I'm writing yet another dbscan question. For those who are familiar with the inputs to the dbscan, the principal parameters are epsilon and minPts.

Epsilon is the neighborhood radius, and I'm curious if anyone can point me to a reference or tell me if epsilon is a euclidean metric",3,scikit_learn,2020-10-13
f1dizs,Identifying smallest frequently occurring value,"I'm not a data science person, but thinking Scikit learn might be able to help here, and looking for suggestions for ideas I should investigate.

Essentially, I'm looking for a way to consistently identify a baseline power readings. If I have minute by minute power consumption readings from a bunch of electrical motors.  For any motor, we want to identify what a 'baseline' or 'normal unloaded steady-state' power value is.

There is definitely noise in the signal, and not even noise - legitimate power reading that are smaller than what we would consider 'normal unloaded steady state'.  The catch is this could be different for the same motor when production composition changes, so there is not just one value that we can look at historical data to arrive at. (Think motors running pumps moving different fluid mixtures / slurry ad different times.

This does not have to be real-time, just take the dataset of power readings for any motor for any production batch and post-process the data in such a way we can identify times the motor is doing its job at a 'near-idle' state.

Currently we just have a basic calculation that looks at a rolling window of 20 per-minute readings and finds the lowest value that occurs at least twice. (basically throwing out the lowest few outliers)

The reason I'm considering Scikit or similar is we can graph these power readings for a time period (say 1 day) and visually we can easily see these 'baselines' we are looking for.  There will be spikes and dips, and time windows where we are definitely running a heavy load (motors spun up on demand), but we can identify when the mixture changes because the visual changes in this baseline value.

Hope that made at least a little sense, if there are details I can clarify, please ask.  I appreciate everyone's thoughts and ideas!",1,scikit_learn,2020-10-13
eylu4p,What's wrong with Scikit-Learn.,,3,scikit_learn,2020-10-13
ev1as7,Is it possible to use a custom-defined decision tree classifier in Scikit-learn?,"I have a predefined decision tree, which I built from knowledge-based splits, that I want to use to make predictions. I could try to implement a decision tree classifier from scratch, but then I would not be able to use build in Scikit functions like predict. Is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? Or do I need to do something completely different? My first attempt was to use “fake training data” to force the algorithm to build the tree the way I like it, this would end up in a lot of work because I need to create different trees depending on the user input.",1,scikit_learn,2020-10-13
eu9qgc,Is HistGradientBoosting the same as LightGBM or is the SKLearn's version different?,"If so, how?",2,scikit_learn,2020-10-13
em9fxf,Is this the proper way to do ML with scikit_learn?,"I have a dataset with 8 features (numeric) and 1 target (0 or 1).  
 I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc.  
 For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

But sometimes I get these errors:  
 The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

What's the proper way to predict binary values?  
 I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Thanks for your support. I'm a beginner.",2,scikit_learn,2020-10-13
effb98,What module/algorithm should I use in order to predict the time in which a certain action will be completed?,"Basically the title, but to explain it even more :

I have to device a model which will predict the total time a patient will have to wait in a hospital environment. For that, we have a dataset consisting of various patients with several diseases and their time durations already recorded. I want to know which module or algorithm should I use to carry this out? This is my first ML project and I could use any help that you guys can do. Thank you!!",2,scikit_learn,2020-10-13
e0h8ao,Column transformer throwing away some features?,"My data frame  has numerical columns a, b, c. And it has categorial text features d, e, f, g, h.

I build a preprocessor like the following.

    num_features = ['a','b','c']
    nom_features = ['d','e','f','g','h']
    
    preprocesser = ColumnTransformer([
        (""scale_numeric"", StandardScaler, num_features),
        (""encode_nominal"", OneHotEncoder(handle_unknown=""ignore""), nom_features)],
        remainder=""drop""
    )
    
    preprocessor.fit_transform(dataframe)

Since I started with 8 features, after one hot encoding I expected to get 8 or more features back, but the result of `preprocessor.fit_transform(dataframe)` only has 3 columns. Not sure what I am doing wrong if anyone can help me.",1,scikit_learn,2020-10-13
dyzwn9,How to Modify(Make unique) the Scikit-learn Multilayer perseptron algorithm (MLP),"Hi folks,

I've been trying to build a rainfall prediction model for last few days. I've used the **Scikit-learn Multilayer perseptron regressor function** straight up. 

1) The accuracy was OK(78%) but I want to increase it

2) I don't want to use the same predominantly given function (I just want to **add uniqueness in my code**, but I want to use scikit-learn)

Is there any way to modify the function or not use the ready-made function? Can anyone please help me with this?

Thanks in advance!",1,scikit_learn,2020-10-13
dtnh3f,The best alpha for ridge regression is... -85???,,3,scikit_learn,2020-10-13
dtiy98,difference between Kfold.split() and shufflesplit.split() in scikitlearn,"I read this [post](https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv), I get the difference when it comes to computation and shufflesplit randomly sampling the dataset when it creates the testing and training subsets, but in the answer on stackoverflow, there is this paragraph  


""**Difference when doing validation**

In KFold, during each round you will use one fold as the test set and *all* the remaining folds as your training set. However, in ShuffleSplit, during each round **n**  you should *only* use the training and test set from iteration **n** ""

I couldn't quite get it. since in kfold, you're bounded by using the training buckets (k-1) and testing bucket (k) in the **k** iteration and in shufflesplit you use the training and testing subsets made by the shufflesplit object in iteration **n.**  so for me it feels like he's saying the same thing.

can anyone please point out the difference for me?",1,scikit_learn,2020-10-13
dcvd2r,When to use these unsupervised algorithms?,"There are a lot of modules in sklearn. I am interested when these unsupervised algorithmes (bellow )are used.  
When to use a Gaussian mixture model? When to use Manifold Learning, When to Biclustering? etc.

&amp;#x200B;

* [2.1. Gaussian mixture models](https://scikit-learn.org/stable/modules/mixture.html) 
* [2.2. Manifold learning](https://scikit-learn.org/stable/modules/manifold.html)
* [2.4. Biclustering](https://scikit-learn.org/stable/modules/biclustering.html) 
* [2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html) 
* [2.6. Covariance estimation](https://scikit-learn.org/stable/modules/covariance.html) 
* [2.7. Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html) 
* [2.8. Density Estimation](https://scikit-learn.org/stable/modules/density.html)",7,scikit_learn,2020-10-13
dar9z6,pattern recognition on texts that are bash commands or software signature?,"hi all.

so I've got my hands on a daily dose of 100,000 connections per day to our servers, and I've got millions of rows of data that includes commands our users have executed on our servers, (\`cd\`, \`ehlo\`, \`scp ....\`, etc). and I have the same amount of data of their application signatures while connecting. like (Firefox 59, Firefox 60, google chrome),... and user agents, ...

basically all the data one can extract out of a socket or using an IDS.

I like to do some pattern matching on these data. like for the commands they are executing and stuff like that...

so to cluster the commands, I've got commands that look like this:

cd Project

cd Images/personal

cd Project/map

cat /var/log/nginx/web\_ui.log

the problem is, I can just split the texts and take in the first part(cd, cat) and make plot out of the commands, but i really would like to make it more automatic and intelligent. so people who \`cd\` into the \`Project/map\` are distinguished from people who cd into \`Images\` folder. I like to know what people are doing on out servers. so a plot that all people whith \`cd\` commands are close to each other, but are really distinguished for each folder that they have \`cd\` into. 

this is just an example of what I want:)

&amp;#x200B;

turns out that scikit\_learn only works on numbers? how can i utilize it for that kind of data? I don't know if this is a nltk problem?",3,scikit_learn,2020-10-13
d8sxus,Exporting Models to build own inference server,"Hello, I was hoping to get pointed in the right direction.  After training a random forest classifier I am looking to export the model in such a way that I can recreate each of the trees in C++.  I am trying to figure out the best approach to this, or if it is even possible.  My research online mainly shows examples of how to visually represent these, and how to create a pickle project for python serialization.  

Am I missing some key terms in my search? Could you point me to what i should be doing to figure this out?

&amp;#x200B;

My approach so far has been exploring the clf.estimators.trees\_ part of the estimators object, but I am not sure if I am on the right track.

&amp;#x200B;

Any help is much appreciated.

&amp;#x200B;

Thanks!",1,scikit_learn,2020-10-13
d244x4,Predict device from flow,"Hey guys, I applied to a competition about AI and my task is to predict device class from flow. I have 13 types of classes which are all in train set but the test set is missing that one column. After I run training and then I try to predict it, I receive an error stating this: ValueError: query data dimension must match training data dimension.

How can I predict a column that is not there? I don't believe that I have to manually put the column to the test.json 

Thanks for advices.",3,scikit_learn,2020-10-13
cvtjk6,Predicting Churn With Nested Data,"Hello All!

Ok, so this is a bit of a challenge and I'm trying to figure out if it is even worth worrying about the nesting aspect of the data. Basically, I'm trying to predict subscription-level churn with a combination of subscription-level and user-level variables.

Since users own subscriptions I figured I should try to account for nesting in my model. Does anyone have any recommendations on how to attack churn predictions using a nested model? Any suggestions would be greatly appreciated. Again, I have code working, but I've never built anything that requires nested analysis.

Basically my question is: Is it possible to run a multi-level SVM?",2,scikit_learn,2020-10-13
cs2urp,What is the most efficient way to implement two-hot encoding using scikit learn?,"I have two very similar features in my dataframe, and I would like to combine their one-hot encoded versions. They are both categorical data, and they both contain the same categories. I was thinking about using OneHotEncoder from scikit learn and getting the union of the corresponding columns. Is there a function or more efficient way that I do not know about?",3,scikit_learn,2020-10-13
cnnacy,Feature elimination doesn't really eliminate anything.,"I had a fairly simple dataset, after plotting the correlation matrix I noticed that one variable has very low correlation with the target (0.04) but instead of deleting it manually I decided to try feature elimination.
I tried both RFE and RFECV with Logistic Regression as an estimator, RFE eliminated some features which seemed correlated with the output and kept that feature.
RFECV didn't eliminate anything at all.

Am I missing something here?",1,scikit_learn,2020-10-13
cnl2a5,k-means output issue,"Hello I've run a k-means over my voice data. I got two class (for best). My problem is why i got this line at the right side? I sit an issue in my dataset?

https://preview.redd.it/n5lz9pggx7f31.png?width=852&amp;format=png&amp;auto=webp&amp;s=e84cc13f9579c026fc6bfa5cbd85849b1ea2939e",1,scikit_learn,2020-10-13
cmmbi5,Running scikit validation on 24 cores is slow?,"Hello guys, maybe anyone can help me out here. I am running following validation code:

```
from sklearn.linear_model import LinearRegression
model = LinearRegression()
from sklearn.preprocessing import PolynomialFeatures
poly_transformer = PolynomialFeatures(degree=2, include_bias=False)
from sklearn.pipeline import Pipeline
pipeline = Pipeline([('poly', poly_transformer), ('reg', model)])
train_scores, valid_scores = validation_curve(estimator=pipeline, # estimator (pipeline) X=features, # features matrix y=target, # target vector param_name='pca__n_components', param_range=range(1,50), # test these k-values cv=5, # 5-fold cross-validation scoring='neg_mean_absolute_error') # use negative validation
```

in the same .py file on different machines, which I would name #1 localhost, #2 staging, #3 live, #4 live. localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds
live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.
In order to get more ""trustworthy"" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different?",1,scikit_learn,2020-10-13
clz2bl,vectorization,"Hi, I just want to know if I can vectorize a text even if its on another language using Count Vectorization",2,scikit_learn,2020-10-13
clpubv,Machine learning final year project,"design and implement an intelligent agent that can detect a fault and can trouble a faulty server on a network

Its a network anormaly project 
But dont know where to start from",1,scikit_learn,2020-10-13
cl88rf,No Scikit-learn after I installed Anaconda in Sublime Text 3," 

I started using Sublime Text as my Text Editor/IDE (not sure what the difference is) to do some Python projects. After watching 2 episodes of the machine learning course by Google Developers. I installed the Anaconda package which has the Scikit-learn included.

Following the video I typed:

    import sklearn

This error appeared:

    ModuleNotFoundError: No module named 'sklearn'

Is there a way to install the Scikit-learn using the Sublime Text 3 or using a different method?",1,scikit_learn,2020-10-13
cgijm0,Unable to find/import,"edit: Title - Unable to find/import IterativeImputer

&amp;#x200B;

&amp;#x200B;

Hello fellow users, I'm wondering if yall could help me out with importing/finding IterativeImputer...

**&gt;&gt;&gt;** *# explicitly require this experimental feature*

**&gt;&gt;&gt; from** **sklearn.experimental** **import** enable\_iterative\_imputer *# noqa*

**&gt;&gt;&gt;** *# now you can import normally from impute*

**&gt;&gt;&gt; from** **sklearn.impute** **import** IterativeImputer

**ModuleNotFoundError**: No module named 'sklearn.impute.\_iterative'; 'sklearn.impute' is not a package

&amp;#x200B;

$pip freeze states I have scikit-learn==0.21.2 and sklearn==0.0

Python version 3.6

&amp;#x200B;

After researching the issue online I see that there's an experimental version I need to install, but I can't seem to find it! Further, I can't find it on their website.. [https://scikit-learn.org/dev/versions.html](https://scikit-learn.org/dev/versions.html)

What did I overlook/miss?",1,scikit_learn,2020-10-13
cc2mxr,How to re-structure a numpy dataframe into a format I can use in sklearn?,"Assuming the dataframe column 0 is the target and columns 1: are the features, and that each column is named, what's the easiest way to split the data for use in sklearn?",1,scikit_learn,2020-10-13
cbm4g6,How to classify dots,"Hello, 

I have a graph with two groups, red and blue dots. These groups are clearly separated, but the problem is that I want to say if a new dot belongs to the red group, to the blue, or to none of them.

What method do you recommend?

Thank you",1,scikit_learn,2020-10-13
c4rlf7,Regression is not yielding many useful predictions,"Hello all,   


I'm using a linear regression to predict continuous values (how long until a client churns measured in months). I have a dataset of cancelled accounts and active account. I'm using the cancelled accounts to predict the active accounts. I have a variety of explanatory variables and in total I have an R-squared of around 35% (obviously R-squared isn't perfect)   


Overall, this works pretty well; however, one issue I'm running into is that, of the predictions I get back, some are negative and very few actually predict that these active clients should still be active. In other words, many of the predicted cancellation dates are in the past.  


Dumb question, but is there a method I could be using to help this? Overall, I'm getting about 10k useful observations from 60k predictions.  Any suggestions would be greatly appreciated.",1,scikit_learn,2020-10-13
c4q5i6,I can't import Kmeans into compiler,"I'm currently using sklearn 0.21.2, and when I do:  


`import sklearn.cluster.KMeans`

&amp;#x200B;

the compiler returns error:

&amp;#x200B;

`no module named sklearn.cluster.KMeans`

&amp;#x200B;

I've found that in the cluster package, there is an module named 'cluster.k\_means\_'  


But when I tried to use this instead, it shows error

&amp;#x200B;

`Module is not callable`

&amp;#x200B;

Now I don't know why I can't import the kmeans package in cluster.",1,scikit_learn,2020-10-13
bylpjd,Sklearn regression with two datasets,"Hello all,   


basically, as the title implies I'm trying to train a regression model on one dataset and the apply that predictive model to another dataset. In other words, I have a model which predicts cancelled accounts and the amount of time in which those accounts cancel.   


I have another dataset full of active accounts (with the same variables) and I'm attempting to use the model from the cancelled accounts to predict when my active accounts will cancel. I'm having trouble with this.  Is there a way to do this without forcing a t  


Is there a way to use the ""active dataset"" without enforcing a Train\_test\_split? Any help would be greatly appreciated. Thank you!",2,scikit_learn,2020-10-13
bvjxzj,Get the function that fits my data,"I have fit a polynomial regressor to a two dimensional data. 
Is there a way to see the function that fits this data?",2,scikit_learn,2020-10-13
bqtxes,Kmeans clustering cache the result,"Hello,

&amp;#x200B;

I am new to scikit and I was wondering if I could cache the result of Kmeans so next time when I run my script I do not create the centroids again - that means save the result of [`kmeans.fit`](https://kmeans.fit)`()`.",2,scikit_learn,2020-10-13
bp7dv5,Get classes name of each estimator in OneVsOneClassifier,"Are there any ways to do that ? I am trying to directly access the classes\_ attributes in the estimator but it only returning \[0,1\]",2,scikit_learn,2020-10-13
bevq9d,Using Blob Detection methods on huge images,"I'm trying to use common blob detection methods from

[https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob\_dog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog)

on a huge images (about 6000x6000 pixels). It takes way too long to compute and show the result. How could I resolve this?",1,scikit_learn,2020-10-13
bcwbv4,Calculate variance of accuracy,"Hello, how can I calculate the variance of accuracy between two models in Random forest.
I mean I made a simple model with DecisionTreeClassifier() and one more with BagginClassofier() using the first model on it.
The accuracy climb +0.237.

How to get variance of that accuracy?
Thansk",1,scikit_learn,2020-10-13
bcbduy,Classification: Minimizing the amount of false positives,"Hey there,

I posted an earlier post (now deleted) that phrased this a bit wrong (thanks Imericle). Here is another try: 

Many  (most?) classification algorithm seem to be about maximizing accuracy  (true positives + negatives). My aim is to minimize the amount of false positives. How would I achieve this?

Only options I see to achieve this is through parameters tuning, is that the right approach?

(Thinking on applying it to a RandomForest),

Thanks,

Bb",2,scikit_learn,2020-10-13
bbxi9t,KMeans: Extracting the parameters/rules that fill up the clusters,"Hi all,

&amp;#x200B;

I have created a 4-cluster k-means customer segmentation in scikit learn. The idea is that every month, the business gets an overview of the shifts in size of our customers in each cluster. 

My question is how to make these clusters 'durable'. If I rerun my script with updated data, the 'boundaries' of the clusters may slightly shift, but I want to keep the old clusters (even though they fit the data slightly worse). My guess is that there should be a way to extract the paramaters that decides which case goes to their respective cluster, but I haven't found the solution yet. 

I would appreciate any help",1,scikit_learn,2020-10-13
b6nfvs,Question about FeatureUnion,"    pipe = Pipeline([
            ('features', FeatureUnion([
                    ('feature_one', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_one')),
                        ('vec', cvec) # Count vectorizer
                    ])),
                    ('feature_two', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_two')),
                        ('vec', tfidf) # Tf-idf vectorizer
                    ]))
                ])),
            ('clf', OneVsRestClassifier(clf)) #clf is a support vector machine
        ])

I'm using this pipeline for a project I'm working on, and I just want to make sure I understand how FeatureUnion works. I'm building a classifier which takes in two different text features and attempts to make a multi-class classification.

&amp;#x200B;

To give a little more detail, I'm trying to classify news articles into one of several categories (sports, business, etc.) Feature one is a list of tokens taken from the article's url, which often, though not always, explicitly states the name of the topic. Feature two is a list of tokens from the body of the article.

&amp;#x200B;

Does it make sense to separate the two features this way? Does this have a different effect than if I had just merged all of the tokens into a single list and vectorized them? My intention was to allow the two features to effect the model to different degrees, since I figured one would be more predictive in most scenarios (and I am getting pretty great results.)",2,scikit_learn,2020-10-13
b36h5a,Ranforest random behaviour,"If I give random forest parameters as RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019) Should it be creating 10 same decision trees? But it is not. I am asking the random forest to
     1.Sample without replacement (bootstrap=False) and each tree have same number of sample (ie the total data )(verified using plot)
     2.Select all features in all trees.
But model.estimators_[2] and model.estimators_[5] are different

",2,scikit_learn,2020-10-13
axgj2c,Predicting the runtime of scikit-learn algorithms,"Hey guys,

We're two friend who met in college and learned Python together, we co-created a package which can provide an estimate for the training time of scikit-learn algorithms.

The main function in this package is called “time”. Given a matrix vector X, the estimated vector Y along with the Scikit Learn model of your choice, time will output both the estimated time and its confidence interval. 

Let’s say you wanted to train a kmeans clustering for example, given an input matrix X. Here’s how you would compute the runtime estimate:

    From sklearn.clusters import KMeans
    from scitime import Estimator 
    kmeans = KMeans()
    estimator = Estimator(verbose=3) 
    # Run the estimation
    estimation, lower_bound, upper_bound = estimator.time(kmeans, X)

We are able to predict the runtime to fit by using our own estimator, we call it meta algorithm (meta\_algo), whose weights are stored in a dedicated pickle file in the package metadata.

The meta algos estimate the time to fit using a set of ‘meta’ features, including the parameters of the algo itself (in this case kmeans) and also external parameters such as cpu, memory or number of rows/columns. 

We built these meta algos by generating the data ourselves using a combination of computers and VM hardwares to simulate what the training time would be on the different systems, circling through different values of the parameters of the algo and dataset sizes . 

Check it out! https://github.com/nathan-toubiana/scitime

Any feedback is greatly appreciated.",5,scikit_learn,2020-10-13
aahf76,"Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?","Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?

&amp;#x200B;

So in the sense that if one applies e.g. a gain factor  (or a function depicting gain changes), then it's applied to the correct direction.",3,scikit_learn,2020-10-13
a75oid,"classification_report + MLPClassifier(): UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn_for)","classification\_report on a prediction done on MLPClassifier() sometimes throws:

&amp;#x200B;

*UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn\_for)*

&amp;#x200B;

but not on all the time.

&amp;#x200B;

What could be wrong?

&amp;#x200B;

\---

&amp;#x200B;

Doing

&amp;#x200B;

set(y\_test) - set(y\_pred) 

&amp;#x200B;

I'm able to see that sometimes some label is missing from y\_pred. But why does this occur only occasionally?

Is something wrong with how I use MLP?",1,scikit_learn,2020-10-13
a753f2,"{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification_report()","{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification\_report()

&amp;#x200B;

Why?

&amp;#x200B;

I call it like:

&amp;#x200B;

    classification_report(y_test, y_pred)

where y\_pred is predicted using a model I built.

&amp;#x200B;

""Quite obviously"" the arguments are incompatible somehow, but how can I find out, how? And how can I make them compatible?

&amp;#x200B;

\---

&amp;#x200B;

I tried:

&amp;#x200B;

    from sklearn.utils.multiclass import type_of_target

&amp;#x200B;

    &gt;&gt;&gt; type_of_target(y_test)
    'multiclass'
    
    &gt;&gt;&gt; type_of_target(y_pred)
    'continuous'

&amp;#x200B;",1,scikit_learn,2020-10-13
a746h0,"Is there any way to ""estimate"" how long a given computation in sklearn will take?","Is there any way to ""estimate"" how long a given computation in sklearn will take?

&amp;#x200B;

So that one doesn't need to wait longer than what one can?

&amp;#x200B;

Also, since Windows Task Manager shows only modest CPU use (&lt; 10%), then how is one supposed to know, what's occurring in the model?",2,scikit_learn,2020-10-13
a73oda,What are the most important parameters in LogisticRegression()?,What are the most important parameters in LogisticRegression()?,3,scikit_learn,2020-10-13
a71xf2,How does one feed hidden_layer_size tuples into GridSearchCV's param_grid?,How does one feed hidden\_layer\_size tuples into GridSearchCV's param\_grid?,1,scikit_learn,2020-10-13
a0yprf,Code review,"Hello,

I'm new to ML and scikit - hope this is the correct place for this. Have created the below code that appears to be working but wanted to get the opinions of people with more experience then me, to check I haven't a made any major errors or if there are any obvious improvements?

&amp;#x200B;

I am trying to train a model on a data set of potentially hundred of thousands emails. Every few days I want to retrain the exported model using incremental learning on the new emails received since the model was last trained.

The below reads the initial data from a csv, runs HashingVectorizer then SGDClassifier. The OnlinePipeline is used to allow me to use partial\_fit when I try to retrain later in the process.

`import pandas as pd`

`data = pd.read_csv('customData1.csv')`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`from sklearn.feature_extraction.text import HashingVectorizer`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

&amp;#x200B;

`from sklearn.linear_model import SGDClassifier`

`text_clf = OnlinePipeline([('vect', HashingVectorizer()),`

`('clf-svm', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=5, random_state=None)),`

`])`

`text_clf = text_clf.fit(X_train,Y_train)`

`predicted = text_clf.predict(X_test)`

`np.mean(predicted == Y_test)`

The above gives me an accuracy of 0.55

&amp;#x200B;

A few days later when I have new emails I import the previously exported model and use partial\_fit on a new csv file.

`import pandas as pd`

`data = pd.read_csv('customData2.csv') #text in column 1, classifier in column 2.`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

&amp;#x200B;

`from sklearn.externals import joblib`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

`text_clf2 = joblib.load('text_clf.joblib')`

&amp;#x200B;

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`text_clf2 = text_clf2.partial_fit(X_train,Y_train)`

&amp;#x200B;

`predicted = text_clf2.predict(X_test)`

`np.mean(predicted == Y_test)`

This returns the improved accuracy of: 0.84

&amp;#x200B;

Sorry for so much code!  I obviously need to tidy it all up so its a single method and handle the import/export logic properly.

&amp;#x200B;

Have a made any major errors or are there any obvious improvements? Thanks!

&amp;#x200B;",1,scikit_learn,2020-10-13
a0iz4i,Does cross_val_score tell something about generalizability?,"Does cross\_val\_score tell something about generalizability?

&amp;#x200B;

Or do I need to use something else for measuring generalizability?",0,scikit_learn,2020-10-13
a0c8dw,"Is there a problem if MLPRegressor doesn't converge for max_iter=100, but nor max_iter=5000 either?","Is there a problem if MLPRegressor doesn't converge for max\_iter=100, but nor max\_iter=5000 either?

&amp;#x200B;

Anything else I could try?",1,scikit_learn,2020-10-13
a0b260,What do cv (number of folds) and the number of outputs in cross_val_score correspond to?,"What do cv (number of folds) and the number of outputs in cross\_val\_score correspond to?

&amp;#x200B;

Does it mean that it produces cv number of different scores? Or (as I read somewhere) that only the last score might be the meaningful one (I read something like the others than the last used to ""fit"", while the last is the score)?",2,scikit_learn,2020-10-13
a0awuh,"Getting values in range [-191806. ..., 0.77642 ...] from cross_val_score, am I doing something wrong?","Getting values in range \[-191806. ..., 0.77642 ...\] from cross\_val\_score, am I doing something wrong?

&amp;#x200B;

    mlp = MLPRegressor(hidden_layer_sizes=(7,))

mlp.fit(X\_train,y\_train) mlp\_y\_pred = mlp.predict(X\_test)

&amp;#x200B;

y\_pred is an earlier prediction using LinearRegression().

&amp;#x200B;

I call cross\_val\_score like:

&amp;#x200B;

    cross_val_score(mlp, y_pred, mlp_y_pred, cv=10)

&amp;#x200B;

Output is:

&amp;#x200B;

    00 = {float64} -4.4409160725075605
    01 = {float64} -673636.0674512024
    02 = {float64} -51282.162171235206
    03 = {float64} -399557.4789466267
    04 = {float64} -35.73093353875776
    05 = {float64} -1406.9741325253574
    06 = {float64} -80853.84044929259
    07 = {float64} -5132.870883709122
    08 = {float64} -283.7432365432288
    09 = {float64} -2.860321933844385

&amp;#x200B;

I think I should be getting values in range \[0,1\].",1,scikit_learn,2020-10-13
a0at1q,"Is MLPRegressor's hidden_layer_sizes=(7,) equivalent to hidden_layer_sizes=7?","Is MLPRegressor's hidden\_layer\_sizes=(7,) equivalent to hidden\_layer\_sizes=7?",1,scikit_learn,2020-10-13
a09ukl,"Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train_test_split(Xy,shuffle = False, test_size = 0.33)?","Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train\_test\_split(Xy,shuffle = False, test\_size = 0.33)?

Xy has been constructed like:

&amp;#x200B;

    X = dat.data
    y = dat.target 
    Xy = np.hstack((X,np.array([y]).T))

It seems that it returns only two arrays, even when I saw an example ([https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input](https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input)) do 

 

    X_train, X_test, y_train, y_test = train_test_split(Xy,shuffle = False, test_size = 0.33) ",1,scikit_learn,2020-10-13
a06iin,Runtime Error in RandomizedSearchCV,"I've been running a RandomForestClassifier on a dataset I took from UCI repository, which was taken from a research paper. My accuracy is \~70% compared to the paper's 99% (they used Random Forrest with WEKA), so I want to hypertune parameters in my scikit learn RF to get the same result (I already optimized feature dimensions and scaled). I use the following code to attempt this (random\_grid is simply some hard coded values for various parameters):

&amp;#x200B;

    rf = RandomForestClassifier()
    # Random search of parameters, using 2 fold cross validation,
    # search across 100 different combinations, and use all available cores
    rf_random = RandomizedSearchCV(estimator = rf,  param_distributions = random_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)
    # Fit the random search model
    rf_random.fit(x_train, x_test)

When I attempt to run this code though my python runs indefinitely (for at least 40 min before I killed it) without giving any results. I've tried reducing the \`cv\` and \`n\_iter\` as much as possible but this still doesn't help. I've looked everywhere to see if there's a mistake in my code but can't find anything. I'm running Python 3.6 on Spyder 3.1.2, on a crappy laptop with 8Gb RAM and i5 processor :P

&amp;#x200B;

Here is the random\_grid if it helps:

    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
    max_features = ['auto', 'sqrt']
    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
    max_depth.append(None)
    min_samples_split = [2, 5, 10]
    min_samples_leaf = [1, 2, 4]
    bootstrap = [True, False]
    
    # Create the random grid
    random_grid = {'n_estimators': n_estimators,
                   'max_features': max_features,
                   'max_depth': max_depth,
                   'min_samples_split': min_samples_split,
                   'min_samples_leaf': min_samples_leaf,
                   'bootstrap': bootstrap}

&amp;#x200B;",1,scikit_learn,2020-10-13
9yhcqq,Does sklearn have built-in routines for testing results of LinearRegression()?,Does sklearn have built-in routines for testing results of LinearRegression()?,1,scikit_learn,2020-10-13
9ygvsi,How does fit_transform allow for other data to be processed with the same transformer?,"How does fit\_transform allow for other data to be processed with the same transformer?

&amp;#x200B;

Like here:

&amp;#x200B;

[https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)

&amp;#x200B;

Particularly, since one first calls fit\_transform, then why does it allow one to call transform afterwards and still get the same fit? Like how is this kind of functionality implemented?

&amp;#x200B;

&amp;#x200B;",1,scikit_learn,2020-10-13
9tf8yw,Principal component Analysis: predicting values,"I am attempting to forecast a set of multivariate time series data. I have run a PCA (using the scikit-learn module) and have run an AR(1) auto-regression of the 3 components.

Now that I have the projects component values, how do I recast those components into the original variables, in order to find the projection for those variables?",2,scikit_learn,2020-10-13
9t3xwo,Extract a single stratified part of a dataset,"I have a multi-label dataset with N samples, and I want to take a chunk out to reserve for validation, e.g. reserve k% of the dataset.

Note that I want to do this just once, else I could use stratifiedKFold.  
Is there a function to produce such a single chunk, ensuring stratification with respect to  the labels?  
(A workaround would be to produce N\*k  KFold splits, concatenate  all parts but one  for training, and use the last for validation.)

Thanks.",1,scikit_learn,2020-10-13
9sbqvm,Stepping through each iteration of the LogisticRegression fit() function,"Hello guys,

I'm using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class to find the decision functions between my classes. I wanted to ask you - *how can I step through each step of the algorithm?* I know I can give the parameter `max_iter` to determine the number of iterations, but I want to step through each of those `max_iter` iterations - to see how the values of the coefficients change.

Thanks in advance!",1,scikit_learn,2020-10-13
9pcack,"modAL: A modular active learning framework for Python, built on top of scikit-learn",,6,scikit_learn,2020-10-13
8ndofu,Parse Twitter feed and suggest domain names • r/nltk,"I'm working on a hackathon, and I'd like to parse a user's last 100 tweets or so and make recommendations for a domain name using a new TLD.

The plan I've got in my head is

1\) Scrape twitter for a bit and get some data \(How much? How many records?\)

2\) Run tf\-idf against it, save that dataset

3\) split the initial twitter data into groups based on which tweets contain each TLD \- supplies, computer, kitchen, etc.

a\) Run some kind of clustering algorithm against each set? 250 or so TLDs

\-\- This is where I have questions

4\) Scrape their twitter feed and get 100 tweets

5\) Use the tf\-idf data from step 2 to spit out keywords

6\) use those keywords using some kind of distance formula against the clustered data to pick a tld?

7\) use the bigrams or keywords to make up an SLD.

This seemed off to a good start, but can I somehow pickle the cluster results? Or have multiple sets of cluster results in the same object?

Note: 95&amp;#37; of my knowledge on this topic comes from this blog post: [http://brandonrose.org/clustering](http://brandonrose.org/clustering)",2,scikit_learn,2020-10-13
8l4imo,"move partial of decision models from server to client - side, is it good idea?","Hi,
Some time ago tenser flow for js was released. I'm wondering about build bridge for some scikit learn models to move some part of learning and prediction to the client side. I think that it could help minor companies reduce server resource usage and make models and prediction much more personalised. Do you think it's a good idea? Do you know whether someone has tried something similar before?",1,scikit_learn,2020-10-13
8emc3b,How to combine num values with text data for classification?,I build website classifier and use text of each webpage (transformed to bag of words) as train data. But I also want to add each website's PageRank as feature. How can I do that?,2,scikit_learn,2020-10-13
89u8tj,PLSRegression Issues,"I'm working with scikit's cross\_decomposition.PLSRegression(). According to their documentation, x = np.multiply( x\_scores\_, x\_loadings\_.T ). I'm not getting anything close to the same value values. I've tried every combination of using scale=False and sklearn.preprocessing.scale(x) to try and find how this works out, but I haven't been able to find one that works.

    plsr = PLSRegression(n_components=x_df.shape[0]-1).fit(x_df.T,y)
    print(np.matmul(plsr.x_scores_,plsr.x_loadings_.T).T)
    print(x_df)

Using scaled data (i.e. PLSRegression.fit( scale(x\_df).T, scale(y) ) and changing n\_components doesn't help either. If anyone has any idea of what mistake I have made, or if this just a bug in sklearn?",1,scikit_learn,2020-10-13
81xksg,How to remove terms from a term-document matrix?,"Hello,

I have a term document matrix that I've created using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) like so:
    
    X = vectorizer.fit_transform(corpus)
    X
    &lt;1000x10022 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    	with 94340 stored elements in Compressed Sparse Row format&gt;

I'd now like to remove any terms that do not appear in at least 3 documents, and then calculate the TF-IDF scores for each term, and select the vocabulary as the top n terms ordered by TF-IDF scores.

Is there an easy way of removing terms from the term document matrix that do not appear in at least 3 documents, while still conserving the mapping from feature names to feature indices?

I guess one way to do it would be to get the feature names of the terms that appear in at least 3 documents using numpy on the sparse matrix directly, assign them a mapping to indices, and then pass that mapping to the vocabulary parameter in the CountVectorizer constructor.

Any ideas on how to do this more easily?",1,scikit_learn,2020-10-13
7zfjba,How to use partial_fit to update the model trained with fit() instead of training from scratch,"I tried partial_fit with various scikit online learning classifiers like perceptron, passive aggresive classifiers, SGDclassifer... like here: https://ideone.com/uOtRTZ.  I just dont understand why i cant train the new data on top of already trained data. I am doing image classification. I have trained my 10,000 images with fit(). Now i got 1 new image to add to this dataset of already trained images. I want to update the trained model instead of training all 10,001. Is this possible with partial_fit() ? If so, please tell me how ? ",1,scikit_learn,2020-10-13
7z4qi7,SGDClassifier.partial_fit returns error of “classes should include labels”,"I tried to predict label of my newly added data through SGDClassifer.partial_fit as below:

        from sklearn import neighbors, linear_model
    import numpy as np
    
    
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""hinge"")#loss
    
        sgd_clf.fit(X, y)
    
        print(sgd_clf.predict([[6, 9]]))
    
        X.append([6, 9])
        y.append(5)
    
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here

However, this results in error: ValueError: `classes=array([1, 2, 3, 4, 5])` is not the same as on last call to partial_fit, was: array([1, 2, 3, 4])

Any ideas or references ? ",1,scikit_learn,2020-10-13
7vw4ap,Retrain a KNN classified model (scikit),"I trianed my knn classifer over multiple images and saved the model. I am getting some new images to train. I dont want to retrain the already existing model.

How to add the newly tranied model to the existing saved model ?

Could someone guide if this is possible or any articles describing the same ?

Thank you,",2,scikit_learn,2020-10-13
70m5l1,How do I add matplotlib to a django webapp and display the code's output on the webpage?,Trying to make a User Interface for a Support Vector Machine from the SVM function in the matplotlib,1,scikit_learn,2020-10-13
6lec9n,"K-NN and custom metrics, speed up sklearn using Cython",,2,scikit_learn,2020-10-13
6k5uvu,Build my first CART based algorithm feedback is welcome!,hey guys! i just made this: https://github.com/lucas-aragno/pokemon-classifier im pretty new to scikit so I'll appreciate any kind of feedback :),2,scikit_learn,2020-10-13
6ev2y7,FastICA,"It seems like all of the examples using fastICA involves taking 2 frequencies, mixing them a certain way, then unmixing them.

What about if I have a wav file. How can I use fastICA to break it down into multiple parts?

Any help would be appreciated. Thank you!",1,scikit_learn,2020-10-13
6eu5cr,Automate your Machine Learning in Python – TPOT and Genetic Algorithms,,2,scikit_learn,2020-10-13
69huhq,[P] Tracking and reproducibility in data projects (CLI tool),,1,scikit_learn,2020-10-13
5xk0iy,Scikit learn vs Open Cv for small problems in image processing,I am a Image processing noob. I've used Numpy and Scipy for some matrix related stuff before and OpenCV for some image processing problems. I recently learned that scipy lets me manipulate images too. What are the pros and cons of using OpenCv and Scipy I am not able to figure out which would be better for me. Appreciate your help!,6,scikit_learn,2020-10-13
5rb5ww,Using Category Encoders library in Scikit-learn,,1,scikit_learn,2020-10-13
5m0352,MLPClassifier: Multiple output activation,"I'm using MLPClassifier but some of the outputs have more than one activation, i.e. [0 1 1 0].
How can I get only one activation?

My code is:
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                    hidden_layer_sizes=(15,), random_state=1, activation='relu')

Thank you!",1,scikit_learn,2020-10-13
5fasve,Need help on scikit kfold validation,"Objective: To create 5 folds of training and test dataset using StratifiedKFold method. I have referred the documentation at http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.cross_validation.StratifiedKFold.html

I am able to print the indices alright but am unable to generate the actual folds. Here follows my code

from sklearn.cross_validation import StratifiedKFold
import pandas as pd
df=pd.read_csv('C:\Comb_features_to_be_used.txt')

##Getting only numeric columns
p_input=df._get_numeric_data()
## Considering all the features except labels
p_input_features = p_input.drop('labels',axis=1)
## Considering only labels [single column]
p_input_label = p_input['labels']
skf = StratifiedKFold(p_input_label, n_folds=5, shuffle=True)
i={1,2,3,4,5}
for i,(train_index, test_index) in enumerate(skf):
    ##print(""TRAIN:"", train_index, ""TEST:"", test_index)
    p_input_features_train = p_input_features[train_index] 
    p_input_features_test =  p_input_features[test_index]
        
I am getting the error: IndexError: indices are out-of-bounds

",2,scikit_learn,2020-10-13
54exlw,scikit-learn doc translation,"translate sklearn doc to chinese
feel free to join us
https://github.com/lzjqsdd/scikit-learn-doc-cn",2,scikit_learn,2020-10-13
52b7od,Improving the Interpretation of Topic Models,,1,scikit_learn,2020-10-13
50qfgg,Topic Modeling with Scikit Learn,,3,scikit_learn,2020-10-13
4rjkcq,Overfit Random Forest,"I have data where Random Forest models overfit to noise whatever 
hyperparameter I put.
(= excellent accuracy on training, but poor accuracy on prediction).


So, this is the process I did to over-come:
    1) Tweak the input data and reduce the sampling of noise (negative example)

    2) Fit the RF and test (confusion matrix) on cross-validation data. 

    3) Repeat it and choose the best cross validation data.

Is there a way to overcome this monte carlo approach,
using OOBag process during training ?

Also incorporate Cross validation to reduce the over-fitting ?

Importance features change every time a new RF is fit (it seems a lot of co-linearity and noise into the data).










",1,scikit_learn,2020-10-13
4h6ypj,Building scikit-learn transformers,,3,scikit_learn,2020-10-13
3zvwqw,Hello everyone! I want to write an oversampling module in compliance with scikit-learn. Advice needed!,"As mentioned in the title I want to write a module for oversampling classes in skewed datasets. I recently came to need such a module and I noticed that no such thing exists officialy in scikit-learn. I want it to be compatible with scikit-learn as I very often use it. Do you have any resources to redirect me to, apart from the official scikit-learn developer guidelines? Any tips for writing a python module in general?

Thanks in advance!",2,scikit_learn,2020-10-13
2f830i,Official Scikit-Learn page.,,1,scikit_learn,2020-10-13
j9q6bp,A scikit-learn compatible library to construct and benchmark rule-based systems that are designed by humans,,6,scikit_learn,2020-10-13
j93854,Best performance on Scikit-learn’s load_digits dataset,"On Scikit-learn’s load_digits dataset:
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html

Does anyone know what is the best performance achieved so far on this dataset? 

I tried googling around but can’t find examples with 100% score performance. I am thinking since this is a standard dataset, it would be easy to get a 100% score performance?",1,scikit_learn,2020-10-13
j8vepi,"could someone ELi5 the hyperparameters (penalty, C, tol, max_inter)",I am currently working on a beginner project on logistic regression using scikit\_learn. I am trying to fine tune my regression model but cant seem to find any websites that can explain what the parameters mentioned in the title mean exactly and how to use them. I was wondering if anyone could give me a quick explanation on what/how to use these parameters to fine tune my regression model.,0,scikit_learn,2020-10-13
j7jz85,SVC rbf kernel seems to be nonstandard?,"I am currently testing a precomputed version of rbf I implemented to get a better feel for how it works and possibly later check out some other kernels.

It seems that whatever I do, I get different results using my precomputed gram matrix vs using the scikit rbf kernel:

To calculate a kernel entry for datapoints xm &amp; xn, based on some extra parameters theta

`k = thetas[0] * np.exp(-(thetas[1]/2.) * (np.sqrt((xn-xm).T @ (xn-xm)))) + thetas[2] + thetas[3] * (xn.T @ xm)`

using theta = \[1,2,0,0\]

This should recover the formulation given [here](https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel) (setting gamma=1)

1 \* exp( - 2/2|xn-xm|^(2) )   


is there something I'm missing? \[here's the code if you wanna take a look\]([https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit\_test.py](https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit_test.py)) (only dependencies are matplotlib, scikit &amp; numpy, so you're probably good if you're on this sub)",1,scikit_learn,2020-10-13
j5pcm6,ImportError,"I've got a long error which ends with:  
ImportError: DLL load failed while importing \_arpack: Não foi possível encontrar o procedimento especificado. (rough translation: Unable to find the specified procedure)  


Any idea on the issue? It seems like I have some sort of update issue, but I'm unable to find what.",2,scikit_learn,2020-10-13
j2v11w,"Scikit-learn. In the case of a single point, k-nearest neighbours predictions doesn’t literally match with the literally nearest point. I think I know why. Correct me if I’m wrong.","Hello.  I’ve looked at the source code. 

Case population sizes in the range 10 ^ 2 to 10 ^ 5 ish. Vanilla, straight out the box knn from scikit-learn.  Except 1 nearest neighbours not the default 5.  

When I try to predict the nearest neighbour of a point, using 1 nearest neighbours. after using knn.fit to make a model, it doesn’t _always_ return the same value of the actual nearest neighbour.  I’ve worked out the actual real nearest neighbour myself to check, using trig, and unit tested it.  

I think that’s because for pragmatic reasons knn is just a probabilistic model applied at group level.  Not exactly the actual knn for each and every point.  

Am I right?

EDIT:  My. Trig. Was.  Wrong.  Due. To.  A. Data frame. Handling.  Issue.  Ggaaahhhh.",7,scikit_learn,2020-10-13
j1idtx,RadomizedSearch CV taking forever,"Hi ,

I have the below snippet.

Trying to run on GCP . its getting stuck and not even updating.

&amp;#x200B;

https://preview.redd.it/bp2zfi71sxp51.png?width=1463&amp;format=png&amp;auto=webp&amp;s=6d97d1ff6083f6eb65e62d983770c69f06d45f4c",2,scikit_learn,2020-10-13
iv8jv9,Neuraxle - a Sklearn-Based Clean Machine Learning Framework,,1,scikit_learn,2020-10-13
it82un,How the 'init' parameter of GradientBoostingRegressor works?,"i'm trying to create an ensemble of an determined regressor, with this in mind i've searched for some way to use the sklearn already existing ensemble methods, and try to change the base estimator of the ensemble. the bagging documentation is clear because it says that you can change the base estimator by passing your regressor as parameter to ""base_estimator"", but with GradientBoosting you can pass a regressor in the ""init"" parameter. my question is: passing my regressor in the init parameter of the GradientBoosting, will make it use the regressor i've specified as base estimator instead of trees? the documentation says that the init value must be ""An estimator object that is used to compute the initial predictions"", so i dont know if the estimator i'll pass in init will be the one used in fact as the weak learner to be enhanced by the bosting method, or it will just be used at the beginning and after that all the work is done by decision trees. If someone can help me with this question i would be grateful.",4,scikit_learn,2020-10-13
igtkry,Best way to get T-Stastic and P-value etc?,"I'm using scikit learn for linear regression.  Is there a way to use that library to generate things like T-Stastic and p-value and standard error etc?

On stack overflow i found this, but wondering if there's a way within scikit

    import statsmodels.api as sm
    from scipy import stats
    X2 = sm.add_constant(X)
    est = sm.OLS(y, X2)
    est2 = est.fit()
    print(est2.summary())

&amp;#x200B;",1,scikit_learn,2020-10-13
i4ulg2,"Data Visualization using ""Python"" with ""Seaborn"" | Part- I",https://youtu.be/X400eIcV-So,3,scikit_learn,2020-10-13
i3546z,Recommendation based on other user following,"Hello,

I try to build a recommendation system.

My service allow users to follow people (not rate them, just follow) and I would like to be able to propose to users to follow people based on other user’s database activity.

Is scikit a good path for this ? 

Do you recommend specific method or useful ressource to read to achieve this ?

For your help guys!",2,scikit_learn,2020-10-13
hzw282,How to use TensorFlow Object detection API to detect objects in live feed of webcam in real-time,,1,scikit_learn,2020-10-13
hwmcf1,sklearn CCA - how to get variance explained for first canonical relationship?,"Hi. I'm exploring multivariate brain-behaviour relationships with sklearn's canonical correlation analysis tool ([https://scikit-learn.org/stable/modules/generated/sklearn.cross\_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca)). I am interested mostly in the first canonical relationship between the two datasets. The decomposition is working fine and i have the weights/canonical scores etcetera - but what i'd really like to know is how much of the variance in either dataset is explained by that one relationship (analogous to eg variance explained by first principal component).

There is a method named 'score' that i can call on the CCA object but I am not quite sure this is what I need. This score is not the same as 'canonical scores above but will supposedly get some coefficient of determination r\^2 between 'observed' and 'predicted' - not sure how to understand this. The description on the webpage is quite terse and it does not behave the way i might expect.

I'm hoping to find someone who might know whether that 'score' method  will get me to what i want - and if so, maybe how to use it. Or point me otherwise in the right direction to get into the variance explained for CCA.

Cheers!",2,scikit_learn,2020-10-13
hu6y83,KMeans Algorithm Question,"Hey all.

I am new with using scikit-learn and had a question regarding the KMeans algorithm functions. After running the algorithm and plotting the clusters, are the clusters with the centroids plotted the final clusters after training is done or is there training that I have to do on the clusters? 

Thanks everyone",1,scikit_learn,2020-10-13
htugnl,"How to handle ""Missing Value"" from ""Dataset"" using ""Pandas"" &amp; ""Sci-Kit Learn""??",https://youtu.be/8IORSsZIyIQ,0,scikit_learn,2020-10-13
htmc59,"How to handle ""Text"" and ""Categorical Attributes"" using Python and Pandas??",https://youtu.be/4sO7Pezlegk,0,scikit_learn,2020-10-13
ht4ol1,Making ROC curves with results from cross_validate?,"I am running 5 fold cross validation with a random forest as such:

from sklearn.ensemble import RandomForestClassifier

from sklearn.model\_selection import cross\_validate

forest = RandomForestClassifier(n\_estimators=100, max\_depth=8, max\_features=6)

cv\_results = cross\_validate(forest, X, y, cv=5, scoring=scoring)

However, I want to plot the ROC curves for the 5 outputs on one graph. The documentation only provides an example to plot the roc curve with cross validation when specifically using StratifiedKFold cross validation (see documentation here: [https://scikit-learn.org/stable/auto\_examples/model\_selection/plot\_roc\_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py))

I tried tweeking the code to make it work for cross\_validate but to no avail.

How do I make a ROC curve with the 5 results from the cross\_validate output being plotted on a single graph?

Thanks in advance",2,scikit_learn,2020-10-13
hm6td7,Best performance on MNIST - Fashion dataset,Does anyone know what is the best performance achieved so far for the MNIST - Fashion dataset along with what model that was used?,1,scikit_learn,2020-10-13
hm22vz,"How to ""Predict"" my friends weight using ""Machine Learning"" and ""Sci-Kit Learn""??",https://youtu.be/A4JwnkTFEXI,2,scikit_learn,2020-10-13
hl4k0u,Factor analysis “model” in CS229,"In one of Stanford’s CS229 lecture by Andrew Ng (https://m.youtube.com/watch?v=tw6cmL5STuY), he talks about a factor analysis “model” in which is to deal with situations where you have a lot more features than samples in your dataset. He even said he used a modified version of this factor analysis “model” in some recent work he did for a manufacturing company in the lecture.

Now my understanding of factor analysis is just a dimension reduction technique. So how did Andrew used factor analysis to build a “model” which deals with datasets which has a lot more features than samples?",2,scikit_learn,2020-10-13
hkm9qn,StackingRegressor Inconsistent Output,"Is it intentional that StackingRegressor returns different accuracy outputs when running multiple times given the same parameters, models and using numpy set seed?",1,scikit_learn,2020-10-13
hjctba,"This lecture that talks about what Multilabel and Multioutput classifications are, along with their implementation using scikit learn.",,1,scikit_learn,2020-10-13
hg5j3s,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,2,scikit_learn,2020-10-13
hf60u0,"Hey guys, here is a lecture on how to implement gradient descent with scikit-learn. Enjoy :)",,1,scikit_learn,2020-10-13
hakk07,How do I create a linear regression for this groupedby dataframe?,"I have this assignment for a job interview and I really want to impress by using some machine learning. I don't know too much about it and I essentially don't have much time to learn that much about it. I have the following [dataframe](https://postimg.cc/0zNbyrV8) and I want to create a linear regression using scikitlearn of \['profit'\] vs \['dateReceived'\] for each \['Language'\]. 

Does anyone know what I can do for that to work? I guess it should be just a few lines of code, but I could be wrong?",0,scikit_learn,2020-10-13
h7ay1o,"Visualize Scikit-learn models – ROC, PR curves, confusion matrices etc",,8,scikit_learn,2020-10-13
h172dr,Scikit Learn Tutorial in One Hour,,4,scikit_learn,2020-10-13
h0w3xx,Books about classification algorithms,"Hi all,

I am completely new to data mining and have to write a seminar paper about classification and do some programming in python.

With the help of datacamp I was able to implement the classification algorithms in python.

Now I am looking for some sources that I can cite in my paper that briefly explain these algorithms.

My problem is that most books that I have look into so far are very mathematical and since I don’t have a data mining/computer science background, they are hard to understand.

Do you have any recommendations for some text books that explain classification algorithms such as SVM, Naive Bayes, Trees, etc. that are well recognized, but explain them in an easy way?

Many thanks in advance!",1,scikit_learn,2020-10-13
gziaus,How to choose best pair of random state and class label values?,"For the last few days, I was trying to implement the KMeans algorithm using SciKit Learn, But I came across a very confusing problem. I have a dataset that has two class labels ['ALL', 'AML'] where ALL has 47 and AML has 25 samples and 100 attributes to train from and now I want to use this dataset for KMeans clustering so that I can compare the predicted results with the original class labels. Before asking my question let me explain certain scenarios. In all the scenarios I have taken all the 100 attributes to fit the model.

Scenario 1:

In the first run, I started with a model that is created with pretty much default arguments i.e. model = KMeans(n_clusters=2). For comparing the predicted class labels(which are numeric) with the original labels(which are strings), I set the original class labels as ALL = 1 and AML = 0. After that, while comparing using a classification report I got an average accuracy of 35%. Then I run the algorithm once again and got an accuracy of 44%. For the third try, I got 33% and so on.

However, I looked about it and came to know that the random_state argument needs to have a fixed value to get same accuracy throughout all runs.

Scenario 2:

After knowing about random_state, this time I started with random state 0 and created the model as model = KMeans(n_clusters=2, random_state=0) and kept the original class labels as before i.e ALL as 1 and AML as 0. However, this time the output didn't change on different runs and I got an accuracy of 53%. But, out of curiosity, I swap the original class label i.e. I set ALL as 0 and AML as 1 which results in 47%.

Scenario 3:

This time I choosed random_state as 1 i.e. model = KMeans(n_cluster=2, random_state=1) and having ALL as 0 and AML as 1 gave 67% accuracy while considering ALL as 1 and AML as 0 gave 33% accuracy.

So, My question is what I am doing wrong here? Am I implementing something wrong? If I am right then why the result is changing so much depending on random_state and class labels? What's the solution and how to choose the best pair of random_state and class labels?",1,scikit_learn,2020-10-13
gwgzy9,estimate_transform works when using 'similar' but not when using 'affine',"I have two 512x512 grayscales images (src and dst). To try to understand estimate transform I applied the following transformation

    tform = transform.AffineTransform(scale=(1.3, 1.1), 
                                        rotation=0.5, 
                                        translation=(0, -200)) 

to the src to create the dst. Then I want to find back the parameters using estimate\_transform.

With the parameter 'similar' I obtain parameters very close to the one I used (as expected). But when I want to use 'affine', I obtain the following error :

     matmul: Input operand 1 has a mismatch in its core dimension 0, 
    with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 513 is different from 3) 

Any idea why ? Here is my code :

    src = rgb2gray(data.astronaut())
    dst = rgb2gray(data.astronaut())
    tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5,
                                      translation=(0, -200))
    dst = transform.warp(img1, tform)
    tform_fin = transform.estimate_transform('affine', src, dst)
    dst_corr = transform.warp(img3, tform.inverse)",1,scikit_learn,2020-10-13
gtw4p9,What can I do when I keep exceeding memory used while using Dask-ML,"I am using Dask-ML to run some code which uses quite a bit of RAM memory during training. The training dataset itself is not large but it's during training which uses a fair bit of RAM memory. I keep getting the following error message, even though I have tried using different values for n_jobs:

```
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
```

What can I do?

Ps: I have also tried using Kaggle Kernel (which allows up to 16GB RAM) and this didn't work. So I am trying Dask-ML now. I am also just connected to the Dask cluster using its default parameter values, with the code below:

```
from dask.distributed import Client
import joblib

client = Client()

with joblib.parallel_backend('dask'):
    # My own codes
```",1,scikit_learn,2020-10-13
gsx926,MLPRegressor newby with some (probably very basic) questions in need of some assitance,"Hello!

I'm building MLPRegressor for the first time ever (I've been learning how to code with online courses since end of March) and I know something is wrong but I don't know what. Bellow you can see my code so far. It runs and I have a value for r2 ( -9035355.06 ) and a plot. However the r2 score doesn't make sense (it should be around 0.7)  and the plot doesn't make sense either.

I have run this analysis with SPSS multilayer perceptron feature so I know more or less how my results should be and that's why I know whatever I am doing with python is wrong.

Any advice/suggestion of what I'm doing wrong is very welcome! This coding world is kinda of frustrating for me:/

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    
    from sklearn import neighbors, datasets, preprocessing 
    from sklearn.model_selection import train_test_split
    from sklearn.neural_network import MLPRegressor
    from sklearn.metrics import r2_score
    
    vhdata = pd.read_csv('vhrawdata.csv')
    vhdata.head()
    
    X = vhdata[['PA NH4', 'PH NH4', 'PA K', 'PH K', 'PA NH4 + PA K', 'PH NH4 + PH K', 'PA IS', 'PH IS']]
    y = vhdata['PMI']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 
    
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_test_norm = scaler.fit_transform(X_test)
    
    nnref = MLPRegressor(hidden_layer_sizes = [4], activation = 'logistic', solver = 'sgd', alpha = 0.1, learning_rate= 'constant',
                         learning_rate_init= 0.6, max_iter=200, random_state=0, momentum= 0.3, nesterovs_momentum= False)
    nnref.fit(X_train_norm, y_train)
    
    y_predictions= nnref.predict(X_test_norm)
    
    print('Accuracy of NN classifier on training set (R2 score): {:.3f}'.format(nnref.score(X_train_norm, y_train)))
    print('Accuracy of NN classifier on test set (R2 score): {:.3f}'.format(nnref.score(X_test_norm, y_test)))
    print('Current loss : {:.2f}'.format(nnref.loss_))
    
    plt.figure()
    plt.scatter(y_test,y_predictions, marker = 'o', color='blue')
    plt.xlabel('PMI expected (hrs)')
    plt.ylabel('PMI predicted (hrs)')
    plt.title('Correlation of PMI predicted by MLP regressor and the actual PMI')
    plt.show()",1,scikit_learn,2020-10-13
gsol3k,What are the default values for the parameters in Dask-ML's Client() function,"I am trying to understand Dask-ML's Client() function parameters. Say I have the following code using Dask-ML's Client() function:

```
from dask.distributed import Client
import joblib

client = Client()
```

If I don't specify any values for the parameters in the Client() function, what are the default values for the parameters:

(i) n_workers

(ii) threads_per_worker

(iii) memory_limit

From my understanding, Python has the Global Interpreter Lock (GIL) feature which prevents multi-threading. If so, why does Dask-ML's Client() function have the parameter threads_per_worker when multi-threading is prevented in Python?

Does memory_limit refers to the maximum memory limit allowed for **each** worker/machine/node or does this refer to the maximum memory limit allowed for **all combined** worker/machine/node?

I have already looked through the documentation in Dask-ML (see here: https://docs.dask.org/en/latest/setup/single-distributed.html), but the documentation is not clear in regards to these questions above.

Thank you in advance if anyone could explain this?",1,scikit_learn,2020-10-13
glb4cy,Why does PolynomialFeatures has multiple pair of coefficient after fitted the data?,"After I create an PolynomialFeatures object, and fit the data by :

[`poly.fit`](https://poly.fit)`(x,)`

I wanted to look for the coefficient, so I do:

`poly.transform(x,y)`

&amp;#x200B;

And it will return an array with (n\_samples, n\_coeff), but why does the polynomial fit with multiple pair of coefficient? Wouldn't the model fit the data and get a final best coefficient?

&amp;#x200B;

And what is the final coefficient that Polynomial get after fitting?",1,scikit_learn,2020-10-13
gi4jw3,How to add sample_weight into a scikit-learn estimator,"I have recently developed a scikit-learn estimator (a classifier) and I am now wanting to add sample_weight to the estimator. The reason is so I could apply boosting (ie. Adaboost) to the estimator (as Adaboost requires sample_weight to be present in the estimator).

I had a look at a few different scikit-learn estimators such as linear regression, logistic regression and SVM, but they all seem to have a different way of adding sample_weight into their estimators and it's not very clear to me:

Linear regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_base.py#L375

Logistic regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_logistic.py#L1459

SVM: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/svm/_base.py#L796

So I am confused now and wanting to know how do I add sample_weight into my estimator? Is there a standard way of doing this in scikit-learn or it just depends on the estimator? Any templates or any examples would really be appreciated. Many thanks in advance.",2,scikit_learn,2020-10-13
get2kh,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,,2,scikit_learn,2020-10-13
gdb7a8,Code a Decision Tree in 20 lines.,,0,scikit_learn,2020-10-13
gd81x4,why does Scikit Learn's Power Transform always transform the data to zero standard deviation?,"all of my input features are positive. Whenever I tried to apply PowerTransformer with box-cox method, the lambdas are s.t. the transformed values have zero variance. i.e. the features become constants

&amp;#x200B;

I even tried with randomly generated log normal data and it still transform the data into zero variance.

&amp;#x200B;

I do understand that mathematically, finding the lambda s.t. the standard deviation is the smallest, would mean the distribution would be the most normal-like.

&amp;#x200B;

But when the standard deviation is zero, then what's the point of using it?

&amp;#x200B;

&amp;#x200B;

p.s. so one of the values of lambda I get by using PowerTranformer is -4.78 

If you apply it into the box-cox equation for lambda != 0.0, then for any input feature y values, you technically get the same values. i.e. (100\^(-4.78)-1.0)/(-4.78) is technically equals to (500\^(-4.78)-1.0)/(-4.78)",2,scikit_learn,2020-10-13
gcvtsm,how to combine recursive feature elimination and grid/random search inside one CV loop?,"I've seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:

[Feature selection and cross-validation](https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751)

[Nested cross-validation and feature selection: when to perform the feature selection?](https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec)

[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)

&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...

[Here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.

However I'd like to do recursive feature elimination inside random/grid CV, so that ""feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)"", so that data from other folds influence neither feature selection nor hyperparameter optimization.

Is this possible natively with sklearn methods and/or pipelines? Basically, I'm trying to find an sklearn native way to do this before I go code it from scratch.",3,scikit_learn,2020-10-13
gc2z28,How to write a scikit-learn estimator in PyTorch,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",3,scikit_learn,2020-10-13
g5ugws,Code a Neural Network in 20 lines.,,2,scikit_learn,2020-10-13
g4yc73,Basic question re: gaussian mixture models,"I wasn't able to find this in the documentation, but is the covariance parameter you access with model.covariances\_ sigma or sigma\^2? Seems like it can be either thing as I've seen the notations N(x| mu, sigma\^2) and N(x|mu, sigma) both used in various places.",1,scikit_learn,2020-10-13
fzm7mm,"Should scikit-learn include an ""Estimated Time to Arrival"" (ETA) feature? Discuss.",,7,scikit_learn,2020-10-13
fx6kdy,Clustering of t-SNE,"Hello,

I have recently tried out t-SNE on the sklearn.datasets.load_digits dataset. Then i applied KNeighborClassifier to it via a GridSearchCV with cv=5.

In the test set (20% of the overall dataset) i get a accuracy of 99%

I dont think i overfitted or smth. t-SNE delivers awesome clusters. Is it common to use them both for classifying? Because the results are really great. I will try to perform it on more data. 

I am just curious on what you (probably much more experienced users than me) think.",1,scikit_learn,2020-10-13
fx0i7x,Search over preprocessing and ensemble hyperparameters?,"In scikit-learn there are some handy tools like `GridSearchCV` for tuning the hyperparameters to a model or pipeline.

Suppose you'd like the preprocessing in your pipeline to include some user-defined options (e.g. whether to encode a certain categorical variable via one-hot encoding or something weird like frequency encoding) and you'd like to include those options among the hyperparameters you're searching over.

Suppose further that you're using an ensemble model -- e.g. a random forest plus few linear regression specifications, and you'd like to tune the hyperparameters for each of them, as well as the voting weight of each.

Does scikit-learn provide a predefined way to search over such spaces? It looks like the parameter space is intended only to dictate the behavior of a single model, not preprocessing steps or ensemble parameters.",1,scikit_learn,2020-10-13
ft2pcp,"How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?","How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?",1,scikit_learn,2020-10-13
ft1kmb,facing an error,"import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

&amp;#x200B;

\# Importing the dataset

dataset = pd.read\_csv('50\_Startups.csv')

X = dataset.iloc\[:, :-1\].values

y = dataset.iloc\[:, 4\].values

X2=dataset.iloc\[:, 3\].values

\# Encoding categorical data

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

le = LabelEncoder()

X2 = le.fit\_transform(X2)

oh = OneHotEncoder(categories = 'X\[:, 3\]')

X= oh.fit\_transform(X).toarray()

&amp;#x200B;

https://preview.redd.it/7bgiwdp238q41.png?width=871&amp;format=png&amp;auto=webp&amp;s=d2386c5cda100706a85803c036586beec8b9e843",1,scikit_learn,2020-10-13
fm1oov,I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?,"I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?

    preprocess =     make_column_transformer((SimpleImputer(strategy='median'), cols_numeric),     
    (SimpleImputer(strategy='constant', fill_value='missing'), cols_onehot),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_target),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_ordinal),     (OneHotEncoder(handle_unknown='ignore'), cols_onehot),     
    (TargetEncoder(), cols_target),     
    (OrdinalEncoder(), cols_ordinal),     
    (StandardScaler(), cols_numeric)) 
    lr_wpipe = make_pipeline(preprocess, LinearRegression()) 
    lr_scores = cross_val_score(lr_wpipe, X_train, y_train) 
    np.mean(lr_scores) 
    print(""Linear Regression R^2: "", lr_scores)",1,scikit_learn,2020-10-13
flg6a1,how to find 'the math' being done in sklearn source code?,"hi.  I'm trying to find where in sklearn the actual math is being done, mostly for my own learning so I can answer questions like 'when using `sklearn.neighbors` , what math is being used to calculate Euclidean distance?'    


If you see here: [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/\_base.py#L360](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/_base.py#L360)  
you'll see that Euclidian and other distance calculations can be *specified*, but I don't see anywhere where the actual math is being done in code.",1,scikit_learn,2020-10-13
fl4fj1,Adding Standard Scaler to GridSearchCV,"I'm looking to use the Standard Scaler as a hyper parameter, i.e check if performance is higher with/without scaling the inputs. In order to tune with other hyperparameters, I would like to incorporate it into my GridSearchCV function (provided by Scikit Learn). Can someone advise me on how to do it?",1,scikit_learn,2020-10-13
ffx9zw,How to use tfidfvectorizer fit_transform for multiple docs,"Hey, 

Let's say my corpus is a list of lists , each of the inner lists represent a parsed doc (each value is a word) 

I want to compute a tf-idf score for my corpus. 

It's seems like the fit-transform function can't use my corpus as its inputs should be itratable with string values (which is each of my docs) 


    V = tfidfvectorizer ()
    For doc in corpus:
       Vectors = v.fit_trabsform(doc)

So my question is, how does it calculate IDF if it get only one doc at a time?",1,scikit_learn,2020-10-13
ff9602,Classifiers' score method clarification,"Hi,

I don't fully understand what the score method of classifiers does. For example, the [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method's documentation says ""Return the mean accuracy on the given test data and labels."" Now, I know what is accuracy: (TP+TN)/(TP+TN+FP+FN), but I don't understand why ""mean"" is in there. Mean over what? of what?  

That is, I give the method as parameters a dataset with true labels, and it can calculate the accuracy from that (given the model), but where does the mean come into place? 

Thanks in advance!",3,scikit_learn,2020-10-13
fbfky4,Is epsilon in dbscan a euclidean measure?,"Hello everyone,
I'm writing yet another dbscan question. For those who are familiar with the inputs to the dbscan, the principal parameters are epsilon and minPts.

Epsilon is the neighborhood radius, and I'm curious if anyone can point me to a reference or tell me if epsilon is a euclidean metric",3,scikit_learn,2020-10-13
f1dizs,Identifying smallest frequently occurring value,"I'm not a data science person, but thinking Scikit learn might be able to help here, and looking for suggestions for ideas I should investigate.

Essentially, I'm looking for a way to consistently identify a baseline power readings. If I have minute by minute power consumption readings from a bunch of electrical motors.  For any motor, we want to identify what a 'baseline' or 'normal unloaded steady-state' power value is.

There is definitely noise in the signal, and not even noise - legitimate power reading that are smaller than what we would consider 'normal unloaded steady state'.  The catch is this could be different for the same motor when production composition changes, so there is not just one value that we can look at historical data to arrive at. (Think motors running pumps moving different fluid mixtures / slurry ad different times.

This does not have to be real-time, just take the dataset of power readings for any motor for any production batch and post-process the data in such a way we can identify times the motor is doing its job at a 'near-idle' state.

Currently we just have a basic calculation that looks at a rolling window of 20 per-minute readings and finds the lowest value that occurs at least twice. (basically throwing out the lowest few outliers)

The reason I'm considering Scikit or similar is we can graph these power readings for a time period (say 1 day) and visually we can easily see these 'baselines' we are looking for.  There will be spikes and dips, and time windows where we are definitely running a heavy load (motors spun up on demand), but we can identify when the mixture changes because the visual changes in this baseline value.

Hope that made at least a little sense, if there are details I can clarify, please ask.  I appreciate everyone's thoughts and ideas!",1,scikit_learn,2020-10-13
eylu4p,What's wrong with Scikit-Learn.,,3,scikit_learn,2020-10-13
ev1as7,Is it possible to use a custom-defined decision tree classifier in Scikit-learn?,"I have a predefined decision tree, which I built from knowledge-based splits, that I want to use to make predictions. I could try to implement a decision tree classifier from scratch, but then I would not be able to use build in Scikit functions like predict. Is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? Or do I need to do something completely different? My first attempt was to use “fake training data” to force the algorithm to build the tree the way I like it, this would end up in a lot of work because I need to create different trees depending on the user input.",1,scikit_learn,2020-10-13
eu9qgc,Is HistGradientBoosting the same as LightGBM or is the SKLearn's version different?,"If so, how?",2,scikit_learn,2020-10-13
em9fxf,Is this the proper way to do ML with scikit_learn?,"I have a dataset with 8 features (numeric) and 1 target (0 or 1).  
 I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc.  
 For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

But sometimes I get these errors:  
 The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

What's the proper way to predict binary values?  
 I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Thanks for your support. I'm a beginner.",2,scikit_learn,2020-10-13
effb98,What module/algorithm should I use in order to predict the time in which a certain action will be completed?,"Basically the title, but to explain it even more :

I have to device a model which will predict the total time a patient will have to wait in a hospital environment. For that, we have a dataset consisting of various patients with several diseases and their time durations already recorded. I want to know which module or algorithm should I use to carry this out? This is my first ML project and I could use any help that you guys can do. Thank you!!",2,scikit_learn,2020-10-13
e0h8ao,Column transformer throwing away some features?,"My data frame  has numerical columns a, b, c. And it has categorial text features d, e, f, g, h.

I build a preprocessor like the following.

    num_features = ['a','b','c']
    nom_features = ['d','e','f','g','h']
    
    preprocesser = ColumnTransformer([
        (""scale_numeric"", StandardScaler, num_features),
        (""encode_nominal"", OneHotEncoder(handle_unknown=""ignore""), nom_features)],
        remainder=""drop""
    )
    
    preprocessor.fit_transform(dataframe)

Since I started with 8 features, after one hot encoding I expected to get 8 or more features back, but the result of `preprocessor.fit_transform(dataframe)` only has 3 columns. Not sure what I am doing wrong if anyone can help me.",1,scikit_learn,2020-10-13
dyzwn9,How to Modify(Make unique) the Scikit-learn Multilayer perseptron algorithm (MLP),"Hi folks,

I've been trying to build a rainfall prediction model for last few days. I've used the **Scikit-learn Multilayer perseptron regressor function** straight up. 

1) The accuracy was OK(78%) but I want to increase it

2) I don't want to use the same predominantly given function (I just want to **add uniqueness in my code**, but I want to use scikit-learn)

Is there any way to modify the function or not use the ready-made function? Can anyone please help me with this?

Thanks in advance!",1,scikit_learn,2020-10-13
dtnh3f,The best alpha for ridge regression is... -85???,,3,scikit_learn,2020-10-13
dtiy98,difference between Kfold.split() and shufflesplit.split() in scikitlearn,"I read this [post](https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv), I get the difference when it comes to computation and shufflesplit randomly sampling the dataset when it creates the testing and training subsets, but in the answer on stackoverflow, there is this paragraph  


""**Difference when doing validation**

In KFold, during each round you will use one fold as the test set and *all* the remaining folds as your training set. However, in ShuffleSplit, during each round **n**  you should *only* use the training and test set from iteration **n** ""

I couldn't quite get it. since in kfold, you're bounded by using the training buckets (k-1) and testing bucket (k) in the **k** iteration and in shufflesplit you use the training and testing subsets made by the shufflesplit object in iteration **n.**  so for me it feels like he's saying the same thing.

can anyone please point out the difference for me?",1,scikit_learn,2020-10-13
dcvd2r,When to use these unsupervised algorithms?,"There are a lot of modules in sklearn. I am interested when these unsupervised algorithmes (bellow )are used.  
When to use a Gaussian mixture model? When to use Manifold Learning, When to Biclustering? etc.

&amp;#x200B;

* [2.1. Gaussian mixture models](https://scikit-learn.org/stable/modules/mixture.html) 
* [2.2. Manifold learning](https://scikit-learn.org/stable/modules/manifold.html)
* [2.4. Biclustering](https://scikit-learn.org/stable/modules/biclustering.html) 
* [2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html) 
* [2.6. Covariance estimation](https://scikit-learn.org/stable/modules/covariance.html) 
* [2.7. Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html) 
* [2.8. Density Estimation](https://scikit-learn.org/stable/modules/density.html)",7,scikit_learn,2020-10-13
dar9z6,pattern recognition on texts that are bash commands or software signature?,"hi all.

so I've got my hands on a daily dose of 100,000 connections per day to our servers, and I've got millions of rows of data that includes commands our users have executed on our servers, (\`cd\`, \`ehlo\`, \`scp ....\`, etc). and I have the same amount of data of their application signatures while connecting. like (Firefox 59, Firefox 60, google chrome),... and user agents, ...

basically all the data one can extract out of a socket or using an IDS.

I like to do some pattern matching on these data. like for the commands they are executing and stuff like that...

so to cluster the commands, I've got commands that look like this:

cd Project

cd Images/personal

cd Project/map

cat /var/log/nginx/web\_ui.log

the problem is, I can just split the texts and take in the first part(cd, cat) and make plot out of the commands, but i really would like to make it more automatic and intelligent. so people who \`cd\` into the \`Project/map\` are distinguished from people who cd into \`Images\` folder. I like to know what people are doing on out servers. so a plot that all people whith \`cd\` commands are close to each other, but are really distinguished for each folder that they have \`cd\` into. 

this is just an example of what I want:)

&amp;#x200B;

turns out that scikit\_learn only works on numbers? how can i utilize it for that kind of data? I don't know if this is a nltk problem?",3,scikit_learn,2020-10-13
d8sxus,Exporting Models to build own inference server,"Hello, I was hoping to get pointed in the right direction.  After training a random forest classifier I am looking to export the model in such a way that I can recreate each of the trees in C++.  I am trying to figure out the best approach to this, or if it is even possible.  My research online mainly shows examples of how to visually represent these, and how to create a pickle project for python serialization.  

Am I missing some key terms in my search? Could you point me to what i should be doing to figure this out?

&amp;#x200B;

My approach so far has been exploring the clf.estimators.trees\_ part of the estimators object, but I am not sure if I am on the right track.

&amp;#x200B;

Any help is much appreciated.

&amp;#x200B;

Thanks!",1,scikit_learn,2020-10-13
d244x4,Predict device from flow,"Hey guys, I applied to a competition about AI and my task is to predict device class from flow. I have 13 types of classes which are all in train set but the test set is missing that one column. After I run training and then I try to predict it, I receive an error stating this: ValueError: query data dimension must match training data dimension.

How can I predict a column that is not there? I don't believe that I have to manually put the column to the test.json 

Thanks for advices.",4,scikit_learn,2020-10-13
cvtjk6,Predicting Churn With Nested Data,"Hello All!

Ok, so this is a bit of a challenge and I'm trying to figure out if it is even worth worrying about the nesting aspect of the data. Basically, I'm trying to predict subscription-level churn with a combination of subscription-level and user-level variables.

Since users own subscriptions I figured I should try to account for nesting in my model. Does anyone have any recommendations on how to attack churn predictions using a nested model? Any suggestions would be greatly appreciated. Again, I have code working, but I've never built anything that requires nested analysis.

Basically my question is: Is it possible to run a multi-level SVM?",2,scikit_learn,2020-10-13
cs2urp,What is the most efficient way to implement two-hot encoding using scikit learn?,"I have two very similar features in my dataframe, and I would like to combine their one-hot encoded versions. They are both categorical data, and they both contain the same categories. I was thinking about using OneHotEncoder from scikit learn and getting the union of the corresponding columns. Is there a function or more efficient way that I do not know about?",3,scikit_learn,2020-10-13
cnnacy,Feature elimination doesn't really eliminate anything.,"I had a fairly simple dataset, after plotting the correlation matrix I noticed that one variable has very low correlation with the target (0.04) but instead of deleting it manually I decided to try feature elimination.
I tried both RFE and RFECV with Logistic Regression as an estimator, RFE eliminated some features which seemed correlated with the output and kept that feature.
RFECV didn't eliminate anything at all.

Am I missing something here?",1,scikit_learn,2020-10-13
cnl2a5,k-means output issue,"Hello I've run a k-means over my voice data. I got two class (for best). My problem is why i got this line at the right side? I sit an issue in my dataset?

https://preview.redd.it/n5lz9pggx7f31.png?width=852&amp;format=png&amp;auto=webp&amp;s=e84cc13f9579c026fc6bfa5cbd85849b1ea2939e",1,scikit_learn,2020-10-13
cmmbi5,Running scikit validation on 24 cores is slow?,"Hello guys, maybe anyone can help me out here. I am running following validation code:

```
from sklearn.linear_model import LinearRegression
model = LinearRegression()
from sklearn.preprocessing import PolynomialFeatures
poly_transformer = PolynomialFeatures(degree=2, include_bias=False)
from sklearn.pipeline import Pipeline
pipeline = Pipeline([('poly', poly_transformer), ('reg', model)])
train_scores, valid_scores = validation_curve(estimator=pipeline, # estimator (pipeline) X=features, # features matrix y=target, # target vector param_name='pca__n_components', param_range=range(1,50), # test these k-values cv=5, # 5-fold cross-validation scoring='neg_mean_absolute_error') # use negative validation
```

in the same .py file on different machines, which I would name #1 localhost, #2 staging, #3 live, #4 live. localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds
live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.
In order to get more ""trustworthy"" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different?",1,scikit_learn,2020-10-13
clz2bl,vectorization,"Hi, I just want to know if I can vectorize a text even if its on another language using Count Vectorization",2,scikit_learn,2020-10-13
clpubv,Machine learning final year project,"design and implement an intelligent agent that can detect a fault and can trouble a faulty server on a network

Its a network anormaly project 
But dont know where to start from",1,scikit_learn,2020-10-13
cl88rf,No Scikit-learn after I installed Anaconda in Sublime Text 3," 

I started using Sublime Text as my Text Editor/IDE (not sure what the difference is) to do some Python projects. After watching 2 episodes of the machine learning course by Google Developers. I installed the Anaconda package which has the Scikit-learn included.

Following the video I typed:

    import sklearn

This error appeared:

    ModuleNotFoundError: No module named 'sklearn'

Is there a way to install the Scikit-learn using the Sublime Text 3 or using a different method?",1,scikit_learn,2020-10-13
cgijm0,Unable to find/import,"edit: Title - Unable to find/import IterativeImputer

&amp;#x200B;

&amp;#x200B;

Hello fellow users, I'm wondering if yall could help me out with importing/finding IterativeImputer...

**&gt;&gt;&gt;** *# explicitly require this experimental feature*

**&gt;&gt;&gt; from** **sklearn.experimental** **import** enable\_iterative\_imputer *# noqa*

**&gt;&gt;&gt;** *# now you can import normally from impute*

**&gt;&gt;&gt; from** **sklearn.impute** **import** IterativeImputer

**ModuleNotFoundError**: No module named 'sklearn.impute.\_iterative'; 'sklearn.impute' is not a package

&amp;#x200B;

$pip freeze states I have scikit-learn==0.21.2 and sklearn==0.0

Python version 3.6

&amp;#x200B;

After researching the issue online I see that there's an experimental version I need to install, but I can't seem to find it! Further, I can't find it on their website.. [https://scikit-learn.org/dev/versions.html](https://scikit-learn.org/dev/versions.html)

What did I overlook/miss?",1,scikit_learn,2020-10-13
cc2mxr,How to re-structure a numpy dataframe into a format I can use in sklearn?,"Assuming the dataframe column 0 is the target and columns 1: are the features, and that each column is named, what's the easiest way to split the data for use in sklearn?",1,scikit_learn,2020-10-13
cbm4g6,How to classify dots,"Hello, 

I have a graph with two groups, red and blue dots. These groups are clearly separated, but the problem is that I want to say if a new dot belongs to the red group, to the blue, or to none of them.

What method do you recommend?

Thank you",1,scikit_learn,2020-10-13
c4rlf7,Regression is not yielding many useful predictions,"Hello all,   


I'm using a linear regression to predict continuous values (how long until a client churns measured in months). I have a dataset of cancelled accounts and active account. I'm using the cancelled accounts to predict the active accounts. I have a variety of explanatory variables and in total I have an R-squared of around 35% (obviously R-squared isn't perfect)   


Overall, this works pretty well; however, one issue I'm running into is that, of the predictions I get back, some are negative and very few actually predict that these active clients should still be active. In other words, many of the predicted cancellation dates are in the past.  


Dumb question, but is there a method I could be using to help this? Overall, I'm getting about 10k useful observations from 60k predictions.  Any suggestions would be greatly appreciated.",1,scikit_learn,2020-10-13
c4q5i6,I can't import Kmeans into compiler,"I'm currently using sklearn 0.21.2, and when I do:  


`import sklearn.cluster.KMeans`

&amp;#x200B;

the compiler returns error:

&amp;#x200B;

`no module named sklearn.cluster.KMeans`

&amp;#x200B;

I've found that in the cluster package, there is an module named 'cluster.k\_means\_'  


But when I tried to use this instead, it shows error

&amp;#x200B;

`Module is not callable`

&amp;#x200B;

Now I don't know why I can't import the kmeans package in cluster.",1,scikit_learn,2020-10-13
bylpjd,Sklearn regression with two datasets,"Hello all,   


basically, as the title implies I'm trying to train a regression model on one dataset and the apply that predictive model to another dataset. In other words, I have a model which predicts cancelled accounts and the amount of time in which those accounts cancel.   


I have another dataset full of active accounts (with the same variables) and I'm attempting to use the model from the cancelled accounts to predict when my active accounts will cancel. I'm having trouble with this.  Is there a way to do this without forcing a t  


Is there a way to use the ""active dataset"" without enforcing a Train\_test\_split? Any help would be greatly appreciated. Thank you!",2,scikit_learn,2020-10-13
bvjxzj,Get the function that fits my data,"I have fit a polynomial regressor to a two dimensional data. 
Is there a way to see the function that fits this data?",2,scikit_learn,2020-10-13
bqtxes,Kmeans clustering cache the result,"Hello,

&amp;#x200B;

I am new to scikit and I was wondering if I could cache the result of Kmeans so next time when I run my script I do not create the centroids again - that means save the result of [`kmeans.fit`](https://kmeans.fit)`()`.",2,scikit_learn,2020-10-13
bp7dv5,Get classes name of each estimator in OneVsOneClassifier,"Are there any ways to do that ? I am trying to directly access the classes\_ attributes in the estimator but it only returning \[0,1\]",2,scikit_learn,2020-10-13
bevq9d,Using Blob Detection methods on huge images,"I'm trying to use common blob detection methods from

[https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob\_dog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog)

on a huge images (about 6000x6000 pixels). It takes way too long to compute and show the result. How could I resolve this?",1,scikit_learn,2020-10-13
bcwbv4,Calculate variance of accuracy,"Hello, how can I calculate the variance of accuracy between two models in Random forest.
I mean I made a simple model with DecisionTreeClassifier() and one more with BagginClassofier() using the first model on it.
The accuracy climb +0.237.

How to get variance of that accuracy?
Thansk",1,scikit_learn,2020-10-13
bcbduy,Classification: Minimizing the amount of false positives,"Hey there,

I posted an earlier post (now deleted) that phrased this a bit wrong (thanks Imericle). Here is another try: 

Many  (most?) classification algorithm seem to be about maximizing accuracy  (true positives + negatives). My aim is to minimize the amount of false positives. How would I achieve this?

Only options I see to achieve this is through parameters tuning, is that the right approach?

(Thinking on applying it to a RandomForest),

Thanks,

Bb",2,scikit_learn,2020-10-13
bbxi9t,KMeans: Extracting the parameters/rules that fill up the clusters,"Hi all,

&amp;#x200B;

I have created a 4-cluster k-means customer segmentation in scikit learn. The idea is that every month, the business gets an overview of the shifts in size of our customers in each cluster. 

My question is how to make these clusters 'durable'. If I rerun my script with updated data, the 'boundaries' of the clusters may slightly shift, but I want to keep the old clusters (even though they fit the data slightly worse). My guess is that there should be a way to extract the paramaters that decides which case goes to their respective cluster, but I haven't found the solution yet. 

I would appreciate any help",1,scikit_learn,2020-10-13
b6nfvs,Question about FeatureUnion,"    pipe = Pipeline([
            ('features', FeatureUnion([
                    ('feature_one', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_one')),
                        ('vec', cvec) # Count vectorizer
                    ])),
                    ('feature_two', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_two')),
                        ('vec', tfidf) # Tf-idf vectorizer
                    ]))
                ])),
            ('clf', OneVsRestClassifier(clf)) #clf is a support vector machine
        ])

I'm using this pipeline for a project I'm working on, and I just want to make sure I understand how FeatureUnion works. I'm building a classifier which takes in two different text features and attempts to make a multi-class classification.

&amp;#x200B;

To give a little more detail, I'm trying to classify news articles into one of several categories (sports, business, etc.) Feature one is a list of tokens taken from the article's url, which often, though not always, explicitly states the name of the topic. Feature two is a list of tokens from the body of the article.

&amp;#x200B;

Does it make sense to separate the two features this way? Does this have a different effect than if I had just merged all of the tokens into a single list and vectorized them? My intention was to allow the two features to effect the model to different degrees, since I figured one would be more predictive in most scenarios (and I am getting pretty great results.)",2,scikit_learn,2020-10-13
b36h5a,Ranforest random behaviour,"If I give random forest parameters as RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019) Should it be creating 10 same decision trees? But it is not. I am asking the random forest to
     1.Sample without replacement (bootstrap=False) and each tree have same number of sample (ie the total data )(verified using plot)
     2.Select all features in all trees.
But model.estimators_[2] and model.estimators_[5] are different

",2,scikit_learn,2020-10-13
axgj2c,Predicting the runtime of scikit-learn algorithms,"Hey guys,

We're two friend who met in college and learned Python together, we co-created a package which can provide an estimate for the training time of scikit-learn algorithms.

The main function in this package is called “time”. Given a matrix vector X, the estimated vector Y along with the Scikit Learn model of your choice, time will output both the estimated time and its confidence interval. 

Let’s say you wanted to train a kmeans clustering for example, given an input matrix X. Here’s how you would compute the runtime estimate:

    From sklearn.clusters import KMeans
    from scitime import Estimator 
    kmeans = KMeans()
    estimator = Estimator(verbose=3) 
    # Run the estimation
    estimation, lower_bound, upper_bound = estimator.time(kmeans, X)

We are able to predict the runtime to fit by using our own estimator, we call it meta algorithm (meta\_algo), whose weights are stored in a dedicated pickle file in the package metadata.

The meta algos estimate the time to fit using a set of ‘meta’ features, including the parameters of the algo itself (in this case kmeans) and also external parameters such as cpu, memory or number of rows/columns. 

We built these meta algos by generating the data ourselves using a combination of computers and VM hardwares to simulate what the training time would be on the different systems, circling through different values of the parameters of the algo and dataset sizes . 

Check it out! https://github.com/nathan-toubiana/scitime

Any feedback is greatly appreciated.",7,scikit_learn,2020-10-13
aahf76,"Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?","Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?

&amp;#x200B;

So in the sense that if one applies e.g. a gain factor  (or a function depicting gain changes), then it's applied to the correct direction.",3,scikit_learn,2020-10-13
a75oid,"classification_report + MLPClassifier(): UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn_for)","classification\_report on a prediction done on MLPClassifier() sometimes throws:

&amp;#x200B;

*UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn\_for)*

&amp;#x200B;

but not on all the time.

&amp;#x200B;

What could be wrong?

&amp;#x200B;

\---

&amp;#x200B;

Doing

&amp;#x200B;

set(y\_test) - set(y\_pred) 

&amp;#x200B;

I'm able to see that sometimes some label is missing from y\_pred. But why does this occur only occasionally?

Is something wrong with how I use MLP?",1,scikit_learn,2020-10-13
a753f2,"{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification_report()","{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification\_report()

&amp;#x200B;

Why?

&amp;#x200B;

I call it like:

&amp;#x200B;

    classification_report(y_test, y_pred)

where y\_pred is predicted using a model I built.

&amp;#x200B;

""Quite obviously"" the arguments are incompatible somehow, but how can I find out, how? And how can I make them compatible?

&amp;#x200B;

\---

&amp;#x200B;

I tried:

&amp;#x200B;

    from sklearn.utils.multiclass import type_of_target

&amp;#x200B;

    &gt;&gt;&gt; type_of_target(y_test)
    'multiclass'
    
    &gt;&gt;&gt; type_of_target(y_pred)
    'continuous'

&amp;#x200B;",1,scikit_learn,2020-10-13
a746h0,"Is there any way to ""estimate"" how long a given computation in sklearn will take?","Is there any way to ""estimate"" how long a given computation in sklearn will take?

&amp;#x200B;

So that one doesn't need to wait longer than what one can?

&amp;#x200B;

Also, since Windows Task Manager shows only modest CPU use (&lt; 10%), then how is one supposed to know, what's occurring in the model?",2,scikit_learn,2020-10-13
a73oda,What are the most important parameters in LogisticRegression()?,What are the most important parameters in LogisticRegression()?,3,scikit_learn,2020-10-13
a71xf2,How does one feed hidden_layer_size tuples into GridSearchCV's param_grid?,How does one feed hidden\_layer\_size tuples into GridSearchCV's param\_grid?,1,scikit_learn,2020-10-13
a0yprf,Code review,"Hello,

I'm new to ML and scikit - hope this is the correct place for this. Have created the below code that appears to be working but wanted to get the opinions of people with more experience then me, to check I haven't a made any major errors or if there are any obvious improvements?

&amp;#x200B;

I am trying to train a model on a data set of potentially hundred of thousands emails. Every few days I want to retrain the exported model using incremental learning on the new emails received since the model was last trained.

The below reads the initial data from a csv, runs HashingVectorizer then SGDClassifier. The OnlinePipeline is used to allow me to use partial\_fit when I try to retrain later in the process.

`import pandas as pd`

`data = pd.read_csv('customData1.csv')`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`from sklearn.feature_extraction.text import HashingVectorizer`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

&amp;#x200B;

`from sklearn.linear_model import SGDClassifier`

`text_clf = OnlinePipeline([('vect', HashingVectorizer()),`

`('clf-svm', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=5, random_state=None)),`

`])`

`text_clf = text_clf.fit(X_train,Y_train)`

`predicted = text_clf.predict(X_test)`

`np.mean(predicted == Y_test)`

The above gives me an accuracy of 0.55

&amp;#x200B;

A few days later when I have new emails I import the previously exported model and use partial\_fit on a new csv file.

`import pandas as pd`

`data = pd.read_csv('customData2.csv') #text in column 1, classifier in column 2.`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

&amp;#x200B;

`from sklearn.externals import joblib`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

`text_clf2 = joblib.load('text_clf.joblib')`

&amp;#x200B;

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`text_clf2 = text_clf2.partial_fit(X_train,Y_train)`

&amp;#x200B;

`predicted = text_clf2.predict(X_test)`

`np.mean(predicted == Y_test)`

This returns the improved accuracy of: 0.84

&amp;#x200B;

Sorry for so much code!  I obviously need to tidy it all up so its a single method and handle the import/export logic properly.

&amp;#x200B;

Have a made any major errors or are there any obvious improvements? Thanks!

&amp;#x200B;",1,scikit_learn,2020-10-13
a0iz4i,Does cross_val_score tell something about generalizability?,"Does cross\_val\_score tell something about generalizability?

&amp;#x200B;

Or do I need to use something else for measuring generalizability?",0,scikit_learn,2020-10-13
a0c8dw,"Is there a problem if MLPRegressor doesn't converge for max_iter=100, but nor max_iter=5000 either?","Is there a problem if MLPRegressor doesn't converge for max\_iter=100, but nor max\_iter=5000 either?

&amp;#x200B;

Anything else I could try?",1,scikit_learn,2020-10-13
a0b260,What do cv (number of folds) and the number of outputs in cross_val_score correspond to?,"What do cv (number of folds) and the number of outputs in cross\_val\_score correspond to?

&amp;#x200B;

Does it mean that it produces cv number of different scores? Or (as I read somewhere) that only the last score might be the meaningful one (I read something like the others than the last used to ""fit"", while the last is the score)?",2,scikit_learn,2020-10-13
a0awuh,"Getting values in range [-191806. ..., 0.77642 ...] from cross_val_score, am I doing something wrong?","Getting values in range \[-191806. ..., 0.77642 ...\] from cross\_val\_score, am I doing something wrong?

&amp;#x200B;

    mlp = MLPRegressor(hidden_layer_sizes=(7,))

mlp.fit(X\_train,y\_train) mlp\_y\_pred = mlp.predict(X\_test)

&amp;#x200B;

y\_pred is an earlier prediction using LinearRegression().

&amp;#x200B;

I call cross\_val\_score like:

&amp;#x200B;

    cross_val_score(mlp, y_pred, mlp_y_pred, cv=10)

&amp;#x200B;

Output is:

&amp;#x200B;

    00 = {float64} -4.4409160725075605
    01 = {float64} -673636.0674512024
    02 = {float64} -51282.162171235206
    03 = {float64} -399557.4789466267
    04 = {float64} -35.73093353875776
    05 = {float64} -1406.9741325253574
    06 = {float64} -80853.84044929259
    07 = {float64} -5132.870883709122
    08 = {float64} -283.7432365432288
    09 = {float64} -2.860321933844385

&amp;#x200B;

I think I should be getting values in range \[0,1\].",1,scikit_learn,2020-10-13
a0at1q,"Is MLPRegressor's hidden_layer_sizes=(7,) equivalent to hidden_layer_sizes=7?","Is MLPRegressor's hidden\_layer\_sizes=(7,) equivalent to hidden\_layer\_sizes=7?",1,scikit_learn,2020-10-13
a09ukl,"Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train_test_split(Xy,shuffle = False, test_size = 0.33)?","Why I get ""ValueError: not enough values to unpack (expected 4, got 2)"" using train\_test\_split(Xy,shuffle = False, test\_size = 0.33)?

Xy has been constructed like:

&amp;#x200B;

    X = dat.data
    y = dat.target 
    Xy = np.hstack((X,np.array([y]).T))

It seems that it returns only two arrays, even when I saw an example ([https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input](https://stats.stackexchange.com/questions/310972/sklearn-should-i-create-a-minmaxscaler-for-the-target-and-one-for-the-input)) do 

 

    X_train, X_test, y_train, y_test = train_test_split(Xy,shuffle = False, test_size = 0.33) ",1,scikit_learn,2020-10-13
a06iin,Runtime Error in RandomizedSearchCV,"I've been running a RandomForestClassifier on a dataset I took from UCI repository, which was taken from a research paper. My accuracy is \~70% compared to the paper's 99% (they used Random Forrest with WEKA), so I want to hypertune parameters in my scikit learn RF to get the same result (I already optimized feature dimensions and scaled). I use the following code to attempt this (random\_grid is simply some hard coded values for various parameters):

&amp;#x200B;

    rf = RandomForestClassifier()
    # Random search of parameters, using 2 fold cross validation,
    # search across 100 different combinations, and use all available cores
    rf_random = RandomizedSearchCV(estimator = rf,  param_distributions = random_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)
    # Fit the random search model
    rf_random.fit(x_train, x_test)

When I attempt to run this code though my python runs indefinitely (for at least 40 min before I killed it) without giving any results. I've tried reducing the \`cv\` and \`n\_iter\` as much as possible but this still doesn't help. I've looked everywhere to see if there's a mistake in my code but can't find anything. I'm running Python 3.6 on Spyder 3.1.2, on a crappy laptop with 8Gb RAM and i5 processor :P

&amp;#x200B;

Here is the random\_grid if it helps:

    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
    max_features = ['auto', 'sqrt']
    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
    max_depth.append(None)
    min_samples_split = [2, 5, 10]
    min_samples_leaf = [1, 2, 4]
    bootstrap = [True, False]
    
    # Create the random grid
    random_grid = {'n_estimators': n_estimators,
                   'max_features': max_features,
                   'max_depth': max_depth,
                   'min_samples_split': min_samples_split,
                   'min_samples_leaf': min_samples_leaf,
                   'bootstrap': bootstrap}

&amp;#x200B;",1,scikit_learn,2020-10-13
9yhcqq,Does sklearn have built-in routines for testing results of LinearRegression()?,Does sklearn have built-in routines for testing results of LinearRegression()?,1,scikit_learn,2020-10-13
9ygvsi,How does fit_transform allow for other data to be processed with the same transformer?,"How does fit\_transform allow for other data to be processed with the same transformer?

&amp;#x200B;

Like here:

&amp;#x200B;

[https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)

&amp;#x200B;

Particularly, since one first calls fit\_transform, then why does it allow one to call transform afterwards and still get the same fit? Like how is this kind of functionality implemented?

&amp;#x200B;

&amp;#x200B;",1,scikit_learn,2020-10-13
9tf8yw,Principal component Analysis: predicting values,"I am attempting to forecast a set of multivariate time series data. I have run a PCA (using the scikit-learn module) and have run an AR(1) auto-regression of the 3 components.

Now that I have the projects component values, how do I recast those components into the original variables, in order to find the projection for those variables?",2,scikit_learn,2020-10-13
9t3xwo,Extract a single stratified part of a dataset,"I have a multi-label dataset with N samples, and I want to take a chunk out to reserve for validation, e.g. reserve k% of the dataset.

Note that I want to do this just once, else I could use stratifiedKFold.  
Is there a function to produce such a single chunk, ensuring stratification with respect to  the labels?  
(A workaround would be to produce N\*k  KFold splits, concatenate  all parts but one  for training, and use the last for validation.)

Thanks.",1,scikit_learn,2020-10-13
9sbqvm,Stepping through each iteration of the LogisticRegression fit() function,"Hello guys,

I'm using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class to find the decision functions between my classes. I wanted to ask you - *how can I step through each step of the algorithm?* I know I can give the parameter `max_iter` to determine the number of iterations, but I want to step through each of those `max_iter` iterations - to see how the values of the coefficients change.

Thanks in advance!",1,scikit_learn,2020-10-13
9pcack,"modAL: A modular active learning framework for Python, built on top of scikit-learn",,5,scikit_learn,2020-10-13
8ndofu,Parse Twitter feed and suggest domain names • r/nltk,"I'm working on a hackathon, and I'd like to parse a user's last 100 tweets or so and make recommendations for a domain name using a new TLD.

The plan I've got in my head is

1\) Scrape twitter for a bit and get some data \(How much? How many records?\)

2\) Run tf\-idf against it, save that dataset

3\) split the initial twitter data into groups based on which tweets contain each TLD \- supplies, computer, kitchen, etc.

a\) Run some kind of clustering algorithm against each set? 250 or so TLDs

\-\- This is where I have questions

4\) Scrape their twitter feed and get 100 tweets

5\) Use the tf\-idf data from step 2 to spit out keywords

6\) use those keywords using some kind of distance formula against the clustered data to pick a tld?

7\) use the bigrams or keywords to make up an SLD.

This seemed off to a good start, but can I somehow pickle the cluster results? Or have multiple sets of cluster results in the same object?

Note: 95&amp;#37; of my knowledge on this topic comes from this blog post: [http://brandonrose.org/clustering](http://brandonrose.org/clustering)",2,scikit_learn,2020-10-13
8l4imo,"move partial of decision models from server to client - side, is it good idea?","Hi,
Some time ago tenser flow for js was released. I'm wondering about build bridge for some scikit learn models to move some part of learning and prediction to the client side. I think that it could help minor companies reduce server resource usage and make models and prediction much more personalised. Do you think it's a good idea? Do you know whether someone has tried something similar before?",1,scikit_learn,2020-10-13
8emc3b,How to combine num values with text data for classification?,I build website classifier and use text of each webpage (transformed to bag of words) as train data. But I also want to add each website's PageRank as feature. How can I do that?,2,scikit_learn,2020-10-13
89u8tj,PLSRegression Issues,"I'm working with scikit's cross\_decomposition.PLSRegression(). According to their documentation, x = np.multiply( x\_scores\_, x\_loadings\_.T ). I'm not getting anything close to the same value values. I've tried every combination of using scale=False and sklearn.preprocessing.scale(x) to try and find how this works out, but I haven't been able to find one that works.

    plsr = PLSRegression(n_components=x_df.shape[0]-1).fit(x_df.T,y)
    print(np.matmul(plsr.x_scores_,plsr.x_loadings_.T).T)
    print(x_df)

Using scaled data (i.e. PLSRegression.fit( scale(x\_df).T, scale(y) ) and changing n\_components doesn't help either. If anyone has any idea of what mistake I have made, or if this just a bug in sklearn?",1,scikit_learn,2020-10-13
81xksg,How to remove terms from a term-document matrix?,"Hello,

I have a term document matrix that I've created using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) like so:
    
    X = vectorizer.fit_transform(corpus)
    X
    &lt;1000x10022 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    	with 94340 stored elements in Compressed Sparse Row format&gt;

I'd now like to remove any terms that do not appear in at least 3 documents, and then calculate the TF-IDF scores for each term, and select the vocabulary as the top n terms ordered by TF-IDF scores.

Is there an easy way of removing terms from the term document matrix that do not appear in at least 3 documents, while still conserving the mapping from feature names to feature indices?

I guess one way to do it would be to get the feature names of the terms that appear in at least 3 documents using numpy on the sparse matrix directly, assign them a mapping to indices, and then pass that mapping to the vocabulary parameter in the CountVectorizer constructor.

Any ideas on how to do this more easily?",1,scikit_learn,2020-10-13
7zfjba,How to use partial_fit to update the model trained with fit() instead of training from scratch,"I tried partial_fit with various scikit online learning classifiers like perceptron, passive aggresive classifiers, SGDclassifer... like here: https://ideone.com/uOtRTZ.  I just dont understand why i cant train the new data on top of already trained data. I am doing image classification. I have trained my 10,000 images with fit(). Now i got 1 new image to add to this dataset of already trained images. I want to update the trained model instead of training all 10,001. Is this possible with partial_fit() ? If so, please tell me how ? ",1,scikit_learn,2020-10-13
7z4qi7,SGDClassifier.partial_fit returns error of “classes should include labels”,"I tried to predict label of my newly added data through SGDClassifer.partial_fit as below:

        from sklearn import neighbors, linear_model
    import numpy as np
    
    
    def train_predict():
        X = [[1, 1], [2, 2.5], [2, 6.8], [4, 7]]
        y = [1, 2, 3, 4]
    
    
        sgd_clf = linear_model.SGDClassifier(loss=""hinge"")#loss
    
        sgd_clf.fit(X, y)
    
        print(sgd_clf.predict([[6, 9]]))
    
        X.append([6, 9])
        y.append(5)
    
    
        X1 = X[-1:]
        y1 = y[-1:]
    
        classes = np.unique(y)
    
        f1 = sgd_clf.partial_fit(X1, y1, classes=classes)
    
        print(f1.predict([[6, 9]]))
    
        return f1
    
    
    if __name__ == ""__main__"":
        clf = train_predict()  # your code goes here

However, this results in error: ValueError: `classes=array([1, 2, 3, 4, 5])` is not the same as on last call to partial_fit, was: array([1, 2, 3, 4])

Any ideas or references ? ",1,scikit_learn,2020-10-13
7vw4ap,Retrain a KNN classified model (scikit),"I trianed my knn classifer over multiple images and saved the model. I am getting some new images to train. I dont want to retrain the already existing model.

How to add the newly tranied model to the existing saved model ?

Could someone guide if this is possible or any articles describing the same ?

Thank you,",2,scikit_learn,2020-10-13
70m5l1,How do I add matplotlib to a django webapp and display the code's output on the webpage?,Trying to make a User Interface for a Support Vector Machine from the SVM function in the matplotlib,1,scikit_learn,2020-10-13
6lec9n,"K-NN and custom metrics, speed up sklearn using Cython",,2,scikit_learn,2020-10-13
6k5uvu,Build my first CART based algorithm feedback is welcome!,hey guys! i just made this: https://github.com/lucas-aragno/pokemon-classifier im pretty new to scikit so I'll appreciate any kind of feedback :),2,scikit_learn,2020-10-13
6ev2y7,FastICA,"It seems like all of the examples using fastICA involves taking 2 frequencies, mixing them a certain way, then unmixing them.

What about if I have a wav file. How can I use fastICA to break it down into multiple parts?

Any help would be appreciated. Thank you!",1,scikit_learn,2020-10-13
6eu5cr,Automate your Machine Learning in Python – TPOT and Genetic Algorithms,,2,scikit_learn,2020-10-13
69huhq,[P] Tracking and reproducibility in data projects (CLI tool),,1,scikit_learn,2020-10-13
5xk0iy,Scikit learn vs Open Cv for small problems in image processing,I am a Image processing noob. I've used Numpy and Scipy for some matrix related stuff before and OpenCV for some image processing problems. I recently learned that scipy lets me manipulate images too. What are the pros and cons of using OpenCv and Scipy I am not able to figure out which would be better for me. Appreciate your help!,6,scikit_learn,2020-10-13
5rb5ww,Using Category Encoders library in Scikit-learn,,1,scikit_learn,2020-10-13
5m0352,MLPClassifier: Multiple output activation,"I'm using MLPClassifier but some of the outputs have more than one activation, i.e. [0 1 1 0].
How can I get only one activation?

My code is:
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                    hidden_layer_sizes=(15,), random_state=1, activation='relu')

Thank you!",1,scikit_learn,2020-10-13
5fasve,Need help on scikit kfold validation,"Objective: To create 5 folds of training and test dataset using StratifiedKFold method. I have referred the documentation at http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.cross_validation.StratifiedKFold.html

I am able to print the indices alright but am unable to generate the actual folds. Here follows my code

from sklearn.cross_validation import StratifiedKFold
import pandas as pd
df=pd.read_csv('C:\Comb_features_to_be_used.txt')

##Getting only numeric columns
p_input=df._get_numeric_data()
## Considering all the features except labels
p_input_features = p_input.drop('labels',axis=1)
## Considering only labels [single column]
p_input_label = p_input['labels']
skf = StratifiedKFold(p_input_label, n_folds=5, shuffle=True)
i={1,2,3,4,5}
for i,(train_index, test_index) in enumerate(skf):
    ##print(""TRAIN:"", train_index, ""TEST:"", test_index)
    p_input_features_train = p_input_features[train_index] 
    p_input_features_test =  p_input_features[test_index]
        
I am getting the error: IndexError: indices are out-of-bounds

",2,scikit_learn,2020-10-13
54exlw,scikit-learn doc translation,"translate sklearn doc to chinese
feel free to join us
https://github.com/lzjqsdd/scikit-learn-doc-cn",2,scikit_learn,2020-10-13
52b7od,Improving the Interpretation of Topic Models,,1,scikit_learn,2020-10-13
50qfgg,Topic Modeling with Scikit Learn,,3,scikit_learn,2020-10-13
4rjkcq,Overfit Random Forest,"I have data where Random Forest models overfit to noise whatever 
hyperparameter I put.
(= excellent accuracy on training, but poor accuracy on prediction).


So, this is the process I did to over-come:
    1) Tweak the input data and reduce the sampling of noise (negative example)

    2) Fit the RF and test (confusion matrix) on cross-validation data. 

    3) Repeat it and choose the best cross validation data.

Is there a way to overcome this monte carlo approach,
using OOBag process during training ?

Also incorporate Cross validation to reduce the over-fitting ?

Importance features change every time a new RF is fit (it seems a lot of co-linearity and noise into the data).










",1,scikit_learn,2020-10-13
4h6ypj,Building scikit-learn transformers,,3,scikit_learn,2020-10-13
3zvwqw,Hello everyone! I want to write an oversampling module in compliance with scikit-learn. Advice needed!,"As mentioned in the title I want to write a module for oversampling classes in skewed datasets. I recently came to need such a module and I noticed that no such thing exists officialy in scikit-learn. I want it to be compatible with scikit-learn as I very often use it. Do you have any resources to redirect me to, apart from the official scikit-learn developer guidelines? Any tips for writing a python module in general?

Thanks in advance!",2,scikit_learn,2020-10-13
2f830i,Official Scikit-Learn page.,,1,scikit_learn,2020-10-13
j9q6bp,A scikit-learn compatible library to construct and benchmark rule-based systems that are designed by humans,,6,scikit_learn,2020-10-13
j93854,Best performance on Scikit-learn’s load_digits dataset,"On Scikit-learn’s load_digits dataset:
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html

Does anyone know what is the best performance achieved so far on this dataset? 

I tried googling around but can’t find examples with 100% score performance. I am thinking since this is a standard dataset, it would be easy to get a 100% score performance?",1,scikit_learn,2020-10-13
j8vepi,"could someone ELi5 the hyperparameters (penalty, C, tol, max_inter)",I am currently working on a beginner project on logistic regression using scikit\_learn. I am trying to fine tune my regression model but cant seem to find any websites that can explain what the parameters mentioned in the title mean exactly and how to use them. I was wondering if anyone could give me a quick explanation on what/how to use these parameters to fine tune my regression model.,0,scikit_learn,2020-10-13
j7jz85,SVC rbf kernel seems to be nonstandard?,"I am currently testing a precomputed version of rbf I implemented to get a better feel for how it works and possibly later check out some other kernels.

It seems that whatever I do, I get different results using my precomputed gram matrix vs using the scikit rbf kernel:

To calculate a kernel entry for datapoints xm &amp; xn, based on some extra parameters theta

`k = thetas[0] * np.exp(-(thetas[1]/2.) * (np.sqrt((xn-xm).T @ (xn-xm)))) + thetas[2] + thetas[3] * (xn.T @ xm)`

using theta = \[1,2,0,0\]

This should recover the formulation given [here](https://scikit-learn.org/stable/modules/metrics.html#rbf-kernel) (setting gamma=1)

1 \* exp( - 2/2|xn-xm|^(2) )   


is there something I'm missing? \[here's the code if you wanna take a look\]([https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit\_test.py](https://github.com/rlhjansen/test-kernel-stuff/blob/main/scikit_test.py)) (only dependencies are matplotlib, scikit &amp; numpy, so you're probably good if you're on this sub)",1,scikit_learn,2020-10-13
j5pcm6,ImportError,"I've got a long error which ends with:  
ImportError: DLL load failed while importing \_arpack: Não foi possível encontrar o procedimento especificado. (rough translation: Unable to find the specified procedure)  


Any idea on the issue? It seems like I have some sort of update issue, but I'm unable to find what.",2,scikit_learn,2020-10-13
j2v11w,"Scikit-learn. In the case of a single point, k-nearest neighbours predictions doesn’t literally match with the literally nearest point. I think I know why. Correct me if I’m wrong.","Hello.  I’ve looked at the source code. 

Case population sizes in the range 10 ^ 2 to 10 ^ 5 ish. Vanilla, straight out the box knn from scikit-learn.  Except 1 nearest neighbours not the default 5.  

When I try to predict the nearest neighbour of a point, using 1 nearest neighbours. after using knn.fit to make a model, it doesn’t _always_ return the same value of the actual nearest neighbour.  I’ve worked out the actual real nearest neighbour myself to check, using trig, and unit tested it.  

I think that’s because for pragmatic reasons knn is just a probabilistic model applied at group level.  Not exactly the actual knn for each and every point.  

Am I right?

EDIT:  My. Trig. Was.  Wrong.  Due. To.  A. Data frame. Handling.  Issue.  Ggaaahhhh.",6,scikit_learn,2020-10-13
j1idtx,RadomizedSearch CV taking forever,"Hi ,

I have the below snippet.

Trying to run on GCP . its getting stuck and not even updating.

&amp;#x200B;

https://preview.redd.it/bp2zfi71sxp51.png?width=1463&amp;format=png&amp;auto=webp&amp;s=6d97d1ff6083f6eb65e62d983770c69f06d45f4c",2,scikit_learn,2020-10-13
iv8jv9,Neuraxle - a Sklearn-Based Clean Machine Learning Framework,,1,scikit_learn,2020-10-13
it82un,How the 'init' parameter of GradientBoostingRegressor works?,"i'm trying to create an ensemble of an determined regressor, with this in mind i've searched for some way to use the sklearn already existing ensemble methods, and try to change the base estimator of the ensemble. the bagging documentation is clear because it says that you can change the base estimator by passing your regressor as parameter to ""base_estimator"", but with GradientBoosting you can pass a regressor in the ""init"" parameter. my question is: passing my regressor in the init parameter of the GradientBoosting, will make it use the regressor i've specified as base estimator instead of trees? the documentation says that the init value must be ""An estimator object that is used to compute the initial predictions"", so i dont know if the estimator i'll pass in init will be the one used in fact as the weak learner to be enhanced by the bosting method, or it will just be used at the beginning and after that all the work is done by decision trees. If someone can help me with this question i would be grateful.",4,scikit_learn,2020-10-13
igtkry,Best way to get T-Stastic and P-value etc?,"I'm using scikit learn for linear regression.  Is there a way to use that library to generate things like T-Stastic and p-value and standard error etc?

On stack overflow i found this, but wondering if there's a way within scikit

    import statsmodels.api as sm
    from scipy import stats
    X2 = sm.add_constant(X)
    est = sm.OLS(y, X2)
    est2 = est.fit()
    print(est2.summary())

&amp;#x200B;",1,scikit_learn,2020-10-13
i4ulg2,"Data Visualization using ""Python"" with ""Seaborn"" | Part- I",https://youtu.be/X400eIcV-So,3,scikit_learn,2020-10-13
i3546z,Recommendation based on other user following,"Hello,

I try to build a recommendation system.

My service allow users to follow people (not rate them, just follow) and I would like to be able to propose to users to follow people based on other user’s database activity.

Is scikit a good path for this ? 

Do you recommend specific method or useful ressource to read to achieve this ?

For your help guys!",2,scikit_learn,2020-10-13
hzw282,How to use TensorFlow Object detection API to detect objects in live feed of webcam in real-time,,1,scikit_learn,2020-10-13
hwmcf1,sklearn CCA - how to get variance explained for first canonical relationship?,"Hi. I'm exploring multivariate brain-behaviour relationships with sklearn's canonical correlation analysis tool ([https://scikit-learn.org/stable/modules/generated/sklearn.cross\_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#examples-using-sklearn-cross-decomposition-cca)). I am interested mostly in the first canonical relationship between the two datasets. The decomposition is working fine and i have the weights/canonical scores etcetera - but what i'd really like to know is how much of the variance in either dataset is explained by that one relationship (analogous to eg variance explained by first principal component).

There is a method named 'score' that i can call on the CCA object but I am not quite sure this is what I need. This score is not the same as 'canonical scores above but will supposedly get some coefficient of determination r\^2 between 'observed' and 'predicted' - not sure how to understand this. The description on the webpage is quite terse and it does not behave the way i might expect.

I'm hoping to find someone who might know whether that 'score' method  will get me to what i want - and if so, maybe how to use it. Or point me otherwise in the right direction to get into the variance explained for CCA.

Cheers!",2,scikit_learn,2020-10-13
hu6y83,KMeans Algorithm Question,"Hey all.

I am new with using scikit-learn and had a question regarding the KMeans algorithm functions. After running the algorithm and plotting the clusters, are the clusters with the centroids plotted the final clusters after training is done or is there training that I have to do on the clusters? 

Thanks everyone",1,scikit_learn,2020-10-13
htugnl,"How to handle ""Missing Value"" from ""Dataset"" using ""Pandas"" &amp; ""Sci-Kit Learn""??",https://youtu.be/8IORSsZIyIQ,0,scikit_learn,2020-10-13
htmc59,"How to handle ""Text"" and ""Categorical Attributes"" using Python and Pandas??",https://youtu.be/4sO7Pezlegk,0,scikit_learn,2020-10-13
ht4ol1,Making ROC curves with results from cross_validate?,"I am running 5 fold cross validation with a random forest as such:

from sklearn.ensemble import RandomForestClassifier

from sklearn.model\_selection import cross\_validate

forest = RandomForestClassifier(n\_estimators=100, max\_depth=8, max\_features=6)

cv\_results = cross\_validate(forest, X, y, cv=5, scoring=scoring)

However, I want to plot the ROC curves for the 5 outputs on one graph. The documentation only provides an example to plot the roc curve with cross validation when specifically using StratifiedKFold cross validation (see documentation here: [https://scikit-learn.org/stable/auto\_examples/model\_selection/plot\_roc\_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py))

I tried tweeking the code to make it work for cross\_validate but to no avail.

How do I make a ROC curve with the 5 results from the cross\_validate output being plotted on a single graph?

Thanks in advance",2,scikit_learn,2020-10-13
hm6td7,Best performance on MNIST - Fashion dataset,Does anyone know what is the best performance achieved so far for the MNIST - Fashion dataset along with what model that was used?,1,scikit_learn,2020-10-13
hm22vz,"How to ""Predict"" my friends weight using ""Machine Learning"" and ""Sci-Kit Learn""??",https://youtu.be/A4JwnkTFEXI,2,scikit_learn,2020-10-13
hl4k0u,Factor analysis “model” in CS229,"In one of Stanford’s CS229 lecture by Andrew Ng (https://m.youtube.com/watch?v=tw6cmL5STuY), he talks about a factor analysis “model” in which is to deal with situations where you have a lot more features than samples in your dataset. He even said he used a modified version of this factor analysis “model” in some recent work he did for a manufacturing company in the lecture.

Now my understanding of factor analysis is just a dimension reduction technique. So how did Andrew used factor analysis to build a “model” which deals with datasets which has a lot more features than samples?",2,scikit_learn,2020-10-13
hkm9qn,StackingRegressor Inconsistent Output,"Is it intentional that StackingRegressor returns different accuracy outputs when running multiple times given the same parameters, models and using numpy set seed?",1,scikit_learn,2020-10-13
hjctba,"This lecture that talks about what Multilabel and Multioutput classifications are, along with their implementation using scikit learn.",,1,scikit_learn,2020-10-13
hg5j3s,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,What are some well-known binary classification datasets where neural nets or deep learning fails badly?,2,scikit_learn,2020-10-13
hf60u0,"Hey guys, here is a lecture on how to implement gradient descent with scikit-learn. Enjoy :)",,1,scikit_learn,2020-10-13
hakk07,How do I create a linear regression for this groupedby dataframe?,"I have this assignment for a job interview and I really want to impress by using some machine learning. I don't know too much about it and I essentially don't have much time to learn that much about it. I have the following [dataframe](https://postimg.cc/0zNbyrV8) and I want to create a linear regression using scikitlearn of \['profit'\] vs \['dateReceived'\] for each \['Language'\]. 

Does anyone know what I can do for that to work? I guess it should be just a few lines of code, but I could be wrong?",0,scikit_learn,2020-10-13
h7ay1o,"Visualize Scikit-learn models – ROC, PR curves, confusion matrices etc",,7,scikit_learn,2020-10-13
h172dr,Scikit Learn Tutorial in One Hour,,5,scikit_learn,2020-10-13
h0w3xx,Books about classification algorithms,"Hi all,

I am completely new to data mining and have to write a seminar paper about classification and do some programming in python.

With the help of datacamp I was able to implement the classification algorithms in python.

Now I am looking for some sources that I can cite in my paper that briefly explain these algorithms.

My problem is that most books that I have look into so far are very mathematical and since I don’t have a data mining/computer science background, they are hard to understand.

Do you have any recommendations for some text books that explain classification algorithms such as SVM, Naive Bayes, Trees, etc. that are well recognized, but explain them in an easy way?

Many thanks in advance!",1,scikit_learn,2020-10-13
gziaus,How to choose best pair of random state and class label values?,"For the last few days, I was trying to implement the KMeans algorithm using SciKit Learn, But I came across a very confusing problem. I have a dataset that has two class labels ['ALL', 'AML'] where ALL has 47 and AML has 25 samples and 100 attributes to train from and now I want to use this dataset for KMeans clustering so that I can compare the predicted results with the original class labels. Before asking my question let me explain certain scenarios. In all the scenarios I have taken all the 100 attributes to fit the model.

Scenario 1:

In the first run, I started with a model that is created with pretty much default arguments i.e. model = KMeans(n_clusters=2). For comparing the predicted class labels(which are numeric) with the original labels(which are strings), I set the original class labels as ALL = 1 and AML = 0. After that, while comparing using a classification report I got an average accuracy of 35%. Then I run the algorithm once again and got an accuracy of 44%. For the third try, I got 33% and so on.

However, I looked about it and came to know that the random_state argument needs to have a fixed value to get same accuracy throughout all runs.

Scenario 2:

After knowing about random_state, this time I started with random state 0 and created the model as model = KMeans(n_clusters=2, random_state=0) and kept the original class labels as before i.e ALL as 1 and AML as 0. However, this time the output didn't change on different runs and I got an accuracy of 53%. But, out of curiosity, I swap the original class label i.e. I set ALL as 0 and AML as 1 which results in 47%.

Scenario 3:

This time I choosed random_state as 1 i.e. model = KMeans(n_cluster=2, random_state=1) and having ALL as 0 and AML as 1 gave 67% accuracy while considering ALL as 1 and AML as 0 gave 33% accuracy.

So, My question is what I am doing wrong here? Am I implementing something wrong? If I am right then why the result is changing so much depending on random_state and class labels? What's the solution and how to choose the best pair of random_state and class labels?",1,scikit_learn,2020-10-13
gwgzy9,estimate_transform works when using 'similar' but not when using 'affine',"I have two 512x512 grayscales images (src and dst). To try to understand estimate transform I applied the following transformation

    tform = transform.AffineTransform(scale=(1.3, 1.1), 
                                        rotation=0.5, 
                                        translation=(0, -200)) 

to the src to create the dst. Then I want to find back the parameters using estimate\_transform.

With the parameter 'similar' I obtain parameters very close to the one I used (as expected). But when I want to use 'affine', I obtain the following error :

     matmul: Input operand 1 has a mismatch in its core dimension 0, 
    with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 513 is different from 3) 

Any idea why ? Here is my code :

    src = rgb2gray(data.astronaut())
    dst = rgb2gray(data.astronaut())
    tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5,
                                      translation=(0, -200))
    dst = transform.warp(img1, tform)
    tform_fin = transform.estimate_transform('affine', src, dst)
    dst_corr = transform.warp(img3, tform.inverse)",1,scikit_learn,2020-10-13
gtw4p9,What can I do when I keep exceeding memory used while using Dask-ML,"I am using Dask-ML to run some code which uses quite a bit of RAM memory during training. The training dataset itself is not large but it's during training which uses a fair bit of RAM memory. I keep getting the following error message, even though I have tried using different values for n_jobs:

```
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
```

What can I do?

Ps: I have also tried using Kaggle Kernel (which allows up to 16GB RAM) and this didn't work. So I am trying Dask-ML now. I am also just connected to the Dask cluster using its default parameter values, with the code below:

```
from dask.distributed import Client
import joblib

client = Client()

with joblib.parallel_backend('dask'):
    # My own codes
```",1,scikit_learn,2020-10-13
gsx926,MLPRegressor newby with some (probably very basic) questions in need of some assitance,"Hello!

I'm building MLPRegressor for the first time ever (I've been learning how to code with online courses since end of March) and I know something is wrong but I don't know what. Bellow you can see my code so far. It runs and I have a value for r2 ( -9035355.06 ) and a plot. However the r2 score doesn't make sense (it should be around 0.7)  and the plot doesn't make sense either.

I have run this analysis with SPSS multilayer perceptron feature so I know more or less how my results should be and that's why I know whatever I am doing with python is wrong.

Any advice/suggestion of what I'm doing wrong is very welcome! This coding world is kinda of frustrating for me:/

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    
    from sklearn import neighbors, datasets, preprocessing 
    from sklearn.model_selection import train_test_split
    from sklearn.neural_network import MLPRegressor
    from sklearn.metrics import r2_score
    
    vhdata = pd.read_csv('vhrawdata.csv')
    vhdata.head()
    
    X = vhdata[['PA NH4', 'PH NH4', 'PA K', 'PH K', 'PA NH4 + PA K', 'PH NH4 + PH K', 'PA IS', 'PH IS']]
    y = vhdata['PMI']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 
    
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_test_norm = scaler.fit_transform(X_test)
    
    nnref = MLPRegressor(hidden_layer_sizes = [4], activation = 'logistic', solver = 'sgd', alpha = 0.1, learning_rate= 'constant',
                         learning_rate_init= 0.6, max_iter=200, random_state=0, momentum= 0.3, nesterovs_momentum= False)
    nnref.fit(X_train_norm, y_train)
    
    y_predictions= nnref.predict(X_test_norm)
    
    print('Accuracy of NN classifier on training set (R2 score): {:.3f}'.format(nnref.score(X_train_norm, y_train)))
    print('Accuracy of NN classifier on test set (R2 score): {:.3f}'.format(nnref.score(X_test_norm, y_test)))
    print('Current loss : {:.2f}'.format(nnref.loss_))
    
    plt.figure()
    plt.scatter(y_test,y_predictions, marker = 'o', color='blue')
    plt.xlabel('PMI expected (hrs)')
    plt.ylabel('PMI predicted (hrs)')
    plt.title('Correlation of PMI predicted by MLP regressor and the actual PMI')
    plt.show()",1,scikit_learn,2020-10-13
gsol3k,What are the default values for the parameters in Dask-ML's Client() function,"I am trying to understand Dask-ML's Client() function parameters. Say I have the following code using Dask-ML's Client() function:

```
from dask.distributed import Client
import joblib

client = Client()
```

If I don't specify any values for the parameters in the Client() function, what are the default values for the parameters:

(i) n_workers

(ii) threads_per_worker

(iii) memory_limit

From my understanding, Python has the Global Interpreter Lock (GIL) feature which prevents multi-threading. If so, why does Dask-ML's Client() function have the parameter threads_per_worker when multi-threading is prevented in Python?

Does memory_limit refers to the maximum memory limit allowed for **each** worker/machine/node or does this refer to the maximum memory limit allowed for **all combined** worker/machine/node?

I have already looked through the documentation in Dask-ML (see here: https://docs.dask.org/en/latest/setup/single-distributed.html), but the documentation is not clear in regards to these questions above.

Thank you in advance if anyone could explain this?",1,scikit_learn,2020-10-13
glb4cy,Why does PolynomialFeatures has multiple pair of coefficient after fitted the data?,"After I create an PolynomialFeatures object, and fit the data by :

[`poly.fit`](https://poly.fit)`(x,)`

I wanted to look for the coefficient, so I do:

`poly.transform(x,y)`

&amp;#x200B;

And it will return an array with (n\_samples, n\_coeff), but why does the polynomial fit with multiple pair of coefficient? Wouldn't the model fit the data and get a final best coefficient?

&amp;#x200B;

And what is the final coefficient that Polynomial get after fitting?",1,scikit_learn,2020-10-13
gi4jw3,How to add sample_weight into a scikit-learn estimator,"I have recently developed a scikit-learn estimator (a classifier) and I am now wanting to add sample_weight to the estimator. The reason is so I could apply boosting (ie. Adaboost) to the estimator (as Adaboost requires sample_weight to be present in the estimator).

I had a look at a few different scikit-learn estimators such as linear regression, logistic regression and SVM, but they all seem to have a different way of adding sample_weight into their estimators and it's not very clear to me:

Linear regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_base.py#L375

Logistic regression: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/linear_model/_logistic.py#L1459

SVM: https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/svm/_base.py#L796

So I am confused now and wanting to know how do I add sample_weight into my estimator? Is there a standard way of doing this in scikit-learn or it just depends on the estimator? Any templates or any examples would really be appreciated. Many thanks in advance.",2,scikit_learn,2020-10-13
get2kh,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,,2,scikit_learn,2020-10-13
gdb7a8,Code a Decision Tree in 20 lines.,,0,scikit_learn,2020-10-13
gd81x4,why does Scikit Learn's Power Transform always transform the data to zero standard deviation?,"all of my input features are positive. Whenever I tried to apply PowerTransformer with box-cox method, the lambdas are s.t. the transformed values have zero variance. i.e. the features become constants

&amp;#x200B;

I even tried with randomly generated log normal data and it still transform the data into zero variance.

&amp;#x200B;

I do understand that mathematically, finding the lambda s.t. the standard deviation is the smallest, would mean the distribution would be the most normal-like.

&amp;#x200B;

But when the standard deviation is zero, then what's the point of using it?

&amp;#x200B;

&amp;#x200B;

p.s. so one of the values of lambda I get by using PowerTranformer is -4.78 

If you apply it into the box-cox equation for lambda != 0.0, then for any input feature y values, you technically get the same values. i.e. (100\^(-4.78)-1.0)/(-4.78) is technically equals to (500\^(-4.78)-1.0)/(-4.78)",2,scikit_learn,2020-10-13
gcvtsm,how to combine recursive feature elimination and grid/random search inside one CV loop?,"I've seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:

[Feature selection and cross-validation](https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751)

[Nested cross-validation and feature selection: when to perform the feature selection?](https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec)

[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)

&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...

[Here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.

However I'd like to do recursive feature elimination inside random/grid CV, so that ""feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)"", so that data from other folds influence neither feature selection nor hyperparameter optimization.

Is this possible natively with sklearn methods and/or pipelines? Basically, I'm trying to find an sklearn native way to do this before I go code it from scratch.",3,scikit_learn,2020-10-13
gc2z28,How to write a scikit-learn estimator in PyTorch,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",3,scikit_learn,2020-10-13
g5ugws,Code a Neural Network in 20 lines.,,2,scikit_learn,2020-10-13
g4yc73,Basic question re: gaussian mixture models,"I wasn't able to find this in the documentation, but is the covariance parameter you access with model.covariances\_ sigma or sigma\^2? Seems like it can be either thing as I've seen the notations N(x| mu, sigma\^2) and N(x|mu, sigma) both used in various places.",1,scikit_learn,2020-10-13
fzm7mm,"Should scikit-learn include an ""Estimated Time to Arrival"" (ETA) feature? Discuss.",,7,scikit_learn,2020-10-13
fx6kdy,Clustering of t-SNE,"Hello,

I have recently tried out t-SNE on the sklearn.datasets.load_digits dataset. Then i applied KNeighborClassifier to it via a GridSearchCV with cv=5.

In the test set (20% of the overall dataset) i get a accuracy of 99%

I dont think i overfitted or smth. t-SNE delivers awesome clusters. Is it common to use them both for classifying? Because the results are really great. I will try to perform it on more data. 

I am just curious on what you (probably much more experienced users than me) think.",1,scikit_learn,2020-10-13
fx0i7x,Search over preprocessing and ensemble hyperparameters?,"In scikit-learn there are some handy tools like `GridSearchCV` for tuning the hyperparameters to a model or pipeline.

Suppose you'd like the preprocessing in your pipeline to include some user-defined options (e.g. whether to encode a certain categorical variable via one-hot encoding or something weird like frequency encoding) and you'd like to include those options among the hyperparameters you're searching over.

Suppose further that you're using an ensemble model -- e.g. a random forest plus few linear regression specifications, and you'd like to tune the hyperparameters for each of them, as well as the voting weight of each.

Does scikit-learn provide a predefined way to search over such spaces? It looks like the parameter space is intended only to dictate the behavior of a single model, not preprocessing steps or ensemble parameters.",1,scikit_learn,2020-10-13
ft2pcp,"How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?","How to setup DBSCAN so that it doesn't classify all points? Or it leaves some as ""unclassified""?",1,scikit_learn,2020-10-13
ft1kmb,facing an error,"import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

&amp;#x200B;

\# Importing the dataset

dataset = pd.read\_csv('50\_Startups.csv')

X = dataset.iloc\[:, :-1\].values

y = dataset.iloc\[:, 4\].values

X2=dataset.iloc\[:, 3\].values

\# Encoding categorical data

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

le = LabelEncoder()

X2 = le.fit\_transform(X2)

oh = OneHotEncoder(categories = 'X\[:, 3\]')

X= oh.fit\_transform(X).toarray()

&amp;#x200B;

https://preview.redd.it/7bgiwdp238q41.png?width=871&amp;format=png&amp;auto=webp&amp;s=d2386c5cda100706a85803c036586beec8b9e843",1,scikit_learn,2020-10-13
fm1oov,I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?,"I am using SimpleImputer in a columntransformer + pipeline and I continue to receive message that my input contains NaN. What am I doing wrong?

    preprocess =     make_column_transformer((SimpleImputer(strategy='median'), cols_numeric),     
    (SimpleImputer(strategy='constant', fill_value='missing'), cols_onehot),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_target),      (SimpleImputer(strategy='constant', fill_value='missing'), cols_ordinal),     (OneHotEncoder(handle_unknown='ignore'), cols_onehot),     
    (TargetEncoder(), cols_target),     
    (OrdinalEncoder(), cols_ordinal),     
    (StandardScaler(), cols_numeric)) 
    lr_wpipe = make_pipeline(preprocess, LinearRegression()) 
    lr_scores = cross_val_score(lr_wpipe, X_train, y_train) 
    np.mean(lr_scores) 
    print(""Linear Regression R^2: "", lr_scores)",1,scikit_learn,2020-10-13
flg6a1,how to find 'the math' being done in sklearn source code?,"hi.  I'm trying to find where in sklearn the actual math is being done, mostly for my own learning so I can answer questions like 'when using `sklearn.neighbors` , what math is being used to calculate Euclidean distance?'    


If you see here: [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/\_base.py#L360](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/_base.py#L360)  
you'll see that Euclidian and other distance calculations can be *specified*, but I don't see anywhere where the actual math is being done in code.",1,scikit_learn,2020-10-13
fl4fj1,Adding Standard Scaler to GridSearchCV,"I'm looking to use the Standard Scaler as a hyper parameter, i.e check if performance is higher with/without scaling the inputs. In order to tune with other hyperparameters, I would like to incorporate it into my GridSearchCV function (provided by Scikit Learn). Can someone advise me on how to do it?",1,scikit_learn,2020-10-13
ffx9zw,How to use tfidfvectorizer fit_transform for multiple docs,"Hey, 

Let's say my corpus is a list of lists , each of the inner lists represent a parsed doc (each value is a word) 

I want to compute a tf-idf score for my corpus. 

It's seems like the fit-transform function can't use my corpus as its inputs should be itratable with string values (which is each of my docs) 


    V = tfidfvectorizer ()
    For doc in corpus:
       Vectors = v.fit_trabsform(doc)

So my question is, how does it calculate IDF if it get only one doc at a time?",1,scikit_learn,2020-10-13
ff9602,Classifiers' score method clarification,"Hi,

I don't fully understand what the score method of classifiers does. For example, the [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method's documentation says ""Return the mean accuracy on the given test data and labels."" Now, I know what is accuracy: (TP+TN)/(TP+TN+FP+FN), but I don't understand why ""mean"" is in there. Mean over what? of what?  

That is, I give the method as parameters a dataset with true labels, and it can calculate the accuracy from that (given the model), but where does the mean come into place? 

Thanks in advance!",3,scikit_learn,2020-10-13
fbfky4,Is epsilon in dbscan a euclidean measure?,"Hello everyone,
I'm writing yet another dbscan question. For those who are familiar with the inputs to the dbscan, the principal parameters are epsilon and minPts.

Epsilon is the neighborhood radius, and I'm curious if anyone can point me to a reference or tell me if epsilon is a euclidean metric",3,scikit_learn,2020-10-13
f1dizs,Identifying smallest frequently occurring value,"I'm not a data science person, but thinking Scikit learn might be able to help here, and looking for suggestions for ideas I should investigate.

Essentially, I'm looking for a way to consistently identify a baseline power readings. If I have minute by minute power consumption readings from a bunch of electrical motors.  For any motor, we want to identify what a 'baseline' or 'normal unloaded steady-state' power value is.

There is definitely noise in the signal, and not even noise - legitimate power reading that are smaller than what we would consider 'normal unloaded steady state'.  The catch is this could be different for the same motor when production composition changes, so there is not just one value that we can look at historical data to arrive at. (Think motors running pumps moving different fluid mixtures / slurry ad different times.

This does not have to be real-time, just take the dataset of power readings for any motor for any production batch and post-process the data in such a way we can identify times the motor is doing its job at a 'near-idle' state.

Currently we just have a basic calculation that looks at a rolling window of 20 per-minute readings and finds the lowest value that occurs at least twice. (basically throwing out the lowest few outliers)

The reason I'm considering Scikit or similar is we can graph these power readings for a time period (say 1 day) and visually we can easily see these 'baselines' we are looking for.  There will be spikes and dips, and time windows where we are definitely running a heavy load (motors spun up on demand), but we can identify when the mixture changes because the visual changes in this baseline value.

Hope that made at least a little sense, if there are details I can clarify, please ask.  I appreciate everyone's thoughts and ideas!",1,scikit_learn,2020-10-13
eylu4p,What's wrong with Scikit-Learn.,,3,scikit_learn,2020-10-13
ev1as7,Is it possible to use a custom-defined decision tree classifier in Scikit-learn?,"I have a predefined decision tree, which I built from knowledge-based splits, that I want to use to make predictions. I could try to implement a decision tree classifier from scratch, but then I would not be able to use build in Scikit functions like predict. Is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? Or do I need to do something completely different? My first attempt was to use “fake training data” to force the algorithm to build the tree the way I like it, this would end up in a lot of work because I need to create different trees depending on the user input.",1,scikit_learn,2020-10-13
eu9qgc,Is HistGradientBoosting the same as LightGBM or is the SKLearn's version different?,"If so, how?",2,scikit_learn,2020-10-13
em9fxf,Is this the proper way to do ML with scikit_learn?,"I have a dataset with 8 features (numeric) and 1 target (0 or 1).  
 I'm using, DecisionTreeClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, SGD, testing all parameters for K etc.  
 For each for I save the predicted target and at the end of the process I just sum how many times he prompt 0 and 1 to get somehow the probability of both results.

But sometimes I get these errors:  
 The predicted array is always the same for LogisticRegression and SGD, like 1 1 1 1 1 1 1 1 1 or 0 0 0 0 0 0 0 0.

MLPClassifier says: ConvergenceWarning Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. Warning. But only after a few runs.

What's the proper way to predict binary values?  
 I read that this is called the No Free Lunch problem and we should brute force test all parameters and methods to get the best model and avoid using bad ones. Am I right?

Thanks for your support. I'm a beginner.",2,scikit_learn,2020-10-13
effb98,What module/algorithm should I use in order to predict the time in which a certain action will be completed?,"Basically the title, but to explain it even more :

I have to device a model which will predict the total time a patient will have to wait in a hospital environment. For that, we have a dataset consisting of various patients with several diseases and their time durations already recorded. I want to know which module or algorithm should I use to carry this out? This is my first ML project and I could use any help that you guys can do. Thank you!!",2,scikit_learn,2020-10-13
e0h8ao,Column transformer throwing away some features?,"My data frame  has numerical columns a, b, c. And it has categorial text features d, e, f, g, h.

I build a preprocessor like the following.

    num_features = ['a','b','c']
    nom_features = ['d','e','f','g','h']
    
    preprocesser = ColumnTransformer([
        (""scale_numeric"", StandardScaler, num_features),
        (""encode_nominal"", OneHotEncoder(handle_unknown=""ignore""), nom_features)],
        remainder=""drop""
    )
    
    preprocessor.fit_transform(dataframe)

Since I started with 8 features, after one hot encoding I expected to get 8 or more features back, but the result of `preprocessor.fit_transform(dataframe)` only has 3 columns. Not sure what I am doing wrong if anyone can help me.",1,scikit_learn,2020-10-13
dyzwn9,How to Modify(Make unique) the Scikit-learn Multilayer perseptron algorithm (MLP),"Hi folks,

I've been trying to build a rainfall prediction model for last few days. I've used the **Scikit-learn Multilayer perseptron regressor function** straight up. 

1) The accuracy was OK(78%) but I want to increase it

2) I don't want to use the same predominantly given function (I just want to **add uniqueness in my code**, but I want to use scikit-learn)

Is there any way to modify the function or not use the ready-made function? Can anyone please help me with this?

Thanks in advance!",1,scikit_learn,2020-10-13
dtnh3f,The best alpha for ridge regression is... -85???,,3,scikit_learn,2020-10-13
dtiy98,difference between Kfold.split() and shufflesplit.split() in scikitlearn,"I read this [post](https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv), I get the difference when it comes to computation and shufflesplit randomly sampling the dataset when it creates the testing and training subsets, but in the answer on stackoverflow, there is this paragraph  


""**Difference when doing validation**

In KFold, during each round you will use one fold as the test set and *all* the remaining folds as your training set. However, in ShuffleSplit, during each round **n**  you should *only* use the training and test set from iteration **n** ""

I couldn't quite get it. since in kfold, you're bounded by using the training buckets (k-1) and testing bucket (k) in the **k** iteration and in shufflesplit you use the training and testing subsets made by the shufflesplit object in iteration **n.**  so for me it feels like he's saying the same thing.

can anyone please point out the difference for me?",1,scikit_learn,2020-10-13
dcvd2r,When to use these unsupervised algorithms?,"There are a lot of modules in sklearn. I am interested when these unsupervised algorithmes (bellow )are used.  
When to use a Gaussian mixture model? When to use Manifold Learning, When to Biclustering? etc.

&amp;#x200B;

* [2.1. Gaussian mixture models](https://scikit-learn.org/stable/modules/mixture.html) 
* [2.2. Manifold learning](https://scikit-learn.org/stable/modules/manifold.html)
* [2.4. Biclustering](https://scikit-learn.org/stable/modules/biclustering.html) 
* [2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html) 
* [2.6. Covariance estimation](https://scikit-learn.org/stable/modules/covariance.html) 
* [2.7. Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html) 
* [2.8. Density Estimation](https://scikit-learn.org/stable/modules/density.html)",8,scikit_learn,2020-10-13
dar9z6,pattern recognition on texts that are bash commands or software signature?,"hi all.

so I've got my hands on a daily dose of 100,000 connections per day to our servers, and I've got millions of rows of data that includes commands our users have executed on our servers, (\`cd\`, \`ehlo\`, \`scp ....\`, etc). and I have the same amount of data of their application signatures while connecting. like (Firefox 59, Firefox 60, google chrome),... and user agents, ...

basically all the data one can extract out of a socket or using an IDS.

I like to do some pattern matching on these data. like for the commands they are executing and stuff like that...

so to cluster the commands, I've got commands that look like this:

cd Project

cd Images/personal

cd Project/map

cat /var/log/nginx/web\_ui.log

the problem is, I can just split the texts and take in the first part(cd, cat) and make plot out of the commands, but i really would like to make it more automatic and intelligent. so people who \`cd\` into the \`Project/map\` are distinguished from people who cd into \`Images\` folder. I like to know what people are doing on out servers. so a plot that all people whith \`cd\` commands are close to each other, but are really distinguished for each folder that they have \`cd\` into. 

this is just an example of what I want:)

&amp;#x200B;

turns out that scikit\_learn only works on numbers? how can i utilize it for that kind of data? I don't know if this is a nltk problem?",3,scikit_learn,2020-10-13
d8sxus,Exporting Models to build own inference server,"Hello, I was hoping to get pointed in the right direction.  After training a random forest classifier I am looking to export the model in such a way that I can recreate each of the trees in C++.  I am trying to figure out the best approach to this, or if it is even possible.  My research online mainly shows examples of how to visually represent these, and how to create a pickle project for python serialization.  

Am I missing some key terms in my search? Could you point me to what i should be doing to figure this out?

&amp;#x200B;

My approach so far has been exploring the clf.estimators.trees\_ part of the estimators object, but I am not sure if I am on the right track.

&amp;#x200B;

Any help is much appreciated.

&amp;#x200B;

Thanks!",1,scikit_learn,2020-10-13
d244x4,Predict device from flow,"Hey guys, I applied to a competition about AI and my task is to predict device class from flow. I have 13 types of classes which are all in train set but the test set is missing that one column. After I run training and then I try to predict it, I receive an error stating this: ValueError: query data dimension must match training data dimension.

How can I predict a column that is not there? I don't believe that I have to manually put the column to the test.json 

Thanks for advices.",4,scikit_learn,2020-10-13
cvtjk6,Predicting Churn With Nested Data,"Hello All!

Ok, so this is a bit of a challenge and I'm trying to figure out if it is even worth worrying about the nesting aspect of the data. Basically, I'm trying to predict subscription-level churn with a combination of subscription-level and user-level variables.

Since users own subscriptions I figured I should try to account for nesting in my model. Does anyone have any recommendations on how to attack churn predictions using a nested model? Any suggestions would be greatly appreciated. Again, I have code working, but I've never built anything that requires nested analysis.

Basically my question is: Is it possible to run a multi-level SVM?",2,scikit_learn,2020-10-13
cs2urp,What is the most efficient way to implement two-hot encoding using scikit learn?,"I have two very similar features in my dataframe, and I would like to combine their one-hot encoded versions. They are both categorical data, and they both contain the same categories. I was thinking about using OneHotEncoder from scikit learn and getting the union of the corresponding columns. Is there a function or more efficient way that I do not know about?",3,scikit_learn,2020-10-13
cnnacy,Feature elimination doesn't really eliminate anything.,"I had a fairly simple dataset, after plotting the correlation matrix I noticed that one variable has very low correlation with the target (0.04) but instead of deleting it manually I decided to try feature elimination.
I tried both RFE and RFECV with Logistic Regression as an estimator, RFE eliminated some features which seemed correlated with the output and kept that feature.
RFECV didn't eliminate anything at all.

Am I missing something here?",1,scikit_learn,2020-10-13
cnl2a5,k-means output issue,"Hello I've run a k-means over my voice data. I got two class (for best). My problem is why i got this line at the right side? I sit an issue in my dataset?

https://preview.redd.it/n5lz9pggx7f31.png?width=852&amp;format=png&amp;auto=webp&amp;s=e84cc13f9579c026fc6bfa5cbd85849b1ea2939e",1,scikit_learn,2020-10-13
cmmbi5,Running scikit validation on 24 cores is slow?,"Hello guys, maybe anyone can help me out here. I am running following validation code:

```
from sklearn.linear_model import LinearRegression
model = LinearRegression()
from sklearn.preprocessing import PolynomialFeatures
poly_transformer = PolynomialFeatures(degree=2, include_bias=False)
from sklearn.pipeline import Pipeline
pipeline = Pipeline([('poly', poly_transformer), ('reg', model)])
train_scores, valid_scores = validation_curve(estimator=pipeline, # estimator (pipeline) X=features, # features matrix y=target, # target vector param_name='pca__n_components', param_range=range(1,50), # test these k-values cv=5, # 5-fold cross-validation scoring='neg_mean_absolute_error') # use negative validation
```

in the same .py file on different machines, which I would name #1 localhost, #2 staging, #3 live, #4 live. localhost and staging have both i7 cpus, localhost needs around 40s for the validation, staging needs around 13-14 seconds
live (#3) and live (#4) need almost 10 minutes for executing the validation - both of these servers have intel cpus with 48 threads.
In order to get more ""trustworthy"" numbers I dockerized the images and run them on the servers. Anyone has an idea why the speed is so different?",1,scikit_learn,2020-10-13
clz2bl,vectorization,"Hi, I just want to know if I can vectorize a text even if its on another language using Count Vectorization",2,scikit_learn,2020-10-13
clpubv,Machine learning final year project,"design and implement an intelligent agent that can detect a fault and can trouble a faulty server on a network

Its a network anormaly project 
But dont know where to start from",1,scikit_learn,2020-10-13
cl88rf,No Scikit-learn after I installed Anaconda in Sublime Text 3," 

I started using Sublime Text as my Text Editor/IDE (not sure what the difference is) to do some Python projects. After watching 2 episodes of the machine learning course by Google Developers. I installed the Anaconda package which has the Scikit-learn included.

Following the video I typed:

    import sklearn

This error appeared:

    ModuleNotFoundError: No module named 'sklearn'

Is there a way to install the Scikit-learn using the Sublime Text 3 or using a different method?",1,scikit_learn,2020-10-13
cgijm0,Unable to find/import,"edit: Title - Unable to find/import IterativeImputer

&amp;#x200B;

&amp;#x200B;

Hello fellow users, I'm wondering if yall could help me out with importing/finding IterativeImputer...

**&gt;&gt;&gt;** *# explicitly require this experimental feature*

**&gt;&gt;&gt; from** **sklearn.experimental** **import** enable\_iterative\_imputer *# noqa*

**&gt;&gt;&gt;** *# now you can import normally from impute*

**&gt;&gt;&gt; from** **sklearn.impute** **import** IterativeImputer

**ModuleNotFoundError**: No module named 'sklearn.impute.\_iterative'; 'sklearn.impute' is not a package

&amp;#x200B;

$pip freeze states I have scikit-learn==0.21.2 and sklearn==0.0

Python version 3.6

&amp;#x200B;

After researching the issue online I see that there's an experimental version I need to install, but I can't seem to find it! Further, I can't find it on their website.. [https://scikit-learn.org/dev/versions.html](https://scikit-learn.org/dev/versions.html)

What did I overlook/miss?",1,scikit_learn,2020-10-13
cc2mxr,How to re-structure a numpy dataframe into a format I can use in sklearn?,"Assuming the dataframe column 0 is the target and columns 1: are the features, and that each column is named, what's the easiest way to split the data for use in sklearn?",1,scikit_learn,2020-10-13
cbm4g6,How to classify dots,"Hello, 

I have a graph with two groups, red and blue dots. These groups are clearly separated, but the problem is that I want to say if a new dot belongs to the red group, to the blue, or to none of them.

What method do you recommend?

Thank you",1,scikit_learn,2020-10-13
c4rlf7,Regression is not yielding many useful predictions,"Hello all,   


I'm using a linear regression to predict continuous values (how long until a client churns measured in months). I have a dataset of cancelled accounts and active account. I'm using the cancelled accounts to predict the active accounts. I have a variety of explanatory variables and in total I have an R-squared of around 35% (obviously R-squared isn't perfect)   


Overall, this works pretty well; however, one issue I'm running into is that, of the predictions I get back, some are negative and very few actually predict that these active clients should still be active. In other words, many of the predicted cancellation dates are in the past.  


Dumb question, but is there a method I could be using to help this? Overall, I'm getting about 10k useful observations from 60k predictions.  Any suggestions would be greatly appreciated.",1,scikit_learn,2020-10-13
c4q5i6,I can't import Kmeans into compiler,"I'm currently using sklearn 0.21.2, and when I do:  


`import sklearn.cluster.KMeans`

&amp;#x200B;

the compiler returns error:

&amp;#x200B;

`no module named sklearn.cluster.KMeans`

&amp;#x200B;

I've found that in the cluster package, there is an module named 'cluster.k\_means\_'  


But when I tried to use this instead, it shows error

&amp;#x200B;

`Module is not callable`

&amp;#x200B;

Now I don't know why I can't import the kmeans package in cluster.",1,scikit_learn,2020-10-13
bylpjd,Sklearn regression with two datasets,"Hello all,   


basically, as the title implies I'm trying to train a regression model on one dataset and the apply that predictive model to another dataset. In other words, I have a model which predicts cancelled accounts and the amount of time in which those accounts cancel.   


I have another dataset full of active accounts (with the same variables) and I'm attempting to use the model from the cancelled accounts to predict when my active accounts will cancel. I'm having trouble with this.  Is there a way to do this without forcing a t  


Is there a way to use the ""active dataset"" without enforcing a Train\_test\_split? Any help would be greatly appreciated. Thank you!",2,scikit_learn,2020-10-13
bvjxzj,Get the function that fits my data,"I have fit a polynomial regressor to a two dimensional data. 
Is there a way to see the function that fits this data?",2,scikit_learn,2020-10-13
bqtxes,Kmeans clustering cache the result,"Hello,

&amp;#x200B;

I am new to scikit and I was wondering if I could cache the result of Kmeans so next time when I run my script I do not create the centroids again - that means save the result of [`kmeans.fit`](https://kmeans.fit)`()`.",2,scikit_learn,2020-10-13
bp7dv5,Get classes name of each estimator in OneVsOneClassifier,"Are there any ways to do that ? I am trying to directly access the classes\_ attributes in the estimator but it only returning \[0,1\]",2,scikit_learn,2020-10-13
bevq9d,Using Blob Detection methods on huge images,"I'm trying to use common blob detection methods from

[https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob\_dog](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog)

on a huge images (about 6000x6000 pixels). It takes way too long to compute and show the result. How could I resolve this?",1,scikit_learn,2020-10-13
bcwbv4,Calculate variance of accuracy,"Hello, how can I calculate the variance of accuracy between two models in Random forest.
I mean I made a simple model with DecisionTreeClassifier() and one more with BagginClassofier() using the first model on it.
The accuracy climb +0.237.

How to get variance of that accuracy?
Thansk",1,scikit_learn,2020-10-13
bcbduy,Classification: Minimizing the amount of false positives,"Hey there,

I posted an earlier post (now deleted) that phrased this a bit wrong (thanks Imericle). Here is another try: 

Many  (most?) classification algorithm seem to be about maximizing accuracy  (true positives + negatives). My aim is to minimize the amount of false positives. How would I achieve this?

Only options I see to achieve this is through parameters tuning, is that the right approach?

(Thinking on applying it to a RandomForest),

Thanks,

Bb",2,scikit_learn,2020-10-13
bbxi9t,KMeans: Extracting the parameters/rules that fill up the clusters,"Hi all,

&amp;#x200B;

I have created a 4-cluster k-means customer segmentation in scikit learn. The idea is that every month, the business gets an overview of the shifts in size of our customers in each cluster. 

My question is how to make these clusters 'durable'. If I rerun my script with updated data, the 'boundaries' of the clusters may slightly shift, but I want to keep the old clusters (even though they fit the data slightly worse). My guess is that there should be a way to extract the paramaters that decides which case goes to their respective cluster, but I haven't found the solution yet. 

I would appreciate any help",1,scikit_learn,2020-10-13
b6nfvs,Question about FeatureUnion,"    pipe = Pipeline([
            ('features', FeatureUnion([
                    ('feature_one', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_one')),
                        ('vec', cvec) # Count vectorizer
                    ])),
                    ('feature_two', Pipeline([
                        ('selector', DataFrameColumnExtracter('feature_two')),
                        ('vec', tfidf) # Tf-idf vectorizer
                    ]))
                ])),
            ('clf', OneVsRestClassifier(clf)) #clf is a support vector machine
        ])

I'm using this pipeline for a project I'm working on, and I just want to make sure I understand how FeatureUnion works. I'm building a classifier which takes in two different text features and attempts to make a multi-class classification.

&amp;#x200B;

To give a little more detail, I'm trying to classify news articles into one of several categories (sports, business, etc.) Feature one is a list of tokens taken from the article's url, which often, though not always, explicitly states the name of the topic. Feature two is a list of tokens from the body of the article.

&amp;#x200B;

Does it make sense to separate the two features this way? Does this have a different effect than if I had just merged all of the tokens into a single list and vectorized them? My intention was to allow the two features to effect the model to different degrees, since I figured one would be more predictive in most scenarios (and I am getting pretty great results.)",2,scikit_learn,2020-10-13
b36h5a,Ranforest random behaviour,"If I give random forest parameters as RandomForestClassifier(n_estimators=10,bootstrap=False,max_features=None,random_state=2019) Should it be creating 10 same decision trees? But it is not. I am asking the random forest to
     1.Sample without replacement (bootstrap=False) and each tree have same number of sample (ie the total data )(verified using plot)
     2.Select all features in all trees.
But model.estimators_[2] and model.estimators_[5] are different

",2,scikit_learn,2020-10-13
axgj2c,Predicting the runtime of scikit-learn algorithms,"Hey guys,

We're two friend who met in college and learned Python together, we co-created a package which can provide an estimate for the training time of scikit-learn algorithms.

The main function in this package is called “time”. Given a matrix vector X, the estimated vector Y along with the Scikit Learn model of your choice, time will output both the estimated time and its confidence interval. 

Let’s say you wanted to train a kmeans clustering for example, given an input matrix X. Here’s how you would compute the runtime estimate:

    From sklearn.clusters import KMeans
    from scitime import Estimator 
    kmeans = KMeans()
    estimator = Estimator(verbose=3) 
    # Run the estimation
    estimation, lower_bound, upper_bound = estimator.time(kmeans, X)

We are able to predict the runtime to fit by using our own estimator, we call it meta algorithm (meta\_algo), whose weights are stored in a dedicated pickle file in the package metadata.

The meta algos estimate the time to fit using a set of ‘meta’ features, including the parameters of the algo itself (in this case kmeans) and also external parameters such as cpu, memory or number of rows/columns. 

We built these meta algos by generating the data ourselves using a combination of computers and VM hardwares to simulate what the training time would be on the different systems, circling through different values of the parameters of the algo and dataset sizes . 

Check it out! https://github.com/nathan-toubiana/scitime

Any feedback is greatly appreciated.",5,scikit_learn,2020-10-13
aahf76,"Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?","Is there a built-in way for: ""if signal &gt; 0 then ADD, if signal &lt; 0 then MINUS""?

&amp;#x200B;

So in the sense that if one applies e.g. a gain factor  (or a function depicting gain changes), then it's applied to the correct direction.",3,scikit_learn,2020-10-13
a75oid,"classification_report + MLPClassifier(): UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn_for)","classification\_report on a prediction done on MLPClassifier() sometimes throws:

&amp;#x200B;

*UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.'precision', 'predicted', average, warn\_for)*

&amp;#x200B;

but not on all the time.

&amp;#x200B;

What could be wrong?

&amp;#x200B;

\---

&amp;#x200B;

Doing

&amp;#x200B;

set(y\_test) - set(y\_pred) 

&amp;#x200B;

I'm able to see that sometimes some label is missing from y\_pred. But why does this occur only occasionally?

Is something wrong with how I use MLP?",1,scikit_learn,2020-10-13
a753f2,"{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification_report()","{ValueError}Mix type of y not allowed, got types {'continuous', 'multiclass'} from classification\_report()

&amp;#x200B;

Why?

&amp;#x200B;

I call it like:

&amp;#x200B;

    classification_report(y_test, y_pred)

where y\_pred is predicted using a model I built.

&amp;#x200B;

""Quite obviously"" the arguments are incompatible somehow, but how can I find out, how? And how can I make them compatible?

&amp;#x200B;

\---

&amp;#x200B;

I tried:

&amp;#x200B;

    from sklearn.utils.multiclass import type_of_target

&amp;#x200B;

    &gt;&gt;&gt; type_of_target(y_test)
    'multiclass'
    
    &gt;&gt;&gt; type_of_target(y_pred)
    'continuous'

&amp;#x200B;",1,scikit_learn,2020-10-13
a746h0,"Is there any way to ""estimate"" how long a given computation in sklearn will take?","Is there any way to ""estimate"" how long a given computation in sklearn will take?

&amp;#x200B;

So that one doesn't need to wait longer than what one can?

&amp;#x200B;

Also, since Windows Task Manager shows only modest CPU use (&lt; 10%), then how is one supposed to know, what's occurring in the model?",2,scikit_learn,2020-10-13
a73oda,What are the most important parameters in LogisticRegression()?,What are the most important parameters in LogisticRegression()?,3,scikit_learn,2020-10-13
a71xf2,How does one feed hidden_layer_size tuples into GridSearchCV's param_grid?,How does one feed hidden\_layer\_size tuples into GridSearchCV's param\_grid?,1,scikit_learn,2020-10-13
a0yprf,Code review,"Hello,

I'm new to ML and scikit - hope this is the correct place for this. Have created the below code that appears to be working but wanted to get the opinions of people with more experience then me, to check I haven't a made any major errors or if there are any obvious improvements?

&amp;#x200B;

I am trying to train a model on a data set of potentially hundred of thousands emails. Every few days I want to retrain the exported model using incremental learning on the new emails received since the model was last trained.

The below reads the initial data from a csv, runs HashingVectorizer then SGDClassifier. The OnlinePipeline is used to allow me to use partial\_fit when I try to retrain later in the process.

`import pandas as pd`

`data = pd.read_csv('customData1.csv')`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`from sklearn.feature_extraction.text import HashingVectorizer`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

&amp;#x200B;

`from sklearn.linear_model import SGDClassifier`

`text_clf = OnlinePipeline([('vect', HashingVectorizer()),`

`('clf-svm', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=5, random_state=None)),`

`])`

`text_clf = text_clf.fit(X_train,Y_train)`

`predicted = text_clf.predict(X_test)`

`np.mean(predicted == Y_test)`

The above gives me an accuracy of 0.55

&amp;#x200B;

A few days later when I have new emails I import the previously exported model and use partial\_fit on a new csv file.

`import pandas as pd`

`data = pd.read_csv('customData2.csv') #text in column 1, classifier in column 2.`

`import numpy as np`

`numpy_array = data.values`

`X = numpy_array[:,0]`

`Y = numpy_array[:,1]`

&amp;#x200B;

`from sklearn.externals import joblib`

`from sklearn.pipeline import Pipeline`

&amp;#x200B;

`class OnlinePipeline(Pipeline):`

`def partial_fit(self, X, y=None):`

`for i, step in enumerate(self.steps):`

`name, est = step`

`est.partial_fit(X, y)`

`if i &lt; len(self.steps) - 1:`

`X = est.transform(X)`

`return self`

`text_clf2 = joblib.load('text_clf.joblib')`

&amp;#x200B;

`from sklearn.model_selection import train_test_split`

`X_train, X_test, Y_train, Y_test = train_test_split(`

`X, Y, test_size=0.4, random_state=42)`

&amp;#x200B;

`text_clf2 = text_clf2.partial_fit(X_train,Y_train)`

&amp;#x200B;

`predicted = text_clf2.predict(X_test)`

`np.mean(predicted == Y_test)`

This returns the improved accuracy of: 0.84

&amp;#x200B;

Sorry for so much code!  I obviously need to tidy it all up so its a single method and handle the import/export logic properly.

&amp;#x200B;

Have a made any major errors or are there any obvious improvements? Thanks!

&amp;#x200B;",1,scikit_learn,2020-10-13
a0iz4i,Does cross_val_score tell something about generalizability?,"Does cross\_val\_score tell something about generalizability?

&amp;#x200B;

Or do I need to use something else for measuring generalizability?",0,scikit_learn,2020-10-13
