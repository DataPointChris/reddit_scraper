comment_id,post_id,comment,upvotes
g8puhlw,jahgte,"I'll speak very generally. Obviously each position is going to differ, but here's a starting skillset that I see a lot of the people we interview have:

Language:  Many people see to be using Python now. I'm old school and before ML took off used C or Java for all my workhorse apps.  But python seem to be what a lot of data scientists coming out of school as well as in the field are using to analyze their data. (R is great for statistics as well and seems to have replaces SAS in a lot of way)

Frameworks: There's quite a few machine learning libraries to play with if you go with Python.  From a 'big data' perspective, learning Spark as a way to access large, distributed datasets is a good start. It can take a bit to 'master', but at least learning the concepts behind it and distributed data would go a long way. 

You can use the above technologies with one another. Spark with Python (PySpark) works really nicely.  Many spark developers also use the Scala language, but for whatever reason I see a lot of data scientists coming out of school using Python, not Scala (or Java) for accessing Spark.

Something like Jupyter is an easy way to get started with Python (and other languages) as well as accessing Spark via Python.  Our data scientists use Jupyter notebooks as a way to access, manipulate and visualize data all the time.  Eventually any models they create this way gets run outside the fancy Jupyter GUI environment when productionalzed, but it's a great tool for getting started and creating your models and seeing the results.

There's probably no wrong answers here, but that's what I've seen our own data scientists use as well as kids we're recruiting coming in with.",1
g8otn5a,jac1go,"Bounces through tracker, gets ad blocked.  Share actual post URL.",3
g8nwajw,ja2aja,Pfffffffffff,1
g8n9bpt,ja1hg8,"My data engineering team currently manage tens of terabytes of data in the healthcare space, mostly structured and semi-structured data. Lots of batch processing and an increasing amount of stream processing and/or micro-batching.

The technologies we use and look for experience with include Git, Bash, Python, SQL, Docker, Airflow, Snowflake, Kafka, Kubernetes, and several AWS services (EC2, EKS, S3, EFS, SQS, and AGW.)

We don’t currently use Spark but you can’t go wrong learning it and I would definitely prioritize it. We are also considering adding IaC knowledge like Terraform, not as a requirement but a nice to have, but this is more situational.

Industry specific knowledge can be helpful—like HL7 or FHIR for healthcare interoperability. Same for experience with master data management and compliance initiatives—although these can certainly be learned on the job.

The field is very broad. Might not hurt to look at job ads in regions you’d like to work from (or remote) and see if there are particular trends.

Best of luck!",3
g8nhmc9,ja1hg8,"Thanks a bunch for the great info! I am in banking and financial infra. We have tables that are in the 1tb range (1.5 to 5 billion records) with a mix of structured and unstructured data.

My main technologies are hadoop framework using hive and Unix mainly. I have basic exposure to spark. My skills with java and Python are minimal professionally. But I can understand what I'm looking at and make code changes if needed. I am also familiar with git and how it works. We mainly use in house tools for or ETL processes and version control. So I'm definitely going to get very familiar with git.


Like I said, I have only 3 ish total years of experience. So I'm still super new. Im also very young. Its very daunting and I feel the imposter syndrome creep up when I look at the trends and at the job openings I pass by.",2
g8nsiny,ja1hg8,"Get good at Python, SQL, and Spark. Then learn AWS products like Lambda, Athena, Redshift, Glue and of course, S3. Study machine learning - look up the book Real World Machine Learning as a starting place. Another useful language might be R, but is concentrate on the above first.",1
g8nx6ue,ja1hg8,Thanks!,1
g8nv48z,ja1hg8,"Learn Spark (Scala/python). Along with that spend some time on Azure or AWS... Having good understanding of cloud architecture is becoming must. Having some streaming knowledge with Kafka, Spark streaming would be huge add on.",1
g8nx8u4,ja1hg8,Alright I'll check those out! Thanks!,1
g8nixhl,ja025x,"It should add quite a bit to the existing Flink play. This video helped me put it in context:

[https://www.youtube.com/watch?v=PUI3I\_v8q8M&amp;feature=emb\_logo](https://www.youtube.com/watch?v=PUI3I_v8q8M&amp;feature=emb_logo)",1
g8nj0mr,ja025x,"Also, not every company needs near-real time so that should be a good indicator of whether it's relevant to you. If you have Spark Streaming today ... maybe:-)",1
g8ox8dy,ja025x,"Yeah ok, seems kinda pointless to acquire though",1
g8oac3y,j9rzml,"If big data were too big, people would already be scared, and therefore dishonest.

But it rolls off the tongue like a good headline, so it may be worth enough to Murdoch's people to buy you a small farm, and we definitely need to get this pesky fertilizer out of the air.

Good luck.",1
g8oc3rl,j9rzml,"In this context of too big I mean are we even aware of what data we are giving up how, why, who and whether awareness make any difference to the perception we initially displayed. In fact a lot of my data is suggesting a fear for data which is beyond the control of people despite their depth knowledge on the topic and thus they become tolerant this is a natural human phenomenon known as loss-aversion this is most apparent in gen z who have grown up in an era of “false privacy” whereas gen y display actions and attitudes of “data activists” these are data haters and will avoid or disrupt the authenticity of their own data. I personally believe there is an ignorance to this debate a lot of people don’t have enough knowledge in the topic, from what’s possible to their rights. This research is to identify the difference knowledge makes. See I don’t believe it’s too big per say it is very beneficial but I believe the advances in technology have far surpassed the law which enables companies to take and use data easier than most would think which to me is still not okay. There is a higher priority on profitability from data collection than there is on privacy.",1
g8odmqv,j9rzml,"It's an ""indirect self-reference"" issue. However one asks ""has diversifying analogies of asking, includes those that are undisclosed, including old-fashioned surveillance, already got  scary?""

They may think ""asking whether something like asking already scares me? that sounds like asking to become pretty scary, we'd better sound brave or the scary man will hurt us - eventually - no, let's just leave.""

Indirect self-reference can be a source of intrinsically biased sampling.",1
g8knaov,j9lwmj,"Cloudera quick VM, the first option, is a good option to start and play with a Hadoop cluster. 

Option 2 isn’t so hard, but always make sure that the requirements are met before installation, so you don’t spend days in troubleshooting to find out it can’t work.",3
g8kshno,j9lwmj,"**What I did:** 

1. Downloaded Ubuntu ISO file
2. Created 3 Virtual Machines 1 or 2 GB RAM + 1 Core CPU + 10 GB HDD + Networking ON
3. Installed Ubuntu in all 3 machines
4. Created Passwordless SSH connection between all the three Machines \[all combinations\] . (Most important step)
5. Downloaded Apache Hadoop form [archive.apache.org](https://archive.apache.org)
6. Copied Configuration Files + .bashrc files from some repositories on Github
7. scp hadoop and files to all the three machines

Thats all. Then you can just start working around it. If you understand these steps. Installing other things such as Spark, Hive, Hbase, Zookeeper etc will be easier.   


I am myself new into it but tried installation a few times.",2
g8kkb6x,j9lwmj,"I am a beginner and have tried installing multiple times by the 2nd method. Never was able to get it to work... Got disheartened and never wanted to try again. 
I did end up finding a virtual machine with the ecosystem already installed... I downloaded that and it worked real good. Maybe you should try that too! Saves you the trouble of trying to do it yourself... Unless you want to explore!",1
g8klx2e,j9lwmj,Would you please tell me specifically which virtual machine and ecosystem u used?,1
g8kn43v,j9lwmj,Probably from a relevant online course.,3
g8kpj5t,j9lwmj,would you like to suggest one course which you would recommend to beginners ?,1
g8kr4xs,j9lwmj,"I've looked through my coursework from the past, and it seems like The Data Scientist's Toolbox from Coursera still exists! I'm not sure if they offer premade VMs as part of the coursework, those memories might actually be from other courses I did (involving genomics, so probably not very relevant to you).",1
g8k9b9z,j9ist1,"I think before you try read a specific technology, you need to look at the volumetrics of your use case and identify your driver for a big data solution.

If you come at this with a mentality to learn ""a product"" you won't gain much depth in this field. If you were to instead look at the problems which were solved by a big data solution you will understand the products better in their context.",2
g8k3nvp,j9ist1,"Spark, kafka, hdfs, cloud native storage like adls, s3 and lot more.",1
g8k43ke,j9ist1,"Damit.

Are there not 1 or 2 that are standards or do i need to learn all of them?

I am actually playing around with Tableau.",1
g8k4b12,j9ist1,"Well in that case, it is Apache spark. Spark is the distributed data processing framework, basically you have to code in programming language like scala, python, Java etc. Tableau, power bi are more like for visualization and lighter data analysis.",4
g8k4r4s,j9ist1,"&gt;Apache spark

Does Apache Spark stand for Big data as Photoshop for Photo editing?

And thank you so much for your help it was really very useful because i got confused on searching on the net. Everyone tells you something different",2
g8k4zb1,j9ist1,"All good, you can start reading into it and you will get a better understanding. And spark is pretty much like Adobe photo shop for editing, and prior to spark, it used to be apache hadoop.",1
g8k52xq,j9ist1,"Ok 

Thank you my friend and have a nice day",1
g8mwfxt,j9ist1,"Because big data is so vaguely defined, that's why everyone tells something else. 
I am genuinely interested what volume is for you big data. 
Spark just won in the marketing buzzword claims.",1
g8k6n26,j9ist1,Google CNCF Landscape.,1
g8k861h,j9ist1,"As others have mentioned there isnt a single answer. Although Spark is capable to handle a lot of the scenarios commonly encountered in data pipelines and even extends further (supports some ML algorithms).

I think its a good starting point and then you could easily move to other frameworks. A lot of them are built on Java/Scala so its also a good idea to get familiar with those languages",1
g8jl7ys,j9g61r,"There’s a lot about that question that I either disagree with or is out of date. It’s worded in such a way that it sounds like it’s based off a certain set of material. 

That said, if I were to take that question as is, I’d reference the particular programming model and the particular storage model that Hadoop implements and discuss the advantages (and possibly disadvantages if you wish). The Wikipedia page should give you enough info to dig further.",2
g8jqm85,j9g61r,"This is a badly worded question, and I agree that it's a little out of date. If I were asked this, I would say something about 

1. Bringing compute to the data

2. Schema on read

However, I suspect that the ""right"" answer is in your notes somewhere. Good luck!",1
g8ca7o7,j8o65q,Beautiful! Incredible having such diversity (in countries) in the latter years. I wonder if it is due to poor tracking of stats in other countries orgs?,2
g8dvyya,j8o5vm,"shill.

Obvious shill is obvious.",2
g8cd8ae,j8o5vm,I'm trying to separate the hype from the buzz.,1
g8bxobu,j8j0zb,"Cloud companies want you to they are the ""easy"" button as presented above. The truth is these same companies have run the numbers and have proven that investing in your own infrastructure and not paying rent to a third party makes way more financial sense and provides the benefit of meeting performance goals without losing control of your data.  Companies that continue to just pay these high monthly cloud fees will eventually be bankrupt.",1
g8dtxw9,j8j0zb,"That's interesting, do you have any source or article about that?",3
g84jwms,j7fo0s,"You would need to use delta core package when starting your shell:

pyspark --packages io.delta:delta-core_2.12:0.7.0 --conf ""spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"" --conf ""spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog""

Or you can also include the libraries required in your SBT project: 

libraryDependencies += ""io.delta"" %% ""delta-core"" % ""0.7.0""

ref: https://docs.delta.io/latest/quick-start.html",1
g84jxks,j7fo0s,"**I found links in your comment that were not hyperlinked:**

* [io.delta](https://io.delta)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g84jwms&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1
g84uh68,j7d8pm,I learn a lot from my coworker that are phds and I try to summarize in 10 min videos on my YouTube. But it’s hard if you don’t work in the field. Maybe medium blog post but you also need to know what to search for,1
g840m8z,j7ccwy,"15 minute intervals might be hard to find...

You might have better luck asking in one of the weather or meteorology subs.",1
g842d78,j7ccwy,"""meteorology subs""? What are those? subreddits? btw I tried also emailing the company publishing the hourly data from the link I attached, no response yet. Where else can I search?",1
g84b938,j7ccwy,I mean weather-oriented subreddits.,1
g830rgh,j76rnj,"where are you getting stuck ? what steps are you following ? ( if you need a development environment, a quickstart vm would be a better idea )",1
g831fs9,j76rnj,"I want to run ambari-server and the other components on a single node so in the hosts section i am giving the fqdn of the same node. It just fails to register. The exact same procedure has worked on 18.04. The steps i am following for this are:

\- add own ip to /etc/hosts

wget -O /etc/apt/sources.list.d/ambari.list [http://public-repo-1.hortonworks.com/ambari/ubuntu18/2.x/updates/2.7.4.0/ambari.list](http://public-repo-1.hortonworks.com/ambari/ubuntu18/2.x/updates/2.7.4.0/ambari.list)

apt-key adv --recv-keys --keyserver [keyserver.ubuntu.com](https://keyserver.ubuntu.com) B9733A7A07513CAD

apt-get install ambari-server

ambari-server setup -s

ambari-server start

apt-get install ambari-agent

\-Make sure under the \[server\] section in  /etc/ambari-agent/conf/ambari-agent.ini, hostname is localhost.

ambari-agent start

THen proceed with the installation in ambari web ui

&amp;#x200B;

Since i need pseudo distributed i tried installing hadoop and hbase manually but hbase master gets stuck in the OPENING status everytime even though i followed every step exactly as seen in guides like this:  [https://computingforgeeks.com/install-apache-hadoop-hbase-on-ubuntu-linux/](https://computingforgeeks.com/install-apache-hadoop-hbase-on-ubuntu-linux/)",1
g836zzr,j76rnj,"Instead of this get a quickstart VM, that should get you going as all you want is a single node",1
g83847n,j76rnj,Can you suggest any based on ubuntu 18? Or any other popular ones?,1
g838fa9,j76rnj,Install virtualbox on the OS you have and then download a quickstart VM,1
g82q56p,j73yob,"Not on the data side - and more like 4 years (plus another 16 building Linux + Unix) - don’t trust vendors when they say “cheap hardware is fine”, because they will then say - “so with cheap hardware at X spec, you need 400 servers”, and then also... “due to your large number of servers, we’re increasing the price per server by 2.5x to X”.  
Also - be open to change.  Just because one tech was the best 4 or 6 years ago - doesn’t mean it’ll stay being the top.  It might turn out that something small turns out to be the big winner years later, so just be flexible and open to change.  
Latency in jobs kills - if you have high random or i/o latency, this will basically kill the whole purpose of said bigdata cluster, regardless of what platform it is on.  
Don’t forget about the need to do development on that data and platform, which may be actually represent a fairly large challenge if you need access to all that data to do data science exploratory or development level work.  
Just my 2c...",4
g81o16w,j6zlaf,"Snowflake is a cloud version of Teradata. Apply all the same reasons you'd like to use Teradata but without the infrastructure investment or support overheads.

Also apply many of the same reasons to not use Teradata to Snowflake: it's a proprietary format and the cost ""savings"" are by putting all your data in their tenancies sharing with other tenants. It's also a duplication of data, if you use Snowflake as the storage location for your DW users, you'll need something else to service your Data Science users.

In terms of why not just an Object Store ... look at reasons why you need a Data Warehouse at all. Many if not all are still valid, even as the interest in Data Science, ML and Deep Learning has grown. Far too many organisations BI capabilities is an excel spreadsheet backing onto a DW of some sort.

Last point: it looks like a lot of the Snowflake adoption has been by small to medium business or small parts of a large organisation. Even the large firms who've signed on are generally well below a petabyte of data ... many business users just swipe there credit cards for an initial trial. When you have petabytes of data, the shared tenancy model starts to fall down - you spend more time waiting for resources and the ""noise"" from neighbors can slow down your entire query.

So to your point, why not an object store or Snow Flake ... you'd be more likely to have both, with the object store for your raw and semi-structured data and Snow Flake for your transformed data for serving.",1
g82qj02,j6zlaf,"If you have a load of money they’ll do a private instance in your own AWS/Azure account, but last I heard they didn’t advertise it. May have changed since IPO. It’s more than you want but less than building your own with all those capabilities if you have a good use case. I think they do oversell into orgs that don’t always need it.",3
g821hue,j6zlaf,What if most of your data isn't a blob?  What if you can't hire people skilled enough to take stuff out of a data lake?  What if you YOLOD your Roth into $SNOW?  There's a million reasons.,1
g81gvph,j6uqpd,"Instead of HBase, consider Apache Ozone.

I've written many HBase apps but there's one thing HBase is terrible for ... storing and receiving Binary. The GET and PUT APIs are all byte array based, no streaming IO patterns.

Consider Ozone, you can run it in a docker environment for testing, it has an S3 API and will replace HDFS. You can use a traditional operational store for file metadata as the volumetrics are likely heavy on the image sizes but small data on the number of images (millions?)",1
g82hqsl,j6uqpd,"Thank you, but I am supposed to stick to hadoop.",1
g82hz9v,j6uqpd,"Ozone is a sub project of Apache Hadoop ... moreso than HBase which is a different top level project.

https://hadoop.apache.org/ozone/",1
g7zc0lg,j6kezj,Usually there is some kind of a broker in between. We use mqtt for example but it can be anything else,3
g7ze75h,j6kezj,Thanks for your inputs,1
g7ze0ic,j6kezj,"Splunk Forwarders, Elastic Beats, MiNiFi but theres some specialists in the IoT landscape though I haven't had experience.

Some features to look at:
- what operations, like pre-filtering, can I do before data hits the server?
- what can I send data to? HTTP and Kafka are two which come up
- Can I manage my fleet of sensors and is it CLI, REST and/or UI
- What is the process to upgrade sensors, can I do it centrally and target different configs to different classes of sensor. 

Generally updating sensors config won't be atomic, they usually have some kind of heartbeat and may be offline before they update ... eventually.",3
g7zeg3r,j6kezj,"Oh nice , I will look at each point separately , thanks hopefully this will be helpful to get me started.",2
g7zego3,j6kezj,Do you guys recommend any videos or books to get started ?,1
g7tunia,j5ls6v,just eww. I feel like I need to take shower.,1
g7tzuwa,j5ls6v,"At the concept, or the proposed options? :)",1
g7rhcvs,j5cnhw,"Apache Nifi can work with a schema registry I believe. So, you can store your schemas in the registry and use them for each data flavor you receive.
Nifi can also work with Kafka. So, why not push the data from API s into single or multiple kafka topics, have a consumer dump them as is with some audit info into a db - this will serve as your original output backup from APIs.

Have a second consumer to read the kafka messages, apply the schema over it, aggregate it and push to a sink/destination.

This would scale nicely and can work in both batch and realtime if the source frequency changes.",1
g7qsv7l,j590sh,Chellenges?,1
g7qk5dg,j57662,"Snowflake makes working with our piles of semi-structured data very easy (and the syntax is clean and intuitive). You can also create fairly complex JSON objects with Snowflake.

I haven’t worked with any significant volume of unstructured data with Snowflake.",3
g7qjp5k,j57662,"It’s got some nice json querying functionality, if that’s what you’re driving at.    I was able to build a POC off the twitter API and do some hashtag aggregations pretty easily w/o any prior snowflake experience.",3
g7rbdtf,j57662,"btw, building an active community at /r/snowflake, if you'd like to join :)

(as other comments say, Snowflake has some great JSON capabilities)",2
g7qn0z9,j56jpt,"Can’t go wrong with a classic.

https://www.oreilly.com/library/view/hadoop-the-definitive/9780596521974/",1
g7p4xgd,j50jvn,"Read Google's MapReduce paper from 2004, and then read papers that cite MapReduce until you reach modern research. This was basically how I started in undergrad before actually beginning to design &amp; build a next-gen data engine with my lab as a graduate student.",0
g7my5l0,j4rc2h,"Is this a sort of basket analysis? Where you have sets / collections of items and want to group collections that share the same items? To me the thing that will alter your approach is how many words does each item have. E.g are they distinct “Apple”, “Pear”, “Banana” or are they more variable “2 months consulting”, “a bushel of apples”, “a desk of Cheetos”, “a bag of apples”  and similar items some variation across sets. 

Depending on the above. You could look at either: 
- TFIDV vectorisation and cluster or topic modelling approaches: http://brandonrose.org/clustering_mobile
- set ratios such as Jaccard Score: https://www.statisticshowto.com/jaccard-index/
- or network theory and modularity: https://www.sciencedirect.com/science/article/pii/B9780124079083000091",1
g7tjes6,j4rc2h,"There seems to be a mutiple methods to do this see

[https://www.yworks.com/pages/clustering-graphs-and-networks](https://www.yworks.com/pages/clustering-graphs-and-networks)

[https://datavizcatalogue.com/methods/network\_diagram.html](https://datavizcatalogue.com/methods/network_diagram.html)

[https://www.jasondavies.com/wordcloud/about/](https://www.jasondavies.com/wordcloud/about/)

&amp;#x200B;

&gt; are they more variable “2 months 

I dont' understand why this would matter? In op I said this is an item, 

so “a desk of Cheetos”, “a bag of apples” would each be 1 item, “a bag of apples” is 1 item

So I don't understand why it'd matter

&amp;#x200B;

Does it matter in that that method mentioned [https://www.kdnuggets.com/2019/12/market-basket-analysis.html](https://www.kdnuggets.com/2019/12/market-basket-analysis.html)",1
g7tofjz,j4rc2h,"Ok, just needed to clarify as you would need to to clustering of items otherwise.",1
g7bpcc9,j3fkwj,"I developed a CI/local workflow to test Airflow DAG integration tests. I used some dockers and makefiles to create the eng. For data I used simple data (csv, txt) to test transformation etc.. for my purpose work great because I now can validate a complex pipeline local or on Github. I'll upload in a side project to discuss that!",3
g7c93c7,j3fkwj,We have been building CI workflow for data pipelines for years??,1
g7ct58n,j3fkwj,"Qualifier being ""directly from PRs on GitHub""",1
g7dr0uu,j3fkwj,"“We have been building CI workflow for data pipelines for years, Directly from PRs on Jenkins/AzureDevOps/Gitlab/GitHub” :)",1
g7e81py,j3fkwj,Fair enough! Haha,1
g7fbj1o,j3fkwj,"Hey everyone, I’m attending a webinar on starting a side hustle this Sunday to try and profit off my data analytics skills. Not too late to sign up! https://www.eventbrite.com/e/side-hustle-101-helpful-tips-and-strategies-from-successful-entrepreneurs-tickets-119201684447",1
g7bcgrb,j3dnrv,Looks interesting.,1
g75rhml,j2guu5,"Some pros and cons, think of NiFi as really good at moving files, with some flow control logic, that prefers fewer larger files.

It can do a lot of different scheduling methods and lots of different connectors.

You can do a lot of different format conversions with integration with a schema registry, some data manipulation but it's can't do Complex Event Processing: coordinating an event based on multiple streams of data with state.

It is good with variable size use cases but use cases with millions of 10KB messages sent per second are likely better on Kafka. NiFi works best by grouping messages like that for larger event sizes with slightly higher latency but better throughput. You can also send GBs in a single file which would qualify out Kafka.

You can get a single instance of NiFi running pretty simply but setting up a cluster can be complex and you'll like prefer the bundles distro of Zookeeper, Ranger and NiFi as part of Cloudera Flow manager. Having used the bunded version in CFM 2.0.1 for the first time, I really enjoyed the Ranger integration ... by comparison setting up authorisation on Process Control groups without it was challenging.",5
g7786hp,j2guu5,"Having looked through your requirements I suggest you check out how NiFi registry works first. Supporting multiple clients with customized flows may quickly become a pain depending on your setup.

If you decide to keep it all in one flow - it may turn into a bunch of RouteOnAttribute processors, cluttering everything around.

On the other hand, if you decide to keep each client's flow separately - rolling out updates to common parts of the flow and just keeping them all up to date will be a nightmare.

Overall, in my limited experience, NiFi is a pretty good tool as long as your flows are small and simple, without much branching in the logic. It can handle large files very well and is pretty stable (although you may have trouble setting it up as a cluster).",2
g77tn9l,j2guu5,"Registry gets my vote too - with nipyapi as a nice python library for automating CI/CD pipelines. There are some good blogs by Pierre Vallard on this. 

https://pierrevillard.com/tag/nifi-registry/",1
g78q7qk,j2guu5,"Nifi is bottlenecked by the way it's implemented. 

First, nifi server is a single process and as the number of pipelines and flow contents increase, the whole system becomes unstable and crashes. Hence, it's very difficult to scale. We had to create different nifi clusters for different applications. 

processes which are cpu bound like changing flow file data formats, compressing, uncompressing bottlenecks the whole pipeline. 

Security reviews can be difficult as anyone having access to the nifi cluster can download/view the contents of flow files(abstra tion of smallest unit of data in nifi world) in between processes 

Having worked on a petabytes scale data pipeline, and after lots of time wasted on Nifi, we choose to use Kafka (for file metadata- filename, hdfs location), hdfs (for actual file content), spark application ( can be streaming / batch ) for distributed processing , hive for data warehouse. 
All the components in the above pipeline can be individually scaled as per requirement. 

Recommendation : if you have few GBs of data processing which doesn't involve cpu bound activities like uncompressing, compressing, encryption, decryption and just want some kind of a data integration tool to move data around with minor transformations, feel free to use Nifi. But If you want a scalable system which should support 100s of data pipelines, invest in building generic application using some distributed processing framework like spark.",2
g73yd10,j282ob,"You need a strong culture of accountability and ensure the idea of being audited isn't some remote team who are primarily doing network event detection. It should be team leads and peers or an independent team co-located.

Securing and masking PII data outside of these teams is also key.

For apps teams, separation of concerns is still fantastic.

Or just put all PII data in an object store and give everyone in the org read access /s.",4
g75i0bo,j282ob,"The data is already out there, so it’s either me dealing with their data or it’s someone else.",1
g73chuz,j20pf6,A tear jerking moment for all the SQL Folks !,2
g73cud9,j1zcz9,Thanks nice one !,1
g71xuc2,j1y4xq,I'm interested!,2
g71ykdz,j1y4xq,Sweet! Sending you the link via PM.,1
g726eok,j1y4xq,How does it work? DDL tracking?,2
g728lxr,j1y4xq,"Not that advanced yet. Right now, it’s all manual upload of meta-data and more of a centralized knowledge management tool entered by users. I tried to make it look almost like a social network to encourage use by everyone using the data, so you have common work, questions, analysis all in one place around the data table! 

After getting feedback on this version, my next step is building out common integrations. Let me know if you’d like to check it out :)",1
g7ygyyl,j1y4xq,"Hey ! I'm interested! 

thanks",2
g7zp18y,j1y4xq,Sending you the link via PM!,1
g76u6sq,j1va2s,"Check out this blog: [https://www.inzata.com/data-analytics-blog-big-data-analysis-software/](https://www.inzata.com/data-analytics-blog-big-data-analysis-software/)

Inzata does sell a software, but they also have a weekly newsletter created specifically to educate people like us, not to try to sell their product. You can subscribe to it on any of their blog posts.",2
g71meko,j1tdqa,"My study from 2004 showed that your study from 2016 is bullshit. Anyway, good try to sell your certificates.",3
g6yj3dx,j1dssf,Tldr shit in shit out,4
g71uy4y,j1dssf,"Quite frankly, that is the real issue and sometimes the catch-22: Need ""good data"" &lt;-&gt; need ml to identify ""good data"".",1
g7f08vq,j16sk9,very interesting and some timely news. Thanks for putting this together.,1
g6gznj8,iyz56u,"Normally ... when your data is in Petabytes a lot of normals go outside what is considered typical.

Firstly, petabytes of data can mean a lot of different things. If it is one giant table, queries will likely benefit from storage using Direct Attached Storage (DAS) ... the now out of favour paradigm compared to disaggregated storage and compute.

Maybe this isn't an issue if its many smaller tables?

You should also investigate whether you can convert data on prem before transferring it - converting a days worth of data to Parquet can reduce it by 80% which makes the problem much smaller. This may also help with continual ingestion though Avro is still regarded as a better wire format for smaller, continual updates.

Finally at Multi-Petabyte scale, it may still be cheaper to run in your own data centre.  Cloud is essentially a giant computer lease scheme; it offers customers the ability to scale up/down but those economies disappear at certain levels of scale. Even Kubernetes starts to become an issue when you need thousands of pods at the drop of a hat.

Long story short, it isn't a simple decision how you move to cloud and also worth considering whether a move will save you money or meet your business SLAs.",1
g6i5umb,iyz56u,"Buying hardware to store the data, then buying a tool to transform it, and *then* moving to a cloud service would induce huge capital and operational expenditure. At that point, you'd be much better off cost-wise to keep your data on-premises. 

The cloud is just a data center, you are paying for your data to have a spot on their hardware. The benefits of the cloud is less hands-on management for hardware, and being able to quickly scale up or down resources for changing or unpredictable demands. If that does not apply to you, there likely isn't any real benefit for a complete cloud migration.

There is no one-size-fits-all cloud architecture. It is completely possible to run cloud services on a company's own on-premises data center, or build a private cloud specified only for a company's data. How a company would approach cloud data warehousing really depends on how much you want to spend, and how much you want to manage.",1
g6f3q32,iyw3gh,"Nice work but some links seems to have wrong destinations.
I noticed that when I click View Profile for Anaconda, it takes me to Big Query.",4
g6f4qij,iyw3gh,Thanks! Fixed it.,2
g6f1t9g,iyw3gh,nice work :),2
g6eztoy,iyw3gh,"If you got feedback or find errata, let me know!",1
g6ge5xs,iyw3gh,"I couldn't find anything related to Spark, SparklyR, John Snow Labs NLP libraries etc.

Are they being intentionally omitted?",1
g6gf1tf,iyw3gh,"Hi u/prasannask  
  
Spark is in there. The other ones aren't in there (yet). Will definitely look into it!  
  
Thanks for the tips!  
  
Roel",1
g6gkg7g,iyw3gh,What about Kinetica :),1
g6f15og,iyw3gh,So it's a database of big data tools?,1
g6cmr0c,iycidx,"Just from a highlevel, Snowflake is like a huge manageable database, where as hadoop ecosystem and hdfs is more like a filesystem and tools to manipulate and manage data.  

So these two aren't mutually exclusive.  You may store a lot of data within hdfs / s3 of all kinds of unstructured data.  You will process that data with tools like spark and then will probably want it to be easily accessible and structured by storing the data in a data warehouse like Snowflake.  

Are there tools within the hadoop ecosystem to work like a sql database, yes.  Are there tools that snowflake provides that can do what you'd use hadoop for, yes.  I think there are a lot of other factors that you'd need to consider when making this type of decision.",3
g6eegp4,iycidx,Excellent answer!,1
g6caf7o,iycidx,"I have not worked on Snowflake. From what I briefly read, snowflake seems to be purely SaaS (cloud based service) . CDP has to be installed on a cloud vendor/On-prem. I don't think it's SaaS. CDP supports hybrid architecture (on prem, cloud, both). 

I don't know enough about snowflake components to list other differences. But CDP has CDF (data flow components), CML (machine learning) etc. Not sure Snowflake supports all that out of the box.",2
g6e3su6,iy7zam,"hi please check h2kinfosys for data scientist course

# Hadoop Tutorial HDFS, HIVE, PIG Online Training What is PIG and HIVE 

[https://www.youtube.com/watch?v=JojAD010OiQ](https://www.youtube.com/watch?v=JojAD010OiQ)",1
g69m9jt,ixyshj,"This isnt an article, its a sales page for 11 pages.",3
g66nfzo,ixduwo,This video is reposted,5
g67b6ps,ixduwo,"Awesome BD skills. Video editing, not so much.",2
g688pxr,ixduwo,yah I agree with you. I need to do better on video editing since I am not an expert in that.,3
g68d2yb,ixduwo,Can you give me some specific feedback? I would like to improve my skill,3
g68hg07,ixduwo,"i'm not the one you want to learn from - at least not to learn how to do it \*correctly\*. the only way I can learn is by failing. again. and again... and.... again...  ooooh, wait! no..... and failing....... AGAIN!  

I was looking more for your insight on the Big Data stuff. Not the video editing... it just came up as unavailable - so really no video editing to 'critique'. 

/sometimes i think i must be a masochist...",3
g68wyax,ixduwo,Yah I am learning too cuz it’s my first time making videos lol but thank you I always try to look for ways to improve like I know so many good ways to add animation or maybe make it more entertaining. Yah I am sorry about this video cuz I have to record it again to remove some sensitive information but if you want to check my other video to give me feedback. Channel name is Data Lu.,1
g6716of,ixduwo,RemindMe!2days,4
g6a2mwg,ixduwo,[https://www.youtube.com/watch?v=isVHh5DV07I](https://www.youtube.com/watch?v=isVHh5DV07I),1
g68rp0w,ixduwo,RemindMe!2days,1
g68tyjg,ixduwo,"There is a 18.0 minute delay fetching comments.

I will be messaging you in 2 days on [**2020-09-24 19:14:44 UTC**](http://www.wolframalpha.com/input/?i=2020-09-24%2019:14:44%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/bigdata/comments/ixduwo/start_from_excel_to_machine_learning_in_my_career/g68rp0w/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fbigdata%2Fcomments%2Fixduwo%2Fstart_from_excel_to_machine_learning_in_my_career%2Fg68rp0w%2F%5D%0A%0ARemindMe%21%202020-09-24%2019%3A14%3A44%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20ixduwo)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1
g6a2n43,ixduwo,[https://www.youtube.com/watch?v=isVHh5DV07I](https://www.youtube.com/watch?v=isVHh5DV07I),1
g651e1v,ix6kx1,Post hard drives. Azure Data Box.,8
g6522go,ix1lhj,"The Kafka streaming architecture automates all of the steps. It will call your model, predict results, and spin up more workers if needed on demand as data lands",1
g63bydo,iwyxn9,You can find some ideas here: [https://mynoteshala.wordpress.com/big-data-project-ideas/](https://mynoteshala.wordpress.com/big-data-project-ideas/),3
g63dp48,iwyxn9,Thanks,1
g651eyi,iwyxn9,maybe you can build a ml project [https://www.youtube.com/watch?v=wCZG\_dECQc4](https://www.youtube.com/watch?v=wCZG_dECQc4),1
g60mi13,iwj0lb,"If you can tell, what it does under the hood? That will be great",1
g60pb7i,iwj0lb,"It lets users model their IT infrastructure using a web interface (or, if preferred for automation purposes, an API).

This model contains the configuration files. An engine, using a proprietary extensible database of checks, is then run through this model and the concrete configuration files, and reports then on any failed checks. Each check has a severity assigned to it, from 1 = ""you can keep that configuration if you like, but it best practice would be X"" to 5 = ""Your data is currently up for grabs"".

This differs from current approaches to this problem, which are more reactive (like SIEM). We want to put the practices that are so ubiquitous in the world of code (e.g. PMD, Coverity) to the setup of compute clusters as well, to prevent human error.

If you wish to learn more or see a demo, feel free to reach out. We will probably soon post a video about it as well.",1
g5va3l1,ivanid,"Your contribution is very good and i totally agree that nowadays both large and small companies should consider making sense of the data, and take advantage of all the information that lives in the cloud or that in the same company where it is You want to make use of data mining, you have the history that you have stored of your operations that are called the catalog of clients, suppliers, among others.

It is also important to point out that companies, apart from what is data mining, can opt for Big Data since its function, like data mining, is to analyze large volumes of data that exceed the capacity of the usual computer processing. Its objective is to analyze all the information in the shortest possible time and efficiently.

But if we ask ourselves when and who can use big data or data mining from the moment that a company and organization have data stored on their customer or supplier activity, useful information can probably be extracted by analyzing that data.

In particular when a company has its information stored, for example of its customers, when analyzing the information it can determine how to better serve them, how to have them more satisfied, get more business or increase loyalty towards the company.

So your contribution was very valuable to me since companies must see beyond what they are used to working today and that they must turn to see all these new technologies and turn to data mining or big data for improvement of your company. And not stay with the past and that each data they handle or have can get more out of it than they get today.",1
g5wdjgs,ivanid,Thank you,1
g5f278u,iti9in,Tag,1
g572bwu,is8at8,"There are entire courses in classical statistics on the use of augmented and synthetic datasets (i.e., bootstrapping and others). It's a well-established method of building models with incomplete data, but that doesn't mean the models in turn always translate to the real world. Using this method to build an accurate real-world model is an annealing process, i.e., 1) make a synthetic dataset, 2) make a model, 3) test the model, 4) update the dataset after testing, 5) update the model, 6) repeat \[3-5\] until achieving the desired performance. 

The closer your data are to reality, the fewer times you have to repeat this process.",1
g87mioz,is8at8,"I can't access your quiz anymore for some reason, but I just wanted to pitch in with another benefit: [imbalanced datasets can be fixed with synthetic data technology](https://mostly.ai/2020/05/07/tackling-ai-bias-at-its-source-with-fair-synthetic-data-fairness-series-part-4/), so human bias in data can be corrected. It will also be very important in all the [behavioral datasets collected during Covid lockdowns](https://www.technologyreview.com/2020/05/11/1001563/covid-pandemic-broken-ai-machine-learning-amazon-retail-fraud-humans-in-the-loop/?cid=other-eml-onp-mip-mck&amp;hlkid=957f385c45e7435bb521c31f30f0b7b4&amp;hctky=12332578&amp;hdpid=0d4ce30b-2850-4eea-a646-66ff53fb6395). People behave very differently during these strange times and this can throw off algorithms trained on these skewed samples.",1
g55u6b7,is6069,[deleted],1
g55zh69,is6069,"This is something interesting I found that is relavent

[https://www.pingshiuanchua.com/blog/post/using-youtube-api-to-analyse-youtube-comments-on-python](https://www.pingshiuanchua.com/blog/post/using-youtube-api-to-analyse-youtube-comments-on-python)",1
g57fbe9,is6069,"I think things skew towards open source technology when dealing with social media.  A lot of people are on YouTube learning Python, R, or Spark while not a lot of people are going to be watching videos or commenting on Vertica or Teradata as they are more enterprise players.  

So, while I am sure there is information to be gleaned, I don’t know you will be replacing Gartner with data scraped from YouTube.",1
g55yhzb,irxuvs,Yes! Totally agree. So many use cases and helps answer a wide array of questions.,1
g5b6t3e,irxuvs,"This was a great read, thank you.",1
g51vgdf,irrpah,[deleted],1
g54c3k9,irrpah,"Maybe it is because the Nvidia GPU Acceleration plugin is not part of the Apache Spark project. The most they might mention in their release notes is “Support for Accelerator-aware Scheduling” ([SPARK-24615](https://issues.apache.org/jira/browse/SPARK-24615)), and support for custom Spark `DriverPlugin` implementations. 

It would’ve been nice to have that shoutout (i.e. infrastructure for custom accelerators like `spark-rapids`), but heck.",1
g54r9e0,irrpah,"NVIDIA GPU acceleration which covers in ""Accelerator-aware task scheduling for Spark"" is also one of the great features of Spark 3.0. This Part I features covers some, I am writing another article which covers more.",1
g4xy38i,ired4x,"Been working with Flink for about 2 years now, the way I see it the pros are big, it’s pretty efficient, the DataStream API is easy to work with, and the checkpoints and back pressure mechanisms work well and helps a lot.

As far as cons, I say joining stream windows on event time can be a bit tricky (if one stream is blocked it won’t have any output), if your running on yarn cpu starving is pretty common if you have more then one task managers on the same host. And, if you’re working with Scala, some sink connectors are annoying to work with in the sense of converting Scala types to java types to SQL types. 
But these are minors considering the whole state of Flink",3
g4y6rou,ired4x,"Thank you for the response.  


I am new to Apache Flink and at the end of my project I wanted to add the Pro/Cons section, but to be honest I couldn't individuate any disadvantage aspect.  


  
In another forum, someone responded to me with some disadvantages (his opinion) about Flink. I will list them here with you and others if you find them useful.  
Seems more as subjective opinions, but can fit the Pro/cons section.  


* Less community and forums for discussion: Flink may be difficult to understand starting as a beginner because there are not many active communities and forums to exchange problems and doubt about Flink features.
* Less open-source projects: There are not many open-source projects to study and practice Flink. The lack of a code source makes it very difficult the familiarization with the most innovative features and mechanisms it offers.
* Immaturity: Immaturity in the industry is a disadvantage for Apache Flink because is a new technology and many features are constantly being updated and modified. We should avoid Apache Flink if we need a more matured framework compared to other competitors in the same space.
* API support: Apache Flink is not the best choice if we need more API support apart from Java and Scala languages.
* Data representation: Flink uses raw bytes as internal data representation, which if needed, can be hard to program",1
g54l8jz,ired4x,Part of the reason you're not getting any response is because you aren't asking about a use case. The question is too broad to answer. The answer you received from another forum wasn't helpful. Read why https://www.jesse-anderson.com/2017/07/this-is-useless-without-use-cases/.,1
g4tjicx,iqmvfm,"I liked most of the content but the omissions stood out: why Hive &amp; Impala, but not BigQuery, Redshift, Snowflake, Teradata, Netezza, DB2, or CitusDB?",2
g4ozgnm,iq4bj4,"I’m sorry for this potentially nonproductive response, but: welcome to big data, things are messy.

You’ll need to figure out what criteria matters to you. The first thing that comes to mind is perhaps some kind of sentiment analysis. This would take into consideration the thread and see if it was actually about COVID or not",5
g4q83y7,iq4bj4,"Building on the other guy's ""welcome to big data"" answer, maybe you should just scrape from feeds you trust contain good information.


If you can't do that, then you need to think about what makes a tweet ""fundamentally right"" for you. You could then do things like looking for mentions of words, entities, pattern matching, semantic distance. Yada yada yada",3
g4j57zm,ip9m1p,Is it free?,1
g4iply5,ip9ipx,[deleted],1
g4ipym2,ip9ipx,haha I guess I should use the dude voice. I just can't hear my own voice.,2
g4mob5j,ip9ipx,Really nice intro for Data Engineers to know what their ML counterparts do !,1
g4mw9zr,ip9ipx,Thank you,1
g4hq6ac,ip2w87,"Clickhouse, because it's fast, SQL-compliant, MPP database with good number of features. Not very mature though.",4
g4iou4m,ip2w87,"oh, that's cool! How do you find the clickhouse eco-system? Snowflake  seems to have tons of integrations, partners, etc that are pretty powerful. Does something similar existing for Clickhouse?",1
g4i2n8p,ip2w87,"Oracle DB. Because legacy code base. 😭

Slowly converting to Spark and Hadoop.",2
g4i9598,ip2w87,"Last job we were using Redshift, new job we're using Snowflake (I start in about 2 weeks tho).",2
g4i11kx,ip2w87,"Azure Synapse analytics.
All the other tech were MSFT so is our data engineering framework.",1
g4iyeq4,ip2w87,"""Homemade"" data warehouse : A sorta lambda architecture with Spark/Hadoop for batching, and Kafka/Spark Streaming for streams.

Data is served to users through various PostgreSQL dbs,
and we have currently an Elasticsearch rolling out for timeseries and real-time analytics.",1
g4evxet,ion5we,Also interested!,1
g4f9cvh,ion5we,Me too. I have strong sas/sql/unix/python programming skills and am looking to learn big data to add to my repertoire. I would also like to contribute to a big data project if possible - show some productive activity during 2020.,1
g4f9qa8,ion5we,Following,1
g4gordm,ion5we,"I would recommend checking out [Presto GitHub](https://github.com/prestosql/presto) as well as the developer page containing [philosophy and how to contribute](https://prestosql.io/development/). Presto is a distributed analytics query engine that uses ANSI SQL, meaning it follows a standard querying language and Presto itself doesn't store the data but rather is just an engine that can perform complex queries of RDMBS, Hadoop, and other NoSQL data stores. Since you touch [so many types of databases](https://prestosql.io/docs/current/connector.html), it's easy to start working on the database/data model connector that you're familiar with and that will help you understand Presto basics. Also if you're wanting to break into new data stores, you can then transfer over contributing to other connectors as you feel like. Otherwise, if you want to learn about data engine specifics like how the parser, query planner, and optimizer works, you could always work on core Presto.This project is used and contributed to by a bunch of high profile companies like Uber, Netflix, LinkedIn, and was born out of Facebook.To get a history, there are a few recent podcasts that have come out about this:

* [https://www.contributor.fyi/presto](https://www.contributor.fyi/presto) \- This is an interview with the founders of Presto talking about the origin story and where things are headed
* [https://www.dataengineeringpodcast.com/presto-distributed-sql-episode-149/](https://www.dataengineeringpodcast.com/presto-distributed-sql-episode-149/)  \- This is an interview with Presto co-creator Martin Traverso jumping into where Presto fits in the Big Data architecture.

I will also be starting running a twitch show that highlights the efforts of our contributors by discussing their PRs, talk Presto news, etc... More to come on that later but here are a few test interviews we did for our first episode.

* [How do you make a thriving developer community around Presto?](https://www.twitch.tv/videos/729152420)
* [Favorite part of implementing a feature?](https://www.twitch.tv/videos/729201925)

To find out more, find me on the [presto community slack](https://prestosql.slack.com/) just look up ""Brian Olsen"" or you can post in the #general or #dev channels. This slack [community is 2,300 strong](https://twitter.com/prestosql/status/1278393800092643328) and growing. This includes the founders and many of the early contributors to the project who are personally invested in nurturing and growing the community to make it the most inclusive and make presto the best analytics engine.",1
g4ed7yw,ioh9ts,Umm...what else would you optimize for?,1
g4dsxp3,ioetic,"Just curious, why is it that all these companies don’t have R listed, but Python over R? Is python really that much more superior? 

I’m a predominantly R user myself, as far as statistical language. I know SQL, and a small amount of SAS. It’s surprising to me that python is listed and R is not. 

Just seeing if anyone had any insight into this.",2
g4e7ayw,ioetic,I am curious too. I’ve started learning python lately because I’ve been so frustrated of the lacking areas of R,1
g4hdkn5,ioetic,"Python is the most common, but having R in your skillset is valuable too",1
g4cv3k5,io955z,"I have worked in big data for a while, and streaming and real-time projects for the last 5 years or so.

Kafka is very popular for large platforms, Kinesis is also popular.

Hadoop platforms are still prevalent in large orgs, but cloud native is growing faster.

Spark is very popular

Most devs I know use IntelliJ and git , CI/CD depends on the org.

Cluster size - I have worked on everything from server less with 3 containers, to 5000 node clusters.

Data size - if it’s small enough to fit on one machine it’s not big data (but people will still act like it is). Most frequently I am working with GB to TB per day.  Largest I personally have dealt with is .8PB per day.",2
g4fweps,io955z,Thank you for sharing :),1
g4be5sr,inz7rg,https://www.youtube.com/channel/UCR5fQQoUywhm3tE3O0HSVVg,1
g42oamm,ims0ji,"Not sure if this is what you are looking for.

""FoodData Central"" https://fdc.nal.usda.gov",2
g3wbzto,ilrxl0,Who are y’all dating ...,1
g3y3daj,ilrxl0,Always thought data mining was just scrapping data from other places but at a larger scale.,1
g3vcs2z,ilrxl0,Omg thank you. I just joined a data consulting firm in sales and this is very helpful. Any others to add that you or others can think of would be very appreciated.,0
g3u2ffu,ilr90i,"If there is any girl DE who knows all the above , lease ping me , I want to marry you.

The list is all exhaustive ..
 So some people know all these stuff ?",1
g3ttrde,ilnsgf,"So lake = dump all data in the cheap cloud for the future
Warehouse = ETL with a plan for today's use",1
g3rtbi1,ilen9c,Stop screaming ya pansie,1
g3r9npl,ildec4,"To do what? The question is too vague. Is it speech recognition, translation, etc.?",3
g3r9wvf,ildec4,By process I mean run transformations on the data and put it through a Deep Neural Network.,1
g3rnx1u,ildec4,"I think the pre-processing of the audio data will be relative lightweight when compared to the actual forward pass of the Neural Network (specially for very large models). So I would  suggest to focus on the hardware that improves the network performance.

&amp;#x200B;

A GPU is great to speed up the Network's calculations, where a mid-range gaming GPU will outperform a high end CPU by 10x easily.",1
g3tsaan,il8w00,Does it mean we can apply data mining technique on football games result data and make bets based on that?,1
g3v9gkd,il8w00,"Yes, absolutely!",1
g3oivy7,ikzecl,"Lot of words and no concrete examples of anything real word. Still, decent read.",1
g3kiq6f,ikgfdd,Not bad but would be nice to see a CDP deployment on OpenShift or CDP Public deployment war stories.,1
g3dc5qw,ijeiqd,Following,2
g3ek911,ijeiqd,"Your data volume no where approaches big data - and is *fully* within the realm best served by RDBMS.  The data is structured, and te tasks you've identified are fully capable of being run off of a RDBMS.  There is *no* need for any big data technologies at all here - unless your intent is to learn big data technologies.

So really - what's the purpose of this data repository - the tasks identified above, or to apply the tasks above using big data tech?  If its just the listed tasks - stick with a RDBMS.",2
g3em5hm,ijeiqd,"Completely agree here - note that you’ll likely need some indexes to get good performance on queries but it is well within the capabilities. If it’s an option for price / platform, Microsoft SQL does have Python and R interfaces for ML.",2
g3g4ear,ijeiqd,"Thanks for your answer! My purpose is to apply data analysis, data mining and deep learning.

&amp;#x200B;

But as you and u/guacjockey said, you feel I am better served with simple R or python interfaces for a simple RDBMS right?

&amp;#x200B;

And what about the data not leaving provider institutions? not pushing my luck here, but do you reckon there are alternatives in the market to support this case and RDBMS?

&amp;#x200B;

Thanks for the help!",1
g3h6zei,ijeiqd,"It gets tricky - depending on what the source data is stored in you might be able to create a linked server / foreign-data-wrapper/ etc. But the difficulty is at some point the data will leave said institutions during query, you just may not be storing it. The question then comes of whether performance is adequate for whatever analysis you want to run (and/or without blasting their servers in the process).",2
g33fq4c,ihvaid,"If you want ""tools that will maintain their relevance a few years down the line"", you're investing in the wrong industry at the wrong time. We've just hit the data boom. Things are moving fast.",2
g33mnvc,ihvaid,Cloudera = dead bro,2
g39q6md,ihvaid,So is Aws EMR or azure hdinsight pave the way for Hadoop ?,1
g30cisl,ihicop,That site has toxic ads. Can't even seem to click past them.,1
g2u2qal,ig7tk4,I think Holden Karau is doing similar things. You could ping her.,1
g2w8nl2,ig7tk4,"Thanks for the recommendation, Im on it.",1
g2perbi,ifr81g,"You can define the schema prior to load and run the various import modes to determine the rows that work. You can also use the `columnNameOfCorruptRecord` option to save off (most of) the bad data. Note however that it doesn’t count extra / fewer columns as corrupt. To work around that, you can do the load everything into a single column dataframe and manually process the data (ie, perform splits on the data).

Beyond that, the GreatExpectations package will generate Spark usable code to further validate data.",2
g2jmryu,ieqj93,Data discovery refers to finding the source of truth about your data (think data catalogs and metadata about your data) whereas data profiling relates to summarizing or collecting statistics about your data.,3
g2jmfnb,ieqj93,I use the term with my network engineers when we are figuring out what data sources are available and what columns they have.,2
g2irfx3,ieqj93,"I don't know the answer but I wanna add that this community is very offline. There are barely discussion threads, comments interactions. I used to love it but these days is getting more and more of a ""spam"" target.  


Maybe I am wrong and this is only my point of view. Hope you will receive your aspected response and have fun with data.  


Best regards",1
g8puhlw,jahgte,"I'll speak very generally. Obviously each position is going to differ, but here's a starting skillset that I see a lot of the people we interview have:

Language:  Many people see to be using Python now. I'm old school and before ML took off used C or Java for all my workhorse apps.  But python seem to be what a lot of data scientists coming out of school as well as in the field are using to analyze their data. (R is great for statistics as well and seems to have replaces SAS in a lot of way)

Frameworks: There's quite a few machine learning libraries to play with if you go with Python.  From a 'big data' perspective, learning Spark as a way to access large, distributed datasets is a good start. It can take a bit to 'master', but at least learning the concepts behind it and distributed data would go a long way. 

You can use the above technologies with one another. Spark with Python (PySpark) works really nicely.  Many spark developers also use the Scala language, but for whatever reason I see a lot of data scientists coming out of school using Python, not Scala (or Java) for accessing Spark.

Something like Jupyter is an easy way to get started with Python (and other languages) as well as accessing Spark via Python.  Our data scientists use Jupyter notebooks as a way to access, manipulate and visualize data all the time.  Eventually any models they create this way gets run outside the fancy Jupyter GUI environment when productionalzed, but it's a great tool for getting started and creating your models and seeing the results.

There's probably no wrong answers here, but that's what I've seen our own data scientists use as well as kids we're recruiting coming in with.",1
g8otn5a,jac1go,"Bounces through tracker, gets ad blocked.  Share actual post URL.",3
g8nwajw,ja2aja,Pfffffffffff,1
g8n9bpt,ja1hg8,"My data engineering team currently manage tens of terabytes of data in the healthcare space, mostly structured and semi-structured data. Lots of batch processing and an increasing amount of stream processing and/or micro-batching.

The technologies we use and look for experience with include Git, Bash, Python, SQL, Docker, Airflow, Snowflake, Kafka, Kubernetes, and several AWS services (EC2, EKS, S3, EFS, SQS, and AGW.)

We don’t currently use Spark but you can’t go wrong learning it and I would definitely prioritize it. We are also considering adding IaC knowledge like Terraform, not as a requirement but a nice to have, but this is more situational.

Industry specific knowledge can be helpful—like HL7 or FHIR for healthcare interoperability. Same for experience with master data management and compliance initiatives—although these can certainly be learned on the job.

The field is very broad. Might not hurt to look at job ads in regions you’d like to work from (or remote) and see if there are particular trends.

Best of luck!",3
g8nhmc9,ja1hg8,"Thanks a bunch for the great info! I am in banking and financial infra. We have tables that are in the 1tb range (1.5 to 5 billion records) with a mix of structured and unstructured data.

My main technologies are hadoop framework using hive and Unix mainly. I have basic exposure to spark. My skills with java and Python are minimal professionally. But I can understand what I'm looking at and make code changes if needed. I am also familiar with git and how it works. We mainly use in house tools for or ETL processes and version control. So I'm definitely going to get very familiar with git.


Like I said, I have only 3 ish total years of experience. So I'm still super new. Im also very young. Its very daunting and I feel the imposter syndrome creep up when I look at the trends and at the job openings I pass by.",2
g8nsiny,ja1hg8,"Get good at Python, SQL, and Spark. Then learn AWS products like Lambda, Athena, Redshift, Glue and of course, S3. Study machine learning - look up the book Real World Machine Learning as a starting place. Another useful language might be R, but is concentrate on the above first.",1
g8nx6ue,ja1hg8,Thanks!,1
g8nv48z,ja1hg8,"Learn Spark (Scala/python). Along with that spend some time on Azure or AWS... Having good understanding of cloud architecture is becoming must. Having some streaming knowledge with Kafka, Spark streaming would be huge add on.",1
g8nx8u4,ja1hg8,Alright I'll check those out! Thanks!,1
g8nixhl,ja025x,"It should add quite a bit to the existing Flink play. This video helped me put it in context:

[https://www.youtube.com/watch?v=PUI3I\_v8q8M&amp;feature=emb\_logo](https://www.youtube.com/watch?v=PUI3I_v8q8M&amp;feature=emb_logo)",1
g8nj0mr,ja025x,"Also, not every company needs near-real time so that should be a good indicator of whether it's relevant to you. If you have Spark Streaming today ... maybe:-)",1
g8ox8dy,ja025x,"Yeah ok, seems kinda pointless to acquire though",1
g8oac3y,j9rzml,"If big data were too big, people would already be scared, and therefore dishonest.

But it rolls off the tongue like a good headline, so it may be worth enough to Murdoch's people to buy you a small farm, and we definitely need to get this pesky fertilizer out of the air.

Good luck.",1
g8oc3rl,j9rzml,"In this context of too big I mean are we even aware of what data we are giving up how, why, who and whether awareness make any difference to the perception we initially displayed. In fact a lot of my data is suggesting a fear for data which is beyond the control of people despite their depth knowledge on the topic and thus they become tolerant this is a natural human phenomenon known as loss-aversion this is most apparent in gen z who have grown up in an era of “false privacy” whereas gen y display actions and attitudes of “data activists” these are data haters and will avoid or disrupt the authenticity of their own data. I personally believe there is an ignorance to this debate a lot of people don’t have enough knowledge in the topic, from what’s possible to their rights. This research is to identify the difference knowledge makes. See I don’t believe it’s too big per say it is very beneficial but I believe the advances in technology have far surpassed the law which enables companies to take and use data easier than most would think which to me is still not okay. There is a higher priority on profitability from data collection than there is on privacy.",1
g8odmqv,j9rzml,"It's an ""indirect self-reference"" issue. However one asks ""has diversifying analogies of asking, includes those that are undisclosed, including old-fashioned surveillance, already got  scary?""

They may think ""asking whether something like asking already scares me? that sounds like asking to become pretty scary, we'd better sound brave or the scary man will hurt us - eventually - no, let's just leave.""

Indirect self-reference can be a source of intrinsically biased sampling.",1
g8knaov,j9lwmj,"Cloudera quick VM, the first option, is a good option to start and play with a Hadoop cluster. 

Option 2 isn’t so hard, but always make sure that the requirements are met before installation, so you don’t spend days in troubleshooting to find out it can’t work.",3
g8kshno,j9lwmj,"**What I did:** 

1. Downloaded Ubuntu ISO file
2. Created 3 Virtual Machines 1 or 2 GB RAM + 1 Core CPU + 10 GB HDD + Networking ON
3. Installed Ubuntu in all 3 machines
4. Created Passwordless SSH connection between all the three Machines \[all combinations\] . (Most important step)
5. Downloaded Apache Hadoop form [archive.apache.org](https://archive.apache.org)
6. Copied Configuration Files + .bashrc files from some repositories on Github
7. scp hadoop and files to all the three machines

Thats all. Then you can just start working around it. If you understand these steps. Installing other things such as Spark, Hive, Hbase, Zookeeper etc will be easier.   


I am myself new into it but tried installation a few times.",2
g8kkb6x,j9lwmj,"I am a beginner and have tried installing multiple times by the 2nd method. Never was able to get it to work... Got disheartened and never wanted to try again. 
I did end up finding a virtual machine with the ecosystem already installed... I downloaded that and it worked real good. Maybe you should try that too! Saves you the trouble of trying to do it yourself... Unless you want to explore!",1
g8klx2e,j9lwmj,Would you please tell me specifically which virtual machine and ecosystem u used?,1
g8kn43v,j9lwmj,Probably from a relevant online course.,3
g8kpj5t,j9lwmj,would you like to suggest one course which you would recommend to beginners ?,1
g8kr4xs,j9lwmj,"I've looked through my coursework from the past, and it seems like The Data Scientist's Toolbox from Coursera still exists! I'm not sure if they offer premade VMs as part of the coursework, those memories might actually be from other courses I did (involving genomics, so probably not very relevant to you).",1
g8k9b9z,j9ist1,"I think before you try read a specific technology, you need to look at the volumetrics of your use case and identify your driver for a big data solution.

If you come at this with a mentality to learn ""a product"" you won't gain much depth in this field. If you were to instead look at the problems which were solved by a big data solution you will understand the products better in their context.",2
g8k3nvp,j9ist1,"Spark, kafka, hdfs, cloud native storage like adls, s3 and lot more.",1
g8k43ke,j9ist1,"Damit.

Are there not 1 or 2 that are standards or do i need to learn all of them?

I am actually playing around with Tableau.",1
g8k4b12,j9ist1,"Well in that case, it is Apache spark. Spark is the distributed data processing framework, basically you have to code in programming language like scala, python, Java etc. Tableau, power bi are more like for visualization and lighter data analysis.",5
g8k4r4s,j9ist1,"&gt;Apache spark

Does Apache Spark stand for Big data as Photoshop for Photo editing?

And thank you so much for your help it was really very useful because i got confused on searching on the net. Everyone tells you something different",2
g8k4zb1,j9ist1,"All good, you can start reading into it and you will get a better understanding. And spark is pretty much like Adobe photo shop for editing, and prior to spark, it used to be apache hadoop.",1
g8k52xq,j9ist1,"Ok 

Thank you my friend and have a nice day",1
g8mwfxt,j9ist1,"Because big data is so vaguely defined, that's why everyone tells something else. 
I am genuinely interested what volume is for you big data. 
Spark just won in the marketing buzzword claims.",1
g8k6n26,j9ist1,Google CNCF Landscape.,1
g8k861h,j9ist1,"As others have mentioned there isnt a single answer. Although Spark is capable to handle a lot of the scenarios commonly encountered in data pipelines and even extends further (supports some ML algorithms).

I think its a good starting point and then you could easily move to other frameworks. A lot of them are built on Java/Scala so its also a good idea to get familiar with those languages",1
g8jl7ys,j9g61r,"There’s a lot about that question that I either disagree with or is out of date. It’s worded in such a way that it sounds like it’s based off a certain set of material. 

That said, if I were to take that question as is, I’d reference the particular programming model and the particular storage model that Hadoop implements and discuss the advantages (and possibly disadvantages if you wish). The Wikipedia page should give you enough info to dig further.",2
g8jqm85,j9g61r,"This is a badly worded question, and I agree that it's a little out of date. If I were asked this, I would say something about 

1. Bringing compute to the data

2. Schema on read

However, I suspect that the ""right"" answer is in your notes somewhere. Good luck!",1
g8ca7o7,j8o65q,Beautiful! Incredible having such diversity (in countries) in the latter years. I wonder if it is due to poor tracking of stats in other countries orgs?,2
g8dvyya,j8o5vm,"shill.

Obvious shill is obvious.",2
g8cd8ae,j8o5vm,I'm trying to separate the hype from the buzz.,1
g8bxobu,j8j0zb,"Cloud companies want you to they are the ""easy"" button as presented above. The truth is these same companies have run the numbers and have proven that investing in your own infrastructure and not paying rent to a third party makes way more financial sense and provides the benefit of meeting performance goals without losing control of your data.  Companies that continue to just pay these high monthly cloud fees will eventually be bankrupt.",1
g8dtxw9,j8j0zb,"That's interesting, do you have any source or article about that?",3
g84jwms,j7fo0s,"You would need to use delta core package when starting your shell:

pyspark --packages io.delta:delta-core_2.12:0.7.0 --conf ""spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"" --conf ""spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog""

Or you can also include the libraries required in your SBT project: 

libraryDependencies += ""io.delta"" %% ""delta-core"" % ""0.7.0""

ref: https://docs.delta.io/latest/quick-start.html",1
g84jxks,j7fo0s,"**I found links in your comment that were not hyperlinked:**

* [io.delta](https://io.delta)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g84jwms&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1
g84uh68,j7d8pm,I learn a lot from my coworker that are phds and I try to summarize in 10 min videos on my YouTube. But it’s hard if you don’t work in the field. Maybe medium blog post but you also need to know what to search for,1
g840m8z,j7ccwy,"15 minute intervals might be hard to find...

You might have better luck asking in one of the weather or meteorology subs.",1
g842d78,j7ccwy,"""meteorology subs""? What are those? subreddits? btw I tried also emailing the company publishing the hourly data from the link I attached, no response yet. Where else can I search?",1
g84b938,j7ccwy,I mean weather-oriented subreddits.,1
g830rgh,j76rnj,"where are you getting stuck ? what steps are you following ? ( if you need a development environment, a quickstart vm would be a better idea )",1
g831fs9,j76rnj,"I want to run ambari-server and the other components on a single node so in the hosts section i am giving the fqdn of the same node. It just fails to register. The exact same procedure has worked on 18.04. The steps i am following for this are:

\- add own ip to /etc/hosts

wget -O /etc/apt/sources.list.d/ambari.list [http://public-repo-1.hortonworks.com/ambari/ubuntu18/2.x/updates/2.7.4.0/ambari.list](http://public-repo-1.hortonworks.com/ambari/ubuntu18/2.x/updates/2.7.4.0/ambari.list)

apt-key adv --recv-keys --keyserver [keyserver.ubuntu.com](https://keyserver.ubuntu.com) B9733A7A07513CAD

apt-get install ambari-server

ambari-server setup -s

ambari-server start

apt-get install ambari-agent

\-Make sure under the \[server\] section in  /etc/ambari-agent/conf/ambari-agent.ini, hostname is localhost.

ambari-agent start

THen proceed with the installation in ambari web ui

&amp;#x200B;

Since i need pseudo distributed i tried installing hadoop and hbase manually but hbase master gets stuck in the OPENING status everytime even though i followed every step exactly as seen in guides like this:  [https://computingforgeeks.com/install-apache-hadoop-hbase-on-ubuntu-linux/](https://computingforgeeks.com/install-apache-hadoop-hbase-on-ubuntu-linux/)",1
g836zzr,j76rnj,"Instead of this get a quickstart VM, that should get you going as all you want is a single node",1
g83847n,j76rnj,Can you suggest any based on ubuntu 18? Or any other popular ones?,1
g838fa9,j76rnj,Install virtualbox on the OS you have and then download a quickstart VM,1
g82q56p,j73yob,"Not on the data side - and more like 4 years (plus another 16 building Linux + Unix) - don’t trust vendors when they say “cheap hardware is fine”, because they will then say - “so with cheap hardware at X spec, you need 400 servers”, and then also... “due to your large number of servers, we’re increasing the price per server by 2.5x to X”.  
Also - be open to change.  Just because one tech was the best 4 or 6 years ago - doesn’t mean it’ll stay being the top.  It might turn out that something small turns out to be the big winner years later, so just be flexible and open to change.  
Latency in jobs kills - if you have high random or i/o latency, this will basically kill the whole purpose of said bigdata cluster, regardless of what platform it is on.  
Don’t forget about the need to do development on that data and platform, which may be actually represent a fairly large challenge if you need access to all that data to do data science exploratory or development level work.  
Just my 2c...",5
g81o16w,j6zlaf,"Snowflake is a cloud version of Teradata. Apply all the same reasons you'd like to use Teradata but without the infrastructure investment or support overheads.

Also apply many of the same reasons to not use Teradata to Snowflake: it's a proprietary format and the cost ""savings"" are by putting all your data in their tenancies sharing with other tenants. It's also a duplication of data, if you use Snowflake as the storage location for your DW users, you'll need something else to service your Data Science users.

In terms of why not just an Object Store ... look at reasons why you need a Data Warehouse at all. Many if not all are still valid, even as the interest in Data Science, ML and Deep Learning has grown. Far too many organisations BI capabilities is an excel spreadsheet backing onto a DW of some sort.

Last point: it looks like a lot of the Snowflake adoption has been by small to medium business or small parts of a large organisation. Even the large firms who've signed on are generally well below a petabyte of data ... many business users just swipe there credit cards for an initial trial. When you have petabytes of data, the shared tenancy model starts to fall down - you spend more time waiting for resources and the ""noise"" from neighbors can slow down your entire query.

So to your point, why not an object store or Snow Flake ... you'd be more likely to have both, with the object store for your raw and semi-structured data and Snow Flake for your transformed data for serving.",1
g82qj02,j6zlaf,"If you have a load of money they’ll do a private instance in your own AWS/Azure account, but last I heard they didn’t advertise it. May have changed since IPO. It’s more than you want but less than building your own with all those capabilities if you have a good use case. I think they do oversell into orgs that don’t always need it.",3
g821hue,j6zlaf,What if most of your data isn't a blob?  What if you can't hire people skilled enough to take stuff out of a data lake?  What if you YOLOD your Roth into $SNOW?  There's a million reasons.,1
g81gvph,j6uqpd,"Instead of HBase, consider Apache Ozone.

I've written many HBase apps but there's one thing HBase is terrible for ... storing and receiving Binary. The GET and PUT APIs are all byte array based, no streaming IO patterns.

Consider Ozone, you can run it in a docker environment for testing, it has an S3 API and will replace HDFS. You can use a traditional operational store for file metadata as the volumetrics are likely heavy on the image sizes but small data on the number of images (millions?)",1
g82hqsl,j6uqpd,"Thank you, but I am supposed to stick to hadoop.",1
g82hz9v,j6uqpd,"Ozone is a sub project of Apache Hadoop ... moreso than HBase which is a different top level project.

https://hadoop.apache.org/ozone/",1
g7zc0lg,j6kezj,Usually there is some kind of a broker in between. We use mqtt for example but it can be anything else,3
g7ze75h,j6kezj,Thanks for your inputs,1
g7ze0ic,j6kezj,"Splunk Forwarders, Elastic Beats, MiNiFi but theres some specialists in the IoT landscape though I haven't had experience.

Some features to look at:
- what operations, like pre-filtering, can I do before data hits the server?
- what can I send data to? HTTP and Kafka are two which come up
- Can I manage my fleet of sensors and is it CLI, REST and/or UI
- What is the process to upgrade sensors, can I do it centrally and target different configs to different classes of sensor. 

Generally updating sensors config won't be atomic, they usually have some kind of heartbeat and may be offline before they update ... eventually.",3
g7zeg3r,j6kezj,"Oh nice , I will look at each point separately , thanks hopefully this will be helpful to get me started.",2
g7zego3,j6kezj,Do you guys recommend any videos or books to get started ?,1
g7tunia,j5ls6v,just eww. I feel like I need to take shower.,1
g7tzuwa,j5ls6v,"At the concept, or the proposed options? :)",1
g7rhcvs,j5cnhw,"Apache Nifi can work with a schema registry I believe. So, you can store your schemas in the registry and use them for each data flavor you receive.
Nifi can also work with Kafka. So, why not push the data from API s into single or multiple kafka topics, have a consumer dump them as is with some audit info into a db - this will serve as your original output backup from APIs.

Have a second consumer to read the kafka messages, apply the schema over it, aggregate it and push to a sink/destination.

This would scale nicely and can work in both batch and realtime if the source frequency changes.",1
g7qsv7l,j590sh,Chellenges?,1
g7qk5dg,j57662,"Snowflake makes working with our piles of semi-structured data very easy (and the syntax is clean and intuitive). You can also create fairly complex JSON objects with Snowflake.

I haven’t worked with any significant volume of unstructured data with Snowflake.",4
g7qjp5k,j57662,"It’s got some nice json querying functionality, if that’s what you’re driving at.    I was able to build a POC off the twitter API and do some hashtag aggregations pretty easily w/o any prior snowflake experience.",3
g7rbdtf,j57662,"btw, building an active community at /r/snowflake, if you'd like to join :)

(as other comments say, Snowflake has some great JSON capabilities)",2
g7qn0z9,j56jpt,"Can’t go wrong with a classic.

https://www.oreilly.com/library/view/hadoop-the-definitive/9780596521974/",1
g7p4xgd,j50jvn,"Read Google's MapReduce paper from 2004, and then read papers that cite MapReduce until you reach modern research. This was basically how I started in undergrad before actually beginning to design &amp; build a next-gen data engine with my lab as a graduate student.",0
g7my5l0,j4rc2h,"Is this a sort of basket analysis? Where you have sets / collections of items and want to group collections that share the same items? To me the thing that will alter your approach is how many words does each item have. E.g are they distinct “Apple”, “Pear”, “Banana” or are they more variable “2 months consulting”, “a bushel of apples”, “a desk of Cheetos”, “a bag of apples”  and similar items some variation across sets. 

Depending on the above. You could look at either: 
- TFIDV vectorisation and cluster or topic modelling approaches: http://brandonrose.org/clustering_mobile
- set ratios such as Jaccard Score: https://www.statisticshowto.com/jaccard-index/
- or network theory and modularity: https://www.sciencedirect.com/science/article/pii/B9780124079083000091",1
g7tjes6,j4rc2h,"There seems to be a mutiple methods to do this see

[https://www.yworks.com/pages/clustering-graphs-and-networks](https://www.yworks.com/pages/clustering-graphs-and-networks)

[https://datavizcatalogue.com/methods/network\_diagram.html](https://datavizcatalogue.com/methods/network_diagram.html)

[https://www.jasondavies.com/wordcloud/about/](https://www.jasondavies.com/wordcloud/about/)

&amp;#x200B;

&gt; are they more variable “2 months 

I dont' understand why this would matter? In op I said this is an item, 

so “a desk of Cheetos”, “a bag of apples” would each be 1 item, “a bag of apples” is 1 item

So I don't understand why it'd matter

&amp;#x200B;

Does it matter in that that method mentioned [https://www.kdnuggets.com/2019/12/market-basket-analysis.html](https://www.kdnuggets.com/2019/12/market-basket-analysis.html)",1
g7tofjz,j4rc2h,"Ok, just needed to clarify as you would need to to clustering of items otherwise.",1
g7bpcc9,j3fkwj,"I developed a CI/local workflow to test Airflow DAG integration tests. I used some dockers and makefiles to create the eng. For data I used simple data (csv, txt) to test transformation etc.. for my purpose work great because I now can validate a complex pipeline local or on Github. I'll upload in a side project to discuss that!",3
g7c93c7,j3fkwj,We have been building CI workflow for data pipelines for years??,1
g7ct58n,j3fkwj,"Qualifier being ""directly from PRs on GitHub""",1
g7dr0uu,j3fkwj,"“We have been building CI workflow for data pipelines for years, Directly from PRs on Jenkins/AzureDevOps/Gitlab/GitHub” :)",1
g7e81py,j3fkwj,Fair enough! Haha,1
g7fbj1o,j3fkwj,"Hey everyone, I’m attending a webinar on starting a side hustle this Sunday to try and profit off my data analytics skills. Not too late to sign up! https://www.eventbrite.com/e/side-hustle-101-helpful-tips-and-strategies-from-successful-entrepreneurs-tickets-119201684447",1
g7bcgrb,j3dnrv,Looks interesting.,1
g75rhml,j2guu5,"Some pros and cons, think of NiFi as really good at moving files, with some flow control logic, that prefers fewer larger files.

It can do a lot of different scheduling methods and lots of different connectors.

You can do a lot of different format conversions with integration with a schema registry, some data manipulation but it's can't do Complex Event Processing: coordinating an event based on multiple streams of data with state.

It is good with variable size use cases but use cases with millions of 10KB messages sent per second are likely better on Kafka. NiFi works best by grouping messages like that for larger event sizes with slightly higher latency but better throughput. You can also send GBs in a single file which would qualify out Kafka.

You can get a single instance of NiFi running pretty simply but setting up a cluster can be complex and you'll like prefer the bundles distro of Zookeeper, Ranger and NiFi as part of Cloudera Flow manager. Having used the bunded version in CFM 2.0.1 for the first time, I really enjoyed the Ranger integration ... by comparison setting up authorisation on Process Control groups without it was challenging.",6
g7786hp,j2guu5,"Having looked through your requirements I suggest you check out how NiFi registry works first. Supporting multiple clients with customized flows may quickly become a pain depending on your setup.

If you decide to keep it all in one flow - it may turn into a bunch of RouteOnAttribute processors, cluttering everything around.

On the other hand, if you decide to keep each client's flow separately - rolling out updates to common parts of the flow and just keeping them all up to date will be a nightmare.

Overall, in my limited experience, NiFi is a pretty good tool as long as your flows are small and simple, without much branching in the logic. It can handle large files very well and is pretty stable (although you may have trouble setting it up as a cluster).",2
g77tn9l,j2guu5,"Registry gets my vote too - with nipyapi as a nice python library for automating CI/CD pipelines. There are some good blogs by Pierre Vallard on this. 

https://pierrevillard.com/tag/nifi-registry/",1
g78q7qk,j2guu5,"Nifi is bottlenecked by the way it's implemented. 

First, nifi server is a single process and as the number of pipelines and flow contents increase, the whole system becomes unstable and crashes. Hence, it's very difficult to scale. We had to create different nifi clusters for different applications. 

processes which are cpu bound like changing flow file data formats, compressing, uncompressing bottlenecks the whole pipeline. 

Security reviews can be difficult as anyone having access to the nifi cluster can download/view the contents of flow files(abstra tion of smallest unit of data in nifi world) in between processes 

Having worked on a petabytes scale data pipeline, and after lots of time wasted on Nifi, we choose to use Kafka (for file metadata- filename, hdfs location), hdfs (for actual file content), spark application ( can be streaming / batch ) for distributed processing , hive for data warehouse. 
All the components in the above pipeline can be individually scaled as per requirement. 

Recommendation : if you have few GBs of data processing which doesn't involve cpu bound activities like uncompressing, compressing, encryption, decryption and just want some kind of a data integration tool to move data around with minor transformations, feel free to use Nifi. But If you want a scalable system which should support 100s of data pipelines, invest in building generic application using some distributed processing framework like spark.",2
g73yd10,j282ob,"You need a strong culture of accountability and ensure the idea of being audited isn't some remote team who are primarily doing network event detection. It should be team leads and peers or an independent team co-located.

Securing and masking PII data outside of these teams is also key.

For apps teams, separation of concerns is still fantastic.

Or just put all PII data in an object store and give everyone in the org read access /s.",5
g75i0bo,j282ob,"The data is already out there, so it’s either me dealing with their data or it’s someone else.",1
g73chuz,j20pf6,A tear jerking moment for all the SQL Folks !,2
g73cud9,j1zcz9,Thanks nice one !,1
g71xuc2,j1y4xq,I'm interested!,2
g71ykdz,j1y4xq,Sweet! Sending you the link via PM.,1
g726eok,j1y4xq,How does it work? DDL tracking?,2
g728lxr,j1y4xq,"Not that advanced yet. Right now, it’s all manual upload of meta-data and more of a centralized knowledge management tool entered by users. I tried to make it look almost like a social network to encourage use by everyone using the data, so you have common work, questions, analysis all in one place around the data table! 

After getting feedback on this version, my next step is building out common integrations. Let me know if you’d like to check it out :)",1
g7ygyyl,j1y4xq,"Hey ! I'm interested! 

thanks",2
g7zp18y,j1y4xq,Sending you the link via PM!,1
g76u6sq,j1va2s,"Check out this blog: [https://www.inzata.com/data-analytics-blog-big-data-analysis-software/](https://www.inzata.com/data-analytics-blog-big-data-analysis-software/)

Inzata does sell a software, but they also have a weekly newsletter created specifically to educate people like us, not to try to sell their product. You can subscribe to it on any of their blog posts.",2
g71meko,j1tdqa,"My study from 2004 showed that your study from 2016 is bullshit. Anyway, good try to sell your certificates.",4
g6yj3dx,j1dssf,Tldr shit in shit out,4
g71uy4y,j1dssf,"Quite frankly, that is the real issue and sometimes the catch-22: Need ""good data"" &lt;-&gt; need ml to identify ""good data"".",1
g7f08vq,j16sk9,very interesting and some timely news. Thanks for putting this together.,1
g6gznj8,iyz56u,"Normally ... when your data is in Petabytes a lot of normals go outside what is considered typical.

Firstly, petabytes of data can mean a lot of different things. If it is one giant table, queries will likely benefit from storage using Direct Attached Storage (DAS) ... the now out of favour paradigm compared to disaggregated storage and compute.

Maybe this isn't an issue if its many smaller tables?

You should also investigate whether you can convert data on prem before transferring it - converting a days worth of data to Parquet can reduce it by 80% which makes the problem much smaller. This may also help with continual ingestion though Avro is still regarded as a better wire format for smaller, continual updates.

Finally at Multi-Petabyte scale, it may still be cheaper to run in your own data centre.  Cloud is essentially a giant computer lease scheme; it offers customers the ability to scale up/down but those economies disappear at certain levels of scale. Even Kubernetes starts to become an issue when you need thousands of pods at the drop of a hat.

Long story short, it isn't a simple decision how you move to cloud and also worth considering whether a move will save you money or meet your business SLAs.",1
g6i5umb,iyz56u,"Buying hardware to store the data, then buying a tool to transform it, and *then* moving to a cloud service would induce huge capital and operational expenditure. At that point, you'd be much better off cost-wise to keep your data on-premises. 

The cloud is just a data center, you are paying for your data to have a spot on their hardware. The benefits of the cloud is less hands-on management for hardware, and being able to quickly scale up or down resources for changing or unpredictable demands. If that does not apply to you, there likely isn't any real benefit for a complete cloud migration.

There is no one-size-fits-all cloud architecture. It is completely possible to run cloud services on a company's own on-premises data center, or build a private cloud specified only for a company's data. How a company would approach cloud data warehousing really depends on how much you want to spend, and how much you want to manage.",1
g6f3q32,iyw3gh,"Nice work but some links seems to have wrong destinations.
I noticed that when I click View Profile for Anaconda, it takes me to Big Query.",3
g6f4qij,iyw3gh,Thanks! Fixed it.,2
g6f1t9g,iyw3gh,nice work :),2
g6eztoy,iyw3gh,"If you got feedback or find errata, let me know!",1
g6ge5xs,iyw3gh,"I couldn't find anything related to Spark, SparklyR, John Snow Labs NLP libraries etc.

Are they being intentionally omitted?",1
g6gf1tf,iyw3gh,"Hi u/prasannask  
  
Spark is in there. The other ones aren't in there (yet). Will definitely look into it!  
  
Thanks for the tips!  
  
Roel",1
g6gkg7g,iyw3gh,What about Kinetica :),1
g6f15og,iyw3gh,So it's a database of big data tools?,1
g6cmr0c,iycidx,"Just from a highlevel, Snowflake is like a huge manageable database, where as hadoop ecosystem and hdfs is more like a filesystem and tools to manipulate and manage data.  

So these two aren't mutually exclusive.  You may store a lot of data within hdfs / s3 of all kinds of unstructured data.  You will process that data with tools like spark and then will probably want it to be easily accessible and structured by storing the data in a data warehouse like Snowflake.  

Are there tools within the hadoop ecosystem to work like a sql database, yes.  Are there tools that snowflake provides that can do what you'd use hadoop for, yes.  I think there are a lot of other factors that you'd need to consider when making this type of decision.",4
g6eegp4,iycidx,Excellent answer!,1
g6caf7o,iycidx,"I have not worked on Snowflake. From what I briefly read, snowflake seems to be purely SaaS (cloud based service) . CDP has to be installed on a cloud vendor/On-prem. I don't think it's SaaS. CDP supports hybrid architecture (on prem, cloud, both). 

I don't know enough about snowflake components to list other differences. But CDP has CDF (data flow components), CML (machine learning) etc. Not sure Snowflake supports all that out of the box.",2
g6e3su6,iy7zam,"hi please check h2kinfosys for data scientist course

# Hadoop Tutorial HDFS, HIVE, PIG Online Training What is PIG and HIVE 

[https://www.youtube.com/watch?v=JojAD010OiQ](https://www.youtube.com/watch?v=JojAD010OiQ)",1
g69m9jt,ixyshj,"This isnt an article, its a sales page for 11 pages.",3
g66nfzo,ixduwo,This video is reposted,5
g67b6ps,ixduwo,"Awesome BD skills. Video editing, not so much.",2
g688pxr,ixduwo,yah I agree with you. I need to do better on video editing since I am not an expert in that.,3
g68d2yb,ixduwo,Can you give me some specific feedback? I would like to improve my skill,3
g68hg07,ixduwo,"i'm not the one you want to learn from - at least not to learn how to do it \*correctly\*. the only way I can learn is by failing. again. and again... and.... again...  ooooh, wait! no..... and failing....... AGAIN!  

I was looking more for your insight on the Big Data stuff. Not the video editing... it just came up as unavailable - so really no video editing to 'critique'. 

/sometimes i think i must be a masochist...",3
g68wyax,ixduwo,Yah I am learning too cuz it’s my first time making videos lol but thank you I always try to look for ways to improve like I know so many good ways to add animation or maybe make it more entertaining. Yah I am sorry about this video cuz I have to record it again to remove some sensitive information but if you want to check my other video to give me feedback. Channel name is Data Lu.,1
g6716of,ixduwo,RemindMe!2days,4
g6a2mwg,ixduwo,[https://www.youtube.com/watch?v=isVHh5DV07I](https://www.youtube.com/watch?v=isVHh5DV07I),1
g68rp0w,ixduwo,RemindMe!2days,1
g68tyjg,ixduwo,"There is a 18.0 minute delay fetching comments.

I will be messaging you in 2 days on [**2020-09-24 19:14:44 UTC**](http://www.wolframalpha.com/input/?i=2020-09-24%2019:14:44%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/bigdata/comments/ixduwo/start_from_excel_to_machine_learning_in_my_career/g68rp0w/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fbigdata%2Fcomments%2Fixduwo%2Fstart_from_excel_to_machine_learning_in_my_career%2Fg68rp0w%2F%5D%0A%0ARemindMe%21%202020-09-24%2019%3A14%3A44%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20ixduwo)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1
g6a2n43,ixduwo,[https://www.youtube.com/watch?v=isVHh5DV07I](https://www.youtube.com/watch?v=isVHh5DV07I),1
g651e1v,ix6kx1,Post hard drives. Azure Data Box.,8
g6522go,ix1lhj,"The Kafka streaming architecture automates all of the steps. It will call your model, predict results, and spin up more workers if needed on demand as data lands",1
g63bydo,iwyxn9,You can find some ideas here: [https://mynoteshala.wordpress.com/big-data-project-ideas/](https://mynoteshala.wordpress.com/big-data-project-ideas/),3
g63dp48,iwyxn9,Thanks,1
g651eyi,iwyxn9,maybe you can build a ml project [https://www.youtube.com/watch?v=wCZG\_dECQc4](https://www.youtube.com/watch?v=wCZG_dECQc4),1
g60mi13,iwj0lb,"If you can tell, what it does under the hood? That will be great",1
g60pb7i,iwj0lb,"It lets users model their IT infrastructure using a web interface (or, if preferred for automation purposes, an API).

This model contains the configuration files. An engine, using a proprietary extensible database of checks, is then run through this model and the concrete configuration files, and reports then on any failed checks. Each check has a severity assigned to it, from 1 = ""you can keep that configuration if you like, but it best practice would be X"" to 5 = ""Your data is currently up for grabs"".

This differs from current approaches to this problem, which are more reactive (like SIEM). We want to put the practices that are so ubiquitous in the world of code (e.g. PMD, Coverity) to the setup of compute clusters as well, to prevent human error.

If you wish to learn more or see a demo, feel free to reach out. We will probably soon post a video about it as well.",1
g5va3l1,ivanid,"Your contribution is very good and i totally agree that nowadays both large and small companies should consider making sense of the data, and take advantage of all the information that lives in the cloud or that in the same company where it is You want to make use of data mining, you have the history that you have stored of your operations that are called the catalog of clients, suppliers, among others.

It is also important to point out that companies, apart from what is data mining, can opt for Big Data since its function, like data mining, is to analyze large volumes of data that exceed the capacity of the usual computer processing. Its objective is to analyze all the information in the shortest possible time and efficiently.

But if we ask ourselves when and who can use big data or data mining from the moment that a company and organization have data stored on their customer or supplier activity, useful information can probably be extracted by analyzing that data.

In particular when a company has its information stored, for example of its customers, when analyzing the information it can determine how to better serve them, how to have them more satisfied, get more business or increase loyalty towards the company.

So your contribution was very valuable to me since companies must see beyond what they are used to working today and that they must turn to see all these new technologies and turn to data mining or big data for improvement of your company. And not stay with the past and that each data they handle or have can get more out of it than they get today.",1
g5wdjgs,ivanid,Thank you,1
g5f278u,iti9in,Tag,1
g572bwu,is8at8,"There are entire courses in classical statistics on the use of augmented and synthetic datasets (i.e., bootstrapping and others). It's a well-established method of building models with incomplete data, but that doesn't mean the models in turn always translate to the real world. Using this method to build an accurate real-world model is an annealing process, i.e., 1) make a synthetic dataset, 2) make a model, 3) test the model, 4) update the dataset after testing, 5) update the model, 6) repeat \[3-5\] until achieving the desired performance. 

The closer your data are to reality, the fewer times you have to repeat this process.",1
g87mioz,is8at8,"I can't access your quiz anymore for some reason, but I just wanted to pitch in with another benefit: [imbalanced datasets can be fixed with synthetic data technology](https://mostly.ai/2020/05/07/tackling-ai-bias-at-its-source-with-fair-synthetic-data-fairness-series-part-4/), so human bias in data can be corrected. It will also be very important in all the [behavioral datasets collected during Covid lockdowns](https://www.technologyreview.com/2020/05/11/1001563/covid-pandemic-broken-ai-machine-learning-amazon-retail-fraud-humans-in-the-loop/?cid=other-eml-onp-mip-mck&amp;hlkid=957f385c45e7435bb521c31f30f0b7b4&amp;hctky=12332578&amp;hdpid=0d4ce30b-2850-4eea-a646-66ff53fb6395). People behave very differently during these strange times and this can throw off algorithms trained on these skewed samples.",1
g55u6b7,is6069,[deleted],1
g55zh69,is6069,"This is something interesting I found that is relavent

[https://www.pingshiuanchua.com/blog/post/using-youtube-api-to-analyse-youtube-comments-on-python](https://www.pingshiuanchua.com/blog/post/using-youtube-api-to-analyse-youtube-comments-on-python)",1
g57fbe9,is6069,"I think things skew towards open source technology when dealing with social media.  A lot of people are on YouTube learning Python, R, or Spark while not a lot of people are going to be watching videos or commenting on Vertica or Teradata as they are more enterprise players.  

So, while I am sure there is information to be gleaned, I don’t know you will be replacing Gartner with data scraped from YouTube.",1
g55yhzb,irxuvs,Yes! Totally agree. So many use cases and helps answer a wide array of questions.,1
g5b6t3e,irxuvs,"This was a great read, thank you.",1
g51vgdf,irrpah,[deleted],1
g54c3k9,irrpah,"Maybe it is because the Nvidia GPU Acceleration plugin is not part of the Apache Spark project. The most they might mention in their release notes is “Support for Accelerator-aware Scheduling” ([SPARK-24615](https://issues.apache.org/jira/browse/SPARK-24615)), and support for custom Spark `DriverPlugin` implementations. 

It would’ve been nice to have that shoutout (i.e. infrastructure for custom accelerators like `spark-rapids`), but heck.",1
g54r9e0,irrpah,"NVIDIA GPU acceleration which covers in ""Accelerator-aware task scheduling for Spark"" is also one of the great features of Spark 3.0. This Part I features covers some, I am writing another article which covers more.",1
g4xy38i,ired4x,"Been working with Flink for about 2 years now, the way I see it the pros are big, it’s pretty efficient, the DataStream API is easy to work with, and the checkpoints and back pressure mechanisms work well and helps a lot.

As far as cons, I say joining stream windows on event time can be a bit tricky (if one stream is blocked it won’t have any output), if your running on yarn cpu starving is pretty common if you have more then one task managers on the same host. And, if you’re working with Scala, some sink connectors are annoying to work with in the sense of converting Scala types to java types to SQL types. 
But these are minors considering the whole state of Flink",3
g4y6rou,ired4x,"Thank you for the response.  


I am new to Apache Flink and at the end of my project I wanted to add the Pro/Cons section, but to be honest I couldn't individuate any disadvantage aspect.  


  
In another forum, someone responded to me with some disadvantages (his opinion) about Flink. I will list them here with you and others if you find them useful.  
Seems more as subjective opinions, but can fit the Pro/cons section.  


* Less community and forums for discussion: Flink may be difficult to understand starting as a beginner because there are not many active communities and forums to exchange problems and doubt about Flink features.
* Less open-source projects: There are not many open-source projects to study and practice Flink. The lack of a code source makes it very difficult the familiarization with the most innovative features and mechanisms it offers.
* Immaturity: Immaturity in the industry is a disadvantage for Apache Flink because is a new technology and many features are constantly being updated and modified. We should avoid Apache Flink if we need a more matured framework compared to other competitors in the same space.
* API support: Apache Flink is not the best choice if we need more API support apart from Java and Scala languages.
* Data representation: Flink uses raw bytes as internal data representation, which if needed, can be hard to program",1
g54l8jz,ired4x,Part of the reason you're not getting any response is because you aren't asking about a use case. The question is too broad to answer. The answer you received from another forum wasn't helpful. Read why https://www.jesse-anderson.com/2017/07/this-is-useless-without-use-cases/.,1
g4tjicx,iqmvfm,"I liked most of the content but the omissions stood out: why Hive &amp; Impala, but not BigQuery, Redshift, Snowflake, Teradata, Netezza, DB2, or CitusDB?",2
g4ozgnm,iq4bj4,"I’m sorry for this potentially nonproductive response, but: welcome to big data, things are messy.

You’ll need to figure out what criteria matters to you. The first thing that comes to mind is perhaps some kind of sentiment analysis. This would take into consideration the thread and see if it was actually about COVID or not",4
g4q83y7,iq4bj4,"Building on the other guy's ""welcome to big data"" answer, maybe you should just scrape from feeds you trust contain good information.


If you can't do that, then you need to think about what makes a tweet ""fundamentally right"" for you. You could then do things like looking for mentions of words, entities, pattern matching, semantic distance. Yada yada yada",3
g4j57zm,ip9m1p,Is it free?,1
g4iply5,ip9ipx,[deleted],1
g4ipym2,ip9ipx,haha I guess I should use the dude voice. I just can't hear my own voice.,2
g4mob5j,ip9ipx,Really nice intro for Data Engineers to know what their ML counterparts do !,1
g4mw9zr,ip9ipx,Thank you,1
g4hq6ac,ip2w87,"Clickhouse, because it's fast, SQL-compliant, MPP database with good number of features. Not very mature though.",4
g4iou4m,ip2w87,"oh, that's cool! How do you find the clickhouse eco-system? Snowflake  seems to have tons of integrations, partners, etc that are pretty powerful. Does something similar existing for Clickhouse?",1
g4i2n8p,ip2w87,"Oracle DB. Because legacy code base. 😭

Slowly converting to Spark and Hadoop.",2
g4i9598,ip2w87,"Last job we were using Redshift, new job we're using Snowflake (I start in about 2 weeks tho).",2
g4i11kx,ip2w87,"Azure Synapse analytics.
All the other tech were MSFT so is our data engineering framework.",1
g4iyeq4,ip2w87,"""Homemade"" data warehouse : A sorta lambda architecture with Spark/Hadoop for batching, and Kafka/Spark Streaming for streams.

Data is served to users through various PostgreSQL dbs,
and we have currently an Elasticsearch rolling out for timeseries and real-time analytics.",1
g4evxet,ion5we,Also interested!,1
g4f9cvh,ion5we,Me too. I have strong sas/sql/unix/python programming skills and am looking to learn big data to add to my repertoire. I would also like to contribute to a big data project if possible - show some productive activity during 2020.,1
g4f9qa8,ion5we,Following,1
g4gordm,ion5we,"I would recommend checking out [Presto GitHub](https://github.com/prestosql/presto) as well as the developer page containing [philosophy and how to contribute](https://prestosql.io/development/). Presto is a distributed analytics query engine that uses ANSI SQL, meaning it follows a standard querying language and Presto itself doesn't store the data but rather is just an engine that can perform complex queries of RDMBS, Hadoop, and other NoSQL data stores. Since you touch [so many types of databases](https://prestosql.io/docs/current/connector.html), it's easy to start working on the database/data model connector that you're familiar with and that will help you understand Presto basics. Also if you're wanting to break into new data stores, you can then transfer over contributing to other connectors as you feel like. Otherwise, if you want to learn about data engine specifics like how the parser, query planner, and optimizer works, you could always work on core Presto.This project is used and contributed to by a bunch of high profile companies like Uber, Netflix, LinkedIn, and was born out of Facebook.To get a history, there are a few recent podcasts that have come out about this:

* [https://www.contributor.fyi/presto](https://www.contributor.fyi/presto) \- This is an interview with the founders of Presto talking about the origin story and where things are headed
* [https://www.dataengineeringpodcast.com/presto-distributed-sql-episode-149/](https://www.dataengineeringpodcast.com/presto-distributed-sql-episode-149/)  \- This is an interview with Presto co-creator Martin Traverso jumping into where Presto fits in the Big Data architecture.

I will also be starting running a twitch show that highlights the efforts of our contributors by discussing their PRs, talk Presto news, etc... More to come on that later but here are a few test interviews we did for our first episode.

* [How do you make a thriving developer community around Presto?](https://www.twitch.tv/videos/729152420)
* [Favorite part of implementing a feature?](https://www.twitch.tv/videos/729201925)

To find out more, find me on the [presto community slack](https://prestosql.slack.com/) just look up ""Brian Olsen"" or you can post in the #general or #dev channels. This slack [community is 2,300 strong](https://twitter.com/prestosql/status/1278393800092643328) and growing. This includes the founders and many of the early contributors to the project who are personally invested in nurturing and growing the community to make it the most inclusive and make presto the best analytics engine.",1
g4ed7yw,ioh9ts,Umm...what else would you optimize for?,1
g4dsxp3,ioetic,"Just curious, why is it that all these companies don’t have R listed, but Python over R? Is python really that much more superior? 

I’m a predominantly R user myself, as far as statistical language. I know SQL, and a small amount of SAS. It’s surprising to me that python is listed and R is not. 

Just seeing if anyone had any insight into this.",2
g4e7ayw,ioetic,I am curious too. I’ve started learning python lately because I’ve been so frustrated of the lacking areas of R,1
g4hdkn5,ioetic,"Python is the most common, but having R in your skillset is valuable too",1
g4cv3k5,io955z,"I have worked in big data for a while, and streaming and real-time projects for the last 5 years or so.

Kafka is very popular for large platforms, Kinesis is also popular.

Hadoop platforms are still prevalent in large orgs, but cloud native is growing faster.

Spark is very popular

Most devs I know use IntelliJ and git , CI/CD depends on the org.

Cluster size - I have worked on everything from server less with 3 containers, to 5000 node clusters.

Data size - if it’s small enough to fit on one machine it’s not big data (but people will still act like it is). Most frequently I am working with GB to TB per day.  Largest I personally have dealt with is .8PB per day.",2
g4fweps,io955z,Thank you for sharing :),1
g4be5sr,inz7rg,https://www.youtube.com/channel/UCR5fQQoUywhm3tE3O0HSVVg,1
g42oamm,ims0ji,"Not sure if this is what you are looking for.

""FoodData Central"" https://fdc.nal.usda.gov",2
g3wbzto,ilrxl0,Who are y’all dating ...,1
g3y3daj,ilrxl0,Always thought data mining was just scrapping data from other places but at a larger scale.,1
g3vcs2z,ilrxl0,Omg thank you. I just joined a data consulting firm in sales and this is very helpful. Any others to add that you or others can think of would be very appreciated.,0
g3u2ffu,ilr90i,"If there is any girl DE who knows all the above , lease ping me , I want to marry you.

The list is all exhaustive ..
 So some people know all these stuff ?",1
g3ttrde,ilnsgf,"So lake = dump all data in the cheap cloud for the future
Warehouse = ETL with a plan for today's use",1
g3rtbi1,ilen9c,Stop screaming ya pansie,1
g3r9npl,ildec4,"To do what? The question is too vague. Is it speech recognition, translation, etc.?",3
g3r9wvf,ildec4,By process I mean run transformations on the data and put it through a Deep Neural Network.,1
g3rnx1u,ildec4,"I think the pre-processing of the audio data will be relative lightweight when compared to the actual forward pass of the Neural Network (specially for very large models). So I would  suggest to focus on the hardware that improves the network performance.

&amp;#x200B;

A GPU is great to speed up the Network's calculations, where a mid-range gaming GPU will outperform a high end CPU by 10x easily.",1
g3tsaan,il8w00,Does it mean we can apply data mining technique on football games result data and make bets based on that?,1
g3v9gkd,il8w00,"Yes, absolutely!",1
g3oivy7,ikzecl,"Lot of words and no concrete examples of anything real word. Still, decent read.",1
g3kiq6f,ikgfdd,Not bad but would be nice to see a CDP deployment on OpenShift or CDP Public deployment war stories.,1
g3dc5qw,ijeiqd,Following,2
g3ek911,ijeiqd,"Your data volume no where approaches big data - and is *fully* within the realm best served by RDBMS.  The data is structured, and te tasks you've identified are fully capable of being run off of a RDBMS.  There is *no* need for any big data technologies at all here - unless your intent is to learn big data technologies.

So really - what's the purpose of this data repository - the tasks identified above, or to apply the tasks above using big data tech?  If its just the listed tasks - stick with a RDBMS.",2
g3em5hm,ijeiqd,"Completely agree here - note that you’ll likely need some indexes to get good performance on queries but it is well within the capabilities. If it’s an option for price / platform, Microsoft SQL does have Python and R interfaces for ML.",2
g3g4ear,ijeiqd,"Thanks for your answer! My purpose is to apply data analysis, data mining and deep learning.

&amp;#x200B;

But as you and u/guacjockey said, you feel I am better served with simple R or python interfaces for a simple RDBMS right?

&amp;#x200B;

And what about the data not leaving provider institutions? not pushing my luck here, but do you reckon there are alternatives in the market to support this case and RDBMS?

&amp;#x200B;

Thanks for the help!",1
g3h6zei,ijeiqd,"It gets tricky - depending on what the source data is stored in you might be able to create a linked server / foreign-data-wrapper/ etc. But the difficulty is at some point the data will leave said institutions during query, you just may not be storing it. The question then comes of whether performance is adequate for whatever analysis you want to run (and/or without blasting their servers in the process).",2
g33fq4c,ihvaid,"If you want ""tools that will maintain their relevance a few years down the line"", you're investing in the wrong industry at the wrong time. We've just hit the data boom. Things are moving fast.",2
g33mnvc,ihvaid,Cloudera = dead bro,2
g39q6md,ihvaid,So is Aws EMR or azure hdinsight pave the way for Hadoop ?,1
g30cisl,ihicop,That site has toxic ads. Can't even seem to click past them.,1
g2u2qal,ig7tk4,I think Holden Karau is doing similar things. You could ping her.,1
g2w8nl2,ig7tk4,"Thanks for the recommendation, Im on it.",1
g2perbi,ifr81g,"You can define the schema prior to load and run the various import modes to determine the rows that work. You can also use the `columnNameOfCorruptRecord` option to save off (most of) the bad data. Note however that it doesn’t count extra / fewer columns as corrupt. To work around that, you can do the load everything into a single column dataframe and manually process the data (ie, perform splits on the data).

Beyond that, the GreatExpectations package will generate Spark usable code to further validate data.",2
g2jmryu,ieqj93,Data discovery refers to finding the source of truth about your data (think data catalogs and metadata about your data) whereas data profiling relates to summarizing or collecting statistics about your data.,3
g2jmfnb,ieqj93,I use the term with my network engineers when we are figuring out what data sources are available and what columns they have.,2
g2irfx3,ieqj93,"I don't know the answer but I wanna add that this community is very offline. There are barely discussion threads, comments interactions. I used to love it but these days is getting more and more of a ""spam"" target.  


Maybe I am wrong and this is only my point of view. Hope you will receive your aspected response and have fun with data.  


Best regards",1
g8puhlw,jahgte,"I'll speak very generally. Obviously each position is going to differ, but here's a starting skillset that I see a lot of the people we interview have:

Language:  Many people see to be using Python now. I'm old school and before ML took off used C or Java for all my workhorse apps.  But python seem to be what a lot of data scientists coming out of school as well as in the field are using to analyze their data. (R is great for statistics as well and seems to have replaces SAS in a lot of way)

Frameworks: There's quite a few machine learning libraries to play with if you go with Python.  From a 'big data' perspective, learning Spark as a way to access large, distributed datasets is a good start. It can take a bit to 'master', but at least learning the concepts behind it and distributed data would go a long way. 

You can use the above technologies with one another. Spark with Python (PySpark) works really nicely.  Many spark developers also use the Scala language, but for whatever reason I see a lot of data scientists coming out of school using Python, not Scala (or Java) for accessing Spark.

Something like Jupyter is an easy way to get started with Python (and other languages) as well as accessing Spark via Python.  Our data scientists use Jupyter notebooks as a way to access, manipulate and visualize data all the time.  Eventually any models they create this way gets run outside the fancy Jupyter GUI environment when productionalzed, but it's a great tool for getting started and creating your models and seeing the results.

There's probably no wrong answers here, but that's what I've seen our own data scientists use as well as kids we're recruiting coming in with.",1
g8otn5a,jac1go,"Bounces through tracker, gets ad blocked.  Share actual post URL.",3
g8nwajw,ja2aja,Pfffffffffff,1
g8n9bpt,ja1hg8,"My data engineering team currently manage tens of terabytes of data in the healthcare space, mostly structured and semi-structured data. Lots of batch processing and an increasing amount of stream processing and/or micro-batching.

The technologies we use and look for experience with include Git, Bash, Python, SQL, Docker, Airflow, Snowflake, Kafka, Kubernetes, and several AWS services (EC2, EKS, S3, EFS, SQS, and AGW.)

We don’t currently use Spark but you can’t go wrong learning it and I would definitely prioritize it. We are also considering adding IaC knowledge like Terraform, not as a requirement but a nice to have, but this is more situational.

Industry specific knowledge can be helpful—like HL7 or FHIR for healthcare interoperability. Same for experience with master data management and compliance initiatives—although these can certainly be learned on the job.

The field is very broad. Might not hurt to look at job ads in regions you’d like to work from (or remote) and see if there are particular trends.

Best of luck!",3
g8nhmc9,ja1hg8,"Thanks a bunch for the great info! I am in banking and financial infra. We have tables that are in the 1tb range (1.5 to 5 billion records) with a mix of structured and unstructured data.

My main technologies are hadoop framework using hive and Unix mainly. I have basic exposure to spark. My skills with java and Python are minimal professionally. But I can understand what I'm looking at and make code changes if needed. I am also familiar with git and how it works. We mainly use in house tools for or ETL processes and version control. So I'm definitely going to get very familiar with git.


Like I said, I have only 3 ish total years of experience. So I'm still super new. Im also very young. Its very daunting and I feel the imposter syndrome creep up when I look at the trends and at the job openings I pass by.",2
g8nsiny,ja1hg8,"Get good at Python, SQL, and Spark. Then learn AWS products like Lambda, Athena, Redshift, Glue and of course, S3. Study machine learning - look up the book Real World Machine Learning as a starting place. Another useful language might be R, but is concentrate on the above first.",1
g8nx6ue,ja1hg8,Thanks!,1
g8nv48z,ja1hg8,"Learn Spark (Scala/python). Along with that spend some time on Azure or AWS... Having good understanding of cloud architecture is becoming must. Having some streaming knowledge with Kafka, Spark streaming would be huge add on.",1
g8nx8u4,ja1hg8,Alright I'll check those out! Thanks!,1
g8nixhl,ja025x,"It should add quite a bit to the existing Flink play. This video helped me put it in context:

[https://www.youtube.com/watch?v=PUI3I\_v8q8M&amp;feature=emb\_logo](https://www.youtube.com/watch?v=PUI3I_v8q8M&amp;feature=emb_logo)",1
g8nj0mr,ja025x,"Also, not every company needs near-real time so that should be a good indicator of whether it's relevant to you. If you have Spark Streaming today ... maybe:-)",1
g8ox8dy,ja025x,"Yeah ok, seems kinda pointless to acquire though",1
g8oac3y,j9rzml,"If big data were too big, people would already be scared, and therefore dishonest.

But it rolls off the tongue like a good headline, so it may be worth enough to Murdoch's people to buy you a small farm, and we definitely need to get this pesky fertilizer out of the air.

Good luck.",1
g8oc3rl,j9rzml,"In this context of too big I mean are we even aware of what data we are giving up how, why, who and whether awareness make any difference to the perception we initially displayed. In fact a lot of my data is suggesting a fear for data which is beyond the control of people despite their depth knowledge on the topic and thus they become tolerant this is a natural human phenomenon known as loss-aversion this is most apparent in gen z who have grown up in an era of “false privacy” whereas gen y display actions and attitudes of “data activists” these are data haters and will avoid or disrupt the authenticity of their own data. I personally believe there is an ignorance to this debate a lot of people don’t have enough knowledge in the topic, from what’s possible to their rights. This research is to identify the difference knowledge makes. See I don’t believe it’s too big per say it is very beneficial but I believe the advances in technology have far surpassed the law which enables companies to take and use data easier than most would think which to me is still not okay. There is a higher priority on profitability from data collection than there is on privacy.",1
g8odmqv,j9rzml,"It's an ""indirect self-reference"" issue. However one asks ""has diversifying analogies of asking, includes those that are undisclosed, including old-fashioned surveillance, already got  scary?""

They may think ""asking whether something like asking already scares me? that sounds like asking to become pretty scary, we'd better sound brave or the scary man will hurt us - eventually - no, let's just leave.""

Indirect self-reference can be a source of intrinsically biased sampling.",1
g8knaov,j9lwmj,"Cloudera quick VM, the first option, is a good option to start and play with a Hadoop cluster. 

Option 2 isn’t so hard, but always make sure that the requirements are met before installation, so you don’t spend days in troubleshooting to find out it can’t work.",3
g8kshno,j9lwmj,"**What I did:** 

1. Downloaded Ubuntu ISO file
2. Created 3 Virtual Machines 1 or 2 GB RAM + 1 Core CPU + 10 GB HDD + Networking ON
3. Installed Ubuntu in all 3 machines
4. Created Passwordless SSH connection between all the three Machines \[all combinations\] . (Most important step)
5. Downloaded Apache Hadoop form [archive.apache.org](https://archive.apache.org)
6. Copied Configuration Files + .bashrc files from some repositories on Github
7. scp hadoop and files to all the three machines

Thats all. Then you can just start working around it. If you understand these steps. Installing other things such as Spark, Hive, Hbase, Zookeeper etc will be easier.   


I am myself new into it but tried installation a few times.",2
g8kkb6x,j9lwmj,"I am a beginner and have tried installing multiple times by the 2nd method. Never was able to get it to work... Got disheartened and never wanted to try again. 
I did end up finding a virtual machine with the ecosystem already installed... I downloaded that and it worked real good. Maybe you should try that too! Saves you the trouble of trying to do it yourself... Unless you want to explore!",1
g8klx2e,j9lwmj,Would you please tell me specifically which virtual machine and ecosystem u used?,1
g8kn43v,j9lwmj,Probably from a relevant online course.,3
g8kpj5t,j9lwmj,would you like to suggest one course which you would recommend to beginners ?,1
g8kr4xs,j9lwmj,"I've looked through my coursework from the past, and it seems like The Data Scientist's Toolbox from Coursera still exists! I'm not sure if they offer premade VMs as part of the coursework, those memories might actually be from other courses I did (involving genomics, so probably not very relevant to you).",1
g8k9b9z,j9ist1,"I think before you try read a specific technology, you need to look at the volumetrics of your use case and identify your driver for a big data solution.

If you come at this with a mentality to learn ""a product"" you won't gain much depth in this field. If you were to instead look at the problems which were solved by a big data solution you will understand the products better in their context.",2
g8k3nvp,j9ist1,"Spark, kafka, hdfs, cloud native storage like adls, s3 and lot more.",1
g8k43ke,j9ist1,"Damit.

Are there not 1 or 2 that are standards or do i need to learn all of them?

I am actually playing around with Tableau.",1
g8k4b12,j9ist1,"Well in that case, it is Apache spark. Spark is the distributed data processing framework, basically you have to code in programming language like scala, python, Java etc. Tableau, power bi are more like for visualization and lighter data analysis.",3
g8k4r4s,j9ist1,"&gt;Apache spark

Does Apache Spark stand for Big data as Photoshop for Photo editing?

And thank you so much for your help it was really very useful because i got confused on searching on the net. Everyone tells you something different",2
g8k4zb1,j9ist1,"All good, you can start reading into it and you will get a better understanding. And spark is pretty much like Adobe photo shop for editing, and prior to spark, it used to be apache hadoop.",1
g8k52xq,j9ist1,"Ok 

Thank you my friend and have a nice day",1
g8mwfxt,j9ist1,"Because big data is so vaguely defined, that's why everyone tells something else. 
I am genuinely interested what volume is for you big data. 
Spark just won in the marketing buzzword claims.",1
g8k6n26,j9ist1,Google CNCF Landscape.,1
g8k861h,j9ist1,"As others have mentioned there isnt a single answer. Although Spark is capable to handle a lot of the scenarios commonly encountered in data pipelines and even extends further (supports some ML algorithms).

I think its a good starting point and then you could easily move to other frameworks. A lot of them are built on Java/Scala so its also a good idea to get familiar with those languages",1
g8jl7ys,j9g61r,"There’s a lot about that question that I either disagree with or is out of date. It’s worded in such a way that it sounds like it’s based off a certain set of material. 

That said, if I were to take that question as is, I’d reference the particular programming model and the particular storage model that Hadoop implements and discuss the advantages (and possibly disadvantages if you wish). The Wikipedia page should give you enough info to dig further.",2
g8jqm85,j9g61r,"This is a badly worded question, and I agree that it's a little out of date. If I were asked this, I would say something about 

1. Bringing compute to the data

2. Schema on read

However, I suspect that the ""right"" answer is in your notes somewhere. Good luck!",1
g8ca7o7,j8o65q,Beautiful! Incredible having such diversity (in countries) in the latter years. I wonder if it is due to poor tracking of stats in other countries orgs?,2
g8dvyya,j8o5vm,"shill.

Obvious shill is obvious.",2
g8cd8ae,j8o5vm,I'm trying to separate the hype from the buzz.,1
g8bxobu,j8j0zb,"Cloud companies want you to they are the ""easy"" button as presented above. The truth is these same companies have run the numbers and have proven that investing in your own infrastructure and not paying rent to a third party makes way more financial sense and provides the benefit of meeting performance goals without losing control of your data.  Companies that continue to just pay these high monthly cloud fees will eventually be bankrupt.",1
g8dtxw9,j8j0zb,"That's interesting, do you have any source or article about that?",3
g84jwms,j7fo0s,"You would need to use delta core package when starting your shell:

pyspark --packages io.delta:delta-core_2.12:0.7.0 --conf ""spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"" --conf ""spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog""

Or you can also include the libraries required in your SBT project: 

libraryDependencies += ""io.delta"" %% ""delta-core"" % ""0.7.0""

ref: https://docs.delta.io/latest/quick-start.html",1
g84jxks,j7fo0s,"**I found links in your comment that were not hyperlinked:**

* [io.delta](https://io.delta)

*I did the honors for you.*

***

^[delete](https://www.reddit.com/message/compose?to=%2Fu%2FLinkifyBot&amp;subject=delete%20g84jwms&amp;message=Click%20the%20send%20button%20to%20delete%20the%20false%20positive.) ^| ^[information](https://np.reddit.com/u/LinkifyBot/comments/gkkf7p) ^| ^&lt;3",1
g84uh68,j7d8pm,I learn a lot from my coworker that are phds and I try to summarize in 10 min videos on my YouTube. But it’s hard if you don’t work in the field. Maybe medium blog post but you also need to know what to search for,1
g840m8z,j7ccwy,"15 minute intervals might be hard to find...

You might have better luck asking in one of the weather or meteorology subs.",1
g842d78,j7ccwy,"""meteorology subs""? What are those? subreddits? btw I tried also emailing the company publishing the hourly data from the link I attached, no response yet. Where else can I search?",1
g84b938,j7ccwy,I mean weather-oriented subreddits.,1
g830rgh,j76rnj,"where are you getting stuck ? what steps are you following ? ( if you need a development environment, a quickstart vm would be a better idea )",1
g831fs9,j76rnj,"I want to run ambari-server and the other components on a single node so in the hosts section i am giving the fqdn of the same node. It just fails to register. The exact same procedure has worked on 18.04. The steps i am following for this are:

\- add own ip to /etc/hosts

wget -O /etc/apt/sources.list.d/ambari.list [http://public-repo-1.hortonworks.com/ambari/ubuntu18/2.x/updates/2.7.4.0/ambari.list](http://public-repo-1.hortonworks.com/ambari/ubuntu18/2.x/updates/2.7.4.0/ambari.list)

apt-key adv --recv-keys --keyserver [keyserver.ubuntu.com](https://keyserver.ubuntu.com) B9733A7A07513CAD

apt-get install ambari-server

ambari-server setup -s

ambari-server start

apt-get install ambari-agent

\-Make sure under the \[server\] section in  /etc/ambari-agent/conf/ambari-agent.ini, hostname is localhost.

ambari-agent start

THen proceed with the installation in ambari web ui

&amp;#x200B;

Since i need pseudo distributed i tried installing hadoop and hbase manually but hbase master gets stuck in the OPENING status everytime even though i followed every step exactly as seen in guides like this:  [https://computingforgeeks.com/install-apache-hadoop-hbase-on-ubuntu-linux/](https://computingforgeeks.com/install-apache-hadoop-hbase-on-ubuntu-linux/)",1
g836zzr,j76rnj,"Instead of this get a quickstart VM, that should get you going as all you want is a single node",1
g83847n,j76rnj,Can you suggest any based on ubuntu 18? Or any other popular ones?,1
g838fa9,j76rnj,Install virtualbox on the OS you have and then download a quickstart VM,1
g82q56p,j73yob,"Not on the data side - and more like 4 years (plus another 16 building Linux + Unix) - don’t trust vendors when they say “cheap hardware is fine”, because they will then say - “so with cheap hardware at X spec, you need 400 servers”, and then also... “due to your large number of servers, we’re increasing the price per server by 2.5x to X”.  
Also - be open to change.  Just because one tech was the best 4 or 6 years ago - doesn’t mean it’ll stay being the top.  It might turn out that something small turns out to be the big winner years later, so just be flexible and open to change.  
Latency in jobs kills - if you have high random or i/o latency, this will basically kill the whole purpose of said bigdata cluster, regardless of what platform it is on.  
Don’t forget about the need to do development on that data and platform, which may be actually represent a fairly large challenge if you need access to all that data to do data science exploratory or development level work.  
Just my 2c...",3
g81o16w,j6zlaf,"Snowflake is a cloud version of Teradata. Apply all the same reasons you'd like to use Teradata but without the infrastructure investment or support overheads.

Also apply many of the same reasons to not use Teradata to Snowflake: it's a proprietary format and the cost ""savings"" are by putting all your data in their tenancies sharing with other tenants. It's also a duplication of data, if you use Snowflake as the storage location for your DW users, you'll need something else to service your Data Science users.

In terms of why not just an Object Store ... look at reasons why you need a Data Warehouse at all. Many if not all are still valid, even as the interest in Data Science, ML and Deep Learning has grown. Far too many organisations BI capabilities is an excel spreadsheet backing onto a DW of some sort.

Last point: it looks like a lot of the Snowflake adoption has been by small to medium business or small parts of a large organisation. Even the large firms who've signed on are generally well below a petabyte of data ... many business users just swipe there credit cards for an initial trial. When you have petabytes of data, the shared tenancy model starts to fall down - you spend more time waiting for resources and the ""noise"" from neighbors can slow down your entire query.

So to your point, why not an object store or Snow Flake ... you'd be more likely to have both, with the object store for your raw and semi-structured data and Snow Flake for your transformed data for serving.",1
g82qj02,j6zlaf,"If you have a load of money they’ll do a private instance in your own AWS/Azure account, but last I heard they didn’t advertise it. May have changed since IPO. It’s more than you want but less than building your own with all those capabilities if you have a good use case. I think they do oversell into orgs that don’t always need it.",3
g821hue,j6zlaf,What if most of your data isn't a blob?  What if you can't hire people skilled enough to take stuff out of a data lake?  What if you YOLOD your Roth into $SNOW?  There's a million reasons.,1
g81gvph,j6uqpd,"Instead of HBase, consider Apache Ozone.

I've written many HBase apps but there's one thing HBase is terrible for ... storing and receiving Binary. The GET and PUT APIs are all byte array based, no streaming IO patterns.

Consider Ozone, you can run it in a docker environment for testing, it has an S3 API and will replace HDFS. You can use a traditional operational store for file metadata as the volumetrics are likely heavy on the image sizes but small data on the number of images (millions?)",1
g82hqsl,j6uqpd,"Thank you, but I am supposed to stick to hadoop.",1
g82hz9v,j6uqpd,"Ozone is a sub project of Apache Hadoop ... moreso than HBase which is a different top level project.

https://hadoop.apache.org/ozone/",1
g7zc0lg,j6kezj,Usually there is some kind of a broker in between. We use mqtt for example but it can be anything else,3
g7ze75h,j6kezj,Thanks for your inputs,1
g7ze0ic,j6kezj,"Splunk Forwarders, Elastic Beats, MiNiFi but theres some specialists in the IoT landscape though I haven't had experience.

Some features to look at:
- what operations, like pre-filtering, can I do before data hits the server?
- what can I send data to? HTTP and Kafka are two which come up
- Can I manage my fleet of sensors and is it CLI, REST and/or UI
- What is the process to upgrade sensors, can I do it centrally and target different configs to different classes of sensor. 

Generally updating sensors config won't be atomic, they usually have some kind of heartbeat and may be offline before they update ... eventually.",3
g7zeg3r,j6kezj,"Oh nice , I will look at each point separately , thanks hopefully this will be helpful to get me started.",2
g7zego3,j6kezj,Do you guys recommend any videos or books to get started ?,1
g7tunia,j5ls6v,just eww. I feel like I need to take shower.,1
g7tzuwa,j5ls6v,"At the concept, or the proposed options? :)",1
g7rhcvs,j5cnhw,"Apache Nifi can work with a schema registry I believe. So, you can store your schemas in the registry and use them for each data flavor you receive.
Nifi can also work with Kafka. So, why not push the data from API s into single or multiple kafka topics, have a consumer dump them as is with some audit info into a db - this will serve as your original output backup from APIs.

Have a second consumer to read the kafka messages, apply the schema over it, aggregate it and push to a sink/destination.

This would scale nicely and can work in both batch and realtime if the source frequency changes.",1
g7qsv7l,j590sh,Chellenges?,1
g7qk5dg,j57662,"Snowflake makes working with our piles of semi-structured data very easy (and the syntax is clean and intuitive). You can also create fairly complex JSON objects with Snowflake.

I haven’t worked with any significant volume of unstructured data with Snowflake.",3
g7qjp5k,j57662,"It’s got some nice json querying functionality, if that’s what you’re driving at.    I was able to build a POC off the twitter API and do some hashtag aggregations pretty easily w/o any prior snowflake experience.",3
g7rbdtf,j57662,"btw, building an active community at /r/snowflake, if you'd like to join :)

(as other comments say, Snowflake has some great JSON capabilities)",2
g7qn0z9,j56jpt,"Can’t go wrong with a classic.

https://www.oreilly.com/library/view/hadoop-the-definitive/9780596521974/",1
g7p4xgd,j50jvn,"Read Google's MapReduce paper from 2004, and then read papers that cite MapReduce until you reach modern research. This was basically how I started in undergrad before actually beginning to design &amp; build a next-gen data engine with my lab as a graduate student.",0
g7my5l0,j4rc2h,"Is this a sort of basket analysis? Where you have sets / collections of items and want to group collections that share the same items? To me the thing that will alter your approach is how many words does each item have. E.g are they distinct “Apple”, “Pear”, “Banana” or are they more variable “2 months consulting”, “a bushel of apples”, “a desk of Cheetos”, “a bag of apples”  and similar items some variation across sets. 

Depending on the above. You could look at either: 
- TFIDV vectorisation and cluster or topic modelling approaches: http://brandonrose.org/clustering_mobile
- set ratios such as Jaccard Score: https://www.statisticshowto.com/jaccard-index/
- or network theory and modularity: https://www.sciencedirect.com/science/article/pii/B9780124079083000091",1
g7tjes6,j4rc2h,"There seems to be a mutiple methods to do this see

[https://www.yworks.com/pages/clustering-graphs-and-networks](https://www.yworks.com/pages/clustering-graphs-and-networks)

[https://datavizcatalogue.com/methods/network\_diagram.html](https://datavizcatalogue.com/methods/network_diagram.html)

[https://www.jasondavies.com/wordcloud/about/](https://www.jasondavies.com/wordcloud/about/)

&amp;#x200B;

&gt; are they more variable “2 months 

I dont' understand why this would matter? In op I said this is an item, 

so “a desk of Cheetos”, “a bag of apples” would each be 1 item, “a bag of apples” is 1 item

So I don't understand why it'd matter

&amp;#x200B;

Does it matter in that that method mentioned [https://www.kdnuggets.com/2019/12/market-basket-analysis.html](https://www.kdnuggets.com/2019/12/market-basket-analysis.html)",1
g7tofjz,j4rc2h,"Ok, just needed to clarify as you would need to to clustering of items otherwise.",1
g7bpcc9,j3fkwj,"I developed a CI/local workflow to test Airflow DAG integration tests. I used some dockers and makefiles to create the eng. For data I used simple data (csv, txt) to test transformation etc.. for my purpose work great because I now can validate a complex pipeline local or on Github. I'll upload in a side project to discuss that!",3
g7c93c7,j3fkwj,We have been building CI workflow for data pipelines for years??,1
g7ct58n,j3fkwj,"Qualifier being ""directly from PRs on GitHub""",1
g7dr0uu,j3fkwj,"“We have been building CI workflow for data pipelines for years, Directly from PRs on Jenkins/AzureDevOps/Gitlab/GitHub” :)",1
g7e81py,j3fkwj,Fair enough! Haha,1
g7fbj1o,j3fkwj,"Hey everyone, I’m attending a webinar on starting a side hustle this Sunday to try and profit off my data analytics skills. Not too late to sign up! https://www.eventbrite.com/e/side-hustle-101-helpful-tips-and-strategies-from-successful-entrepreneurs-tickets-119201684447",1
g7bcgrb,j3dnrv,Looks interesting.,1
g75rhml,j2guu5,"Some pros and cons, think of NiFi as really good at moving files, with some flow control logic, that prefers fewer larger files.

It can do a lot of different scheduling methods and lots of different connectors.

You can do a lot of different format conversions with integration with a schema registry, some data manipulation but it's can't do Complex Event Processing: coordinating an event based on multiple streams of data with state.

It is good with variable size use cases but use cases with millions of 10KB messages sent per second are likely better on Kafka. NiFi works best by grouping messages like that for larger event sizes with slightly higher latency but better throughput. You can also send GBs in a single file which would qualify out Kafka.

You can get a single instance of NiFi running pretty simply but setting up a cluster can be complex and you'll like prefer the bundles distro of Zookeeper, Ranger and NiFi as part of Cloudera Flow manager. Having used the bunded version in CFM 2.0.1 for the first time, I really enjoyed the Ranger integration ... by comparison setting up authorisation on Process Control groups without it was challenging.",4
g7786hp,j2guu5,"Having looked through your requirements I suggest you check out how NiFi registry works first. Supporting multiple clients with customized flows may quickly become a pain depending on your setup.

If you decide to keep it all in one flow - it may turn into a bunch of RouteOnAttribute processors, cluttering everything around.

On the other hand, if you decide to keep each client's flow separately - rolling out updates to common parts of the flow and just keeping them all up to date will be a nightmare.

Overall, in my limited experience, NiFi is a pretty good tool as long as your flows are small and simple, without much branching in the logic. It can handle large files very well and is pretty stable (although you may have trouble setting it up as a cluster).",2
g77tn9l,j2guu5,"Registry gets my vote too - with nipyapi as a nice python library for automating CI/CD pipelines. There are some good blogs by Pierre Vallard on this. 

https://pierrevillard.com/tag/nifi-registry/",1
g78q7qk,j2guu5,"Nifi is bottlenecked by the way it's implemented. 

First, nifi server is a single process and as the number of pipelines and flow contents increase, the whole system becomes unstable and crashes. Hence, it's very difficult to scale. We had to create different nifi clusters for different applications. 

processes which are cpu bound like changing flow file data formats, compressing, uncompressing bottlenecks the whole pipeline. 

Security reviews can be difficult as anyone having access to the nifi cluster can download/view the contents of flow files(abstra tion of smallest unit of data in nifi world) in between processes 

Having worked on a petabytes scale data pipeline, and after lots of time wasted on Nifi, we choose to use Kafka (for file metadata- filename, hdfs location), hdfs (for actual file content), spark application ( can be streaming / batch ) for distributed processing , hive for data warehouse. 
All the components in the above pipeline can be individually scaled as per requirement. 

Recommendation : if you have few GBs of data processing which doesn't involve cpu bound activities like uncompressing, compressing, encryption, decryption and just want some kind of a data integration tool to move data around with minor transformations, feel free to use Nifi. But If you want a scalable system which should support 100s of data pipelines, invest in building generic application using some distributed processing framework like spark.",2
g73yd10,j282ob,"You need a strong culture of accountability and ensure the idea of being audited isn't some remote team who are primarily doing network event detection. It should be team leads and peers or an independent team co-located.

Securing and masking PII data outside of these teams is also key.

For apps teams, separation of concerns is still fantastic.

Or just put all PII data in an object store and give everyone in the org read access /s.",3
g75i0bo,j282ob,"The data is already out there, so it’s either me dealing with their data or it’s someone else.",1
g73chuz,j20pf6,A tear jerking moment for all the SQL Folks !,2
g73cud9,j1zcz9,Thanks nice one !,1
g71xuc2,j1y4xq,I'm interested!,2
g71ykdz,j1y4xq,Sweet! Sending you the link via PM.,1
g726eok,j1y4xq,How does it work? DDL tracking?,2
g728lxr,j1y4xq,"Not that advanced yet. Right now, it’s all manual upload of meta-data and more of a centralized knowledge management tool entered by users. I tried to make it look almost like a social network to encourage use by everyone using the data, so you have common work, questions, analysis all in one place around the data table! 

After getting feedback on this version, my next step is building out common integrations. Let me know if you’d like to check it out :)",1
g7ygyyl,j1y4xq,"Hey ! I'm interested! 

thanks",2
g7zp18y,j1y4xq,Sending you the link via PM!,1
g76u6sq,j1va2s,"Check out this blog: [https://www.inzata.com/data-analytics-blog-big-data-analysis-software/](https://www.inzata.com/data-analytics-blog-big-data-analysis-software/)

Inzata does sell a software, but they also have a weekly newsletter created specifically to educate people like us, not to try to sell their product. You can subscribe to it on any of their blog posts.",2
g71meko,j1tdqa,"My study from 2004 showed that your study from 2016 is bullshit. Anyway, good try to sell your certificates.",3
g6yj3dx,j1dssf,Tldr shit in shit out,5
g71uy4y,j1dssf,"Quite frankly, that is the real issue and sometimes the catch-22: Need ""good data"" &lt;-&gt; need ml to identify ""good data"".",1
g7f08vq,j16sk9,very interesting and some timely news. Thanks for putting this together.,1
g6gznj8,iyz56u,"Normally ... when your data is in Petabytes a lot of normals go outside what is considered typical.

Firstly, petabytes of data can mean a lot of different things. If it is one giant table, queries will likely benefit from storage using Direct Attached Storage (DAS) ... the now out of favour paradigm compared to disaggregated storage and compute.

Maybe this isn't an issue if its many smaller tables?

You should also investigate whether you can convert data on prem before transferring it - converting a days worth of data to Parquet can reduce it by 80% which makes the problem much smaller. This may also help with continual ingestion though Avro is still regarded as a better wire format for smaller, continual updates.

Finally at Multi-Petabyte scale, it may still be cheaper to run in your own data centre.  Cloud is essentially a giant computer lease scheme; it offers customers the ability to scale up/down but those economies disappear at certain levels of scale. Even Kubernetes starts to become an issue when you need thousands of pods at the drop of a hat.

Long story short, it isn't a simple decision how you move to cloud and also worth considering whether a move will save you money or meet your business SLAs.",1
g6i5umb,iyz56u,"Buying hardware to store the data, then buying a tool to transform it, and *then* moving to a cloud service would induce huge capital and operational expenditure. At that point, you'd be much better off cost-wise to keep your data on-premises. 

The cloud is just a data center, you are paying for your data to have a spot on their hardware. The benefits of the cloud is less hands-on management for hardware, and being able to quickly scale up or down resources for changing or unpredictable demands. If that does not apply to you, there likely isn't any real benefit for a complete cloud migration.

There is no one-size-fits-all cloud architecture. It is completely possible to run cloud services on a company's own on-premises data center, or build a private cloud specified only for a company's data. How a company would approach cloud data warehousing really depends on how much you want to spend, and how much you want to manage.",1
g6f3q32,iyw3gh,"Nice work but some links seems to have wrong destinations.
I noticed that when I click View Profile for Anaconda, it takes me to Big Query.",4
g6f4qij,iyw3gh,Thanks! Fixed it.,2
g6f1t9g,iyw3gh,nice work :),2
g6eztoy,iyw3gh,"If you got feedback or find errata, let me know!",1
g6ge5xs,iyw3gh,"I couldn't find anything related to Spark, SparklyR, John Snow Labs NLP libraries etc.

Are they being intentionally omitted?",1
g6gf1tf,iyw3gh,"Hi u/prasannask  
  
Spark is in there. The other ones aren't in there (yet). Will definitely look into it!  
  
Thanks for the tips!  
  
Roel",1
g6gkg7g,iyw3gh,What about Kinetica :),1
g6f15og,iyw3gh,So it's a database of big data tools?,1
g6cmr0c,iycidx,"Just from a highlevel, Snowflake is like a huge manageable database, where as hadoop ecosystem and hdfs is more like a filesystem and tools to manipulate and manage data.  

So these two aren't mutually exclusive.  You may store a lot of data within hdfs / s3 of all kinds of unstructured data.  You will process that data with tools like spark and then will probably want it to be easily accessible and structured by storing the data in a data warehouse like Snowflake.  

Are there tools within the hadoop ecosystem to work like a sql database, yes.  Are there tools that snowflake provides that can do what you'd use hadoop for, yes.  I think there are a lot of other factors that you'd need to consider when making this type of decision.",4
g6eegp4,iycidx,Excellent answer!,1
g6caf7o,iycidx,"I have not worked on Snowflake. From what I briefly read, snowflake seems to be purely SaaS (cloud based service) . CDP has to be installed on a cloud vendor/On-prem. I don't think it's SaaS. CDP supports hybrid architecture (on prem, cloud, both). 

I don't know enough about snowflake components to list other differences. But CDP has CDF (data flow components), CML (machine learning) etc. Not sure Snowflake supports all that out of the box.",2
g6e3su6,iy7zam,"hi please check h2kinfosys for data scientist course

# Hadoop Tutorial HDFS, HIVE, PIG Online Training What is PIG and HIVE 

[https://www.youtube.com/watch?v=JojAD010OiQ](https://www.youtube.com/watch?v=JojAD010OiQ)",1
g69m9jt,ixyshj,"This isnt an article, its a sales page for 11 pages.",3
g66nfzo,ixduwo,This video is reposted,5
g67b6ps,ixduwo,"Awesome BD skills. Video editing, not so much.",2
g688pxr,ixduwo,yah I agree with you. I need to do better on video editing since I am not an expert in that.,3
g68d2yb,ixduwo,Can you give me some specific feedback? I would like to improve my skill,3
g68hg07,ixduwo,"i'm not the one you want to learn from - at least not to learn how to do it \*correctly\*. the only way I can learn is by failing. again. and again... and.... again...  ooooh, wait! no..... and failing....... AGAIN!  

I was looking more for your insight on the Big Data stuff. Not the video editing... it just came up as unavailable - so really no video editing to 'critique'. 

/sometimes i think i must be a masochist...",3
g68wyax,ixduwo,Yah I am learning too cuz it’s my first time making videos lol but thank you I always try to look for ways to improve like I know so many good ways to add animation or maybe make it more entertaining. Yah I am sorry about this video cuz I have to record it again to remove some sensitive information but if you want to check my other video to give me feedback. Channel name is Data Lu.,1
g6716of,ixduwo,RemindMe!2days,3
g6a2mwg,ixduwo,[https://www.youtube.com/watch?v=isVHh5DV07I](https://www.youtube.com/watch?v=isVHh5DV07I),1
g68rp0w,ixduwo,RemindMe!2days,1
g68tyjg,ixduwo,"There is a 18.0 minute delay fetching comments.

I will be messaging you in 2 days on [**2020-09-24 19:14:44 UTC**](http://www.wolframalpha.com/input/?i=2020-09-24%2019:14:44%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/bigdata/comments/ixduwo/start_from_excel_to_machine_learning_in_my_career/g68rp0w/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fbigdata%2Fcomments%2Fixduwo%2Fstart_from_excel_to_machine_learning_in_my_career%2Fg68rp0w%2F%5D%0A%0ARemindMe%21%202020-09-24%2019%3A14%3A44%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20ixduwo)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|",1
g6a2n43,ixduwo,[https://www.youtube.com/watch?v=isVHh5DV07I](https://www.youtube.com/watch?v=isVHh5DV07I),1
g651e1v,ix6kx1,Post hard drives. Azure Data Box.,7
g6522go,ix1lhj,"The Kafka streaming architecture automates all of the steps. It will call your model, predict results, and spin up more workers if needed on demand as data lands",1
g63bydo,iwyxn9,You can find some ideas here: [https://mynoteshala.wordpress.com/big-data-project-ideas/](https://mynoteshala.wordpress.com/big-data-project-ideas/),3
g63dp48,iwyxn9,Thanks,1
g651eyi,iwyxn9,maybe you can build a ml project [https://www.youtube.com/watch?v=wCZG\_dECQc4](https://www.youtube.com/watch?v=wCZG_dECQc4),1
g60mi13,iwj0lb,"If you can tell, what it does under the hood? That will be great",1
g60pb7i,iwj0lb,"It lets users model their IT infrastructure using a web interface (or, if preferred for automation purposes, an API).

This model contains the configuration files. An engine, using a proprietary extensible database of checks, is then run through this model and the concrete configuration files, and reports then on any failed checks. Each check has a severity assigned to it, from 1 = ""you can keep that configuration if you like, but it best practice would be X"" to 5 = ""Your data is currently up for grabs"".

This differs from current approaches to this problem, which are more reactive (like SIEM). We want to put the practices that are so ubiquitous in the world of code (e.g. PMD, Coverity) to the setup of compute clusters as well, to prevent human error.

If you wish to learn more or see a demo, feel free to reach out. We will probably soon post a video about it as well.",1
g5va3l1,ivanid,"Your contribution is very good and i totally agree that nowadays both large and small companies should consider making sense of the data, and take advantage of all the information that lives in the cloud or that in the same company where it is You want to make use of data mining, you have the history that you have stored of your operations that are called the catalog of clients, suppliers, among others.

It is also important to point out that companies, apart from what is data mining, can opt for Big Data since its function, like data mining, is to analyze large volumes of data that exceed the capacity of the usual computer processing. Its objective is to analyze all the information in the shortest possible time and efficiently.

But if we ask ourselves when and who can use big data or data mining from the moment that a company and organization have data stored on their customer or supplier activity, useful information can probably be extracted by analyzing that data.

In particular when a company has its information stored, for example of its customers, when analyzing the information it can determine how to better serve them, how to have them more satisfied, get more business or increase loyalty towards the company.

So your contribution was very valuable to me since companies must see beyond what they are used to working today and that they must turn to see all these new technologies and turn to data mining or big data for improvement of your company. And not stay with the past and that each data they handle or have can get more out of it than they get today.",1
g5wdjgs,ivanid,Thank you,1
g5f278u,iti9in,Tag,1
g572bwu,is8at8,"There are entire courses in classical statistics on the use of augmented and synthetic datasets (i.e., bootstrapping and others). It's a well-established method of building models with incomplete data, but that doesn't mean the models in turn always translate to the real world. Using this method to build an accurate real-world model is an annealing process, i.e., 1) make a synthetic dataset, 2) make a model, 3) test the model, 4) update the dataset after testing, 5) update the model, 6) repeat \[3-5\] until achieving the desired performance. 

The closer your data are to reality, the fewer times you have to repeat this process.",1
g87mioz,is8at8,"I can't access your quiz anymore for some reason, but I just wanted to pitch in with another benefit: [imbalanced datasets can be fixed with synthetic data technology](https://mostly.ai/2020/05/07/tackling-ai-bias-at-its-source-with-fair-synthetic-data-fairness-series-part-4/), so human bias in data can be corrected. It will also be very important in all the [behavioral datasets collected during Covid lockdowns](https://www.technologyreview.com/2020/05/11/1001563/covid-pandemic-broken-ai-machine-learning-amazon-retail-fraud-humans-in-the-loop/?cid=other-eml-onp-mip-mck&amp;hlkid=957f385c45e7435bb521c31f30f0b7b4&amp;hctky=12332578&amp;hdpid=0d4ce30b-2850-4eea-a646-66ff53fb6395). People behave very differently during these strange times and this can throw off algorithms trained on these skewed samples.",1
g55u6b7,is6069,[deleted],1
g55zh69,is6069,"This is something interesting I found that is relavent

[https://www.pingshiuanchua.com/blog/post/using-youtube-api-to-analyse-youtube-comments-on-python](https://www.pingshiuanchua.com/blog/post/using-youtube-api-to-analyse-youtube-comments-on-python)",1
g57fbe9,is6069,"I think things skew towards open source technology when dealing with social media.  A lot of people are on YouTube learning Python, R, or Spark while not a lot of people are going to be watching videos or commenting on Vertica or Teradata as they are more enterprise players.  

So, while I am sure there is information to be gleaned, I don’t know you will be replacing Gartner with data scraped from YouTube.",1
g55yhzb,irxuvs,Yes! Totally agree. So many use cases and helps answer a wide array of questions.,1
g5b6t3e,irxuvs,"This was a great read, thank you.",1
g51vgdf,irrpah,[deleted],1
g54c3k9,irrpah,"Maybe it is because the Nvidia GPU Acceleration plugin is not part of the Apache Spark project. The most they might mention in their release notes is “Support for Accelerator-aware Scheduling” ([SPARK-24615](https://issues.apache.org/jira/browse/SPARK-24615)), and support for custom Spark `DriverPlugin` implementations. 

It would’ve been nice to have that shoutout (i.e. infrastructure for custom accelerators like `spark-rapids`), but heck.",1
g54r9e0,irrpah,"NVIDIA GPU acceleration which covers in ""Accelerator-aware task scheduling for Spark"" is also one of the great features of Spark 3.0. This Part I features covers some, I am writing another article which covers more.",1
g4xy38i,ired4x,"Been working with Flink for about 2 years now, the way I see it the pros are big, it’s pretty efficient, the DataStream API is easy to work with, and the checkpoints and back pressure mechanisms work well and helps a lot.

As far as cons, I say joining stream windows on event time can be a bit tricky (if one stream is blocked it won’t have any output), if your running on yarn cpu starving is pretty common if you have more then one task managers on the same host. And, if you’re working with Scala, some sink connectors are annoying to work with in the sense of converting Scala types to java types to SQL types. 
But these are minors considering the whole state of Flink",3
g4y6rou,ired4x,"Thank you for the response.  


I am new to Apache Flink and at the end of my project I wanted to add the Pro/Cons section, but to be honest I couldn't individuate any disadvantage aspect.  


  
In another forum, someone responded to me with some disadvantages (his opinion) about Flink. I will list them here with you and others if you find them useful.  
Seems more as subjective opinions, but can fit the Pro/cons section.  


* Less community and forums for discussion: Flink may be difficult to understand starting as a beginner because there are not many active communities and forums to exchange problems and doubt about Flink features.
* Less open-source projects: There are not many open-source projects to study and practice Flink. The lack of a code source makes it very difficult the familiarization with the most innovative features and mechanisms it offers.
* Immaturity: Immaturity in the industry is a disadvantage for Apache Flink because is a new technology and many features are constantly being updated and modified. We should avoid Apache Flink if we need a more matured framework compared to other competitors in the same space.
* API support: Apache Flink is not the best choice if we need more API support apart from Java and Scala languages.
* Data representation: Flink uses raw bytes as internal data representation, which if needed, can be hard to program",1
g54l8jz,ired4x,Part of the reason you're not getting any response is because you aren't asking about a use case. The question is too broad to answer. The answer you received from another forum wasn't helpful. Read why https://www.jesse-anderson.com/2017/07/this-is-useless-without-use-cases/.,1
g4tjicx,iqmvfm,"I liked most of the content but the omissions stood out: why Hive &amp; Impala, but not BigQuery, Redshift, Snowflake, Teradata, Netezza, DB2, or CitusDB?",2
g4ozgnm,iq4bj4,"I’m sorry for this potentially nonproductive response, but: welcome to big data, things are messy.

You’ll need to figure out what criteria matters to you. The first thing that comes to mind is perhaps some kind of sentiment analysis. This would take into consideration the thread and see if it was actually about COVID or not",5
g4q83y7,iq4bj4,"Building on the other guy's ""welcome to big data"" answer, maybe you should just scrape from feeds you trust contain good information.


If you can't do that, then you need to think about what makes a tweet ""fundamentally right"" for you. You could then do things like looking for mentions of words, entities, pattern matching, semantic distance. Yada yada yada",3
g4j57zm,ip9m1p,Is it free?,1
g4iply5,ip9ipx,[deleted],1
g4ipym2,ip9ipx,haha I guess I should use the dude voice. I just can't hear my own voice.,2
g4mob5j,ip9ipx,Really nice intro for Data Engineers to know what their ML counterparts do !,1
g4mw9zr,ip9ipx,Thank you,1
g4hq6ac,ip2w87,"Clickhouse, because it's fast, SQL-compliant, MPP database with good number of features. Not very mature though.",5
g4iou4m,ip2w87,"oh, that's cool! How do you find the clickhouse eco-system? Snowflake  seems to have tons of integrations, partners, etc that are pretty powerful. Does something similar existing for Clickhouse?",1
g4i2n8p,ip2w87,"Oracle DB. Because legacy code base. 😭

Slowly converting to Spark and Hadoop.",2
g4i9598,ip2w87,"Last job we were using Redshift, new job we're using Snowflake (I start in about 2 weeks tho).",2
g4i11kx,ip2w87,"Azure Synapse analytics.
All the other tech were MSFT so is our data engineering framework.",1
g4iyeq4,ip2w87,"""Homemade"" data warehouse : A sorta lambda architecture with Spark/Hadoop for batching, and Kafka/Spark Streaming for streams.

Data is served to users through various PostgreSQL dbs,
and we have currently an Elasticsearch rolling out for timeseries and real-time analytics.",1
g4evxet,ion5we,Also interested!,1
g4f9cvh,ion5we,Me too. I have strong sas/sql/unix/python programming skills and am looking to learn big data to add to my repertoire. I would also like to contribute to a big data project if possible - show some productive activity during 2020.,1
g4f9qa8,ion5we,Following,1
g4gordm,ion5we,"I would recommend checking out [Presto GitHub](https://github.com/prestosql/presto) as well as the developer page containing [philosophy and how to contribute](https://prestosql.io/development/). Presto is a distributed analytics query engine that uses ANSI SQL, meaning it follows a standard querying language and Presto itself doesn't store the data but rather is just an engine that can perform complex queries of RDMBS, Hadoop, and other NoSQL data stores. Since you touch [so many types of databases](https://prestosql.io/docs/current/connector.html), it's easy to start working on the database/data model connector that you're familiar with and that will help you understand Presto basics. Also if you're wanting to break into new data stores, you can then transfer over contributing to other connectors as you feel like. Otherwise, if you want to learn about data engine specifics like how the parser, query planner, and optimizer works, you could always work on core Presto.This project is used and contributed to by a bunch of high profile companies like Uber, Netflix, LinkedIn, and was born out of Facebook.To get a history, there are a few recent podcasts that have come out about this:

* [https://www.contributor.fyi/presto](https://www.contributor.fyi/presto) \- This is an interview with the founders of Presto talking about the origin story and where things are headed
* [https://www.dataengineeringpodcast.com/presto-distributed-sql-episode-149/](https://www.dataengineeringpodcast.com/presto-distributed-sql-episode-149/)  \- This is an interview with Presto co-creator Martin Traverso jumping into where Presto fits in the Big Data architecture.

I will also be starting running a twitch show that highlights the efforts of our contributors by discussing their PRs, talk Presto news, etc... More to come on that later but here are a few test interviews we did for our first episode.

* [How do you make a thriving developer community around Presto?](https://www.twitch.tv/videos/729152420)
* [Favorite part of implementing a feature?](https://www.twitch.tv/videos/729201925)

To find out more, find me on the [presto community slack](https://prestosql.slack.com/) just look up ""Brian Olsen"" or you can post in the #general or #dev channels. This slack [community is 2,300 strong](https://twitter.com/prestosql/status/1278393800092643328) and growing. This includes the founders and many of the early contributors to the project who are personally invested in nurturing and growing the community to make it the most inclusive and make presto the best analytics engine.",1
g4ed7yw,ioh9ts,Umm...what else would you optimize for?,1
g4dsxp3,ioetic,"Just curious, why is it that all these companies don’t have R listed, but Python over R? Is python really that much more superior? 

I’m a predominantly R user myself, as far as statistical language. I know SQL, and a small amount of SAS. It’s surprising to me that python is listed and R is not. 

Just seeing if anyone had any insight into this.",2
g4e7ayw,ioetic,I am curious too. I’ve started learning python lately because I’ve been so frustrated of the lacking areas of R,1
g4hdkn5,ioetic,"Python is the most common, but having R in your skillset is valuable too",1
g4cv3k5,io955z,"I have worked in big data for a while, and streaming and real-time projects for the last 5 years or so.

Kafka is very popular for large platforms, Kinesis is also popular.

Hadoop platforms are still prevalent in large orgs, but cloud native is growing faster.

Spark is very popular

Most devs I know use IntelliJ and git , CI/CD depends on the org.

Cluster size - I have worked on everything from server less with 3 containers, to 5000 node clusters.

Data size - if it’s small enough to fit on one machine it’s not big data (but people will still act like it is). Most frequently I am working with GB to TB per day.  Largest I personally have dealt with is .8PB per day.",2
g4fweps,io955z,Thank you for sharing :),1
g4be5sr,inz7rg,https://www.youtube.com/channel/UCR5fQQoUywhm3tE3O0HSVVg,1
g42oamm,ims0ji,"Not sure if this is what you are looking for.

""FoodData Central"" https://fdc.nal.usda.gov",2
g3wbzto,ilrxl0,Who are y’all dating ...,1
g3y3daj,ilrxl0,Always thought data mining was just scrapping data from other places but at a larger scale.,1
g3vcs2z,ilrxl0,Omg thank you. I just joined a data consulting firm in sales and this is very helpful. Any others to add that you or others can think of would be very appreciated.,0
g3u2ffu,ilr90i,"If there is any girl DE who knows all the above , lease ping me , I want to marry you.

The list is all exhaustive ..
 So some people know all these stuff ?",1
g3ttrde,ilnsgf,"So lake = dump all data in the cheap cloud for the future
Warehouse = ETL with a plan for today's use",1
g3rtbi1,ilen9c,Stop screaming ya pansie,1
g3r9npl,ildec4,"To do what? The question is too vague. Is it speech recognition, translation, etc.?",3
g3r9wvf,ildec4,By process I mean run transformations on the data and put it through a Deep Neural Network.,1
g3rnx1u,ildec4,"I think the pre-processing of the audio data will be relative lightweight when compared to the actual forward pass of the Neural Network (specially for very large models). So I would  suggest to focus on the hardware that improves the network performance.

&amp;#x200B;

A GPU is great to speed up the Network's calculations, where a mid-range gaming GPU will outperform a high end CPU by 10x easily.",1
g3tsaan,il8w00,Does it mean we can apply data mining technique on football games result data and make bets based on that?,1
g3v9gkd,il8w00,"Yes, absolutely!",1
g3oivy7,ikzecl,"Lot of words and no concrete examples of anything real word. Still, decent read.",1
g3kiq6f,ikgfdd,Not bad but would be nice to see a CDP deployment on OpenShift or CDP Public deployment war stories.,1
g3dc5qw,ijeiqd,Following,2
g3ek911,ijeiqd,"Your data volume no where approaches big data - and is *fully* within the realm best served by RDBMS.  The data is structured, and te tasks you've identified are fully capable of being run off of a RDBMS.  There is *no* need for any big data technologies at all here - unless your intent is to learn big data technologies.

So really - what's the purpose of this data repository - the tasks identified above, or to apply the tasks above using big data tech?  If its just the listed tasks - stick with a RDBMS.",2
g3em5hm,ijeiqd,"Completely agree here - note that you’ll likely need some indexes to get good performance on queries but it is well within the capabilities. If it’s an option for price / platform, Microsoft SQL does have Python and R interfaces for ML.",2
g3g4ear,ijeiqd,"Thanks for your answer! My purpose is to apply data analysis, data mining and deep learning.

&amp;#x200B;

But as you and u/guacjockey said, you feel I am better served with simple R or python interfaces for a simple RDBMS right?

&amp;#x200B;

And what about the data not leaving provider institutions? not pushing my luck here, but do you reckon there are alternatives in the market to support this case and RDBMS?

&amp;#x200B;

Thanks for the help!",1
g3h6zei,ijeiqd,"It gets tricky - depending on what the source data is stored in you might be able to create a linked server / foreign-data-wrapper/ etc. But the difficulty is at some point the data will leave said institutions during query, you just may not be storing it. The question then comes of whether performance is adequate for whatever analysis you want to run (and/or without blasting their servers in the process).",2
g33fq4c,ihvaid,"If you want ""tools that will maintain their relevance a few years down the line"", you're investing in the wrong industry at the wrong time. We've just hit the data boom. Things are moving fast.",2
g33mnvc,ihvaid,Cloudera = dead bro,2
g39q6md,ihvaid,So is Aws EMR or azure hdinsight pave the way for Hadoop ?,1
g30cisl,ihicop,That site has toxic ads. Can't even seem to click past them.,1
g2u2qal,ig7tk4,I think Holden Karau is doing similar things. You could ping her.,1
g2w8nl2,ig7tk4,"Thanks for the recommendation, Im on it.",1
g2perbi,ifr81g,"You can define the schema prior to load and run the various import modes to determine the rows that work. You can also use the `columnNameOfCorruptRecord` option to save off (most of) the bad data. Note however that it doesn’t count extra / fewer columns as corrupt. To work around that, you can do the load everything into a single column dataframe and manually process the data (ie, perform splits on the data).

Beyond that, the GreatExpectations package will generate Spark usable code to further validate data.",2
g2jmryu,ieqj93,Data discovery refers to finding the source of truth about your data (think data catalogs and metadata about your data) whereas data profiling relates to summarizing or collecting statistics about your data.,3
g2jmfnb,ieqj93,I use the term with my network engineers when we are figuring out what data sources are available and what columns they have.,2
g2irfx3,ieqj93,"I don't know the answer but I wanna add that this community is very offline. There are barely discussion threads, comments interactions. I used to love it but these days is getting more and more of a ""spam"" target.  


Maybe I am wrong and this is only my point of view. Hope you will receive your aspected response and have fun with data.  


Best regards",1
g8puhlw,jahgte,"I'll speak very generally. Obviously each position is going to differ, but here's a starting skillset that I see a lot of the people we interview have:

Language:  Many people see to be using Python now. I'm old school and before ML took off used C or Java for all my workhorse apps.  But python seem to be what a lot of data scientists coming out of school as well as in the field are using to analyze their data. (R is great for statistics as well and seems to have replaces SAS in a lot of way)

Frameworks: There's quite a few machine learning libraries to play with if you go with Python.  From a 'big data' perspective, learning Spark as a way to access large, distributed datasets is a good start. It can take a bit to 'master', but at least learning the concepts behind it and distributed data would go a long way. 

You can use the above technologies with one another. Spark with Python (PySpark) works really nicely.  Many spark developers also use the Scala language, but for whatever reason I see a lot of data scientists coming out of school using Python, not Scala (or Java) for accessing Spark.

Something like Jupyter is an easy way to get started with Python (and other languages) as well as accessing Spark via Python.  Our data scientists use Jupyter notebooks as a way to access, manipulate and visualize data all the time.  Eventually any models they create this way gets run outside the fancy Jupyter GUI environment when productionalzed, but it's a great tool for getting started and creating your models and seeing the results.

There's probably no wrong answers here, but that's what I've seen our own data scientists use as well as kids we're recruiting coming in with.",1
g8otn5a,jac1go,"Bounces through tracker, gets ad blocked.  Share actual post URL.",3
g8nwajw,ja2aja,Pfffffffffff,1
g8n9bpt,ja1hg8,"My data engineering team currently manage tens of terabytes of data in the healthcare space, mostly structured and semi-structured data. Lots of batch processing and an increasing amount of stream processing and/or micro-batching.

The technologies we use and look for experience with include Git, Bash, Python, SQL, Docker, Airflow, Snowflake, Kafka, Kubernetes, and several AWS services (EC2, EKS, S3, EFS, SQS, and AGW.)

We don’t currently use Spark but you can’t go wrong learning it and I would definitely prioritize it. We are also considering adding IaC knowledge like Terraform, not as a requirement but a nice to have, but this is more situational.

Industry specific knowledge can be helpful—like HL7 or FHIR for healthcare interoperability. Same for experience with master data management and compliance initiatives—although these can certainly be learned on the job.

The field is very broad. Might not hurt to look at job ads in regions you’d like to work from (or remote) and see if there are particular trends.

Best of luck!",3
g8nhmc9,ja1hg8,"Thanks a bunch for the great info! I am in banking and financial infra. We have tables that are in the 1tb range (1.5 to 5 billion records) with a mix of structured and unstructured data.

My main technologies are hadoop framework using hive and Unix mainly. I have basic exposure to spark. My skills with java and Python are minimal professionally. But I can understand what I'm looking at and make code changes if needed. I am also familiar with git and how it works. We mainly use in house tools for or ETL processes and version control. So I'm definitely going to get very familiar with git.


Like I said, I have only 3 ish total years of experience. So I'm still super new. Im also very young. Its very daunting and I feel the imposter syndrome creep up when I look at the trends and at the job openings I pass by.",2
g8nsiny,ja1hg8,"Get good at Python, SQL, and Spark. Then learn AWS products like Lambda, Athena, Redshift, Glue and of course, S3. Study machine learning - look up the book Real World Machine Learning as a starting place. Another useful language might be R, but is concentrate on the above first.",1
g8nx6ue,ja1hg8,Thanks!,1
g8nv48z,ja1hg8,"Learn Spark (Scala/python). Along with that spend some time on Azure or AWS... Having good understanding of cloud architecture is becoming must. Having some streaming knowledge with Kafka, Spark streaming would be huge add on.",1
g8nx8u4,ja1hg8,Alright I'll check those out! Thanks!,1
g8nixhl,ja025x,"It should add quite a bit to the existing Flink play. This video helped me put it in context:

[https://www.youtube.com/watch?v=PUI3I\_v8q8M&amp;feature=emb\_logo](https://www.youtube.com/watch?v=PUI3I_v8q8M&amp;feature=emb_logo)",1
g8nj0mr,ja025x,"Also, not every company needs near-real time so that should be a good indicator of whether it's relevant to you. If you have Spark Streaming today ... maybe:-)",1
g8ox8dy,ja025x,"Yeah ok, seems kinda pointless to acquire though",1
g8oac3y,j9rzml,"If big data were too big, people would already be scared, and therefore dishonest.

But it rolls off the tongue like a good headline, so it may be worth enough to Murdoch's people to buy you a small farm, and we definitely need to get this pesky fertilizer out of the air.

Good luck.",1
g8oc3rl,j9rzml,"In this context of too big I mean are we even aware of what data we are giving up how, why, who and whether awareness make any difference to the perception we initially displayed. In fact a lot of my data is suggesting a fear for data which is beyond the control of people despite their depth knowledge on the topic and thus they become tolerant this is a natural human phenomenon known as loss-aversion this is most apparent in gen z who have grown up in an era of “false privacy” whereas gen y display actions and attitudes of “data activists” these are data haters and will avoid or disrupt the authenticity of their own data. I personally believe there is an ignorance to this debate a lot of people don’t have enough knowledge in the topic, from what’s possible to their rights. This research is to identify the difference knowledge makes. See I don’t believe it’s too big per say it is very beneficial but I believe the advances in technology have far surpassed the law which enables companies to take and use data easier than most would think which to me is still not okay. There is a higher priority on profitability from data collection than there is on privacy.",1
g8odmqv,j9rzml,"It's an ""indirect self-reference"" issue. However one asks ""has diversifying analogies of asking, includes those that are undisclosed, including old-fashioned surveillance, already got  scary?""

They may think ""asking whether something like asking already scares me? that sounds like asking to become pretty scary, we'd better sound brave or the scary man will hurt us - eventually - no, let's just leave.""

Indirect self-reference can be a source of intrinsically biased sampling.",1
g8knaov,j9lwmj,"Cloudera quick VM, the first option, is a good option to start and play with a Hadoop cluster. 

Option 2 isn’t so hard, but always make sure that the requirements are met before installation, so you don’t spend days in troubleshooting to find out it can’t work.",3
g8kshno,j9lwmj,"**What I did:** 

1. Downloaded Ubuntu ISO file
2. Created 3 Virtual Machines 1 or 2 GB RAM + 1 Core CPU + 10 GB HDD + Networking ON
3. Installed Ubuntu in all 3 machines
4. Created Passwordless SSH connection between all the three Machines \[all combinations\] . (Most important step)
5. Downloaded Apache Hadoop form [archive.apache.org](https://archive.apache.org)
6. Copied Configuration Files + .bashrc files from some repositories on Github
7. scp hadoop and files to all the three machines

Thats all. Then you can just start working around it. If you understand these steps. Installing other things such as Spark, Hive, Hbase, Zookeeper etc will be easier.   


I am myself new into it but tried installation a few times.",2
g8kkb6x,j9lwmj,"I am a beginner and have tried installing multiple times by the 2nd method. Never was able to get it to work... Got disheartened and never wanted to try again. 
I did end up finding a virtual machine with the ecosystem already installed... I downloaded that and it worked real good. Maybe you should try that too! Saves you the trouble of trying to do it yourself... Unless you want to explore!",1
g8klx2e,j9lwmj,Would you please tell me specifically which virtual machine and ecosystem u used?,1
g8kn43v,j9lwmj,Probably from a relevant online course.,3
g8kpj5t,j9lwmj,would you like to suggest one course which you would recommend to beginners ?,1
g8kr4xs,j9lwmj,"I've looked through my coursework from the past, and it seems like The Data Scientist's Toolbox from Coursera still exists! I'm not sure if they offer premade VMs as part of the coursework, those memories might actually be from other courses I did (involving genomics, so probably not very relevant to you).",1
g8k9b9z,j9ist1,"I think before you try read a specific technology, you need to look at the volumetrics of your use case and identify your driver for a big data solution.

If you come at this with a mentality to learn ""a product"" you won't gain much depth in this field. If you were to instead look at the problems which were solved by a big data solution you will understand the products better in their context.",2
g8k3nvp,j9ist1,"Spark, kafka, hdfs, cloud native storage like adls, s3 and lot more.",1
g8k43ke,j9ist1,"Damit.

Are there not 1 or 2 that are standards or do i need to learn all of them?

I am actually playing around with Tableau.",1
g8k4b12,j9ist1,"Well in that case, it is Apache spark. Spark is the distributed data processing framework, basically you have to code in programming language like scala, python, Java etc. Tableau, power bi are more like for visualization and lighter data analysis.",4
g8k4r4s,j9ist1,"&gt;Apache spark

Does Apache Spark stand for Big data as Photoshop for Photo editing?

And thank you so much for your help it was really very useful because i got confused on searching on the net. Everyone tells you something different",2
g8k4zb1,j9ist1,"All good, you can start reading into it and you will get a better understanding. And spark is pretty much like Adobe photo shop for editing, and prior to spark, it used to be apache hadoop.",1
g8k52xq,j9ist1,"Ok 

Thank you my friend and have a nice day",1
g8mwfxt,j9ist1,"Because big data is so vaguely defined, that's why everyone tells something else. 
I am genuinely interested what volume is for you big data. 
Spark just won in the marketing buzzword claims.",1
g8k6n26,j9ist1,Google CNCF Landscape.,1
g8k861h,j9ist1,"As others have mentioned there isnt a single answer. Although Spark is capable to handle a lot of the scenarios commonly encountered in data pipelines and even extends further (supports some ML algorithms).

I think its a good starting point and then you could easily move to other frameworks. A lot of them are built on Java/Scala so its also a good idea to get familiar with those languages",1
g8jl7ys,j9g61r,"There’s a lot about that question that I either disagree with or is out of date. It’s worded in such a way that it sounds like it’s based off a certain set of material. 

That said, if I were to take that question as is, I’d reference the particular programming model and the particular storage model that Hadoop implements and discuss the advantages (and possibly disadvantages if you wish). The Wikipedia page should give you enough info to dig further.",2
g8jqm85,j9g61r,"This is a badly worded question, and I agree that it's a little out of date. If I were asked this, I would say something about 

1. Bringing compute to the data

2. Schema on read

However, I suspect that the ""right"" answer is in your notes somewhere. Good luck!",1
